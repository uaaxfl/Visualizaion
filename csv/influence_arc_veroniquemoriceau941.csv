2010.jeptalnrecital-court.6,tannier-moriceau-2010-fidji,1,0.840409,"Missing"
2010.jeptalnrecital-court.6,C08-1120,0,0.0649711,"Missing"
2011.jeptalnrecital-court.6,N10-1086,0,0.0502098,"Missing"
2011.jeptalnrecital-court.6,levy-andrew-2006-tregex,0,0.0623288,"Missing"
2011.jeptalnrecital-court.6,N10-1048,0,0.0309641,"Missing"
2011.jeptalnrecital-court.6,W03-1605,0,0.0322934,"Missing"
2015.jeptalnrecital-court.22,P11-2102,0,0.0580601,"Missing"
2015.jeptalnrecital-court.22,W07-0101,0,0.139745,"Missing"
2015.jeptalnrecital-court.22,W13-1605,0,0.0721841,"Missing"
2015.jeptalnrecital-court.22,W11-1715,0,0.0440982,"Missing"
2015.jeptalnrecital-court.22,D13-1066,0,0.0453047,"Missing"
2015.jeptalnrecital-court.22,C96-2162,0,0.208197,"Missing"
2015.jeptalnrecital-court.40,moriceau-tannier-2014-french,1,0.880988,"Missing"
2015.jeptalnrecital-long.3,P10-1052,0,0.066151,"Missing"
2015.jeptalnrecital-long.3,moriceau-tannier-2014-french,1,0.877846,"Missing"
2015.jeptalnrecital-long.3,S10-1071,0,0.0564766,"Missing"
2015.jeptalnrecital-long.3,P05-3021,0,0.110123,"Missing"
2019.jeptalnrecital-court.21,S19-2007,0,0.078056,"Missing"
2019.jeptalnrecital-court.21,esuli-sebastiani-2006-sentiwordnet,0,0.0472632,"Missing"
2019.jeptalnrecital-court.21,P16-1190,0,0.0566159,"Missing"
2019.jeptalnrecital-court.21,W17-3007,0,0.120871,"Missing"
2019.jeptalnrecital-court.21,L18-1550,0,0.0625283,"Missing"
2019.jeptalnrecital-court.21,I17-1093,0,0.0406281,"Missing"
2019.jeptalnrecital-court.21,W17-2902,0,0.289939,"Missing"
2019.jeptalnrecital-court.21,D14-1162,0,0.08064,"Missing"
2019.jeptalnrecital-court.21,W17-1101,0,0.0458601,"Missing"
2019.jeptalnrecital-court.21,N16-2013,0,0.116852,"Missing"
2020.acl-main.373,C18-1139,0,0.0226767,". BERTR features . We relied on state of the art features that have shown to be useful for the task of hate speech detection: Surface features (tweet length in words, the presence of personal 10 In case a particular web page is not available anymore, the URL is removed from the tweet. 11 We relied on a manually built emoji lexicon that contains 1,644 emojis along with their polarity and detailed description. 12 We experimented with different configurations by incorporating different French pre-trained embeddings available: Glove (Pennington et al., 2014), FastText (Grave et al., 2018), Flair (Akbik et al., 2018) and CamemBERT (Martin et al., 2019) but none of the configurations were able to achieve results better than BERTbase . 13 https://github.com/google/ sentencepiece 4060 pronoun and third-person pronoun, punctuation marks, URLs, images, hashtags, @userMentions and the number of words written in capital), Emoji features11 (number of positive and negative emojis), Opinion features (number of positive, negative and neutral words in each tweet relying on opinion (Benamara et al., 2014), emotion (Piolat and Bannour, 2009) and slang French lexicons. We also account for hedges (negation and modality),"
2020.acl-main.373,W14-6305,1,0.805459,"Missing"
2020.acl-main.373,W19-3621,0,0.0111274,"h-Tweets 4056 istics, reference to private life, etc. From a sociological perspective, studies focus on social media contents (tweets) or SMS in order to analyze public opinion on gender-based violence (Purohit et al., 2016) or violence and sexist behaviours (Barak, 2005; Megarry, 2014). Gender bias in word embeddings. Bolukbasi et al. (2016) have shown that word embeddings trained on news articles exhibit female/male gender stereotypes. Several algorithms have then been proposed to attenuate this bias (Dev and Phillips, 2019) or to make embeddings gender-neutral (Zhao et al., 2018), although Gonen and Goldberg (2019) consider that bias removal techniques are insufficient. Debiased embeddings were used by Park et al. (2018) observing a decrease in sexism detection performance compared to the non-debiased model. To overcome this limitation, Badjatiya et al. (2019) propose neural methods for stereotypical bias removal for hate speech detection (i.e., hateful vs. non-hateful). They first identify a set of bias sensitive words, then mitigate their impact by replacing them with their POS, NER tags, K-nearest neighbours and hypernyms obtained via WordNet. Automatic sexism detection. To our knowledge, the automat"
2020.acl-main.373,N19-1423,0,0.0821389,"Missing"
2020.acl-main.373,L16-1218,0,0.0217864,"Missing"
2020.acl-main.373,S19-2009,0,0.101898,"tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype, objectification, sexual harassment, threat of violence, dominance and derailing. Most participants used SVM models and ensemble of classifiers for both tasks with features such as n-grams and opinions (Fersini et al., 2018b). These datasets have also been used in the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter shared task at SemEval 2019. Best results were obtained with an SVM model using sentence embeddings as features (Indurthi et al., 2019). There are also a few notable neural network techniques. Jha and Mamidi (2017) employ an LSTM model to classify messages as: benevolent, hostile and non-sexist. Zhang and Luo (2018) implement two deep neural network models (CNN + Gated Recurrent Unit layer and CNN + modified CNN layers for feature extraction) in order to classify social media texts as racist, sexist, or non-hateful. Karlekar and Bansal (2018) use a single-label CNN-LSTM model with character-level embeddings to classify three forms of sexual harassment: commenting, ogling/staring, and touching/groping. Sharifirad et al. (2018)"
2020.acl-main.373,W17-2902,0,0.626604,"not be moderated. As far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Au"
2020.acl-main.373,D18-1303,0,0.520173,"orts/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Austin, 1962) and discourse studies in gender (Lazar, 2007; Mills, 2008). We"
2020.acl-main.373,2020.acl-main.645,0,0.174441,"Missing"
2020.acl-main.373,D19-1474,0,0.0110437,"ton, 2012; Bianchi, 2014) or concentrate on the analytical level at which the derogatory content is interpreted, whether it provides meaning at the level of the presupposition (or more largely non at-issue content (Potts, 2005)) or of the assertion (Cepollaro, 2015). We have chosen to distinguish cases where the 4057 addressee is directly addressed from those in which she is not, as done in hate speech analysis. For example, Waseem et al. (2017) and ElSherief et al. (2018) consider that directed hate speech is explicitly directed at a person while generalized hate speech targets a group. For (Ousidhoum et al., 2019), a hateful tweet is direct when the target is explicitly named, or indirect when ”less easily discernible”. Unlike these approaches and the definitions of target used in (Basile et al., 2019; Fersini et al., 2018a), we do not consider the number of targets of a sexist message (it can indifferently be a woman, a group of women or all women) but rather distinguish the target from the addressee. Our use of the notions of directness and indirectness are also transverse to the ones used in (Lazar, 2007; Chew and Kelley-Chew, 2007) or (Mills, 2008), who resort to the label indirectness for subtle f"
2020.acl-main.373,D19-1174,0,0.461026,"m experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Austin, 1962) and discourse studies in gender (Lazar, 2007; Mills, 2008). We distinguish different"
2020.acl-main.373,D18-1302,0,0.0370247,"a contents (tweets) or SMS in order to analyze public opinion on gender-based violence (Purohit et al., 2016) or violence and sexist behaviours (Barak, 2005; Megarry, 2014). Gender bias in word embeddings. Bolukbasi et al. (2016) have shown that word embeddings trained on news articles exhibit female/male gender stereotypes. Several algorithms have then been proposed to attenuate this bias (Dev and Phillips, 2019) or to make embeddings gender-neutral (Zhao et al., 2018), although Gonen and Goldberg (2019) consider that bias removal techniques are insufficient. Debiased embeddings were used by Park et al. (2018) observing a decrease in sexism detection performance compared to the non-debiased model. To overcome this limitation, Badjatiya et al. (2019) propose neural methods for stereotypical bias removal for hate speech detection (i.e., hateful vs. non-hateful). They first identify a set of bias sensitive words, then mitigate their impact by replacing them with their POS, NER tags, K-nearest neighbours and hypernyms obtained via WordNet. Automatic sexism detection. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the A"
2020.acl-main.373,D14-1162,0,0.0903832,"Missing"
2020.acl-main.373,D15-1309,0,0.0293218,"e said “who’s gonna take care of your children when you are at ACL?”). Indeed, whereas messages could be reported and moderated in the first case as recommended by European laws, messages relating sexism experiences should not be moderated. As far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types o"
2020.acl-main.373,W18-5114,0,0.084013,"far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Austin, 1962) and discourse"
2020.acl-main.373,N19-1214,0,0.0217527,"Missing"
2020.acl-main.373,W17-3012,0,0.0266952,"ated in a variety of manners. Most accounts however either focus on the type of act (assault-like, propaganda, authoritative, etc.) that derogatory language performs (Langton, 2012; Bianchi, 2014) or concentrate on the analytical level at which the derogatory content is interpreted, whether it provides meaning at the level of the presupposition (or more largely non at-issue content (Potts, 2005)) or of the assertion (Cepollaro, 2015). We have chosen to distinguish cases where the 4057 addressee is directly addressed from those in which she is not, as done in hate speech analysis. For example, Waseem et al. (2017) and ElSherief et al. (2018) consider that directed hate speech is explicitly directed at a person while generalized hate speech targets a group. For (Ousidhoum et al., 2019), a hateful tweet is direct when the target is explicitly named, or indirect when ”less easily discernible”. Unlike these approaches and the definitions of target used in (Basile et al., 2019; Fersini et al., 2018a), we do not consider the number of targets of a sexist message (it can indifferently be a woman, a group of women or all women) but rather distinguish the target from the addressee. Our use of the notions of dir"
2020.acl-main.373,N16-2013,0,0.113851,"ping this pregnant woman shooting), and messages which relate sexism experiences (e.g., He said “who’s gonna take care of your children when you are at ACL?”). Indeed, whereas messages could be reported and moderated in the first case as recommended by European laws, messages relating sexism experiences should not be moderated. As far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et a"
2020.acl-main.373,D18-1521,0,0.0181732,"-for-Sexism-Detectionin-French-Tweets 4056 istics, reference to private life, etc. From a sociological perspective, studies focus on social media contents (tweets) or SMS in order to analyze public opinion on gender-based violence (Purohit et al., 2016) or violence and sexist behaviours (Barak, 2005; Megarry, 2014). Gender bias in word embeddings. Bolukbasi et al. (2016) have shown that word embeddings trained on news articles exhibit female/male gender stereotypes. Several algorithms have then been proposed to attenuate this bias (Dev and Phillips, 2019) or to make embeddings gender-neutral (Zhao et al., 2018), although Gonen and Goldberg (2019) consider that bias removal techniques are insufficient. Debiased embeddings were used by Park et al. (2018) observing a decrease in sexism detection performance compared to the non-debiased model. To overcome this limitation, Badjatiya et al. (2019) propose neural methods for stereotypical bias removal for hate speech detection (i.e., hateful vs. non-hateful). They first identify a set of bias sensitive words, then mitigate their impact by replacing them with their POS, NER tags, K-nearest neighbours and hypernyms obtained via WordNet. Automatic sexism dete"
2020.lrec-1.175,2019.jeptalnrecital-court.21,1,0.827557,"sexism detection in French. Furthermore, in a context of offensive content moderation on social media (see the recommendations of the European commission6 ), we think that it is important not 6 only to be able to automatically detect messages with a sexist content but also to distinguish between real sexist messages and reports/denunciations of sexism experiences. Indeed, whereas messages could be reported and moderated in the first case, messages reporting sexism experiences should not be moderated. 3. 3.1. Data and Annotation Data Collection Our corpus is new and extends the one we used in (Chiril et al., 2019). It contains French tweets collected between October 2017 and May 2018. In order to collect sexist and non sexist tweets, we followed Anzovino et al. (2018) approach using: • a set of representative keywords: femme, fille (woman, girl), enceinte (pregnant), some activities (cuisine (cooking), football, ...), insults, etc., • the names of women/men potentially victims or guilty of sexism (mainly politicians): Ségolène Royal, Nadine Morano, Theresa May, Hillary Clinton, Dominique Strauss-Kahn, Nicolas Hulot, etc., • specific hashtags to collect stories of sexism experiences: #balancetonporc, #s"
2020.lrec-1.175,N19-1423,0,0.065004,"Missing"
2020.lrec-1.175,L18-1550,0,0.0299867,"Missing"
2020.lrec-1.175,S19-2009,0,0.0612504,"d ensemble of classifiers for both tasks with features such as n-grams and opinions (Fersini et al., 2018). Very few participants used deep learning approaches with word embeddings and best results were obtained with SVM models. These datasets have also been used in the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter shared task at SemEval 2019. The tasks were the same as those of AMI except that it concerned not only sexism against women but also hate speech against immigrants. The best results were obtained with a SVM model using sentence embeddings as feature (Indurthi et al., 2019). As far as we know, no work have addressed sexism detection in French. Furthermore, in a context of offensive content moderation on social media (see the recommendations of the European commission6 ), we think that it is important not 6 only to be able to automatically detect messages with a sexist content but also to distinguish between real sexist messages and reports/denunciations of sexism experiences. Indeed, whereas messages could be reported and moderated in the first case, messages reporting sexism experiences should not be moderated. 3. 3.1. Data and Annotation Data Collection Our co"
2020.lrec-1.175,W17-2902,0,0.664991,"7), (Waseem and Hovy, 2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the Automatic Misogyny Identification (AMI) shared task at IberEval and EvalIta 2018, the tasks consisted in detecting sexist tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype"
2020.lrec-1.175,P15-2106,1,0.88968,"Missing"
2020.lrec-1.175,D19-1474,0,0.0124371,"ther it provides meaning at the level of the presupposition (or more largely non at-issue content (Potts, 2005)) or of the assertion (Cepollaro, 2015). Our study pursues a different line of analysis, whereby speech acts bearing on derogatory content are ranked according to their perlocutionary force and assertions are classified as more or less direct. Specifically, in order to make emerge different degrees of downgrading tones, we have chosen to distinguish cases where the addressee is directly addressed from those in which she is not, as done in hate speech analysis (ElSherief et al., 2018; Ousidhoum et al., 2019). ElSherief et al. (2018) consider that directed hate speech is explicitly directed at a https://eur-lex.europa.eu/legal-content/ EN/TXT/HTML/?uri=CELEX:52017DC0555 1398 person while generalized hate speech targets a group. For (Ousidhoum et al., 2019), a hateful tweet is direct when the target is explicitly named, or indirect when &quot;less easily discernible&quot;. Unlike these approaches, we newly consider three different stages in the scale of ‘directedness’ of an assertion: assertions directed to the addressee, descriptive assertions and reported assertions. Sexist content in directed assertions i"
2020.lrec-1.175,D19-1174,0,0.206688,"ainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the Automatic Misogyny Identification (AMI) shared task at IberEval and EvalIta 2018, the tasks consisted in detecting sexist tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype, objectification, sexual harassment, threat of violence, d"
2020.lrec-1.175,W17-1101,0,0.0274121,"on 4. presents the experiments we carried out on our data. We conclude providing some perspectives for future work. 2. Related Work In corpus construction, sexism is often considered as “hate speech” (Golbeck et al., 2017) and has been widely studied as such in a purpose of automatic detection (Badjatiya et al., 2017), (Waseem and Hovy, 2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only wit"
2020.lrec-1.175,W18-5114,0,0.0121795,"2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the Automatic Misogyny Identification (AMI) shared task at IberEval and EvalIta 2018, the tasks consisted in detecting sexist tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype, objectification, sexual"
2020.lrec-1.175,N19-1214,0,0.041673,"Missing"
2020.lrec-1.175,N16-2013,0,0.0780955,"ts are encouraging and constitute the novel state of the art on sexism detection in French. The paper is organized as follows. Section 2. presents state of the art. Section 3. describes our data, the characterization of sexism content we propose and the annotation scheme. Section 4. presents the experiments we carried out on our data. We conclude providing some perspectives for future work. 2. Related Work In corpus construction, sexism is often considered as “hate speech” (Golbeck et al., 2017) and has been widely studied as such in a purpose of automatic detection (Badjatiya et al., 2017), (Waseem and Hovy, 2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Shari"
2021.findings-emnlp.242,2020.lrec-1.175,1,0.865363,"Missing"
2021.findings-emnlp.242,S19-2007,0,0.0226553,"gnize myself in the &quot;Just Fab&quot; ad with a media is mainly supported by dedicated shared screaming hysterical bitch?. tasks that developed their own datasets, for ex• Activities are activities, jobs, hobbies that are ample the AMI corpus mentioned above. These stereotypically assigned to women as in Never datasets (in English, Spanish and Italian) have also marry a woman who cannot cook which imbeen used in the Multilingual Detection of Hate plies that a woman’s place is in the kitchen, or Speech Against Immigrants and Women in Twitter no woman understands football. shared task at SemEval 2019 (Basile et al., 2019). Compared to existing datasets annotated for GS, Best results were obtained with an SVM model ours offers a finer characterization (e.g., 2 cateusing sentence embeddings as features (Indurthi gories in (Parikh et al., 2019) and only 1 in AMI), et al., 2019). Lazzardi et al. (2021) conducted a while capturing major stereotypes dimensions, as study on this corpus to understand why participants proposed in gender and communication science obtained low scores on the identification of the studies (Ellemers, 2018; Crawford et al., 2002). particular type of misogynous behaviour against 3 women (amon"
2021.findings-emnlp.242,2020.acl-main.373,1,0.764726,"s of stereotypes. Note that when a stereotype is present, it can be expressed explicitly, implicitly (i.e., one can infer a content such as ‘(all) women are...’) or it can be a denunciation/criticism of a GS.4 Waseem and Hovy (2016) provide the first corpus of tweets annotated with racism and sexism and use a logistic regression classifier with n-grams features for hate speech detection. There are also a few notable neural network techniques: LSTM • Physical characteristics are related to physi(Jha and Mamidi, 2017) or CNN+GRU (Zhang cal strength or aspect. For example, the mesand Luo, 2018). Chiril et al. (2020b) use a BERT sage Short hair for a girl it’s a bad idea conmodel trained on word embeddings, linguistic feaveys the stereotype &quot;Girls must have long tures and generalization strategies to distinguish hair&quot;. reports/denunciations of sexism from real sexist • Behavioural characteristics are related to incontent that are directly addressed to a target. telligence, emotions, sensibility or behaviour Overall, as for stereotype detection, the work on as in the denouncing tweet Am I supposed to automatic detection of sexist messages on social recognize myself in the &quot;Just Fab&quot; ad with a media is mai"
2021.findings-emnlp.242,W19-2306,0,0.0136897,"/bit.ly/FrenchSexism The corpus being quite small, especially the stereotype class, we decided to augment the training data to counter class imbalance. There are several strategies for data augmentation among which (see (Padurariu and Breaban, 2019) for an overview): oversampling (adding instances to the minority class with replacement (bootstrapping)), weighting the data during classification, adapting the loss function of the classification model, collecting more data or generating new instances similar to the ones belonging to the minority class. To generate new data, Ray et al. (2018) and Cho et al. (2019) use paraphrase generation in the domain of Spoken Language Understanding. Chawla et al. (2002) use the Synthetic Minority Oversampling Technique (SMOTE) which finds an instance similar to the one being oversampled and creates an instance that is a randomly weighted average of the original and the neighboring instance. Wei and Zou (2019) propose to extend data with simple operations: synonym replacement, random insertion, random swap, and random deletion. Hemker and Schuller (2018) use Natural Language Generation models for auto-generating new semantically similar instances based on the traini"
2021.findings-emnlp.242,N19-1423,0,0.0769604,"Missing"
2021.findings-emnlp.242,D19-1635,0,0.0489517,"Missing"
2021.findings-emnlp.242,L18-1590,0,0.0329763,". 5 http://bit.ly/FrenchSexism The corpus being quite small, especially the stereotype class, we decided to augment the training data to counter class imbalance. There are several strategies for data augmentation among which (see (Padurariu and Breaban, 2019) for an overview): oversampling (adding instances to the minority class with replacement (bootstrapping)), weighting the data during classification, adapting the loss function of the classification model, collecting more data or generating new instances similar to the ones belonging to the minority class. To generate new data, Ray et al. (2018) and Cho et al. (2019) use paraphrase generation in the domain of Spoken Language Understanding. Chawla et al. (2002) use the Synthetic Minority Oversampling Technique (SMOTE) which finds an instance similar to the one being oversampled and creates an instance that is a randomly weighted average of the original and the neighboring instance. Wei and Zou (2019) propose to extend data with simple operations: synonym replacement, random insertion, random swap, and random deletion. Hemker and Schuller (2018) use Natural Language Generation models for auto-generating new semantically similar instanc"
2021.findings-emnlp.242,S19-2009,0,0.0448745,"Missing"
2021.findings-emnlp.242,W17-2902,0,0.0407662,"occupational status). These both definitions lead us to the definition of the following 3 categories of stereotypes. Note that when a stereotype is present, it can be expressed explicitly, implicitly (i.e., one can infer a content such as ‘(all) women are...’) or it can be a denunciation/criticism of a GS.4 Waseem and Hovy (2016) provide the first corpus of tweets annotated with racism and sexism and use a logistic regression classifier with n-grams features for hate speech detection. There are also a few notable neural network techniques: LSTM • Physical characteristics are related to physi(Jha and Mamidi, 2017) or CNN+GRU (Zhang cal strength or aspect. For example, the mesand Luo, 2018). Chiril et al. (2020b) use a BERT sage Short hair for a girl it’s a bad idea conmodel trained on word embeddings, linguistic feaveys the stereotype &quot;Girls must have long tures and generalization strategies to distinguish hair&quot;. reports/denunciations of sexism from real sexist • Behavioural characteristics are related to incontent that are directly addressed to a target. telligence, emotions, sensibility or behaviour Overall, as for stereotype detection, the work on as in the denouncing tweet Am I supposed to automati"
2021.findings-emnlp.242,D18-1303,0,0.0602845,"Missing"
2021.findings-emnlp.242,2020.lrec-1.302,0,0.0884107,"Missing"
2021.findings-emnlp.242,2020.acl-main.45,0,0.0606229,"Missing"
2021.findings-emnlp.242,P19-1441,0,0.0298015,"ate speech and stereotypes. However, in our case, since we rely on two different datasets (one for each task), we used the stereotype predictions of the best performing stereotype model (i.e., BERTConceptNet ) to automatically label the sexism dataset with stereotype information. AngryBERT (Awal et al., 2021). This model was specifically designed to address the problem of imbalanced datasets by jointly learning hate speech detection with emotion classification and target identification as secondary tasks. It has been shown to outperform many strong existing multitask models, including MT-DNN (Liu et al., 2019). In our case, the primary task of AngryBERT is sexism detection while the second being the detection of stereotypes. In addition to this initial configuration (AngryBERTbase ), four models are newly proposed, depending on both (i) the number of labels to predict in the auxiliary task, and (ii) the dataset on which the generalization with hypernyms is performed. Chiril et al. (2020b) showed that on their sexism dataset the generalization strategy performs well. In addition, we observed that a similar generalization can be employed for our task with good results. Based on these observations we"
2021.findings-emnlp.242,2020.acl-main.645,0,0.0895932,"Missing"
2021.findings-emnlp.242,D19-1174,0,0.247066,"al. (2020) consider the interaction between hate speech and stereotype detection by employing a multitask learning approach achieving the best scores in the competition. The presence of stereotypes against immigrants has also been annotated in Italian (Sanguinetti et al., 2018) and Spanish political debates (Sánchez-Junquera et al., 2021), the latter being annotated according to a finegrained taxonomy to capture the positive (threats) and negative dimensions (victims) of stereotypes. Concerning GS, there are some datasets dedicated to sexist hate speech annotated with stereotype. Among them, Parikh et al. (2019) propose a dataset which contains 13,023 accounts of sexism extracted from the Everyday Sexism Project 2 Related Work website manually annotated with 23 labels. The annotation scheme includes two categories for GS: 2.1 Stereotypes in Social Sciences role stereotyping (i.e., false generalizations about Stereotypes can be useful for making quick assercertain roles being more appropriate for women) tions, but the reader should keep in mind that by and attribute stereotyping (i.e., linking women to categorizing people only based on their gender, relisome physical, psychological, or behavioural qua"
2021.findings-emnlp.242,D18-1302,0,0.0391436,"Missing"
2021.findings-emnlp.242,D19-1410,0,0.0288869,"nstances based on the training data. However, the new instances with these methods may contain the same or similar words as the original instance but in a different order, which may result in generating instances that do not make sense to humans. In addition, these methods do not guarantee that the new generated instances belong to the same class as the original ones. To avoid this, we propose a new approach for data augmentation based on sentence similarity. We use SentenceBERT, a modification of BERT that derives semantically sentence embeddings that can be compared using cosine-similarity (Reimers and Gurevych, 2019), to extend our training dataset with the most similar sentences from two sources: (S1) New tweets in French collected with a small set of keywords usually used in stereotypes about women: moche (ugly), fesses (butt), jupe (skirt), bavarde (gossipy), dépensière (spendthrift), dévouée (devoted), infirmière (nurse), poupée (doll). These keywords are different from those used for the initial data collection; and (S2) New tweets from existing multilingual datasets annotated for stereotypes. Since there is no other available resource in French, we tried to extend our initial training corpus in two"
2021.findings-emnlp.242,P18-1216,0,0.0430395,"Missing"
2021.findings-emnlp.242,N16-2013,0,0.236135,"tribute supposedly “natural” and “normal” characteristics (psychological traits, behaviours, social roles or activities) to women and men. Deaux and Lewis (1984) define GS as having different and independent components (i.e., trait descriptors, physical characteristics, role behaviours and occupational status). These both definitions lead us to the definition of the following 3 categories of stereotypes. Note that when a stereotype is present, it can be expressed explicitly, implicitly (i.e., one can infer a content such as ‘(all) women are...’) or it can be a denunciation/criticism of a GS.4 Waseem and Hovy (2016) provide the first corpus of tweets annotated with racism and sexism and use a logistic regression classifier with n-grams features for hate speech detection. There are also a few notable neural network techniques: LSTM • Physical characteristics are related to physi(Jha and Mamidi, 2017) or CNN+GRU (Zhang cal strength or aspect. For example, the mesand Luo, 2018). Chiril et al. (2020b) use a BERT sage Short hair for a girl it’s a bad idea conmodel trained on word embeddings, linguistic feaveys the stereotype &quot;Girls must have long tures and generalization strategies to distinguish hair&quot;. repor"
2021.findings-emnlp.242,D19-1670,0,0.0286437,"(bootstrapping)), weighting the data during classification, adapting the loss function of the classification model, collecting more data or generating new instances similar to the ones belonging to the minority class. To generate new data, Ray et al. (2018) and Cho et al. (2019) use paraphrase generation in the domain of Spoken Language Understanding. Chawla et al. (2002) use the Synthetic Minority Oversampling Technique (SMOTE) which finds an instance similar to the one being oversampled and creates an instance that is a randomly weighted average of the original and the neighboring instance. Wei and Zou (2019) propose to extend data with simple operations: synonym replacement, random insertion, random swap, and random deletion. Hemker and Schuller (2018) use Natural Language Generation models for auto-generating new semantically similar instances based on the training data. However, the new instances with these methods may contain the same or similar words as the original instance but in a different order, which may result in generating instances that do not make sense to humans. In addition, these methods do not guarantee that the new generated instances belong to the same class as the original on"
2021.findings-emnlp.242,N18-2003,0,0.0508564,"Missing"
2021.findings-emnlp.242,L18-1443,0,0.0225369,"peeDe 2 shared task contains annotated tweets and newspaper headlines, with the main goal of identifying contents that convey hate or prejudice against a given target (immigrants, Muslims and Roma people) with an auxiliary task of determining the presence or absence of a stereotype towards that given target. Among participants, only Lavergne et al. (2020) consider the interaction between hate speech and stereotype detection by employing a multitask learning approach achieving the best scores in the competition. The presence of stereotypes against immigrants has also been annotated in Italian (Sanguinetti et al., 2018) and Spanish political debates (Sánchez-Junquera et al., 2021), the latter being annotated according to a finegrained taxonomy to capture the positive (threats) and negative dimensions (victims) of stereotypes. Concerning GS, there are some datasets dedicated to sexist hate speech annotated with stereotype. Among them, Parikh et al. (2019) propose a dataset which contains 13,023 accounts of sexism extracted from the Everyday Sexism Project 2 Related Work website manually annotated with 23 labels. The annotation scheme includes two categories for GS: 2.1 Stereotypes in Social Sciences role ster"
bittar-etal-2012-temporal,W08-2229,0,\N,Missing
bittar-etal-2012-temporal,2009.jeptalnrecital-long.17,1,\N,Missing
bittar-etal-2012-temporal,2009.jeptalnrecital-court.23,0,\N,Missing
C14-1114,C10-1037,0,0.0397853,"nce extraction is essential in extractive text summarization. In the unsupervised approach, sentences are scored using term weight and term proximity induced from a document collection (Goldstein et al., 2000). In the supervised approach, training data generated from reference summaries are used to learn classification or ranking models. New sentences are selected based on their confidence value on learned models (Wan et al., 2007). As information comes from documents on the same topic, it should be noticed that it is also important to reduce redundancy in MDS (Carbonell and Goldstein, 1998). Filippova (2010) builds a co-occurrence word graph from a collection of related sentences and generates a generic summary from the graph based on shortest path finding. Her algorithm is a hybrid method between extractive and abstractive approaches to MDS. 3 3.1 Resources and System Overview Corpus and Chronologies For this work, we use a corpus of newswire texts provided by the AFP French news agency. The English AFP corpus is composed of 1.3 million texts that span the 2004-2011 period (511 documents/day in average and 426 millions words). Each document is an XML file containing title, document creation time"
C14-1114,W00-0405,0,0.0707587,"b documents. Similarly, Zhao et al. (2007) use text similarity and time intensity for event clustering on social streams. Kessler et al. (2012) exploit temporal analysis to detect salient dates of an event from raw text. Following this direction, Battistelli et al. (2013) apply sequential pattern mining to select a one-sentence description for each salient date of an event. 2.2 Multidocument Summarization Sentence extraction is essential in extractive text summarization. In the unsupervised approach, sentences are scored using term weight and term proximity induced from a document collection (Goldstein et al., 2000). In the supervised approach, training data generated from reference summaries are used to learn classification or ranking models. New sentences are selected based on their confidence value on learned models (Wan et al., 2007). As information comes from documents on the same topic, it should be noticed that it is also important to reduce redundancy in MDS (Carbonell and Goldstein, 1998). Filippova (2010) builds a co-occurrence word graph from a collection of related sentences and generates a generic summary from the graph based on shortest path finding. Her algorithm is a hybrid method between"
C14-1114,P12-1077,1,0.727246,"Missing"
C14-1114,W04-1013,0,0.029008,"hown in Table 1, our method is close to ML. This result is encouraging as ML requires training data; and on the other hand, our system is not designed to directly solve the task of date selection. As expected, our system beats the unsupervised system DFIDF by a large margin. This superiority shows that the mixture of temporal information and content leads to an improvement on date selection over using only the former. 7.2 Evaluate Summary Generation In order to evaluate timelines as text summaries, we ignore dates and consider all the entries in a timeline as one summary. We use ROUGE metric (Lin, 2004) to evaluate generated timelines against reference summaries. The following baselines are implemented (Table 2): In DFIDF∗ , salient dates are taken from the outputs of the DFIDF system described in previous section. Each salient date is equivalent to a cluster containing all the events happening in that date. We then select the event the most relevant to the query, i.e. the event with the highest Lucene score, as representative of that salient date. Note that consequently, DFIDF∗ makes an assumption, which is not assumed in RaRE, that there is only one event happens in a particular date. The"
C14-1114,N10-1021,0,0.116098,"Missing"
C14-1114,D11-1040,0,0.103633,"Kadhafi and the rebels. Apr 10 2011. The former accepts their peace plan, but the latter refuse, saying Kadhafi and his sons must step down. Apr 12 2011. Britain and France call on their NATO allies to step up operations against Kadhafi’s forces. ... Figure 1: A chronology about “Libya conflict” written by journalists. (Sayyadi et al., 2009). These papers do not consider time, which is an essential dimension of event timelines. Attempts to use temporal information for EDT are significant in the literature. To name but a few, Alonso et al. (2009) apply time-based clustering on search results. Yan et al. (2011) use document timestamps to calculate temporal proximity for timeline generation from web documents. Similarly, Zhao et al. (2007) use text similarity and time intensity for event clustering on social streams. Kessler et al. (2012) exploit temporal analysis to detect salient dates of an event from raw text. Following this direction, Battistelli et al. (2013) apply sequential pattern mining to select a one-sentence description for each salient date of an event. 2.2 Multidocument Summarization Sentence extraction is essential in extractive text summarization. In the unsupervised approach, senten"
D13-1098,P08-1090,0,0.359935,"f temporal relations, but also to decide whether a temporal relation existed or not between two elements, either clinical concepts or temporal expressions. But, as in TempEval, the temporal analysis were only to be performed within a single document. Other works focus on event ordering. For example, Fujiki et al. (2003) and Talukdar et al. (2012) proposed methods for automatic acquisition of event sequences from texts. They did not use temporal information present in texts and extracted sequences of events (e.g. arrest/escape) from sentences which were already arranged in chronological order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before relation. Concerning the identification of the reaction relation, to our knowledge, there is no work on the detection of reaction between several documents. Pouliquen et al. (2007), Krestel et al. (2008) and Balahur et al. (2009) focused on the identification of reported speech or opinions in quotations in a doc"
D13-1098,P07-2044,0,0.0213023,"the same sentence or in consecutive sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is 959 Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having the best results (accuracy about 0.6) use statistical learning based on temporal features (modality, tense, aspect, etc.) (Mani et al., 2006; Chambers et al., 2007). More recently, Mirroshandel and Ghassem-Sani (2012) proposed a new method for temporal relation extraction by using a bootstrapping method on annotated data and have a better accuracy than state-of-the-art systems. Their method is based on the assumption that similar event pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temporal relations, but also"
D13-1098,E03-1061,0,0.0415483,"vent pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temporal relations, but also to decide whether a temporal relation existed or not between two elements, either clinical concepts or temporal expressions. But, as in TempEval, the temporal analysis were only to be performed within a single document. Other works focus on event ordering. For example, Fujiki et al. (2003) and Talukdar et al. (2012) proposed methods for automatic acquisition of event sequences from texts. They did not use temporal information present in texts and extracted sequences of events (e.g. arrest/escape) from sentences which were already arranged in chronological order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before relati"
D13-1098,P12-1077,1,0.841401,"Missing"
D13-1098,krestel-etal-2008-minding,0,0.0194484,"nces of events (e.g. arrest/escape) from sentences which were already arranged in chronological order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before relation. Concerning the identification of the reaction relation, to our knowledge, there is no work on the detection of reaction between several documents. Pouliquen et al. (2007), Krestel et al. (2008) and Balahur et al. (2009) focused on the identification of reported speech or opinions in quotations in a document, but not on the identification of an event which is the source of a reaction and which can possibly be in another document. As we can see, all these approaches, as well as traditional information extraction approaches, lean on information contained by a single document, and consider an event as a word or a phrase. However, Ahmed et al. (2011) proposed a framework to group temporally and tocipally related news articles into same story clusters in order to reveal the temporal evolu"
D13-1098,P06-1095,0,0.0216086,"vents and times in the same sentence or in consecutive sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is 959 Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having the best results (accuracy about 0.6) use statistical learning based on temporal features (modality, tense, aspect, etc.) (Mani et al., 2006; Chambers et al., 2007). More recently, Mirroshandel and Ghassem-Sani (2012) proposed a new method for temporal relation extraction by using a bootstrapping method on annotated data and have a better accuracy than state-of-the-art systems. Their method is based on the assumption that similar event pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temp"
D13-1098,S07-1014,0,0.0153643,"results are given in Section 5. We also propose an end-user application to this work. When a user reads an article, the system will then be able to provide her with a thread of events having occurred before or after, helping her to contextualize the information she is reading. This application is described in Section 6. 2 Related work The identification of temporal relations between events in texts has been the focus of increasing attention because of its importance in NLP applications such as information extraction, question-answering or summarization. The evaluation campaigns TempEval 2007 (Verhagen et al., 2007) and TempEval 2010 (Verhagen et al., 2010) focused on temporal relation identification, mainly on temporal relations between events and times in the same sentence or in consecutive sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is 959 Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems havi"
D13-1098,H05-1044,0,0.0254726,"Missing"
E17-1025,barbieri-saggion-2014-modelling-irony,0,0.0169511,"Related work Most state of the art approaches rely on automatically built social media data collections to detect irony using a variety of features gleaned from the utterance-internal context going from ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers automatically identified reliably, without need of manual correction, are reported (e.g. emoticons, negations). Values for other markers are currently missing since they require a manual check, for instance the case of capital letters, because of the presence in the Italian corpus where all the letters are capital. 6 For both settings, frequencie"
E17-1025,W13-1614,0,0.0120924,"Missing"
E17-1025,L16-1256,1,0.860899,"sentiment polarity and irony levels. E.g. Van Hee et al. (2016) distinguish between ironic, possibly ironic, and non-ironic tweets in English and Dutch. For ironic statements, polarity change that causes irony was annotated to specify whether the change comes from an opposition explicitly marked by a contrast between a positive situation and a negative one, an hyperbole, or an understatement. Stranisci et al. (2016) recently extend the Italian Senti-TUT schema (cf. Section 2) to mark the aspects of the topic being discussed in the tweet, as well as the sentiment expressed towards each aspect. Bosco et al. (2016) propose a second extension with the annotation of French tweets using three labels: positive irony, negative irony, and metaphorical expression. Current state of the art corpus-based studies are mainly oriented to a sentiment analysis perspective on irony, focusing almost exclusively on cap8 Exploiting the annotated corpus for automatic irony detection The French and Italian parts of the annotated corpus have been respectively exploited as datasets for the first irony detection shared tasks DEFT@TALN20177 and for the SENTIPOLC@Evalita shared task on irony detection8 in both 2014 and 2016 edit"
E17-1025,P09-2041,0,0.00722378,"arkers are correlated to irony categories, the more discriminant markers are: intensifiers, punctuation, false assertion and opinion words for French (large Cramer’s V); negations, discourse connectors and personal pronouns for English (medium Cramer’s V); and punctuation, interjections and named entities for Italian (medium Cramer’s V). 7 Related work Most state of the art approaches rely on automatically built social media data collections to detect irony using a variety of features gleaned from the utterance-internal context going from ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers autom"
E17-1025,S15-2117,1,0.0614441,"Missing"
E17-1025,W14-2608,0,0.0178236,"ges have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter’s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first tim"
E17-1025,P15-2124,0,0.0531044,"nd opinion words for French (large Cramer’s V); negations, discourse connectors and personal pronouns for English (medium Cramer’s V); and punctuation, interjections and named entities for Italian (medium Cramer’s V). 7 Related work Most state of the art approaches rely on automatically built social media data collections to detect irony using a variety of features gleaned from the utterance-internal context going from ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers automatically identified reliably, without need of manual correction, are reported (e.g. emoticons, negations). Values f"
E17-1025,P15-2106,1,0.432443,"d here http://github.com/ IronyAndTweets/. addressed languages in order to tackle their features. In English and French, users employ specific hashtags (#irony, #sarcasm, #sarcastic) to mark their intention to be ironic. These hashtags have been often used as gold labels to detect irony in a supervised learning setting. Although this approach cannot be generalized well since not all ironic tweets contain hashtags, it has however shown to be quite reliable as good inter-annotator agreements (kappa around 0.75) between annotators’ irony label and the reference irony hashtags have been reported (Karoui et al., 2015). Nevertheless, irony corpus construction through hashtag filtering is not always possible for all languages. For instance, both in Czech and Italian, Twitter users generally do not use the sarcasm (i.e. ‘#sarkasmus’, in Czech; ‘#sarcasmo’ in Italian) or irony (‘#ironie’ in Czech or ‘#ironia’ in Italian) hashtag variants to mark their intention to be ironic, thus in such cases relying on simple self-tagging for collecting ironic samples is not an option (Pt´acˇ ek et al., 2014; Bosco et al., 2013). Similar considerations hold for Chinese (Tang and Chen, 2014). For what concerns Italian, we obs"
E17-1025,W10-2914,0,0.355093,"between the categories and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter’s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts lo"
E17-1025,S15-2080,0,0.0470233,"is thus four folds: (1) analyse if these categories are also valid in social media contents, focusing on tweets which are short messages (140 characters) where the context may not be explicitly represented; (2) examine whether these categories are linguistically marked; (3) test if there is a correlation between the categories and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of"
E17-1025,W13-1605,0,0.024899,"Missing"
E17-1025,maynard-greenwood-2014-cares,0,0.0325935,"media. The goal of the paper is thus four folds: (1) analyse if these categories are also valid in social media contents, focusing on tweets which are short messages (140 characters) where the context may not be explicitly represented; (2) examine whether these categories are linguistically marked; (3) test if there is a correlation between the categories and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis"
E17-1025,P11-2102,0,0.340944,"s and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter’s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by"
E17-1025,L16-1283,0,0.152755,"Missing"
E17-1025,P15-1100,0,0.00952849,"ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers automatically identified reliably, without need of manual correction, are reported (e.g. emoticons, negations). Values for other markers are currently missing since they require a manual check, for instance the case of capital letters, because of the presence in the Italian corpus where all the letters are capital. 6 For both settings, frequencies &lt; 5 were removed. 268 Emoticon F E I Negation F E I Discourse F E I Humour #* F E I Intensifier F E I Ex Im NI 7 2 1 6 4 7 5 10 0 Opposition F E I 37 34 58 58 15 61 9 75 9 Capital F E I 6 41 29"
E17-1025,L16-1462,1,0.889268,"Missing"
E17-1025,C14-1120,0,0.0591015,"irony hashtags have been reported (Karoui et al., 2015). Nevertheless, irony corpus construction through hashtag filtering is not always possible for all languages. For instance, both in Czech and Italian, Twitter users generally do not use the sarcasm (i.e. ‘#sarkasmus’, in Czech; ‘#sarcasmo’ in Italian) or irony (‘#ironie’ in Czech or ‘#ironia’ in Italian) hashtag variants to mark their intention to be ironic, thus in such cases relying on simple self-tagging for collecting ironic samples is not an option (Pt´acˇ ek et al., 2014; Bosco et al., 2013). Similar considerations hold for Chinese (Tang and Chen, 2014). For what concerns Italian, we observe that even if occasionally Italian tweeters do use creative hashtags to explicitly mark the presence of irony, no generic shared hashtags have been used for longtime which can be considered as firmly established indicators of irony like those used for English. • A qualitative and quantitative study, focusing in particular on the interactions between irony activation types and markers, irony categories and markers, and the impact of external knowledge on irony detection. Our results demonstrate that implicit activation of irony is a major challenge for fut"
E17-1025,C96-2162,0,0.349092,"-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first time a multi-layered annotation schema for irony and its application to a corpus of French, English and Italian tweets.We detail each layer, explore their interactions, and discuss our results according to a qualitative and quantitative perspective. 1 Introduction Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Glossing over differences across approaches, irony can be defined as an incongruity between the literal meaning of an utterance and its intended meaning. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that includes sarcasm, although some researchers make a distinction between them, considering that sarcasm tends to be more aggressive (Lee and Katz, 1998; Clift, 1999). Different categories of irony have been studied in the linguistic liter"
E17-1025,C14-1022,0,\N,Missing
F14-2011,falco-etal-2012-kitten,1,0.85603,"Missing"
F14-2011,quintard-etal-2010-question,1,0.884846,"Missing"
F14-2011,tannier-2012-webannotator,0,0.040234,"Missing"
falco-etal-2012-kitten,quintard-etal-2010-question,1,\N,Missing
falco-etal-2012-kitten,baroni-etal-2008-cleaneval,0,\N,Missing
grappy-etal-2010-corpus,cramer-etal-2006-building,0,\N,Missing
grappy-etal-2010-corpus,rosset-petel-2006-ritel,0,\N,Missing
grappy-etal-2010-corpus,varasai-etal-2008-building,0,\N,Missing
moriceau-2006-language,J98-3005,0,\N,Missing
moriceau-2006-language,W06-1808,1,\N,Missing
moriceau-tannier-2014-french,P12-1077,1,\N,Missing
moriceau-tannier-2014-french,S10-1010,0,\N,Missing
moriceau-tannier-2014-french,pustejovsky-etal-2010-iso,0,\N,Missing
moriceau-tannier-2014-french,P11-2023,0,\N,Missing
moriceau-tannier-2014-french,S13-2003,0,\N,Missing
P12-1077,W11-0219,0,0.0587123,"Missing"
P12-1077,I05-1037,0,0.0592621,"Missing"
P12-1077,pustejovsky-etal-2010-iso,0,0.0363039,"nt with respect to the given topic. Each date is presented with a set of relevant sentences. We can see this work as a new, easily evaluable task of “date extraction”, which is an important component of timeline summarization. In what follows, we first review some of the related work in Section 2. Section 3 presents the resources used and gives an overview of the system. The system used for temporal analysis is described in Section 4, and the strategy used for indexing and finding salient dates, as well as the results obtained, are given in Section 51 . 2 Related Work The ISO-TimeML language (Pustejovsky et al., 2010) is a specification language for manual annotation of temporal information in texts, but, to the best of our knowledge, it has not yet actually been used in information retrieval systems. Neverthe1 This work has been partially funded by French National Research Agency (ANR) under project Chronolines (ANR-10CORD-010). We would like to thank the French News Agency (AFP) for providing us with the corpus. 730 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 730–739, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguisti"
P12-1077,2004.jeptalnrecital-long.30,0,0.0335375,"pache.org 732 4 Temporal and Linguistic Processing In this section, we describe the linguistic and temporal information extracted during the pre-processing phase and how the extraction is carried out. We rely on the powerful linguistic analyzer XIP (A¨ıtMokhtar et al., 2002), that we adapted for our purposes. 4.1 XIP The linguistic analyzer we use performs a deep syntactic analysis of running text. It takes as input XML files and analyzes the textual content enclosed in the various XML tags in different ways that are specified in an XML guide (a file providing instructions to the parser, see (Roux, 2004) for details). XIP performs complete linguistic processing ranging from tokenization to deep grammatical dependency analysis. It also performs named entity recognition (NER) of the most usual named entity categories and recognizes temporal expressions. Linguistic units manipulated by the parser are either terminal categories or chunks. Each of these units is associated with an attribute-value matrix that contains the unit’s relevant morphological, syntactic and semantic information. Linguistic constituents are linked by oriented and labelled n-ary relations denoting syntactic or semantic prope"
P12-1077,S07-1014,0,0.026926,"cognition Named Entity (NE) Recognition is one of the outputs provided by XIP. NEs are represented as unary relations in the parser output. We used the existing NE recognition module of the English grammar which tags the following NE types: location names, person names and organization names. Ambiguous NE types (ambiguity between type location or organization for country names for instance) are also considered. 4.3 Temporal Analysis A previous module for temporal analysis was developed and integrated into the English grammar (Hag`ege and Tannier, 2008), and evaluated during TempEval campaign (Verhagen et al., 2007). This module was adapted for tagging salient dates. Our goal with temporal analysis is to be able to tag and normalize3 a selected subset of temporal expressions (TEs) which we consider to be relevant for our task. This subset of expressions is described in the following sections. 4.3.1 Absolute Dates Absolute dates are dates that can be normalized without external or contextual knowledge. This is the case, for instance, of “On January 5th 2003”. In these expressions, all information needed for normalization is contained in the linguistic expression. 3 We call normalization the operation of t"
P12-1077,D11-1040,0,0.0395753,"present a system that uses measures of pertinence and novelty to construct timelines that consist of one sentence per date. (Chieu and Lee, 2004) propose a similar system that extracts events relevant to a query from a collection of documents. Important events are those reported in a large number of news articles and each event is constructed according to one single query and represented by a set of sentences. (Swan and Allen, 2000) present an approach to generating graphical timelines that involves extracting clusters of noun phrases and named entities. More recently, (Yan et 731 al., 2011b; Yan et al., 2011a) used a summarizationbased approach to automatically generate timelines, taking into account the evolutionary characteristics of news. 3 Resources and System Overview 3.1 AFP Corpus For this work, we used a corpus of newswire texts provided by the AFP French news agency. The English AFP corpus is composed of 1.3 million texts that span the 2004-2011 period (511 documents/day in average and 426 millions words). Each document is an XML file containing a title, a date of creation (DCT), set of keywords, and textual content split into paragraphs. 3.2 AFP Chronologies AFP “chronologies” (textual"
P15-2106,W14-6305,1,0.810208,"Missing"
P15-2106,P09-2041,0,0.111092,"n is quite a hot topic in the research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc644 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference o"
P15-2106,W14-2608,0,0.180302,"and testing on 430 tweets. CAll has been trained on 2,472 tweets (1432 contain negation –404 IR and 1028 NIR) and tested on 618 tweets (360 contain negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons,"
P15-2106,W10-2914,0,0.181061,"he research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc644 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro"
P15-2106,S15-2080,0,0.0191979,"e the writer believes that his audience can detect the disparity between P and P 0 on the basis of contextual knowledge or common background shared with the writer. For example, in “#Hollande is really a good diplomat #Algeria.”, the writer critics the foreign policy of the French president Hollande in Algeria, whereas in ”The #NSA wiretapped a whole country. No worries for #Belgium: it is not a whole country.“, the irony occurs because the fact in bold font is not true. Irony detection is quite a hot topic in the research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and"
P15-2106,P11-2102,0,0.658509,"estions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc644 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 644–650, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tion of named entities present in a given document and queries the web for the conjunction of those entities. (Gonzalez-Ibanez et al., 2011) exploited the common ground between speaker and hearer by looking if a tweet is a reply to another tweet. (Reyes et al., 2013) employed opposition in time (adverbs of time such as now and suddenly) and context imbalance to estimate the semantic similarity of concepts in a text to each other. (Barbieri and Saggion, 2014) captured the gap between rare and common words as well as the use of common vs. rare synonyms. Finally, (Buschmeier et al., 2014) measured the imbalance between the overall polarity of words in a review and the star-rating. Most of these pragmatic features rely on linguistic a"
P15-2106,W13-1605,0,0.195893,"Missing"
P15-2106,D13-1066,0,0.236497,"Missing"
P15-2106,C96-2162,0,0.604809,"in tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form N ot(P1 ) is ironic if and only if one can assess the absurdity of P1 . Our first results are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search f"
P15-2106,Y13-1035,0,0.644036,"Missing"
P15-2106,W10-3111,0,0.0909516,"Missing"
P15-2106,barbieri-saggion-2014-modelling-irony,0,\N,Missing
quintard-etal-2010-question,ayache-etal-2006-equer,1,\N,Missing
quintard-etal-2010-question,bernard-etal-2010-question,1,\N,Missing
S16-1190,S16-1165,0,0.0333094,"d, the HeidelTime tool identifies DOCTIME and TIMEX3 elements, and computes DocTimeRel for each EVENT identified by the CRF. Third, another CRF system computes DocTimeRel for each previously identified EVENT, based on DocTimeRel computed by HeidelTime. In the first submission, all EVENTS and TIMEX3 are identified through one general CRF model while in the second submission, we combined two CRF models (one for both EVENT and TIMEX3, and one only for TIMEX3) and we applied post-processing rules on the outputs. 1 Task description Presentation The 2016 Clinical TempEval track3 proposed six tasks (Bethard et al., 2016). We participated in the first five tasks which concern the identification of: (i) spans of time expressions (TS task), (ii) spans of event expressions (ES task), (iii) the attribute of time expressions (TA task), (iv) attributes of event expressions (EA task), and (v) the relation between each event and the document creation time (DR task). We did not participate in the narrative container relation task (CR). Introduction In this paper, we present the methods we used while participating in the 2016 Clinical TempEval task as part of the SemEval-2016 challenge. A few recent NLP challenges focus"
S16-1190,J92-4003,0,0.189152,"Missing"
S16-1190,P10-1052,0,0.0513262,"Missing"
S16-1190,S10-1071,0,0.0624188,"Missing"
S19-2087,S19-2007,0,0.011968,"r, these messages may express threats, harassment, intimidation or ”disparage a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic” (Nockleby, 2000). In this paper, we focus on automatic hate speech detection towards two different targets – immigrants and women and we propose several multi-target hate speech detection systems. The task is performed over a collection of English tweets annotated as conveying hate speech against both immigrants and women, as part of HateEval@SemEval2019 (Basile et al., 2019). The first 2 Related work Hateful speech can be expressed at different linguistic granularity levels going from lexical to discursive (Cameron, 1992). Both sexism and racism can be expressed explicitly or implicitly (see the following tweets from our data) using different pragmatic devices, including: • Negative opinion, abusive message: Stop tweeting about football. You’re a girl and you opinion doesn’t count. #WomenSuck. • Stereotype: Illegals are dumping their kids heres o they can get welfare, aid and U.S School Ripping off U.S Taxpayers #SendThemBack ! Stop Alowing illegals to Abuse the"
S19-2087,D14-1162,0,0.0822688,"ion on Poland • C3 : combines the number of words in the offensive lexicon, the number of positive and negative emojis and emoticons and performs linear dimensionality reduction by means of truncated Singular Value Decomposition and used Random Forest only for intermediate classification, whose output were then combined and passed onto a final Extreme Gradient Booster classifier • C4 : the same as C3 but applied on the extended dataset Neural model. The last model (C5 ) used a Bidirectional LSTM with an attention mechanism. For the task at hand, we used pre-trained on tweets Glove embeddings (Pennington et al., 2014). 4 Discussion Results We tried several machine learning algorithms in order to evaluate and select the best performing one. Hereby, the hate speech system baseline is a Random Forest classifier. Table 2 shows how the experiments were set up and presents the results in terms of accuracy (A), macro-averaged Fscore (F), precision (P) and recall (R). For each of 6 As only the last submitted system was taken into consideration, this also corresponds to the system used for the official ranking 4 https://www.noswearing.com/dictionary 5 http://www.cs.cmu.edu/ biglou/resources/bad-words.txt 491 10-cro"
tannier-etal-2012-evolution,arnulphy-etal-2012-event,1,\N,Missing
tannier-etal-2012-evolution,day-etal-2004-callisto,0,\N,Missing
tannier-moriceau-2010-fidji,quintard-etal-2010-question,1,\N,Missing
W04-0202,C04-1170,1,0.886212,"Missing"
W04-0202,miltsakaki-etal-2004-penn,0,0.0738972,"Missing"
W04-0202,P02-1006,0,0.0237204,"yms, sisters, subtypes. 3.4.3 Linguistic marks In this section, for space reasons, we explore only three typical CR: justifications (AJ), restrictions (AR) and warnings (AA). These MUs are characterized by markers which are general terms, domain independent for most of them. The study of these marks for French reveals that there is little marker overlap between units. Markers have been defined in a first stage from corpus analysis and then generalized to similar terms in order to have a larger basis for evaluation. We also used, to a limited extend, a bootstrapping technique to get more data (Ravinchandran and Hovy 2002), a method that starts by an unambiguous set of anchors (often arguments of a relational term) for a target sense. Searching text fragments on the Web based on these anchors then produces a number of ways of relating these anchors. Let us now characterize linguistic markers for each of these categories: Restrictions (AR) are an important unit in cooperative discourse. There is a quite large literature in linguistics about the expression of restrictions. In cooperative discourse, the expression of restrictions is realized quite straightforwardly by a small number of classes of terms: (a) restri"
W04-0202,C82-1066,0,\N,Missing
W04-0202,W03-2120,0,\N,Missing
W04-0202,baumann-etal-2004-muli,0,\N,Missing
W04-0202,W03-2301,1,\N,Missing
W05-1625,W04-2501,0,0.0609657,"Missing"
W05-1625,N03-1022,0,0.24398,"Missing"
W05-1625,J98-3005,0,0.214574,"Missing"
W06-1415,W04-2501,0,0.0334902,"ty criteria (Salton, 1989; Page et al., 1998). Some QA systems on the Web use other techniques: candidate answers are ranked according to a score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc. (Moldovan et al., 2003), (Narayanan and Harabagiu, 2004), (Radev and McKeown, 1998). Recently, advanced QA systems defined relationships (equivalence, contradiction, ...) between Web page extracts or texts containing possible answers in order to combine them and to produce a single answer (Radev and McKeown, 1998), (Harabagiu and Lacatusu, 2004), (Webber et al., 2002). Most systems provide the user with either a set of potential answers (ranked or not), or the ”best” answer according to some relevance criteria. They do not provide answers which take into account information from a set of candidate answers or answer inconsistencies. As for logical approaches used for database query, they are based on majority approach or on source reliability. But, contrary to the assumption of (Motro et al., 2004), we noted that reliability information (information about the author, date of Web pages, ...) is rather difficult to obtain, so we assume"
W06-1415,N03-1022,0,0.0291172,"n quite difficult for the user to know which answer is the correct one. Thus, an analysis of relevance and coherence of candidate answers is essential. 1.1 Related work Search engines on the Web produce a set of answers to a question in the form of hyperlinks or page extracts, ranked according to content or popularity criteria (Salton, 1989; Page et al., 1998). Some QA systems on the Web use other techniques: candidate answers are ranked according to a score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc. (Moldovan et al., 2003), (Narayanan and Harabagiu, 2004), (Radev and McKeown, 1998). Recently, advanced QA systems defined relationships (equivalence, contradiction, ...) between Web page extracts or texts containing possible answers in order to combine them and to produce a single answer (Radev and McKeown, 1998), (Harabagiu and Lacatusu, 2004), (Webber et al., 2002). Most systems provide the user with either a set of potential answers (ranked or not), or the ”best” answer according to some relevance criteria. They do not provide answers which take into account information from a set of candidate answers or answer"
W06-1415,W04-2502,0,0.0184077,"user to know which answer is the correct one. Thus, an analysis of relevance and coherence of candidate answers is essential. 1.1 Related work Search engines on the Web produce a set of answers to a question in the form of hyperlinks or page extracts, ranked according to content or popularity criteria (Salton, 1989; Page et al., 1998). Some QA systems on the Web use other techniques: candidate answers are ranked according to a score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc. (Moldovan et al., 2003), (Narayanan and Harabagiu, 2004), (Radev and McKeown, 1998). Recently, advanced QA systems defined relationships (equivalence, contradiction, ...) between Web page extracts or texts containing possible answers in order to combine them and to produce a single answer (Radev and McKeown, 1998), (Harabagiu and Lacatusu, 2004), (Webber et al., 2002). Most systems provide the user with either a set of potential answers (ranked or not), or the ”best” answer according to some relevance criteria. They do not provide answers which take into account information from a set of candidate answers or answer inconsistencies. As for logical a"
W06-1415,J98-3005,0,0.0502561,"correct one. Thus, an analysis of relevance and coherence of candidate answers is essential. 1.1 Related work Search engines on the Web produce a set of answers to a question in the form of hyperlinks or page extracts, ranked according to content or popularity criteria (Salton, 1989; Page et al., 1998). Some QA systems on the Web use other techniques: candidate answers are ranked according to a score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc. (Moldovan et al., 2003), (Narayanan and Harabagiu, 2004), (Radev and McKeown, 1998). Recently, advanced QA systems defined relationships (equivalence, contradiction, ...) between Web page extracts or texts containing possible answers in order to combine them and to produce a single answer (Radev and McKeown, 1998), (Harabagiu and Lacatusu, 2004), (Webber et al., 2002). Most systems provide the user with either a set of potential answers (ranked or not), or the ”best” answer according to some relevance criteria. They do not provide answers which take into account information from a set of candidate answers or answer inconsistencies. As for logical approaches used for database"
W06-1415,W06-1808,1,\N,Missing
W06-1808,N03-1022,0,0.0652782,"ce information (defined in (McGuinness and Pinheiro da Silva, 2004) e.g., source, date, author, etc.) is difficult to obtain. To adequately deal with data integration in question-answering, it is essential to define precisely relations existing between potential answers. In this introduction, we first present related works. Then, we define a general typology of relations between candidate answers. 42 1.1 Related works Most of existing systems on the web produce a set of answers to a question in the form of hyperlinks or page extracts, ranked according to a relevance score. For example, COGEX (Moldovan et al., 2003) uses its logic prover to extract lexical relationships between the question and its candidate answers. The answers are then ranked based on their proof scores. Other systems define relationships between web page extracts or texts containing possible answers: for example, (Radev and McKeown, 1998) and (Harabagiu and Lacatusu, 2004) define agreement (when two sources report the same information), addition (when a second source reports additional information), contradiction (when two sources report conflicting information), etc. These relations can be classified into the 4 relations defined by ("
W06-1808,W05-1625,1,0.730216,"n be in Tokyo, Paris, Hong-Kong and Los Angeles. 1.2.4 Alternative The alternative relation defines a set of inconsistent answers. In the case of questions expecting a unique answer, only one answer among candidates 43 is correct. On the contrary, all candidates can be correct answers. (1) A simple solution is to propose a disjunction of candidate answers. For example, if the question When does autumn begin? has the candidate answers Autumn begins on September 21st and Autumn begins on September 20th, an answer such as Autumn begins on either September 20th or September 21st can be proposed. (Moriceau, 2005) proposes an integration method for answers of type date. (2) If candidate answers have common characteristics, it is possible to integrate them according to these characteristics (”greatest common denominator”). For example, the question When does the ”fˆete de la musique” take place? has the following answers June 1st 1982, June 21st 1983, ..., June 21st 2005. Here, the extraction engine selects pages containing the dates of music festivals over the years. Since these candidate answers have day and month in common, an answer such as The ”fˆete de la musique” takes place every June 21st can b"
W06-1808,J98-3005,0,0.0745855,"first present related works. Then, we define a general typology of relations between candidate answers. 42 1.1 Related works Most of existing systems on the web produce a set of answers to a question in the form of hyperlinks or page extracts, ranked according to a relevance score. For example, COGEX (Moldovan et al., 2003) uses its logic prover to extract lexical relationships between the question and its candidate answers. The answers are then ranked based on their proof scores. Other systems define relationships between web page extracts or texts containing possible answers: for example, (Radev and McKeown, 1998) and (Harabagiu and Lacatusu, 2004) define agreement (when two sources report the same information), addition (when a second source reports additional information), contradiction (when two sources report conflicting information), etc. These relations can be classified into the 4 relations defined by (Webber et al., 2002), i.e. inclusion, equivalence, aggregation and alternative which we present below. Most question-answering systems provide answers which take into account neither information given by all candidate answers nor their inconsistency. This is the point we focus on in the following"
