2007.iwslt-1.17,P96-1021,0,0.0581114,"Missing"
2007.iwslt-1.17,N03-1017,0,0.00951,"Missing"
2007.iwslt-1.17,P02-1038,0,0.132631,"Missing"
2007.iwslt-1.17,koen-2004-pharaoh,0,\N,Missing
2007.iwslt-1.17,I05-1007,1,\N,Missing
2007.iwslt-1.17,P06-1077,1,\N,Missing
2007.iwslt-1.17,P06-1066,1,\N,Missing
2007.iwslt-1.17,P00-1056,0,\N,Missing
2007.iwslt-1.17,P03-1021,0,\N,Missing
2008.iwslt-evaluation.7,P06-1077,1,0.887612,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P07-1089,1,0.885881,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P08-1023,1,0.802664,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,D08-1022,1,0.818999,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,P89-1018,0,0.0164582,"ws. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head, T (e) ∈ V ∗ is a vector of tail nodes, and f (e) is a weight function from R|T (e) |to R. Figure"
2008.iwslt-evaluation.7,N04-1035,0,0.105473,"the outside probability of its root, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment: Y Y αβ(t) = α(root(t)) × P (e) × β(v) (2) (1) where plm (s) is the language model score, |d |is the number of rules in a derivation, and |s |is the number of target words produced. The derivation probability P r(d|T ) is the product of probabilities of translation rules involved in d: Y P r(d|T ) = P r(r) (5) d∈D r∈d Table 1 gives a derivation for the example forest-string pair. To learn tree-to-string rules from annotated training data, we follow GHKM [6] to first identify minimal rules and then obtain composed rules. Like in tree-based extraction, we extract rules from a packed forest F in two steps: frontier set computation (where to cut) and fragmentation (how to cut). It turns out that the exact formulation developed for frontier set in tree-based case can be applied to a forest without change. The fragmentation step, however, becomes much more complicated since we now face a choice of multiple hyperedges at each node. We develop a breadth-first search algorithm for extracting tree-to-string rules from packed forests. The basic idea is to"
2008.iwslt-evaluation.7,P02-1038,0,0.152821,"ves(t) where α(·) and β(·) are the outside and inside probabilities of nodes, root(·) returns the root of a tree fragment and leaves(·) returns the leaf nodes of a tree fragment. Now, the fractional count of a rule r is simply lines denote word alignments. Each hyperedge is associated with a probability, which we omit in Figure 1 for clarity. In a forest, a node usually has multiple incoming hyperedges. For example, the source node IP0,6 has two incoming hyperedges: c(r) = αβ(lhs(r)) αβ(¯ v) (3) where v¯ denotes the root of the forest. We extend the simple model in Eq. 1 to a log-linear model [7]: dˆ = argmax P r(d|T )λ1 × plm (s)λ2 × eλ3 |d |× eλ4 |s| e1 = h(NP-B0,1 , VP1,6 ), IP0,6 , 0.6i e2 = h(NP0,3 , VP-B3,6 ), IP0,6 , 0.4i (4) d∈D Silenus searches for the best derivation (a sequence of translation rules) dˆ that converts a source tree T in the packed forest into a target-language string s: dˆ = argmax P r(d|T ) should be penalized accordingly and should have fractional counts instead of unit count. We penalize a rule r by the posterior probability of the corresponding tree fragment t = lhs(r), which can be computed as the product of the outside probability of its root, the insid"
2008.iwslt-evaluation.7,J07-2003,0,0.096572,"ach tree has its own probability (that is, product of hyperedge probabilities). As a result, a rule extracted from non 1-best parse - 53 - where each P r(r) can be decomposed into the product of six probabilities: P r(r) = p(r|lhs(r))λ5 × p(r|rhs(r))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct tr"
2008.iwslt-evaluation.7,W05-1506,0,0.0292297,"))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and t"
2008.iwslt-evaluation.7,P06-1066,1,0.895245,"ties based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the followin"
2008.iwslt-evaluation.7,J97-3002,0,0.019739,"of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the following, we will define the model by separating different features (including the language model) from the ru"
2008.iwslt-evaluation.7,P08-2041,1,0.861149,"asures how precisely a feature f predicts a class c: IGR(f, c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the num"
2008.iwslt-evaluation.7,P07-2045,0,0.00671916,"c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the output"
2008.iwslt-evaluation.7,D07-1105,0,0.0588403,". similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the outputs of single SMT systems at sentence level, similarly to the work by Macherey and Och [14]. Global linear models are used as a framework for reranking a merged n-best list: yˆ = argmax f (x, y) · W (17) y∈GEN(x) Note that we only consider two source phrases that have the same length. To make partially matching more reliable, we further restrict that they share with the same parts-ofspeech sequence. Our hope is that similar bilingual phrases can be used to create translation templates if one source phrase cannot find translations in the phrase table. For example, suppose that we cannot find translations for a source phrase “yu zuotian dida taiguo” in a phrase table, in which we find"
2008.iwslt-evaluation.7,P03-1021,0,0.031464,"se table, in which we find a similar source phrase “yu zuowan dida bulage” with its translation “arrived in Prague last evening”. According to the alignment information, we obtain a translation template: where x is a source sentence, y is a translation, f (x, y) is a feature vector, W is a weight vector, and GEN(x) is the set of possible candidate translations. There types of features are used: (1) relative BLEU scores against 1-best translations from other candidates, (2) language model scores, and (3) length of the translation. The feature weights are tuned using minimum-error-rate training [15]. In this year’s evaluation, each single SMT system generated 200-best list translations, which were merged and served as the input to the combiner. hyu X1 dida X2 , arrived in X2 X1 i 3. Experimental Results Then, the unmatched source substrings “zuotian” and “taiguo” can be translated into “yesterday” and “Thailand”, respectively. As a result, the translation for “yu zuotian dida taiguo” is “arrived in Thailand yesterday”. Given a source sentence, the decoder firstly search for all possible translation options from the phrase table by exact matching. For source phrases which have no translat"
2008.iwslt-evaluation.7,I05-1007,1,0.800287,"ran GIZA++ and used the “growdiagfinal” heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our"
2008.iwslt-evaluation.7,P05-1022,0,0.131775,"heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 C"
2008.iwslt-evaluation.7,P08-1067,0,0.0287062,"more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 Chinese-English development set. Prior to the evaluation, we used the development sets from 200"
2009.iwslt-evaluation.8,P06-1121,0,0.0208436,"ligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four different systems are listed below: 1. Silenus, a linguistically syntax-based system that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b))"
2009.iwslt-evaluation.8,N03-1017,0,0.00351692,"that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b)) by pattern-matching. Finally, Silenus searches for the best derivation on the translation forest and outputs the target string. Beside the features we computed in rule extraction procedure, the additional features used in decoding step are listed here: • The number of rules in the derivation; Figure 1: Forest-based Rule Extraction and Translation • T"
2009.iwslt-evaluation.8,P08-1023,1,0.889278,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,D08-1022,1,0.876308,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,P06-1077,1,0.841838,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P07-1089,1,0.882882,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P89-1018,0,0.0587337,"ection 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four diff"
2009.iwslt-evaluation.8,W05-1506,0,0.153435,"d-side of r, while the root(lhs(r) denotes the root node of the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM"
2009.iwslt-evaluation.8,J07-2003,0,0.10516,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,P07-1019,0,0.0142273,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,J97-3002,0,0.00823424,"uting the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. For more details, please refer to [1] and [2]. Proceedings of IWSLT 2009, Tokyo - Japan 2.2. Bruin Bruin is a formally syntax-based SMT system, which implements the maximum entropy based reordering model on BTG [11] rules. This model considers the reorder as a problem of classification, where the Maximum Entropy model is introduced. To complete the decoding procedure, three BTG rules are used to derive the translation: [] A → (A1 , A2 ) hi (7) A → (A1 , A2 ) (8) A → (x, y) (9) The lexical rule (3) is used to translate source phrase y into target phrase x and generate a block A. The merging rules (1) and (2) are used to merge two consecutive blocks into a single larger block in the straight or inverted order. Three essential elements must be illustrated in Bruin. The first one is a stochastic BTG, whose r"
2009.iwslt-evaluation.8,P03-1021,0,0.0105949,"nguage model score of the two blocks according to their final order, λLM is its weight. For the lexical rule, applying it is assigned a probability P rl (A): P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|x|)λ6 LM ·pλLM (x) (11) where p(·) are the phrase translation probabilities in both directions, plex (·) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively. The feature weights λs are tuned to maximize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchica"
2009.iwslt-evaluation.8,P06-1066,1,0.846081,"imize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchical phrase-based model [9]. This model can be formalized as a synchronous contextfree grammar, which is automatically acquired from wordaligned parallel data without any syntactic information. X →&lt; γ, α, ∼&gt; (13) Where X is a non-terminal, γ, α are strings of terminals and non-terminals, and ∼ is one-to-one correspondence between the non-terminal in γ, α. Our work faithfully followed Chiang’s [9] work. The only exception is the condition for terminating cube pruning. Chiang’s [9] implementation quits upon considering t"
2009.iwslt-evaluation.8,I05-1007,1,0.670097,"T07 dev IWSLT08 dev selection 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set fo"
2009.iwslt-evaluation.8,P05-1022,0,0.209137,"on 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is"
2009.iwslt-evaluation.8,P08-1067,0,0.0326858,"ric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is selected automatically from all the development sentences according to the n-gram similarity, which is calculated against the current test set sentences. Our method works as follows: Fi"
2010.iwslt-evaluation.8,I05-1007,1,\N,Missing
2010.iwslt-evaluation.8,C10-1135,1,\N,Missing
2010.iwslt-evaluation.8,N04-1023,0,\N,Missing
2010.iwslt-evaluation.8,D08-1022,1,\N,Missing
2010.iwslt-evaluation.8,D09-1108,0,\N,Missing
2010.iwslt-evaluation.8,P10-1146,0,\N,Missing
2010.iwslt-evaluation.8,W06-3110,0,\N,Missing
2010.iwslt-evaluation.8,P07-1089,1,\N,Missing
2010.iwslt-evaluation.8,P08-1023,1,\N,Missing
2010.iwslt-evaluation.8,P06-1077,1,\N,Missing
2010.iwslt-evaluation.8,P06-1066,1,\N,Missing
2010.iwslt-evaluation.8,P07-2045,0,\N,Missing
2010.iwslt-evaluation.8,P05-1022,0,\N,Missing
2010.iwslt-evaluation.8,P08-1067,0,\N,Missing
2010.iwslt-evaluation.8,P05-1033,0,\N,Missing
2010.iwslt-evaluation.8,P05-1066,0,\N,Missing
2010.iwslt-evaluation.8,2009.iwslt-evaluation.8,1,\N,Missing
2010.iwslt-evaluation.8,2006.iwslt-papers.4,0,\N,Missing
2011.mtsummit-papers.33,P02-1040,0,0.0842457,"Missing"
2011.mtsummit-papers.33,P03-1021,0,0.0258257,"btain the N unique translation candidates. It should be noticed that two translation candidates are identical only if their translation string and the corresponding feature vector values are identical at the same time 3) For each of the N translation candidates we calculate the total voting score of M systems by simply adding the voting score of each system. The score of each system for each candidate is calculated using the linear combination of the system’s weight vector and the candidate’s feature vector. The weight vector of each member system is gained in the training process using MERT (Och, 2003). While the feature vector of each translation candidate is gained in step 1). The formula for calculating the total voting score of candidate c is listed below:  ሬሬሬሬሬሬሬሬሬԦୡ  ሬሬሬԦ θ  ሺ ሻ ൌ    ୀଵ 4) Finally we re-rank these N translation candidates by the total voting scores and output the one with the highest score In order to be concise and to prove the effectiveness of bagging, we do not add any extra features in our implements and experiments. 4 Experiments 4.1 Experimental Setup We performed the experiments on Chinese-English translation using an in-house implementation of t"
2011.mtsummit-papers.33,P05-1033,0,0.088627,"candidate is gained in step 1). The formula for calculating the total voting score of candidate c is listed below:  ሬሬሬሬሬሬሬሬሬԦୡ  ሬሬሬԦ θ  ሺ ሻ ൌ    ୀଵ 4) Finally we re-rank these N translation candidates by the total voting scores and output the one with the highest score In order to be concise and to prove the effectiveness of bagging, we do not add any extra features in our implements and experiments. 4 Experiments 4.1 Experimental Setup We performed the experiments on Chinese-English translation using an in-house implementation of the hierarchical phrase based SMT model (David Chiang, 2005). The model is tuned using standard MERT (Och, 2003). We use the corpus of NTCIR9 Patent translation task3 Chinese-English part which contains one million sentence pairs. We obtain one thousand sentence pairs for tuning and testing respectively 3 http://ntcir.nii.ac.jp/PatentMT/ 295 without overlap. We use GIZA++4 to perform the bi-directional word alignment between source and target side of each sentence pair. The final word alignment is generated using the grow-diag-final method. And at last all sentence pairs with alignment information is used to extract rules and phrases. A 5-gram language"
2011.mtsummit-papers.33,A00-2005,0,0.0700437,"Missing"
2011.mtsummit-papers.33,N09-3001,0,0.0166854,"t from that of training set. Bagging uses the voting result of m classifiers each with a unique distribution of the same model, so generally it is stable in statistics. Secondly bagging can avoid the over-fitting problem which a plenty of classifiers suffer. Finally bagging can be seen as an unsupervised method which doesn’t need the labeled corpus used to train the recognizer in domain recognizing methods. Bagging has been used successfully in many NLP applications such as Syntactic Parsing (Hen294 derson and Brill, 2000), Semantic Parsing (Nielsen and Pradhan, 2004), Coreference Resolution (Vemulapalli et al., 2009; Vemulapalli et al., 2010), Word Sense Disambiguation (Nielsen and Pradhan, 2004) and so on. 3 Bagging-based domain adaptation Suppose that there are M available statistics machine systems {ɊሺɅଵ ሻǡ ɊሺɅଶ ሻǡ ǥ ǡ ɊሺɅ ሻ}, the task of system combination is to build a new translation system ɋሺɊሺɅଵ ሻǡ ɊሺɅଶ ሻǡ ǥ ǡ ɊሺɅ ሻሻ which denotes the combination system. It combines the translation outputs from each of its cell system ɊሺɅ୧ ሻ which we call here a member system of it. As discussed in section 1, hardly any single system can achieve a good performance on multidomain translation problem. Besides, th"
2011.mtsummit-papers.33,W02-1405,0,0.0201586,"data. We test the results with the cluster number from 2 to 5, and the results are listed below: clusters 2 3 4 5 BLEU 31.09 31.24 31.05 30.61 Table 4 results of the unsupervised domain recognizing based method From the above results we can see a similar situation: when there are too many clusters the translation performance drops due to data sparsity; and as the cluster number decreases, the performance ascends at first and reaches the highest record of 31.24 BLEU score when the cluster number is three; and finally drops as the discrimination of class recognizer becomes weak. 5 Related Work Langlais (2002) first mention Domain Adaptation problem in SMT area by mention the problem of how to use a SMT to translate a corpus far different from the one it has been trained on. Then he makes notable achievement by integrating specific lexicon tables. Eck et al. (2004) proposed a language model adaptation technique in SMT using information retrieval techniques. Firstly, each test document is translated with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-transla"
2011.mtsummit-papers.33,eck-etal-2004-language,0,0.0275279,"tuation: when there are too many clusters the translation performance drops due to data sparsity; and as the cluster number decreases, the performance ascends at first and reaches the highest record of 31.24 BLEU score when the cluster number is three; and finally drops as the discrimination of class recognizer becomes weak. 5 Related Work Langlais (2002) first mention Domain Adaptation problem in SMT area by mention the problem of how to use a SMT to translate a corpus far different from the one it has been trained on. Then he makes notable achievement by integrating specific lexicon tables. Eck et al. (2004) proposed a language model adaptation technique in SMT using information retrieval techniques. Firstly, each test document is translated with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-translated using the adapted language model. Hasan and Ney (2005) proposed a method for building class-based language models. He applies regular expressions based method to cluster the sentences into specific classes. And then he interpolates them with the main langu"
2011.mtsummit-papers.33,2005.eamt-1.17,0,0.0955281,"antly over a hierarchical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since an SMT system trained on a corpus with heterogeneous topics may fail to achieve a good performance on domain-specific translation, while an SMT system trained on a domain-specific corpus may achieve a deteriorative performance for outof-domain translation (Haque et al., 2009). Besides more and more evaluation tasks begin to focus on mul"
2011.mtsummit-papers.33,W07-0733,0,0.0286338,"with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-translated using the adapted language model. Hasan and Ney (2005) proposed a method for building class-based language models. He applies regular expressions based method to cluster the sentences into specific classes. And then he interpolates them with the main language models to elude the data sparseness. And finally this method achieves improvements in terms of perplexity reduction and error rates. Koehn and Schroeder (2007) carried out a scheme of integrating in-domain and out-ofdomain language models using log-linear features of an SMT model, and used multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder6. Xu et al. (2007) proposed a method which uses the information retrieval approaches to classify the input sentences. This method is based on domains along with domain-dependent language models and feature weights which are gained in the training process of SMT models. This method resulted in a significant improvement in domain-dependent translation."
2011.mtsummit-papers.33,2007.mtsummit-papers.68,0,0.343632,"sually resort to statistical classifiers, but they require annotated monolingual data in different domains, which may not be available in some cases. We instead propose a simple but effective bagging-based approach without using any annotated data. Large-scale experiments show that our new method improves translation quality significantly over a hierarchical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since"
2011.mtsummit-papers.33,Y09-2027,0,0.0315654,"Missing"
2011.mtsummit-papers.33,2010.amta-papers.16,0,0.234962,"Missing"
2011.mtsummit-papers.33,D07-1054,0,0.0139736,"ical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since an SMT system trained on a corpus with heterogeneous topics may fail to achieve a good performance on domain-specific translation, while an SMT system trained on a domain-specific corpus may achieve a deteriorative performance for outof-domain translation (Haque et al., 2009). Besides more and more evaluation tasks begin to focus on multi-domain translation. For e"
2021.acl-long.379,N19-1423,0,0.19262,"vising a structural model of language capable of learning both representations and meaningful syntactic structure without any humanannotated trees has been a long-standing but challenging goal. Across a diverse range of linguistic theories, human language is assumed to possess a recursive hierarchical structure (Chomsky, 1956, 2014; de Marneffe et al., 2006) such that lowerlevel meaning is combined to infer higher-level semantics. Humans possess notions of characters, words, phrases, and sentences, which children naturally learn to segment and combine. Pretrained language models such as BERT (Devlin et al., 2019) have achieved substantial gains ∗ Equal contribution. The code is available at: https://github.com/ alipay/StructuredLM_RTDT 1 In this paper, we revisit these ideas, and propose a model applying recursive Transformers along differentiable trees (R2D2). To obtain differentiability, we adopt Gumbel-Softmax estimation (Jang et al., 2017) as an elegant solution. Our encoder parser operates in a bottom-up fashion akin to CKY parsing, yet runs in linear time with regard to the number of composition steps, thanks to a novel pruned tree induction algorithm. As a training objective, the model seeks to"
2021.acl-long.379,2020.emnlp-main.392,0,0.0509019,"Missing"
2021.acl-long.379,N19-1116,0,0.0111656,"shows the training time of our R2D2 with and without pruning. The last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we test all models with two types of input: word level (W) and word-pi"
2021.acl-long.379,N16-1024,0,0.0711279,"Missing"
2021.acl-long.379,W02-1039,0,0.256459,"Missing"
2021.acl-long.379,W18-5452,0,0.0255333,"e last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we test all models with two types of input: word level (W) and word-piece level (WP)3 . To support word-piece level evaluation, we"
2021.acl-long.379,P19-1228,0,0.0279278,"Missing"
2021.acl-long.379,N19-1114,0,0.223604,"2 with and without pruning. The last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we test all models with two types of input: word level (W) and word-piece level (WP)3 . To support"
2021.acl-long.379,D15-1137,0,0.0262106,"erarchical process. This paper proposes a recursive Transformer model based on differentiable CKY style binary trees to emulate the composition process. We extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruned tree induction algorithm to enable encoding in just a linear number of composition steps. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.1 1 Inspired by Le and Zuidema (2015), Maillard et al. (2017) proposed a fully differentiable CKY parser to model the hierarchical process explicitly. To make their parser differentiable, they primarily introduce an energy function to combine all possible derivations when constructing each cell representation. However, their model is based on Tree-LSTMs (Tai et al., 2015; Zhu et al., 2015) and requires O(n3 ) time complexity. Hence, it is hard to scale up to large training data. Introduction The idea of devising a structural model of language capable of learning both representations and meaningful syntactic structure without any"
2021.acl-long.379,P14-5010,0,0.00677007,"Missing"
2021.acl-long.379,J93-2004,0,0.0765019,"eces, we force it to not prune or select spans that conflict with word spans during prediction, and then merge word-pieces into words in the final output. However, note that this constraint is only used for word-level prediction. For training, we use the same hyperparameters as in Section 3.1.1. Our model pretrained on WikiText-2 is finetuned on the training set with the same unsupervised loss objective. For Chinese, we use a subset of Chinese Wikipedia for pretraining, specifically the first 100,000 sentences shorter than 150 characters. Data. We test our approach on the Penn Treebank (PTB) (Marcus et al., 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent work (Kim et al., 2019a), where we discard punctuation and lower-case all tokens. To explore the universality of the model across languages, we also run experiments on Chinese Penn Treebank (CTB) 8 (Xue et al., 2005), on which we also remove punctuation. Note that in all settings, the training is conducted entirely on raw unannotated text. 3.2.2 Results and Discussion Table 3 provides the unlabeled F1 scores of all systems on the WSJ and CTB test sets. It is clear that all syst"
2021.acl-long.379,de-marneffe-etal-2006-generating,0,0.0815312,"Missing"
2021.acl-long.379,N18-1202,0,0.0427421,"nd compatible with a tree structure. To further understand the strengths and weaknesses of each baseline, we analyzed the compatibility of different sentence length ranges. Interestingly, we find that our approach performs better on long sentences compared with C-PCFG at the word-piece level. This shows that a bidirectional language modeling objective can learn to induce accurate structures even on very long sentences, on which custom-tailored methods may not work as well. 4904 4 Related Work Pre-trained models. Pre-trained models have achieved significant success across numerous tasks. ELMo (Peters et al., 2018), pretrained on bidirectional language modeling based on bi-LSTMs, was the first model to show significant improvements across many downstream tasks. GPT (Radford et al., 2018) replaces bi-LSTMs with a Transformer (Vaswani et al., 2017). As the global attention mechanism may reveal contextual information, it uses a left-to-right Transformer to predict the next word given the previous context. B ERT (Devlin et al., 2019) proposes masked language modeling (MLM) to enable bidirectional modeling while avoiding contextual information leakage by directly masking part of input tokens. As masking inpu"
2021.acl-long.379,2020.acl-main.240,0,0.0174738,". 3 Experiments As our approach (R2D2) is able to learn both representations and intermediate structure, we evaluate its representation learning ability on bidirectional language modeling and evaluate the intermediate structures on unsupervised parsing. 3.1 Bidirectional Language Modeling 3.1.1 trained from scratch on WikiText-2 with different settings (number of Transformer layers and training epochs). m is the pruning threshold. Setup Baselines and Evaluation. As the objective of our model is to predict each word with its left and right context, we use the pseudo-perplexity (PPPL) metric of Salazar et al. (2020) to evaluate bidirectional language modeling. n 1X logP (si |s1:i−1 , si+1:n , θ) L(S) = n i=1   N X 1 PPPL(S) = exp − L(Sj ) N j=1 PPPL is a bidirectional version of perplexity, establishing a macroscopic assessment of the model’s ability to deal with diverse linguistic phenomena. We compared our approach with SOTA autoencoding and autoregressive language models capable of capturing bidirectional contexts, including B ERT, XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2020). For a fair apples to apples comparison, all models use the same vocabulary and are trained from scratch on a l"
2021.acl-long.379,D11-1014,0,0.204692,"Missing"
2021.acl-long.379,2020.acl-main.383,0,0.0139466,"del may perform even better with further deep layers. Table 2 shows the training time of our R2D2 with and without pruning. The last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we"
2021.acl-long.379,P15-1150,0,0.118091,"Missing"
2021.findings-acl.58,P17-1152,0,0.0729847,"ment procedure online and provides necessary intelligent assistance for the assessor. To this end, we propose an Intelligent Insurance Assessment System, called IIAS (shown in Figure 1) for medical insurance assessment. The system aims at promoting the work efficiency of the insurance assessor through dialogue-based information extraction. For these purposes, We use recent NLP technologies, such as streaming automatic speech recognition (ASR) (Moritz et al., 2020; Mani et al., 2020), large scale pre-trained language models (Devlin et al., 2019; Cui et al., 2019), sentence similarity learning (Chen et al., 2017; Peng et al., 2020), named entity recognition/linking (NER/EL) (Le and Titov, 2018; Devlin et al., 2019) and dialogue state tracking (DST) (Ouyang et al., 2020; Zhang et al., 2020b), to ensure high performance while requiring only a small amount of annotated data. Our IIAS alleviates the cognitive workload of assessors in several steps: (1) Our streaming ASR transforms speech signal into the conversation text in real-time, and our sentence similarity learning method labels corresponding topics for each conversation on-the-fly. (2) Our NER component then extracts raw entities from the real-tim"
2021.findings-acl.58,N19-1423,0,0.0227885,"nformation extraction system that shifts the previous insurance assessment procedure online and provides necessary intelligent assistance for the assessor. To this end, we propose an Intelligent Insurance Assessment System, called IIAS (shown in Figure 1) for medical insurance assessment. The system aims at promoting the work efficiency of the insurance assessor through dialogue-based information extraction. For these purposes, We use recent NLP technologies, such as streaming automatic speech recognition (ASR) (Moritz et al., 2020; Mani et al., 2020), large scale pre-trained language models (Devlin et al., 2019; Cui et al., 2019), sentence similarity learning (Chen et al., 2017; Peng et al., 2020), named entity recognition/linking (NER/EL) (Le and Titov, 2018; Devlin et al., 2019) and dialogue state tracking (DST) (Ouyang et al., 2020; Zhang et al., 2020b), to ensure high performance while requiring only a small amount of annotated data. Our IIAS alleviates the cognitive workload of assessors in several steps: (1) Our streaming ASR transforms speech signal into the conversation text in real-time, and our sentence similarity learning method labels corresponding topics for each conversation on-the-fly"
2021.findings-acl.58,2020.emnlp-demos.20,0,0.026747,"ed on the stress test, the online service’s average response 660 Discussions on Technique Limitations like GPT-2 (Radford et al., 2019)). Although now the end-to-end method cannot be applied in the realworld setting (especially in our insurance scenario that requires controllability), it has some advantages and is still part of our plan. For these limitations, we are working on a new solution to improve the current system. 6 Related Work With the advances in NLP, agent-assist systems have been used in various domains, including technical support, reservation systems, and banking applications (Fadnis et al., 2020). In this section, we briefly introduce some related technologies used in IIAS. 6.1 Sentence Similarity Learning Sentence similarity learning is a fundamental and important NLP task which may be greatly enhanced by modeling the underlying semantic representations of compared sentences. In particular, a model should not be susceptible to variations of wording or syntax used to express the same idea. Moreover, a good model should also have the capacity to learn sentence similarity regardless of the length of the text and also needs to be efficient when applied to real-world applications (Chen et"
2021.findings-acl.58,W19-5932,0,0.0116816,"iented dialogue systems to identify users’ goals and requests as a dialogue proceeds (Zhu et al., 2019). The traditional DST system assumes that each slot’s candidate values are within a limit number. However, this assumption does not apply to slots with an unlimited number of values in advance. It is more difficult for zero-shot domains like the scene of insurance assessment to predefine the slot values in advance (Ma et al., 2019; Ouyang et al., 2020). Some recent researches are on converting fixed slot values into the substring of the dialogue context (Xu and Hu, 2018; Zhang et al., 2020a; Gao et al., 2019). In this way, many researchers have proposed many neural networks to complete DST tasks in a reasonable way (Perez and Liu, 2017; Zhang et al., 2020b). Inspired by the recent progress, we also use the approach that treats the dialogue context as the source of slot values. Named Entity Recognition and Linking Named entity recognition is the NLP task of tagging entities in the text with the corresponding type and recent large-scale language model pretraining methods such as ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) further enhanced the performance of NER, yielding state-of-the-"
2021.findings-acl.58,D11-1072,0,0.0386337,"Missing"
2021.findings-acl.58,P18-1148,0,0.0443087,"this end, we propose an Intelligent Insurance Assessment System, called IIAS (shown in Figure 1) for medical insurance assessment. The system aims at promoting the work efficiency of the insurance assessor through dialogue-based information extraction. For these purposes, We use recent NLP technologies, such as streaming automatic speech recognition (ASR) (Moritz et al., 2020; Mani et al., 2020), large scale pre-trained language models (Devlin et al., 2019; Cui et al., 2019), sentence similarity learning (Chen et al., 2017; Peng et al., 2020), named entity recognition/linking (NER/EL) (Le and Titov, 2018; Devlin et al., 2019) and dialogue state tracking (DST) (Ouyang et al., 2020; Zhang et al., 2020b), to ensure high performance while requiring only a small amount of annotated data. Our IIAS alleviates the cognitive workload of assessors in several steps: (1) Our streaming ASR transforms speech signal into the conversation text in real-time, and our sentence similarity learning method labels corresponding topics for each conversation on-the-fly. (2) Our NER component then extracts raw entities from the real-time conversation text, and the EL part links the raw entities into the unified insura"
2021.findings-acl.58,P17-1001,0,0.0124042,"nce assessment’s topic distribution is diverse, and the accuracy of question identification is highly relevant to the concrete topic (i.e., resident/work address, medication experience, or disease history). Therefore, training a single model for covering all the topics is actually not the best solution in our scenario, and it is necessary to train different identification models for different topics. However, the total amount of manually labeled data is not large enough to train multiple classifiers for every topic. To alleviate this issue, we adopt the adversarial multi-task training method (Liu et al., 2017; Yang et al., 2020). Compared with the multi-task training method, the adversarial-based training method improves the question identification model’s performance for general purposes and a particular topic with only a few labeled data. Experimental results on different training meth657 Real-Time Conversation Text Agent Do you have diabetes ? Topic： Disease History User No, I don&apos;t. Possible Keywords Agent Do you have heart disease ? User Yes. Disease: Heart disease, Diabetes Hospital: The Shanghai Renmin hospital Time: 2020-01 NER Agent When were you diagnosed with heart disease ? User In Jan"
2021.findings-acl.58,2020.acl-main.5,0,0.0858334,"ed IIAS (shown in Figure 1) for medical insurance assessment. The system aims at promoting the work efficiency of the insurance assessor through dialogue-based information extraction. For these purposes, We use recent NLP technologies, such as streaming automatic speech recognition (ASR) (Moritz et al., 2020; Mani et al., 2020), large scale pre-trained language models (Devlin et al., 2019; Cui et al., 2019), sentence similarity learning (Chen et al., 2017; Peng et al., 2020), named entity recognition/linking (NER/EL) (Le and Titov, 2018; Devlin et al., 2019) and dialogue state tracking (DST) (Ouyang et al., 2020; Zhang et al., 2020b), to ensure high performance while requiring only a small amount of annotated data. Our IIAS alleviates the cognitive workload of assessors in several steps: (1) Our streaming ASR transforms speech signal into the conversation text in real-time, and our sentence similarity learning method labels corresponding topics for each conversation on-the-fly. (2) Our NER component then extracts raw entities from the real-time conversation text, and the EL part links the raw entities into the unified insurance knowledge base (KB) for getting all possible keywords. (3) Our DST method"
2021.findings-acl.58,E17-1029,0,0.0260346,"stem assumes that each slot’s candidate values are within a limit number. However, this assumption does not apply to slots with an unlimited number of values in advance. It is more difficult for zero-shot domains like the scene of insurance assessment to predefine the slot values in advance (Ma et al., 2019; Ouyang et al., 2020). Some recent researches are on converting fixed slot values into the substring of the dialogue context (Xu and Hu, 2018; Zhang et al., 2020a; Gao et al., 2019). In this way, many researchers have proposed many neural networks to complete DST tasks in a reasonable way (Perez and Liu, 2017; Zhang et al., 2020b). Inspired by the recent progress, we also use the approach that treats the dialogue context as the source of slot values. Named Entity Recognition and Linking Named entity recognition is the NLP task of tagging entities in the text with the corresponding type and recent large-scale language model pretraining methods such as ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) further enhanced the performance of NER, yielding state-of-the-art performances (Sutton et al., 2007; Li et al., 2019). Entity linking is the task of recognizing and disambiguating named entit"
2021.findings-acl.58,N18-1202,0,0.0160049,"ixed slot values into the substring of the dialogue context (Xu and Hu, 2018; Zhang et al., 2020a; Gao et al., 2019). In this way, many researchers have proposed many neural networks to complete DST tasks in a reasonable way (Perez and Liu, 2017; Zhang et al., 2020b). Inspired by the recent progress, we also use the approach that treats the dialogue context as the source of slot values. Named Entity Recognition and Linking Named entity recognition is the NLP task of tagging entities in the text with the corresponding type and recent large-scale language model pretraining methods such as ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) further enhanced the performance of NER, yielding state-of-the-art performances (Sutton et al., 2007; Li et al., 2019). Entity linking is the task of recognizing and disambiguating named entities to a knowledge base (Hoffart et al., 2011; Le and Titov, 2018). EL can be split into two classes of approaches: • End-to-End: processing a piece of text to extract the entities and then disambiguate these extracted entities to the correct entry in a given knowledge base. 661 7 Conclusions and Future Work In this paper, we presented a dialogue-based information extracti"
2021.findings-acl.58,P18-1134,0,0.0125183,"g is an important component in task-oriented dialogue systems to identify users’ goals and requests as a dialogue proceeds (Zhu et al., 2019). The traditional DST system assumes that each slot’s candidate values are within a limit number. However, this assumption does not apply to slots with an unlimited number of values in advance. It is more difficult for zero-shot domains like the scene of insurance assessment to predefine the slot values in advance (Ma et al., 2019; Ouyang et al., 2020). Some recent researches are on converting fixed slot values into the substring of the dialogue context (Xu and Hu, 2018; Zhang et al., 2020a; Gao et al., 2019). In this way, many researchers have proposed many neural networks to complete DST tasks in a reasonable way (Perez and Liu, 2017; Zhang et al., 2020b). Inspired by the recent progress, we also use the approach that treats the dialogue context as the source of slot values. Named Entity Recognition and Linking Named entity recognition is the NLP task of tagging entities in the text with the corresponding type and recent large-scale language model pretraining methods such as ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) further enhanced the pe"
2021.findings-acl.58,2020.starsem-1.17,0,0.108817,"ure 1) for medical insurance assessment. The system aims at promoting the work efficiency of the insurance assessor through dialogue-based information extraction. For these purposes, We use recent NLP technologies, such as streaming automatic speech recognition (ASR) (Moritz et al., 2020; Mani et al., 2020), large scale pre-trained language models (Devlin et al., 2019; Cui et al., 2019), sentence similarity learning (Chen et al., 2017; Peng et al., 2020), named entity recognition/linking (NER/EL) (Le and Titov, 2018; Devlin et al., 2019) and dialogue state tracking (DST) (Ouyang et al., 2020; Zhang et al., 2020b), to ensure high performance while requiring only a small amount of annotated data. Our IIAS alleviates the cognitive workload of assessors in several steps: (1) Our streaming ASR transforms speech signal into the conversation text in real-time, and our sentence similarity learning method labels corresponding topics for each conversation on-the-fly. (2) Our NER component then extracts raw entities from the real-time conversation text, and the EL part links the raw entities into the unified insurance knowledge base (KB) for getting all possible keywords. (3) Our DST method, including question"
2021.findings-acl.58,W19-5905,0,0.0222625,"ene of insurance assessment, both efficiency and accuracy are of equal importance. Therefore we use the recent proposed EnhancedRCNN model (Peng et al., 2020) that achieves good performance on Chinese paraphrase identification datasets for learning sentence similarity. 6.2 • Disambiguation-Only: directly takes gold standard named entities as input and only disambiguates them to the correct entry in a given knowledge base. 6.3 Dialogue State Tracking Dialogue state tracking is an important component in task-oriented dialogue systems to identify users’ goals and requests as a dialogue proceeds (Zhu et al., 2019). The traditional DST system assumes that each slot’s candidate values are within a limit number. However, this assumption does not apply to slots with an unlimited number of values in advance. It is more difficult for zero-shot domains like the scene of insurance assessment to predefine the slot values in advance (Ma et al., 2019; Ouyang et al., 2020). Some recent researches are on converting fixed slot values into the substring of the dialogue context (Xu and Hu, 2018; Zhang et al., 2020a; Gao et al., 2019). In this way, many researchers have proposed many neural networks to complete DST tas"
C08-1049,P08-1102,1,0.516716,"rt equals to t. For example, a tag sequence b N N m N N e N N represents a three-character word with POS tag N N . 4 The features we use to build the classifier are generated from the templates of Ng and Low (2004). For convenience of comparing with other, they didn’t adopt the ones containing external knowledge, such as punctuation information. All their templates are shown in Table 2. C denotes a character, while its subscript indicates its position relative to the current considering character(it has the subscript 0). Baseline Perceptron Classifier 4.1 Joint S&T as Classification Following Jiang et al. (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C1:n = C1 C2 .. Cn the goal of segmentation is splitting the sequence into several subsequences: C1:e1 Ce1 +1:e2 .. Cem−1 +1:em While in Joint S&T, each of these subsequences is labelled a POS tag: C1:e1 /t1 Ce1 +1:e2 /t2 .. Cem−1 +1:em /tm Where Ci (i = 1..n) denotes a character, Cl:r (l ≤ r) denotes the subsequence ranging from Cl to Cr , and ti (i = 1..m, m ≤ n) denotes the POS tag of Cei−1 +1:ei . If we label each character a positional tag indicating its relative position in"
C08-1049,W04-3236,0,0.483609,"presenting features, and usually obtains almost perfect accuracy in two tasks. Originated by Xue and Shen (2003), the typical approach of discriminative models conducts c 2008. Licensed to the Coling 2008 Organizing Com mittee for publication in Coling 2008 and for re-publishing in any form or medium. segmentation in a classification style, by assigning each character a positional tag indicating its relative position in the word. If we extend these positional tags to include POS information, segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2004) shown that, compared with performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging. Besides the usual local features such as the character-based ones (Xue and Shen, 2003; Ng and Low, 2004), many non-local features related to POSs or words can also be employed to improve performance. However, as such features are generated dynamically during the decoding procedure, incorporating these features directly into the c"
C08-1049,W03-1728,0,0.011118,"perceptron baseline and the n-best list reranking. 1 Introduction Recent work for Chinese word segmentation and POS tagging pays much attention to discriminative methods, such as Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), Conditional Random Fields (CRFs) (Lafferty et al., 2001), perceptron training algorithm (Collins, 2002), etc. Compared to generative ones such as Hidden Markov Model (HMM) (Rabiner, 1989; Fine et al., 1998), discriminative models have the advantage of flexibility in representing features, and usually obtains almost perfect accuracy in two tasks. Originated by Xue and Shen (2003), the typical approach of discriminative models conducts c 2008. Licensed to the Coling 2008 Organizing Com mittee for publication in Coling 2008 and for re-publishing in any form or medium. segmentation in a classification style, by assigning each character a positional tag indicating its relative position in the word. If we extend these positional tags to include POS information, segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2"
C08-1049,W02-1001,0,0.828167,"l features as the baseline, word lattice reranking performs reranking with non-local features that can’t be easily incorporated into the perceptron baseline. Experimental results show that, this strategy achieves improvement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking. 1 Introduction Recent work for Chinese word segmentation and POS tagging pays much attention to discriminative methods, such as Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), Conditional Random Fields (CRFs) (Lafferty et al., 2001), perceptron training algorithm (Collins, 2002), etc. Compared to generative ones such as Hidden Markov Model (HMM) (Rabiner, 1989; Fine et al., 1998), discriminative models have the advantage of flexibility in representing features, and usually obtains almost perfect accuracy in two tasks. Originated by Xue and Shen (2003), the typical approach of discriminative models conducts c 2008. Licensed to the Coling 2008 Organizing Com mittee for publication in Coling 2008 and for re-publishing in any form or medium. segmentation in a classification style, by assigning each character a positional tag indicating its relative position in the word."
C08-1049,P08-1067,0,0.477087,"ttice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging Wenbin Jiang † ‡ Haitao Mi † ‡ Qun Liu † † Key Lab. of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China ‡ Graduate University of Chinese Academy of Sciences Beijing, 100049, China {jiangwenbin,htmi,liuqun}@ict.ac.cn Abstract In this paper, we describe a new reranking strategy named word lattice reranking, for the task of joint Chinese word segmentation and part-of-speech (POS) tagging. As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking. With a perceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local features that can’t be easily incorporated into the perceptron baseline. Experimental results show that, this strategy achieves improvement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking. 1 Introduction Recent work for Chinese word segment"
C08-1049,W96-0213,0,\N,Missing
C08-1049,W05-1506,0,\N,Missing
C08-1049,P07-1019,0,\N,Missing
C08-1049,J07-2003,0,\N,Missing
C10-2033,P05-1066,0,0.0967175,"s, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, speed and search ability with a smaller 289 training data set (Section 4.2). All experiments were done on Chinese-to-English translation tasks and all results are reported with case insensitive BLEU score. Statistical significance were computed using the sign-test described in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses ar"
C10-2033,D08-1089,0,0.198885,"ransitions LShift and RShift push [i, j] into St , they check whether [i, j] is adjacent to the top block of St . If so, they change the top block into the merged block directly. In practical implementation, in order to further restrict search space, distortion limit is applied besides ITG constraints: a source phrase can be covered next only when it is ITG-legal and its distortion does not exceed distortion limit. The distortion d is calculated by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints"
C10-2033,W05-1506,0,0.0420627,"models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, sp"
C10-2033,J99-4005,0,0.793313,"resent a shift-reduce decoding algorithm that can generate ITG-legal translation from left to right in linear time. This algorithm runs in a reduce-eager style and is suited to phrase-based models. Using the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the"
C10-2033,P07-2045,0,0.011696,"ple in Figure 2. The top nine transitions correspond to Figure 3 (a), ... , Figure 3 (i), respectively. the help of ITG structure, it can be extended to syntax-based models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the perfor"
C10-2033,koen-2004-pharaoh,0,0.0590687,"der Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly"
C10-2033,W99-0604,0,0.0555584,"by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints. Besides, their algorithm checks validity via cover vector and does not formalize ITG structure. The shift-reduce decoding algorithm holds ITG structure via three stacks. As a result, it can offer ITG-legal spans directly and decode faster. Furthermore, with Sl ∅ [1, 4] ∅ [2] [2] [2] ∅ ∅ ∅ ∅ ∅ ∅ ∅ Sr [1, 6] [6] [2, 4][6] [4][6] [6] [6] [6] [6] [6] [6] [6] ∅ ∅ Figure 5: Transition sequence for the example in Figure 2. The top nine transi"
C10-2033,P03-1021,0,0.0707834,"scribed in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses are considered: one is the normal search algorithm without cubing pruning (denoted as Moses), the other is the search algorithm with cube pruning (denoted as Moses-cb). For all the decoders, the distortion limit was set to 6, the nbest size was set to 100 and the phrase table limit was 50. In the first experiment, the development set is part of NIST MT06 data set including 862 sentences, the test set is NIST MT08 data set and the training data set contains 5 million sentence pairs. We used a 5-gram language model which were trained on the Xinhua and AFP port"
C10-2033,J03-1005,0,0.0606595,"the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are"
C10-2033,P96-1021,0,0.0604209,"pted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a"
C10-2033,J97-3002,0,0.701493,"is algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-ri"
C10-2033,P06-1066,1,0.934889,"radeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings. The shift-reduce algorithm is differen"
C10-2033,P03-1019,0,0.245335,"ranslation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings"
C10-2033,C04-1030,0,\N,Missing
C10-2096,J07-2003,0,0.0215215,"s: P (r) = P (r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram l"
C10-2096,P05-1066,0,0.0822847,"Missing"
C10-2096,P09-1063,1,0.884078,"Missing"
C10-2096,P09-1064,0,0.0274916,"ove 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extraction and decoding. Ou"
C10-2096,D08-1076,0,0.0294601,"09) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattic"
C10-2096,P08-1115,0,0.33427,"alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un.” The solid lines show the 1-best result, which is wrong. Jiang et al. (2008b) stress the problem"
C10-2096,D08-1022,1,0.953642,"oxes. As a result, the search space is reduced without losing any derivations. Theoretically, the parsing over fs will speed up in O((n/nr )3 ). And our experiments in Section 6 show the efficiency of our new approach. It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change. The non-terminals inducted are also shown in Figure 3(c) in italic style. 4 Rule Extraction with Lattice & Forest We now explore the extraction algorithm from aligned source lattice-forest and target string2 , which is a tuple F, τ, a in Figure 5(b). Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps: (1) frontier set computation (2) fragmentation Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b). Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3). Each fragment is associated with a list of expansion sites (front) being 2 For simplicity and consistency, we use character-based lattice-forest for the runnin"
C10-2096,N09-1046,0,0.0192013,"evious works on SMT. To alleviate the problem of parsing error in 1-best tree-to-string translation model, Mi et al. (2008) first use forest to direct translation. Then Mi and Huang (2008) use forest in rule extraction step. Following the same direction, Liu et al. (2009) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate th"
C10-2096,P06-1121,0,0.0508816,".pop()  expand frontier 11: for each e ∈ IN (u) do 12: f ← front ∪ (tails(e)  fs) 13: open.append(frag ∪ {e}, f ) (line 7) . Otherwise we pop one expansion node u to grow and spin-off new fragments by IN (u), adding new expansion sites (lines 11- 13), until all active fragments are complete and open queue is empty. The extra minimal rules extracted on latticeforest are listed at the right bottom of Figure 5(c). Compared with the forest-only approach, we can extract smaller and more general rules. After we get all the minimal rules, we compose two or more minimal rules into composed rules (Galley et al., 2006), which will be used in our experiments. For each rule r extracted, we also assign a fractional count which is computed by using insideoutside probabilities: c(r) = α(root(r)) · P (lhs(r)) · Q v∈yield(root(r)) β(TOP) β(v) , (1) where root(r) is the root of the rule, lhs(r) is the left-hand-side of rule, rhs(r) is the righthand-side of rule, P (lhs(r)) is the product of all probabilities of hyperedges involved in lhs(r), yield(root(r)) is the leave nodes, TOP is the root node of the forest, α(v) and β(v) are outside and inside probabilities, respectively. Then we compute three conditional proba"
C10-2096,W05-1506,1,0.746831,"nals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test"
C10-2096,P07-1019,1,0.838862,"r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail, we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser"
C10-2096,P08-1067,1,0.865734,"s a result, the search space is reduced without losing any derivations. Theoretically, the parsing over fs will speed up in O((n/nr )3 ). And our experiments in Section 6 show the efficiency of our new approach. It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change. The non-terminals inducted are also shown in Figure 3(c) in italic style. 4 Rule Extraction with Lattice & Forest We now explore the extraction algorithm from aligned source lattice-forest and target string2 , which is a tuple F, τ, a in Figure 5(b). Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps: (1) frontier set computation (2) fragmentation Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b). Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3). Each fragment is associated with a list of expansion sites (front) being 2 For simplicity and consistency, we use character-based lattice-forest for the runnin"
C10-2096,P08-1102,1,0.871066,"Missing"
C10-2096,C08-1049,1,0.920892,"e of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un.” The solid lines show the 1-best result, which is wrong. Jiang et al. (2008b) stress the problems in reranking phase. Both lattices and forests have become popular in machine translation literature. However, to the best of our knowledge, previous work only focused on one module at a time. In this paper, we investigate the combination of lattice and forest (Section 2), as shown in Figure 1(b). We explore the algorithms of lattice parsing (Section 3.2), rule extraction (Section 4) and decoding (Section 5). More importantly, in the decoding step, our model can search among not only more parse-trees but also more segmentations encoded in the lattice-forests and can take"
C10-2096,P08-1023,1,0.95054,"ee separate phases. To alleviate this problem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results. For example Venugopal et al. (2008) use k-best alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9,"
C10-2096,P00-1056,0,0.0903879,"take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following Mi and Huang 843 (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string. 6.1.2 Lattice-forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingme"
C10-2096,P03-1021,0,0.0918259,"guage model and eλ3 |τ (d) |is the length penalty term on target translation. The P (d|T ) decomposes into the product of rule probabilities P (r), each of which is decomposed further into P (d|T ) =  P (r). (6) r∈d Each P (r) in Equation 6 is decomposed further into the production of five probabilities: P (r) = P (r|lhs(r))λ4 · P (r|rhs(r))λ5 · P (r|root(lhs(r))λ6 (7) · Plex (lhs(r)|rhs(r))λ7 · Plex (rhs(r)|lhs(r))λ8 , where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003). Following Mi et al. (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al. (2008), and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives. For more detail,"
C10-2096,P02-1040,0,0.0803448,"we refer to the algorithms of Mi et al. (2008). 6 Experiments 6.1 Data Preparation Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set. We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). We use the standard MERT (Och, 2003) to tune the weights. 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. For"
C10-2096,D08-1065,0,0.0217086,"o-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extrac"
C10-2096,2008.amta-papers.18,0,0.0310293,"Missing"
C10-2096,N03-1017,0,0.00419903,"ults into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following Mi and Huang 843 (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string. 6.1.2 Lattice-forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingment with the pairs of Chinese characters and target-string will obviously re"
C10-2096,I05-1007,1,0.88287,"se the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set. We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). We use the standard MERT (Och, 2003) to tune the weights. 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are pf = 5 and pf = 10 at rule extraction and decoding steps respectively. We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method “grow-diag-final-and” (Koehn et al., 2003) to get the final alignments. Following"
C10-2096,P09-1019,0,0.0266168,"3 BLEU points. Zhang et al. (2009) use forest in 844 tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system. 8 Conclusion and Future Work In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extraction and decoding. Our model postpones th"
C10-2096,P09-1020,0,0.0592981,"oblem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results. For example Venugopal et al. (2008) use k-best alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008). Another efficient method is to use compact data structures instead of k-best lists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and 837 Coling 2010: Poster Volume, pages 837–845, Beijing, August 2010 (0, 2, NR) 0 c0 :B`u 1 c1 :sh´ı (2, 3, CC) 2 c2 :yˇu (3, 5, NR) 3 c3 :Sh¯a (5, 6, VV) 4 c4 :l´ong 5 (6, 8, NN) c5 :jˇu 6 c6 :x´ıng 7 (5, 7, VV) (2, 3, P) (8, 9, NN) c7 :tˇao 8 c8 :l`un (7, 9, NN) 9 Figure 2: The lattice of the example"
C10-2096,D09-1005,0,\N,Missing
C10-2096,P08-1000,0,\N,Missing
C10-2136,P05-1067,0,0.108695,"ing of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over"
C10-2136,P03-2041,0,0.165157,"while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achie"
C10-2136,W02-1039,0,0.0891427,"cently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (Chiang, 2007). So 1 A block is a bilingual phrase without maximum length limitation. 1185 Coling 2010: Poster Volume, pages 1185–1193, Beijing, August 2010 we think it will be a promising way to integrate the target-side dependency str"
C10-2136,P06-1121,0,0.0348927,"9). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a"
C10-2136,P07-1090,0,0.0199902,"cted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be d"
C10-2136,P08-1066,0,0.446303,"influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (C"
C10-2136,C10-1123,1,0.783262,"Missing"
C10-2136,P96-1021,0,0.0727476,"w that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases. 1 Introduction Bracketing transduction grammar (BTG) (Wu, 1995) is an important subclass of synchronous context free grammar, which employs a special synchronous rewriting mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these met"
C10-2136,P06-1066,1,0.905951,"writing mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue th"
C10-2136,C08-1127,0,0.0148475,"chine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constitu"
C10-2136,D09-1127,1,0.839344,"DIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the training corpus. During the process of bilingual phrase extraction, we collect the neighboring blocks without 1 The training corpus consists of six LDC corpora: LDC2002E18, LDC2003E07, LDC2003E14, Hansards part"
C10-2136,D09-1073,0,0.0134639,"MT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Ga"
C10-2136,C04-1090,0,0.414998,"g the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a signi"
C10-2136,zhang-etal-2004-interpreting,0,0.068922,"Missing"
C10-2136,P09-1063,1,0.918036,"Missing"
C10-2136,P00-1056,0,0.0838278,"s-head) · P (wl2 |wl1 , wh -as-head) 6 where ‘-as-head’ is used to distinguish the head word from child word in the language model. In like manner, P (wR |wh -as-head) has a similar calculation method. KDIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the trainin"
C10-2136,D07-1056,0,0.0175994,"on in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two cate"
C10-2136,P03-1021,0,0.041434,"Missing"
C10-2136,P08-1064,0,0.0141864,"thods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language"
C10-2136,P02-1040,0,0.0785539,"Missing"
C10-2136,P05-1034,0,0.0710455,"syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical ph"
C10-2136,N03-1017,0,\N,Missing
C10-2136,J07-2003,0,\N,Missing
C14-1107,P81-1022,0,0.553007,"meeting S 4 : Bush held a with held Bush meeting Sharon S 8 : Bush held a meeting with meeting sh with Sharon S 9 : Bush held a meeting with Sharon rey held with S 10 : Bush held a meeting with Sharon Bush meeting Sharon rey S 11 : Bush held a meeting with Sharon held Bush meeting with Figure 1: Linear-time left-to-right dependency parsing. A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step, chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re) the top two (or more) items at the end of the stack (Aho and Ullman, 1972). In the dependency parsing scenario, the reduce action is further divided into two cases: left-reduce (rex ) and right-reduce (rey ), depending on which one of the two items becomes the head after reduction. Each parsing derivation can be represented by a sequence of parsing actions. 1134 2.1 Shift-reduce Dependency Parsing We will use the following sentence as the running example: Bush held a meeting with Sharon Given an input sentence e, where ei is the ith token, ei ...e j is the substring of e from i to j, a shift-reduce parser searches for a dependency tree with a sequence of shift-reduc"
C14-1107,2003.mtsummit-papers.6,0,0.067579,"rget syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating a Slm into a tree-to-string model will not face the problems that tree-to-tree models have. However, integration is not easy, as the following two questions arise. First, the search space grows significantly, as a partial translation has a lot of syntax structures. Second, hypotheses in the same bin may not be comparable, since their syntactic structures may no"
C14-1107,P04-1015,0,0.0412563,"window, consisting of current partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way classification based on f, and conjoin these feature instances with each action: [f ◦ (action=sh /rex /rey )] We extract all the feature templates from training data, and use the average perceptron algorithm and early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model. 3 Incremental Tree-to-string Translation with Slm The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps: parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and the linear incremental decoder then searches for the best derivation that generates a target-language string in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the following section. 3.1 Decoding with Slm Since the incremental tree-to-string model"
C14-1107,P09-1087,0,0.0145008,"ontributes more, the larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-theart tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions beyond their work. Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation mod"
C14-1107,N04-1035,0,0.0601723,"parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maxim"
C14-1107,P06-1121,0,0.0289536,"to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). Ho"
C14-1107,D09-1123,0,0.0497972,"Missing"
C14-1107,D10-1027,1,0.850995,"tring model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than"
C14-1107,P10-1110,1,0.918487,"ad. (a) (b) unigram bigram trigram feature templates s0 .w s0 .t s1 .w s1 .t q0 .w q0 .t s0 .w ◦ s1 .w s0 .t ◦ q0 .t s0 .w ◦ s1 .w ◦ s1 .t s0 .w ◦ s0 .t ◦ s1 .w s0 .t ◦ s1 .t ◦ q0 .t s1 .t ◦ s0 .t ◦ q0 .t (c) ... atomic features s0 .w s0 .t s1 .w s1 .t s0 .lc.t s0 .rc.t q0 .w q0 .t s1 s0 .w ◦ s0 .t s1 .w ◦ s1 .t q0 .w ◦ q0 .t s0 .t ◦ s1 .t s0 .w ◦ s0 .t ◦ s1 .t s0 .t ◦ s1 .w ◦ s1 .t s1 .t ◦ s0 .t ◦ s0 .lc.t s1 .t ◦ s0 .t ◦ s0 .rc.t ←− parsing stack s0 s0 .lc · · · parsing queue −→ q0 s0 .rc Table 1: (a) atomic features, used for parsing signatures. (b): parsing feature templates, adapted from Huang and Sagae (2010). x.w and x.t denotes the root word and POS tag of the partial dependency tree, x.lc and x.rc denote x’s leftmost and rightmost child respectively. (c) the feature window. 2.2 Features We view features as “abstractions” or (partial) observations of the current structure. Feature templates f are functions that draw information from the feature window, consisting of current partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree"
C14-1107,2006.amta-papers.8,1,0.780582,"he target syntax to guarantee the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate a structured language model, a left-to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approac"
C14-1107,N12-1015,1,0.848658,"rrent partial tree and first word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic 1135 IP NP VP VP PP B`ush´ı NP P VV yˇu Sh¯al´ong jˇux´ıng AS NP le hu`ıt´an Figure 2: A parse tree features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way classification based on f, and conjoin these feature instances with each action: [f ◦ (action=sh /rex /rey )] We extract all the feature templates from training data, and use the average perceptron algorithm and early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model. 3 Incremental Tree-to-string Translation with Slm The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps: parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and the linear incremental decoder then searches for the best derivation that generates a target-language string in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the following section. 3.1 Decoding with Slm Since the incremental tree-to-string model generates translatio"
C14-1107,N03-1017,0,0.05875,"he following equation: X fi · wi ) (1) e∗ = argmax exp(Slm(e) · w s + e∈E i where Slm(e) is the dependency parsing score calculated by our parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010"
C14-1107,W04-3250,0,0.0210447,"m baseline +Slm MT03 Bleu (T-B)/2 19.94 10.73 21.49 9.44 MT04 Bleu (T-B)/2 22.03 18.63 22.33 18.38 MT05 Bleu (T-B)/2 19.92 11.45 20.51 10.71 MT08 Bleu (T-B)/2 21.06 10.37 21.64 9.88 Avg. (T-B)/2 12.80 12.10 Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p &lt; 0.5). 4.3 Final Results on All Test Sets Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p &lt; 0.05, using bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times slower than the baseline. 5 Related Work The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways. First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can only tune their system on short sentences less than 20 words. Furthermore, their results are from a much bigger beam (10 times larger than their baseline), so it is not clear which factor contributes mor"
C14-1107,P06-1077,1,0.793255,"but they lack in the target syntax to guarantee the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate a structured language model, a left-to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system. 1 Introduction Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target sid"
C14-1107,P09-1063,1,0.891121,"Missing"
C14-1107,H05-1066,0,0.0363915,"h gains significant improvements over a state-of-theart tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions beyond their work. Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation model, we instead model the dependency structures with a monolingual parsi"
C14-1107,P10-1145,1,0.907751,"(Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating a Slm into a tree-to-string model wi"
C14-1107,P08-1023,1,0.824443,"rd-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our depe"
C14-1107,W04-0308,0,0.0529218,"us using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We add the structured language model as an additional feature into the baseline system. We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. We also report the Ter scores. 4.2 Complete Comparisons on MT08 To explore the soundness of our ap"
C14-1107,P03-1021,0,0.0375581,"alley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. Our dependency parser is an implementation of the “arc-standard” shift-reduce parser (Nivre, 2004), and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We add the structured language model as an additional feature into the baseline system. We evaluate translation quality using case-insensitive I"
C14-1107,N07-1051,0,0.0353772,"e) is the dependency parsing score calculated by our parser, w s is the weight of Slm(e), fi are the features in the baseline model and wi are the weights. 1139 4 Experiments 4.1 Data Preparation The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08 (newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement option “grow-diag-and” (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string translation rules. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern-matching (Mi et al., 2008). Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the same feature set shown in Huang and Mi (2010), and tune all the weights usin"
C14-1107,P05-1034,0,0.0501425,"than string-to-tree counterparts (e.g. (Galley et al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster by proposing an incremental tree-to-string model, which generates the target translation exactly in a leftto-right manner. Although, tree-to-string models have made those progresses, they can not utilize the target syntax information to guarantee the grammaticality of the output, as they only generate strings on the target side. One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al., 2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010). Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Charniak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is independent of any translation model. Thus, integrating"
C14-1107,P11-1063,0,0.0169543,"B)/2 19.92 11.45 20.51 10.71 MT08 Bleu (T-B)/2 21.06 10.37 21.64 9.88 Avg. (T-B)/2 12.80 12.10 Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p &lt; 0.5). 4.3 Final Results on All Test Sets Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p &lt; 0.05, using bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times slower than the baseline. 5 Related Work The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways. First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can only tune their system on short sentences less than 20 words. Furthermore, their results are from a much bigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, the larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-theart tree-t"
C14-1107,P08-1066,0,0.101418,"tem, and achieve a significant improvement. Different from their work, we pay more attention to the dependency parser, and we also test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they are suffering from the local parsing errors. Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005) to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental parser remains in quadratic time. Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008) and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in translation rules. Both papers integrate dependency structures into translation model, we instead model the dependency structures with a monolingual parsing model over translation strings. 6 Conclusion In this paper, we presented an efficient algorithm to integrate a structured language model (an incremental shift-reduce parser in specific) into an incremental tree-to-string system. We calcul"
C16-1127,C04-1051,0,0.134959,"ed by mean average precision (MAP) and mean reciprocal rank (MRR). We experiment on two datasets: QASent and WikiQA. The statistics of the two datasets can be found in Yang et al. (2015), where QASent (Wang et al., 2007) was created from the TREC QA track, and WikiQA (Yang et al., 2015) is constructed from real queries of Bing and Wikipedia. The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them. The metrics include the accuracy and the positive class F1 score. We experiment on the Microsoft Research Paraphrase corpus (MSRP) (Dolan et al., 2004), which includes 2753 true and 1323 false instances in the training set, and 1147 true and 578 false instances in the test set. We build a development set by randomly selecting 100 true and 100 false instances from the training set. In all experiments, we set the size of word vector dimension as d =300, and pre-train the vectors with the word2vec toolkit (Mikolov et al., 2013) on the English Gigaword (LDC2011T07). 4.2 Model Properties There are several alternative options in our model, e.g., the semantic matching functions, the decomposition operations, and the filter types. The choice of thes"
C16-1127,N13-1092,0,0.0709807,"Missing"
C16-1127,D15-1181,0,0.468852,"estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task. 1 Introduction Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences. It plays an important role for a variety of tasks in both NLP and IR communities. For example, in paraphrase identification task, sentence similarity is used to determine whether two sentences are paraphrases or not (Yin and Sch¨utze, 2015; He et al., 2015). For question answering and information retrieval tasks, sentence similarities between query-answer pairs are used for assessing the relevance and ranking all the candidate answers (Severyn and Moschitti, 2015; Wang and Ittycheriah, 2015). However, sentence similarity learning has following challenges: 1. There is a lexical gap between semantically equivalent sentences. Take the E1 and E2 in Table 1 for example, they have the similar meaning but with different lexicons. 2. Semantic similarity should be measured at different levels of granularity (word-level, phrase-level and syntax-level). E."
C16-1127,N10-1145,0,0.0802761,", researchers have been working on sentence similarity algorithms for a long time. To bridge the lexical gap (challenge 1), some word similarity metrics were proposed to match different but semantically related words. Examples include knowledge-based metrics (Resnik, 1995) and corpus-based metrics (Jiang and Conrath, 1997; Yin and Sch¨utze, 2015; He et al., 2015). To measure sentence similarity from various granularities (challenge 2), researchers have explored features extracted from n-grams, continuous phrases, discontinuous phrases, and parse trees (Yin and Sch¨utze, 2015; He et al., 2015; Heilman and Smith, 2010). The third challenge did not get much 1340 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1340–1349, Osaka, Japan, December 11-17 2016. E1 E2 E3 E4 E5 The research is [irrelevant] to sockeye. The study is [not related] to salmon. The research is relevant to salmon. The study is relevant to sockeye, hinstead of cohoi. The study is relevant to sockeye, hrather than flounderi. Table 1: Examples for sentence similarity learning, where sockeye means “red salmon”, and coho means “silver salmon”. “coho” and “sockeye” are in the sal"
C16-1127,D13-1090,0,0.0701327,"e 3 shows that our model is more effective than the other models. MSRP dataset. Table 4 summarized the results from our model and several state-of-the-art models. Yin and Sch¨utze (2015) employed a CNN model to learn sentence representations on multiple level of 1346 Models Acc F1 Yin and Sch¨utze (2015) (without pretraining) 72.5 81.4 Yin and Sch¨utze (2015) (with pretraining) 78.4 84.6 He et al. (2015) (without POS embeddings) 77.8 N/A He et al. (2015) (without Para. embeddings) 77.3 N/A He et al. (2015) (POS and Para. embeddings) 78.6 84.7 Yin et al. (2015) (with sparse features) 78.9 84.8 Ji and Eisenstein (2013) 80.4 86.0 This work 78.4 84.7 Table 4: Experimental results for paraphrase identification on MSRP corpus. granularity and modeled interaction features at each level for a pair of sentences. They obtained their best performance by pretraining the model on a language modeling task (the 3rd row of Table 4). However, their model heavily depends on the pretraining strategy. Without pretraining, they got a much worse performance (the second row of Table 4). He et al. (2015) proposed a similar model to Yin and Sch¨utze (2015). Similarly, they also used a CNN model to extract features at multiple lev"
C16-1127,O97-1002,0,0.267216,"Whereas the meaning of E4 is quite different from E3 , which emphasizes “The study is about red (a special kind of) salmon”, because both “sockeye” and “coho” are in the salmon family. How we can extract and utilize those information becomes another challenge. In order to handle the above challenges, researchers have been working on sentence similarity algorithms for a long time. To bridge the lexical gap (challenge 1), some word similarity metrics were proposed to match different but semantically related words. Examples include knowledge-based metrics (Resnik, 1995) and corpus-based metrics (Jiang and Conrath, 1997; Yin and Sch¨utze, 2015; He et al., 2015). To measure sentence similarity from various granularities (challenge 2), researchers have explored features extracted from n-grams, continuous phrases, discontinuous phrases, and parse trees (Yin and Sch¨utze, 2015; He et al., 2015; Heilman and Smith, 2010). The third challenge did not get much 1340 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1340–1349, Osaka, Japan, December 11-17 2016. E1 E2 E3 E4 E5 The research is [irrelevant] to sockeye. The study is [not related] to salmon."
C16-1127,D14-1181,0,0.0108218,"lel component and a perpendicular component. Then, the parallel component is viewed as the similar component s+ i , and perpendicular component is − taken as the dissimilar component si . Eq. (9) gives the concrete definitions. si · sˆi sˆi sˆi · sˆi + s− i = si − si s+ i = 3.3 parallel perpendicular (9) Composition Functions The aim of composition function fcomp in Eq. (3) is to extract features from both the similar component matrix and the dissimilar component matrix. We also want to acquire similarities and dissimilarities of various granularity during the composition phase. Inspired from Kim (2014), we utilize a two-channel convolutional neural networks (CNN) and design filters based on various order of n-grams, e.g., unigram, bigram and trigram. The CNN model involves two sequential operations: convolution and max-pooling. For the convolution operation, we define a list of filters {wo }. The shape of each filter is d × h, where d is the dimension of word vectors and h is the window size. Each filter is applied to two patches (a window size h of vectors) from both similar and dissimilar channels, and generates a feature. Eq. (10) expresses this process. − + + bo ) + wo ∗ S[i:i+h] co,i ="
C16-1127,D15-1166,0,0.0266684,"our model obtained a comparable performance (the last row of Table 4) without using any sparse features, extra annotated resources and specific training strategies. However, the best performance so far on this dataset is obtained by Ji and Eisenstein (2013). In their model, they just utilized several hand-crafted features in a Support Vector Machine (SVM) model. Therefore, the deep learning methods still have a long way to go for this task. 5 Related Work The semantic matching functions in subsection 3.1 are inspired from the attention-based neural machine translation (Bahdanau et al., 2014; Luong et al., 2015). However, most of the previous work using the attention mechanism in only LSTM models. Whereas our model introduces the attention mechanism into the CNN model. A similar work is the attention-based CNN model proposed by Yin et al. (2015). They first build an attention matrix for a sentence pair, and then directly take the attention matrix as a new channel of the CNN model. Differently, our model uses the attention matrix (or similarity matrix) to decompose the original sentence matrix into a similar component matrix and a dissimilar component matrix, and then feeds these two matrixes into a t"
C16-1127,W06-1603,0,0.042638,"rnational Conference on Computational Linguistics: Technical Papers, pages 1340–1349, Osaka, Japan, December 11-17 2016. E1 E2 E3 E4 E5 The research is [irrelevant] to sockeye. The study is [not related] to salmon. The research is relevant to salmon. The study is relevant to sockeye, hinstead of cohoi. The study is relevant to sockeye, hrather than flounderi. Table 1: Examples for sentence similarity learning, where sockeye means “red salmon”, and coho means “silver salmon”. “coho” and “sockeye” are in the salmon family, while “flounder” is not. attention in the past, the only related work of Qiu et al. (2006) explored the dissimilarity between sentences in a pair for paraphrase identification task, but they require human annotations in order to train a classifier, and their performance is still below the state of the art. In this paper, we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences. Given a sentence pair, the model represents each word as a low-dimensional vector (challenge 1), and calculates a semantic matching vector for each word based on all words in the other sentence (challenge 2). Then based on the semantic matc"
C16-1127,D07-1003,0,0.27962,"atical expressions with Theano (Bastien et al., 2012) and use Adam (Kingma and Ba, 2014) for optimization. 1344 4 Experiment 4.1 Experimental Setting We evaluate our model on two tasks: answer sentence selection and paraphrase identification. The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence, and the performance is measured by mean average precision (MAP) and mean reciprocal rank (MRR). We experiment on two datasets: QASent and WikiQA. The statistics of the two datasets can be found in Yang et al. (2015), where QASent (Wang et al., 2007) was created from the TREC QA track, and WikiQA (Yang et al., 2015) is constructed from real queries of Bing and Wikipedia. The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them. The metrics include the accuracy and the positive class F1 score. We experiment on the Microsoft Research Paraphrase corpus (MSRP) (Dolan et al., 2004), which includes 2753 true and 1323 false instances in the training set, and 1147 true and 578 false instances in the test set. We build a development set by randomly selecting 100 true and 100 false i"
C16-1127,D15-1237,0,0.627078,"gn Li = 0. We implement the mathematical expressions with Theano (Bastien et al., 2012) and use Adam (Kingma and Ba, 2014) for optimization. 1344 4 Experiment 4.1 Experimental Setting We evaluate our model on two tasks: answer sentence selection and paraphrase identification. The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence, and the performance is measured by mean average precision (MAP) and mean reciprocal rank (MRR). We experiment on two datasets: QASent and WikiQA. The statistics of the two datasets can be found in Yang et al. (2015), where QASent (Wang et al., 2007) was created from the TREC QA track, and WikiQA (Yang et al., 2015) is constructed from real queries of Bing and Wikipedia. The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them. The metrics include the accuracy and the positive class F1 score. We experiment on the Microsoft Research Paraphrase corpus (MSRP) (Dolan et al., 2004), which includes 2753 true and 1323 false instances in the training set, and 1147 true and 578 false instances in the test set. We build a development set by randomly"
C16-1127,N15-1091,0,0.0263024,"Missing"
C16-1127,Q16-1019,0,\N,Missing
D08-1022,P89-1018,0,0.35909,"e right. These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig. 1) (Galley et al., 2006). Our experiments use composed rules. 3 Forest-based Rule Extraction We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. These two parse tre"
D08-1022,P05-1033,0,0.674171,"-tree models (see Table 1). Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in handling non-local reorderings, and have achieved promising translation results.1 However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors. To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT. By contrast, formally syntax-based models (Chiang, 2005) do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in"
D08-1022,P05-1066,0,0.156363,"Missing"
D08-1022,P05-1067,0,0.157243,"Missing"
D08-1022,P81-1022,0,0.805013,"o a rule on the right. These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig. 1) (Galley et al., 2006). Our experiments use composed rules. 3 Forest-based Rule Extraction We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target wo"
D08-1022,N04-1035,0,0.758849,"ld r4 ⇓ a meeting with NPB Sh¯al´ong r5 ⇓ with Sharon NPB(B`ush´ı) → Bush VPB(VV(jˇux´ıng) AS(le) x1 :NPB) → held x1 NPB(Sh¯al´ong) → Sharon NPB(hu`ıt´an) → a meeting Figure 2: Example derivation of tree-to-string translation, with rules used. Each shaded region denotes a tree fragment that is pattern-matched with the rule being applied. hu`ıt´an (1) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le Bush and/with Sharon1 hold past. meeting2 Tree-based Translation We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006). 2.1 Tree-to-String System Current tree-based systems perform translation in two separate steps: parsing and decoding. The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules. For example, consider the following example translating from Chinese to English: 207 “Bush held a meeting2 with Sharon1 ” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into a parse tree (b), which will be converted into an English string in 5 s"
D08-1022,P06-1121,0,0.430054,"tion quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses. When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points. 1 Liang Huang2,1 2 Dept. of Computer & Information Science University of Pennsylvania 3330 Walnut St., Levine Hall Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu examples (partial) Ding and Palmer (2005) Liu et al. (2006); Huang et al. (2006) Galley et al. (2006) Chiang (2005) Table 1: A classification of syntax-based MT. The first three use linguistic syntax, while the last one only formal syntax. Our experiments cover the second type using a packed forest in place of the tree for rule-extraction. Introduction Automatic extraction of translation rules is a fundamental problem in statistical machine translation, especially for many syntax-based models where translation rules directly encode linguistic knowledge. Typically, these models extract rules using parse trees from both or either side(s) of the bitext. The former case, with trees on both sides,"
D08-1022,W05-1506,1,0.423446,"ther a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6 , as shown in Figure 4. Such a forest has a structure of a hypergraph (Huang and Chiang, 2005), where items like NP0, 3 are called nodes, whose indices denote the source span, and combinations like e1 : IP0, 6 → NPB0, 3 VP3, 6 we call hyperedges. We denote head (e) and tails(e) to be the consequent and antecedant items of hyperedge e, respectively. For example, head (e1 ) = IP0, 6 , tails(e1 ) = {NPB0, 3 , VP3, 6 }. We also denote BS (v) to be the set of incoming hyperedges of node v, being different ways of deriving it. For example, in Figure 4, BS (IP0, 6 ) = {e1 , e2 }. 3.2 Forest-based Rule Extraction Algorithm Like in tree-based extraction, we extract rules from a packed forest F"
D08-1022,P07-1019,1,0.350269,"traction, for example in rule composition (Galley et al., 2006) and tree binarization (Wang et al., 2007). However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding. 5 0.254 pe=8 0.252 BLEU score Our experiments will use both default 1-best decoding and forest-based decoding. As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule extraction and decoding. Th"
D08-1022,2006.amta-papers.8,1,0.839789,") Bush r2 r3 r4 r5 NPB Sh¯al´ong VV r2 ⇓ (d) with VPB held r4 ⇓ a meeting with NPB Sh¯al´ong r5 ⇓ with Sharon NPB(B`ush´ı) → Bush VPB(VV(jˇux´ıng) AS(le) x1 :NPB) → held x1 NPB(Sh¯al´ong) → Sharon NPB(hu`ıt´an) → a meeting Figure 2: Example derivation of tree-to-string translation, with rules used. Each shaded region denotes a tree fragment that is pattern-matched with the rule being applied. hu`ıt´an (1) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le Bush and/with Sharon1 hold past. meeting2 Tree-based Translation We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006). 2.1 Tree-to-String System Current tree-based systems perform translation in two separate steps: parsing and decoding. The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules. For example, consider the following example translating from Chinese to English: 207 “Bush held a meeting2 with Sharon1 ” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into a parse tree (b"
D08-1022,P08-1067,1,0.796502,"on, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors. To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT. By contrast, formally syntax-based models (Chiang, 2005) do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in recent NIST Evaluations, some of these models (Galley et al., 2006; Quirk et al., 2005; Liu et al., 2006) ranked among top 10. See http://www.nist.gov/speech/tests/mt/. 206 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, c Honolulu, October 2008. 2008 Asso"
D08-1022,P06-1077,0,0.88243,"ticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in recent NIST Evaluations, some of these models (Galley et al., 2006; Quirk et al., 2005; Liu et al., 2006) ranked among top 10. See http://www.nist.gov/speech/tests/mt/. 206 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, c Honolulu, October 2008. 2008 Association for Computational Linguistics IP NP x3 :VPB → x1 x3 with x2 (a) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an (b) ⇓ 1-best parser IP x1 :NPB CC x2 :NPB NP yˇu Figure 1: Example translation rule r1 . The Chinese conjunction yˇu “and” is translated into English prep. “with”. VPB NPB CC NPB VV AS NPB B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 ⇓ also inefficient to extract rules separate"
D08-1022,P08-1023,1,0.82596,"tion yˇu “and” is translated into English prep. “with”. VPB NPB CC NPB VV AS NPB B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 ⇓ also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k 2 similar tree-pairs in tree-to-tree models). We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. Experiments (Section 5) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system (Liu et al., 2006; Mi et al., 2008), which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses. When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date. Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-t"
D08-1022,P03-1021,0,0.0445397,"nd the last two terms are derivation and translation length penalties, respectively. The conditional probability P(d |T ) decomposes into the product of rule probabilities: Y P(r). (8) P(d |T ) = r∈d Each P(r) is in turn a product of five probabilities: P(r) = P(r |lhs(r))λ4 · P(r |rhs(r))λ5 · P(r |root(lhs(r)))λ6 · Plex (lhs(r) |rhs(r)) λ7 (9) · Plex (rhs(r) |lhs(r))λ8 where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities. These parameters λ1 . . . λ8 are tuned by minimum error rate training (Och, 2003) on the dev sets. We refer readers to Mi et al. (2008) for details of the decoding algorithm. 212 Figure 6: Comparison of extraction time and BLEU score: forest-based vs.1-best and 30-best. rules from... 1-best trees 30-best trees forest: pe =8 Pharaoh extraction 0.24 5.56 2.36 - decoding 1.74 3.31 3.40 - BLEU 0.2430 0.2488 0.2533 0.2297 Table 2: Results with different rule extraction methods. Extraction and decoding columns are running times in secs per 1000 sentences and per sentence, respectively. We use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Follo"
D08-1022,P05-1034,0,0.175825,"inguistically sophisticated counterparts. To alleviate this problem, an obvious idea is to extract rules from k-best parses instead. However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008). This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space. In addition, many subtrees are repeated across different parses, so it is 1 For example, in recent NIST Evaluations, some of these models (Galley et al., 2006; Quirk et al., 2005; Liu et al., 2006) ranked among top 10. See http://www.nist.gov/speech/tests/mt/. 206 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, c Honolulu, October 2008. 2008 Association for Computational Linguistics IP NP x3 :VPB → x1 x3 with x2 (a) B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an (b) ⇓ 1-best parser IP x1 :NPB CC x2 :NPB NP yˇu Figure 1: Example translation rule r1 . The Chinese conjunction yˇu “and” is translated into English prep. “with”. VPB NPB CC NPB VV AS NPB B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 ⇓ also inefficient to ext"
D08-1022,2008.amta-papers.18,0,0.06992,"al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding. 5 0.254 pe=8 0.252 BLEU score Our experiments will use both default 1-best decoding and forest-based decoding. As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule extraction and decoding. There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective. pe=5 0.250 0.248 k=30 pe=2 0.246 0.244 0.242 1-best forest extraction k-best extraction 0.240 0 1 2 3 4 5 6 average extracting time (secs/1000 sentences) Experiments 5.1 System Our experiments are on Chinese-to-English translation based on a tree-to-string system similar to (Huang et al., 2006; Liu et al., 2006). Given a 1best tree T , the decoder searches for the best derivation d∗ among the set of all possible derivations D: d∗ = arg max λ0 log P(d |T ) + λ1 log Plm (τ"
D08-1022,D07-1078,0,0.352611,"ially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3 Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6 , as shown in Figure 4. Such a forest has a structure of a hypergraph (Huang and Chiang, 2005), where items like NP0, 3 are called nodes, whose indices denote the source span, and combinations like e1 : IP0, 6 → NPB0, 3 VP3, 6 we call hyperedges. We denote head (e) a"
D08-1022,I05-1007,0,0.0292535,"λ1 . . . λ8 are tuned by minimum error rate training (Och, 2003) on the dev sets. We refer readers to Mi et al. (2008) for details of the decoding algorithm. 212 Figure 6: Comparison of extraction time and BLEU score: forest-based vs.1-best and 30-best. rules from... 1-best trees 30-best trees forest: pe =8 Pharaoh extraction 0.24 5.56 2.36 - decoding 1.74 3.31 3.40 - BLEU 0.2430 0.2488 0.2533 0.2297 Table 2: Results with different rule extraction methods. Extraction and decoding columns are running times in secs per 1000 sentences and per sentence, respectively. We use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008). We will first report results trained on a small-scaled dataset with detailed analysis, and then scale to a larger one, where we also combine the technique of forest-based decoding (Mi et al., 2008). 5.2 Results and Analysis on Small Data To test the effect of forest-based rule extraction, we parse the training set into parse forests and use"
D08-1022,P05-1022,0,\N,Missing
D09-1115,P08-1115,0,0.0751179,"eels like apples He prefer lations. Note that the phrase “is fond of” is attached to an edge. Now, it is unlikely to obtain a translation like “He is like of apples”. A lattice G = hV, Ei is a directed acyclic graph, formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges. The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation. Each edge in a lattice is attached with a sequence of words as well as the associated probability. As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination. He feels like apples apples He prefer apples He feels like apples He feels like apples He is fond of apples He is fond of apples (a) unidirectional alignments (b) bidirectional alignments He ε feels prefer like of ε apples is fond (c) confusion network he ε feels prefer like apples is fond of (d) lattice 2.2 Figure 1: Comparison of a confusion network and a lattice. 2 Background 2.1 Confusion Network and Lattice We use an example shown in Figure 1 to illustrate our idea. Suppose that there are"
D09-1115,A94-1016,0,0.202214,"in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way t"
D09-1115,D08-1011,0,0.510095,"s significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be capable of” and “"
D09-1115,W07-0711,0,0.0521771,"s were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W05-1506,0,0.0415363,"hich store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. α and β are the corresponding weights of the numbers, respectively. Nword (e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps (arc). ps (arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights we"
D09-1115,N03-1017,0,0.027572,"he hypothesis with the minimum cost of edits against all hypotheses is selected. The backbone is significant for it influences not only the word order, but also the following alignments. The backbone is selected as follows: EB = argmin E ′ ∈E X T ER(E ′ , E) (8) E∈E (3) Get the alignments of the backbone and hypothesis pairs. First, each pair is aligned in both directions using the IHMM-based alignment method. In the IHMM alignment model, bilingual dictionaries in both directions are indispensable. Then, we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction (Koehn et al., 2003) to monolingual alignments. The bidirectional alignments are combined to one resorting to the grow-diag-final algorithm, allowing n-to-n mappings. (4)Normalize the alignment pairs. The word order of the backbone determines the word order of consensus outputs, so the word order of hypotheses must be consistent with that of the backbone. All words of a hypotheses are reordered according to the alignment to the backbone. For a word aligned to null, an actual null word may be inserted to the proper position. The alignment units are extracted first and then the hypothesis words in each unit are shi"
D09-1115,P07-2045,0,0.00971279,"1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s meth"
D09-1115,E06-1005,0,0.230719,"nts and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more co"
D09-1115,P07-1040,0,0.376587,"idate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected"
D09-1115,N07-1029,0,0.250842,"ere collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W08-0329,0,0.156101,"our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be"
D09-1115,C96-2141,0,0.491588,"ed as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set which included 289K sentenc"
D10-1027,J07-2003,0,0.385174,"al (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-"
D10-1027,P81-1022,0,0.759931,"bles replaced by matched nodes: [NP@1  VP@2 ] [ held NP@2.2.3 with NP@2.1.2 ] Note that this is a reordering rule, and the stack always follows the English word order because we generate hypothesis incrementally left-to-right. Figure 4 works out the full example. We formalize this algorithm in Figure 5. Each item hs, ρi consists of a stack s and a hypothesis ρ. Similar to phrase-based dynamic programming, only the last g−1 words of ρ are part of the signature for decoding with g-gram LM. Each stack is a list of dotted rules, i.e., rules with dot positions indicting progress, in the style of Earley (1970). We call the last (rightmost) rule on the stack the top rule, which is the rule being processed currently. The symbol after the dot in the top rule is called the next symbol, since it is the symbol to expand or process next. Depending on the next symbol a, we can perform one of the three actions: • if a is a node η, we perform a Predict action which expands η using a rule r that can patternmatch the subtree rooted at η; we push r is to the stack, with the dot at the beginning; • if a is an English word, we perform a Scan action which immediately adds it to the current hypothesis, advancing th"
D10-1027,D08-1089,0,0.484862,"er only needs to do this at one end since the translation is always growing left-to-right. As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful. Can we combine the merits of both approaches? While other authors have explored the possibilities 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics of enhancing phrase-based decoding with syntaxaware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibac"
D10-1027,N04-1035,0,0.040328,"of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, wh"
D10-1027,P07-1019,1,0.829324,"ractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right. As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful. Can we combine the merits of both approaches? While other authors have explored the possibilities 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics of enhancing phrase-based decoding"
D10-1027,2006.amta-papers.8,1,0.733611,"e) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following contributions: • we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see Table 1); • large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-based system Moses (in C"
D10-1027,W07-0405,1,0.844387,"ebased case in Section 2, here we have to remember both sides the leftmost and the rightmost boundary words: each node is now split into +LM items like (η a ⋆ b ) where η is a tree node, and a and b are left and right English boundary words. For example, a bigram +LM item for node VP@2 might be (VP@2 held ⋆ Sharon ). This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007). In theory +LM decoding is O(nc|V |4(g−1) ), where V denotes English vocabulary (Huang, 2007). In practice we have to resort to beam search again: at each node we would only allow top-b +LM items. With beam search, tree-to-string decoding with an integrated language model runs in time O(ncb2 ), where b is the size of the beam at each node, and c is (maximum) number of translation rules matched at each node (Huang, 2007). See Table 2 for a summary. 276 3.2 Incremental Decoding Can we borrow the idea of phrase-based decoding, so that we also grow the hypothesis strictly leftto-right, and only need to maintain the rightmost boundary words? The key intuition is to adapt the coverage-vecto"
D10-1027,J99-4005,0,0.195544,"able 1: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based. In practice means “approximate search with beams.” Introduction Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models. From a theoretical point of view, phrasebased models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderin"
D10-1027,P07-2045,0,0.0437933,"sort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is"
D10-1027,koen-2004-pharaoh,0,0.707006,"d with phrase-based. In practice means “approximate search with beams.” Introduction Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models. From a theoretical point of view, phrasebased models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those"
D10-1027,P06-1077,0,0.630383,"ferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following contributions: • we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see Table 1); • large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-bas"
D10-1027,P08-1023,1,0.751406,"ning corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is caseinsensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. We first verify the assumptions we made in Section 3.3 in order to prove the"
D10-1027,P03-1021,0,0.0200701,"ide of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is caseinsensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. We first verify the assumptions we made in Section 3.3 in order to prove the theorem that tree depth (as a random variable) is normally-distributed with O(log n) mean and variance. Qualitatively, we verified that for most n, tree depth d(n) does look like a normal distribution. Quantitatively, Figure 6 shows that average tree height correlates extremely well with 3.5 log n, while tree height variance is bounded by 5.5 log n. 5.2 Comparison with Cube pruning We implemented our incremental decoding algorithm in"
D10-1027,N07-1051,0,0.0292411,"CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluat"
D10-1027,N07-1063,0,0.0122181,"n run-time with beam search. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube prunin"
D10-1027,P06-1098,0,0.72306,". Can we combine the merits of both approaches? While other authors have explored the possibilities 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics of enhancing phrase-based decoding with syntaxaware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is"
D10-1027,J97-3002,0,0.201309,"e in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is alway"
D10-1027,P08-1025,0,0.0170771,"rch. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we 280 also compare our incremental decoder with the standard cube pruning approach on the same t"
D10-1027,N10-1128,0,\N,Missing
D11-1020,P05-1033,0,0.925102,"nd has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristi"
D11-1020,J07-2003,0,0.700046,"the head-dependents relation rooted at n, and checks the rule set for matched translation rules. If there is no matched rule, we construct a pseudo translation rule according to the word order of the head-dependents relation. For example, suppose that we can not find any translation rule about to “(2010年) (FIFA) 世 界杯”, we will construct a pseudo translation rule “(x1 :2010年) (x2 :FIFA) x3 :世界杯 → x1 x2 x3 ”. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. We make use of cube pruning (Chiang, 2007; Huang and Chiang, 2007) to find the k-best items with integrated language model for each node. To balance performance and speed, we prune the search space in several ways. First, beam threshold β , items with a score worse than β times of the best score in the same cell will be discarded; second, beam size b, items with a score worse than the bth best item in the same cell will be discarded. The item consist of the necessary information used in decoding. Each cell contains all the items standing for the subtree rooted at it. For our experiments, we set β = 10−3 and b = 300. Additionally, we"
D11-1020,P05-1067,0,0.56847,"the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the target side as string."
D11-1020,W02-1039,0,0.14595,"g distance reordering, and outperforms the stateof-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. 1 Introduction Dependency structure represents the grammatical relations that hold between the words in a sentence. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cub"
D11-1020,P06-1121,0,0.45011,"., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned words. In our experiments of this paper, we set the tatTable-limit=20, tatTable-threshold=10−1 , stack-limit=100, stackthreshold=10−1 ,hight-limit=3, and length-limit=7. 7.3 Results We display the results of our experiments in Table 1. Our dependency-to-string model dep2str significantly outperforms its constituency structure-based counterpart (cons2str) with +1.27 and +1.68 BLEU on MT04 and MT05 respectively. Moreover, without resort to phrases or parse forest, dep2str surpasses the hierarchical phrase-based model (hierore) over +0.53 and +0.4 BLEU on MT04 and"
D11-1020,P07-1019,0,0.0860775,"dents relation rooted at n, and checks the rule set for matched translation rules. If there is no matched rule, we construct a pseudo translation rule according to the word order of the head-dependents relation. For example, suppose that we can not find any translation rule about to “(2010年) (FIFA) 世 界杯”, we will construct a pseudo translation rule “(x1 :2010年) (x2 :FIFA) x3 :世界杯 → x1 x2 x3 ”. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. We make use of cube pruning (Chiang, 2007; Huang and Chiang, 2007) to find the k-best items with integrated language model for each node. To balance performance and speed, we prune the search space in several ways. First, beam threshold β , items with a score worse than β times of the best score in the same cell will be discarded; second, beam size b, items with a score worse than the bth best item in the same cell will be discarded. The item consist of the necessary information used in decoding. Each cell contains all the items standing for the subtree rooted at it. For our experiments, we set β = 10−3 and b = 300. Additionally, we also prune rules that hav"
D11-1020,W06-3601,0,0.136216,"ttractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005;"
D11-1020,N03-1017,0,0.0607523,"Missing"
D11-1020,C04-1090,0,0.850261,"endency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependent"
D11-1020,P06-1077,1,0.876701,"of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned words. In our experiments of this paper, we set the tatTable-limit=20, tatTable-threshold=10−1 , stack-limit=100, stackthreshold=10−1 ,hight-limit=3, and length-limit=7. 7.3 Results We display the results of our experiments in Table 1. Our dependency-to-string model dep2str significantly outperforms its constituency structure-based counterpart (cons2str) with +1.27 and +1.68 BLEU on MT04 and MT05 respectively. Moreover, without resort to phrases or parse forest, dep2str sur"
D11-1020,P02-1038,0,0.26238,"lized and unlexicalized head-dependents rules from f 10 append the induced rules to R 11 end 12 end spans simultaneously. In this process, we might obtain m(m ≥ 1) head-dependents rules from a headdependent fragment in handling unaligned words. Each of these rules is assigned with a fractional count 1/m. 4.4 Algorithm for Rule Acquisition The rule acquisition is a three-step process, which is summarized in Algorithm 1. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 5 The model Following (Och and Ney, 2002), we adopt a general log-linear model. Let d be a derivation that convert a source dependency structure T into a target string e. The probability of d is defined as: P (d) ∝ ∏ ϕi (d)λi (1) i where ϕi are features defined on derivations and λi are feature weights. In our experiments of this paper, we used seven features as follows: - translation probabilities P (t|s) and P (s|t); - lexical translation probabilities Plex (t|s) and Plex (s|t); - rule penalty exp(−1); 222 - language model Plm (e); - word penalty exp(|e|). 6 Decoding Our decoder is based on bottom up chart parsing. It finds the bes"
D11-1020,J03-1002,0,0.00889922,"of the hierarchical phrase-based model and the tree-tostring models on Chinese-English translation. 7.1 Data preparation Our training corpus consists of 1.5M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. We parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures, whose nodes are annotated by POS tags and edges by typed dependencies. In our implementation of this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maxi"
D11-1020,J04-4002,0,0.852881,"licity, we use {1-5} to denotes the contiguous span {1, 2, 3, 4, 5}. ′ Definition 4. Given a subtree T rooted at n, the dependency span dsp(n) of n is defined as: ∪ dsp(n) = cloz( hsp(n′ )). ′ n′ ∈T hsp(n′ ) is consistent ′ If the head spans of all the nodes of T is not consistent, dsp(n) = ∅. For example, since hsp(在) is not consistent, dsp(在)=dsp(南非)={9, 10}, which corresponds to the target words “South” and “Africa”. The tree annotation can be accomplished by a single postorder transversal of T . The extraction of head rules from each node can be readily achieved with the same criteria as (Och and Ney, 2004). In the following, we focus on head-dependents rules acquisition. Ѯ㹼/VV {6}{2-10} (a) 4.2 Head-Dependents Fragments Identification ᡀ/AD /P ц⭼ᶟ/NR /P {3,4}{2-4} {5,8}{9,10} {5,8}{9,10} {7}{7}} We then identify the head-dependents fragments that are suitable for rule induction from the annotated dependency structure. To facilitate the identification process, we first define two sets of dependency structure related to head spans and dependency spans. Definition 5. A acceptable head set ahs(T) of a dependency structure T is a set of nodes, each of which has a consistent head span. For example,"
D11-1020,P03-1021,0,0.0927096,"e word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned wor"
D11-1020,P02-1040,0,0.104432,"this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1 System cons2str hiero-re dep2str ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 223 We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold β = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley e"
D11-1020,P05-1034,0,0.867324,"el, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the"
D11-1020,P08-1066,0,0.434244,"ithout resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. 1 Introduction Dependency structure represents the grammatical relations that hold between the words in a sentence. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency"
D11-1020,W07-0706,1,0.871506,"hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the target side as string. The head-dependents r"
D13-1052,N09-1025,0,0.0609013,"Missing"
D13-1052,P05-1033,0,0.381694,"Missing"
D13-1052,N10-1141,0,0.0524598,"Missing"
D13-1052,N04-1035,0,0.133071,"Missing"
D13-1052,P06-1121,0,0.18684,"Missing"
D13-1052,D08-1011,0,0.0521456,"Missing"
D13-1052,2006.amta-papers.8,0,0.167307,"Missing"
D13-1052,N03-1017,0,0.00858727,"Missing"
D13-1052,P06-1077,0,0.130571,"Missing"
D13-1052,P09-1065,1,0.92946,"Missing"
D13-1052,D08-1022,1,0.907993,"Missing"
D13-1052,P08-1023,1,0.912261,"Missing"
D13-1052,N07-1051,0,0.121787,"Missing"
D13-1052,P07-1040,0,0.062928,"Missing"
D13-1052,P11-1125,0,0.0275882,"Missing"
D13-1052,D09-1108,0,0.354295,"Missing"
D13-1112,J92-4003,0,0.204004,"Missing"
D13-1112,P05-1022,0,0.130309,":7] = </s>, |c(r2 ) |= 3 ... (combos of the above atomic features) ... e(r0 ◦ r1 )[−2:] ◦ id(r2 ) id(r1 ) ◦ id(r2 ) Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. If w is ChiWe first add the rule identification feature for each nese we also include its word type (punctuations, rule: id(ri ). We also introduce lexicalized Word- digits, alpha, or otherwise) and (leftmost or rightEdges features, which are shown to be very effec- most) character. In such a way, we significantly intive in parsing (Charniak and Johnson, 2005) and crease the feature coverage on unseen data. However, if we allow arbitrary combinations, we MT (Liu et al., 2008; He et al., 2008) literatures. We use the following atomic features when apply- can extract a hexalexical feature (4 Chinese + 2 Ening a rule ri = hc(ri ), e(ri )i: the source-side length glish words) for a local window in Figure 5, which |c(ri )|, the boundary words of both c(ri ) and e(ri ), is unlikely to be seen at test time. To control model and the surrounding words of c(ri ) on the input sen- complexity we introduce a feature budget for each tence x. See Figure 5 for exa"
D13-1112,D08-1024,0,0.0581263,"s, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, and up to +1.5/+1.5 over P RO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 0 1 We will use the following running example from Chinese to English from Mi et al. (2008): 1113 3 4 5 6 Figure 1: Standard beam-search phrase-based decoding. B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. For example, the following is one possible derivation: ) : (0, “”) r1 (•1 ) : (s1 , “Bush”) r2"
D13-1112,P04-1015,0,0.710243,"se features. This is the first successful effort of large-scale online discriminative training for MT. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2"
D13-1112,N12-1015,1,0.82011,"lish and Spanish-to-English tasks show substantial gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, thanks to 20M+ sparse features. This is the first successful effort of large-scale online discriminative training for MT. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-lik"
D13-1112,P08-1067,1,0.834225,"s are ICTCLAS for Chinese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we also introduce them to capture the contextual information in MT. Our non-local features, shown in Figure 5, include bigram rule-ids and the concatenation of a rule id with the translation history, i.e. the last two English words. Note that we also use backoffs (Table 1) for the words included. Experiments (Section 5.3) show that although the set of non-local features is just a tiny fraction of all features, it contributes substantially to the improvement in B LEU. Scale small large large Language Pair C H -E N S P -E N Training Data # sent. # words 30K 0.8M/1"
D13-1112,P07-2045,0,0.00940476,"Design We compare the above new update methods with the two existing ones from Liang et al. (2006). Standard update (also known as “bold update” in Liang et al. (2006)) simply updates at the very end, from the best derivation in the beam towards the best gold-standard derivation (regardless of whether Our feature set includes the following 11 dense features: LM, four conditional and lexical translation probabilities (pc (e|f ), pc (f |e), pl (e|f ), pl (f |e)), length and phrase penalties, distortion cost, and three lexicalized reordering features. All these features are inherited from Moses (Koehn et al., 2007). 1116 ,Bush ) : (s01 , “<s> Bush”) (•1 (• •••6 ,talks ) : (s02 , “<s> Bush held talks”) <s> B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an r1 <s> features for applying r2 on span x[3:6] r2 WordEdges </s> r2 Bush held talks non-local c(r2 )[0] = jˇux´ıng, c(r2 )[−1] = hu`ıt´an e(r2 )[0] = held, e(r2 )[−1] = talks x[2:3] = Sh¯al´ong, x[6:7] = </s>, |c(r2 ) |= 3 ... (combos of the above atomic features) ... e(r0 ◦ r1 )[−2:] ◦ id(r2 ) id(r1 ) ◦ id(r2 ) Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. I"
D13-1112,koen-2004-pharaoh,0,0.104318,"set of sparse, lexicalized, and non-local features, and we propose various ways to alleviate overfitting. For simplicity and efficiency reasons, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, and up to +1.5/+1.5 over P RO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 0 1 We will use the following running example from Chinese to English from Mi et al. (2008): 1113 3 4 5 6 Figure 1: Standard beam-search phrase-based decoding. B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phra"
D13-1112,P06-1096,0,0.252679,"Missing"
D13-1112,W02-1001,0,0.345469,"ng data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can no"
D13-1112,D08-1010,0,0.0674233,"amples of WordEdges and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. If w is ChiWe first add the rule identification feature for each nese we also include its word type (punctuations, rule: id(ri ). We also introduce lexicalized Word- digits, alpha, or otherwise) and (leftmost or rightEdges features, which are shown to be very effec- most) character. In such a way, we significantly intive in parsing (Charniak and Johnson, 2005) and crease the feature coverage on unseen data. However, if we allow arbitrary combinations, we MT (Liu et al., 2008; He et al., 2008) literatures. We use the following atomic features when apply- can extract a hexalexical feature (4 Chinese + 2 Ening a rule ri = hc(ri ), e(ri )i: the source-side length glish words) for a local window in Figure 5, which |c(ri )|, the boundary words of both c(ri ) and e(ri ), is unlikely to be seen at test time. To control model and the surrounding words of c(ri ) on the input sen- complexity we introduce a feature budget for each tence x. See Figure 5 for examples. These atomic level of backoffs, shown in the last column in Tafeatures are concatenated to generate all kinds"
D13-1112,N13-1025,0,0.0872383,"updates in standard update. than half of the updates remain invalid even at a beam of 30. These analyses provide an alternative but theoretically more reasonable explanation to the findings of Liang et al. (2006): while they blame “unreasonable” gold derivations for the failure of standard update, we observe that it is the search errors that make the real difference, and that an update that respects search errors towards a gold subderivation is indeed helpful, even if that subderivation might be “unreasonable”. In order to speedup training, we use mini-batch parallelization of Zhao and Huang (2013) which has been shown to be much faster than previous parallelization methods. We set the mini-batch size to 24 and train M AX F ORCE with 1, 6, and 24 cores on a small subset of the our original reachable sen24 26 25 PRO-dense 24 BLEU BLEU MERT 23 minibatch(24-core) minibatch(6-core) minibatch(1 core) single processor MERT 23 22 21 +non-local +word-edges +ruleid dense 20 19 22 18 0 0.5 1 1.5 2 2.5 Time 3 3.5 4 Figure 10: Minibatch parallelization speeds up learning. 2 4 6 8 10 12 Number of iteration 14 16 Figure 12: Incremental contributions of different feature sets (dense features, ruleid,"
D13-1112,N13-1048,0,0.153221,"mparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scales, for the first time, to a large portion of the whole training data, and enables us t"
D13-1112,N12-1023,0,0.0612216,"ive training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not † 3 {huang@cs.qc,kzhao@gc}.cuny.edu Abstract ∗ Haitao Mi3 Work done while visiting City University of New York. Corresponding author. To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excellent empirical performance on beam search parsing and tagging. The basic idea is to update when search"
D13-1112,J93-2004,0,0.0461422,"for large datasets, we Brown cluster, prefix 4 4 4 2 simply remove all one-count rules, but for small 52 36 2 POS tag datasets where out-of-vocabulary words (OOVs) word type 4 1 abound, we use a simple leave-one-out method: Table 1: Various levels of backoff for WordEdges fea- when training on a sentence pair (x, y), do not use tures. Class size is estimated on the small Chinese- the one-count rules extracted from (x, y) itself. 4.1 Local Sparse Features: Ruleid & WordEdges English dataset (Sec. 5.3). The POS tagsets are ICTCLAS for Chinese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we also introduce them to capture the contextual infor"
D13-1112,P05-1012,0,0.107618,"tured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexic"
D13-1112,P08-1023,1,0.766752,"ncy reasons, in this paper we use phrase-based translation, but our method has the potential to be applicable to other translation paradigms. Extensive experiments on both Chineseto-English and Spanish-to-English tasks show statistically significant gains in B LEU by up to +2.3/+2.0 on dev/test over M ERT, and up to +1.5/+1.5 over P RO, thanks to 20M+ sparse features. 2 Phrase-Based MT and Forced Decoding We first review the basic phrase-based decoding algorithm (Koehn, 2004), which will be adapted for forced decoding. 0 1 We will use the following running example from Chinese to English from Mi et al. (2008): 1113 3 4 5 6 Figure 1: Standard beam-search phrase-based decoding. B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with Sharon hold -ed meeting ‘Bush held a meeting with Sharon’ Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (or states) (Koehn, 2004). Each hypothesis has a coverage vector capturing the sourcelanguage words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. For example, the following is one possible derivation: ) : (0, “”) r1 (•1 ) : (s1 , “Bush”) r2"
D13-1112,P12-1031,0,0.0155333,"These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scales, for the first time, to a large portion of the whole training dat"
D13-1112,P03-1021,0,0.209362,"aining (see Table 3). In order to test our approach in different language pairs, we conduct three experiments, shown in Table 2, on two significantly different language pairs (long vs. short distance reorderings), Chinese-toEnglish (C H -E N) and Spanish-to-English (S P -E N). 5.1 System Preparation and Data We base our experiments on Cubit, a state-of-art phrase-based system in Python (Huang and Chiang, 2007).1 We set phrase-limit to 7 in rule extraction, and beam size to 30 and distortion limit 6 in decoding. We compare our violation-fixing perceptron with two popular tuning methods: M ERT (Och, 2003) and P RO (Hopkins and May, 2011). For word alignments we use GIZA++-`0 (Vaswani et al., 2012) which produces sparser alignments, alleviating the garbage collection problem. We use the SRILM toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on 1.5M English sentences. Our dev and test sets for C H -E N task are from the newswire portion of 2006 and 2008 NIST MT Evaluations (616/691 sentences, 18575/18875 words), with four references.2 The dev and test sets for S P E N task are from newstest2012 and newstest2013, with only one reference. Below both M ER"
D13-1112,C08-1041,0,0.0607494,"s and non-local features. The notation uses the Python style subscript syntax. al., 1992), and w’s part-of-speech tag. If w is ChiWe first add the rule identification feature for each nese we also include its word type (punctuations, rule: id(ri ). We also introduce lexicalized Word- digits, alpha, or otherwise) and (leftmost or rightEdges features, which are shown to be very effec- most) character. In such a way, we significantly intive in parsing (Charniak and Johnson, 2005) and crease the feature coverage on unseen data. However, if we allow arbitrary combinations, we MT (Liu et al., 2008; He et al., 2008) literatures. We use the following atomic features when apply- can extract a hexalexical feature (4 Chinese + 2 Ening a rule ri = hc(ri ), e(ri )i: the source-side length glish words) for a local window in Figure 5, which |c(ri )|, the boundary words of both c(ri ) and e(ri ), is unlikely to be seen at test time. To control model and the surrounding words of c(ri ) on the input sen- complexity we introduce a feature budget for each tence x. See Figure 5 for examples. These atomic level of backoffs, shown in the last column in Tafeatures are concatenated to generate all kinds of ble 1. The tota"
D13-1112,N13-1034,0,0.0544204,"3/+1.1 B LEU on dev/test. These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scales, for the first time, to a large portion of the"
D13-1112,D11-1125,0,0.288793,"ntroduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not † 3 {huang@cs.qc,kzhao@gc}.cuny.edu Abstract ∗ Haitao Mi3 Work done while visiting City University of New York. Corresponding author. To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excellent empirical performance on beam search parsing and tagging. The"
D13-1112,P07-1019,1,0.333359,", 5.3 5.2, 5.4 5.5 Table 2: Overview of all experiments. The ∆B LEU column shows the absolute improvements of our method M AX F ORCE on dev/test sets over M ERT. The Chinese datasets also use prefix-pairs in training (see Table 3). In order to test our approach in different language pairs, we conduct three experiments, shown in Table 2, on two significantly different language pairs (long vs. short distance reorderings), Chinese-toEnglish (C H -E N) and Spanish-to-English (S P -E N). 5.1 System Preparation and Data We base our experiments on Cubit, a state-of-art phrase-based system in Python (Huang and Chiang, 2007).1 We set phrase-limit to 7 in rule extraction, and beam size to 30 and distortion limit 6 in decoding. We compare our violation-fixing perceptron with two popular tuning methods: M ERT (Och, 2003) and P RO (Hopkins and May, 2011). For word alignments we use GIZA++-`0 (Vaswani et al., 2012) which produces sparser alignments, alleviating the garbage collection problem. We use the SRILM toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on 1.5M English sentences. Our dev and test sets for C H -E N task are from the newswire portion of 2006 and 2008 NIST"
D13-1112,P12-1002,0,0.0244538,"oves the translation quality over M ERT by +1.3/+1.1 B LEU on dev/test. These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine translation which scale"
D13-1112,P11-1086,1,0.860759,"nese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we also introduce them to capture the contextual information in MT. Our non-local features, shown in Figure 5, include bigram rule-ids and the concatenation of a rule id with the translation history, i.e. the last two English words. Note that we also use backoffs (Table 1) for the words included. Experiments (Section 5.3) show that although the set of non-local features is just a tiny fraction of all features, it contributes substantially to the improvement in B LEU. Scale small large large Language Pair C H -E N S P -E N Training Data # sent. # words 30K 0.8M/1.0M 230K 6.9M/8.9M 174K 4.9M/4"
D13-1112,P12-1033,1,0.356264,"conduct three experiments, shown in Table 2, on two significantly different language pairs (long vs. short distance reorderings), Chinese-toEnglish (C H -E N) and Spanish-to-English (S P -E N). 5.1 System Preparation and Data We base our experiments on Cubit, a state-of-art phrase-based system in Python (Huang and Chiang, 2007).1 We set phrase-limit to 7 in rule extraction, and beam size to 30 and distortion limit 6 in decoding. We compare our violation-fixing perceptron with two popular tuning methods: M ERT (Och, 2003) and P RO (Hopkins and May, 2011). For word alignments we use GIZA++-`0 (Vaswani et al., 2012) which produces sparser alignments, alleviating the garbage collection problem. We use the SRILM toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on 1.5M English sentences. Our dev and test sets for C H -E N task are from the newswire portion of 2006 and 2008 NIST MT Evaluations (616/691 sentences, 18575/18875 words), with four references.2 The dev and test sets for S P E N task are from newstest2012 and newstest2013, with only one reference. Below both M ERT and P RO tune weights on the dev set, while our method on the training set. Specifically, ou"
D13-1112,D07-1080,0,0.134666,"rror is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P RO (Hopkins and May, 2011), and R AMP (Gimpel and Smith, 2012). However, it is well-known that the most important features for NLP are lexicalized, most of which can not † 3 {huang@cs.qc,kzhao@gc}.cuny.edu Abstract ∗ Haitao Mi3 Work done while visiting City University of New York. Corresponding author. To address the search error problem we propose a very simple approach based on the recent framework of “violation-fixing perceptron” (Huang et al., 2012) which is designed specifically for inexact search, with a theoretical convergence guarantee and excell"
D13-1112,P10-1049,0,0.0524699,"that M AX F ORCE improves the translation quality over M ERT by +1.3/+1.1 B LEU on dev/test. These gains are comparable to the improvements on the C H -E N task, since it is well accepted in MT literature that a change of δ in 1-reference B LEU is roughly equivalent to a change of 2δ with 4 references. 1121 Besides those discussed in Section 1, there are also some research on tuning sparse features on the training data, but they integrate those sparse features into the MT log-linear model as a single feature weight, and tune its weight on the dev set (e.g. (Liu et al., 2008; He et al., 2008; Wuebker et al., 2010; Simianer et al., 2012; Flanigan et al., 2013; Setiawan and Zhou, 2013; He and Deng, 2012; Gao and He, 2013)). By contrast, our approach learns sparse features only on the training set, and use dev set as heldout to know when to stop. Forced decoding has been used in the MT literature. For example, open source MT systems Moses and cdec have implemented it. Liang et al. (2012) also use the it to boost the M ERT tuning by adding more y-good derivations to the standard k-best list. 7 Conclusions and Future Work We have presented a simple yet effective approach of structured learning for machine"
D13-1112,W03-1730,0,0.0131616,"luster, prefix 6 6 8 2 ting due to one-count rules: for large datasets, we Brown cluster, prefix 4 4 4 2 simply remove all one-count rules, but for small 52 36 2 POS tag datasets where out-of-vocabulary words (OOVs) word type 4 1 abound, we use a simple leave-one-out method: Table 1: Various levels of backoff for WordEdges fea- when training on a sentence pair (x, y), do not use tures. Class size is estimated on the small Chinese- the one-count rules extracted from (x, y) itself. 4.1 Local Sparse Features: Ruleid & WordEdges English dataset (Sec. 5.3). The POS tagsets are ICTCLAS for Chinese (Zhang et al., 2003) and Penn Treebank for English (Marcus et al., 1993). 4.2 Addressing Overfitting With large numbers of lexicalized combo features we will face the overfitting problem, where some combo features found in the training data are too rare to be seen in the test data. Thus we propose three ways to alleviate this problem. First, we introduce various levels of backoffs for each word w (see Table 1). We include w’s Brown cluster and its prefixes of lengths 4 and 6 (Brown et 1117 4.3 Non-Local Features Following the success of non-local features in parsing (Huang, 2008) and MT (Vaswani et al., 2011), we"
D13-1112,D13-1093,1,0.086367,"first successful effort of large-scale online discriminative training for MT. As a notable exception, Liang et al. (2006) do train a structured perceptron model on the training data with sparse features, but fail to outperform M ERT. We argue this is because structured perceptron, like many structured learning algorithms such as CRF and M IRA, assumes exact search, and search errors inevitably break theoretical properties such as convergence (Huang et al., 2012). Empirically, it is now well accepted that standard perceptron performs poorly when search error is severe (Collins and Roark, 2004; Zhang et al., 2013). 1 Introduction Large-scale discriminative training has witnessed great success in many NLP problems such as parsing (McDonald et al., 2005) and tagging (Collins, 2002), but not yet for machine translation (MT) despite numerous recent efforts. Due to scalability issues, most of these recent methods can only train on a small dev set of about a thousand sentences rather than on the full training set, and only with 2,000–10,000 rather “dense-like” features (either unlexicalized or only considering highest-frequency words), as in M IRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012), P"
D13-1112,N13-1038,1,0.225664,"atio of invalid updates in standard update. than half of the updates remain invalid even at a beam of 30. These analyses provide an alternative but theoretically more reasonable explanation to the findings of Liang et al. (2006): while they blame “unreasonable” gold derivations for the failure of standard update, we observe that it is the search errors that make the real difference, and that an update that respects search errors towards a gold subderivation is indeed helpful, even if that subderivation might be “unreasonable”. In order to speedup training, we use mini-batch parallelization of Zhao and Huang (2013) which has been shown to be much faster than previous parallelization methods. We set the mini-batch size to 24 and train M AX F ORCE with 1, 6, and 24 cores on a small subset of the our original reachable sen24 26 25 PRO-dense 24 BLEU BLEU MERT 23 minibatch(24-core) minibatch(6-core) minibatch(1 core) single processor MERT 23 22 21 +non-local +word-edges +ruleid dense 20 19 22 18 0 0.5 1 1.5 2 2.5 Time 3 3.5 4 Figure 10: Minibatch parallelization speeds up learning. 2 4 6 8 10 12 Number of iteration 14 16 Figure 12: Incremental contributions of different feature sets (dense features, ruleid,"
D16-1096,W14-4012,0,0.192539,"Missing"
D16-1096,D13-1052,1,0.906077,"Missing"
D16-1096,N16-1102,0,0.0381397,"xj = zt,j ◦ ct−1,xj + (1 − zt,j ) ◦ c˜t,xj , where, zt is the update gate, rt is the reset gate, c˜t is the new memory content, and ct is the final memory. The matrix W zy , W zα , U z , W ry , W rα , U r , W y , W α and U are shared across different position j. ◦ is a pointwise operation. t=1 i=1 r c˜t,xj = tanh(W yt + W α αt,j + rt,j ◦ U ct−1,xj ) 3.1.2 Objectives t=1 ) l m X X −λ ( ||cj,xi ||) , i=1 j=axi (7) where axi is the maximum index on the target sentence xi can be aligned to. 4 Related Work There are several parallel and independent related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one. In their paper, they also employ a GRU to model the coverage vector. One main difference is that our model introduces a specific coverage embedding vector for each source word, in contrast, their work initializes the word coverage vector with a scalar with a uniform distribution. Another difference lays in the fertility part, Tu et al. (2016) add an accumulate operation and a fertility function to simulate the process of one-to-many alignments. In our approach, we add fertility information directly to coverage embeddings, as each source word has its"
D16-1096,P05-1066,0,0.0470867,"the hybrid system. We test four different settings for our coverage embedding models: • UGRU : updating with a GRU; • USub : updating as a subtraction; • UGRU + USub : combination of two methods (do not share coverage embedding vectors); • +Obj.: UGRU + USub plus an additional objective in Equation 61 . UGRU improves the translation quality by 1.3 points on average over LVNMT. And UGRU + USub achieves the best average score of 13.14, which is about 2.6 points better than LVNMT. All the improvements of our coverage embedding models over LVNMT are statistically significant with the signtest of Collins et al. (2005). We believe that we need to explore more hyper-parameters of +Obj. in order to get even better results over UGRU + USub . 1 We use two λs for UGRU and USub separately, and we test λGRU = 1 × 10−4 and λSub = 1 × 10−2 in our experiments. single system Ours Tree-to-string LVNMT UGRU USub UGRU +USub +Obj. MT08 MT06 BP 0.95 0.96 0.92 0.91 0.92 0.93 B LEU T- B B P 34.93 9.45 0.94 34.53 12.25 0.93 35.59 10.71 0.89 35.90 10.29 0.88 36.60 9.36 0.89 36.80 9.78 0.90 News B LEU 31.12 28.86 30.18 30.49 31.86 31.83 T- B 12.90 17.40 15.33 15.23 13.69 14.20 BP 0.90 0.97 0.97 0.96 0.95 0.95 Web B LEU 23.45 26"
D16-1096,N13-1073,0,0.0138262,"T08 web. For all NMT systems, the full vocabulary sizes for thr two training sets are 300k and 500k respectively. The coverage embedding vector size is 100. In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80. Following Mi et al. (2016b), the output vocabulary for each mini-batch or sentence is a sub-set of the full vocabulary. For each source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. Following Jean et al. (2015), We dump the align958 ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tr"
D16-1096,D11-1125,0,0.00854951,"o-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER- B LEU)/2 on the development set. 5.2 Translation Results Table 1 shows the results of all systems on 5 million training set. The traditional syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, and 13.36 on average in terms of (T ER- B LEU)/2. The largevocabulary NMT (LVNMT), our baseline, achieves an average (T ER- B LEU)/2 score of 15.74, which is about 2 points worse than the hybrid system. We test four different settings for our coverage embedding models: • UGRU : updating with a GRU; • USub : updating as a subtraction;"
D16-1096,P15-1001,0,0.289679,"ding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. 1 Introduction Neural machine translation (NMT) has gained popularity in recent years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) ad"
D16-1096,koen-2004-pharaoh,0,0.0467355,"ean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) address the above issues by employing a source side “coverage vector” for each sentence to indicate explicitly which words have been translated, which parts have not yet. A coverage vector starts with all zeros, meaning no word has been translated. If a source word at position j got translated, the coverage vector sets position j as 1, and they won’t use this source ∗ Work done while at IBM. To contact Abe, aittycheriah@google.com. word in future translation. This mechanism avoids the repeating or dropping translation problems. However, it is not easy to adapt the “coverage vector” to NMT di"
D16-1096,P09-1065,1,0.845573,"ies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. Following Jean et al. (2015), We dump the align958 ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER- B LEU)/2 on the development set. 5.2 Transl"
D16-1096,D15-1166,0,0.0333544,"age embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. 1 Introduction Neural machine translation (NMT) has gained popularity in recent years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) address the above issu"
D16-1096,D16-1249,1,0.64441,"to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. 1 Introduction Neural machine translation (NMT) has gained popularity in recent years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016)), especially for the attentionbased models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations. The traditional statistical machine translation (SMT) systems (e.g. (Koehn, 2004)) address the above issues by employing a"
D16-1096,P16-2021,1,0.389163,"Missing"
D16-1096,D16-1160,0,0.0181148,"with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER- B LEU)/2 on the development set. 5.2 Translation Results Table 1 shows the results of all systems on 5 million training set. The traditional syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, and 13.36 on average in terms of (T ER- B LEU)/2. The largevocabulary NMT (LVNMT), our baseline, achieves an average (T ER- B LEU)/2 score of 15.74, which is about 2 points worse than the hybrid syst"
D16-1096,D08-1060,0,0.0338928,"source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. Following Jean et al. (2015), We dump the align958 ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of Liu et al. (2009) and Cmejrek et al. (2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. We tune our system with PRO (Hopkins and May, 2011) to minimize (T ER-"
D16-1096,P05-1033,0,\N,Missing
D16-1249,mauser-etal-2008-automatic,0,0.014603,"tic merge of both directions of GIZA++), • MaxEnt (trained on 67k hand-aligned sentences). 1 The metric used for optimization in this work is (T ERB LEU)/2 to prevent the system from using sentence length alone to impact B LEU or T ER. Typical SMT systems use target word count as a feature and it has been observed that B LEU can be optimized by tweaking the weighting of the target word count with no improvement in human assessments of translation quality. Conversely, in order to optimize T ER shorter sentences can be produced. Optimizing the combination of metrics alleviates this effect (Arne Mauser and Ney, 2008). 5.3 Alignment Results Table 2 shows the alignment F1 scores on the alignment test set (447 hand aligned sentences). The MaxEnt model is trained on 67k hand-aligned sentences, and achieves an F1 score of 75.96. For NMT systems, we dump the alignment matrixes and convert them into alignments with following steps. For each target word, we sort the alphas and add the max probability link if it is higher than 0.2. If we only tune the alignment component (A in line 3), we improve the alignment F1 score from 45.76 to 47.87. 2287 system MaxEnt Cov LVNMT (Mi et al., 2016b) A A→J Zh → En A→T A→T→J J G"
D16-1249,D13-1052,1,0.917255,"Missing"
D16-1249,P05-1066,0,0.018233,"es 3 to 5). We have to conduct joint optimization J in order to get a comparable or better result (lines 3, 5 and 6) over the baseline system. Second, when we change the training alignment seeds (Zh → En, GDFA, and MaxEnt) NMT model does not yield significant different results (lines 6 to 8). Third, the smoothed transformation (J + Gau.) gives some improvements over the simple transformation (the last two lines), and achieves the best result (1.2 better than LVNMT, and 0.3 better than Tree-to-string). In terms of B LEU scores, we conduct the statistical significance tests with the signtest of Collins et al. (2005), the results show that the improvements of our J + Gau. over LVNMT are significant on three test sets (p < 0.01). At last, the brevity penalty (BP) consistently gets better after we add the alignment cost to NMT objective. Our alignment objective adjusts the translation length to be more in line with the human references accordingly. pre. 74.86 51.11 50.88 53.18 50.29 53.71 54.29 53.88 44.42 48.90 rec. 77.10 41.42 45.19 49.37 44.90 49.33 48.02 48.25 55.25 55.38 F1 75.96 45.76 47.87 51.21 47.44 51.43 50.97 50.91 49.25 51.94 Table 2: Alignment F1 scores of different models. And we further boost"
D16-1249,N13-1073,0,0.0546246,"7.04 15.61 16.72 16.21 15.80 T- B 13.36 14.24 13.87 20.40 13.97 13.36 13.96 13.24 13.04 Table 1: Single system results in terms of (T ER-B LEU)/2 (T- B, the lower the better) on 5 million Chinese to English training set. B P denotes the brevity penalty. NMT results are on a large vocabulary (300k) and with UNK replaced. The second column shows different alignments (Zh → En (one direction), GDFA (“grow-diag-final-and”), and MaxEnt (Ittycheriah and Roukos, 2005). A, T, and J mean optimize alignment only, translation only, and jointly. Gau. denotes the smoothed transformation. from ‘fast align’ (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. May, 2011) to minimize (T ER- B LEU)/2 1 on the development set. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al. (2015). The coverage embedding dimension of each source word is 100. Table 1 shows the translation results of all systems. The syntax-based statistical machine translation model achieves an average"
D16-1249,D11-1125,0,0.0180526,"Missing"
D16-1249,H05-1012,0,0.275361,"s the n-th sentence pair (xn , y∗ n ) in the training set, N is the total number of pairs. 3 Alignment Component The attentions, αt,1 ...αt,l , in each step t play an important role in NMT. However, the accuracy is still far behind the traditional MaxEnt alignment model in terms of alignment F1 score (Mi et al., 2016b; Tu et al., 2016). Thus, in this section, we explicitly add an alignment distance to the objective function in Eq. 5. The “truth” alignments for each sentence pair can be from human annotated data, unsupervised or supervised alignments (e.g. GIZA++ (Och and Ney, 2000) or MaxEnt (Ittycheriah and Roukos, 2005)). Given an alignment matrix A for a sentence pair (x, y) in Figure 2 (a), where we have an end-ofsource-sentence token heosi = xl , and we align all the unaligned target words (y3∗ in this example) to ∗ (end-of-target-sentence) to heosi, also we force ym be aligned to xl with probability one. Then we conduct two transformations to get the probability distribution matrices ((b) and (c) in Figure 2). 2284 Simple Transformation The first transformation simply normalizes each row. Figure 2 (b) shows the result matrix A∗ . The last column in red dashed lines shows the alignments of the special end"
D16-1249,P15-1001,0,0.149248,"ing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the “true” alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment dis"
D16-1249,N06-1014,0,0.0159153,"ignment only, then optimize translation). Thus, we divide the network in Figure 1 into alignment A and translation T parts: • A: all networks before the hidden state st , ∗ ). • T: the network g(st , yt−1 If we only optimize A, we keep the parameters in T unchanged. We can also optimize them jointly J. In our experiments, we test different optimization strategies. 4 By contrast, our approach directly uses and optimizes NMT parameters using the “supervised” alignments. Related Work In order to improve the attention or alignment accuracy, Cheng et al. (2016) adapted the agreementbased learning (Liang et al., 2006; Liang et al., 2008), and introduced a combined objective that takes into account both translation directions (source-to-target and target-to-source) and an agreement term between the two alignment directions. 2285 Data Preparation We run our experiments on Chinese to English task. The training corpus consists of approximately 5 million sentences available within the DARPA BOLT Chinese-English task. The corpus includes a mix of newswire, broadcast news, and webblog. We do not include HK Law, HK Hansard and UN data. The Chinese text is segmented with a segmenter trained on CTB data using condi"
D16-1249,P09-1065,1,0.821065,"verage (T ER-B LEU)/2 of 14.24, which is about 0.9 points worse than Tree-to-string SMT system. Please note that all systems are single systems. It is highly possible that ensemble of NMT systems with different random seeds can lead to better results over SMT. We test three different alignments: Following Jean et al. (2015), we dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our SMT system is a hybrid syntax-based tree-tostring model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our two 5-gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07), respectively.As suggested by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. In this paper, our NMT systems only use the bilingual data. We tune our system with PRO (Hopkins and 2286 5.2 Tr"
D16-1249,D15-1166,0,0.0873705,"of training sentence pairs. We simply compute the distance between the machine attentions and the “true” alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the ob"
D16-1249,D16-1096,1,0.667593,"pairs. We simply compute the distance between the machine attentions and the “true” alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the objective function."
D16-1249,P16-5005,0,0.0391706,"tem, and even beats a state-of-the-art traditional syntax-based system. 1 Neural Machine Translation Introduction Neural machine translation (NMT) has gained popularity in recent two years (e.g. (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016b; Li et al., 2016), especially for the attentionbased models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the objective function. Thus, we not only maximize the log translation probabilities, but also minimize the alignment distance cost. Large-scale experiments over Chineseto-English on various test sets show that our best method for a single system improves the translation quality significantly over the large vocabulary NMT system (Section 5) and beats the state-of-"
D16-1249,D16-1160,0,0.0343645,"word-to-word translation model or the aligned source word. Our SMT system is a hybrid syntax-based tree-tostring model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our two 5-gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07), respectively.As suggested by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. In this paper, our NMT systems only use the bilingual data. We tune our system with PRO (Hopkins and 2286 5.2 Translation Results • Zh → En (one direction of GIZA++), • GDFA (the “grow-diag-final-and” heuristic merge of both directions of GIZA++), • MaxEnt (trained on 67k hand-aligned sentences). 1 The metric used for optimization in this work is (T ERB LEU)/2 to prevent the system from using sentence length alone to impact B LEU or T ER. Typical SMT systems use target word count as a feature and it has been o"
D16-1249,D08-1060,0,0.0535876,"LEU)/2 of 13.36 on three test sets. The Cov. LVNMT system achieves an average (T ER-B LEU)/2 of 14.24, which is about 0.9 points worse than Tree-to-string SMT system. Please note that all systems are single systems. It is highly possible that ensemble of NMT systems with different random seeds can lead to better results over SMT. We test three different alignments: Following Jean et al. (2015), we dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our SMT system is a hybrid syntax-based tree-tostring model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our two 5-gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07), respectively.As suggested by Zhang (2016), NMT systems can achieve better results with the help of those monolingual corpora. In this paper, our NMT systems only use the biling"
D16-1249,P16-2021,1,\N,Missing
D16-1249,P00-1056,0,\N,Missing
I08-1066,P06-1067,0,0.122811,"Missing"
I08-1066,H05-1098,0,0.184236,"h space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a st"
I08-1066,J07-2003,0,0.0945551,"exity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed is cube pruning proposed by Chiang (2007) which reduces search space significantly. In this paper, we propose two refinements to address the two issues, including (1) reordering heuristics to prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT a"
I08-1066,W00-2010,0,0.0338613,"swapping window and punctuation restriction. Swapping Window (SW): It constrains block swapping in the following way ACTIVATE A → hA1 , A2 i IF |A1s |+ |A2s |&lt; sws where |Ais |denotes the number of words on the source side Ais of block Ai , sws is a pre-defined swapping window size. Any inverted reordering beyond the pre-defined swapping window size is prohibited. Punctuation Restriction (PR): If two neighboring blocks include any of the punctuation marks p ∈ {， 、 ： ； 「 」 《 》 （ ） “ ”}, the two blocks will be merged with straight order. Punctuation marks were already used in parsing (Christine Doran, 2000) and statistical machine translation (Och et al., 2003). In (Och et al., 2003), three kinds of features are defined, all related to punctuation marks like quotes, parentheses and commas. Unfortunately, no statistically significant improvement on the BLEU score was reported in (Och et al., 2003). In this paper, we consider this problem from a different perspective. We emphasize that words around punctuation marks are reordered ungrammatically and therefore we positively use punctuation marks as a hard decision to restrict such reordering around punctuations. This is straightforward but yet resu"
I08-1066,W05-1507,0,0.0156052,"of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed"
I08-1066,koen-2004-pharaoh,0,0.044683,"ss, BTG restriction is widely used for reordering in SMT (Zens et al., 2004). However, BTG restriction does not provide a mechanism to predict final orders between two neighboring blocks. 505 {htmi, liuqun, sxlin}@ict.ac.cn To solve this problem, Xiong et al. (2006) proposed an enhanced BTG with a maximum entropy (MaxEnt) based reordering model (MEBTG). MEBTG uses boundary words of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 )."
I08-1066,P07-2045,0,0.00543539,"prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has"
I08-1066,P06-1077,1,0.851772,"tion restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted"
I08-1066,W02-2018,0,0.00938526,"hrase penalty and word penalty, respectively and λs are weights of features. These features are commonly used in the state-of-the-art systems (Koehn et al., 2005; Chiang et al., 2005). 2.2 MaxEnt-based Reordering Model The MaxEnt-based reordering model is defined on two consecutive blocks A1 and A2 together with their order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) P Ω = pθ (o|A , A ) = P 1 2 o exp( i θi hi (o, A , A )) (6) where the functions hi ∈ {0, 1} are model features and θi are weights of the model features trained automatically (Malouf, 2002). There are three steps to train a MaxEnt-based reordering model. First, we need to extract reordering examples from unannotated bilingual data, then generate features from these examples and finally estimate feature weights. 1 2 For extracting reordering examples, there are two points worth mentioning: 1. In the extraction of useful reordering examples, there is no length limitation over blocks compared with extracting bilingual phrases. 2. When enumerating all combinations of neighboring blocks, a good way to keep the number of reordering examples acceptable is to extract smallest blocks wit"
I08-1066,W06-1606,0,0.050471,"g window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose"
I08-1066,P02-1038,0,0.0523918,"ining data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probability P rm (A) is defined as follows LM P rm (A) = ΩλΩ · 4λpLM (A1 ,A2 ) (4) where Ω, the reordering score of block A1 and A2 , is calculated using the MaxEnt-based reordering model (Xiong et al., 2006) described in the next section, λΩ is the weight of Ω, and 4pLM (A1 ,A2 ) is the increment of language model score of the two blocks according to their final order, λLM is its weight. For the lexical rule (3), it is applied with a probability P rl (A) P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|y|)λ6 LM"
I08-1066,P96-1021,0,0.151694,"valuation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probabilit"
I08-1066,P06-1066,1,0.959397,"tegies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source ph"
I08-1066,C04-1030,0,0.374843,"Missing"
I08-1066,zhang-etal-2004-interpreting,0,0.0650612,"Missing"
I08-1066,2005.iwslt-1.8,0,\N,Missing
K16-1004,W15-1509,0,0.0303111,"ns: apple {a, b, c} and orange {d, e, f} with a fruit type intention, or what-question {a, d}, when-question {b, e}, and yes/no-question cluster {c, f} with a question type intension. To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al., 2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowledge, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov, 2006; Xu et al., 2015), becomes more interesting. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can we have a unified model to integrate neural networks into the semi-supervised framework? In this paper, we propose a unified framework for the short text clustering task. We employ a Introduction Text clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar"
K16-1004,D14-1181,0,0.00666017,"Representation Learning for Short Texts We represent each word with a dense vector w, so that a short text s is first represented as a matrix S = [w1 , ..., w|s |], which is a concatenation of all vectors of w in s, |s |is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long shortterm memory (LSTM). More formally, we define the presentation function as x = f (s), where x represents the vector of the text s. We test two encoding functions (CNN and LSTM) in our experiments. Inspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and max-pooling. Then, a fully connected layer is 32 mean of the hidden states over the entire sentence is taken as the final representation vector. 3.2 3 The classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learn"
K16-1004,C02-1150,0,0.12767,"the weight of sn for µk . The second term is acquired from labeled data, and wnk is the weight of a labeled instance sn for µk . The update parameter step minimizes Jsemi with respect to f (·) by keeping {rnk } and {µk } fixed, which has no counterpart in the k-means algorithm. The main goal is to update parameters for the text representation model. We take Jsemi as the loss function, and train neural networks with the Adam algorithm (Kingma and Ba, 2014). 4 4.1 Experiment Experimental Setting We evaluate our method on four short text datasets. (1) question type is the TREC question dataset (Li and Roth, 2002), where all the questions are classified into 6 categories: abbreviation, description, entity, human, location and numeric. (2) ag news dataset contains short texts extracted from the AG’s news corpus, where all the texts are classified into 4 categories: World, Sports, Business, and Sci/Tech (Zhang and LeCun, 2015). (3) dbpedia is the DBpedia ontology dataset, which is constructed by picking 14 non-overlapping classes from DBpedia 2014 (Lehmann et al., 2014). (4) yahoo answer is the 10 topics classification dataset extracted from Yahoo! Answers Comprehensive Questions and Answers version 1.0"
N15-1108,W08-2102,0,0.0434111,"Missing"
N15-1108,A00-2018,0,0.703316,"Missing"
N15-1108,D11-1114,0,0.0443331,"Missing"
N15-1108,P04-1015,0,0.110605,"Missing"
N15-1108,I11-1136,0,0.0140991,"Missing"
N15-1108,P10-1110,1,0.573589,"the head word of its leftmost child). Figure 1: Shift-reduce system, omitting rexy . c is the model score, and csh , crexx , etc. are the action scores. NP state q: l0 : hS 0 |s01 , Q0 i : (c0 , v 0 ) l : hS|s1 |s0 , Qi : (c, v) l+1 : hS 0 |x(s01 , s0 ), Qi : (c0 +v+δ, v 0 +v+δ) l and l0 are even, p ∈ π(q) Figure 2: DP shift-reduce, omitting rexy and st. c and v are prefix and inside scores, and δ = csh (p) + crexx (q). State equivalence is defined below in Section 3. goal 2(2n − 1) : hs0 , i : c • l : hS, (t, w)|Qi : (c, v) l is even l+1 : hS|t(w), Qi : (c+csh , 0) VP VP0 PP V NP Following Huang and Sagae (2010), we represent feature templates as functions f (·, ·) on stack S and queue Q. Table 1 shows the 43 feature templates we use in this paper, all adopted from Zhu et al. (2013). They are combinations of the 32 atomic features ˜ f (S, Q) (e.g. s0 .t and s0 .c denote the head tag and 1031 3 Dynamic Programming The key idea towards DP is the merging of equivalent states, after which the stacks are organized in a “graph-structured stack” (GSS)(Tomita, 1988). Following Huang and Sagae (2010), “equivalent states” ∼ in a same beam are defined by the atomic features ˜ f (S, Q) and the span of s0 : hS, Q"
N15-1108,N12-1015,1,0.560902,"Missing"
N15-1108,P11-1068,0,0.0271685,"nd achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing suggests a potentially even bigger advantage by DP. However, with unary rules and more-than-binary branchings, constituency parsing presents challenges not found in dependency parsing that must be addressed before applying DP. Thus, we first present an odd-even Odd-Even Shift-Reduce CFG Parser One major challenge in constituency parsing is unary rules. Unlike dependency parsing where shiftreduce"
N15-1108,D11-1109,0,0.0130639,"Missing"
N15-1108,W04-0308,0,0.039793,"ent the first dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremen"
N15-1108,N07-1051,0,0.13874,"Missing"
N15-1108,W97-0301,0,0.35554,"Missing"
N15-1108,P06-2089,0,0.844202,"Missing"
N15-1108,P88-1031,0,0.704449,"ection 3. goal 2(2n − 1) : hs0 , i : c • l : hS, (t, w)|Qi : (c, v) l is even l+1 : hS|t(w), Qi : (c+csh , 0) VP VP0 PP V NP Following Huang and Sagae (2010), we represent feature templates as functions f (·, ·) on stack S and queue Q. Table 1 shows the 43 feature templates we use in this paper, all adopted from Zhu et al. (2013). They are combinations of the 32 atomic features ˜ f (S, Q) (e.g. s0 .t and s0 .c denote the head tag and 1031 3 Dynamic Programming The key idea towards DP is the merging of equivalent states, after which the stacks are organized in a “graph-structured stack” (GSS)(Tomita, 1988). Following Huang and Sagae (2010), “equivalent states” ∼ in a same beam are defined by the atomic features ˜ f (S, Q) and the span of s0 : hS, Qi ∼ hS 0 , Q0 i ⇔˜ f (S, Q) = ˜ f (S 0 , Q0 ) and s0 .span = s00 .span. Similarly, for each state p, π(p) is a set of predictor states, each of which can be combined with p in a rexx or rexy action. For each action, we have different operations on π(p). If a state p makes a sh action and generates a state p0 , then π(p0 ) = {p}. If two shifted states p0 and p00 are equivalent, p0 ∼ p00 , we merge π(p0 ) and π(p00 ). If a state p makes a reduce (rexx o"
N15-1108,P14-1069,0,0.130842,"ver, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing suggests a potentially even bigger advantage by DP. However, with unary rules and more-than-binary branchings, constituency parsing presents challenges not found in dependency parsing that must be addressed before applying DP. Thus, we first present an odd-even Odd-Even Shift-Reduce CFG Parser One major challenge in constituency parsing is unary rules. Unlike dependency parsing where shiftreduce always finishes in 2n−1 steps, existing incremental constituency parsers (Zhu et al., 2013; Wang and Xue, 2014) reach the goal state (full parse tree) in different steps due to different number of unary rules. So we propose a new, synchronized, “oddeven” system to reach the goal in the same 4n − 2 steps. A state is notated p = hS, Qi, where S is a stack of trees ..., s1 , s0 , and Q is a queue of wordtag pairs. At even steps (when step index is even) we can choose one of the three standard actions • sh: shift the head of Q, a word-tag pair (t, w), onto S as a singleton tree t(w); • rexx : combine the top two trees on the stack and replace them with a new tree x(s1 , s0 ), x being the root nonterminal,"
N15-1108,D08-1059,0,0.0150903,"dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremental parsing, and the big"
N15-1108,N13-1038,1,0.665188,"Missing"
N15-1108,P13-1043,0,0.79332,"uce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers. 1 2 Introduction Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014). However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates. To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states. This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms. In constituency parsing, however, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing su"
N15-1108,J03-4003,0,\N,Missing
P08-1023,P89-1018,0,0.664511,"re head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes. For example, the hyperedge for deduction (*) is notated: h(NPB0,1 , CC1,2 , NPB2,3 ), NP0,3 i. There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length. 3.2 Translation Forest 3.1 Parse Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989). For 194 Given a parse forest and a translation rule set R, we can generate a translation forest which has a similar hypergraph structure. Basically, just as the depthfirst traversal procedure in tree-based decoding (Figure 2), we visit in top-down order each node v in the IP0,6 VP1,6 NP0,3 (a) PP1,3 P1,2 CC1,2 NPB0,1 VPB3,6 NPB2,3 VV3,4 AS4,5 NR2,3 NR0,1 Sh¯al´ong yˇu B`ush´ı NPB5,6 NN5,6 jˇux´ıng le hu`ıt´an ⇓ translation rule set R IP0,6 e1 e2 NP0,3 e5 (b) VP1,6 e4 e3 PP1,3 NPB0,1 (c) translation hyperedge e1 e2 e3 e4 e5 e6 CC1,2 r1 r6 r3 r7 r8 r9 P1,2 VPB3,6 NPB2,3 VV3,4 AS4,5 e6 NPB5,6 t"
P08-1023,P05-1033,0,0.909811,"tially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary"
P08-1023,J07-2003,0,0.655297,"he subtranslations of the matched variable nodes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple htails(e), head (e), si where s is the target string from the rule, for example, up the computation. An +LM item of node v has the form (v a⋆b ), where a and b are the target-language ⋆ Sharon boundary words. For example, (VP held ) is an 1,6 +LM item with its translation starting with “held” and ending with “Sharon”. This scheme can be easily extended to work with a general n-gram by storing n − 1 words at both ends (Chiang, 2007). For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. 3.4 Forest Pruning Algorithm We use the prun"
P08-1023,P05-1066,0,0.13289,"Missing"
P08-1023,P05-1067,0,0.390349,"1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and"
P08-1023,N04-1035,0,0.515361,"ide tree, whose internal nodes are labeled by nonterminal symbols in N , and whose frontier nodes are labeled by source-side terminals in Σ or variables from a set X = {x1 , x2 , . . .}; s ∈ (X ∪ ∆)∗ is the target-side string where ∆ is the target language terminal set; and φ is a mapping from X to nonterminals in N . Each variable xi ∈ X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set. A similar formalism appears in another form in (Liu et al., 2006). These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al. (2004). Finally, from step (d) we apply rules r4 and r5 example, consider the Chinese sentence in Example (2) above, which has (at least) two readings depending on the part-of-speech of the word yˇu, which can be either a preposition (P “with”) or a conjunction (CC “and”). The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ush´ı and Sh¯al´ong) are combined to form a coordinated NP NPB0,1 CC1,2 NP0,3 NPB2,3 (*) which functions as the subject of the sentence. In this case the Chinese sentence is translated into ("
P08-1023,P06-1121,0,0.50156,"rses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as"
P08-1023,W05-1506,1,0.816055,"ize. The pruned parse forest will then be used to direct the translation. In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2). Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3). Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6 . Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair hV, Ei, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head (e)i, where head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes. For example, the hypered"
P08-1023,P07-1019,1,0.473411,"or each node v, and then compute the merit αβ(e) for each hyperedge: X β(ui ) (4) αβ(e) = α(head (e)) + ui ∈tails(e) e3 = h(NPB2,3 , NPB5,6 ), VP1,6 , “held x2 with x1 ”i. This procedure is summarized in Pseudocode 1. 3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005). For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed 196 Intuitively, this merit is the cost of the best derivation that traverses e, and the difference δ(e) = αβ(e) − β(TOP) can be seen as the distance away from the globally best derivation. We prune away a hyperedge e if δ(e) &gt; p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned. 4 Experiments We can extend the s"
P08-1023,2006.amta-papers.8,1,0.895534,"decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2"
P08-1023,P08-1067,1,0.711475,"languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26 ), and many subtrees are repeated across different parses (Huang, 2008). It is thus inefficient either to decode separately with each of these very similar trees. Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length. We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 1 There has been some confusion in the MT literature regarding the term forest: the word “forest” in “forest-to-string rules” 192 Proceedings of ACL-08: HLT, pages 192–199, c Columbus, Ohio, USA, June 2008. 2008 Association for Computati"
P08-1023,W01-1812,0,0.351606,"duce it to a reasonable size. The pruned parse forest will then be used to direct the translation. In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2). Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3). Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6 . Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair hV, Ei, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head (e)i, where head (e) ∈ V is the consequent node in the deductive step, and tails(e) ∈ V ∗ is the list of antecedent nodes."
P08-1023,N03-1017,0,0.0444445,"babilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Y P(t |Hp ) = P(ep ). (8) ep ∈Hp , ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 197 BLEU score on target translation. The derivation probability conditioned on 1-best tree"
P08-1023,koen-2004-pharaoh,0,0.0609331,"Missing"
P08-1023,C04-1090,0,0.192459,"ts over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free gram"
P08-1023,P06-1077,1,0.882144,"oints higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the la"
P08-1023,P07-1089,1,0.746035,"nguage string among all possible derivations D: ∗ d = arg max P(d|T ). d∈D À Æ (d) Bush (1) We will now proceed with a running example translating from Chinese to English: (2) r2 ⇓  &gt;L   B`ush´ı yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Bush with/and Sharon1 hold pass. talk2 “Bush held a talk2 with Sharon1 ” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (r1 ) IP(x1 :NPB x2 :VP) → x1 x2 (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept of packed forest. 193 hu`ıt´an r3 ⇓ held NPB with NN NR hu`ıt´an Sh¯al´ong r4 ⇓ (e) Bush NPB [held a talk]2 r5 ⇓ [with Sharon]1 Figure 2: An example derivation of tree-to-string translation. Shaded regions denote parts of the tree that is pattern-matched with the rule being applied. which results in two unfinished subtrees in (c). Then rule r2 grabs the B`ush´ı subtree and transliterate it (r2 ) NPB(NR(B`ush´ı)) → Bush. Similarly, rule r3 shown"
P08-1023,P03-1021,0,0.0947585,"bilities of translation rules r ∈ d: Y P(r) (6) P(d |Hp ) = 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 p=12 p=5 k=30 k=100 k=10 1-best k-best trees forests decoding 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results The BLEU score of the baseline 1-best decoding is 0.2325, wh"
P08-1023,P02-1040,0,0.104735,"where Hp is the parse forest, which decomposes into the product of probabilities of translation rules r ∈ d: Y P(r) (6) P(d |Hp ) = 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 p=12 p=5 k=30 k=100 k=10 1-best k-best trees forests decoding 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results"
P08-1023,W06-1608,0,0.0649947,"ne Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26 ),"
P08-1023,P05-1034,0,0.485947,"result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution"
P08-1023,J97-3002,0,0.0539771,"of exponentially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not re"
P08-1023,I05-1007,1,0.648768,"of rule r, respectively, P(t |s) and P(s |t) are the two translation probabilities, and Plex (·) are the lexical probabilities. The only extra term in forest-based decoding is P(t |Hp ) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Y P(t |Hp ) = P(ep ). (8) ep ∈Hp , ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Mo"
P08-1023,N06-1033,1,0.506875,"tems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic ‡ Department of Computer & Information Science University of Pennsylvania Levine Hall, 3330 Walnut Street Philadelphia, PA 19104, USA lhuang3@cis.upenn.edu time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is"
P09-1065,P06-1077,1,0.898463,"e word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings"
P09-1065,P07-1089,1,0.914806,"Missing"
P09-1065,D08-1076,0,0.100386,"Missing"
P09-1065,D08-1022,1,0.459626,"case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly lower than that of jo"
P09-1065,D08-1023,0,0.00900115,"j − i = l do for all m ∈ M do A DD(G, i, j, m) end for P RUNE(G, i, j) end for end for end procedure IP(x1 :VV, x2 :NN) → x1 x2 X → hfabiao, givei X → hyanjiang, a talki Figure 4: A derivation composed of both SCFG and tree-to-string rules. pairs and tree-to-string rules. Hierarchical phrase pairs are used for translating smaller units and tree-to-string rules for bigger ones. It is appealing to combine them in such a way because the hierarchical phrase-based model provides excellent rule coverage while the tree-to-string model offers linguistically motivated non-local reordering. Similarly, Blunsom and Osborne (2008) use both hierarchical phrase pairs and tree-to-string rules in decoding, where source parse trees serve as conditioning context rather than hard constraints. Depending on the target side output, we distinguish between string-targeted and tree-targeted models. String-targeted models include phrasebased, hierarchical phrase-based, and tree-tostring models. Tree-targeted models include string-to-tree and tree-to-tree models. All models can be combined at the translation level. Models that share with same target output structure can be further combined at the derivation level. The joint decoder u"
P09-1065,P08-1024,0,0.0833649,"ces of decisions that translate a source sentence into a target sentence step by step. For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. Such sequence of decisions is called a derivation. In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases. In syntax-based models, decisions usually correspond to transduction rules. Often, there are many derivations that are distinct yet produce the same translation. Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly. Given a source sentence f , the probability of a target sentence e being its translation is the sum over all possible derivations: ˆ = argmax e e P r(d, e|f ) = m λm hm (d, e, f ) Z(f ) ( X d∈∆(e,f ) ˆ ≈ argmax e e,d exp X ) λm hm (d, e, f ) m X m  λm hm (d, e, f ) (6) We refer to Eq. (5) as max-translation decoding and Eq. (6) as max-derivation decoding, which are first termed by Blunsom et al. (2008). By now, most current SMT systems, adopting either max-derivation decoding or max-t"
P09-1065,P05-1033,0,0.867394,"of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phas"
P09-1065,J07-2003,0,0.876253,"of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than p"
P09-1065,P08-1023,1,0.682905,"decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly"
P09-1065,P02-1038,0,0.474353,"and corresponding translation e conditioned on a source sentence f : P (4) where Z(f ) is not needed in decoding because it is independent of e. Most SMT systems approximate the summation over all possible derivations by using 1-best derivation for efficiency. They search for the 1best derivation and take its target yield as the best translation: d∈∆(e,f ) exp p(d) where d is a decision in the derivation d. Although originally proposed for supporting large sets of non-independent and overlapping features, the latent variable model is actually a more general form of conventional linear model (Och and Ney, 2002). Accordingly, decoding for the latent variable model can be formalized as 2 Background P r(d, e|f ) Y d∈d ing with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5). X (3) A feature value is usually decomposed as the product of decision probabilities: 2 h(d, e, f ) = P r(e|f ) = λm hm (d, e, f ) m e d∈∆(e,f ) Figure 1: A derivation composed of SCFG rules that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. X 3 Joint Decoding There are two major challenges for combining multiple mo"
P09-1065,J03-1002,0,0.00639286,"Missing"
P09-1065,J04-4002,0,0.208011,"ning further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576–584, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP where hm is a feature function, λm is the associated feature weight, and Z(f ) is a constant for normalization: S → hX1 , X1 i X → hfabiao X1 , give a X1 i X → hyanjiang, talki Z(f ) = X X exp Stati"
P09-1065,P03-1021,0,0.815341,"rtial translations (Section 3.2). Although such translation-level combination will not produce new translations, it does change the way of selecting promising candidates. • Two models could even share derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a seq"
P09-1065,A94-1016,0,0.256094,"derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in"
P09-1065,P06-1121,0,0.228974,"ion hypergraph produced by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a"
P09-1065,D08-1011,0,0.0936928,"are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly"
P09-1065,W99-0623,0,0.227171,"Missing"
P09-1065,W05-1506,0,0.6237,"g translation rule derivation model first parses f to obtain a source tree T (f ) and then transforms T (f ) to the target sentence e. Conversely, a string-to-tree model first parses f into a target tree T (e) and then takes the surface string e as the translation. Despite different inside, their derivations must begin with f and end with e. This situation remains the same for derivations between a source substring fij and its partial translation t during joint decoding: Table 1: Correspondence between translation hypergraph and decoding. More formally, a hypergraph (Klein and Manning., 2001; Huang and Chiang, 2005) is a tuple hV, E, Ri, where V is a set of nodes, E is a set of hyperedges, and R is a set of weights. For a given source sentence f = f1n = f1 . . . fn , each node v ∈ V is in the form of ht, [i, j]i, which denotes the recognition of t as one translation of the source substring spanning from i through j (that is, fi+1 . . . fj ). Each hyperedge e ∈ E is a tuple e = htails(e), head(e), w(e)i, where head(e) ∈ V is the consequent node in the deductive step, tails(e) ∈ V ∗ is the list of antecedent nodes, and w(e) is a weight function from R|tails(e) |to R. As a general representation, a translat"
P09-1065,P07-1019,0,0.0420047,"o surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of"
P09-1065,P08-1067,0,0.0234869,"ax-translation decoding still failed to surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the sy"
P09-1065,W01-1812,0,0.192643,"Missing"
P09-1065,P02-1040,0,0.106668,"ections while two lines have at most one intersection. Fortunately, as the curve is monotonically increasing, 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST 2005 test set as test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The 581 Model Combination hierarchical tree-to-string N/A N/A translation derivation both Max-derivation Time BLEU 40.53 30.11 6.13 27.23 N/A N/A 48.45 31.63 Max-translation Time BLEU 44.87 29.82 6.69 27.11 55.89 30.79 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). 5.2 percentage first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) a"
P09-1065,P07-1040,0,0.331325,"ultiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translati"
P09-1065,P08-1066,0,0.0477497,"ed by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a hypergraph denotes"
P09-1065,P05-1044,0,0.021838,"Missing"
P09-1065,P06-1098,0,0.0135744,"ensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a set of translation models M Translation-Level Combination The conventional interpretation of Eq. (1) is that the probability of a translation is the sum over all possible derivations coming from the same model"
P09-1065,J97-3002,0,0.0490287,"model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a"
P09-1065,P06-1066,1,0.502138,"rst model and the dashed lines denote those of the second model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The i"
P09-1065,N03-1017,0,\N,Missing
P09-2035,N03-1017,0,0.0252889,"Missing"
P09-2035,P06-1077,1,0.876446,"Missing"
P09-2035,P09-1063,1,0.78314,"Missing"
P09-2035,D08-1022,1,0.882756,"(Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especially for forest approaches. So splitting long sentences into subsentences becomes a natural way in MT literature. A simple way is to split long sentences by punctuations. However, without concerning about the original whole tree structures, this approach will result in ill-formed sub-trees which don’t respect to original structures. In this paper, we present a new approach, which pays more attention to parse trees on the long sentences. We firstly parse the long sentences into trees, and then divide them accordingly"
P09-2035,P03-1021,0,0.0109418,"on forest-based tree-tostring translation model (Mi et al. 2008). Our training corpus consists of 2.56 million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our results with case-sensitive BLEU-4 metric (Papineni et al., 2002). The pruning threshold p for parse forest in decoding time is 12. 5.2 Results The final BLEU results are shown in Table 2, our approach has achieved a BLEU score that is 1.1 higher than direct decoding and 0.3 higher than always splitting on commas. The decoding time results are presented in Table 3. The search space of our experiment is extremely large due to the large pruning threshold (p=12), thus resulting in a long decoding time. However, our approach has reduced the decoding ti"
P09-2035,P02-1040,0,0.0782487,"Missing"
P09-2035,P05-1034,0,0.145683,"Missing"
P09-2035,W05-1506,0,0.0668665,"Missing"
P09-2035,2006.amta-papers.8,0,\N,Missing
P09-2035,I05-1007,1,\N,Missing
P09-2035,P09-1020,0,\N,Missing
P09-2035,P08-1023,1,\N,Missing
P09-2035,P08-1064,0,\N,Missing
P09-2035,W06-1608,0,\N,Missing
P09-2035,W06-1606,0,\N,Missing
P09-2035,P07-1019,0,\N,Missing
P10-1145,P89-1018,0,0.329379,"translates source constituency forests into target dependency trees with a set of features (Section 4). Medium data experiments (Section 5) show a statistically significant improvement of +0.7 BLEU points over a stateof-the-art forest-based tree-to-string system even with less translation rules, this is also the first time that a tree-to-tree model can surpass tree-to-string counterparts. 2 2.1 Constituency Forests on the Source Side A constituency forest (in Figure 1 left) is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a contextfree grammar (Billot and Lang, 1989). More formally, following Huang (2008), such a constituency forest is a pair Fc = Gf = hV f , H f i, where V f is the set of nodes, and H f the set of hyperedges. For a given source sentence c1:m = c1 . . . cm , each node v f ∈ V f is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, ci+1 . . . cj ). Each hyperedge hf ∈ H f is a pair htails(hf ), head (hf )i, where head (hf ) ∈ V f is the consequent node in the deductive step, and tails(hf ) ∈ (V f )∗ is the list of antecedent nodes. For example, the hyperedge hf0"
P10-1145,A00-2018,0,0.163944,". In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentenc"
P10-1145,P05-1033,0,0.657891,"he head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,m and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. · P(el2 |el1 , eh §) the POS tag information on the target side for each constituency-to-dependency rule. So we will also generate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data spa"
P10-1145,J07-2003,0,0.790092,"or parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and depende"
P10-1145,P05-1066,0,0.0597057,"EU points if it is used), their forest-based constituency-to-constituency model achieves a BLEU score of 30.6, which is similar to Moses (Koehn et al., 2007). So our baseline system is much better than the BLEU score (30.6+1) of the constituency-to-constituency system and Moses. System forest c2s forest c2d Rule Set Type # c2s 31.9M s2s 77.9M c2d 13.8M s2d 9.0M c2d 13.8M s2s 77.9M c2d 13.8M s2d 9.0M c2d 13.8M s2s-dep 77.9M BLEU 34.17 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) 34.88(↑0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following t"
P10-1145,P05-1067,0,0.218116,"lize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (S"
P10-1145,W02-1039,0,0.230077,"atched at high level, which is more reasonable than using glue rules in Shen et al. (2008)’s scenario; finally, the most important one is that our model runs very faster. Liu et al. (2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage. By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules. Theoretically, we can extract more rules since dependency tree has the best inter-lingual phrasal cohesion properties (Fox, 2002). 7 Conclusion and Future Work In this paper, we presented a novel forest-based constituency-to-dependency translation model, which combines the advantages of both tree-tostring and string-to-tree systems, runs fast and guarantees grammaticality of the output. To learn the constituency-to-dependency translation rules, we first identify the frontier set for all the nodes in the constituency forest on the source side. Then we fragment them and extract minimal rules. Finally, we glue them together to be composed rules. At the decoding step, we first parse the input sentence into a constituency fo"
P10-1145,N04-1035,0,0.452953,"and vkd ∈ / M, ∀hd if tails(hd ) = vkd ⇒ head (hd ) ∈ ei:j . Take the “ (Bush) held ((a) talk))(with (Sharon)) ” for example: partial fixed examples are “ (Bush) held ” and “ held ((a) talk)”; while the partial floating examples are “ (talk) (with (Sharon)) ” and “ ((a) talk) (with (Sharon)) ”. Please note that the floating structure “ (talk) (with (Sharon)) ” can not be allowed in Shen et al. (2008)’s model. The dependency structure “ held ((a))” is not a well formed structure, since the head of word “a” is out of scope of this structure. (2) fragmentation, (3) composition. The frontier set (Galley et al., 2004) is the potential points to “cut” the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al., 2006). However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures. So we say a fragment is extractable if it respects to all restrictions. The root node of every extractable tree fragment corresponds to a faithful structure on the target side, in which case there is a “translational equivalence” between the subtree rooted at the node and th"
P10-1145,P06-1121,0,0.875619,"near time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-"
P10-1145,W05-1506,0,0.117175,":NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,m and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. · P(el2 |el1 , eh §) the POS tag information on the target side for each constituency-to-dependency rule. So we will also generate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data spa"
P10-1145,P07-1019,0,0.234129,"y Shen et al. (2008). For each nonterminal node vhd = eh in De and its children sequences Ll = el1 , el2 ...eli and Lr = er1 , er2 ...erj , the probability of a trigram is computed as follows: P(Ll , Lr |eh §) = P(Ll |eh §) · P(Lr |eh §), (9) where the P(Ll |eh §) is decomposed to be: P(Ll |eh §) =P(ell |eh §) ... 5 Experiments 5.1 Data Preparation since the x3 :NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,m and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. · P(el2 |el1 , eh §) the POS tag information on the target side for each constituency-to-dependency rule. So we will"
P10-1145,2006.amta-papers.8,0,0.130613,"ed better than phrasebased counterparts in reorderings. Depending on the type of input, these models can be broadly divided into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee gram"
P10-1145,J09-4009,0,0.0296507,"account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art fo"
P10-1145,P08-1067,0,0.118665,"o two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-"
P10-1145,N03-1017,0,0.155896,"urce forest Fc , the decoder searches for the best derivation o∗ among the set of all possible derivations O, each of which forms a source side constituent tree Tc (o), a target side string e(o), and where each P(r) is the product of five probabilities: P(r) =P(r |lhs(r))λ9 · P(r |rhs(r))λ10 · P(r |root(lhs(r)))λ11 · Plex (lhs(r) |rhs(r))λ12 (8) · Plex (rhs(r) |lhs(r))λ13 , where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.4, and the last two are lexical probabilities. When computing the lexical translation probabilities described in (Koehn et al., 2003), we only take into accout the terminals in a rule. If there is no terminal, we set the lexical probability to 1. The decoding algorithm works in a bottom-up search fashion by traversing each node in forest Fc . We first use pattern-matching algorithm of Mi et al. (2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule. However, pattern-matching failure2 at a node v f will Pattern-matching failure at a node v f means there is no translation rule can be matched at v f or no translation hyperedge can be constructed"
P10-1145,P07-2045,0,0.0461637,"onstituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into target dependency trees with a set of features (Section 4). Medium data experiments (Section 5) show a statistically significant improvement of +0.7 BLEU points over a stateof-the-art forest-based tree-to-string system even with less translation rules, this is also the first time that a tree-to-tree model can surpass tree-to-string counter"
P10-1145,P06-1077,1,0.915915,"t, they are believed better than phrasebased counterparts in reorderings. Depending on the type of input, these models can be broadly divided into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechan"
P10-1145,P07-1089,1,0.806526,"pt of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on th"
P10-1145,P09-1063,1,0.787813,"Missing"
P10-1145,P08-1066,0,0.646018,"ry-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel cons"
P10-1145,I05-1007,1,0.747477,"ch ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to tr"
P10-1145,W07-0706,1,0.884932,"e source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constit"
P10-1145,N06-1033,0,0.0319615,"n we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over th"
P10-1145,2007.mtsummit-papers.71,0,0.459482,"us combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into t"
P10-1145,P09-1020,0,0.505547,"the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the gramm"
P10-1145,P95-1037,0,0.506184,"Mi et al. (2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule. However, pattern-matching failure2 at a node v f will Pattern-matching failure at a node v f means there is no translation rule can be matched at v f or no translation hyperedge can be constructed at v f . 1438 2 cut the derivation path and lead to translation failure. To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hf ∈ IN (v f ) by mapping the CFG rule into a target dependency tree using the head rules of Magerman (1995). Take the hyperedge hf0 in Figure1 for example, the corresponding pseudo translation rule is: NP(x1 :NPB x2 :CC x3 :NPB) → (x1 ) (x2 ) x3 , 4.1 Dependency Language Model Computing We compute the score of a dependency language model for a dependency tree De in the same way proposed by Shen et al. (2008). For each nonterminal node vhd = eh in De and its children sequences Ll = el1 , el2 ...eli and Lr = er1 , er2 ...erj , the probability of a trigram is computed as follows: P(Ll , Lr |eh §) = P(Ll |eh §) · P(Lr |eh §), (9) where the P(Ll |eh §) is decomposed to be: P(Ll |eh §) =P(ell |eh §) ..."
P10-1145,W06-1606,0,0.0341931,"ime, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 As"
P10-1145,D08-1022,1,0.634082,"ded into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-"
P10-1145,P08-1023,1,0.779093,"n be broadly divided into two categories (see Table 1): the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the ot"
P10-1145,P00-1056,0,0.144015,"erate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei ). (10) · P(eln |eln−1 , eln−2 ). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately."
P10-1145,P03-1021,0,0.0525495,"oothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. 5.2 Results Table 2 shows the results on the test set. Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest c2s for short, which translates a source forest into a target string by pattern-matching the 1439 constituency-to-string (c2s) rules and the bilingual phrases (s2s). The baseline system extracts 31.9M c2s rules, 77.9M s2s rules respectively and achieves a BLEU score of 34.17 on the test set3 . At first, we investigate the influence of different"
P10-1145,P02-1040,0,0.0858967,"parately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. 5.2 Results Table 2 shows the results on the test set. Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest c2s for short, which translates a source forest into a target string by pattern-matching the 1439 constituency-to-string (c2s) rules and the bilingual phrases (s2s)."
P10-1145,P05-1034,0,0.168882,"ic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and trans"
P10-1145,P08-1064,0,\N,Missing
P10-2003,P05-1022,0,0.10367,"Missing"
P10-2003,P08-1067,0,0.0732652,"Missing"
P10-2003,N03-1017,0,0.0461596,"ations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering mod"
P10-2003,P07-2045,0,0.162702,"ion evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bi"
P10-2003,W04-3250,0,0.0231813,"e used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. However,"
P10-2003,J03-1002,0,0.00561231,"inuous. If a msd-bidirectional-fe model is used, then the number of features doubles: one for each direction. 3.1 Experiment Setup Two different sizes of training corpora are used in our experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC. The 2002 NIST MT evaluation test data is used as the development set and the 2003, 2004, 2005 NIST MT test data are the test sets. We choose the MOSES1 (Koehn et al., 2007) as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Fin"
P10-2003,J04-4002,0,0.0483088,"pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advant"
P10-2003,P03-1021,0,0.210537,"cally, the model collects bilingual phrases and distinguishes their orientations with respect to the previous bilingual phrase into three categories:   M o= S   D ai − ai−1 = 1 ai − ai−1 = −1 |ai − ai−1 |6= 1 (1) Using the relative-frequency approach, the reordering probability regarding bp is Count(o, bp) 0 o0 Count(o , bp) p(o|bp) = P (2) 2.2 Reordering Graph For a parallel sentence pair, its reordering graph indicates all possible translation derivations consisting of the extracted bilingual phrases. To construct a reordering graph, we first extract bilingual phrases using the way of (Och, 2003). Then, the adjacent bilingual phrases are linked according to the targetside order. Some bilingual phrases, which have no adjacent bilingual phrases because of maximum length limitation, are linked to the nearest bilingual phrases in the target-side order. Shown in Figure 2(b), the reordering graph for the parallel sentence pair (Figure 2(a)) can be represented as an undirected graph, where each rectangle corresponds to a phrase pair, each link is the orientation relationship between adjacent bilingual phrases, and two distinguished rectangles bs and be indicate the beginning and ending of th"
P10-2003,P02-1040,0,0.082129,"GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (T"
P10-2003,N04-4026,0,0.371205,"vered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model"
P10-2003,P06-1066,1,0.837358,"n performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) de"
P10-2003,W06-3108,0,0.02101,"ent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presenc"
P10-2003,D08-1089,0,\N,Missing
P11-1086,E03-1005,0,0.0230277,"scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have investigated whether we can eliminate composed rules without any loss in translation quality. We have developed a rule Markov model that captures vertical bigrams and trigrams of minimal rules, and tested it in the framework of treeto-string translation. We draw three main conclusions from our experiments. First, our rule Markov models dramatically improve a grammar of minimal rules, giving an improvement of 2.3 Bleu. Seco"
P11-1086,P05-1067,0,0.0482976,"us settings.2 In the second line (2.9 million rules), the drop in Bleu score resulting from adding the rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2 For these experiments, a beam size of 100 was used. 863 efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and g"
P11-1086,W08-0306,0,0.0127101,"V is the target-language vocabulary, and g is the order of the n-gram language model (Huang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style 861 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained usin"
P11-1086,N04-1035,0,0.687748,"ause a combinatorial explosion in the number of rules. To avoid this, ad-hoc limits are placed during composition, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk"
P11-1086,P06-1121,0,0.121012,"n, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study"
P11-1086,D10-1027,1,0.903437,"rts at step 1, where we predict rule r1 (the shaded rule) with probability P(r1 |ǫ) and push its English side onto the stack, with variables replaced by the corresponding tree nodes: x1 becomes NP@1 and x2 becomes VP@2 . This gives us the following stack: In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history s = [ NP@1 VP@2 ] is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This The dot () indicates the next symbol to process in 859 stack hyp. 0 [<s>  IP@ǫ 1 [<s>  IP </s> 2 [<s>  IP@ǫ </s> 3 [<s>  IP@ǫ </s> 4 [<s>  IP@ǫ </s> 5 [<s>  IP@ǫ </s> 6 [<s>  IP@ǫ </s> 7 [<s>  IP@ǫ </s> @1 @2 <s> P(r1 |ǫ) ] [ NP@1 VP@2 ] [ Bush] <s> P(r"
P11-1086,2006.amta-papers.8,1,0.944897,"is above another threshold. 3 Tree-to-string decoding with rule Markov models IP@ǫ VP@2 NP@1 B`ush´ı PP@2.1 VP@2.2 P@2.1.1 NP@2.1.2 VV@2.2.1 AS@2.2.2 NP@2.2.3 yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Figure 3: Example input parse tree with tree addresses. makes incremental decoding a natural fit with our generative story. In this section, we describe how to integrate our rule Markov model into this incremental decoding algorithm. Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al., 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. Algorithm Given the input parse tree in Figure 3, Figure 4 illustrates the search process of the incremental decoder with the grammar of Figure 1. We write X @η for a tree node with label X at tree address η (Shieber et al., 1995). The root node has address ǫ, and the ith child of node η has address η.i. At each step, the decoder maintains a stack of active rules, which are rules that have not been completed yet, and the rightmost"
P11-1086,J98-4004,0,0.0855515,"el in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have"
P11-1086,W04-3250,0,0.135584,".3+0.5 4.9+7.6 0.3+0.6 176.8 1.3 17.5 1.6 448.7 3.3 448.7+7.6 3.3+1.0 Bleu test 24.2 25.7 26.5 26.5 26.4 27.5 28.0 time (sec/sent) 1.2 1.8 2.0 2.9 2.2 6.8 9.2 Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for both the full model and the model filtered for the concatenation of the development and test sets (dev+test). These gains are statistically significant with p < 0.01, using bootstrap resampling with 1000 samples (Koehn, 2004). We find that by just using bigram context, we are able to get at least 1 Bleu point higher than the minimal rule grammar. It is interesting to see that using just bigram rule interactions can give us a reasonable boost. We get our highest gains from using trigram context where our best performing rule Markov model gives us 2.3 Bleu points over minimal rules. This suggests that using longer contexts helps the decoder to find better translations. We also compared rule Markov models against composed rules. Since our models are currently limited to conditioning on vertical context, the closest c"
P11-1086,W08-0308,0,0.0492019,"he rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2 For these experiments, a beam size of 100 was used. 863 efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which i"
P11-1086,P06-1077,0,0.337664,"f the decoder. The last column in the figure shows the rule Markov model probabilities with the conditioning context. In this example, we use a trigram rule Markov model. After initialization, the process starts at step 1, where we predict rule r1 (the shaded rule) with probability P(r1 |ǫ) and push its English side onto the stack, with variables replaced by the corresponding tree nodes: x1 becomes NP@1 and x2 becomes VP@2 . This gives us the following stack: In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history s = [ NP@1 VP@2 ] is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This The dot () indicates the next symbol to process in 8"
P11-1086,P08-1023,1,0.888401,"es). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baseline"
P11-1086,P03-1021,0,0.0444013,"entences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baselines. For decoding, we used a beam size of 50. Using the best bigram rule Markov models and the minimal rule grammar gives us a"
P11-1086,N07-1051,0,0.0208804,"uang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style 861 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development se"
P11-1086,N06-1002,0,0.516674,"2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study leaves unanswered whether a rule Markov model can take the place of composed rules. In this work, we investigate the use of rule Markov models in the context of treeProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856–864, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics to-string translation (Liu et al., 2006; Huang et al., 2006). We make three new contributions. First, we carry out a detailed comparison of rule Markov m"
P14-2127,N12-1015,1,0.95011,"r ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (H IERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT"
P14-2127,2007.mtsummit-papers.3,0,0.366235,"extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous a"
P14-2127,P08-1067,1,0.885443,"Missing"
P14-2127,P08-1024,0,0.059643,"tent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and"
P14-2127,P07-2045,0,0.0114073,"Missing"
P14-2127,P05-1022,0,0.167409,"Missing"
P14-2127,P09-1019,0,0.238999,"Missing"
P14-2127,D08-1024,0,0.312341,"Missing"
P14-2127,P13-1008,1,0.823747,"latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that as"
P14-2127,P05-1033,0,0.0899251,"such as M ERT and P RO. 1 ‡ Abe Ittycheriah‡ ↓ hypergraph Zhang et al. (13) latent −→ variable −→ variable Yu et al. (13) ↓ this work Figure 1: Relationship with previous work. small-scale M ERT/P RO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (H IERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural langua"
P14-2127,P06-1096,0,0.146431,"Missing"
P14-2127,W02-1001,0,0.0903093,"e capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: i"
P14-2127,P05-1012,0,0.0712727,"d MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitab"
P14-2127,P10-4002,0,0.078474,"for each such index.2 We have a similar deductive system for forced decoding. For the previous example, rule r5 in Figure 2 (a) is rewritten as X → hyˇu X 1 jˇux´ıng X 2 , 1 X 2 4 X 1 i, where 1 and 4 are the indexes for reference words “held” and “with” respectively. The deduction for X[1:5] in Figure 2 (b) is X5?5 [2:3] : s1 X2?3 [4:5] : s2 1?5 X[1:5] : s(r5 ) + λ + s1 + s2 where λ = log w · Φ(x, d), 4 Q i∈{1,3,4} Pforced (i r5 , + 1 |i) = 0. Experiments Following Yu et al. (2013), we call our maxviolation method M AX F ORCE. Our implementation is mostly in Python on top of the cdec system (Dyer et al., 2010) via the pycdec interface (Chahuneau et al., 2012). In addition, we use minibatch parallelization of (Zhao and Huang, w · Φ(x, d), where Φ(x, d) is the feature vector for derivation d. Then it finds the group N[i∗∗ :j ∗ ] with the maximal score difference between the Viterbi derivation and the best y-good derivation: 1 We only consider single reference in this paper. Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a substring appears more than once in the reference (e.g. “the man...the man...”); thanks to Dan Gildea for pointing it out. ∆"
P14-2127,P03-1021,0,0.191299,"Missing"
P14-2127,D07-1080,0,0.256905,"e just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, whi"
P14-2127,N13-1025,0,0.281361,"ibutions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottomup parsing as special cases (see Fig."
P14-2127,D13-1112,1,0.891392,"c}.cuny.edu T. J. Watson Research Center IBM {hmi,abei}@us.ibm.com inexact Abstract Collins (02) −→ Huang et al. (12) search Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT (Yu et al., 2013) significantly outperforms mainstream methods that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable “violation-fixing” perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 B LEU improvement over mainstream methods such as M ERT and P RO. 1 ‡ Abe Ittycheriah‡ ↓ hypergraph Zhang et al. (13) latent −→ variable −→ variable Yu et al. (13) ↓ this work Figure 1: Relationship with previous work. small-scale M ERT/P RO by a large margin for the first time). However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experi"
P14-2127,P13-1031,0,0.044397,"Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets. What makes large-scale MT training so hard then? After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning. To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming 1. We generalize the latent variable violationfixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottomup parsing as special cases (see Fig. 1). 2. We show that"
P14-2127,D13-1093,1,0.94016,"suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training. To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (H IERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT. The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs. Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding. We just need to further extend it to handle latent variables. We make the following contributions: Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms. However, a similar success on machine translation has been el"
P14-2127,D11-1125,0,0.16939,"Missing"
P14-2127,N13-1038,1,0.898412,"Missing"
P14-2127,P07-1019,1,0.875942,"anslation (i.e., spurious ambiguity), Yu et al. (2013) extends it to handle latent variables which correspond to phrase-based derivations. On the other hand, Zhang et al. (2013) has generalized Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles H IERO decoding. So we just need to combine the two generalizing directions (latent variable and hypergraph, see Fig. 1). Review: Syntax-based MT Decoding For clarity reasons we will describe H IERO decoding as a two-pass process, first without a language model, and then integrating the LM. This section mostly follows Huang and Chiang (2007). In the first, −LM phase, the decoder parses the source sentence using the source projection of the synchronous grammar (see Fig. 2 (a) for an example), producing a −LM hypergraph where each node has a signature N[i:j] , where N is the nonterminal type (either X or S in H IERO) and [i : j] is the span, and each hyperedge e is an application of the translation rule r(e) (see Figure 3). To incorporate the language model, each node also needs to remember its target side boundary words. Thus a −LM node N[i:j] is split into mula?b , where a and tiple +LM nodes of signature N[i:j] b are the boundar"
P15-1110,C14-1076,0,0.0522234,"Missing"
P15-1110,P05-1022,0,0.0395646,"Missing"
P15-1110,A00-2018,0,0.0909756,"entences that can be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an eff"
P15-1110,D14-1082,0,0.145268,"Missing"
P15-1110,E03-1002,0,0.330686,"but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Con1138 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1138–1147, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics DT NN VP MD That debt would VB be V"
P15-1110,P04-1013,0,0.0531726,"Missing"
P15-1110,D10-1002,0,0.0703841,"Missing"
P15-1110,P08-1067,0,0.0590737,"Missing"
P15-1110,P14-1130,0,0.0385871,"the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Co"
P15-1110,J93-2004,0,0.0509943,"ining has demonstrated its effectiveness as a way of initializing neural network models (Erhan et al., 2010). Since our model requires many run-time primitive units (POS tags and constituent labels), we employ an in-house shift-reduce parser to parse a large amount of unlabeled sentences, and pre-train the model with the automatically parsed data. Third, we utilize the Dropout strategy to address the overfitting probExperimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005) and the Wall Street Journal (WSJ) portion of Penn English Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 301-325 as the development set, and Articles 271300 as the testing set. For English, we use sections 2-21 for training, section 22 for developing and section 23 for testing. We also utilized some unlabeled corpora and used the word2vec2 toolkit to train word embeddings. For Chinese, we used the unlabeled Chinese Gigaword (LDC2003T09) and performed Chinese word segmentation using our in-house segmenter. For English, we randomly selected"
P15-1110,N06-1020,0,0.0318095,"Missing"
P15-1110,C10-1093,0,0.0603562,"Missing"
P15-1110,N07-1051,0,0.0498802,"t conversation (bc), weblogs (wb) and discussion forums (df). Since all of the mz domain data is already included in our training set, we only selected sample sentences from the other five domains as the test sets 5 , and made sure these test sets had no overlap with our treebank training, development and test sets. Note that we did not use any data from these five domains for training or development. The models are still the ones described in the previous subsection. The results are presented in Table 7. Although our “Supervised” model got slightly worse performance than the Berkeley Parser (Petrov and Klein, 2007), as shown in Table 5, it outperformed the Berkeley Parser on the cross-domain data sets. This suggests that the learned features can better adapt to cross-domain situations. Compared with the Berkeley Parser, on average our “Pretrain-Finetune” model is 3.4 percentage points better in terms of parsing accuracy, and 3.2 percentage points better in terms of POS tagging accuracy. We also presented the performance of our pre-trained model (“Only-Pretrain”). We found the “Only-Pretrain” model performs poorly on this cross-domain data sets. But even pretraining based on this less than competitive mo"
P15-1110,P06-1055,0,0.0172632,"n be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features"
P15-1110,W96-0213,0,0.123723,"l we cannot say for sure that this is the optimal subset of features for the parsing task. To cope with this problem, we propose to simultaneously optimize feature representation and parsing accuracy via a neural network model. Figure 2 illustrates the architecture of our model. Our model consists of input, projection, hidden and output layers. First, in the input layer, all primitive units (shown in Table 1(a)) are imported to the network. We also import the suffixes and prefixes of the first word in the queue, because these units have been shown to be very effective for predicting POS tags (Ratnaparkhi, 1996). Then, in the projection layer, each primitive unit is projected into a vector. Specifically, word-type units are represented as word embeddings, and other units are transformed into one-hot representations. The p0 w, p0 t,p0 c, p1 w, p1 t,p1 c, p2 w, p2 t,p2 c, p3 w, p3 t,p3 c p0l w, p0l c, p0r w, p0r c,p0u w, p0u c, p1l w, p1l c, p1r w, p1r c,p1u w, p1u c q0 w, q1 w, q2 w, q3 w trigrams p0 tc, p0 wc, p1 tc, p1 wc, p2 tc p2 wc, p3 tc, p3 wc, q0 wt, q1 wt q2 wt, q3 wt, p0l wc, p0r wc p0u wc, p1l wc, p1r wc, p1u wc p0 wp1 w, p0 wp1 c, p0 cp1 w, p0 cp1 c p0 wq0 w, p0 wq0 t, p0 cq0 w, p0 cq0 t q"
P15-1110,W05-1513,0,0.0851226,"egies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had"
P15-1110,W09-3825,0,0.392801,"nd more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the t"
P15-1110,P13-1043,0,0.4563,"ithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after th"
P15-1110,P13-1045,0,0.111934,"Missing"
P15-1110,P14-1069,1,0.936891,"tational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking proces"
P15-1110,I11-1140,1,0.901748,"Missing"
P15-1110,P06-1054,0,0.0708317,"rence algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the lea"
P15-1110,W08-2102,0,\N,Missing
P15-1110,J03-4003,0,\N,Missing
P16-2021,P07-2045,0,0.00793318,"Missing"
P16-2021,W14-3309,0,0.0574079,"Missing"
P16-2021,N13-1073,0,0.0275834,"common words for VxT . Then we check various top n (10, 20, and 50) candidates for the word-to-word dictionary for VxD . train dev. sentence mini-batch sentence Jean (2015) 30k 30k 30k Ours 2080 6153 2067 has 6003 sentences in total. Our test set has 3003 sentences from WMT news-test 2014. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002) with the multi-bleu.perl script. Same as Jean et al. (2015), our full vocabulary size is 500k, we use AdaDelta (Zeiler, 2012), and mini-batch size is 80. Given the training set, we first run the ‘fast align’ (Dyer et al., 2013) in one direction, and use the translation table as our word-to-word dictionary. Then we run the reverse direction and apply ‘grow-diag-final-and’ heuristics to get the alignment. The phrase table is extracted with a standard algorithm in Moses (Koehn et al., 2007). In the decoding procedure, our method is very similar to the ‘candidate list’ of Jean et al. (2015), except that we also use bilingual phrases and we only include top 2k most frequent target words. Following Jean et al. (2015), we dump the alignments for each sentence, and replace UNKs with the word-to-word dictionary or the source"
P16-2021,P02-1040,0,0.102533,"D ∪ VxT 10 20 50 92.7 94.2 96.2 91.7 92.7 94.3 Table 1: The average reference coverage ratios (in word-level) on the training and development sets. We use fixed top 10 candidates for each phrase when generating VxP , and top 2k most common words for VxT . Then we check various top n (10, 20, and 50) candidates for the word-to-word dictionary for VxD . train dev. sentence mini-batch sentence Jean (2015) 30k 30k 30k Ours 2080 6153 2067 has 6003 sentences in total. Our test set has 3003 sentences from WMT news-test 2014. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002) with the multi-bleu.perl script. Same as Jean et al. (2015), our full vocabulary size is 500k, we use AdaDelta (Zeiler, 2012), and mini-batch size is 80. Given the training set, we first run the ‘fast align’ (Dyer et al., 2013) in one direction, and use the translation table as our word-to-word dictionary. Then we run the reverse direction and apply ‘grow-diag-final-and’ heuristics to get the alignment. The phrase table is extracted with a standard algorithm in Moses (Koehn et al., 2007). In the decoding procedure, our method is very similar to the ‘candidate list’ of Jean et al. (2015), exce"
P16-2021,P15-1001,0,0.797784,"ts sentencelevel or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordto-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015). 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) has gained popularity in recent two years. But it can only handle a small vocabulary size due to the computational complexity. In order to capture rich language phenomena and have a better word coverage, neural machine translation models have to use a large vocabulary. Jean et al. (2015) alleviated the large vocabulary issue by proposing an approach that partitions the training corpus and defines a subset of the full target vocabulary for each partition. Thus, they only use a subset vocabulary for each partition in the t"
P16-2021,D14-1179,0,\N,Missing
S16-2009,N09-1004,0,0.0251889,"ense Embedding Learning for Word Sense Induction 1 Linfeng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Ti"
S16-2009,D14-1110,0,0.0217553,"06; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its"
S16-2009,C14-1123,0,0.0578541,"w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (cluster"
S16-2009,N15-1070,0,0.0201635,"with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each ins"
S16-2009,P14-1023,0,0.236732,"rd sense disambiguation (WSD) assumes there exists an already-known sense inventory, and the sense of a word type is disambiguated according to the sense inventory. Therefore, clustering methods are generally applied in WSI tasks, while classification methods 85 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 85–90, Berlin, Germany, August 11-12, 2016. ods separately train a specific VSM for each word. No methods have shown distributional vectors can keep knowledge for multiple words while showing competitive performance. tributional models (Baroni et al., 2014), and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding represent"
S16-2009,S10-1079,0,0.0264613,"raditional methods for WSI tasks, the advantages of our method include: 1) WSI models for all the polysemous words are trained jointly under the multi-task learning framework; 2) distributed sense embeddings are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models (Baroni et al., 2014). To verify the two statements, we carefully designed comparative experiments described in the next section. 3 3.1 3.2 Comparing on SemEval-2010 We compare our methods with the following systems: (1) UoY (Korkontzelos and Manandhar, 2010) which is the best system in the SemEval2010 WSI competition; (2) NMFlib (Van de Cruys and Apidianaki, 2011) which adopts non-negative matrix factorization to factor a matrix and then conducts word sense clustering on the test set; (3) NB (Choe and Charniak, 2013) which adopts naive Bayes with the generative story that a context is generated by picking a sense and then all context words given the sense; and (4) Spectral (Goyal and Hovy, 2014) which applies spectral clustering on a set of distributional context vectors. Experimental results are shown in Table 1. Let us see the results on superv"
S16-2009,E06-1018,0,0.0790095,"Missing"
S16-2009,W15-1504,0,0.0378227,"Missing"
S16-2009,E09-1013,0,0.0267862,"where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have"
S16-2009,E12-1060,0,0.0146655,"centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group simi"
S16-2009,N10-1013,0,0.196952,"-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,D15-1200,0,0.170573,"lity distribution among all the senses for each instance, which can be seen as soft clustering algorithms. As for knowledge representation, existing WSI methods use the vector space model (VSM) to represent each context. In feature-based models, each instance is represented as a vector of values, where a value can be the count of a feature or the co-occurrence between two words. In Bayesian methods, the vectors are represented as co-occurrences between documents and senses or between senses and words. Overall existing methst = arg max sim(µ(wt , k), vc ) k=1,..,K (1) Another group of methods (Li and Jurafsky, 2015) employs non-parametric algorithms to dynamically decide the number of senses for each word, and each instance is assigned to a sense following a probability distribution in Equation 2, where St is the set of already generated senses for wt , and γ is a constant probability for generating a new sense for wt . ( p(k|µ(wt , k), vc ) ∀ k ∈ St st ∼ γ for new sense (2) From the above discussions, we can obviously notice that WSI task and sense embedding task are inter-related. The two factors in sense embedding learning can be aligned to the two factors of WSI task. Concretely, deciding the number"
S16-2009,P15-1173,0,0.0663194,"Missing"
S16-2009,S10-1011,0,0.223826,"Missing"
S16-2009,D10-1012,0,0.0843825,"Missing"
S16-2009,C14-1016,0,0.0236297,"09; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its sense by finding th"
S16-2009,D14-1113,0,0.547899,"itask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding representation for each sense. To decide the number of senses in factor (1), one group of methods (Huang et al., 2012; Neelakantan et al., 2014) set a fixed number K of senses for each word, and each instance is assigned to the most probable sense according to Equation 1, where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each insta"
S16-2009,P11-1148,0,0.044133,"Missing"
S16-2009,S10-1081,0,0.0243244,"than CRP-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,Q15-1005,0,0.0201342,"sentation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (clustering algorithm) and 2"
S16-2009,D14-1162,0,0.0881563,"sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for"
S16-2009,P14-1137,0,0.0424369,"Missing"
S16-2009,W04-2406,0,0.0636251,"feng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding m"
S16-2009,W11-1102,0,0.0490206,"Missing"
S16-2009,P12-1092,0,\N,Missing
S16-2009,D13-1148,0,\N,Missing
Y10-1009,P08-1023,1,0.829903,"Missing"
Y10-1009,D08-1022,1,\N,Missing
Y10-1009,P07-1089,1,\N,Missing
Y10-1009,P06-1077,1,\N,Missing
Y10-1009,P09-1063,1,\N,Missing
Y10-1009,D08-1010,1,\N,Missing
Y10-1009,C08-1041,1,\N,Missing
Y10-1009,C10-1080,1,\N,Missing
