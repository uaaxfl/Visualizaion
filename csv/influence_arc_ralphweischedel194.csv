2020.emnlp-main.351,W18-1505,1,0.774396,"tween the three systems, as it will give the plot more influence over the story surface realisation and ensure that plot improvements appear downstream. 7 Related Work Story Generation without Plots. Diverse efforts have focused on generating stories. Fan et al. (2018) re-purpose an approach for Neural Machine Translation to translate from prompt to a story via Convolutional Seq2Seq models. Guan et al. (2020); Mao et al. (2019) use a similar approach, however they incorporate structured commonsense knowledge from external datasets or knowledge bases to improve a story generated from a prompt. Peng et al. (2018) add control to the story ending valence. Story Generation with Plots. Riedl and Young (2010) use refinement search as a technique to balance between character and plot for solving the narrative generation problem. Li et al. (2013) use plot graphs for story generation that model the intended logical flow of events in the virtual world as a set of precedence constraints between plot events. Martin et al. (2018) decompose the problem of story generation into generation of successive events (event2event) followed by generation of natural language sentences from events (event2sentence). Ammanabrol"
2020.emnlp-main.351,K19-1079,0,0.0339709,"t content plan, or that plan in an unprincipled way.1 1. 2. 3. 4. event choice and arrangement character relevant content2 diction Despite many recent advances in Natural Language Generation, successful creative narrative composition remains elusive. Current neural approaches are plagued by difficulty in mastering structure, will veer between topics, and lack long-range cohesion. They successfully imitate the fluency and style of human writing, but on closer inspection sentences do not fit together to form a whole, and the reader is left with the impression that the generation has no content (See et al., 2019). This lack of structure also degrades the relevance of generations conditioned on a prompt or other source text - a strong language model will repeat key phrases from a given prompt but will not remain on topic. These issues are illustrated in the Naive Generated Story in Table 1, where many of the sentences individually are fine, but do not fit together as one story, and do not all relate to the prompt. An amateur masters skills later in the list, but mastery of event choice and event arrangement is what distinguishes a good writer (Aristotle). Next is character, then relevance, and only fin"
2020.findings-emnlp.273,D14-1181,0,0.0210765,"computer model, considering whether to fry using a fridge is no more ludicrous than considering whether to fry using a plate (which, to an untrained human cook, may be plausible, though is certainly not a good idea). Both actions can be discouraged by negative reinforcement, but a human only needs to learn not to do the latter. Furthermore, a computer player learning that one can chop carrots with a knife may not generalize that one can chop celery the same way, but a human surely will. There is existing work in learning to play text games with RL (Narasimhan et al., 2015; Yuan et al., 2018; Kim, 2014; Zahavy et al., 2018; Yin and May, 2019a; Tessler et al., 2019) but the standard pattern of incorporating large language models such as BERT (Devlin et al., 2019) has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use BERT and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth (Zellers et al., 2018; Wang et al., 2018) or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic (Wolf et al., 2019; Imamura and Sum"
2020.findings-emnlp.273,W02-0109,0,0.345954,"ing P2: [slice potato with knife |drop knife |drink water] Figure 2: The architecture of the DRRN model. Trajectories and actions are encoded by a CNN (in this case) and an LSTM into state and action representations, respectively, followed by a dense layer to compute the Q-values. On the bottom, we show a truncated example of dialogue from a text game in the cooking genre, with S1 and S2 representing the system’s descriptions, and P1 showing the player’s first actions in response to S1. S1 + P1 + S2 is an example of a trajectory. P2 shows a set of admissible actions. word tokenizer from NLTK (Loper and Bird, 2002) since GloVe embeddings are pre-determined and not compatible with BPE. We use a zero vector as the padding token and average of all word embeddings as the unknown token for CNN-GloVe. CNN uses a word embedding size of 64, while for CNNGloVe and BERT, we use the pre-trained word embedding size, i.e., 50 dimensions for CNN-GloVe (we choose this dimension because it is close to our CNN) and 768 for BERT (so does Transformer). 3.2 Action Representations A consequence of learning to play different games is that actions differ from one game to another. Vanilla DQNs, introduced by (Mnih et al., 2015"
2020.findings-emnlp.273,D15-1001,0,0.622749,"zation caused by a lack of commonsense. To a computer model, considering whether to fry using a fridge is no more ludicrous than considering whether to fry using a plate (which, to an untrained human cook, may be plausible, though is certainly not a good idea). Both actions can be discouraged by negative reinforcement, but a human only needs to learn not to do the latter. Furthermore, a computer player learning that one can chop carrots with a knife may not generalize that one can chop celery the same way, but a human surely will. There is existing work in learning to play text games with RL (Narasimhan et al., 2015; Yuan et al., 2018; Kim, 2014; Zahavy et al., 2018; Yin and May, 2019a; Tessler et al., 2019) but the standard pattern of incorporating large language models such as BERT (Devlin et al., 2019) has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use BERT and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth (Zellers et al., 2018; Wang et al., 2018) or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic (Wolf"
2020.findings-emnlp.273,D14-1162,0,0.0905119,"Missing"
2020.findings-emnlp.273,P16-1162,0,0.0475056,"Missing"
2020.findings-emnlp.273,W18-5446,0,0.06074,"Missing"
2020.findings-emnlp.273,D19-1280,0,0.0566779,"Missing"
2020.findings-emnlp.273,D18-1009,0,0.0671083,"not generalize that one can chop celery the same way, but a human surely will. There is existing work in learning to play text games with RL (Narasimhan et al., 2015; Yuan et al., 2018; Kim, 2014; Zahavy et al., 2018; Yin and May, 2019a; Tessler et al., 2019) but the standard pattern of incorporating large language models such as BERT (Devlin et al., 2019) has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use BERT and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth (Zellers et al., 2018; Wang et al., 2018) or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic (Wolf et al., 2019; Imamura and Sumita, 2019). For tasks suited to RL such as the exploration of and interaction with a world, there is no true target or even, initially, a corpus, and thus learning can only proceed iteratively via, e.g., exploration-exploitation (Mnih et al., 2013), which requires millions of training iterations to converge (Yin and May, 2019a; Narasimhan et al., 2017; Mnih et al., 2013). Integrating this process games games response act"
2021.emnlp-main.493,N19-1300,0,0.11093,"to learn by reading. In the third configuration, which we call open book, models can access the textbook during the test. Thus, LEFT supports contrasting QA formulations and reading methods to explore the strengths and weaknesses of various QA approaches. The LEFT data and leaderboard are available at https:// leftleaderboard.isi.edu. 2 Related Work Question Answering. Most previous research specializes QA models to target specific question formulations. Question answering with a relevant paragraph often relies on span selection (Rajpurkar et al., 2016; Yang et al., 2015) or simple reasoning (Clark et al., 2019). Previous open-book QA methods first filter a large corpus to a small set of relevant documents using information retrieval (Karpukhin et al., 2020; Robertson and Zaragoza, 2009). The document set then provides context for answering questions (Dhingra et al., 2017; Dunn et al., 2017; Joshi et al., 2017; Nguyen et al., 2016). Conversely, closed-book QA instead requires models to answer using only their implicit knowledge (Roberts et al., 2020). Taking a step towards generalizing QA, UnifiedQA (Khashabi et al., 2020) proposes a unified architecture that answers various question types relying pa"
2021.emnlp-main.493,P17-1147,0,0.0229022,"tps:// leftleaderboard.isi.edu. 2 Related Work Question Answering. Most previous research specializes QA models to target specific question formulations. Question answering with a relevant paragraph often relies on span selection (Rajpurkar et al., 2016; Yang et al., 2015) or simple reasoning (Clark et al., 2019). Previous open-book QA methods first filter a large corpus to a small set of relevant documents using information retrieval (Karpukhin et al., 2020; Robertson and Zaragoza, 2009). The document set then provides context for answering questions (Dhingra et al., 2017; Dunn et al., 2017; Joshi et al., 2017; Nguyen et al., 2016). Conversely, closed-book QA instead requires models to answer using only their implicit knowledge (Roberts et al., 2020). Taking a step towards generalizing QA, UnifiedQA (Khashabi et al., 2020) proposes a unified architecture that answers various question types relying partly on knowledge encoded in its language model. AG Num. chapters Text size (words) Num. statements USH Dev Test Dev Test 8 137 620 186 9 138 668 214 8 89 765 274 24 301 860 412 Table 1: Data overview for the two textbooks: American Government 2e (AG) and U.S. History (USH). Table 1). Textbook Question"
2021.emnlp-main.493,2020.emnlp-main.550,0,0.0129695,"trasting QA formulations and reading methods to explore the strengths and weaknesses of various QA approaches. The LEFT data and leaderboard are available at https:// leftleaderboard.isi.edu. 2 Related Work Question Answering. Most previous research specializes QA models to target specific question formulations. Question answering with a relevant paragraph often relies on span selection (Rajpurkar et al., 2016; Yang et al., 2015) or simple reasoning (Clark et al., 2019). Previous open-book QA methods first filter a large corpus to a small set of relevant documents using information retrieval (Karpukhin et al., 2020; Robertson and Zaragoza, 2009). The document set then provides context for answering questions (Dhingra et al., 2017; Dunn et al., 2017; Joshi et al., 2017; Nguyen et al., 2016). Conversely, closed-book QA instead requires models to answer using only their implicit knowledge (Roberts et al., 2020). Taking a step towards generalizing QA, UnifiedQA (Khashabi et al., 2020) proposes a unified architecture that answers various question types relying partly on knowledge encoded in its language model. AG Num. chapters Text size (words) Num. statements USH Dev Test Dev Test 8 137 620 186 9 138 668 21"
2021.emnlp-main.493,2020.findings-emnlp.171,0,0.0693973,"form of true/false classification (BoolQ, Clark et al., 2019), multiplechoice, span selection (SQuAD, Rajpurkar et al., 2016), or text generation (TriviaQA, Joshi et al., 2017). Transformer architectures optimized for specific QA formulations have driven recent progress in question answering. For example, some models target IR-oriented QA (Guu et al., 2020) while others optimize their learning strategy to specific question types (e.g., by optimizing for expected answers to factoid questions, Roberts et al., 2020). While specialization improves performance, it limits generalization. UnifiedQA (Khashabi et al., 2020) takes a step forward by generalizing the architecture and training over multiple data sets with different QA formulations. Most research assumes that the information necessary to answer questions is either included with the query (e.g., BoolQ, SQuAD 1.1) or that the information was already stored in language models during initial pre-training or a task-specific second pre-training.1 However, this assumption limits language models relying on massive corpora (Gao et al., 2020; Raffel et al., 2020) to learning oftrepeated facts (Petroni et al., 2019). Valuable, domain-specific information seldom"
2021.emnlp-main.493,P19-1347,0,0.0243111,"(Khashabi et al., 2020) proposes a unified architecture that answers various question types relying partly on knowledge encoded in its language model. AG Num. chapters Text size (words) Num. statements USH Dev Test Dev Test 8 137 620 186 9 138 668 214 8 89 765 274 24 301 860 412 Table 1: Data overview for the two textbooks: American Government 2e (AG) and U.S. History (USH). Table 1). Textbook Question Answering. Researchers have explored machine understanding of elementary- and middle-school science textbooks by visual question answering (Gomez-Perez and Ortega, 2020; Kembhavi et al., 2017; Kim et al., 2019) and information retrieval (Clark et al., 2018). While existing textbook QA tasks focus on general knowledge (which can be gained by pre-training on general web corpora), LEFT focuses on domain-specific knowledge. Furthermore, it quantifies pre-trained language models’ pre-existing knowledge by requiring that models take the task before and after reading LEFT’s two textbooks. 3 Task Description Learning from Textbooks (LEFT) contains two machine-readable college-level introductory textbooks and a set of true/false statements manually derived from review questions written by the textbook author"
2021.emnlp-main.493,Q19-1026,0,0.0127516,"ng, where a system can Knowledge in Pre-trained Language Models. use a textbook paragraph when answering. Pre-trained language models (PTLMs) have shown Our goal is to support testing pre-trained langood performance in cloze-style queries (Petroni guage models, e.g., T5 (Raffel et al., 2020), and et al., 2019), fact-checking (Thorne et al., 2018), also those approaches that extract and store triples entity linking (Guo and Barbosa, 2018; Hoffart during reading (e.g., &lt;U.S. Declaration of indeet al., 2011), and open-domain QA (Joshi et al., pendence; signed; Aug 2, 1776>). While learning 2017; Kwiatkowski et al., 2019; Petroni et al., corpora appear in other question answering tasks 2021). However, in most cases, the PTLMs rely on (e.g., ARC, 14M words, Clark et al., 2018), the knowledge learned from massive corpora during text included in LEFT is small and corresponds to pre-training. LEFT tests domain-specific knowl- the textbook chapters relevant to each question set. edge acquired from a textbook, a small corpus of The largest text in LEFT contains only 300K words only a few hundreds of thousands of words (see (for details, see Table 1). 6105 LEFT includes two openly licensed2 collegelevel introductory"
2021.emnlp-main.493,2021.naacl-main.200,0,0.0727551,"Missing"
2021.emnlp-main.493,D19-1250,0,0.0174362,"rmance, it limits generalization. UnifiedQA (Khashabi et al., 2020) takes a step forward by generalizing the architecture and training over multiple data sets with different QA formulations. Most research assumes that the information necessary to answer questions is either included with the query (e.g., BoolQ, SQuAD 1.1) or that the information was already stored in language models during initial pre-training or a task-specific second pre-training.1 However, this assumption limits language models relying on massive corpora (Gao et al., 2020; Raffel et al., 2020) to learning oftrepeated facts (Petroni et al., 2019). Valuable, domain-specific information seldom is repeated often enough to be captured by language models. An evaluation of domain-specific knowledge without access to a relevant text is even more challenging as simple strategies like identifying the answer by information retrieval are ineffective. Even reasoning tasks such as ARC (Clark et al., 2018) only target general scientific knowledge and offer large text corpora to aid QA systems. We propose Learning from Textbooks (LEFT), a new task to classify domain-specific statements drawn from a textbook’s review questions as true Question answer"
2021.emnlp-main.493,2020.emnlp-demos.6,0,0.0428261,"Missing"
2021.emnlp-main.493,D15-1237,0,0.0915904,"Missing"
2021.emnlp-main.493,D16-1264,0,0.256402,"ive. Even reasoning tasks such as ARC (Clark et al., 2018) only target general scientific knowledge and offer large text corpora to aid QA systems. We propose Learning from Textbooks (LEFT), a new task to classify domain-specific statements drawn from a textbook’s review questions as true Question answering (QA) is a yardstick for measuring machine understanding performance (Hermann et al., 2015). QA’s popularity as an evaluation technique has led to several sub-categories: tasks can require a model to answer questions from either its background knowledge or from a short passage (e.g., SQuAD, Rajpurkar et al., 2016) or with infor1 For example, Roberts et al. (2020) adjust T5’s masking mation retrieval to allow the model to search for the strategy to target named entities as they expect named entities answer in a large corpus (e.g., ARC, Clark et al., to be parts of answers. 6104 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6104–6111 c November 7–11, 2021. 2021 Association for Computational Linguistics or false using three evaluation configurations. The first configuration tests the ability to answer questions without any domain-specific material (e.g., app"
2021.emnlp-main.493,D19-1410,0,0.0339902,"Missing"
2021.emnlp-main.493,2020.emnlp-main.437,0,0.0210731,"Missing"
2021.emnlp-main.493,P19-1355,0,0.0123391,"the correct answers for multiple-choice questions in the two textbooks. While both American Government 2e and U.S. History include answer keys, they are incomplete. We believe releasing the correct answers to all multiplechoice questions in the book would be detrimental to the intended primary users of the two textbooks; in other words, it might hinder students’ learning. We only used full-time employees compensated according to U.S. law to rewrite the multiple-choice review questions in the two textbooks. Environmental. We included baseline results based on large pre-trained language models. Strubell et al. (2019) raised concerns about the environmental impact of training deep learning language models. Patterson et al. (2021) pointed out that most of the energy consumption for deep learning language models comes during the initial pre-training. In this work, we limit ourselves to fine-tuning and light continued pre-training of T5 and GPT-Neo. While we do not have information about GPT-Neo’s training, T5’s training took place in highly efficient data centers whose energy con5 See Diversity and Representation Development Guidelines in the instructor materials for each textbook. 6 See the Errata Release N"
2021.emnlp-main.493,N18-1074,0,0.0504296,"Missing"
2021.naacl-demos.2,D15-1075,0,0.0108584,"se arguments with their entity type and their role in each event using an ontology. That ontology provides only coarse distinctions between entities (e.g., a single category for facilities that does not distinguish a car dealership from a school or a bank). To support finer-grained distinctions and, in the future, leverage external knowledge sources, we incorporate connections to Wikidata7 using KGTK (Ilievski et al., 2020). MASC’s links aim to ground descriptive noun phrases (e.g., car 5 Our Sentence-RoBERTa model is trained on more data. We use the two data sets in the original paper, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), and add the newer ANLI (Nie et al., 2020). 6 The mean reciprocal rank (MRR, Radev et al., 2002) was 0.35 on the top three model predictions. 7 https://www.wikidata.org 8 https://www.elastic.co/ elasticsearch/ 11 characters. We address duplicated output, a considerable concern for GPT-2, especially given the short and similar inputs.9 The filters eliminate strings with duplicates in the alternatives or the human-curated schema. To account for semantic duplicates, such as go to dealership and go to the car dealership, we use a variant of Gestalt Pattern Matchin"
2021.naacl-demos.2,P15-2061,0,0.0301546,"ve related paths (e.g., leasing instead of purchasing a car). 4 analysis. 4.1 Event Type Classification Each sub-event is ontologized with one of 41 event types through a semi-automated process. The ontology labels support connecting information to extraction engines and thus allow a script to provide potential event-event relations given information extraction output. Furthermore, the ontology labels provide language- and media-independent knowledge for identifying potential instances of the scripts. There has been much work on automatic detection of event types (and triggers) in text (e.g., Bronstein et al. (2015); Lin et al. (2020); Peng et al. (2016)). Here, our input data (and goals) are slightly different. The ontology we use, while overlapping with ACE (Walker et al., 2006), introduces several new event types for which we do not have annotated training examples. Instead, the ontology provides a short definition and template for each event type. The curator’s input events tend to be short imperative sentences with different linguistic characteristics than the text annotated in, e.g., ACE. Unlike standard information extraction, we need not identify a specific trigger phase.4 Thus, we use a differen"
2021.naacl-demos.2,D13-1185,0,0.0345333,"and sub-events that may have been forgotten. We illustrate how these automations are useful to the script writer with a few case-study scripts. 1 Introduction Scripts have been of interest for encoding procedural knowledge and understanding stories for over 40 years (Schank and Abelson, 1977). In the form of checklists, recording procedural knowledge has revolutionized fields like medicine and aviation by encoding expert knowledge and best practices (Degani and Wiener, 1993; Gawande, 2010). In the last few years, researchers have turned their attention to automatic script discovery from text (Chambers, 2013; Weber et al., 2020, 2018). However, exclusively data-driven sub-event discovery methods face the challenge that narrative descriptions often omit common knowledge.1 We aim for a process for building a library of scripts through human-machine collaboration leveraging NLP techniques to augment human background knowledge. The resulting demonstration system serves two related purposes. First, it is a knowledge acquisition tool that supports the development of a repository of scripts for use by downstream applications. Second, it is an annotation tool that supports the creation of a library to 2"
2021.naacl-demos.2,2020.lrec-1.308,0,0.0127387,"dels (GPT-2 and RoBERTa). The bias of these algorithms, measuring that bias, and mitigating it is an active area of work. Recent work has provided data sets for measuring bias (Nadeem et al., 2020) and metastudies of the approaches taken to study and address bias (Blodgett et al., 2020). Much work has focused on bias as it impacts demographic groups. MASC focuses on events, not individuals. The publicly available GPT-2 models have learned from data that might not cover current events (e.g., GPT-2 was trained before the COVID19 epidemic), represents only English dialects from the inner-circle (Dunn and Adams, 2020), and contains toxic language (Gehman et al., 2020). In our immediate context, we mitigate against the challenge presented by language model bias by requiring manual review of all automatically suggested output. If the ideas in this paper were extended to a fully automatic approach, language model and domain-specific studies of the impact of bias on LM-based suggestions would be necessary. Data Set. To understand how the tool is used and future research directions, we created five sample scripts which we included in the supplementary material. These scripts provide interesting examples of what"
2021.naacl-demos.2,2020.findings-emnlp.301,0,0.0112739,"ms, measuring that bias, and mitigating it is an active area of work. Recent work has provided data sets for measuring bias (Nadeem et al., 2020) and metastudies of the approaches taken to study and address bias (Blodgett et al., 2020). Much work has focused on bias as it impacts demographic groups. MASC focuses on events, not individuals. The publicly available GPT-2 models have learned from data that might not cover current events (e.g., GPT-2 was trained before the COVID19 epidemic), represents only English dialects from the inner-circle (Dunn and Adams, 2020), and contains toxic language (Gehman et al., 2020). In our immediate context, we mitigate against the challenge presented by language model bias by requiring manual review of all automatically suggested output. If the ideas in this paper were extended to a fully automatic approach, language model and domain-specific studies of the impact of bias on LM-based suggestions would be necessary. Data Set. To understand how the tool is used and future research directions, we created five sample scripts which we included in the supplementary material. These scripts provide interesting examples of what we could learn from a larger scale data set; howev"
2021.naacl-demos.2,2020.acl-main.485,0,0.010738,"ver-represents some issues, while some socially important ones are under-represented or missing. Wikidata linking is optional; thus in a domain that is not well covered, a curator can skip the linking step or replace Wikidata with a domainrelevant resource. The suggestion capabilities described in Section 4 use pretrained language models (GPT-2 and RoBERTa). The bias of these algorithms, measuring that bias, and mitigating it is an active area of work. Recent work has provided data sets for measuring bias (Nadeem et al., 2020) and metastudies of the approaches taken to study and address bias (Blodgett et al., 2020). Much work has focused on bias as it impacts demographic groups. MASC focuses on events, not individuals. The publicly available GPT-2 models have learned from data that might not cover current events (e.g., GPT-2 was trained before the COVID19 epidemic), represents only English dialects from the inner-circle (Dunn and Adams, 2020), and contains toxic language (Gehman et al., 2020). In our immediate context, we mitigate against the challenge presented by language model bias by requiring manual review of all automatically suggested output. If the ideas in this paper were extended to a fully au"
2021.naacl-demos.2,D15-1195,0,0.442034,"Missing"
2021.naacl-demos.2,D19-1282,0,0.0540523,"Missing"
2021.naacl-demos.2,2020.emnlp-main.373,0,0.0345352,"Missing"
2021.naacl-demos.2,2020.acl-main.713,0,0.0117611,"easing instead of purchasing a car). 4 analysis. 4.1 Event Type Classification Each sub-event is ontologized with one of 41 event types through a semi-automated process. The ontology labels support connecting information to extraction engines and thus allow a script to provide potential event-event relations given information extraction output. Furthermore, the ontology labels provide language- and media-independent knowledge for identifying potential instances of the scripts. There has been much work on automatic detection of event types (and triggers) in text (e.g., Bronstein et al. (2015); Lin et al. (2020); Peng et al. (2016)). Here, our input data (and goals) are slightly different. The ontology we use, while overlapping with ACE (Walker et al., 2006), introduces several new event types for which we do not have annotated training examples. Instead, the ontology provides a short definition and template for each event type. The curator’s input events tend to be short imperative sentences with different linguistic characteristics than the text annotated in, e.g., ACE. Unlike standard information extraction, we need not identify a specific trigger phase.4 Thus, we use a different approach to event"
2021.naacl-demos.2,W15-0812,0,0.0604296,"Missing"
2021.naacl-demos.2,2020.acl-main.441,0,0.0134674,"ogy. That ontology provides only coarse distinctions between entities (e.g., a single category for facilities that does not distinguish a car dealership from a school or a bank). To support finer-grained distinctions and, in the future, leverage external knowledge sources, we incorporate connections to Wikidata7 using KGTK (Ilievski et al., 2020). MASC’s links aim to ground descriptive noun phrases (e.g., car 5 Our Sentence-RoBERTa model is trained on more data. We use the two data sets in the original paper, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), and add the newer ANLI (Nie et al., 2020). 6 The mean reciprocal rank (MRR, Radev et al., 2002) was 0.35 on the top three model predictions. 7 https://www.wikidata.org 8 https://www.elastic.co/ elasticsearch/ 11 characters. We address duplicated output, a considerable concern for GPT-2, especially given the short and similar inputs.9 The filters eliminate strings with duplicates in the alternatives or the human-curated schema. To account for semantic duplicates, such as go to dealership and go to the car dealership, we use a variant of Gestalt Pattern Matching (Ratcliff and Metzener, 1988) through Python’s difflib. For usability, we"
2021.naacl-demos.2,D19-1331,0,0.0137514,"buying a car.), followed by the first few events of the script. In the initial version, we used a form of the events as First, Identify your needs. Then, Decide on your budget. Next, Identify car models you can afford. However, a numerical formulation proved much more effective (e.g., 1. Identify your needs 2. Decide on your budget 3. Identify car models you can afford 4.) and resulted in more coherent events. To filter undesirable or redundant output, we pass GPT-2 outputs through a sequence of filters. We remove undesired strings characteristic of neural text generation, like empty strings (Stahlberg and Byrne, 2019), and outputs that are invalid in the context of schema creation: strings of less than two words and those with sequences of non-alphabetic 9 GPT-2 often generates strings with a similar meaning, but lexically different, e.g., for a script on buying a car, it might generate buy, buy the car, and purchase the car. It is superfluous to show users all three suggestions. 12 the diversity, prior knowledge, and level of detail a script author uses. In our analysis, we have seen that the scripts created with MASC encode knowledge that is uncommon in news-like data sets. For example, our curator inclu"
2021.naacl-demos.2,D16-1038,0,0.0648113,"Missing"
2021.naacl-demos.2,radev-etal-2002-evaluating,0,0.0941492,"between entities (e.g., a single category for facilities that does not distinguish a car dealership from a school or a bank). To support finer-grained distinctions and, in the future, leverage external knowledge sources, we incorporate connections to Wikidata7 using KGTK (Ilievski et al., 2020). MASC’s links aim to ground descriptive noun phrases (e.g., car 5 Our Sentence-RoBERTa model is trained on more data. We use the two data sets in the original paper, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), and add the newer ANLI (Nie et al., 2020). 6 The mean reciprocal rank (MRR, Radev et al., 2002) was 0.35 on the top three model predictions. 7 https://www.wikidata.org 8 https://www.elastic.co/ elasticsearch/ 11 characters. We address duplicated output, a considerable concern for GPT-2, especially given the short and similar inputs.9 The filters eliminate strings with duplicates in the alternatives or the human-curated schema. To account for semantic duplicates, such as go to dealership and go to the car dealership, we use a variant of Gestalt Pattern Matching (Ratcliff and Metzener, 1988) through Python’s difflib. For usability, we suggest at most 12 sub-events per script. Figure 4 sho"
2021.naacl-demos.2,D19-1585,0,0.0248727,"Missing"
2021.naacl-demos.2,2020.emnlp-main.612,0,0.0452385,"Missing"
2021.naacl-demos.2,D19-1410,0,0.012403,"ow, we describe the models behind these capabilities and, for each model, report the accuracy using the five sample scripts created for this paper. Given the small sample size, the five sample scripts are best thought of as case studies, not a benchmark. Table 1 provides per-script 4 Triggers are often used as a means to identify arguments of interest. But here, partly because of the telegraphic nature of the text entries, the arguments are often missing and, therefore, explicitly added. 10 To map from the curators’ description of an event to the ontology, we use a version of SentenceRoBERTa (Reimers and Gurevych, 2019)5 to estimate the similarity of the curators’ text input to the prose description of each action in the ontology. For example, for the user input go to a car dealership, the action description Explicit mention of granting or allowing entry or exit from a location receives the highest similarity score, and the corresponding action type Movement.Transportation becomes one of the recommendations. MASC suggests the three ontology actions most similar to the user’s description. The user can accept one of the suggestions or pick a different type from the ontology (Figure 1, second column). As mentio"
2021.naacl-demos.2,D18-1413,0,0.311072,"EVAC 16 25/44/50 FOOD 9 11/33/67 JOB 16 13/44/44 MED 5 20/60/60 MERGER 12 50/67/67 2/26/1 5/18/3 2/24/2 3/11/3 3/24/2 4/8 3/12 0/11 5/12 2/12 Y 1.5 hrs Y 0.5 hr Y 1 hr N 0.5 hr N 2.5 hrs Table 1: Characteristics of five sample scripts. what the curator omits through forgetfulness or because they assume common knowledge. Further exploration of how a machine can aid a person whose knowledge may be incomplete or may forget to be explicit seems promising. Examples of possible research directions include incorporating suggestions from approaches that discover scripts (e.g., Rudinger et al. (2015); Weber et al. (2018, 2020)) and leveraging background knowledge (e.g., Wikidata). 6 Wikidata over-represents some issues, while some socially important ones are under-represented or missing. Wikidata linking is optional; thus in a domain that is not well covered, a curator can skip the linking step or replace Wikidata with a domainrelevant resource. The suggestion capabilities described in Section 4 use pretrained language models (GPT-2 and RoBERTa). The bias of these algorithms, measuring that bias, and mitigating it is an active area of work. Recent work has provided data sets for measuring bias (Nadeem et al."
2021.naacl-demos.2,N18-1101,0,0.0104121,"type and their role in each event using an ontology. That ontology provides only coarse distinctions between entities (e.g., a single category for facilities that does not distinguish a car dealership from a school or a bank). To support finer-grained distinctions and, in the future, leverage external knowledge sources, we incorporate connections to Wikidata7 using KGTK (Ilievski et al., 2020). MASC’s links aim to ground descriptive noun phrases (e.g., car 5 Our Sentence-RoBERTa model is trained on more data. We use the two data sets in the original paper, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), and add the newer ANLI (Nie et al., 2020). 6 The mean reciprocal rank (MRR, Radev et al., 2002) was 0.35 on the top three model predictions. 7 https://www.wikidata.org 8 https://www.elastic.co/ elasticsearch/ 11 characters. We address duplicated output, a considerable concern for GPT-2, especially given the short and similar inputs.9 The filters eliminate strings with duplicates in the alternatives or the human-curated schema. To account for semantic duplicates, such as go to dealership and go to the car dealership, we use a variant of Gestalt Pattern Matching (Ratcliff and Metzener, 1988) t"
2021.naacl-main.343,P19-1470,0,0.0228258,"shows a distinct manipulation technique. The manipulated plots are passed through a generation model to generate implausible samples for the evaluation task. Non-logically Ordered Plots. Logical conflict is one of the sources for implausibility that results from not-logically ordered concepts in the text. While Guan and Huang (2020) covered this type of implausibility by changing the order of sentences, we hypothesize that disrupting the logical order at the concept-level is more efficient. To accomplish concept reordering, we first randomly choose verbs from the plot and leverage the COMET (Bosselut et al., 2019) model to predict their subsequent events. Then we dislocate the resulted concept pairs. COMET, which is trained on tuples of the form ( subject, relation, object), can be used to predict an object given a pair of subject and relation. As an example, given the pair (work, causes) COMET will predict get pay to show that work causes to get paid. We focus on COMET relations HasPrerequisite, HasFirstSubevent, Causes and HasLastSubevent that imply ordering. In the first two relations, object should appear before subject, while in the other two the order is reversed. Therefore, subject work comes be"
2021.naacl-main.343,2020.aacl-main.59,0,0.0179119,"examples and change their structure to generate negative examples. Sentence Substitution. Sentence substitution 3.2.1 Plot Manipulations (briefly H EUR _S ENT _S UB ) replaces a fraction of Studies have shown that high-quality fluent stories sentences in the plausible text with random ones can be generated by planning in advance and lever(See Figure 1). This breaks the discourse-level coaging lucrative plots (Yao et al., 2019; Fan et al., herence, making a story not interpretable (Li and 2019; Goldfarb-Tarrant et al., 2019, 2020; Rashkin Jurafsky, 2016; Holtzman et al., 2018). et al., 2020b; Brahman et al., 2020). Yao et al. Keyword Substitution. Guan and Huang (2020) (2019) leverage a sequence of keywords as the plot proposed to apply random substitutions at the representation (also called storyline). Fan et al. keyword-level (briefly H EUR _K EY _S UB ), where (2019) use semantic role labeling tool to extract a fraction of keywords are randomly substituted plots as abstract presentation of stories over actions with their corresponding antonyms from a com- and entities. Their experiments affirm that plots monsense knowledge base such as ConceptNet have positive effects on generating high-quality sto("
2021.naacl-main.343,N19-1423,0,0.0574184,"these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whole text, and then passes it through a hidden layer. Recently, Guan and Huang (2020) proposed a more accurate automatic evaluation metric called UNION. This metric achieved better performance by using BERT (Devlin et al., 2019) as a more effective classification model and have a broader set of negative samples coming from different heuristics. For all learning-based metrics, the simplicity of heuristically generated data samples makes them inadequate for an accurate evaluation of plausibility in open-domain generated texts. 3 Implausible Text Construction We formulate the evaluation of open-domain story generation as a binary classification task where the goal is to distinguish plausible and implausi• We show the affirmative role of adversarial filter- ble generated stories, also referred to as positive ing techniqu"
2021.naacl-main.343,P18-1082,0,0.375915,"tion of generation models can be classified into two subgroups, non-learning-based and learning-based methods, which we briefly summarize below. Non-learning-based Metrics. Some metrics in this group consider the centrality of a text around a specific topic as a proxy for measuring its quality. The transitions of entities in neighbor sentences and their distribution across text have been served as a measurement for quality assessment (Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005). Perplexity is another commonly used metric to evaluate the quality of text and story generation models (Fan et al., 2018; Peng et al., 2018). Learning-based Metrics. This group of metrics is based on neural-based classifiers trained on a set of positive (plausible) and negative (implausible) texts. The common point between these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whol"
2021.naacl-main.343,P19-1254,0,0.0222618,"ring and negation (See the UNION story in Figure 1). In this work, we hypothesize that heuristically generated data cannot adequately reflect the characteristics of the implausible texts generated by language models, thus result in suboptimal trained evaluation metrics. This deficiency can be mitigated by generating high-quality implausible examples that are closer to the test data. Toward this goal, we propose an approach based on the manipulation of plots, which are high-level structured representations of generated texts originally used as a contentplanning tool for better text generation (Fan et al., 2019; Goldfarb-Tarrant et al., 2020). Specifically, we propose to manipulate plots by injecting incoherence sources into them. The generation models conditioned on such manipulated plots lead to implausible texts that have pertinent similarities with implausible machine-generated texts and thus can serve as good negative examples for training evaluation metrics. We further improve the quality of training data by incorporating the adversarial filtering technique proposed by Zellers et al. (2018) to select more challenging negative samples generated from the manipulated plots (See Figure 1). Eventua"
2021.naacl-main.343,2020.emnlp-main.351,1,0.762193,"(See the UNION story in Figure 1). In this work, we hypothesize that heuristically generated data cannot adequately reflect the characteristics of the implausible texts generated by language models, thus result in suboptimal trained evaluation metrics. This deficiency can be mitigated by generating high-quality implausible examples that are closer to the test data. Toward this goal, we propose an approach based on the manipulation of plots, which are high-level structured representations of generated texts originally used as a contentplanning tool for better text generation (Fan et al., 2019; Goldfarb-Tarrant et al., 2020). Specifically, we propose to manipulate plots by injecting incoherence sources into them. The generation models conditioned on such manipulated plots lead to implausible texts that have pertinent similarities with implausible machine-generated texts and thus can serve as good negative examples for training evaluation metrics. We further improve the quality of training data by incorporating the adversarial filtering technique proposed by Zellers et al. (2018) to select more challenging negative samples generated from the manipulated plots (See Figure 1). Eventually, these samples result in mor"
2021.naacl-main.343,2020.emnlp-main.736,0,0.0750275,"assifiers is a key determinant of the domain natural language generation (NLG), such metric effectiveness. Existing works take humanas dialog systems (Zhang et al., 2020) and story written texts as plausible (positive) examples, while 4334 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4334–4344 June 6–11, 2021. ©2021 Association for Computational Linguistics the negative samples are heuristically generated by randomly substituting keywords or sentences (See Figure 1) (Li and Jurafsky, 2016; Guan and Huang, 2020). Guan and Huang (2020) further improved the quality of evaluators by applying heuristic rules such as adding repetition, reordering and negation (See the UNION story in Figure 1). In this work, we hypothesize that heuristically generated data cannot adequately reflect the characteristics of the implausible texts generated by language models, thus result in suboptimal trained evaluation metrics. This deficiency can be mitigated by generating high-quality implausible examples that are closer to the test data. Toward this goal, we propose an approach based on the manipulation of plots, which are"
2021.naacl-main.343,P02-1040,0,0.116077,"ck) using sentence, keyword and UNION manipulations versus injecting implausible sources into the story plot (the third block, from the left plot to the right one) and generating a more natural implausible story (the last story). Blue highlights show the implausible sections. generators (Rashkin et al., 2020a) necessitates automatic evaluation metrics for quality assessment. The existence of accurate automatic evaluation metrics can accelerate the development cycle by facilitating the process of model comparison and hyperparameter search. Many existing reference-based approaches such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) fail to correlate well with human judgment in open-domain settings due to the fact that there can be potentially many plausible generations that do not have significant overlap with the limited set of given references. This failure invites research on more sophisticated and reliable evaluation metrics. Recently, learning-based approaches have been proposed to overcome this limitation by training classifiers to distinguish between plausible and implausible texts (Li and Jurafsky, 2016; Holtzman 1 Introduction et al., 2018). The choice of training data for learnThe surge of"
2021.naacl-main.343,P18-1152,0,0.0186227,"es to heuristically manipulate positive examples and change their structure to generate negative examples. Sentence Substitution. Sentence substitution 3.2.1 Plot Manipulations (briefly H EUR _S ENT _S UB ) replaces a fraction of Studies have shown that high-quality fluent stories sentences in the plausible text with random ones can be generated by planning in advance and lever(See Figure 1). This breaks the discourse-level coaging lucrative plots (Yao et al., 2019; Fan et al., herence, making a story not interpretable (Li and 2019; Goldfarb-Tarrant et al., 2019, 2020; Rashkin Jurafsky, 2016; Holtzman et al., 2018). et al., 2020b; Brahman et al., 2020). Yao et al. Keyword Substitution. Guan and Huang (2020) (2019) leverage a sequence of keywords as the plot proposed to apply random substitutions at the representation (also called storyline). Fan et al. keyword-level (briefly H EUR _K EY _S UB ), where (2019) use semantic role labeling tool to extract a fraction of keywords are randomly substituted plots as abstract presentation of stories over actions with their corresponding antonyms from a com- and entities. Their experiments affirm that plots monsense knowledge base such as ConceptNet have positive e"
2021.naacl-main.343,W18-5023,0,0.350261,"nd Barzilay, 2005). Perplexity is another commonly used metric to evaluate the quality of text and story generation models (Fan et al., 2018; Peng et al., 2018). Learning-based Metrics. This group of metrics is based on neural-based classifiers trained on a set of positive (plausible) and negative (implausible) texts. The common point between these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whole text, and then passes it through a hidden layer. Recently, Guan and Huang (2020) proposed a more accurate automatic evaluation metric called UNION. This metric achieved better performance by using BERT (Devlin et al., 2019) as a more effective classification model and have a broader set of negative samples coming from different heuristics. For all learning-based metrics, the simplicity of heuristically generated data samples makes them inadequate for an accurate evaluation of p"
2021.naacl-main.343,W18-1505,1,0.837531,"models can be classified into two subgroups, non-learning-based and learning-based methods, which we briefly summarize below. Non-learning-based Metrics. Some metrics in this group consider the centrality of a text around a specific topic as a proxy for measuring its quality. The transitions of entities in neighbor sentences and their distribution across text have been served as a measurement for quality assessment (Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005). Perplexity is another commonly used metric to evaluate the quality of text and story generation models (Fan et al., 2018; Peng et al., 2018). Learning-based Metrics. This group of metrics is based on neural-based classifiers trained on a set of positive (plausible) and negative (implausible) texts. The common point between these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whole text, and then pas"
2021.naacl-main.343,2020.emnlp-main.349,0,0.0170996,"the woman at the store. she brought her worms and a chair and decided to play with them. jenny sat down and laid down on the chair. when she got wet, she packed up and went home disappointed. Figure 1: Heuristically generated implausible stories (the second block) for a given human-written story (the first block) using sentence, keyword and UNION manipulations versus injecting implausible sources into the story plot (the third block, from the left plot to the right one) and generating a more natural implausible story (the last story). Blue highlights show the implausible sections. generators (Rashkin et al., 2020a) necessitates automatic evaluation metrics for quality assessment. The existence of accurate automatic evaluation metrics can accelerate the development cycle by facilitating the process of model comparison and hyperparameter search. Many existing reference-based approaches such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) fail to correlate well with human judgment in open-domain settings due to the fact that there can be potentially many plausible generations that do not have significant overlap with the limited set of given references. This failure invites research on more sophisti"
2021.naacl-main.343,2020.acl-main.704,0,0.0426197,"Missing"
2021.naacl-main.343,speer-havasi-2012-representing,0,0.071041,". Yao et al. Keyword Substitution. Guan and Huang (2020) (2019) leverage a sequence of keywords as the plot proposed to apply random substitutions at the representation (also called storyline). Fan et al. keyword-level (briefly H EUR _K EY _S UB ), where (2019) use semantic role labeling tool to extract a fraction of keywords are randomly substituted plots as abstract presentation of stories over actions with their corresponding antonyms from a com- and entities. Their experiments affirm that plots monsense knowledge base such as ConceptNet have positive effects on generating high-quality sto(Speer and Havasi, 2012) to corrupt the plausibility ries. in the text. ConceptNet consists of (object, relaHere we leverage this idea for generating imtion, subject) triplets. For each selected keyword plausible texts, by controllable injection of implauthat exists as an object or subject in the Concept- sibility sources, or perturbations, into the groundNet, its counterpart is extracted from one of the truth plots. The resulting plot-level manipulations contradiction-type relations; Antonym, NotDesires, will force the model to reflect applied implausiNotCapableOf, or NotHasProperty. For instance, bility in the gene"
2021.naacl-main.343,2021.ccl-1.108,0,0.0262502,"Missing"
2021.naacl-main.343,N16-1098,0,0.172275,"rent in terms of tentions, which reduce the computation complexity length and topic; ROCStories (shortly ROC) and 4339 Dataset H EUR _S ENT _S UB H EUR _K EY _S UB UNION_DATA M AN P LTS AF_M AN P LTS Train/Valid/Test 47.1k/5.9k/5.9k 47.1k/5.9k/5.9k 47.1k/5.9k/5.9k 47.1k/5.9k/5.9k 94.2k/11.8k/11.8k Table 2: Plausibility evaluation datasets for ROC stories using different negative sampling techniques. Writing Prompt (briefly WP) datasets including on average 49.4 and 734.5 tokens in each story. ROCStories. ROCStories is a resource of fivesentence commonsense stories collected via crowdsourcing (Mostafazadeh et al., 2016) covering a logically linked set of daily events. We follow the approach proposed by Yao et al. (2019) to extract story plots (storylines) for the stories and manipulate them to guide conditional language models to generate negative samples. Writing Prompt. Writing Prompt dataset contains abstract high-level prompts and their corresponding long human-written stories from an online forum (Fan et al., 2018). To apply the plot manipulation technique for implausible text construction, we follow the procedure proposed by Fan et al. (2019) to extract the plots with verb and argument type role labeli"
2021.naacl-main.343,2020.acl-demos.30,0,0.0138187,"p with the limited set of given references. This failure invites research on more sophisticated and reliable evaluation metrics. Recently, learning-based approaches have been proposed to overcome this limitation by training classifiers to distinguish between plausible and implausible texts (Li and Jurafsky, 2016; Holtzman 1 Introduction et al., 2018). The choice of training data for learnThe surge of downstream applications for open- ing such classifiers is a key determinant of the domain natural language generation (NLG), such metric effectiveness. Existing works take humanas dialog systems (Zhang et al., 2020) and story written texts as plausible (positive) examples, while 4334 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4334–4344 June 6–11, 2021. ©2021 Association for Computational Linguistics the negative samples are heuristically generated by randomly substituting keywords or sentences (See Figure 1) (Li and Jurafsky, 2016; Guan and Huang, 2020). Guan and Huang (2020) further improved the quality of evaluators by applying heuristic rules such as adding repetition, reordering and negation (Se"
A00-1044,A97-1029,1,0.720136,"ibute to performance? (Section 7) 2 2.1 Algorithms and Data normal punctuation. Printing the on-line text, rather than using the original newsprint, produced the images for OCR, which were all scanned at 600 DPI. Task Definition and Data The named entity (NE) task used for this evaluation requires the system to identify all named locations, named persons, named organizations, dates, times, monetary amounts, and percentages. The task definition is given in Chinchor, et al, (1998). 2.2 Algorithms The information extraction system tested is IdentiFinder(TM), which has previously been detailed in Bikel et al. (1997, 1999). In that system, an HMM labels each word either with one of the desired classes (e.g., person, organization, etc.) or with the label NOT-ANAME (to represent &quot;none of the desired classes&quot;). The states of the HMM fall into regions, one region for each desired class plus one for NOT-A-NAME. (See Figure 2-1.) The HMM thus has a model of each desired class and of the other text. Note that the implementation is not confined to the seven name classes used in the NE task; the particular classes to be recognized can be easily changed via a parameter. For speech recognition, roughly 175 hours of"
A00-1044,M98-1028,0,0.0342635,"Missing"
A00-2030,A97-1029,1,0.567587,"ample of information to be extracted for TR 227 An integrated model can limit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other. A second consideration influenced our decision toward an integrated model. We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al. 1997). Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model. Thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model. Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model. reference relation between ""Nance"" and ""a paid consultant to ABC News"" is indicated by ""per-desc-of."" In this case, because the argument does not connect directly to the"
A00-2030,P96-1025,0,0.125126,"d TREEBANKing of the variety of news sources in MUC-7 be required? Or could the University of Pennsylvania's TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire, which includes dozens of newspapers? Manually creating sourcespecific training data for syntax was not required. Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation. Introduction Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard. Yet, relatively few have embedded one of these algorithms in a task. Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition. * Would semantic annotation require computational linguists? We were able to specify relatively simple guidelines that students with no training in computational linguistics could annotate. 2 Information Extraction Tasks We"
A00-2030,P97-1003,0,0.13058,"imit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other. A second consideration influenced our decision toward an integrated model. We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al. 1997). Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model. Thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model. Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model. reference relation between ""Nance"" and ""a paid consultant to ABC News"" is indicated by ""per-desc-of."" In this case, because the argument does not connect directly to the relation, the intervening nodes are labeled with semantics ""-ptr"" to"
A00-2030,J93-2004,0,0.0419493,"Missing"
A00-2030,W97-0302,0,0.0140505,": P ( W m I C m , t m t h , W h ) , e.g. P(nance I per / np, per / nnp, vbd, said) +23 P(w m ICm,t,,) +~4 P(w, It,,) Finally, for word components are: 231 features, the mixture P'(f,, [c,,,t~,t h, w h, known(w,,)) = 21 P(f,, )c,,,t,,,wh,known(w,,)) +)[2 e(f., [c~,t,,,th,kn°wn(w,,)) +A3 e(L, [c,,,t ,,known(w,,)) +As P(fm [t,,,known(w,,)) 9 threshold of the highest scoring constituent are maintained; all others are pruned. For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997). We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node. Thus, the scores used in pruning can be considered as the product of: Searching the Model Given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation. More precisely, it must find the most likely augmented parse tree. Although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search. The search is kept tractable"
A00-2030,J93-2006,1,\N,Missing
A83-1014,J81-2002,0,\N,Missing
A83-1014,J80-1002,0,\N,Missing
A83-1014,J81-4004,0,\N,Missing
A83-1014,A83-1017,0,\N,Missing
A83-1014,C80-1008,1,\N,Missing
A83-1014,T78-1029,0,\N,Missing
A83-1014,J78-3037,0,\N,Missing
A83-1014,A83-1015,0,\N,Missing
A83-1014,C80-1027,0,\N,Missing
A83-1014,A83-1016,0,\N,Missing
A97-1029,J93-2006,1,\N,Missing
A97-1029,M95-1012,0,\N,Missing
A97-1029,H94-1053,1,\N,Missing
A97-1029,M92-1024,1,\N,Missing
boisen-etal-2000-annotating,A97-1028,0,\N,Missing
boisen-etal-2000-annotating,A00-1044,1,\N,Missing
boisen-etal-2000-annotating,A97-1030,0,\N,Missing
boisen-etal-2000-annotating,W99-0612,0,\N,Missing
boisen-etal-2000-annotating,A97-1029,1,\N,Missing
C80-1008,J77-1008,1,0.752838,"ability to relax certain semantic constraints. With respect to error identification, Heidorn (1972) dealt with incomplete semantic entities by requesting users to supply missing information based on failures to translate from the internal semantic structures to external computer programs. A somewhat similar process is seen in work by Chang (1978) on the RENDEZVOUS system where failure to parse a query leads to a request for clarification from the user. With respect to pragmatic errors, Weischedel (1977) introduced a technique which uses presupposition to find certain incorrect uses of words. Joshi and Weischedel (1977) and Weischedel (1979) show that since presuppositions can be computed by a parser and its lexicon they are a class of assumptions inherent in the user input; therefore they can be checked for discrepancies with the system&apos;s world knowledge. This work was used and extended by Kaplan (1979) in error identification and recovery in those situations where a user&apos;s database query would normally yield only an empty set, i.e. an answer of none. Janas (1979) applied similar techniques to assist the user in the same situations. 3) Improving ill-formedness handling by parallel processing of lexical, sem"
C80-1008,H89-2010,0,0.0550097,"Missing"
C80-1008,J80-2003,1,\N,Missing
C80-1008,P79-1014,0,\N,Missing
C80-1008,P79-1006,1,\N,Missing
C80-1008,P79-1002,0,\N,Missing
C80-1008,P80-1026,0,\N,Missing
D09-1008,2007.mtsummit-papers.11,0,0.121574,"listic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. He et al. (2008) extended the WSD-like approached proposed in (Carpuat and Wu, 2007) to hierarchical decoders. In (He et al., 2008), lexical Current methods of using lexical features in machine translation have difficulty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper,"
D09-1008,D08-1024,0,0.0897575,"06; Tillmann and Zhang, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich f"
D09-1008,P08-1114,0,0.0547306,", 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by Surprisingly, source dependency LM did not provide any improvement over the baseline. There are two possible reasons for this. One is that the source and target parse trees were generated by two stand-alone parsers, which may cause incompatible structures on the source and target sides. By applying the well-formed constraints 78 Model BASE SLM CLM LEN LBL LBL+LEN LBL+LEN+"
D09-1008,H91-1013,0,0.0127462,"side constraint. On the 3 Experiments We designed our experiments to show the impact of each feature separately as well as their cumulative impact: • BASE: baseline string-to-dependency system • SLM: baseline + source dependency LM • CLM: baseline + context LM • LEN: baseline + length distribution • LBL: baseline + syntactic labels • LBL+LEN: baseline + syntactic labels + length distribution • LBL+LEN+CLM: baseline + syntactic labels + length distribution + context LM All the models were optimized on lower-cased IBM BLEU with Powell’s method (Powell, 1964; Brent, 1973) on n-best translations (Ostendorf et al., 1991), but evaluated on both IBM BLEU and 77 Model BASE CLM LEN LBL LBL+LEN LBL+LEN+CLM BASE CLM LEN LBL LBL+LEN LBL+LEN+CLM MT06 MT08 BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80 Rescoring (5-gram LM) 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15 51.57 49.54 41.74 43.88 5"
D09-1008,N09-1025,0,0.113228,"g, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for M"
D09-1008,2001.mtsummit-papers.68,0,0.0209286,"a penalty.) • It provides a strong baseline, which ensures the validity of the improvement we would obtain. The baseline model used in this paper showed state-of-the-art performance at NIST 2008 MT evaluation. • The baseline algorithm can be easily extended to incorporate the features proposed in this paper. The use of source dependency structures is a natural extension of the stringto-tree model to a tree-to-tree model. To ensure the generality of our results, we tested the features on two rather different language pairs, Arabic-to-English and Chinese-to-English, using two metrics, IBM BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). Our experiments show that each of the first three features: nonterminal labels, length distribution and source side context, improves MT performance. Surprisingly, the source dependency feature does not produce an improvement. 2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model (Shen et al., 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. A well-formed dependency structure could be either a single-rooted depe"
D09-1008,P05-1033,0,0.22483,"phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by Surprisingly, source dependency LM did not provide any improvement over the baseline. There are two possible reasons for this. One is that the source and target parse trees were generated by two stand-alone parsers, which may cause incompatible structures on the source and target sides. By applying the well-formed constraints 78 Model BASE SLM CLM LEN LBL LBL+LEN LBL+LEN+CLM BASE SLM CLM LEN LBL LBL+LEN LBL+LEN+CLM MT06 MT08 BLEU TER BLEU TER lower mixed lower mi"
D09-1008,2008.amta-papers.16,0,0.0229043,"ic specific over-tuning. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by Surprisingly, source dependency LM did not provide any improvement over the baseline. There are two possible reasons for this. One is that the source and target parse trees were generated by two stand-alone parsers, which may cause incompatible structur"
D09-1008,J07-2003,0,0.129407,"of the first three features: nonterminal labels, length distribution and source side context, improves MT performance. Surprisingly, the source dependency feature does not produce an improvement. 2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model (Shen et al., 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. A well-formed dependency structure could be either a single-rooted dependency tree or a set of sibling trees. As in the Hiero system (Chiang, 2007), there is only one non-terminal X in the string-to-dependency model. Any sub dependency structure can be used to replace a nonterminal in a rule. For example, we have a source sentence in Chinese as follows. 2.2 Length Distribution In English, the length of a phrase may determine the syntactic structure of a sentence. For example, possessive relations can be represented either as “A’s B” or “B of A”. The former is preferred if A is a short phrase (e.g. “the boy’s mother”) while the latter is preferred if A is a complex structure (e.g. “the mother of the boy who is sick”). Our solution is to b"
D09-1008,P06-1121,0,0.0215521,"r re-scoring, the improvements became smaller, but still noticeable, ranging from 0.7 to 1.4. TER scores were also improved noticeably for all conditions, suggesting there was no metric specific over-tuning. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by Surprisingly, source dependency LM did not provide any improvement ov"
D09-1008,N04-1023,1,0.61861,"guistic and Contextual Information for Statistical Machine Translation Libin Shen and Jinxi Xu and Bing Zhang and Spyros Matsoukas and Ralph Weischedel BBN Technologies Cambridge, MA 02138, USA {lshen,jxu,bzhang,smatsouk,weisched}@bbn.com Abstract 1.1 Previous Work The ideas of using labels, length preference and source side context in MT decoding were explored previously. Broadly speaking, two approaches were commonly used in existing work. One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al., 2006; Tillmann and Zhang, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features"
D09-1008,P08-1066,1,0.72634,"or the source and the other for the target, for scoring translation hypotheses. Our intuition is that the likelihood of source structures provides another piece of evidence about the plausibility of a translation hypothesis and as such would help weed out bad ones. 1 According to footnote 2 of (Ittycheriah and Roukos, 2007), test set adaptation by test set sampling of the training corpus showed an advantage of more than 2 BLEU points over a general system trained on all data. 73 1.2.2 Baseline System and Experimental Setup We take BBN’s HierDec, a string-to-dependency decoder as described in (Shen et al., 2008), as our baseline for the following two reasons: A single source word can be translated into many English words. For example, jiantao can be translated into a review, the review, reviews, the reviews, reviewing, reviewed, etc. Suppose we have source-string-to-target-dependency translation rules as shown in Figure 1. Since there is no constraint on substitution, any translation for jiantao could replace the X-1 slot. One way to alleviate this problem is to limit the search space by using a label system. We could assign a label to each non-terminal on the target side of the rules. Furthermore, w"
D09-1008,D08-1039,0,0.095262,"Missing"
D09-1008,2006.amta-papers.25,0,0.0142926,"ng baseline, which ensures the validity of the improvement we would obtain. The baseline model used in this paper showed state-of-the-art performance at NIST 2008 MT evaluation. • The baseline algorithm can be easily extended to incorporate the features proposed in this paper. The use of source dependency structures is a natural extension of the stringto-tree model to a tree-to-tree model. To ensure the generality of our results, we tested the features on two rather different language pairs, Arabic-to-English and Chinese-to-English, using two metrics, IBM BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). Our experiments show that each of the first three features: nonterminal labels, length distribution and source side context, improves MT performance. Surprisingly, the source dependency feature does not produce an improvement. 2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model (Shen et al., 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. A well-formed dependency structure could be either a single-rooted dependency tree or a set of siblin"
D09-1008,C08-1041,0,0.518934,"y limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. He et al. (2008) extended the WSD-like approached proposed in (Carpuat and Wu, 2007) to hierarchical decoders. In (He et al., 2008), lexical Current methods of using lexical features in machine translation have difficulty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper, we propose methods of using new linguistic and"
D09-1008,P06-1091,0,0.0114133,"Statistical Machine Translation Libin Shen and Jinxi Xu and Bing Zhang and Spyros Matsoukas and Ralph Weischedel BBN Technologies Cambridge, MA 02138, USA {lshen,jxu,bzhang,smatsouk,weisched}@bbn.com Abstract 1.1 Previous Work The ideas of using labels, length preference and source side context in MT decoding were explored previously. Broadly speaking, two approaches were commonly used in existing work. One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al., 2006; Tillmann and Zhang, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang e"
D09-1008,N07-1008,0,0.287886,"se. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. He et al. (2008) extended the WSD-like approached proposed in (Carpuat and Wu, 2007) to hierarchical decoders. In (He et al., 2008), lexical Current methods of using lexical features in machine translation have difficulty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper, we propose methods of using n"
D09-1008,N07-1063,0,0.0179733,"Missing"
D09-1008,D07-1091,0,0.0371564,"wer-cased BLEU by 2.0 on MT06 and 1.7 on MT08 on decoding output. For Chinese-to-English MT, the improvements in lower-cased BLEU were 1.0 on MT06 and 0.8 on MT08. After re-scoring, the improvements became smaller, but still noticeable, ranging from 0.7 to 1.4. TER scores were also improved noticeably for all conditions, suggesting there was no metric specific over-tuning. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of"
D09-1008,D07-1077,0,0.0301588,"bined the three features, we observed significant improvements over the baseline. For Arabic-to-English MT, the LBL+LEN+CLM system improved lower-cased BLEU by 2.0 on MT06 and 1.7 on MT08 on decoding output. For Chinese-to-English MT, the improvements in lower-cased BLEU were 1.0 on MT06 and 0.8 on MT08. After re-scoring, the improvements became smaller, but still noticeable, ranging from 0.7 to 1.4. TER scores were also improved noticeably for all conditions, suggesting there was no metric specific over-tuning. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton"
D09-1008,P06-1096,0,0.057519,"Missing"
D09-1008,P02-1040,0,\N,Missing
D10-1060,W05-0909,0,0.0531557,"Missing"
D10-1060,D09-1021,0,0.0129505,"bed in Section 3.2. Of course, one can use other linguistic resources if similar family information is provided, e.g. VerbNet (Kipper et al., 2006) or WordNet (Fellbaum, 1998). 5 Implementation Nowadays, machine translation systems become more and more complicated. It takes time to write a decoder from scratch and hook it with various modules, so it is not the best solution for research purpose. A common practice is to reduce a new translation model to an old one, so that we can use an existing system, and see the effect of the new model quickly. For example, the tree-based model proposed in (Carreras and Collins, 2009) used a phrasal decoder for sub-clause translation, and recently, DeNeefe and Knight (2009) reduced a TAGbased translation model to a CFG-based model by applying all possible adjunction operations offline and stored the results as rules, which were then used by an existing syntax-based decoder. Here, we use a similar method. Instead of building a new decoder that uses factorized grammars, we reduce factorized rules to baseline string-todependency rules by performing combination of templates and lexical items in an offline mode. This is similar to the rule generation method in (DeNeefe and Knig"
D10-1060,P05-1033,0,0.0846386,"torized grammars, an idea widely accepted in the field of linguistic grammar construction, to generalize translation rules, so as to solve these two problems. We designed a method to take advantage of the XTAG English Grammar to facilitate the extraction of factorized rules. We experimented on various setups of low-resource language translation, and showed consistent significant improvement in BLEU over state-ofthe-art string-to-dependency baseline systems with 200K words of bi-lingual training data. 1 Introduction A statistical phrasal (Koehn et al., 2003; Och and Ney, 2004) or hierarchical (Chiang, 2005; Marcu et al., 2006) machine translation system usually relies on a very large set of translation rules extracted from bi-lingual training data with heuristic methods on word alignment results. According to our own experience, we obtain about 200GB of rules from training data of about 50M words on each side. This immediately becomes an engineering challenge on space and search efficiency. A common practice to circumvent this problem is to filter the rules based on development sets in the step of rule extraction or before the decoding phrase, instead of building a real distributed system. Howe"
D10-1060,D09-1076,0,0.0172613,"mation is provided, e.g. VerbNet (Kipper et al., 2006) or WordNet (Fellbaum, 1998). 5 Implementation Nowadays, machine translation systems become more and more complicated. It takes time to write a decoder from scratch and hook it with various modules, so it is not the best solution for research purpose. A common practice is to reduce a new translation model to an old one, so that we can use an existing system, and see the effect of the new model quickly. For example, the tree-based model proposed in (Carreras and Collins, 2009) used a phrasal decoder for sub-clause translation, and recently, DeNeefe and Knight (2009) reduced a TAGbased translation model to a CFG-based model by applying all possible adjunction operations offline and stored the results as rules, which were then used by an existing syntax-based decoder. Here, we use a similar method. Instead of building a new decoder that uses factorized grammars, we reduce factorized rules to baseline string-todependency rules by performing combination of templates and lexical items in an offline mode. This is similar to the rule generation method in (DeNeefe and Knight, 2009). The procedure is as follows. In the rule extraction phase, we first extract all"
D10-1060,D07-1091,0,0.0116906,"ploys factorization with LTAG e-tree templates and lexical items. Factorized grammars not only relieve the burden on space and search, but also alleviate the sparse data problem, especially for low-resource language translation with few training data. With a factored model, we do not need to observe exact “template – lexical item” occurrences in training. New rules can be generated from template families and lexical items either offline or on the fly, explicitly or implicitly. In fact, the factorization approach has been successfully applied on the morphological level in previous study on MT (Koehn and Hoang, 2007). In this work, we will go further to investigate factorization of rule structures by exploiting the rich XTAG English Grammar. We evaluate the effect of using factorized translation grammars on various setups of low-resource language translation, since low-resource MT suffers greatly on poor generalization capability of trans616 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616–625, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics lation rules. With the help of high-level linguistic knowledge for gener"
D10-1060,N03-1017,0,0.0294532,"Missing"
D10-1060,W04-3250,0,0.0911921,"Missing"
D10-1060,W06-1606,0,0.0534777,"rs, an idea widely accepted in the field of linguistic grammar construction, to generalize translation rules, so as to solve these two problems. We designed a method to take advantage of the XTAG English Grammar to facilitate the extraction of factorized rules. We experimented on various setups of low-resource language translation, and showed consistent significant improvement in BLEU over state-ofthe-art string-to-dependency baseline systems with 200K words of bi-lingual training data. 1 Introduction A statistical phrasal (Koehn et al., 2003; Och and Ney, 2004) or hierarchical (Chiang, 2005; Marcu et al., 2006) machine translation system usually relies on a very large set of translation rules extracted from bi-lingual training data with heuristic methods on word alignment results. According to our own experience, we obtain about 200GB of rules from training data of about 50M words on each side. This immediately becomes an engineering challenge on space and search efficiency. A common practice to circumvent this problem is to filter the rules based on development sets in the step of rule extraction or before the decoding phrase, instead of building a real distributed system. However, this strategy on"
D10-1060,J04-4002,0,0.0586702,"In this paper, we propose to use factorized grammars, an idea widely accepted in the field of linguistic grammar construction, to generalize translation rules, so as to solve these two problems. We designed a method to take advantage of the XTAG English Grammar to facilitate the extraction of factorized rules. We experimented on various setups of low-resource language translation, and showed consistent significant improvement in BLEU over state-ofthe-art string-to-dependency baseline systems with 200K words of bi-lingual training data. 1 Introduction A statistical phrasal (Koehn et al., 2003; Och and Ney, 2004) or hierarchical (Chiang, 2005; Marcu et al., 2006) machine translation system usually relies on a very large set of translation rules extracted from bi-lingual training data with heuristic methods on word alignment results. According to our own experience, we obtain about 200GB of rules from training data of about 50M words on each side. This immediately becomes an engineering challenge on space and search efficiency. A common practice to circumvent this problem is to filter the rules based on development sets in the step of rule extraction or before the decoding phrase, instead of building a"
D10-1060,2001.mtsummit-papers.68,0,0.0198736,"by exploiting the rich XTAG English Grammar. We evaluate the effect of using factorized translation grammars on various setups of low-resource language translation, since low-resource MT suffers greatly on poor generalization capability of trans616 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616–625, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics lation rules. With the help of high-level linguistic knowledge for generalization, factorized grammars provide consistent significant improvement in BLEU (Papineni et al., 2001) over string-todependency baseline systems with 200K words of bi-lingual training data. This work also closes the gap between compact hand-crafted translation rules and large-scale unorganized automatic rules. This may lead to a more effective and efficient statistical translation model that could better leverage generic linguistic knowledge in MT. In the rest of this paper, we will first provide a short description of our baseline system in Section 2. Then, we will introduce factorized translation grammars in Section 3. We will illustrate the use of the XTAG English Grammar to facilitate the"
D10-1060,P08-1066,1,0.875163,"ilable directly from the training data. However, if we obtain the three rules in Figure 1, we are able to predict this missing rule. Furthermore, if we know like and hate are in the same syntactic/semantic class in the source or target language, we will be very confident on the validity of this hypothesis rule. Now, we propose a factorized grammar to solve this generalization problem. In addition, translation rules represented with the new formalism will be more compact. A Baseline String-to-Tree Model As the baseline of our new algorithm, we use a string-to-dependency system as described in (Shen et al., 2008). There are several reasons why we take this model as our baseline. First, it uses syntactic tree structures on the target side, which makes it easy to exploit linguistic information. Second, dependency structures are relatively easier to implement, as compared to phrase structure grammars. Third, a string-to-dependency system provides state-of-theart performance on translation accuracy, so that improvement over such a system will be more convincing. Here, we provide a brief description of the baseline string-to-dependency system, for the sake of completeness. Readers can refer to (Shen et al."
D10-1060,D09-1008,1,0.830299,"are several reasons why we take this model as our baseline. First, it uses syntactic tree structures on the target side, which makes it easy to exploit linguistic information. Second, dependency structures are relatively easier to implement, as compared to phrase structure grammars. Third, a string-to-dependency system provides state-of-theart performance on translation accuracy, so that improvement over such a system will be more convincing. Here, we provide a brief description of the baseline string-to-dependency system, for the sake of completeness. Readers can refer to (Shen et al., 2008; Shen et al., 2009) for related information. In the baseline string-to-dependency model, each translation rule is composed of two parts, source and target. The source sides is a string rewriting rule, and the target side is a tree rewriting rule. Both sides can contain non-terminals, and source and target non-terminals are one-to-one aligned. Thus, in the decoding phase, non-terminal replacement for both sides are synchronized. Decoding is solved with a generic chart parsing 617 3 3.1 Translation with a Factorized Grammar Factorized Rules We decompose a translation rule into two parts, a pair of lexical items an"
D10-1060,J93-2004,0,\N,Missing
D10-1060,P02-1040,0,\N,Missing
D11-1133,P06-1017,0,0.0527744,"Missing"
D11-1133,W10-0908,1,0.795121,"ences2 (MUC), teams typically had a year or more from release of the target to submitting system results. One exception was MUC-6 (Grishman & Sundheim, 1996), in which scenario templates for changing positions were extracted given only one month. Our goal was to confine development to a calendar week, in fact, &lt;50 person hours. This 1 2 http://www.nist.gov/speech/tests/ace/ http://www-nlpir.nist.gov/related_projects/muc/   a variant of (Miller, et al., 2004) to learn two new classes of entities via automatically induced word classes and active learning (6 hours) bootstrap relation learning (Freedman et al, 2010) to learn 5 new relation classes (2.5 hours), handwritten patterns over predicate-argument structure (5 hours), and coreference (20 hours) Our bootstrap learner is initialized with relation tuples (not annotated text) and uses LDC‘s Gigaword and Wikipedia as a background corpus to learn patterns for relation detection that are based on normalized predicate argument structure as well as surface strings. These early empirical results suggest the following: (1) It is possible to specify a domain, adapt our system, and complete manual scoring, includ1437 Proceedings of the 2011 Conference on Empir"
D11-1133,C96-1079,0,0.288437,"learning, bootstrapping, and limited (5 hours) manual rule writing. We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination. We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1. 1   Introduction Throughout the Automatic Content Extraction 1 (ACE) evaluations and the Message Understanding Conferences2 (MUC), teams typically had a year or more from release of the target to submitting system results. One exception was MUC-6 (Grishman & Sundheim, 1996), in which scenario templates for changing positions were extracted given only one month. Our goal was to confine development to a calendar week, in fact, &lt;50 person hours. This 1 2 http://www.nist.gov/speech/tests/ace/ http://www-nlpir.nist.gov/related_projects/muc/   a variant of (Miller, et al., 2004) to learn two new classes of entities via automatically induced word classes and active learning (6 hours) bootstrap relation learning (Freedman et al, 2010) to learn 5 new relation classes (2.5 hours), handwritten patterns over predicate-argument structure (5 hours), and coreference (20 hour"
D11-1133,N10-1087,0,0.0157878,"Missing"
D11-1133,P08-1119,0,0.0658189,"all of our relations included at least one novel class. While MUC-6 systems tended to use finite-state patterns, they did not incorporate bootstrapping or patterns based on the output of a statistical parser. 3 4 For learning entity classes, we follow Miller, et al., (2004), using word clustering and active learning to train a perceptron model, but unlike that work we apply the technique not just to names but also to descriptions. An alternative approach to learning classes, applying structural patterns to bootstrap description recognition without active learning, is seen in Riloff (1996) and Kozareva et al., (2008) Much research (e.g. Ramshaw 2001) has focused on learning relation extractors using large amounts of supervised training, as in ACE. The obvious weakness of such approaches is the resulting reliance on manually annotated examples, which are expensive and time-consuming to create. Others have explored bootstrap relation learning from seed examples. Agichtein & Gravano (2000) and Ravichandran & Hovy (2002) reported results for generating surface patterns for relation identification; others have explored similar approaches (e.g. Pantel & Pennacchiotti, 2006). Mitchell et al. (2009) showed that f"
D11-1133,N04-1043,0,0.113355,"to improve the overall recall and F1. 1   Introduction Throughout the Automatic Content Extraction 1 (ACE) evaluations and the Message Understanding Conferences2 (MUC), teams typically had a year or more from release of the target to submitting system results. One exception was MUC-6 (Grishman & Sundheim, 1996), in which scenario templates for changing positions were extracted given only one month. Our goal was to confine development to a calendar week, in fact, &lt;50 person hours. This 1 2 http://www.nist.gov/speech/tests/ace/ http://www-nlpir.nist.gov/related_projects/muc/   a variant of (Miller, et al., 2004) to learn two new classes of entities via automatically induced word classes and active learning (6 hours) bootstrap relation learning (Freedman et al, 2010) to learn 5 new relation classes (2.5 hours), handwritten patterns over predicate-argument structure (5 hours), and coreference (20 hours) Our bootstrap learner is initialized with relation tuples (not annotated text) and uses LDC‘s Gigaword and Wikipedia as a background corpus to learn patterns for relation detection that are based on normalized predicate argument structure as well as surface strings. These early empirical results suggest"
D11-1133,H01-1024,1,0.824277,"Missing"
D11-1133,P02-1006,0,0.323821,"results for generating surface patterns for relation identification; others have explored similar approaches (e.g. Pantel & Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macro-reading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. None use coreference to find training examples; all use surface (word) patterns. Freedman et. al (2010) report improved performance from using predicate structure for bootstrappped relation learning. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in TREC QA, where extracting one instance of a relation can be sufficient, rather than detecting all instances. Mitchell et al. (2009), while demonstrating high precision, do not measure recall. By contrast, our work emphasizes recall, not just precision. Our question answering task asks list-like questions that require multiple answers. We also include the results of a secondary, extraction evaluation which requires that the system identify every mention of the relations in a small set of documents. This evaluation is loosely based on the relation mention detection task in ACE"
D11-1133,I08-1005,0,0.0459649,"Missing"
D11-1133,P06-1015,0,\N,Missing
H01-1024,A00-2030,1,\N,Missing
H01-1027,A00-2030,1,\N,Missing
H05-1039,H01-1024,1,\N,Missing
H05-1039,P02-1005,0,\N,Missing
H05-1039,H05-1117,0,\N,Missing
H05-1039,N03-1020,0,\N,Missing
H05-1039,C04-1188,0,\N,Missing
H05-1039,P04-1078,0,\N,Missing
H05-1039,P02-1006,0,\N,Missing
H05-1082,A00-2030,1,0.881672,"Missing"
H05-1082,H01-1027,1,0.823597,"Missing"
H86-1001,P84-1085,1,\N,Missing
H86-1001,P86-1036,1,\N,Missing
H86-1001,H86-1008,1,\N,Missing
H86-1007,J83-3004,0,\N,Missing
H86-1007,J83-3003,1,\N,Missing
H86-1007,C80-1027,0,\N,Missing
H86-1007,P86-1036,0,\N,Missing
H86-1007,H86-1008,0,\N,Missing
H86-1007,P82-1010,0,\N,Missing
H86-1017,P84-1030,1,0.89698,"e should be ""'You can't drop 577; Pi isn~ true."" Alternatively, the language generator might paraphrase the whole response as, ""if Pi were true, you could drop."" Of course there are potentially many ways to try to achieve a goal: by a single action, by a single event, or by an event and an action .... In fact, the search for a sequence of events or actions that would achieve the goal may consider many alternatives. If all fail, it is far from obvious which blocked condition to notify Q of, and knowledge is needed to guide the choice. Some heuristics for dealing with that problem ~ .. given in [12]. 3.2. A n n o n p r o d u c t i v e a c t Suppose the proposed action does not achieve Q's l-goal, cL [6]. For example, dropping the course may still mean that failing status would be recorded as a WF (withdrawal while failing). R may initially plan to answer ""You can drop 577 by ...'. However, Q would expect to be told that his proposed action does not achieve his l-goal. Formula [7] states R's belief about this expectation. [6] RB(-holds(-fail(Q,C), drop(Q,C](Sc)) & admissible(drop(Q,C}(Sc)) ) [7] RBQB(RB[ want(Q,-,fail(Q,c)) & -,holds(-fail(Q,C),drop(Q,C](Sc)) I~ admissible( drop (Q, C]( S"
H86-1017,P84-1029,1,\N,Missing
H89-1013,P86-1005,0,0.0628317,"Missing"
H89-1013,P87-1005,1,\N,Missing
H89-2078,1997.mtsummit-papers.6,0,0.0773734,"Missing"
H90-1069,W89-0240,0,0.0236451,"written in any of a few dozen languages. One of the key issues in building systems with this scale of competence is handling large numbers of different words and word senses. Natural language understanding systems today are typically limited to vocabularies of less than 10,000 words; tomorrow's systems will need vocabularies at least 5 times that to effectively handle the volume and diversity of messages needing to be processed. One method of handling large vocabularies is simply increasing the size of the lexicon. Research efforts at IBM [Chodorow, et al. 1988; Neff, et al. 1989], Bell Labs [Church, et al. 1989], New Mexico State University [Wilks 1987], and elsewhere have used mechanical processing of on-line dictionaries to infer at least minimal syntactic and semantic information from dictionary definitions. However, even assuming a very large lexicon already exists, it can never be complete. Systems aiming for coverage of unrestricted language in broad domains must continually deal with new words and novel word senses. Systems with very large lexicons have the additional problems of an exploding search space, of disambiguating multiple syntactic and semantic possibilities when full interpretatio"
H90-1069,P90-1031,0,0.248266,"Missing"
H90-1069,P89-1012,0,0.0512432,"Missing"
H90-1069,H89-2006,0,0.121448,"nique mples in our training set, so the rule of thumb would suggest that 160,000 words would be sufficient training. This would explain why the degradation in performance was slight when the size of the corpus was reduced. The benefits of probabilistic modeling therefore seem applicable to new tag sets, subdomains, or languages without needing prohibitively large corpora. 3. Probabilistic Language Model Probabilities can also quantify the likelihoods of alternative complete interpretations of a sentence. In these experiments, we used the grammar of the Delphi component from BBN's HARC system [Stallard 1989], which combines syntax and semantics in a unification formalism. We developed a context-free model, which estimates the probability of each rule in the grammar independently (in contrast to a context-sensitive model, such as the tri-tag model described above, which bases the probability of a tag on what other tags are in the adjacent context). test set by a factor of two to four, compared to random selection from the interpretations satisfying all knowledgebased constraints. We tested the predictive power of rule probabilities using this model both in unsupervised and in supervised mode. In"
H90-1069,H90-1053,0,0.0896851,"m. The detailed results from using a training set of 81 sentences appear in the histogram in Figure 2. In our context-free model, we associate a probability with each rule of the grammar. For each distinct major category (left-hand side) of the grammar, there is a set of context-free rules 30 .-.-.... 12°&quot;- LHS &lt;- RHS 1 LHS &lt;- RHS2 ~10LHS &lt;- RHSn. For each rule, we estimate the probability of the right-hand side given the left-hand side. 0 TEST 1 The probability of a syntactic structure S, given the input string W, is then modelled by the product of the probabilities of the rules used in S. ([Chitrao & Grishman 1990] used a similar context-free model.) Using this model, we explored the following issues: What method of training the rule probabilities should be employed? How much (little) training data is required for reliable estimates? • How is system performance impacted? • Do the results suggest refinements in the probability model? Our intention is to use the Treebank corpus being developed at the University of Pennsylvania as a source of correct structures for training. However, until that material becomes available, we have run initial experiments using small training sets taken from an existing que"
H90-1069,A88-1020,0,0.013844,"mmonly called &quot;messages&quot;, from highly diverse sources written in any of a few dozen languages. One of the key issues in building systems with this scale of competence is handling large numbers of different words and word senses. Natural language understanding systems today are typically limited to vocabularies of less than 10,000 words; tomorrow's systems will need vocabularies at least 5 times that to effectively handle the volume and diversity of messages needing to be processed. One method of handling large vocabularies is simply increasing the size of the lexicon. Research efforts at IBM [Chodorow, et al. 1988; Neff, et al. 1989], Bell Labs [Church, et al. 1989], New Mexico State University [Wilks 1987], and elsewhere have used mechanical processing of on-line dictionaries to infer at least minimal syntactic and semantic information from dictionary definitions. However, even assuming a very large lexicon already exists, it can never be complete. Systems aiming for coverage of unrestricted language in broad domains must continually deal with new words and novel word senses. Systems with very large lexicons have the additional problems of an exploding search space, of disambiguating multiple syntacti"
H90-1069,A88-1019,0,0.452159,"Missing"
H90-1069,H89-2012,0,\N,Missing
H90-1069,H89-1009,0,\N,Missing
H91-1037,H89-1013,1,0.892892,"Missing"
H91-1037,H90-1069,1,0.876815,"Missing"
H91-1037,P87-1005,1,0.839201,"the norm will be that there are several ways to combine a pair of fragments, we plan to test several alternative heuristics for ranking the alternatives. Probabilistic methods seem particularly powerful and appropriate. Thus far, we have tested this hypothesis on propositional phrase attachment. Such semantic kffowledge called selection restrictions or case frames governs what phrases make sense with a particular verb or noun (what arguments go with a particular verb or noun). Traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (Ayuso et al., 1987; Bates, 1989; Grishman et al., 1986; Weischedel, et al., 1989). Simple manual semantic annotation, 2. Supervised training based on parsed sentences, 3. Estimation of probabilities. From the example one can clearly infer that bombs can explode, or more properly, that bomb can be the logical subject of explode, that at dawn can modify explode, etc. Naturally good generalizations based on the instances are more valuable than the instances themselves. Since we have a hierarchical domain model, and since the manual semantic annotation states the relationship between lexical items and concepts in t"
H91-1037,H89-1008,0,0.0168423,"t there are several ways to combine a pair of fragments, we plan to test several alternative heuristics for ranking the alternatives. Probabilistic methods seem particularly powerful and appropriate. Thus far, we have tested this hypothesis on propositional phrase attachment. Such semantic kffowledge called selection restrictions or case frames governs what phrases make sense with a particular verb or noun (what arguments go with a particular verb or noun). Traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (Ayuso et al., 1987; Bates, 1989; Grishman et al., 1986; Weischedel, et al., 1989). Simple manual semantic annotation, 2. Supervised training based on parsed sentences, 3. Estimation of probabilities. From the example one can clearly infer that bombs can explode, or more properly, that bomb can be the logical subject of explode, that at dawn can modify explode, etc. Naturally good generalizations based on the instances are more valuable than the instances themselves. Since we have a hierarchical domain model, and since the manual semantic annotation states the relationship between lexical items and concepts in the domain mod"
H91-1037,A88-1019,0,0.220854,"Missing"
H91-1037,P90-1031,0,0.133113,"Missing"
H91-1037,H89-2011,0,0.100376,"Missing"
H91-1037,J86-3002,0,0.0300358,"everal ways to combine a pair of fragments, we plan to test several alternative heuristics for ranking the alternatives. Probabilistic methods seem particularly powerful and appropriate. Thus far, we have tested this hypothesis on propositional phrase attachment. Such semantic kffowledge called selection restrictions or case frames governs what phrases make sense with a particular verb or noun (what arguments go with a particular verb or noun). Traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (Ayuso et al., 1987; Bates, 1989; Grishman et al., 1986; Weischedel, et al., 1989). Simple manual semantic annotation, 2. Supervised training based on parsed sentences, 3. Estimation of probabilities. From the example one can clearly infer that bombs can explode, or more properly, that bomb can be the logical subject of explode, that at dawn can modify explode, etc. Naturally good generalizations based on the instances are more valuable than the instances themselves. Since we have a hierarchical domain model, and since the manual semantic annotation states the relationship between lexical items and concepts in the domain model, we can use the doma"
H91-1037,H90-1052,0,0.0122022,"ining algorithm. 1. However, the degree of reduction of error rate should not be taken as the final word, for the following reasons: 2. 20,000 words of training data is much less than one would want. An additional 70,000 words of training data should soon be available through TREEBANK. Since many of the head words in the 20,000 word corpus are not of import in the MUC-3 domain, their semantic type is vague, i.e., &lt;unknown event&gt;, &lt;unknown entity&gt;, etc. Related Work In addition to the work discussed earlier on tools to increase the portability of natural language systems, another recent paper (Hindle and Rooth, 1990) is directly related to our goal of inferring case frame information from examples. Hindle and Rooth focussed only on prepositional phrase attachment using a probabilistic model, whereas our work applies to all case relations. Their work used an unsupervised training corpus of 13 million words to judge the strength of prepositional affinity to verbs, e.g., how likely it is for to to attach to the word go, for from to attach to the word leave, or for to to attach to the word flight. This lexical affinity is measured independent of the object of the preposition. By contrast, we are exploring ind"
H91-1037,H89-2006,0,0.0255255,"Missing"
H91-1037,W89-0240,0,\N,Missing
H91-1037,H89-2012,0,\N,Missing
H91-1037,H91-1065,1,\N,Missing
H91-1065,H90-1078,1,0.773244,"Missing"
H91-1065,H90-1069,1,0.904241,"Missing"
H91-1065,A88-1019,0,0.193383,"2, P a r l a n c e ruz, and Delphi 4, use these techniques quite successfully. However, as we move from the problem of understanding queries in fixed domains to processing open text for applications such as data extraction, we have found rule-based techniques too brittle, and the amount of work necessary to build them intractable, especially when attempting to use the same system on multiple domains. We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text. The effectiveness of such models is well known (Church 1988) and they are currently in use in parsers (e.g. de Marcken 1990). Our work is an incremental improvement on these models in two ways: (1) We have run experiments regarding the amount of training data needed in moving to a new domain; (2) we have added probabilistic models of word features to handle unknown words effectively. We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments. 1 The work reported here was supported by the Advanced Research Projects Agency and was monitored by the Rome Air Development Center under Contract No. F30602-8"
H91-1065,P90-1031,0,0.35487,"Missing"
H91-1065,H89-2011,0,0.0669481,"Missing"
H91-1065,H89-2006,0,0.0264641,"en we describe our extensions, showing the results of our experiments. 1 The work reported here was supported by the Advanced Research Projects Agency and was monitored by the Rome Air Development Center under Contract No. F30602-87-D-0093. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, whether expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government. 2 Weischedel, et al. 1989. 3 Parlance is a trademark of BBN Systems and Technologies. 4 Stallard, 1989. 331 2. POST: USING PROBABILITIES TO TAG PART OF SPEECH Predicting the part of speech o f a word is one straightforward way to use probabilities. Many words are several ways ambiguous, such as the following: a round table: adjective a round of cheese: noun to round out your interests: verb to work the year round: adverb Even in context, part of speech can be ambiguous, as in the famous example: &quot;Time flies.&quot; where both words are two ways a m b i g u o u s , r e s u l t i n g in two g r a m m a t i c a l interpretations as sentences. Models predicting part of speech can serve to cut down the s"
H91-1065,H89-1013,1,0.766068,"atures to handle unknown words effectively. We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments. 1 The work reported here was supported by the Advanced Research Projects Agency and was monitored by the Rome Air Development Center under Contract No. F30602-87-D-0093. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, whether expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government. 2 Weischedel, et al. 1989. 3 Parlance is a trademark of BBN Systems and Technologies. 4 Stallard, 1989. 331 2. POST: USING PROBABILITIES TO TAG PART OF SPEECH Predicting the part of speech o f a word is one straightforward way to use probabilities. Many words are several ways ambiguous, such as the following: a round table: adjective a round of cheese: noun to round out your interests: verb to work the year round: adverb Even in context, part of speech can be ambiguous, as in the famous example: &quot;Time flies.&quot; where both words are two ways a m b i g u o u s , r e s u l t i n g in two g r a m m a t i c a l interpretatio"
H92-1063,P88-1012,0,0.0443852,"Missing"
H92-1063,H91-1065,1,0.868039,"Missing"
H92-1063,H91-1037,1,0.879452,"ambiguous even regarding part of speech. In the Foreign Broadcast Information Service texts of MUC-3, we estimate that the vocabulary had an average ambiguity of over two parts of speech in the TREEBANK tag system. The PLUM (Probabilistic Language Understanding Model) natural language understanding system for extracting data from text is based on three unusual features: probabilistic language models, a domain-independent deterministic parser, and processing of (the resulting) fragments at the semantic and discourse level. Earlier papers have focused on the probabilistic aspects of the system [Weischedel et al., 1991; de Marcken, 1990]; here we focus on the other two design features. age Morphological Analyzer Fast Partial Parser While several deterministic parsers have been constructed based on Marcus's Determinism Hypothesis, PLUM seems to be the first application system that employs a deterministic parser. Many systems have been built based on semantic and discourse-level processing of fragments, most notably systems based on conceptual dependency and scripts [Schank and Riesbeck, 1981]. However, PLUM may be the first system that uses a hybnd of such semantic techniques with the purely syntactic proces"
H92-1063,H91-1066,0,\N,Missing
H92-1063,P90-1031,0,\N,Missing
H93-1045,H92-1022,0,0.0335364,"Missing"
H93-1045,P90-1031,0,0.180309,"Missing"
H93-1045,J88-1003,0,0.0148664,"ts and their part of speech in Japanese text. Rather than using hand-crafted rules, the algorithm employs example data, drawing generalizations during training. A rule-based Japanese morphological processor (JUMAN) from Kyoto University. A context-free grammar of Japanese based on part of speech labels distinct from those produced by JUMAN. - - A probabilistic part-of-speech tagger (POST) [Meteer, et al., 1991] which assumed a single sequence of words as input. 1. INTRODUCTION - Probabilistic part of speech taggers have proven to be successful in English part of speech labelling [Church 1988; DeRose, 1988; de Marcken, 1990; Meteer, et. al. 1991, etc.]. Such stochastic models perform very well given adequate amounts of training data representative of operational data. Instead of merely stating what is possible, as a non-stochastic rule-based model does, probabilistic models predict the likelihood of an event. In determining the part of speech of a highly ambiguous word in context or in determining the part of speech of an unknown word, they have proven quite effective for English. Limited human resources for creating training data. This presented us with four issues: 1) how to reduce the cost o"
H93-1045,H91-1065,1,0.948836,"tical algorithms rely on frequency of phenomena or events in corpora; however, low frequency events are often inadequately represented. Here we report on an examplebased technique used in finding word segments and their part of speech in Japanese text. Rather than using hand-crafted rules, the algorithm employs example data, drawing generalizations during training. A rule-based Japanese morphological processor (JUMAN) from Kyoto University. A context-free grammar of Japanese based on part of speech labels distinct from those produced by JUMAN. - - A probabilistic part-of-speech tagger (POST) [Meteer, et al., 1991] which assumed a single sequence of words as input. 1. INTRODUCTION - Probabilistic part of speech taggers have proven to be successful in English part of speech labelling [Church 1988; DeRose, 1988; de Marcken, 1990; Meteer, et. al. 1991, etc.]. Such stochastic models perform very well given adequate amounts of training data representative of operational data. Instead of merely stating what is possible, as a non-stochastic rule-based model does, probabilistic models predict the likelihood of an event. In determining the part of speech of a highly ambiguous word in context or in determining t"
H93-1045,H92-1063,1,\N,Missing
H93-1045,A88-1019,0,\N,Missing
I17-1068,P81-1022,0,0.210714,"Missing"
I17-1068,S10-1006,0,0.400818,"e are not aware of prior work on bilingual relation extraction in similar settings, we first benchmark our baseline monolingual model on two popular monolingual datasets. We also use the ACE development dataset (described below) for tuning the parameters mentioned previously. The baseline monolingual model is similar to (Nguyen and Grishman, 2015) and only takes a English dataset as input. It is simplified from the model in Figure 1 by replacing the F C and LC layers with a fullyconnected layer followed by a softmax loss. We evaluate it on two English datasets: the SemEval2010 Task 8 dataset (Hendrickx et al., 2010) and the ACE 2005 dataset. The SemEval dataset contains 10,717 annotated examples (8,000 for training and 2,717 for testing). For ACE, to be comparable to state-of-the-art Neural Network models, we use the split in (Gormley et al., 2015; Nguyen and Grishman, 2015): find the ACE articles from news domains: broadcast conversation (bc), broadcast news (bn), newswire (nw), and uses news (bn & nw) as the training set, half of bc as the development set, the other half of bc as the test set. Table 1 and Table 2 3 show that the performance of our monolingual baseline and various other systems. The mon"
I17-1068,P11-1055,0,0.0434032,"parallel corpora. (Kim et al., 2010) and (Kim et al., 2014) proposed cross-lingual annotation projection approach for relation detection with parallel corpora. In contrast, our work don’t require parallel corpora nor Machine Translation. More recently, (Faruqui and Kumar, 2015) applied cross-lingual projection for open-domain relation extraction in languages other than English. (Blessing and Sch¨utze, 2012) and Compositional Universal Schema (Verga et al., 2016) performs cross-lingual relation extraction with distant supervision (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Hoffmann et al., 2011; Ritter et al., 2013). These works are significantly different from ours in that they either operates in the open-domain(Faruqui and Kumar, 2015) without a pre-defined relation schema, or in a distant supervision setting with a KB as source of supervision. POLY (Nakashole et al., 2012) mines relational paraphrases from multilingual sentences which can be useful for relation extraction. Besides relation extraction, (Huang et al., 2013) performs cross-language knowledge transfer with deep neural networks for speech recognition. (Guo et al., 2016) proposed a distributed representation-based fram"
I17-1068,N07-1015,0,0.083125,"Missing"
I17-1068,H05-1091,0,0.202043,"Missing"
I17-1068,P04-3022,0,0.0916253,"Missing"
I17-1068,C10-1064,0,0.186175,"Missing"
I17-1068,N15-1151,0,0.171098,"8th International Joint Conference on Natural Language Processing, pages 674–684, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP parsing or composing multiple models. There is very little work on multilingual relation extraction. (Qian et al., 2014) proposed an active learning approach for bilingual relation extraction with pseudo parallel corpora. (Kim et al., 2010) and (Kim et al., 2014) proposed cross-lingual annotation projection approach for relation detection with parallel corpora. In contrast, our work don’t require parallel corpora nor Machine Translation. More recently, (Faruqui and Kumar, 2015) applied cross-lingual projection for open-domain relation extraction in languages other than English. (Blessing and Sch¨utze, 2012) and Compositional Universal Schema (Verga et al., 2016) performs cross-lingual relation extraction with distant supervision (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Hoffmann et al., 2011; Ritter et al., 2013). These works are significantly different from ours in that they either operates in the open-domain(Faruqui and Kumar, 2015) without a pre-defined relation schema, or in a distant supervision setting with a KB as source of supervision."
I17-1068,D15-1205,0,0.363702,"Missing"
I17-1068,S10-1057,0,0.0783994,"Missing"
I17-1068,Q13-1030,0,0.015227,"et al., 2010) and (Kim et al., 2014) proposed cross-lingual annotation projection approach for relation detection with parallel corpora. In contrast, our work don’t require parallel corpora nor Machine Translation. More recently, (Faruqui and Kumar, 2015) applied cross-lingual projection for open-domain relation extraction in languages other than English. (Blessing and Sch¨utze, 2012) and Compositional Universal Schema (Verga et al., 2016) performs cross-lingual relation extraction with distant supervision (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Hoffmann et al., 2011; Ritter et al., 2013). These works are significantly different from ours in that they either operates in the open-domain(Faruqui and Kumar, 2015) without a pre-defined relation schema, or in a distant supervision setting with a KB as source of supervision. POLY (Nakashole et al., 2012) mines relational paraphrases from multilingual sentences which can be useful for relation extraction. Besides relation extraction, (Huang et al., 2013) performs cross-language knowledge transfer with deep neural networks for speech recognition. (Guo et al., 2016) proposed a distributed representation-based framework for cross-lingua"
I17-1068,P09-1113,0,0.082339,"learning approach for bilingual relation extraction with pseudo parallel corpora. (Kim et al., 2010) and (Kim et al., 2014) proposed cross-lingual annotation projection approach for relation detection with parallel corpora. In contrast, our work don’t require parallel corpora nor Machine Translation. More recently, (Faruqui and Kumar, 2015) applied cross-lingual projection for open-domain relation extraction in languages other than English. (Blessing and Sch¨utze, 2012) and Compositional Universal Schema (Verga et al., 2016) performs cross-lingual relation extraction with distant supervision (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Hoffmann et al., 2011; Ritter et al., 2013). These works are significantly different from ours in that they either operates in the open-domain(Faruqui and Kumar, 2015) without a pre-defined relation schema, or in a distant supervision setting with a KB as source of supervision. POLY (Nakashole et al., 2012) mines relational paraphrases from multilingual sentences which can be useful for relation extraction. Besides relation extraction, (Huang et al., 2013) performs cross-language knowledge transfer with deep neural networks for speech recognition."
I17-1068,P15-1061,0,0.113233,"odel is trained with labeled datasets for classifying relations. Traditional methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007) either reply on a set of linguistic or semantic features, or use convolution tree kernels (Moschitti, 2006) with syntactic (Zhang et al., 2006), sub-sequence (Bunescu and Mooney, 2005b), or dependency trees (Bunescu and Mooney, 2005a) as means to represent input sentences. Recently, deep neural networks start to show promising results in relation extraction. In particular, Convolutional Neural Networks (Zeng et al., 2014a; dos Santos et al., 2015; Nguyen and Grishman, 2015), Reccurrent/Recursive Neural Networks such as bidirectional LSTMs (Zhang et al., 2015), LSTM along shortest dependency paths (Xu et al., 2015), bidirectional tree-structured LSTM-RNNs (Miwa and Bansal, 2016) are shown to be effective. Attention mechanism (Wang et al., 2016) is also effective in further improving performance. Our baseline monolingual model is similar to (Nguyen and Grishman, 2015) and we does not require 3 Bilingual Relation Extraction Given a pair of monolingual corpora in two different languages and each corpus having been annotated with sentence-"
I17-1068,P16-1105,0,0.047318,"convolution tree kernels (Moschitti, 2006) with syntactic (Zhang et al., 2006), sub-sequence (Bunescu and Mooney, 2005b), or dependency trees (Bunescu and Mooney, 2005a) as means to represent input sentences. Recently, deep neural networks start to show promising results in relation extraction. In particular, Convolutional Neural Networks (Zeng et al., 2014a; dos Santos et al., 2015; Nguyen and Grishman, 2015), Reccurrent/Recursive Neural Networks such as bidirectional LSTMs (Zhang et al., 2015), LSTM along shortest dependency paths (Xu et al., 2015), bidirectional tree-structured LSTM-RNNs (Miwa and Bansal, 2016) are shown to be effective. Attention mechanism (Wang et al., 2016) is also effective in further improving performance. Our baseline monolingual model is similar to (Nguyen and Grishman, 2015) and we does not require 3 Bilingual Relation Extraction Given a pair of monolingual corpora in two different languages and each corpus having been annotated with sentence-level relations 1 of pre-defined types, the goal of bilingual relation extraction is to learn discriminative representations to identify the relation between a pair of mentions, regardless of which language the mention pair comes from ("
I17-1068,D12-1110,0,0.0257513,"Missing"
I17-1068,D12-1104,0,0.0141162,"d cross-lingual projection for open-domain relation extraction in languages other than English. (Blessing and Sch¨utze, 2012) and Compositional Universal Schema (Verga et al., 2016) performs cross-lingual relation extraction with distant supervision (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Hoffmann et al., 2011; Ritter et al., 2013). These works are significantly different from ours in that they either operates in the open-domain(Faruqui and Kumar, 2015) without a pre-defined relation schema, or in a distant supervision setting with a KB as source of supervision. POLY (Nakashole et al., 2012) mines relational paraphrases from multilingual sentences which can be useful for relation extraction. Besides relation extraction, (Huang et al., 2013) performs cross-language knowledge transfer with deep neural networks for speech recognition. (Guo et al., 2016) proposed a distributed representation-based framework for cross-lingual transfer learning for dependency parsers. The final combined algorithm essentially learns two types of useful representations: a languageindependent relation-specific representation with the shared neurons, and a language-dependent relation-specific representatio"
I17-1068,D12-1042,0,0.0368797,"extraction with pseudo parallel corpora. (Kim et al., 2010) and (Kim et al., 2014) proposed cross-lingual annotation projection approach for relation detection with parallel corpora. In contrast, our work don’t require parallel corpora nor Machine Translation. More recently, (Faruqui and Kumar, 2015) applied cross-lingual projection for open-domain relation extraction in languages other than English. (Blessing and Sch¨utze, 2012) and Compositional Universal Schema (Verga et al., 2016) performs cross-lingual relation extraction with distant supervision (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Hoffmann et al., 2011; Ritter et al., 2013). These works are significantly different from ours in that they either operates in the open-domain(Faruqui and Kumar, 2015) without a pre-defined relation schema, or in a distant supervision setting with a KB as source of supervision. POLY (Nakashole et al., 2012) mines relational paraphrases from multilingual sentences which can be useful for relation extraction. Besides relation extraction, (Huang et al., 2013) performs cross-language knowledge transfer with deep neural networks for speech recognition. (Guo et al., 2016) proposed a distributed re"
I17-1068,W15-1506,0,0.294137,"labeled datasets for classifying relations. Traditional methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007) either reply on a set of linguistic or semantic features, or use convolution tree kernels (Moschitti, 2006) with syntactic (Zhang et al., 2006), sub-sequence (Bunescu and Mooney, 2005b), or dependency trees (Bunescu and Mooney, 2005a) as means to represent input sentences. Recently, deep neural networks start to show promising results in relation extraction. In particular, Convolutional Neural Networks (Zeng et al., 2014a; dos Santos et al., 2015; Nguyen and Grishman, 2015), Reccurrent/Recursive Neural Networks such as bidirectional LSTMs (Zhang et al., 2015), LSTM along shortest dependency paths (Xu et al., 2015), bidirectional tree-structured LSTM-RNNs (Miwa and Bansal, 2016) are shown to be effective. Attention mechanism (Wang et al., 2016) is also effective in further improving performance. Our baseline monolingual model is similar to (Nguyen and Grishman, 2015) and we does not require 3 Bilingual Relation Extraction Given a pair of monolingual corpora in two different languages and each corpus having been annotated with sentence-level relations 1 of pre-def"
I17-1068,N16-1103,0,0.085027,"little work on multilingual relation extraction. (Qian et al., 2014) proposed an active learning approach for bilingual relation extraction with pseudo parallel corpora. (Kim et al., 2010) and (Kim et al., 2014) proposed cross-lingual annotation projection approach for relation detection with parallel corpora. In contrast, our work don’t require parallel corpora nor Machine Translation. More recently, (Faruqui and Kumar, 2015) applied cross-lingual projection for open-domain relation extraction in languages other than English. (Blessing and Sch¨utze, 2012) and Compositional Universal Schema (Verga et al., 2016) performs cross-lingual relation extraction with distant supervision (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Hoffmann et al., 2011; Ritter et al., 2013). These works are significantly different from ours in that they either operates in the open-domain(Faruqui and Kumar, 2015) without a pre-defined relation schema, or in a distant supervision setting with a KB as source of supervision. POLY (Nakashole et al., 2012) mines relational paraphrases from multilingual sentences which can be useful for relation extraction. Besides relation extraction, (Huang et al., 2013) perfo"
I17-1068,P14-1055,0,0.274055,"Missing"
I17-1068,P16-1123,0,0.0695644,"Missing"
I17-1068,D15-1206,0,0.0133658,"r reply on a set of linguistic or semantic features, or use convolution tree kernels (Moschitti, 2006) with syntactic (Zhang et al., 2006), sub-sequence (Bunescu and Mooney, 2005b), or dependency trees (Bunescu and Mooney, 2005a) as means to represent input sentences. Recently, deep neural networks start to show promising results in relation extraction. In particular, Convolutional Neural Networks (Zeng et al., 2014a; dos Santos et al., 2015; Nguyen and Grishman, 2015), Reccurrent/Recursive Neural Networks such as bidirectional LSTMs (Zhang et al., 2015), LSTM along shortest dependency paths (Xu et al., 2015), bidirectional tree-structured LSTM-RNNs (Miwa and Bansal, 2016) are shown to be effective. Attention mechanism (Wang et al., 2016) is also effective in further improving performance. Our baseline monolingual model is similar to (Nguyen and Grishman, 2015) and we does not require 3 Bilingual Relation Extraction Given a pair of monolingual corpora in two different languages and each corpus having been annotated with sentence-level relations 1 of pre-defined types, the goal of bilingual relation extraction is to learn discriminative representations to identify the relation between a pair of men"
I17-1068,C14-1220,0,0.153232,"vised machine learning model is trained with labeled datasets for classifying relations. Traditional methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007) either reply on a set of linguistic or semantic features, or use convolution tree kernels (Moschitti, 2006) with syntactic (Zhang et al., 2006), sub-sequence (Bunescu and Mooney, 2005b), or dependency trees (Bunescu and Mooney, 2005a) as means to represent input sentences. Recently, deep neural networks start to show promising results in relation extraction. In particular, Convolutional Neural Networks (Zeng et al., 2014a; dos Santos et al., 2015; Nguyen and Grishman, 2015), Reccurrent/Recursive Neural Networks such as bidirectional LSTMs (Zhang et al., 2015), LSTM along shortest dependency paths (Xu et al., 2015), bidirectional tree-structured LSTM-RNNs (Miwa and Bansal, 2016) are shown to be effective. Attention mechanism (Wang et al., 2016) is also effective in further improving performance. Our baseline monolingual model is similar to (Nguyen and Grishman, 2015) and we does not require 3 Bilingual Relation Extraction Given a pair of monolingual corpora in two different languages and each corpus having bee"
I17-1068,N06-1037,0,0.083362,"Missing"
I17-1068,Y15-1009,0,0.29157,"2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007) either reply on a set of linguistic or semantic features, or use convolution tree kernels (Moschitti, 2006) with syntactic (Zhang et al., 2006), sub-sequence (Bunescu and Mooney, 2005b), or dependency trees (Bunescu and Mooney, 2005a) as means to represent input sentences. Recently, deep neural networks start to show promising results in relation extraction. In particular, Convolutional Neural Networks (Zeng et al., 2014a; dos Santos et al., 2015; Nguyen and Grishman, 2015), Reccurrent/Recursive Neural Networks such as bidirectional LSTMs (Zhang et al., 2015), LSTM along shortest dependency paths (Xu et al., 2015), bidirectional tree-structured LSTM-RNNs (Miwa and Bansal, 2016) are shown to be effective. Attention mechanism (Wang et al., 2016) is also effective in further improving performance. Our baseline monolingual model is similar to (Nguyen and Grishman, 2015) and we does not require 3 Bilingual Relation Extraction Given a pair of monolingual corpora in two different languages and each corpus having been annotated with sentence-level relations 1 of pre-defined types, the goal of bilingual relation extraction is to learn discriminative repres"
I17-1068,P05-1052,0,0.0564428,"Missing"
I17-1068,P05-1053,0,0.393686,"Missing"
I17-1068,W09-2415,0,\N,Missing
J10-4005,W05-0909,0,0.0310056,"Missing"
J10-4005,2007.mtsummit-tutorials.1,0,0.0545454,"Missing"
J10-4005,D09-1021,0,0.0381574,"Missing"
J10-4005,W08-2102,0,0.0368808,"Missing"
J10-4005,2003.mtsummit-papers.6,0,0.0747567,"Missing"
J10-4005,P00-1058,0,0.013589,"Missing"
J10-4005,P05-1033,0,0.215043,"erate the structured output (dependency trees) required for dependency LM scoring, translation rules in our system represent the target side as dependency structures. We restrict the target side of the rules to well-formed dependency structures to weed out bad translation rules and enable efﬁcient decoding through dynamic programming. Due to the ﬂexibility of well-formed dependency structures, such structures can cover a large set of non-constituent transfer rules (Marcu et al. 2006) that have been shown useful for MT. For comparison purposes, as our baseline, we replicated the Hiero decoder (Chiang 2005), a state-of-the-art hierarchical string-to-string model. Our experiments show that the string-to-dependency decoder signiﬁcantly improves MT performance. Overall, the ∗ 10 Moulton Street, Cambridge, MA 02138. E-mail: libinshen@gmail.com. ∗∗ 10 Moulton Street, Cambridge, MA 02138. E-mail: jxu@bbn.com. † 10 Moulton Street, Cambridge, MA 02138. E-mail: weisched@bbn.com. Submission received: 6 March 2009; revised submission received: 1 December 2009; accepted for publication: 18 March 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 improvement"
J10-4005,J07-2003,0,0.0777265,"MT Phrase-based systems (Koehn, Och, and Marcu 2003; Och 2003) had dominated SMT until recently. Such systems typically treat the input as a sequence of phrases (word n-grams), reorder them, and produce a translation for the reordered sentence based on translation options of each source phrase. A prominent feature of such systems is the use of an n-gram LM to measure the quality of translation hypotheses. A drawback of such systems is that the lack of structural information in the output makes it impossible to score translation hypotheses based on their structural soundness. The Hiero system (Chiang 2007) was a major breakthrough in SMT. Translation rules in Hiero contain non-terminals (NTs), as well as words, which allow the input to be translated in a hierarchical manner. Because both the source and target sides of its translation rules are strings with NTs, Hiero can be viewed as a hierarchical stringto-string model. Despite the hierarchical nature of its decoder, Hiero lacks the ability to measure translation quality based on structural relations such as predicate–argument agreement. Yamada and Knight (2001) proposed a syntax-based translation model that transfers a source parse tree into"
J10-4005,D09-1076,0,0.0277609,"Missing"
J10-4005,D07-1079,0,0.0296523,"Missing"
J10-4005,P05-1067,0,0.132071,"proposed another TAG-based MT model. In their implementation, a TAG grammar was transformed to an equivalent Tree Insertion 650 Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation Grammar (TIG). In this way, they do not have an explicit adjoining operation in their system, and as such reduce the search space in decoding. Sub-trees are combined with NT substitution. Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) to take advantage of structural knowledge on both sides—for example, as in the papers by Hajiˇc et al. (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005). Although tree-to-tree models can represent rich structural information of the input and the output, they have not signiﬁcantly improved MT performance, possibly due to a much larger grammar and search space. On the other hand, Smith and Eisner (2006) showed the necessity of allowing loose transformations between the trees, which made tree-to-tree models even more complicated. 3. Overview of String-to-Dependency Translation Our system is designed to address problems with existing SMT approaches. It is novel in two respects. First, it uses a dependency LM"
J10-4005,P03-2041,0,0.0227162,"d Knight (2009) proposed another TAG-based MT model. In their implementation, a TAG grammar was transformed to an equivalent Tree Insertion 650 Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation Grammar (TIG). In this way, they do not have an explicit adjoining operation in their system, and as such reduce the search space in decoding. Sub-trees are combined with NT substitution. Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) to take advantage of structural knowledge on both sides—for example, as in the papers by Hajiˇc et al. (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005). Although tree-to-tree models can represent rich structural information of the input and the output, they have not signiﬁcantly improved MT performance, possibly due to a much larger grammar and search space. On the other hand, Smith and Eisner (2006) showed the necessity of allowing loose transformations between the trees, which made tree-to-tree models even more complicated. 3. Overview of String-to-Dependency Translation Our system is designed to address problems with existing SMT approaches. It is novel in two respects. First,"
J10-4005,P99-1059,0,0.025649,"Missing"
J10-4005,W02-1039,0,0.106586,"Missing"
J10-4005,P06-1121,0,0.0449518,"rgument agreement. Yamada and Knight (2001) proposed a syntax-based translation model that transfers a source parse tree into a target string. This method depends on the quality of source side parsing, and ignores target information during source side analysis. Mi, Huang, and Liu (2008) later proposed a translation model that takes the source parse forest as MT input to reduce translation errors due to imperfect source side analysis. Galley et al. (2004) proposed an MT model which produces target parse trees for string inputs in order to exploit the syntactic structure of the target language. Galley et al. (2006) formalized this approach with tree transducers (Graehl and Knight 2004) by using context-free parse trees to represent the target side. However, it was later shown by Marcu et al. (2006) and Wang, Knight, and Marcu (2007) that coverage could be a big issue for the constituent based rules, even though the translation rule set was already very large. Carreras and Collins (2009) introduced a string-to-tree MT model based on spinal Tree Adjoining Grammar (TAG) (Joshi and Schabes 1997; Shen, Champollion, and Joshi 2008). In this model, a translation rule is composed of a source string and a target"
J10-4005,N04-1035,0,0.0765447,"ng model. Despite the hierarchical nature of its decoder, Hiero lacks the ability to measure translation quality based on structural relations such as predicate–argument agreement. Yamada and Knight (2001) proposed a syntax-based translation model that transfers a source parse tree into a target string. This method depends on the quality of source side parsing, and ignores target information during source side analysis. Mi, Huang, and Liu (2008) later proposed a translation model that takes the source parse forest as MT input to reduce translation errors due to imperfect source side analysis. Galley et al. (2004) proposed an MT model which produces target parse trees for string inputs in order to exploit the syntactic structure of the target language. Galley et al. (2006) formalized this approach with tree transducers (Graehl and Knight 2004) by using context-free parse trees to represent the target side. However, it was later shown by Marcu et al. (2006) and Wang, Knight, and Marcu (2007) that coverage could be a big issue for the constituent based rules, even though the translation rule set was already very large. Carreras and Collins (2009) introduced a string-to-tree MT model based on spinal Tree"
J10-4005,N04-1014,0,0.0263866,"Missing"
J10-4005,W05-1506,0,0.04357,"Missing"
J10-4005,W04-3250,0,0.347205,"Missing"
J10-4005,N03-1017,0,0.02234,"Missing"
J10-4005,P95-1037,0,0.0278908,"Missing"
J10-4005,W06-1606,0,0.0678225,"opposed to during reranking n-best output) to score alternative translations based on their structural soundness. In order to generate the structured output (dependency trees) required for dependency LM scoring, translation rules in our system represent the target side as dependency structures. We restrict the target side of the rules to well-formed dependency structures to weed out bad translation rules and enable efﬁcient decoding through dynamic programming. Due to the ﬂexibility of well-formed dependency structures, such structures can cover a large set of non-constituent transfer rules (Marcu et al. 2006) that have been shown useful for MT. For comparison purposes, as our baseline, we replicated the Hiero decoder (Chiang 2005), a state-of-the-art hierarchical string-to-string model. Our experiments show that the string-to-dependency decoder signiﬁcantly improves MT performance. Overall, the ∗ 10 Moulton Street, Cambridge, MA 02138. E-mail: libinshen@gmail.com. ∗∗ 10 Moulton Street, Cambridge, MA 02138. E-mail: jxu@bbn.com. † 10 Moulton Street, Cambridge, MA 02138. E-mail: weisched@bbn.com. Submission received: 6 March 2009; revised submission received: 1 December 2009; accepted for publication"
J10-4005,P05-1012,0,0.0820131,"Missing"
J10-4005,P08-1023,0,0.0323325,"Missing"
J10-4005,P03-1021,0,0.0291626,"ur string-to-dependency translation system. Section 4 provides a complete description of our system, including formal deﬁnitions of well-formed dependency structures and their operations, as well as proofs about their key properties. Section 5 describes the implementation details, which include rule extraction, decoding, using dependency LM scores, and using labels in translation rules. We discuss experimental results in Section 6, compare our work with related work in Section 7, and draw conclusions in Section 8. 2. Previous Approaches to SMT Phrase-based systems (Koehn, Och, and Marcu 2003; Och 2003) had dominated SMT until recently. Such systems typically treat the input as a sequence of phrases (word n-grams), reorder them, and produce a translation for the reordered sentence based on translation options of each source phrase. A prominent feature of such systems is the use of an n-gram LM to measure the quality of translation hypotheses. A drawback of such systems is that the lack of structural information in the output makes it impossible to score translation hypotheses based on their structural soundness. The Hiero system (Chiang 2007) was a major breakthrough in SMT. Translation rule"
J10-4005,J03-1002,0,0.0135112,"Missing"
J10-4005,J05-1004,0,0.011216,"Missing"
J10-4005,P05-1034,0,0.0888221,"Missing"
J10-4005,P95-1021,0,0.0928401,"Missing"
J10-4005,H05-1102,1,0.792337,"ter shown by Marcu et al. (2006) and Wang, Knight, and Marcu (2007) that coverage could be a big issue for the constituent based rules, even though the translation rule set was already very large. Carreras and Collins (2009) introduced a string-to-tree MT model based on spinal Tree Adjoining Grammar (TAG) (Joshi and Schabes 1997; Shen, Champollion, and Joshi 2008). In this model, a translation rule is composed of a source string and a target elementary tree. Target hypothesis trees are combined with the adjoining operation, and there are no NT slots for substitution as in LTAG-spinal parsing (Shen and Joshi 2005, 2008; Carreras, Collins, and Koo 2008). Without the constraint of NT slots, the adjoining operation allows very ﬂexible composition, so that the search space becomes much larger. One has to carefully prune the search space. DeNeefe and Knight (2009) proposed another TAG-based MT model. In their implementation, a TAG grammar was transformed to an equivalent Tree Insertion 650 Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation Grammar (TIG). In this way, they do not have an explicit adjoining operation in their system, and as such reduce the search space in decoding."
J10-4005,D08-1052,1,0.910959,"Missing"
J10-4005,D09-1008,1,0.73157,"Missing"
J10-4005,C90-3045,0,0.0866356,"on allows very ﬂexible composition, so that the search space becomes much larger. One has to carefully prune the search space. DeNeefe and Knight (2009) proposed another TAG-based MT model. In their implementation, a TAG grammar was transformed to an equivalent Tree Insertion 650 Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation Grammar (TIG). In this way, they do not have an explicit adjoining operation in their system, and as such reduce the search space in decoding. Sub-trees are combined with NT substitution. Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) to take advantage of structural knowledge on both sides—for example, as in the papers by Hajiˇc et al. (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005). Although tree-to-tree models can represent rich structural information of the input and the output, they have not signiﬁcantly improved MT performance, possibly due to a much larger grammar and search space. On the other hand, Smith and Eisner (2006) showed the necessity of allowing loose transformations between the trees, which made tree-to-tree models even more complicated. 3. Overview of String-to-Depend"
J10-4005,W06-3104,0,0.0163059,"ir system, and as such reduce the search space in decoding. Sub-trees are combined with NT substitution. Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) to take advantage of structural knowledge on both sides—for example, as in the papers by Hajiˇc et al. (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005). Although tree-to-tree models can represent rich structural information of the input and the output, they have not signiﬁcantly improved MT performance, possibly due to a much larger grammar and search space. On the other hand, Smith and Eisner (2006) showed the necessity of allowing loose transformations between the trees, which made tree-to-tree models even more complicated. 3. Overview of String-to-Dependency Translation Our system is designed to address problems with existing SMT approaches. It is novel in two respects. First, it uses a dependency LM to model long-distance relations. Second, it uses well-formed dependency structures to represent translation hypotheses to achieve an effective trade-off between model coverage and decoding complexity. 3.1 Dependency-Based Translation and Language Models Our system generates target depende"
J10-4005,2006.amta-papers.25,0,0.165564,"Missing"
J10-4005,2000.eamt-1.5,0,0.164857,"Missing"
J10-4005,D07-1078,0,0.0401523,"Missing"
J10-4005,P02-1025,0,0.0656954,"Missing"
J10-4005,P01-1067,0,0.0231851,"ossible to score translation hypotheses based on their structural soundness. The Hiero system (Chiang 2007) was a major breakthrough in SMT. Translation rules in Hiero contain non-terminals (NTs), as well as words, which allow the input to be translated in a hierarchical manner. Because both the source and target sides of its translation rules are strings with NTs, Hiero can be viewed as a hierarchical stringto-string model. Despite the hierarchical nature of its decoder, Hiero lacks the ability to measure translation quality based on structural relations such as predicate–argument agreement. Yamada and Knight (2001) proposed a syntax-based translation model that transfers a source parse tree into a target string. This method depends on the quality of source side parsing, and ignores target information during source side analysis. Mi, Huang, and Liu (2008) later proposed a translation model that takes the source parse forest as MT input to reduce translation errors due to imperfect source side analysis. Galley et al. (2004) proposed an MT model which produces target parse trees for string inputs in order to exploit the syntactic structure of the target language. Galley et al. (2006) formalized this approa"
J10-4005,P02-1040,0,\N,Missing
J10-4005,W90-0102,0,\N,Missing
J10-4005,D08-1076,0,\N,Missing
J18-4004,P11-2050,1,0.852111,"Missing"
J77-1008,T75-1005,0,0.0364248,"Missing"
J77-1008,T75-2023,0,0.0663207,"Missing"
J80-2003,T75-2001,0,0.0247246,"are strictly proper for the language. Another way for a system to accept incorrect forms of language is suggested by observing a c o m m o n style of writing grammars. Syntactic input components are often designed using a c o n t e x t - f r e e grammar where each grammar rule may be augmented by predicates operating on the semantic representations or on the constituents linked by the grammar rule. The predicates must be satisfied for the constituents to be grouped as a larger constituent. (Of course, the grammar is no longer c o n t e x t - f r e e then.) Augmented phrase structure grammars (Heidorn, 1975) encode parsers and translators specifically in this way. The augmented transition network formalism also directly lends itself to writing parsers and translators in this way by the predicates on arcs. The version of systemic grammar implemented by Winograd (1972) has this flavor as well. Still another example of this style of writing parsers is the linguistic string parser of Sager (1973) and Grishman (1973). A straightforward example of the use of such predicates is for subject-verb agreement. It is easy for a user to make mistakes in long English sentences, resuiting in parser failure. A so"
J80-2003,J77-1008,1,0.816091,"cution of a user request. Furthermore, such preconditions are directly traceable to particular phrases in that request. The psychological validity of given and new information has been demonstrated by Clark and Haviland (1977) and Haviland and Clark (1974). The psychological process they suggest is that (1) given and new information are first sorted in processing a sentence, (2) memory is then searched to establish that the given information holds in context, and (3) the new information is then asserted in memory. We have modelled this process in natural language systems. Research reported in Joshi and Weischedel (1977) and Weischedel (1979) demonstrated how to organize an augmented transition network and lexicon to compute the given and new information of a sentence. In another system, we implemented the second of the three parts of the psychological process suggested by Clark and Haviland. That system was an intelligent tutor which pinpointed errors a student makes while answering questions in German during a reading comprehension exercise (Weischedel, et.al., 1978). A text presented to English-speaking students in German provides a relatively closed world for the tutor system, since questions refer to ent"
J80-2003,T78-1029,0,0.0428947,"Missing"
J80-2003,H89-1033,0,0.0231093,"h grammar rule may be augmented by predicates operating on the semantic representations or on the constituents linked by the grammar rule. The predicates must be satisfied for the constituents to be grouped as a larger constituent. (Of course, the grammar is no longer c o n t e x t - f r e e then.) Augmented phrase structure grammars (Heidorn, 1975) encode parsers and translators specifically in this way. The augmented transition network formalism also directly lends itself to writing parsers and translators in this way by the predicates on arcs. The version of systemic grammar implemented by Winograd (1972) has this flavor as well. Still another example of this style of writing parsers is the linguistic string parser of Sager (1973) and Grishman (1973). A straightforward example of the use of such predicates is for subject-verb agreement. It is easy for a user to make mistakes in long English sentences, resuiting in parser failure. A solution would be simply to remove the predicate from the rule. H o w e v e r , Grishman (1973) reports from their experience in processing scientific texts that the predicates effectively eliminate a large number of spurious parses. We suggest that, instead of forc"
J80-2003,J78-3037,0,\N,Missing
J80-2003,P79-1006,0,\N,Missing
J83-3003,P80-1024,0,0.0470296,"v) above prints a message when all attempts have failed. We could phrase this heuristic as a meta-rule whose LHS would check that the parser has blocked. This meta-rule would be ordered strictly after the ones for spelling correction and contextual ellipsis. A state would be postulated as the locale of the problem by the same heuristic as for spelling correction. The RHS would print for each arc that leaves that state the category, constituent, or word that was expected by that arc. H a y e s and Mouradian (1980) emphasize recovery techniques for blocking during l e f t - c o r n e r parsing (Aho and Ullman 1972). Their strategies are invoked only if the parser blocks. T w o of them can be reformulated as m e t a - r u l e s as follows. One m e t a - r u l e would check in its LHS that the parser was blocked and that a special parser variable (call it BLOCKED-PARSE) was empty. The RHS would save the blocked configuration in BLOCKED-PARSE, and start parsing as if the current word were the first word of the input. This would enable the system to ignore initial strings that could not be understood. A useful example of this is restarted inputs, such as "" C o p y all print all headers of messages"". A secon"
J83-3003,J76-2004,0,0.0922337,"Missing"
J83-3003,P84-1045,0,0.114163,"Missing"
J83-3003,P79-1002,0,0.282933,"(TO S/NP)) (JUMP S/NP (SETR SURFACE-SUBJECT (GETR TRACE)) (* Allows for elided subjects in reduced relative clauses)) Figure 3. A Simple ATN Graph. An important action for the RHS is NEW-CONFIGURATION, which defines a new parser configuration, thus replacing the failed configuration that the meta-rule matched. It may take any number of arguments which set parts of the configuration. For example, SETR will define the new value of an ATN register. A list of useful arguments to NEW-CONFIGURATION is given in Figure 2. Failed constraints fill the role of the deviance notes of Kwasny and Sondheimer (1979). All parts of the failed configuration that are not explicitly changed in NEW-CONFIGURATION remain the same. Our implementation assumes that there is only one NEW-CONFIGURATION per meta-rule, though one could generalize this so that executing NEW-CONFIGURATION n times in a metarule gives n new configurations to replace the failed one. If a new configuration is generated, the parse can be resumed. Figure 3 gives a trivial ATN which will be used for the sample meta-rules. The start state is S/. A list beginning with an asterisk in the actions of an arc is a comment. (i) 3.1.1. Simple grammatica"
J83-3003,J81-4004,0,0.104346,"Missing"
J83-3003,J83-1003,0,0.0303452,"Missing"
J83-3003,A83-1004,0,0.0232056,"Missing"
J83-3003,P80-1026,0,0.0494419,"Missing"
J83-3003,T75-2001,0,0.0307431,"es dealing with the grammar. Many of our examples here are reformulations of our earlier work (Weischedel et al. 1978, Kwasny and Sondheimer 1979, Weischedel and Black 1980) within the uniform framework of meta-rules. All meta-rules pertaining to syntax should have a first condition which is (SYNTAX-FAILED?); this is true iff the parser is blocked. Since all rules in this section would contain that predicate, we will not include it in the examples. Many syntactic formalisms have a similar framework for expressing rules: these include context-free grammars, augmented phrase structure grammars (Heidorn 1975), P r o g r a m m a r (Winograd 1972), the linguistic string parser (Sager 1981), Lifer (Hendrix et al. 1978), and augmented transition networks (ATNs) (Woods 1970). In fact, all of these can be viewed as formally equivalent to ATNs, and we will describe our techniques in that framework. Figure 1 gives several predicates that should be useful in the LHS of meta-rules. The LHS of the metarule is matched against the environment in which an ATN arc failed. The environment is called a configuration and includes the current ATN state, the arc, all ATN registers, and the remainder of the input strin"
J83-3003,A83-1015,0,0.03203,"e the parser blocked. The only way to block in S/POP is if the verb complement expected for the main verb is not present. Meta-rule (v) could handle this simple case. Notice that this is a different class of meta-rule, for it does not resume computation. Naturally, such rules should be tried only after no other meta-rules are available. One could define different classes of metarules by appropriate declarations; alternatively, this class can be recognized easily, since none of the actions resume processing. This is not the only alternative in the face of failure to parse even with relaxation; Jensen and Heidorn (1983) present heuristics for what to pass to the semantic interpreter in this case, given b o t t o m - u p parsing. 3.2. Meta-rules related to semantics In addition to these syntactic examples, semantic problems can also be addressed within the formalism. If some semantic tests are included in the parser, say a certain arc test contains calls on the semantic component, specific semantic tests can be relaxed by the general mechanism we described for relaxing tests on ATN arcs. Instead, suppose that semantic constraints are encoded in a separate component. Semantic constraints may be expressed in se"
J83-3003,P84-1030,1,0.918027,"should reflect strictly what is normative. Second, the relaxation rules should be made as tight as warranted by patterns of ill-formedness in language use. Third, a partial order on the relaxations should be established. Fourth, not only syntactic constraints and selection restrictions should be used (as in our system) but also pragmatic information to suggest the most promising alternatives. We have begun research on how to use pragmatic knowledge in an information-seeking enviA m e r i c a n J o u r n a l of Computational Linguistics, ronment for this purpose; see Carberry (1983, 1984) and Ramshaw and Weischedel (1984). In the environment of messages reporting events, G r a n g e r (1983) reports on using expectations based on stereotypical events for this purpose. Extensive empirical studies regarding effective control of the search space are needed. The acid test for a framework, relaxation heuristics, and control strategies is not relaxing simple tests like subject-verb agreement or diagnosing obvious problems like a word not in the dictionary. Rather the acid test is a wide spectrum of problems, including examples like misspellings/typographical errors that result in a known word, because in this type o"
J83-3003,J80-1002,0,0.0413601,"ives to the approach we have formulated; our approach is covered in Section 2.2. In describing the five approaches, we use the following informal notation. SYSTEM[s] refers to a system designed to process a set of sentences s. WELL-FORMED is a set of well-formed utterances; ILL-FORMED is a set of illformed utterances. Naturally, an a p p r o a c h that covers the broadest range of linguistic behaviour should be preferred. One alternative is to treat the processing of illformed and well-formed inputs identically, by ignoring constraints. T h a t is, one designs SYSTEM[WELLFORMED U ILL-FORMED]. Schank et al. (1980) and Waltz (1978) have taken this a p p r o a c h toward grammatical constraints. CASPAR ( H a y e s and Carbonell 1981) exhibits this a p p r o a c h for g r a m m a t i c a l constraints as well. Since there is much redundancy in language, the practice of not using certain constraints will often work. However, this will fail on m a n y utterances, since it ignores rules that not only constrain search but also eliminate u n i n t e n d e d interpretations. One can see this by considering s u b j e c t - v e r b agreement, a grammatical constraint that people sometimes violate and that is ofte"
J83-3003,C80-1027,0,0.0339344,"ill-formedness often has specific implications is still needed. In example (4), the selection restriction that "" l i k e "" requires animate agents is violated; a reasonable inference is that the speaker s o m e w h a t personifies the c o m p u t e r in question. (4) My home c o m p u t e r d o e s n ' t like to run BASIC. V o l u m e 9, Numbers 3-4, July-December 1983 163 Ralph M. Weischedel and Norman K. Sondheimer Nor does a metric reflect the fact that there are clear patterns of error, such as those that have been reported in linguistic studies (Fromkin 1973) and in application studies (Thompson 1980, Eastman and M c L e a n 1981). Table I summarizes these four approaches. 2.2. Our approach Based on previous work, both our own and that of others, we propose a framework employing meta-rules to relate the processing of ill-formed input to wellformedness rules. This framework may be stated as follows: 1. Process the input using SYSTEM[WELL-FORMED]. 2. If no interpretation is f o u n d by SYSTEM [WELL-FORMED], apply a meta-rule to the wellformedness rules, based on a ranking of the alternatives, in order to a) diagnose the problem, that is, the rule that is violated and how it is violated, b)"
J83-3003,J80-2003,1,0.891782,"riable is bound in a meta-rule, it retains the binding throughout the rule. Potentially, there may be many places where relaxation can occur. If a meta-rule applies to more than one configuration, it will be applied to each in turn, creating a list of possibilities for processing after recovery is complete. Consequently, the meta-rules will refer to only one failed configuration at a time. Meta-rules related to syntax First, let us consider meta-rules dealing with the grammar. Many of our examples here are reformulations of our earlier work (Weischedel et al. 1978, Kwasny and Sondheimer 1979, Weischedel and Black 1980) within the uniform framework of meta-rules. All meta-rules pertaining to syntax should have a first condition which is (SYNTAX-FAILED?); this is true iff the parser is blocked. Since all rules in this section would contain that predicate, we will not include it in the examples. Many syntactic formalisms have a similar framework for expressing rules: these include context-free grammars, augmented phrase structure grammars (Heidorn 1975), P r o g r a m m a r (Winograd 1972), the linguistic string parser (Sager 1981), Lifer (Hendrix et al. 1978), and augmented transition networks (ATNs) (Woods 1"
J83-3003,P82-1016,1,0.920505,"t, b) w h e t h e r violating the constraint can yield useful inferences, 7 O f c o u r s e , this p r e p r o c e s s i n g a s s u m e s t h a t the L H S contain a form $expr. 174 no patterns c) w h e t h e r examples exist in which the constraint carries meaning, d) whether the constraint, if classified as normative, trims the search space, and e) whether a processing strategy for the constraint can be stated more easily as a modification of normative processing, as in the case of c o n j u n c t i o n (Woods 1973) or the case of contextual ellipsis in the data base e n v i r o n m e n t (Weischedel and Sondheimer, 1982). Thus far we have considered only constraints that are associated with a single point in the processing, such as relaxing a single case f r a m e or relaxing a single ATN arc. Obviously, this need not be the case if, for instance, word or phrase order is permuted. At present, we have no general way of dealing with such problems. 7. Future W o r k The p r o b l e m s of processing ill-formed input require several substantial research efforts. One is collecting additional c o r p o r a to determine patterns of errors and their f r e q u e n c y of occurrence. This is particularly important for"
J83-3003,J76-1007,0,0.0316261,"Missing"
J83-3003,H89-1033,0,0.0216709,"Missing"
J83-3003,J81-2002,1,\N,Missing
J83-3003,A83-1017,0,\N,Missing
J83-3003,P82-1014,0,\N,Missing
J83-3003,P84-1024,1,\N,Missing
J93-2006,P87-1005,1,0.704984,"Missing"
J93-2006,H89-1008,0,0.0358924,"Missing"
J93-2006,H90-1053,0,0.0115364,"(left-hand side) of the grammar, there is a set of context-free rules LHS LHS ,-*-- RHS1 RHS2 LHS ~ RHSn. For each rule, one estimates the probability of the right-hand side given the lefthand side, p(RHSj I LHS). With supervised training, where a set of correct parse trees is provided as training, one estimates p(RHSj I LHS) by the number of times rule LHS *---RHSj appears in the training set divided by the number of times LHS appears in the trees. The probability of a syntactic structure S, given the input string W, is then modeled by the product of the probabilities of the rules used in S. Chitrao and Grishman (1990) used a similar context-free model. Using this model, we explored the following issues: What method of training the rule probabilities should be employed? Our results were much more effective with supervised training, which explains w h y the model performed better in our experiments than Chitrao and Grishman found with unsupervised training. 370 Ralph Weischedel et at. Coping with Ambiguity and Unknown Words START I S /// / NP / / / / PRO V DET it is the  vP ORD N-BAR ORDDIGIT third / ~ N DET bid the .L ~ V P N V V companyhas gotten DET [ ] this N year Rules Used START --) S p(TREE[W) = !-~"
J93-2006,A88-1019,0,0.424873,"ould be diluted. Probability models could be employed where less knowledge was available. 3. Given the vocabulary size, we could not expect to give full syntactic or semantic features. The labor for handcrafted definitions would not be warranted. Statistical language models have a learning component that might supplement handcrafted knowledge. 4. Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences (averaging 29 words per sentence), and the degree of unexpected input. Statistical models based on local information (e.g., DeRose 1988; Church 1988) might operate effectively in spite of sentence length and unexpected input. To see whether our four hypotheses (in italics above) effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity (both at the structural level and at the part-of-speech level) and inferring syntactic and semantic information about unknown words. Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems. Early speech research used purely knowledge-based approaches, analog"
J93-2006,P90-1031,0,0.121148,"Missing"
J93-2006,J88-1003,0,0.373827,"ted domains would be diluted. Probability models could be employed where less knowledge was available. 3. Given the vocabulary size, we could not expect to give full syntactic or semantic features. The labor for handcrafted definitions would not be warranted. Statistical language models have a learning component that might supplement handcrafted knowledge. 4. Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences (averaging 29 words per sentence), and the degree of unexpected input. Statistical models based on local information (e.g., DeRose 1988; Church 1988) might operate effectively in spite of sentence length and unexpected input. To see whether our four hypotheses (in italics above) effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity (both at the structural level and at the part-of-speech level) and inferring syntactic and semantic information about unknown words. Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems. Early speech research used purely knowledge-based appr"
J93-2006,H89-2011,0,0.0290891,"Missing"
J93-2006,J86-3002,0,0.0502357,"Missing"
J93-2006,H90-1052,0,0.016576,"Missing"
J93-2006,H89-2014,0,0.0150366,"e associated with each algorithm. For example, in morphological processing in English (Section 2), the events are the use of a word with a particular part of speech in a string of words. At the level of syntax (Section 3), an event is the use of a particular structure; the model predicts what the most likely rule is given a particular situation. One can similarly use probabilities for assigning semantic structure (Section 4). We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Marcken 1990). Our work is an incremental improvement on these models in three ways: (1) Much less training data than theoretically required proved adequate; (2) we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and (3) we have applied the forward-backward algorithm to accurately compute the most likely tag set. In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semant"
J93-2006,H89-2006,0,0.0214676,"corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques. Based on the results of those experiments, we have constructed a new natural language system (PLUM)for extracting data from text, e.g., newswire text. 1. Introduction Natural language processing, and AI in general, have focused mainly on building rule-based systems with carefully handcrafted rules and domain knowledge. Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTM, 1 and Delphi (Stallard 1989), have used these techniques quite successfully. However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead. 1. We could no longer assume a limited vocabulary. Rather in the domain of terrorist incidents of the Third Message Understanding conference (MUC-3) (Sundheim 1991), roughly 20,000 vocabulary items appear in a corpus 430,000 words long. Additional text from that domain would undoubtedly contain new words"
J93-2006,M91-1001,0,0.0509857,"afted rules and domain knowledge. Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTM, 1 and Delphi (Stallard 1989), have used these techniques quite successfully. However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead. 1. We could no longer assume a limited vocabulary. Rather in the domain of terrorist incidents of the Third Message Understanding conference (MUC-3) (Sundheim 1991), roughly 20,000 vocabulary items appear in a corpus 430,000 words long. Additional text from that domain would undoubtedly contain new words. Probabilistic models offer a mathematically grounded, empirically based means of predicting the most likely interpretation. * BBN Systemsand Technologies,70 FawcettStreet, CambridgeMA 02138. t Sage Lab, RensselaerPolytechnicInstitute,TroyNY 12180. :~ComputerScienceDepartment, BowdoinCollege,BrunswickME 04011. 1 Parlance is a trademarkof BBN Systemsand Technologies. (~) 1993 Associationfor ComputationalLinguistics Computational Linguistics 2. Volume 19,"
J93-2006,H89-1013,1,0.818529,"vinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques. Based on the results of those experiments, we have constructed a new natural language system (PLUM)for extracting data from text, e.g., newswire text. 1. Introduction Natural language processing, and AI in general, have focused mainly on building rule-based systems with carefully handcrafted rules and domain knowledge. Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTM, 1 and Delphi (Stallard 1989), have used these techniques quite successfully. However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead. 1. We could no longer assume a limited vocabulary. Rather in the domain of terrorist incidents of the Third Message Understanding conference (MUC-3) (Sundheim 1991), roughly 20,000 vocabulary items appear in a corpus 430,000 words long. Additional text from that"
J93-2006,J93-1005,0,\N,Missing
K19-1062,S13-2002,0,0.454575,"ise predictions: graph temporal transitivity constraint is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longter"
K19-1062,D11-1027,0,0.0374875,"yper-parameters re-optimize the network to adjust for global properties5 . We denote the local scoring model in the first stage as local model, and the final model as global model in the following sections. Start-point temporal scheme is adopted when outsourcing the annotation task, which contributes to the performance improvement of machine learning models built on this dataset . 4 TCR (Ning et al., 2018a) follows the same annotation scheme for temporal pairs in MATRES. It is also annotated with causal pairs. To get causal pairs, the authors select candidates based on EventCausality dataset (Do et al., 2011). Experimental Setup In this section, we describe the three datasets that are used in the paper. Then we define the evaluation metrics. Finally, we provide details regarding our model implementation and experiments. 4.2 4.1 Data Evaluation Metrics To be consistent with the evaluation metrics used in baseline models, we adopt two slightly different calculations of metrics. Experiments are conducted on TB-Dense, MATRES and TCR datasets and an overview of data statistics are shown in Table 1. We focus on event relation, thus, all numbers refer to EE pairs6 . Note that in all three datasets, event"
K19-1062,W06-1623,0,0.140289,"is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thu"
K19-1062,D12-1062,0,0.184424,"anges the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information c"
K19-1062,P14-2082,0,0.504286,"lysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for future research on temporal relation extraction. 2 Related Work Temporal Relation Data. Temporal relation corpora such as TimeBank (Pustejovsky et al., 2003) and RED (O’Gorman et al., 2016) facilitate the research in temporal relation extraction. The common issue in these corpora is missing annotation. Collecting densely annotated temporal relation corpora with all event pairs fully annotated has been reported to be a challenging task as annotators could easily overlook some pairs (Cassidy et al., 2014; Bethard et al., 2007; Chambers et al., 2014). TB-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences. Recent data construction efforts such as MATRES (Ning et al., 2018a) and TCR (Ning et al., 2018b) further enhance the data quality by using a multiaxis annotation scheme and adopting start-point of events to improve inter-annotator agreements. However, densely annotated datasets are relatively small both in terms of number of documents and event pairs, which restricts the complexity of machine learning models used i"
K19-1062,D19-1041,1,0.522435,"ulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot contradict those from a higher precision model. More recently, neural network-based methods have been employed for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Han et al., 2019a) which achieved impressive results. However, they all treat the task as a pairwise classification problem. Meng and Rumshisky (2018) considered incorporating global context for pairwise relation predictions, but they do not explicitly model the output graph structure for event temporal relation. There are a few prior works exploring structured learning for temporal relation extraction (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, their local models use hand-engineered linguistic features. Despite the effectiveness of hand-crafted features in previous rese"
K19-1062,S13-2012,0,0.239398,"ivity constraint is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the ex"
K19-1062,S13-2015,0,0.445816,": graph temporal transitivity constraint is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between e"
K19-1062,Q14-1022,0,0.791836,"is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1,"
K19-1062,D08-1073,0,0.26688,"re 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contex"
K19-1062,E17-1108,0,0.140085,"state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for correctly predicting temporal relations. • Extensive ablation studies and thorough error analysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for future research on temporal relation extraction."
K19-1062,P07-2044,0,0.149825,"rrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot contradict those from a higher precision model. More"
K19-1062,P06-1095,0,0.335801,"rthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot con"
K19-1062,P17-2001,0,0.118779,"g models used in previous research. In this paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013),"
K19-1062,P18-1049,0,0.460447,"s paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier"
K19-1062,D17-1092,0,0.452005,"us research. In this paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Cham"
K19-1062,C16-1007,0,0.257589,"Missing"
K19-1062,S07-1014,0,0.069377,"nd Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-ba"
K19-1062,N16-1098,0,0.0170875,"temporal relations between the events. Figure 1a illustrates an example of such graph for the text shown above. Different types of edges specify different temporal relations: the event filed is SIMULTANEOUS with claiming, overruled is BEFORE claiming, and overruled is also BEFORE filed. Temporal relation extraction is beneficial for many downstream tasks such as question answering, information retrieval, and natural language generation. An event graph can po∗ ﬁled tentially be leveraged to help time-series forecasting and provide guidances for natural language generation. The CaTeRs dataset (Mostafazadeh et al., 2016) which annotates temporal and causal relations is constructed for this purpose. A major challenge in temporal relation extraction stems from its nature of being a structured The authors contribute equally, alphabetical order. 666 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 666–676 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics We develop a joint end-to-end training scheme that enables the feedback from global structure to directly guide neural networks to learn representations, and hence allows our deep structured"
K19-1062,D17-1108,0,0.74796,"s and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for correctly predicting temporal relations. • Extensive ablation studies and thorough error analysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for future research on"
K19-1062,C08-3012,0,0.279212,"RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot contradict those from a higher precision model. More recently, neural network-based me"
K19-1062,P18-1212,0,0.0633864,"on of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for"
K19-1062,P18-1122,0,0.0567825,"on of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for"
K19-1062,P09-1046,0,0.323718,"strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for correctly predicting temporal relations. • Extensive ablation studies and thorough error analysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for"
K19-1062,W16-5706,0,0.218256,"Missing"
K19-1062,D14-1162,0,0.091123,"of our current framework. Acknowledgments This work is partially funded by DARPA Contracts W911NF-15-1-0543 and an NIH R01 (LM012592). The authors thank the anonymous reviewers for their helpful suggestions and members from USC PLUS lab for early feedbacks. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors. Effect of BERT representations In this section, we explore the impact of contextualized BERT representations under our deep SSVM framework. We replace BERT representations with the GloVe (Pennington et al., 2014) word embeddings. Table 9 shows the F1 scores of our local model and global model using BERT and GloVe11 respectively. BERT improves the performance with a significant margin. Besides, even without BERT representations, our RNN-based local model and the deep structured global model 11 globalGlove 57.0 75.6 76.5 still outperform (MATRES and TCR) or are comparable with (TB-Dense) current SOTA. These results confirm the improvements of our method. significantly in the both-way evaluation. In contrast, the proposed model achieves strong performances in both test scenarios (best F1 scores except fo"
K19-1062,P17-2035,0,0.365421,"exity of machine learning models used in previous research. In this paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime ("
K19-1062,S17-2098,0,0.0651677,"Missing"
K19-1062,S13-2001,0,0.461397,"rate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine l"
K19-1062,S10-1010,0,\N,Missing
L18-1031,D17-1109,0,0.139866,"tTaskDescription_1.0.pdf 1 While the Cold Start KBP evaluation directly measures end-to-end performance on KBP, it has several problems:  The scores will vary by number of participants and the amount of answers they produced. Furthermore, the scores aren’t comparable from year to year, therefore it is hard to measure progress.  Given the high cost of the assessment process, the query set has typically been small relative to the schema size. For example, the 2016 query set contains only 317 queries - not a large number for 42 relation types.  The evaluation suffers from severe pooling bias. Chaganty et al. (2017) show that the Cold Start KBP evaluation is significantly and systematically biased against systems that make novel predictions. For a system that does not participate in each year’s evaluation, the pool is likely to not contain a significantly large fraction of correct answers. Therefore, recall will be significantly underestimated. Precision will also be estimated incorrectly because of novel answers that are not assessed.  The assessment dataset is at the end-to-end (queryanswer pair) level. It offers little for improving the components of a KBP system. A standard approach (Ji and Grishman"
L18-1031,doddington-etal-2004-automatic,1,0.626228,"Missing"
L18-1031,C96-1079,0,0.587548,". The process is performed over all submitted KBs3. Introduction Automatically constructing a Knowledge Base (KB) of entities and relations from unstructured text, has long been a goal of Natural Language Processing (NLP). The task, named Knowledge Based Population (KBP), will unlock the huge potential in unstructured text for applications such as questions answering and web search. 1 Since 2012, the National Institute of Standards and Technology (NIST) has run the TAC Cold Start KBP evaluation, which measures performance of KBP. As the successor to the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) and Automatic Content Extraction (ACE) (Doddington et al., 2004) evaluations, Cold Start KBP evaluates a system’s ability to automatically construct a KB from text. It uses a large corpus of 50,000-90,000 documents which have not gone through a careful selection process. In Cold Start KBP evaluation, a system is required to submit a KB 2 of entities and relations, constructed automatically from the corpus by algorithms. How can one evaluate the quality of a KB? The Cold Start KBP evaluation2 measures it by probing the KB with two types of queries: 1-hop (e.g., which organization(s) is(are) fo"
L18-1031,mcnamee-etal-2010-evaluation,0,0.0601648,"Missing"
L18-1031,E17-1057,1,0.926305,"_1.0.pdf 1 While the Cold Start KBP evaluation directly measures end-to-end performance on KBP, it has several problems:  The scores will vary by number of participants and the amount of answers they produced. Furthermore, the scores aren’t comparable from year to year, therefore it is hard to measure progress.  Given the high cost of the assessment process, the query set has typically been small relative to the schema size. For example, the 2016 query set contains only 317 queries - not a large number for 42 relation types.  The evaluation suffers from severe pooling bias. Chaganty et al. (2017) show that the Cold Start KBP evaluation is significantly and systematically biased against systems that make novel predictions. For a system that does not participate in each year’s evaluation, the pool is likely to not contain a significantly large fraction of correct answers. Therefore, recall will be significantly underestimated. Precision will also be estimated incorrectly because of novel answers that are not assessed.  The assessment dataset is at the end-to-end (queryanswer pair) level. It offers little for improving the components of a KBP system. A standard approach (Ji and Grishman"
L18-1031,min-grishman-2012-challenges,1,0.896009,"Missing"
L18-1031,P11-1115,0,0.0198708,"y et al. (2017) show that the Cold Start KBP evaluation is significantly and systematically biased against systems that make novel predictions. For a system that does not participate in each year’s evaluation, the pool is likely to not contain a significantly large fraction of correct answers. Therefore, recall will be significantly underestimated. Precision will also be estimated incorrectly because of novel answers that are not assessed.  The assessment dataset is at the end-to-end (queryanswer pair) level. It offers little for improving the components of a KBP system. A standard approach (Ji and Grishman, 2011) to KBP is to integrate a range of Information Extraction (IE) technologies including: named entity recognition, within document coreference, relation extraction, and cross document coreference. The KBP dataset cannot be used for (re)training any of the component level algorithms.  The KBP assessment dataset annotation lacks component level annotation to support error analysis. A KBP system developer must trace the cause of an error. On the Slot Filling subtask alone, Min et al. (2012) 3 A time-limited manual run is conducted and used to increase the size of the answer pool. 210 Figure 1 Alig"
L18-1031,W04-3250,0,0.403872,"Missing"
L18-1031,H05-1004,0,0.0541358,"Missing"
M91-1006,P90-1031,0,0.061823,"Missing"
M91-1006,H91-1066,0,0.0459188,"Missing"
M91-1006,H91-1065,1,0.907809,"is Ayuso, Sean Boisen , Robert Ingria, Jeff Palmucc i BBN Systems and Technologies 10 Moulton St . Cambridge, MA 0213 8 weischedel@bbn.com INTRODUCTION Perhaps the most important facts about our participation in MUC-3 reflect our starting point and goals . In March, 1990, we initiated a pilot study on the feasibility and impact of applying statistical algorithms in natura l language processing. The experiments were concluded in March, 1991 and lead us to believe that statistica l approaches can effectively improve knowledge-based approaches [Weischedel, et al., 1991a, Weischedel, Meteer , and Schwartz, 1991] . Due to nature of that effort, we had focussed on many well-defined algorithm experiments . We did not have a complete message processing system ; nor was the pilot study designed to create an applicatio n system . For the Phase I evaluation, we supplied a module to New York University . At the time of the Phase I Workshop (12-14 February 1991) we decided to participate in MUC with our own entry . The Phase I Workshop provide d invaluable insight into what other sites were finding successful in this particular application . On 25 February, we started an intense effort not just to be evaluat"
M91-1006,H91-1037,1,\N,Missing
M91-1021,H90-1069,1,0.811641,"Missing"
M91-1021,A88-1019,0,0.136428,"Missing"
M91-1021,H89-1009,0,0.0615538,"Missing"
M91-1021,P90-1031,0,0.0562786,"ty to train (and re-train) systems based on user markings of correct and incorrect output , • more accurate selection among interpretations when more than one is found, an d • more robust partial interpretation when no complete interpretation can be found . We have previously performed experiments on components of the system with texts from the Wall Stree t Journal, however, the MUC-3 task is the first end-to-end application of PLUM . All components except parsin g were developed in the last 5 months, and cannot therefore be considered fully mature. The parsing component, th e MIT Fast Parser [4], originated outside BBN and has a more extensive history prior to MUC-3 . A central assumption of our approach is that in processing unrestricted text for data extraction, a non-trivia l amount of the text will not be understood . As a result, all components of PLUM are designed to operate on partiall y understood input, taking advantage of information when available, and not failing when information is unavailable. The following section describes the major PLUM components . SYSTEM ARCHITECTUR E The PLUM architecture is presented in Figure 1 . Preprocessin g The input to the system is a file"
M92-1007,M92-1024,1,0.878043,"en the presence of the features . If the sum exceeds a userspecified threshold, the paragraph is considered relevant . If the classifier predicts that the paragraph is relevant, then events found in the paragraph can be used to generate templates ; if not, terrorist events that would otherwis e have been produced from that paragraph are blocked . The performance of the overall system, given variou s thresholds of the text classifier, is shown in Figure 3 . 89 A more detailed description of the system components, their individual outputs, and their knowledge bases is presented in Ayuso et al., [1] . We expect the particular implementations to change and improve substantially during the next three years of research and development . RESULT S Appendix G lists detailed test scores . A number of systems performed better on TST4 than on TST3, and som e performed significantly worse on TST4 than TST3 . The results on the two test sets were so disparate for PLUM tha t we decided to look into the causes of the abnormally low recall of PLUM on TST3 . As table 1 shows, the followin g properties of TST3 stand in stark contrast with TST4, TST1„ and the 1300 message development corpus: • The percen"
M92-1007,J81-4005,0,0.0953188,"art of speec h of highly ambiguous words is done by well-known Markov modeling techniques . To improve the recognition of Latin American names, we employed a statistically derived five-gram (five letter) model of words of Spanish origi n and a similar five-gram model of English words . This model was integrated into the part-of-speech tagger. Another usage of statistical algorithms was a statistical induction algorithm to learn case frames for verbs fro m examples . This saved substantial effort compared to building the case frames by hand. The algorithm and empirical results are described in [3] . Precision 42 41 .5 — 41 — 40.5 40 — n 39.5 — 39 — 38.5 — 38 30 31 32 33 34 U 35 Recall Figure 3 : Impact of Paragraph classifier on recall and precision in the ALL TEMPLATES row . The statistical methods mentioned above were already available and used in the MUC-3 version of PLUM . A new statistical algorithm employed in MUC-4 is a classification algorithm that automatically learns features to discriminate among classes . Given a list of relevant paragraphs and a list of irrelevant ones (made available by Ne w Mexico State University), we employed a chi square measurement to determine word"
M92-1007,H91-1037,1,\N,Missing
M92-1024,M91-1036,1,0.673856,"ormation contained locally in a fragment (after fragment combination); in creating corresponding even t objects, the discourse module must infer other long-distance or indirect relations not explicitly found by th e interpreter, and resolve any references in the text . The template generator then uses the structures created by th e discourse component to generate the final templates . Currently only terrorist incidents (and &quot;possible terrorist incidents&quot;) generate discourse events, since these are the core events for MUC-4 template generation . The discourse component was further discussed in [1] . Two primary structures are created by the discourse processo r which are used by the template generator : the discourse predicate database and the event structure . The database contains all the predicates mentioned in the semantic representation of the message . When references are resolved , corresponding semantic variables are unified in the database . Any other inferences done by the discourse componen t also get added to the database. To create the discourse event structure, the discourse component processes each semantic form produced by the interpreter, adding its information to the"
M92-1024,J93-2006,1,\N,Missing
M92-1024,H93-1049,0,\N,Missing
M92-1024,H91-1037,1,\N,Missing
M92-1024,H93-1045,1,\N,Missing
M92-1024,A88-1019,0,\N,Missing
M92-1024,H90-1069,1,\N,Missing
M92-1024,X93-1019,1,\N,Missing
M92-1024,H91-1065,1,\N,Missing
M92-1024,H89-1009,0,\N,Missing
M92-1024,P90-1031,0,\N,Missing
M93-1010,M92-1024,1,0.848456,"information when available, and not failing whe n information is unavailable . Neither a complete grammatical analysis nor complete semantic interpretation i s required. The system finds the parts of the text it can understand and pieces together a model of the whole from thos e part and their context . PROCESSING STAGE S The PLUM architecture is presented in Figure 1 . Ovals represent declarative knowledge bases ; rectangles represen t processing modules . A more detailed description of the system components, their individual outputs, and thei r knowledge bases is presented in Ayuso et al ., [1] . The processing modules are briefly described below . * Ralph Weischedel (Principal Investigator), Damaris Ayuso, Sean Boisen, Heidi Fox, Robert Ingria, Tomoyosh i Matsukawa, Constantine Papageorgiou (BBN), Dawn MacLaughlin, Masaichiro Kitagawa, Tsutomu Sakai (Bosto n University), June Abe, Hiroto Hosihi, Yoichi Miyamoto (University of Connecticut), and Scott Miller (Northeaster n University) 93 Message Message Reader Morphological Analyzer Part of Speech requency Dat a Concept-based Pattern Matcher CBasic Patterns Grammar Rules Fast Partial Parser Lexicon Semantic Interpreter I y ourse Disc"
M93-1010,M91-1036,1,0.829262,", CAPITALIZED AT 20 MILLION NEW TAIWAN DOLLARS"" i n EJV walkthrough article 0592 (this phrase is parsed within a single fragment by FPP) . Notice that the JOINTVENTURE is linked to the OWNERSHIP information via an unknown role, because the interpreter was unable t o determine a specific relationship between the NP ""THE JOINT VENTURE, BRIDGESTONE SPORTS TAIWA N CO .,"" and the participial modifier ""CAPITALIZED AT . . ."" The discourse component will further refine the relationship between these two semantic objects to the JV-OWNERSHIP-OF relation . Discourse Processing PLUM&apos;s discourse component [2] performs the operations necessary to create a meaning for the whole messag e from the meaning of each sentence . The message level representation is a list of discourse domain objects (DDOs ) for the top-level events of interest in the message (e .g ., JOINT-VENTURE events in the joint-venture domain o r CAPABILITY events in the microelectronics domain) . The semantic representation of a phrase in the text onl y includes information contained nearby in a sentence ; in creating a DDO, the discourse module must infer other longdistance or indirect relations not explicitly found by the semantic"
M93-1010,H93-1045,1,0.711868,"the past two years, we have evaluated muc h of our effort in porting our data extraction system (PLUM) to a new language (Japanese) and to two new domains . KEY SYSTEM FEATURES Three key design features distinguish PLUM : statistical language modeling, learning algorithms and partia l understanding . The first key feature is the use of statistical modeling to guide processing . For the version of PLUM used in MUC-5, part of speech information was determined by using well-known Markov modeling technique s embodied in BBN&apos;s part-of-speech tagger POST [5] . We also used a correction model, AMED [3], for improvin g Japanese segmentation and part-of-speech tags assigned by JUMAN . For the microelectronics domain, we used a probabilistic model to help identify the role of a company in a capability (whether it is a developer, user, etc .) . Statistical modeling in PLUM contributes to portability, robustness, and trainability . The second key feature is our use of learning algorithms both to obtain the knowledge bases used by PLUM&apos; s processing modules and to train the probabilistic algorithms . We feel the key to portability of a data extractio n system is automating the acquisition of the"
M93-1010,H91-1037,1,0.930035,"nd key feature is our use of learning algorithms both to obtain the knowledge bases used by PLUM&apos; s processing modules and to train the probabilistic algorithms . We feel the key to portability of a data extractio n system is automating the acquisition of the knowledge bases that need to change for a particular language o r application . For the MUC-5 applications we used learning algorithms to train POST, AMED, and the template filler model mentioned above . We also used a statistical learning algorithm to learn case frames for verbs fro m examples (the algorithm and empirical results are in [4]) . A third key feture is partial understanding, by which we mean that all components of PLUM are designed t o operate on partially interpretable input, taking advantage of information when available, and not failing whe n information is unavailable . Neither a complete grammatical analysis nor complete semantic interpretation i s required. The system finds the parts of the text it can understand and pieces together a model of the whole from thos e part and their context . PROCESSING STAGE S The PLUM architecture is presented in Figure 1 . Ovals represent declarative knowledge bases ; rectangl"
M93-1010,J93-2006,1,0.692451,"genda approximately three years ago . During the past two years, we have evaluated muc h of our effort in porting our data extraction system (PLUM) to a new language (Japanese) and to two new domains . KEY SYSTEM FEATURES Three key design features distinguish PLUM : statistical language modeling, learning algorithms and partia l understanding . The first key feature is the use of statistical modeling to guide processing . For the version of PLUM used in MUC-5, part of speech information was determined by using well-known Markov modeling technique s embodied in BBN&apos;s part-of-speech tagger POST [5] . We also used a correction model, AMED [3], for improvin g Japanese segmentation and part-of-speech tags assigned by JUMAN . For the microelectronics domain, we used a probabilistic model to help identify the role of a company in a capability (whether it is a developer, user, etc .) . Statistical modeling in PLUM contributes to portability, robustness, and trainability . The second key feature is our use of learning algorithms both to obtain the knowledge bases used by PLUM&apos; s processing modules and to train the probabilistic algorithms . We feel the key to portability of a data extractio n"
M93-1010,E93-1060,0,\N,Missing
M93-1010,1991.mtsummit-papers.11,0,\N,Missing
M98-1009,A97-1029,1,0.686516,"Missing"
M98-1009,P96-1025,0,0.020241,"win Lewis.” 5) Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes. These labels serve to form a continuous chain between the relation and its argument. Figure 3 shows an augmented parse tree corresponding to the semantic annotation in Figure 2. Note that nodes with semantic labels ending in “-r” are MUC reportable names and descriptors. Statistical Model In SIFT’s statistical model, augmented parse trees are generated according to a process similar to that described in Collins (1996, 1997). For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in Weischedel et al. (1993). We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3. At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection depend"
M98-1009,P97-1003,0,0.031701,"Missing"
M98-1009,J93-2004,0,0.0384257,"aining program estimates the parameters of a unified statistical model that accounts for both syntax and semantics. Later, when presented with a new sentence, the search program explores the statistical model to find the most likely combined semantic and syntactic interpretation. syntactic annotations (Penn Treebank) semantic annotations training program training statistical model sentences decoding search program combined semanticsyntactic interpretations Figure 1: Block diagram of sentence-level model. Training Data Our source for syntactically annotated training data was the Penn Treebank (Marcus et al., 1993). Significantly, we do not require that syntactic annotations be from the same source, or cover the same domain, as the target task. For example, while the Penn Treebank consists of Wall Street Journal text, the target source for this evaluation was New York Times newswire. Similarly, although the Penn Treebank domain covers general and financial news, the target domain for this evaluation was space technology. The ability to use syntactic training from a different source and domain than the target is an important feature of our model. Since the Penn Treebank serves as our syntactically annota"
M98-1009,W97-0302,0,0.0350367,"Missing"
N06-2015,I05-1081,1,0.260652,"Missing"
N06-2015,N06-1024,1,0.181596,"ed, allowing the entity mentions that are propositional arguments to be resolved in context. Annotation will cover multiple languages (English, Chinese, and Arabic) and multiple genres (newswire, broadcast news, news groups, weblogs, etc.), to create a resource that is broadly applicable. 2 Treebanking The Penn Treebank (Marcus et al., 1993) is annotated with information to make predicate-argument structure easy to decode, including function tags and markers of “empty” categories that represent displaced constituents. To expedite later stages of annotation, we have developed a parsing system (Gabbard et al., 2006) that recovers both of these latter annotations, the first we know of. A firststage parser matches the Collins (2003) parser on which it is based on the Parseval metric, while simultaneously achieving near state-of-the-art performance on recovering function tags (F-measure 89.0). A second stage, a seven stage pipeline of maximum entropy learners and voted perceptrons, achieves state-of-the-art performance (F-measure 74.7) on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods. * This work"
N06-2015,J93-2004,1,0.0841798,"with parse (TreeBank) and propositional (PropBank) structures, which provide normalization over predicates and their arguments. Word sense ambiguities are then resolved, with each word sense also linked to the appropriate node in the Omega ontology. Coreference is also annotated, allowing the entity mentions that are propositional arguments to be resolved in context. Annotation will cover multiple languages (English, Chinese, and Arabic) and multiple genres (newswire, broadcast news, news groups, weblogs, etc.), to create a resource that is broadly applicable. 2 Treebanking The Penn Treebank (Marcus et al., 1993) is annotated with information to make predicate-argument structure easy to decode, including function tags and markers of “empty” categories that represent displaced constituents. To expedite later stages of annotation, we have developed a parsing system (Gabbard et al., 2006) that recovers both of these latter annotations, the first we know of. A firststage parser matches the Collins (2003) parser on which it is based on the Parseval metric, while simultaneously achieving near state-of-the-art performance on recovering function tags (F-measure 89.0). A second stage, a seven stage pipeline of"
N06-2015,J05-1004,1,0.144555,"Missing"
N06-2015,I05-7009,1,0.218789,"e information extraction, summarization and machine translation. The subtle finegrained sense distinctions in WordNet have not lent themselves to high agreement between human annotators or high automatic tagging performance. Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al., 2004; Palmer et al., 2006), we have developed a process for rapid sense inventory creation and annotation that includes critical links between the grouped word senses and the Omega ontology (Philpot et al., 2005; see Section 5 below). This process is based on recognizing that sense distinctions can be represented by linguists in an hierarchical structure, similar to a decision tree, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntacAnnotate test (2 people) not OK Results: agreement and confusion matrix OK Figure 1. Annotation Procedure As part of OntoNotes we are annotating the most frequent noun and verb se"
N06-2015,P98-1013,0,0.0785645,"her phenomena, including temporal and spatial relations, numerical expressions, deixis, etc. One of the principal aims of OntoNotes is to enable automated semantic analysis. The best current algorithm for semantic role labeling for PropBank style annotation (Pradhan et al., 2005) achieves an F-measure of 81.0 using an SVM. OntoNotes will provide a large amount of new training data for similar efforts. Existing work in the same realm falls into two classes: the development of resources for specific phenomena or the annotation of corpora. An example of the former is Berkeley’s FrameNet project (Baker et al., 1998), which produces rich semantic frames, annotating a set of examples for each predicator (including verbs, nouns and adjectives), and describing the network of relations among the semantic frames. An example of the latter type is the Salsa project (Burchardt et al., 2004), which produced a German lexicon based on the FrameNet semantic frames and annotated a large German newswire corpus. A second example, the Prague Dependency Treebank (Hajic et al., 2001), has annotated a large Czech corpus with several levels of (tectogrammatical) representation, including parts of speech, syntax, and topic/fo"
N06-2015,reeder-etal-2004-interlingual,1,0.282926,"Missing"
N06-2015,W04-2705,0,\N,Missing
N06-2015,W04-2704,1,\N,Missing
N06-2015,P05-1072,0,\N,Missing
N06-2015,C98-1013,0,\N,Missing
P08-1066,2003.mtsummit-papers.6,0,0.111247,"ncy structure does not provide an improvement on performance. However, dependency structures allow the use of a dependency LM which gives rise to significant improvement. 6 Discussion The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005). However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, 584 while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation. Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P (F |E) was employed in the construction of the target forest due to the complexity of the syntax-based LM. Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system. This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees. The use of a dependency LM in MT is similar to the us"
P08-1066,P05-1033,0,0.980442,"cal machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependen"
P08-1066,J07-2003,0,0.903365,"how that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restrict the target side to the so called wellformed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programm"
P08-1066,D07-1079,0,0.159595,"Missing"
P08-1066,P05-1067,0,0.915704,"anslation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restr"
P08-1066,P99-1059,0,0.0204693,"em. After 5-gram rescoring, it achieved 1.21 point improvement in BLEU and 1.19 improvement in TER. The filtered model does not show improvement on BLEU. The filtered string-to-string rules can be viewed the string projection of stringto-dependency rules. It means that just using dependency structure does not provide an improvement on performance. However, dependency structures allow the use of a dependency LM which gives rise to significant improvement. 6 Discussion The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005). However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, 584 while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation. Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P (F |E) was employed in the construction of the target forest due to the complexity of the synta"
P08-1066,W02-1039,0,0.215216,"(Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic programming. 2 String-to-Dependency Translation 2.1 Transfer Rules with Well-Formed Dependency Structures A string-to-dependency grammar G is a 4-tuple G =< R, X, Tf , Te >, where R is a set of transfer rules. X is the only non-terminal, which is similar to the Hiero system (Chiang, 2007). T f is a set of terminals in the source language, and T e is a set of terminals in the target language1 . A string-to-dependency transfer rule R ∈ R is a 4-tuple R =< Sf ,"
P08-1066,N04-1035,0,0.767157,"Missing"
P08-1066,P06-1121,0,0.918149,"uce the model of string-to-dependency decoding. Section 3 illustrates of the use of dependency language models. In section 4, we describe the implementation details of our MT system. We discuss experimental results in section 5, compare to related work in section 6, and draw conclusions in section 7. 1.1 Hierarchical Machine Translation Graehl and Knight (2004) proposed the use of targettree-to-source-string transducers (xRS) to model translation. In xRS rules, the right-hand-side(rhs) of the target side is a tree with non-terminals(NTs), while the rhs of the source side is a string with NTs. Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side. A tree could represent multi-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) 577 Proceedings of ACL-08: HLT, pages 577–585, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics find boy will it"
P08-1066,N04-1014,0,0.0321314,"cy algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target si"
P08-1066,W05-1506,0,0.0472617,"enation rules like X → XX for backup. The 5th feature counts the number of concatenation rules used in a translation. In our system, we allow substitutions of dependency structures with unmatched categories, but there is a discount for such substitutions. Weight Optimization We tune the weights with several rounds of decoding-optimization. Following (Och, 2003), the k-best results are accumulated as the input of the optimizer. Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration. 583 Rescoring We rescore 1000-best translations (Huang and Chiang, 2005) by replacing the 3-gram LM score with the 5-gram LM score computed offline. 5 Experiments We carried out experiments on three models. • baseline: replication of the Hiero system. • filtered: a string-to-string MT system as in baseline. However, we only keep the transfer rules whose target side can be generated by a well-formed dependency structure. • str-dep: a string-to-dependency system with a dependency LM. We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems u"
P08-1066,P95-1037,0,0.10456,"Missing"
P08-1066,W06-1606,0,0.484067,"Missing"
P08-1066,P05-1012,0,0.0801704,"ng, it achieved 1.21 point improvement in BLEU and 1.19 improvement in TER. The filtered model does not show improvement on BLEU. The filtered string-to-string rules can be viewed the string projection of stringto-dependency rules. It means that just using dependency structure does not provide an improvement on performance. However, dependency structures allow the use of a dependency LM which gives rise to significant improvement. 6 Discussion The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005). However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, 584 while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation. Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P (F |E) was employed in the construction of the target forest due to the complexity of the syntax-based LM. Since our de"
P08-1066,J03-1002,0,0.00721665,"onditions in the definitions of corresponding operations on dependency structures and on categories. Theorem 2 (soundness and completeness) Suppose X and Y are well-formed dependency structures. OP(cat(X), cat(Y )) is well-defined for a given operation OP if and only if OP(X, Y ) is well-defined. Furthermore, 581 Rule Extraction Now we explain how we get the string-todependency rules from training data. The procedure is similar to (Chiang, 2007) except that we maintain tree structures on the target side, instead of strings. Given sentence-aligned bi-lingual training data, we first use GIZA++ (Och and Ney, 2003) to generate word level alignment. We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magerman’s rules (1995). Then we use heuristic rules to extract transfer rules recursively based on the GIZA alignment and the target dependency trees. The rule extraction procedure is as follows. 1. Initialization: All the 4-tuples (Pfi,j , Pem,n , D, A) are valid phrase alignments, where source phrase P fi,j is 2 Here we use words instead of word indexes in categories to make the example easy to understand. find it (D1) interesting find X it (D2"
P08-1066,P03-1021,0,0.0534626,"dency language model score 8. Discount on ill-formed dependency structures We have eight features in our system. The values of the first four features are accumulated on the rules used in a translation. Following (Chiang, 2005), we also use concatenation rules like X → XX for backup. The 5th feature counts the number of concatenation rules used in a translation. In our system, we allow substitutions of dependency structures with unmatched categories, but there is a discount for such substitutions. Weight Optimization We tune the weights with several rounds of decoding-optimization. Following (Och, 2003), the k-best results are accumulated as the input of the optimizer. Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration. 583 Rescoring We rescore 1000-best translations (Huang and Chiang, 2005) by replacing the 3-gram LM score with the 5-gram LM score computed offline. 5 Experiments We carried out experiments on three models. • baseline: replication of the Hiero system. • filtered: a string-to-string MT system as in baseline. However, we only keep the transfer rules whose target side can be generated by a well-formed dependency"
P08-1066,J05-1004,0,0.00732772,"Missing"
P08-1066,2001.mtsummit-papers.68,0,0.0451746,"se target side can be generated by a well-formed dependency structure. • str-dep: a string-to-dependency system with a dependency LM. We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems use only one non-terminal label in rules. The major difference is in the representation of target structures. We use dependency structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding. All models are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric. We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5 , and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset"
P08-1066,P05-1034,0,0.751191,"w framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restrict the target side t"
P08-1066,C90-3045,0,0.358843,"-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) 577 Proceedings of ACL-08: HLT, pages 577–585, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics find boy will it interesting the Figure 1: The dependency tree for sentence the boy will find it interesting followed the tree-to-tree approach (Shieber and Schabes, 1990) for translation. In their models, dependency treelets are used to represent both the source and the target sides. Decoding is implemented as tree transduction preceded by source side dependency parsing. While tree-to-tree models can represent richer structural information, existing tree-totree models did not show advantage over string-totree models on translation accuracy due to a much larger search space. One of the motivations of our work is to achieve desirable trade-off between model capability and search space through the use of the so called wellformed dependency structures in rule repr"
P08-1066,2006.amta-papers.25,0,0.026208,"tring-to-dependency system with a dependency LM. We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems use only one non-terminal label in rules. The major difference is in the representation of target structures. We use dependency structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding. All models are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric. We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5 , and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset was also used to train a 3-gram dependency LM. Traditional 3-gram and 5-gram LMs w"
P08-1066,D07-1078,0,0.0405286,"Rule Coverage Marcu et al. (2006) showed that many useful phrasal rules cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al., 2006). For example, the following rule • <(hong)Chinese , (DT(the) JJ(red))English > is not a valid string-to-tree transfer rule since the red is a partial constituent. A number of techniques have been proposed to improve rule coverage. (Marcu et al., 2006) and (Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic"
P08-1066,P02-1025,0,0.0262185,"Missing"
P08-1066,P02-1040,0,\N,Missing
P08-1066,W90-0102,0,\N,Missing
P08-1066,D08-1076,0,\N,Missing
P11-2050,D08-1029,1,0.904515,"Missing"
P11-2050,P06-1017,0,0.0307936,"g surface patterns for relation identification; others have explored similar approaches (e.g. Pantel & Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 & Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. They measure the accuracy of relation extraction alone, without including the added challenge of resolving non-specific relation arguments to name references. They limit their studies to the small ACE corpora where mention markup is manually encoded. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relat"
P11-2050,H01-1024,1,0.825924,"Missing"
P11-2050,P02-1006,0,0.439169,"ce. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 & Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. They measure the accuracy of relation extraction alone, without including the added challenge of resolving non-specific relation arguments to name references. They limit their studies to the small ACE corpora where mention markup is manually encoded. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. Mitchell et al. (2009), while demonstrating high precision, do not measure recall. In contrast, our study has emphasized recall. A primary focus on precision allows one to ignore many relation texts that require coreference or long-distance dependencies; one primary goal of our work is to measure system performance in exactly those areas. There are at least two reasons to not lose sight of recall. For the majority"
P11-2050,I08-1005,0,0.0177367,"or relation identification; others have explored similar approaches (e.g. Pantel & Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 & Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. They measure the accuracy of relation extraction alone, without including the added challenge of resolving non-specific relation arguments to name references. They limit their studies to the small ACE corpora where mention markup is manually encoded. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be s"
P11-2050,N10-1087,0,0.0308529,"Missing"
P11-2050,P06-1015,0,\N,Missing
P11-2059,P09-1026,0,0.0826191,"Missing"
P11-2059,C10-1117,0,0.12343,"disty, 2010), (Somasundaran& 344 Weibe, 2009). The MPQA corpus (Weibe, 2005) annotates polarity for sentences in newswire, but the focus of this corpus is at the sentence level. Both the MPQA corpus and the various corpora of editorials and reviews have tended towards more formal, edited, non-conversational text. Our work in contrast, specifically targets interactive discussions in an informal setting. Work outside of computational linguistics that has looked at persuasion has tended to examine language in a persuasive context (e.g. sales, advertising, or negotiations). Like the current work, Strzalkowski, et al. (2010) investigates language uses over informal dialogue. Their work focuses on chat transcripts in an experimental setting designed to be rich in the phenomena of interest. Like our work, their predictions operate over the conversation, and not a single utterance. The specific language uses in their work (topic/task control, involvement, and disagreement) are different than those discussed here. Our work also differs in the data type of interest. We work with threaded online discussions in which the phenomena in question are rare. Our annotators and system must distinguish between the language use"
P11-2059,D09-1035,0,\N,Missing
P11-2059,N06-2015,1,\N,Missing
P80-1025,P79-1014,0,0.0266415,"or instance, Carbonell (1979) discusses inferring the meaning of new words. Hendrix, e t . a l . (1978) describe a system that provides a means for naive users to define personalized paraphrases and that l i s t s the items expected next at a point where the parser blocks. Weischedel, e t . a l . (1978) show how to relax both syntactic and semantic constraints such that some classes of ungrammatical or semantically inappropriate input are understood. Kwasnyaod Sondheimer (1979) present techniques for understanding several classes of syntactically ill-formed input. Codd, e t . a l . (1978) and Lebowitz (1979) present alternatives to top-down, l e f t - t o - r i g h t parsers as a means of dealing with some of these problems. of the grammar. Only one page of program code and nine pages of constant character strings for use in messages were added. From the experiment we conclude the following: I. The heuristics are powerful for small natural language front ends to an application domain. 2. The heuristics should also be quite effective in a compiler, where parsing is far more deterministic. 3. The heuristics w i l l be more effective in a semantic grammar or in a parser which frequently interacts wi"
P80-1025,P79-1006,0,\N,Missing
P80-1025,P79-1002,0,\N,Missing
P82-1016,J80-1002,0,\N,Missing
P82-1016,P79-1006,1,\N,Missing
P82-1016,C80-1027,0,\N,Missing
P82-1016,P80-1026,0,\N,Missing
P84-1024,J83-3003,1,0.813004,"Missing"
P84-1024,J76-1005,0,0.128711,"Missing"
P84-1024,P80-1030,0,0.187093,"Missing"
P84-1024,J80-1001,0,0.0678757,"Missing"
P84-1024,P83-1010,0,0.052651,"Missing"
P84-1024,P83-1021,0,\N,Missing
P84-1029,P83-1007,1,\N,Missing
P84-1030,J83-3003,1,\N,Missing
P84-1030,C80-1008,1,\N,Missing
P84-1030,P84-1045,0,\N,Missing
P87-1005,J86-3002,0,0.145349,"within the context of our overall approach to representation of domain knowledge and its use in the IRUS natural language system [5, 6,271. An initial version of IRACQ was reported in [19]. Using IRACQ, mappings Semantic knowledge includes at least two kinds of information: selectional restrictions or case frame constraints which can serve as a filter on what makes sense semantically, and rules for translating the word senses present in an input into an underlying semantic representation. Acquiring such selectional restriction information has been studied in TEAM, the Linguistic String Parser [12], and our system. Acquiring the meaning of the word senses has been studied by several individuals, including [11, 17]. This paper 1The work presented here was supported under DARPA contract #N00014-85-C-0016. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessenly representing the officual policies, either expressed or implied, of the Defense Advanced Research Projects Agency or of the United States Government. 32 3 Dimensions of Acquiring Semantic focuses on acquiring such semantic knowledge using IRACQ. Knowledge We discuss i"
P87-1005,P86-1033,0,0.0222447,"ent system and an ad hoc application system for drawing maps, providing calculations, and preparing summaries; both systems may be accessed from the NLI without the user being particularly aware that there are two systems rather than one underneath the NLI. Finally, a fifth kind of knowledge is a set of domain plans. Though no extensive set of such plans has been developed yet, there is growing agreement that such a library of plans is critical for understanding narrative [20], a user's needs [22], ellipsis [8, 2]. and ill-formed input [28], as well as for following the structure of discourse [14, 15]. Tools for acquiring a large collection of domain plans from a domain expert, rather than an AI expert, have not yet appeared. However, inferring plans from textual examples is under way [17]. 3.2 M e a n i n g r e p r e s e n t a t i o n . Another dimension in the design of a semantic knowledge acquisition tool is the style of the underlying semantic representation for natural language input. One could postulate a unique predicate for almost every word sense of the language. TEAM 33 seems to represent this approach. At some later level of processing than the initial semantic acquisition, a l"
P87-1005,P81-1030,0,0.0593054,"of a clause IRule definition using IRACQ is presented. Section 5 describes initial work on an IRule paraphraser. Conclusions are in section 6. 2 Kinds of Knowledge One kind of knowledge that must be acquired is lexical information. This includes morphological information, syntactic categories, complement structure (if any), and pointers to semantic information associated with individual words. Acquiring lexical information may proceed by prompting a user, as in TEAM [13], IRUS [7], and JANUS [9]. Alternatively, efforts are underway to acquire the information directly from on-line dictionaries [3, 16]. This paper describes our contribution to the acquisition of semantic knowledge as evidenced in IRACQ (for Interpretation Rule ACQuisition), within the context of our overall approach to representation of domain knowledge and its use in the IRUS natural language system [5, 6,271. An initial version of IRACQ was reported in [19]. Using IRACQ, mappings Semantic knowledge includes at least two kinds of information: selectional restrictions or case frame constraints which can serve as a filter on what makes sense semantically, and rules for translating the word senses present in an input into an"
P87-1005,P86-1018,0,0.0511698,"of a clause IRule definition using IRACQ is presented. Section 5 describes initial work on an IRule paraphraser. Conclusions are in section 6. 2 Kinds of Knowledge One kind of knowledge that must be acquired is lexical information. This includes morphological information, syntactic categories, complement structure (if any), and pointers to semantic information associated with individual words. Acquiring lexical information may proceed by prompting a user, as in TEAM [13], IRUS [7], and JANUS [9]. Alternatively, efforts are underway to acquire the information directly from on-line dictionaries [3, 16]. This paper describes our contribution to the acquisition of semantic knowledge as evidenced in IRACQ (for Interpretation Rule ACQuisition), within the context of our overall approach to representation of domain knowledge and its use in the IRUS natural language system [5, 6,271. An initial version of IRACQ was reported in [19]. Using IRACQ, mappings Semantic knowledge includes at least two kinds of information: selectional restrictions or case frame constraints which can serve as a filter on what makes sense semantically, and rules for translating the word senses present in an input into an"
P87-1005,P86-1005,0,0.109021,"c information about the semantic categories in the domain and binary relationships holding between semantic categories. For instance, in the domain of Navy decision-making at a US Reet Command Center, such basic domain facts include: All submarines are vessels. All vessels are units. All units are organizational entities. All vessels have a major weapon system. All units have an overall combat readiness rating. 3.1 C l a s s of u n d e r l y i n g s y s t e m s . One could design tools for a specific subclass of underlying systems, such as database management systems, as in TEAM [13] and TELl [4]. The special nature of the class of underlying systems may allow for a more tailored acquisition environment, by having special-purpose, stereotypical sequences of questions for the user, and more powerful special-purpose inferences. For example, in order to acquire the variety of lexical items that can refer to a symbolic field in a database (such as one stating whether a mountain is a volcano), TEAM asks a series of questions, such as ""Adjectives referencing the positive value?"" (e.g., volcanic), and ""Abstract nouns referencing the positive value?"" (e.g., volcano). The fact that the field i"
P87-1005,P85-1024,0,0.055164,"een used to acquire semantic knowledge for access to both a relational database management system and an ad hoc application system for drawing maps, providing calculations, and preparing summaries; both systems may be accessed from the NLI without the user being particularly aware that there are two systems rather than one underneath the NLI. Finally, a fifth kind of knowledge is a set of domain plans. Though no extensive set of such plans has been developed yet, there is growing agreement that such a library of plans is critical for understanding narrative [20], a user's needs [22], ellipsis [8, 2]. and ill-formed input [28], as well as for following the structure of discourse [14, 15]. Tools for acquiring a large collection of domain plans from a domain expert, rather than an AI expert, have not yet appeared. However, inferring plans from textual examples is under way [17]. 3.2 M e a n i n g r e p r e s e n t a t i o n . Another dimension in the design of a semantic knowledge acquisition tool is the style of the underlying semantic representation for natural language input. One could postulate a unique predicate for almost every word sense of the language. TEAM 33 seems to represent th"
P87-1005,J83-3005,0,0.0609239,"anguage system [5, 6,271. An initial version of IRACQ was reported in [19]. Using IRACQ, mappings Semantic knowledge includes at least two kinds of information: selectional restrictions or case frame constraints which can serve as a filter on what makes sense semantically, and rules for translating the word senses present in an input into an underlying semantic representation. Acquiring such selectional restriction information has been studied in TEAM, the Linguistic String Parser [12], and our system. Acquiring the meaning of the word senses has been studied by several individuals, including [11, 17]. This paper 1The work presented here was supported under DARPA contract #N00014-85-C-0016. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessenly representing the officual policies, either expressed or implied, of the Defense Advanced Research Projects Agency or of the United States Government. 32 3 Dimensions of Acquiring Semantic focuses on acquiring such semantic knowledge using IRACQ. Knowledge We discuss in this section several dimensions available in designing a tool for acquiring semantic knowledge within the overall co"
P87-1005,P86-1036,0,0.0346365,"is achieved by having a level of representation for the concepts, actions, and capabilities of the domain, the domain model, separate from the model of the entities in the underlying system. The meaning representation for an input, a logical form, is given in terms of predicates which correspond to domain model concepts and roles (and are hence referred to as domain mode/ predicates). IRules define the mappings from English to these domain model predicates. In our NLI, a separate component then translates from the meaning representation to the specific representation of the underlying system [24, 25]. IRACQ has been used to acquire semantic knowledge for access to both a relational database management system and an ad hoc application system for drawing maps, providing calculations, and preparing summaries; both systems may be accessed from the NLI without the user being particularly aware that there are two systems rather than one underneath the NLI. Finally, a fifth kind of knowledge is a set of domain plans. Though no extensive set of such plans has been developed yet, there is growing agreement that such a library of plans is critical for understanding narrative [20], a user's needs [2"
P87-1005,1985.tmi-1.23,1,0.792954,"knowledge for access to both a relational database management system and an ad hoc application system for drawing maps, providing calculations, and preparing summaries; both systems may be accessed from the NLI without the user being particularly aware that there are two systems rather than one underneath the NLI. Finally, a fifth kind of knowledge is a set of domain plans. Though no extensive set of such plans has been developed yet, there is growing agreement that such a library of plans is critical for understanding narrative [20], a user's needs [22], ellipsis [8, 2]. and ill-formed input [28], as well as for following the structure of discourse [14, 15]. Tools for acquiring a large collection of domain plans from a domain expert, rather than an AI expert, have not yet appeared. However, inferring plans from textual examples is under way [17]. 3.2 M e a n i n g r e p r e s e n t a t i o n . Another dimension in the design of a semantic knowledge acquisition tool is the style of the underlying semantic representation for natural language input. One could postulate a unique predicate for almost every word sense of the language. TEAM 33 seems to represent this approach. At some later"
P87-1005,H86-1008,0,\N,Missing
P89-1024,P87-1005,1,0.804168,"ation program, etc. For example, the constant HARPOON-CAPABLE, which defines a set of vessels equipped with harpoon missiles, is associated with an undedying system model element which states how to select the subset of exactly those vessels. In a Navy relational data base that we have dealt with, the relevant code selects just those records of a table of unit characteristics with a ""Y"" in the HARP field. 4.1.¢ Knowledge Acquisition We have developed two complementary tools to greatly increase our productivity in porting BBN's Janus NL understanding and generation system to new domains. IRACQ [3] supports learning lexical semantics from examples with only one unknown word. IRACQ is used for acquiring the diverse, complex patterns of syntax and semantics arising from verbs, by providing examples of the verb's usage, Since IRACQ assumes that a large vocabulary is available for use in the training examples,"" a way to rapidly infer the knowledge bases for the overwhelming majority of words is an invaluable complement. KNACQ [33] serves that purpose. The domain model is used to organize, guide, and assist in acquiring the syntax and semantics of domain-specific vocabulary. Using the browsi"
P89-1024,P89-1030,0,0.0174038,"ery high. 2. I walked into the room. The chandeliers sparkled brightly. 3. I went shopping yesterday. The time I started was 3 PM. We believe a taxonomic domain model provides the basis for an efficient algorithm for a broad class of examples of bridging, though we do not believe that it will cover all cases. If A is the class of a discourse entity arising from previous utterances, then any entity of class B, such that the NIKL domain model has a role from A to B (or from B to A) can be referred to by a definite NP. This has not yet been integrated into the Janus model of reference processing [4]. 4.3.2. Metonymy Unstated relations in a communication must be inferred for full understanding of nominal compounds and metonymy. Those that can be anticipated can be built into the lexicon; the challenge is to deal with those that are novel to Janus. Finding the omitted relation in novel nominal compounds using a taxonomy has been explored and reported elsewhere [13]. We propose treating many metonymy in the following way: novel cases of 1. Wherepatterns of metonymy can be identified,, such as using a description of a part to refer to the whole (and other patterns identified in [17]), pro-co"
P89-1024,P88-1012,0,0.0210113,"record the failed restriction with the partial interpretation for possible future processing, after all attempts at a literal interpretation of the input have failed. 3. If no literal interpretation of the input can be found, look among the precompiled relations of step 1 above for any class that could be so related to the class of the NP that appears. 4. If a relation is applicable, attempt to resume interpretation assuming the referent of the NP is in the related class. This has not been implemented, but offers an efficient alternative to the abductive theorem-proving approach described in [16]. second definition of cat (p. 150) is ""an animal related to this such as the lion or tiger"" (italics added). Such a vague definition helped us little in axiomatizing the notion. 5. T o p - L e v e l A b s t r a c t i o n s in t h e N I K L Taxonomy WML and NIKL together provide a framework for representation. The highest concepts and relations in the NIKL network provide a representational style in which more concrete constantsmust fit. The first abstraction structure used in Janus was the USC/ISI ""upper structure"" [19]. Because it seemed tied to systemic linguistics in critical ways, rather"
P89-1024,H86-1022,0,0.028994,"iway ambiguous words in the base vocabulary. The appeal, of course, is that if these basic notions were sufficient to define 56,000 words, they are generally applicable, providing a candidate for general-purpose primitives. • KL-TWO[31], which marries a frame system (NIKL) with propositional logic (RUP[20]), Limited inference in propositional logic is the goal of KL-'FWO. Limited aspects of universal"" quantification are achieved via allowing demons in the inference process. KL-TWO and its classification algorithm [27] are at the heart of the lexicalization process of the text generator Penman [28]. The course of action we followed was to build a taxonomy for all of the definitions of approximately 200 items from the base vocabulary using the defini. tJons of those vocabulary items themselves in the dictionary. In this attempt, we encountered the following difficulties: • Definitions of the base vocabulary often involved circularity. • Definitions included assertional information and/or knowledge appropriate in defeasible reasoning, which are not fully supported by NIKL. For example, the first definition of cat is ""a small four-legged animal with soft fur and sharp claws, often kept as"
P89-1024,P84-1024,1,0.921129,"the complexity of the proposition to the complexity of the utterance; that simplicity is at the expense of using a more powerful semantic interpreter and of sacrificing compositionality in those cases where language itself appears non-compositional. • Real-time inference strategies are a challenge for so rich a logic. However, our hypothesis is that large classes of the linguistic examples requiring common sense reasoning can be 194 handled using limited inference algorithms on a taxonomic language. Arguments supporting this hypothesis appear in [2, 13] for interpreting nominal compounds; in [6, 7, 29], for common sense reasoning about modifier attachment; and in [32] for phenomena in definite reference resolution. This second disadvantage, the goal of tractable, real.time inference strategies, is the basis for adding taxonomic reasoning to WML, giving a hybrid representation. 2.2. W h y a T a x o n o m i c L a n g u a g e Our hypothesis is that much of the reasoning needed in semantic processing can be supported by a taxonomy. The ability to pre-compile pre-specified inferential chains, to index them via concept name and role name, and to employ taxonomic inheritance for organizing knowled"
P89-1024,P88-1003,0,0.248329,"e of worlds is to be examined. The potential of timevarying entities existed in some of the applications as well, whether attribute values (as in How often has U$$ Enterprise been C3?) or entities (When was CV22 decommissioned~ The time and world indices of WML provided the opportunity to address such semantic phenomena (though a modal temporal logic or other logics might serve this prupose). • Distributive/collective quantification. Collective readings could arise, though they appear rare, e.g., Do USS Frederick's capabilities include anti.submarine warfare or When did the ships collide? See [25] for a computational treatment of distributive/collective readings in WML. • Generics and Mass Terms. Mass terms and generally true statements arise in these applications, such as in Do nuclear carriers carry JP5?, where JP5 is a kind of jet fuel. Term-forming operators and operators on predicates are one approach and can be accommodated in intensional logics. • Propositional Attitudes. Statements of user preference, e.g., I want to leave in the afternoon, should be accommodated in interfaces to expert systems, as should statements of belief, I believe I must fly with a U.S. carrier. Since int"
P89-1024,H89-1013,1,0.833246,"We have developed two complementary tools to greatly increase our productivity in porting BBN's Janus NL understanding and generation system to new domains. IRACQ [3] supports learning lexical semantics from examples with only one unknown word. IRACQ is used for acquiring the diverse, complex patterns of syntax and semantics arising from verbs, by providing examples of the verb's usage, Since IRACQ assumes that a large vocabulary is available for use in the training examples,"" a way to rapidly infer the knowledge bases for the overwhelming majority of words is an invaluable complement. KNACQ [33] serves that purpose. The domain model is used to organize, guide, and assist in acquiring the syntax and semantics of domain-specific vocabulary. Using the browsing facilities, graphical views, and consistency checker of KREME[1] on NIKL taxonomies, one may select any concept or role for knowledge acquisition. KNACQ presents the user with a few questions and menus to elicit the English expressions used to refer to. that concept or role. To illustrate the kinds of information that must be acquired consider the examples in Figure 4. The vessel speed of Vinson The vessels with speed above 20 kno"
P90-1029,P82-1007,0,0.062856,"lieve this restriction to be typical in NL systems. Most approaches treat this as an inference problem. It can be visualized as finding a relation between two nominal notions faculty member and phone number [1,2]. One such path uses the relation OFFICE(PERSON, ROOM) followed by the relation PHONE(ROOM,PHONE-NUMBER). A general heuristic is to use the shortest path. Computing hidden joins complicates the search space in searching for a solution among the underlying services, as can be seen in the architectures proposed, e.g., [1,4, 9]. 4.3. C o l l a p s e of information. It has long been noted [5] that a complex relation may be represented in a boolean field in a data base, such as the boolean field of the Navy Blue file which for a given vessel was T/F depending on whether there was a doctor onboard the vessel. There was no information about doctors in the data base, except for that field. In a medical data base, a In contrast to the typical approach where one 231 infers the hidden join as needed, we believe such joins are normally anticipatable, and provide support in our lexical definition tools (KNACQ) for specifying them. In KNACQ [15], a knowledge engineer, data base administrato"
P90-1029,P86-1036,0,0.0254745,"ed not derive programs for terms that it does not already know. For the example Inherent in the collectio/: of services covering a DNF expression is the data flow that combines the services into a program to fulfill the DNF request. The next step in the formulatior, process is data flow analysis to extract the data ~low graph corresponding to an abstract program fulfillin~ the request. &apos;Thedistancefunctiontakesanyphysicalobjectsas its arguments and looksuptheirlocation. 230 above, the system should b e expected to respond I don&apos;t know how to compute square root. similar phenomenon was noticed [11]; patient records contained a T/F field depending on whether the patient&apos;s mother had had melanoma, though there was no other information on the patient&apos;s mother or her case of melanoma. By making that assumption, we know that all concepts and relations in the domain model, that is, all primitives appearing in WML as input to the MUS component, have a translation specified by the applications programmer to a composition of underlying services. As stated in Section 2, we further restrict the goals of the MUS component to synthesize programs of a simple structure: acyclic data flow graphs of ser"
P90-1029,P89-1024,1,0.547201,"ology does insulate them from the underlying implemantation idiosyncrasies of one application will expect that our models of language and understanding will extend to simultaneous access of several applications. 2. Scope of the Problem Our view of access to multiple underlying systems is given in Figure 2. As implied in the graphical representation, the user&apos;s request, whatever its modality, is translated into an internal representation of the meaning of what the user needs. We initially explored a first-order logic for this purpose; however, in Janus [13] we have adopted an intensional logic [3, 14] to investigate whether intensional logic offers 227 more appropriate representations for applications more complex than databases, e.g., simulations and other calculations in hypothetical situations. From the statement of what the user needs, we next derive a statement of how to fulfill that need, an execution p/an composed of abstract commands. The execution plan takes the form of a limited class of data flow graphs for a virtual machine that includes the capabilities of all of the application systems. At the level of that virtual machine, specific commands to specific underlying systems are"
P90-1029,H89-1013,1,0.910681,"ing Cases Here we present several well-known challenging classes of problems in translating from logical form to programs. T)))) (OSGP- ENTITY-OVERALL-READINESS-OF ?JX699 C1))))) TiME WORLD)) 4.1. Deriving procedures from descriptions. The challenge is to find a compromise between arbitrary program synthesis and a useful class of program derivation problems. Suppose the user asks for the square root of a value, when the system does not know the meaning of square root, as in Find the square root of the sum of the squares of the residuals. Various knowledge acquisition techniques, such as KNACQ [15], would allow a user to provide syntactic and semantic information for the unknown phrase to be defined. Square root could be defined as a function that computes the number that when multiplied times itseff is the same as the input. However, that is a descriptive definition of square root without any indication of how to compute it. One still must synthesize a program that computes square root; in fact, in early literature on automatic programming and rigorous approaches to developing programs, deriving a program to compute square root was often used as an example problem. (#s (CONTEXT :OPERAT"
P90-1029,H86-1008,0,\N,Missing
W00-1312,W97-0119,0,0.0606653,"result in a substantial increase in the coverage of the query terms. We categorized the missing terms and found that most of them are proper nouns (especially locations and person names), highly technical terms, or numbers. Such words understandably do not normally appear in traditional lexicons. Translation of numbers can be solved using simple rules. Transliteration, a technique that guesses the likely translations of a word based on pronunciation, can be readily used in translating proper nouns. Another technique is automatic discovery of translations from parallel or non-parallel corpora (Fung and Mckeown, 1997). Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies. The lower graph in Figure 1 plots the retrieval performance as a function of the percent of the full lexicon. The figure shows that short queries are more susceptible to incompleteness of the 100 10 Using a Parallel Corpus parallel corpus, more weight should be given to the probability estimates from the corpus. In this section we estimate translation probabilities from a parallel corpus rather"
W00-1312,oard-1998-comparative,0,0.0586148,"Missing"
W10-0908,D08-1029,1,0.822003,"Missing"
W10-0908,W99-0613,0,0.0606913,"Missing"
W10-0908,P09-1113,0,0.0644265,"Missing"
W10-0908,H01-1024,1,0.251069,"Missing"
W10-0908,P02-1006,0,0.289224,"ated examples, which are expensive and time-consuming to create. Co-training circumvents this weakness by playing off two sufficiently different views of a data set to leverage large quantities of unlabeled data (along with a few examples of labeled data), in order to improve the performance of a learning algorithm (Mitchell and Blum, 1998). Co-training will offer our approach to simultaneously learn the patterns of expressing a relation and its arguments. Other researchers have also previously explored automatic pattern generation from unsupervised text, classically in (Riloff & Jones 1999). Ravichandran and Hovy (2002) reported experimental results for automatically generating surface patterns for relation identification; others have explored similar approaches (e.g. Agichtein & Gravano 2000 or Pantel & Pennacchiotti, 2006). More recently (Mitchell et al., 2009) has shown that for macro-reading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. We depart from this work by learning patterns that use the structural features of text-graph patterns and our particular approach to pattern and pair scoring and selection. Most approaches to automat"
W10-0908,P06-1015,0,\N,Missing
W11-1901,W06-0609,1,0.725549,"he Switchboard Treebank. Given the frequency of disfluencies and the performance with which one can identify them automatically,8 a probable processing pipeline would filter them out before parsing. Since we did not have a readily available tagger for tagging disfluencies, we decided to remove them using oracle information available in the Treebank. Propositions The propositions in OntoNotes constitute PropBank semantic roles. Most of the verb predicates in the corpus have been annotated with their arguments. Recent enhancements to the PropBank to make it synchronize better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify"
W11-1901,P06-1005,0,0.929236,"ers including the parses, semantic roles, word senses, and named entities. As is customary for CoNLL tasks, there were two tracks, closed and open. For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data. 4.2.1 Closed Track In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by Bergsma and Lin (2006). For the training and test data, in addition to the underlying text, predicted versions of all the supplementary layers of annotation were provided, where those predictions were derived using off-the-shelf tools (parsers, semantic role labelers, named entity taggers, etc.) as described in Section 4.4.2. For the training data, however, in addition to predicted values for the other layers, we also provided manual gold-standard annotations for all the layers. Participants were allowed to use either the gold-standard or predicted annotation for training their systems. They were also free to use t"
W11-1901,W10-4305,0,0.156408,"Missing"
W11-1901,N01-1016,0,0.0275817,"ze better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. 9 doc/propbank/english-propbank.pdf lease. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the Release 1.0 of the Proposition Bank. This functionality is part of the OntoNotes DB Tool.10 Word Sense Gold word sense annotation was supplied using sense numbers as specified in the O"
W11-1901,P05-1022,0,0.0274644,"coreference but that have been annotated for other layers. For training 10 11 http://cemantix.org/ontonotes.html It should be noted that word sense annotation in OntoNotes is note complete, so only some of the verbs and nouns have word sense tags specified. 10 Senses Lemmas 1 2 &gt;2 1,506 1,046 1,016 Table 6: Word sense polysemy over verb and noun lemmas in OntoNotes models for each of the layers, where feasible, we used all the data that we could for that layer from the training portion of the entire OntoNotes release. Parse Trees Predicted parse trees were produced using the Charniak parser (Charniak and Johnson, 2005).12 Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were appropriately extended. The parser was then re-trained on the training portion of the release 4.0 data using 10-fold crossvalidation. Table 5 shows the performance of the re-trained Charniak parser on the CoNLL-2011 test set. We did not get a chance to re-train the re-ranker, and since the stock re-ranker crashes when run on nbest parses containing NMLs, because it has no"
W11-1901,N07-1011,0,0.525918,"fined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In"
W11-1901,N07-1030,0,0.0811053,"to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress,"
W11-1901,N10-1061,0,0.393386,"y work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surfa"
W11-1901,N01-1008,0,0.21035,"Missing"
W11-1901,N06-2015,1,0.528696,"2009) devoted to joint learning of syntactic and semantic dependencies. A principle ingredient for joint learning is the presence of multiple layers of semantic information. One fundamental question still remains, and that is – what would it take to improve the state of the art in coreference resolution that has not been attempted so far? Many different algorithms have been tried in the past 15 years, but one thing that is still lacking is a corpus comprehensively tagged on a large scale with consistent, multiple layers of semantic information. One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) is to explore whether it can fill this void and help push the progress further – not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that cov1 2 http://projects.ldc.upenn.edu/ace/data/ http://www.bbn.com/nlp/ontonotes 2 ers entities and events not limited to noun phrases or a limited set of entity types. A small portion of this corpus from the newswire and broadcast news genres (∼120k) was recently used for a S EM E VAL task (Recasens et al., 2010)."
W11-1901,H05-1004,0,0.943059,"Missing"
W11-1901,J93-2004,1,0.0615149,"Missing"
W11-1901,P00-1023,0,0.0803844,"partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collabor"
W11-1901,P10-1142,0,0.220993,"and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC). These corpora were tagged with coreferring entities identified by noun phrases in the text. The de facto standard datasets for current coreference studies are the MUC (Hirschman and Chin1 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1–27, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics chor, 1997; Chinchor, 2001; Chinchor and Sun"
W11-1901,J05-1004,1,0.333011,"Missing"
W11-1901,passonneau-2004-computing,0,0.00517084,"ing and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step towards improving"
W11-1901,W05-0311,0,0.0114861,", but represent small training and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step"
W11-1901,P09-5006,0,0.0282076,"Missing"
W11-1901,N06-1025,0,0.730558,"d-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gende"
W11-1901,D09-1101,0,0.522965,"annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques st"
W11-1901,W09-2411,0,0.155304,"Missing"
W11-1901,J01-4004,0,0.993465,"stering them into equivalence classes, has been well recognized in the natural language processing community. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of"
W11-1901,P09-1074,0,0.237943,"ry of evaluations on coreference tasks, variation in the evaluation criteria and in the training data used have made it difficult for researchers to be clear about the state of the art or to determine which particular areas require further attention. There are many different parameters involved in defining a coreference task. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is somewhat easier (Culotta et al., 2007). Given the space constraints, we refer the reader to Stoyanov et al. (2009) for a detailed treatment of the issue. Limitations in the size and scope of the available datasets have also constrained research progress. The MUC and ACE corpora are the two that have been used most for reporting comparative results, but they differ in the types of entities and coreference annotated. The ACE corpus is also one that evolved over a period of almost five years, with different incarnations of the task definition and different corpus cross-sections on which performance numbers have been reported, making it hard to untangle and interpret the results. The availability of the OntoN"
W11-1901,D07-1052,0,0.0160767,"ic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entiti"
W11-1901,M95-1005,0,0.967872,"2011 coreference task are likely to be lower than for coref evaluations based on MUC, where the mention spans are specified in the input,17 or those based on ACE data, where an approximate match is often allowed based on the specified head of the NP mention. 4.5.1 Metrics As noted above, the choice of an evaluation metric for coreference has been a tricky issue and there does not appear to be any silver bullet approach that addresses all the concerns. Three metrics have been proposed for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric (Vilain et al., 1995), ii) The mention based B - CUBED metric (Bagga and Baldwin, 1998) and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric (Luo, 2005). Very recently BLANC (BiLateral Assessment of NounPhrase Coreference) measure (Recasens and Hovy, 17 2011) has been proposed as well. Each of the metric tries to address the shortcomings or biases of the earlier metrics. Given a set of key entities K, and a set of response entities R, with each entity comprising one or more mentions, each metric generates its variation of a precision and recall measure. The MUC measure if the oldest and mos"
W11-1901,W08-2121,0,\N,Missing
W11-1901,E06-2015,0,\N,Missing
W11-1901,D08-1067,0,\N,Missing
W11-1901,S10-1001,0,\N,Missing
W11-1901,doddington-etal-2004-automatic,1,\N,Missing
W11-1901,W04-2327,0,\N,Missing
W13-0908,W09-2206,0,0.00492095,"epts We found that most of our potential source concepts did not correspond to any LDA topic. However, many of these, such as wild west, have fairly strong word co-occurrence patterns, so they plausibly could be found by a different topic modeling algorithm. There are two promising approaches here which could potentially be combined. The first is to use a hierarchical LDA algorithm (Blei et al, 2003b) to allow concepts to align to topics with varying degrees of granularity, from the very general (e.g. war) to the very specific (e.g. wild west). The second is to use constrained LDA approaches (Andrzejewski and Zhu, 2009; Hu et al., 2010) to attempt to force at least one topic to correspond to each of our seed concept lists. A different approach would leave behind seed lists entirely. In our current approach, only about one third of the topics modeled by LDA are successfully aligned with a source concept from our hand-made list. However, some non-aligned LDA topics have properties similar to those that were chosen to represent source concepts. For instance, the topic whose highest ranked terms are [institute, professor, engineering, degree] is comprised of a set of semantically coherent and concrete terms, an"
W13-0908,P98-1013,0,0.0232865,"timate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate amount of raw text in the target language. This work is similar to Bethard et al. (2009), in which an SVM model is trained with LDA-based features to recognize metaphorical text. There the work is framed as a classification task, and supervised methods are used to label metapho"
W13-0908,W09-2002,0,0.182466,"d Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate amount of raw text in the target language. This work is similar to Bethard et al. (2009), in which an SVM model is trained with LDA-based features to recognize metaphorical text. There the work is framed as a classification task, and supervised methods are used to label metaphorical and literal text. Here, the task is one of recognition, and we use heuristic-based, unsu1 See Shutova (2010) for a survey of existing approaches 58 Proceedings of the First Workshop on Metaphor in NLP, pages 58–66, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics pervised methods to identify the presence of metaphor in unlabeled text. We hope to eliminate the need for l"
W13-0908,E06-1042,0,0.0196362,"concept system to describe the same target concept. As Thornborrow notes, the implied British conception of security as “concrete, fixed, and immobile” contrasts deeply with the French conception of security as “a system as a series of processes.” Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate amount of raw text in the"
W13-0908,J91-1003,0,0.156805,"security)” exemplify the French use of the more abstract source concept system to describe the same target concept. As Thornborrow notes, the implied British conception of security as “concrete, fixed, and immobile” contrasts deeply with the French conception of security as “a system as a series of processes.” Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Bl"
W13-0908,W06-3506,0,0.233675,"ibe the same target concept. As Thornborrow notes, the implied British conception of security as “concrete, fixed, and immobile” contrasts deeply with the French conception of security as “a system as a series of processes.” Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate amount of raw text in the target language. This"
W13-0908,W07-0103,0,0.036625,"ncept. As Thornborrow notes, the implied British conception of security as “concrete, fixed, and immobile” contrasts deeply with the French conception of security as “a system as a series of processes.” Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate amount of raw text in the target language. This work is similar to Bethard et"
W13-0908,J04-1002,0,0.0458117,"stract source concept system to describe the same target concept. As Thornborrow notes, the implied British conception of security as “concrete, fixed, and immobile” contrasts deeply with the French conception of security as “a system as a series of processes.” Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate am"
W13-0908,peters-peters-2000-lexicalised,0,0.157431,"French use of the more abstract source concept system to describe the same target concept. As Thornborrow notes, the implied British conception of security as “concrete, fixed, and immobile” contrasts deeply with the French conception of security as “a system as a series of processes.” Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only a"
W13-0908,D09-1026,0,0.00481856,"of a set of semantically coherent and concrete terms, and could be assigned a reasonably accurate label such as higher education. If we were to choose LDA topics based on the terms’ coherence and concreteness (and perhaps other relevant, measurable properties), then assign a label using a method such as that in Mei et al. (2007), we would be able to leverage more of the concepts in the LDA model. This would increase the recall of our system, and also reduce some of the confusion associated with incorrect labeling of concepts in linguistic and conceptual metaphors. Applying Labeled LDA, as in Ramage et al. (2009), would be a similar approach. 8.3 Confusion of Literal and Metaphorical Usage of Source Concepts Another major problem was the confusion between literal and metaphorical usage of source terms. This is partly addressed by our document topics filter, but more sophisticated use of document context for this purpose would be helpful. A similar 65 Fixed Expressions Some of our errors were due to frequent fixed phrases which included a word strongly associated with a source topic, like Tea Party. Minimum description length (MDL) phrase-finding or similar techniques could be used to filter these out."
W13-0908,C10-1113,0,0.0462257,"he implied British conception of security as “concrete, fixed, and immobile” contrasts deeply with the French conception of security as “a system as a series of processes.” Our ultimate goal is to use metaphor to further our knowledge of how different cultures understand complex topics. Our immediate goal in this paper is to create an automated system to find instances of metaphor in English and Spanish text. Most existing work on metaphor identification (Fass, 1991; Martin, 1994; Peters and Peters, 2000; Mason, 2004; Birke and Sarkar, 2006; Gegigan et al., 2006; Krishnakumaran and Zhu, 2007; Shutova et al., 2010; Shutova et al., 2012)1 has relied on some or all of handwritten rules, syntactic parsing, and semantic databases like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998). This limits the approaches to languages with rich linguistic resources. As our ultimate goal is broad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate amount of raw text in the target language. This work is similar to Bethard et al. (2009), in which a"
W13-0908,P10-1071,0,0.0627282,"oad, cross-linguistic application of our system, we cannot rely on resources which would be unavailable in resource-poor languages. Instead, we apply LDA topic modeling (Blei et al., 2003b) which requires only an adequate amount of raw text in the target language. This work is similar to Bethard et al. (2009), in which an SVM model is trained with LDA-based features to recognize metaphorical text. There the work is framed as a classification task, and supervised methods are used to label metaphorical and literal text. Here, the task is one of recognition, and we use heuristic-based, unsu1 See Shutova (2010) for a survey of existing approaches 58 Proceedings of the First Workshop on Metaphor in NLP, pages 58–66, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics pervised methods to identify the presence of metaphor in unlabeled text. We hope to eliminate the need for labeled data which, as discussed in Bethard et al. (2009) and elsewhere, is very difficult to produce for metaphor recognition. 2 Family Farming Fight Pathways Physical structure Planning Weight Wild west Table 1: English Source Concepts 3 High-level system overview Terminology We will refer to a particu"
W13-0908,J13-2003,0,\N,Missing
W13-0908,C98-1013,0,\N,Missing
W13-0908,P11-1026,0,\N,Missing
X93-1019,M92-1024,1,0.737512,"oughly sequential processing of the sentences of an input document through the modules. After the message reader processes the whole document, each sentence is processed through the morphological analyzer, concept-based pattern marcher, parser, semantic interpreter, and the anaphora resolution portion of the discourse processor. After all the sentences are processed in this fashion, final document-level discourse processing and template-generation take place. A more detailed description of the system components, their individual outputs, and their knowledge bases is presented in Ayuso et al., [1]. The processing modules are brieflydescribed below. 3.1 Message Reader This module is like the &quot;text zoner&quot; of Hobbs' description of generic data extraction systems. PLUM's specification of the input format is a declarative component of the message reader, allowing the system to be easily adapted to handle different formats. The input to the PLUM system is a file containing one or more messages. The message reader module determines message boundaries, identifies the message header information, and determines paragraph 195 and sentence boundaries. To date, we have designed format specification"
X93-1019,M91-1036,1,0.322562,"entry indicates that the semantic type is JOINTVENTURE, and that a &quot;with&quot; or &quot;between&quot; PP argument whose type is ENTITY should be given the role PARENTOF, and a &quot;for&quot; PP argument of type ACTIVITY should be given the role ACTIVITY-OF. We used an automatic case frame induction procedure to construct an initial version of the lexicon [4]. Word senses in the semantic lexicon have probability assignments. For MUC-5 probabilities were (automatically) assigned so that each word sense is more probable than the next sense, as entered in the lexicon. 3.6 Discourse Processing PLUM's discourse component [2] performs the operations necessary to create a meaning for the whole message from the meaning of each sentence. The message level representation is a list of discourse domain objects (DDOs) for the events of interest in the message (e.g., JOINTV E N T U R E events in the j o i n t - v e n t u r e domain, CAPABILITY events in the microelectronics domain or ENTITIES in both domains). The semantic representation of a sentence only includes information contained within it; in creating a DDO, the discourse module must infer other long-distance or indirect relations not explicitly found by the seman"
X93-1019,H93-1045,1,0.187273,"ears ago. During the past two years, we have ported our data extraction system (PLUM) to a new language (Japanese) and to two new domains. 2. KEY SYSTEM FEATURES Three key design features distinguish PLUM from other approaches: statistical language modeling, learning algorithms and partial understanding. The first key feature is the use of statistical modeling to guide processing. For the version of PLUM used in MUC-5, part of speech information was determined by using well-known Markov modeling techniques embodied in BBN's part-of-speech tagger POST [5]. We also used a correction model, AMED [3], for improving Japanese segmentation and part-ofspeech tags assigned by JUMAN. For the microelectronics domain, we used a probabilistic model to help identify the role of a company in a capability (whether it is a developer, user, etc.). Statistical modeling in PLUM contributes to portability, robustness, and trainability. The second key feature is our use of learning algorithms both to obtain the knowledge bases used by PLUM's processing modules and to train the probabilistic *Ralph Weischedel (Principal Investigator), Damaris Ayuso, Sean Boisen, Heidi Fox, Tomoyoshi Matsukawa, Constantine P"
X93-1019,H91-1037,1,0.307511,"Kitawa, Tsutomu Sakai (Boston University), June Abe, Hiroto Hosihi, Yoichi Miyamoto (University of Connecticut), and Scott Miller (Northeastern University) algorithms. We feel the key to portability of a data extraction system is automating the acquisition of the knowledge bases that need to change for a particular language or application. For the MUC-5 applications we used learning algorithms to train POST, AMED, and the template-filler model mentioned above. We also used a statistical learning algorithm to learn case frames for verbs from examples (the algorithm and empirical results are in [4]). A third key feature is partial understanding, by which we mean that all components of PLUM are designed to operate on partially interpretable input, taking advantage of information when available, and not failing when information is unavailable. Neither a complete grammatical analysis nor complete semantic interpretation is required. The system finds the parts of the text it can understand and pieces together a model of the whole from those parts and their context. 3. PLUM SYSTEM DESCRIPTION The PLUM architecture is presented in Figure 3-1. Ovals represent declarative knowledge bases; recta"
X93-1019,J93-2006,1,0.726013,"this research agenda approximately three years ago. During the past two years, we have ported our data extraction system (PLUM) to a new language (Japanese) and to two new domains. 2. KEY SYSTEM FEATURES Three key design features distinguish PLUM from other approaches: statistical language modeling, learning algorithms and partial understanding. The first key feature is the use of statistical modeling to guide processing. For the version of PLUM used in MUC-5, part of speech information was determined by using well-known Markov modeling techniques embodied in BBN's part-of-speech tagger POST [5]. We also used a correction model, AMED [3], for improving Japanese segmentation and part-ofspeech tags assigned by JUMAN. For the microelectronics domain, we used a probabilistic model to help identify the role of a company in a capability (whether it is a developer, user, etc.). Statistical modeling in PLUM contributes to portability, robustness, and trainability. The second key feature is our use of learning algorithms both to obtain the knowledge bases used by PLUM's processing modules and to train the probabilistic *Ralph Weischedel (Principal Investigator), Damaris Ayuso, Sean Boisen, He"
X93-1019,H93-1049,1,0.470545,"Missing"
X96-1028,M92-1024,1,0.792273,"Missing"
X96-1028,M91-1036,0,0.035046,"Missing"
X96-1028,H93-1049,0,0.0284743,"Missing"
X96-1028,H93-1045,1,0.727019,"Missing"
X96-1028,H91-1037,1,0.884911,"Missing"
X96-1028,J93-2006,1,0.826027,"Missing"
X96-1028,X93-1019,1,\N,Missing
X96-1028,P95-1037,0,\N,Missing
X98-1014,A97-1029,1,0.810672,"Missing"
X98-1014,P96-1025,0,0.0206753,"c pointer labels are attached to all of the intermediate nodes. These labels serve to form a continuous chain between the relation and its argument. Figure 3 shows an augmented parse tree corresponding to the semantic annotation in Figure 2. Note that nodes with semantic labels ending in &quot;-r&quot; mark MUC reportable names and descriptors. Once a constrained parse is found, it must be augmented to reflect the semantic structure. Augmentation is a five step process. Statistical Model In SIFT's statistical model, augmented parse trees are generated according to a process similar to that described in Collins (1996, 1997). For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in Weischedel et al. (1993). 1) Nodes are inserted into the parse tree to distinguish names and descriptors that are not bracketed in the parse. For example, the parser produces a single noun phrase with no internal structure for &quot;Lt. Cmdr. David Edwin Lewis&quot;. Addition"
X98-1014,W97-0302,0,0.012406,"p,Cm_l,Wp)= 21 P ( c m I Cp,Chp,Cm_l,Wp) -I-~, 2 P ( c m ICp,Chp,Cm-l) For part-of-speech tags, the mixture components Pruning: Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a threshold of the highest scoring constituent are maintained; all others are pruned. For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that are: P'(t m I Cm, t h, w h) = 21 P(t m I cm, w h ) + 2 2 P(t m ]Cm,th) +2 3 P(t m I c m) 80 constituent (Goodman, 1997). We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node. Thus, the scores used in pruning can be considered as the product of: 1. The probability of generating a constituent of the specified category, starting at the topmost node. 2. The probability of generating the structure beneath that constituent, having already generated a constituent of that category. trained on 200 articles annotated with full MUC answer keys, so that even non-local relations were marked. (That level of semantic annot"
X98-1014,J93-2004,0,\N,Missing
X98-1014,J93-2006,1,\N,Missing
X98-1014,P97-1003,0,\N,Missing
