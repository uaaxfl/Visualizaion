2020.emnlp-demos.14,P18-1009,0,0.294604,"et al., 2010; Recasens et al., 2013). Context-based entity typing, the task of assigning semantic types for mentions of entities in textual contexts (e.g., musician, politician, location or battle) therefore has become an important NLP task. While traditional methods often use coarse-grained classes, such as person, location, organization and misc, as targets, recent methods try to classify entities into finergrained types, from hundreds to thousands of them, yet all limited to variants of the real world, like from Wikipedia or news (Lee et al., 2006; Ling and Weld, 2012; Corro et al., 2015; Choi et al., 2018). Entity type information plays an even more important role in literary texts from fictional domains. Fiction and fantasy are core parts of human culture, spanning from traditional folks and myths into books, movies, TV series and games. People have created sophisticated fictional universes such as the Marvel Universe, DC Comics, Middle Earth or Harry Potter. These universes include entities, social structures, and events that are completely different from the real world. Appropriate entity typing for these universes is a prerequisite for several end-user applications. For example, a Game of T"
2020.emnlp-demos.14,2020.emnlp-demos.14,1,0.0809537,"me the second Dark Lord and strove to conquer Arda by creating the Rings” state-of-the-art entity typing methods only return few coarse types for entities, such as person for S AURON and M ELKOR or location for F IRST AGE and A RDA. Moreover, existing methods typically produce predictions for each individual mention, so that different mentions of the same entity may be assigned incompatible types, e.g., A RDA may be predicted as person and location in different contexts. Contribution. The prototype system presented in this demo paper, ENTYFI (fine-grained ENtity TYping on FIctional texts, see Chu et al. (2020) for full details) overcomes the outlined limitations. 100 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 100–106 c November 16-20, 2020. 2020 Association for Computational Linguistics [1] Type System Construction ENTYFI supports long input texts from any kind of literature, as well as texts from standard domains (e.g., news). With the sample text above, ENTYFI is able to predict more specific and meaningful types for entity mentions: M ELKOR : Ainur, Villain S AURON : Maiar, Villain R INGS : Jewelry, Magic Things Taxonomy Induction [2] Reference Universe Ranking Input LSTM Deco"
2020.emnlp-demos.14,D15-1103,1,0.940136,"t al., 2006; Carlson et al., 2010; Recasens et al., 2013). Context-based entity typing, the task of assigning semantic types for mentions of entities in textual contexts (e.g., musician, politician, location or battle) therefore has become an important NLP task. While traditional methods often use coarse-grained classes, such as person, location, organization and misc, as targets, recent methods try to classify entities into finergrained types, from hundreds to thousands of them, yet all limited to variants of the real world, like from Wikipedia or news (Lee et al., 2006; Ling and Weld, 2012; Corro et al., 2015; Choi et al., 2018). Entity type information plays an even more important role in literary texts from fictional domains. Fiction and fantasy are core parts of human culture, spanning from traditional folks and myths into books, movies, TV series and games. People have created sophisticated fictional universes such as the Marvel Universe, DC Comics, Middle Earth or Harry Potter. These universes include entities, social structures, and events that are completely different from the real world. Appropriate entity typing for these universes is a prerequisite for several end-user applications. For"
2020.emnlp-demos.14,P05-1045,0,0.135594,"his demonstration experience in details. LADAN god, body part, arm, etc. 5 Related Work Earliest approaches for entity typing are based on manually designed patterns (e.g., Hearst patterns (Hearst, 1992)) to extract explicit type candidates in given texts. These pattern-based approaches can achieve good precision, but their recall is low, and they are difficult to scale up. Traditional named-entity recognition methods used both rule-based and supervised techniques to recognize and assign entity mentions into few coarse classes like person, location and organization (Sang and De Meulder, 2003; Finkel et al., 2005; Collobert et al., 2011; Lample et al., 2016). Recently, fine-grained namedentity recognition and typing are getting more attention (Ling and Weld, 2012; Corro et al., 2015; Shimaoka et al., 2017; Choi et al., 2018). Ling and Weld (2012) use a classic linear classifier to classify the mentions into a set of 112 types. At much larger scale, FINET (Corro et al., 2015) uses 16k types from the WordNet taxonomy as the targets for entity typing. FINET is a combination of pattern-based, mention-based and verb-based extractors to extract both explicit and implicit type candidates for the mentions fro"
2020.emnlp-demos.14,P17-1044,0,0.0210972,"on Wikia.com, from which 205 reference type systems are induced. Given an input text, ENTYFI then retrieves the most relevant reference type systems and uses them as typing targets. By combining supervised typing method with unsupervised pattern extraction and knowledge base lookups, suitable type candidates are identified. To resolve inconsistencies among candidates, ENTYFI utilizes an integer linear programming (ILP) based consolidation stage. 2 U1: r1 U2: r2 .. u1, u2, ..., un 2.3 Mention Detection To detect entity mentions in the input text, we rely on a BIOES tagging scheme. Inspired by He et al. (2017) from the field of semantic role labeling, we design a BiLSTM network with embeddings and POS tags as input, highway connections between layers to avoid vanishing gradients (Zhang et al., 2016), and recurrent dropout to avoid over-fitting (Gal and Ghahramani, 2016). The output is then put into a decoding step by using dynamic programming to select the tag sequence with maximum score that satisfies the BIOES constraints. The de101 coding step does not add more complexity to the training. 2.4 Mention Typing We produce type candidates for mentions by using a combination of supervised, unsupervise"
2020.emnlp-demos.14,C92-2082,0,0.160285,"database of fictional universes as reference, ENTYFI is able to fill these gaps, predict fictional types in a fine-grained level and remove incompatibilities in the final results. From this interaction, the literature analyst could conclude that the story is much related to The Lord of the Rings, which might help them to draw parallels and direct further manual investigations. Table 1 shows the result of this demonstration experience in details. LADAN god, body part, arm, etc. 5 Related Work Earliest approaches for entity typing are based on manually designed patterns (e.g., Hearst patterns (Hearst, 1992)) to extract explicit type candidates in given texts. These pattern-based approaches can achieve good precision, but their recall is low, and they are difficult to scale up. Traditional named-entity recognition methods used both rule-based and supervised techniques to recognize and assign entity mentions into few coarse classes like person, location and organization (Sang and De Meulder, 2003; Finkel et al., 2005; Collobert et al., 2011; Lample et al., 2016). Recently, fine-grained namedentity recognition and typing are getting more attention (Ling and Weld, 2012; Corro et al., 2015; Shimaoka"
2020.emnlp-demos.14,N16-1030,0,0.0860106,"N god, body part, arm, etc. 5 Related Work Earliest approaches for entity typing are based on manually designed patterns (e.g., Hearst patterns (Hearst, 1992)) to extract explicit type candidates in given texts. These pattern-based approaches can achieve good precision, but their recall is low, and they are difficult to scale up. Traditional named-entity recognition methods used both rule-based and supervised techniques to recognize and assign entity mentions into few coarse classes like person, location and organization (Sang and De Meulder, 2003; Finkel et al., 2005; Collobert et al., 2011; Lample et al., 2016). Recently, fine-grained namedentity recognition and typing are getting more attention (Ling and Weld, 2012; Corro et al., 2015; Shimaoka et al., 2017; Choi et al., 2018). Ling and Weld (2012) use a classic linear classifier to classify the mentions into a set of 112 types. At much larger scale, FINET (Corro et al., 2015) uses 16k types from the WordNet taxonomy as the targets for entity typing. FINET is a combination of pattern-based, mention-based and verb-based extractors to extract both explicit and implicit type candidates for the mentions from the contexts. With the development of deep l"
2020.emnlp-demos.14,C14-2018,0,0.405631,"Missing"
2020.emnlp-demos.14,D14-1162,0,0.0839963,"hold for the cardinality limit. 102 Input Text Typing Modules people, westerosi, exiles, valyrians, living_beings, crownlanders, qeens Predicted Types Type Limit Aggregate Scores 1.55 1.67 Figure 2: ENTYFI Web interface. 3 Web Interface The ENTYFI system is deployed online at https: //d5demos.mpi-inf.mpg.de/entyfi. A screencast video, which demonstrates ENTYFI, is also uploaded at https://youtu.be/g_ESaONagFQ. (Shimaoka et al., 2017) or 10,331 types (Choi et al., 2018). Note: If the later model is selected to run the real-world typing, it requires more time to load the pre-trained embeddings (Pennington et al., 2014). On the other hand, if supervised fiction typing or KB lookup typing are chosen, the system computes the similarity between the given text and reference universes from the database. With the default option, the type system of the most related universe is being used as targets for typing, while with the alternative case, users can choose different universes and use their type systems as targets. Users are also able to decide whether the consolidation step is executed or not. Input. The web interface allows users to enter a text as input. To give a better experience, we provide various sample t"
2020.emnlp-demos.14,N13-1071,0,0.520153,"Missing"
2020.emnlp-demos.14,W03-0419,0,0.582314,"Missing"
2020.emnlp-demos.14,L16-1056,0,0.151852,"real-world, for instance, House of Cards, a satire of American politics. Even fictional stories like Game of Thrones or Lord of the Rings contain types presented in real world, such as King or Battle. To leverage this overlap, we incorporate the Wikipedia- and news-trained typing model from Choi et al. (2018), which is able to predict up to 10,331 real-world types. Unsupervised Typing. Along with supervised technique, we use a pattern-based method to extract type candidates which appear explicitly in contexts for mentions. We use 36 manually crafted Hearst-style patterns for type extraction (Seitner et al., 2016). Moreover, from dependency parsing, Type Consolidation ILP Model. Given an entity mention e with a list of type candidates with corresponding weights, a decision variable Ti is defined for each type candidate ti . Ti = 1 if e belongs to ti , otherwise, Ti = 0. With the constraints mentioned above, the objective function is: maximize X X α Ti ∗ wi + (1 − α) Ti ∗ Tj ∗ vij i i,j subject to Ti + Tj ≤ 1 ∀(ti , tj ) ∈ D Ti − Tj ≤ 0 ∀(ti , tj ) ∈ H X Ti ≤ δ i where wi is the weight of the type candidate ti , α is a hyper parameter, vij is Pearson correlation coefficient between a type pair (ti , tj"
2020.emnlp-demos.14,E17-1119,0,0.540196,"more complexity to the training. 2.4 Mention Typing We produce type candidates for mentions by using a combination of supervised, unsupervised and lookup approaches. Supervised Fiction Types. Given an entity mention and its textual context, we approach typing as multiclass classification problem. The mention representation is the average of all embeddings of tokens in the mention. The context representation is a combination of left and right context around the mention. The contexts are encoded by using BiLSTM models (Graves, 2012) and then put into attention layer to learn the weight factors (Shimaoka et al., 2017). Mention and context representations are concatenated and passed to the final logistic regression layer with cross entropy loss function to predict the type candidates. a noun phrase can be considered as a type candidate if there exists a noun compound modifier (nn) between the noun phrase and the given mention. In the case of candidate types appearing in the mention itself, we extract the head word of the mention and consider it as a candidate if it appears as a noun in WordNet. For example, given the text Queen Cersei was the twentieth ruler of the Seven Kingdoms, queen and kingdom are type"
2020.emnlp-main.434,P19-1370,0,0.021121,"ted availability of training data. Exceptions use supervised learning, with feature-based classifiers (Kim and Baldwin, 2012) or neural sequence tagging models (Zhang et al., 2016). Our neural approach lies in between, as we learn to identify salient keywords for a specific attribute (e.g., profession), without having training data of relevant keywords. Information Retrieval in NLP. Most existing work leveraging Information Retrieval (IR) components to solve NLP tasks focused on Question Answering (QA) (Kratzwald and Feuerriegel, 2018; Wang et al., 2018; Guu et al., 2020) or dialogue systems (Feng et al., 2019; Luo et al., 2019), where the retrieval part is responsible for ranking the most appropriate answers or responses, given a question or chat session. As far as we know, we are the first to leverage a retrieval-based model for inferring attribute values without training samples. 3 Figure 1: The pipeline of CHARM. The Term Scoring Model assigns scores l0 ..lM to the terms in the input utterances u0 ..uN . The terms with the highest scores are passed to the Retrieval Model, which queries the document collection D. The document scores are aggregated to produce attribute value scores for prediction"
2020.emnlp-main.434,P16-1080,0,0.0496369,"Missing"
2020.emnlp-main.434,W04-3252,0,0.0339268,"dings. We consider a zero-shot BERT baseline (Devlin et al., 2018) that matches utterances with rich document representations. Keyword extraction from conversational text. Notable applications of keyword extraction from conversational text include just-in-time information retrieval (Habibi and Popescu-Belis, 2015), with continuous monitoring of users activities (e.g., participation in meetings) and generating personalized tags for Twitter users (Wu et al., 2010) or search for relevant email attachments (Van Gysel et al., 2017). Prior work mostly pursued unsupervised approaches, e.g. TextRank (Mihalcea and Tarau, 2004) and RAKE (Rose et al., 2010), due to limited availability of training data. Exceptions use supervised learning, with feature-based classifiers (Kim and Baldwin, 2012) or neural sequence tagging models (Zhang et al., 2016). Our neural approach lies in between, as we learn to identify salient keywords for a specific attribute (e.g., profession), without having training data of relevant keywords. Information Retrieval in NLP. Most existing work leveraging Information Retrieval (IR) components to solve NLP tasks focused on Question Answering (QA) (Kratzwald and Feuerriegel, 2018; Wang et al., 201"
2020.emnlp-main.434,P14-1037,0,0.0305066,"e, preferred news topics or medication taken, by providing a list of known attribute values, training examples for a subset of these values and access to external documents (e.g., via a Web search engine). 2 Related Work 1 https://github.com/Anna146/CHARM https://www.mpi-inf.mpg.de/ departments/databases-and-information-systems/ research/pkb 5392 2 which builds image classifiers directly from encyclopedia articles without training images. Most zero-shot studies for NLP (Wang et al., 2019) deal with machine translation, cross-lingual retrieval and entity/relation extraction (Levy et al., 2017; Pasupat and Liang, 2014), which are not suitable for our task, because they identify values that are explicitly mentioned rather than inferring them. Our task is similar to zero-shot text classification (Yazdani and Henderson, 2015; Zhang et al., 2019), where the class labels are represented as single-word embeddings. We consider a zero-shot BERT baseline (Devlin et al., 2018) that matches utterances with rich document representations. Keyword extraction from conversational text. Notable applications of keyword extraction from conversational text include just-in-time information retrieval (Habibi and Popescu-Belis, 2"
2020.emnlp-main.434,P15-1169,0,0.0779098,"Missing"
2020.emnlp-main.434,N10-1101,0,0.0376812,"imilar to zero-shot text classification (Yazdani and Henderson, 2015; Zhang et al., 2019), where the class labels are represented as single-word embeddings. We consider a zero-shot BERT baseline (Devlin et al., 2018) that matches utterances with rich document representations. Keyword extraction from conversational text. Notable applications of keyword extraction from conversational text include just-in-time information retrieval (Habibi and Popescu-Belis, 2015), with continuous monitoring of users activities (e.g., participation in meetings) and generating personalized tags for Twitter users (Wu et al., 2010) or search for relevant email attachments (Van Gysel et al., 2017). Prior work mostly pursued unsupervised approaches, e.g. TextRank (Mihalcea and Tarau, 2004) and RAKE (Rose et al., 2010), due to limited availability of training data. Exceptions use supervised learning, with feature-based classifiers (Kim and Baldwin, 2012) or neural sequence tagging models (Zhang et al., 2016). Our neural approach lies in between, as we learn to identify salient keywords for a specific attribute (e.g., profession), without having training data of relevant keywords. Information Retrieval in NLP. Most existing"
2020.emnlp-main.434,D15-1027,0,0.0293609,". 2 Related Work 1 https://github.com/Anna146/CHARM https://www.mpi-inf.mpg.de/ departments/databases-and-information-systems/ research/pkb 5392 2 which builds image classifiers directly from encyclopedia articles without training images. Most zero-shot studies for NLP (Wang et al., 2019) deal with machine translation, cross-lingual retrieval and entity/relation extraction (Levy et al., 2017; Pasupat and Liang, 2014), which are not suitable for our task, because they identify values that are explicitly mentioned rather than inferring them. Our task is similar to zero-shot text classification (Yazdani and Henderson, 2015; Zhang et al., 2019), where the class labels are represented as single-word embeddings. We consider a zero-shot BERT baseline (Devlin et al., 2018) that matches utterances with rich document representations. Keyword extraction from conversational text. Notable applications of keyword extraction from conversational text include just-in-time information retrieval (Habibi and Popescu-Belis, 2015), with continuous monitoring of users activities (e.g., participation in meetings) and generating personalized tags for Twitter users (Wu et al., 2010) or search for relevant email attachments (Van Gysel"
2020.emnlp-main.434,N19-1108,0,0.0263745,"thub.com/Anna146/CHARM https://www.mpi-inf.mpg.de/ departments/databases-and-information-systems/ research/pkb 5392 2 which builds image classifiers directly from encyclopedia articles without training images. Most zero-shot studies for NLP (Wang et al., 2019) deal with machine translation, cross-lingual retrieval and entity/relation extraction (Levy et al., 2017; Pasupat and Liang, 2014), which are not suitable for our task, because they identify values that are explicitly mentioned rather than inferring them. Our task is similar to zero-shot text classification (Yazdani and Henderson, 2015; Zhang et al., 2019), where the class labels are represented as single-word embeddings. We consider a zero-shot BERT baseline (Devlin et al., 2018) that matches utterances with rich document representations. Keyword extraction from conversational text. Notable applications of keyword extraction from conversational text include just-in-time information retrieval (Habibi and Popescu-Belis, 2015), with continuous monitoring of users activities (e.g., participation in meetings) and generating personalized tags for Twitter users (Wu et al., 2010) or search for relevant email attachments (Van Gysel et al., 2017). Prior"
2020.emnlp-main.434,D16-1080,0,0.0196277,"tional text include just-in-time information retrieval (Habibi and Popescu-Belis, 2015), with continuous monitoring of users activities (e.g., participation in meetings) and generating personalized tags for Twitter users (Wu et al., 2010) or search for relevant email attachments (Van Gysel et al., 2017). Prior work mostly pursued unsupervised approaches, e.g. TextRank (Mihalcea and Tarau, 2004) and RAKE (Rose et al., 2010), due to limited availability of training data. Exceptions use supervised learning, with feature-based classifiers (Kim and Baldwin, 2012) or neural sequence tagging models (Zhang et al., 2016). Our neural approach lies in between, as we learn to identify salient keywords for a specific attribute (e.g., profession), without having training data of relevant keywords. Information Retrieval in NLP. Most existing work leveraging Information Retrieval (IR) components to solve NLP tasks focused on Question Answering (QA) (Kratzwald and Feuerriegel, 2018; Wang et al., 2018; Guu et al., 2020) or dialogue systems (Feng et al., 2019; Luo et al., 2019), where the retrieval part is responsible for ranking the most appropriate answers or responses, given a question or chat session. As far as we"
2020.lrec-1.751,D11-1120,0,0.258033,"tion of the labeled attributes with several state-of-the-art models. 2 Related work User Profiling in Online Communication: The popularity of social media and online forums brings about massive amounts of user-generated content that is freely accessible. This has opened many research opportunities on text analysis, in particular on automatically identifying latent demographic features of online users for personalized downstream applications such as personalized search or recommendation. Such latent demographic attributes include age and gender (Basile et al., 2017; Bayot and Gonc¸alves, 2018; Burger et al., 2011; Fabian et al., 2015; Flekova et al., 2016a; Kim et al., 2017; Rao et al., 2010; Sap et al., 2014; Schwartz et al., 2013a; Vijayaraghavan et al., ˇ 2017), personality (Gjurkovi´c and Snajder, 2018; Schwartz et al., 2013a), regional origin (Fabian et al., 2015; Rao et al., 2010), political orientation and ethnicity (Pennacchiotti and Popescu, 2011; Preot¸iuc-Pietro et al., 2017; Preot¸iucPietro and Ungar, 2018; Rao et al., 2010; Vijayaraghavan et al., 2017), as well as occupational class mapped to income (Flekova et al., 2016b; Preot¸iuc-Pietro et al., 2015). 6118 Most prior works on automatic"
2020.lrec-1.751,C18-1126,1,0.693275,"5; Kim et al., 2017; Sloan et al., 2015; Tigunova et al., 2019). Several works (Basile et al., 2017; Bayot and Gonc¸alves, 2018) made use of labelled datasets published within the shared task on author profiling organized by the CLEF PAN lab (Francisco Manuel et al., 2017; Potthast et al., 2017). There has been less effort on identifying demographic attributes of Reddit users compared with the body of work that exists for Twitter users, although Reddit posts have been exploited for other purposes such as determining ˇ users’ personality (Gjurkovi´c and Snajder, 2018), mental health condition (Cohan et al., 2018), domestic abuse (Schrading et al., 2015) and irony detection (Wallace et al., 2014), among others. Thelwall and Stuart (2019) investigate how the topic of subreddit influences the gender ratio within it. The study was performed on 100 subreddits grouped by interest, gender information about the users were collected by guessing it from their usernames, which is arguably a low-precision strategy. Smaller scale Reddit datasets exist for gender, age and location attributes (Fabian et al., 2015; Finlay, 2014), which are unfortunately not publicly available. As far as we know, we are the first to c"
2020.lrec-1.751,P16-1080,0,0.0289058,"Missing"
2020.lrec-1.751,P16-2051,0,0.0405381,"Missing"
2020.lrec-1.751,W18-1112,0,0.0697785,"Missing"
2020.lrec-1.751,P17-2075,0,0.216428,"ls. 2 Related work User Profiling in Online Communication: The popularity of social media and online forums brings about massive amounts of user-generated content that is freely accessible. This has opened many research opportunities on text analysis, in particular on automatically identifying latent demographic features of online users for personalized downstream applications such as personalized search or recommendation. Such latent demographic attributes include age and gender (Basile et al., 2017; Bayot and Gonc¸alves, 2018; Burger et al., 2011; Fabian et al., 2015; Flekova et al., 2016a; Kim et al., 2017; Rao et al., 2010; Sap et al., 2014; Schwartz et al., 2013a; Vijayaraghavan et al., ˇ 2017), personality (Gjurkovi´c and Snajder, 2018; Schwartz et al., 2013a), regional origin (Fabian et al., 2015; Rao et al., 2010), political orientation and ethnicity (Pennacchiotti and Popescu, 2011; Preot¸iuc-Pietro et al., 2017; Preot¸iucPietro and Ungar, 2018; Rao et al., 2010; Vijayaraghavan et al., 2017), as well as occupational class mapped to income (Flekova et al., 2016b; Preot¸iuc-Pietro et al., 2015). 6118 Most prior works on automatically identifying users’ latent attributes from online communic"
2020.lrec-1.751,P15-1169,0,0.0617054,"Missing"
2020.lrec-1.751,C18-1130,0,0.0246531,"Missing"
2020.lrec-1.751,P17-1068,0,0.0299755,"Missing"
2020.lrec-1.751,D14-1121,0,0.0988303,"Online Communication: The popularity of social media and online forums brings about massive amounts of user-generated content that is freely accessible. This has opened many research opportunities on text analysis, in particular on automatically identifying latent demographic features of online users for personalized downstream applications such as personalized search or recommendation. Such latent demographic attributes include age and gender (Basile et al., 2017; Bayot and Gonc¸alves, 2018; Burger et al., 2011; Fabian et al., 2015; Flekova et al., 2016a; Kim et al., 2017; Rao et al., 2010; Sap et al., 2014; Schwartz et al., 2013a; Vijayaraghavan et al., ˇ 2017), personality (Gjurkovi´c and Snajder, 2018; Schwartz et al., 2013a), regional origin (Fabian et al., 2015; Rao et al., 2010), political orientation and ethnicity (Pennacchiotti and Popescu, 2011; Preot¸iuc-Pietro et al., 2017; Preot¸iucPietro and Ungar, 2018; Rao et al., 2010; Vijayaraghavan et al., 2017), as well as occupational class mapped to income (Flekova et al., 2016b; Preot¸iuc-Pietro et al., 2015). 6118 Most prior works on automatically identifying users’ latent attributes from online communication rely on classification over ha"
2020.lrec-1.751,D15-1309,0,0.0191546,"15; Tigunova et al., 2019). Several works (Basile et al., 2017; Bayot and Gonc¸alves, 2018) made use of labelled datasets published within the shared task on author profiling organized by the CLEF PAN lab (Francisco Manuel et al., 2017; Potthast et al., 2017). There has been less effort on identifying demographic attributes of Reddit users compared with the body of work that exists for Twitter users, although Reddit posts have been exploited for other purposes such as determining ˇ users’ personality (Gjurkovi´c and Snajder, 2018), mental health condition (Cohan et al., 2018), domestic abuse (Schrading et al., 2015) and irony detection (Wallace et al., 2014), among others. Thelwall and Stuart (2019) investigate how the topic of subreddit influences the gender ratio within it. The study was performed on 100 subreddits grouped by interest, gender information about the users were collected by guessing it from their usernames, which is arguably a low-precision strategy. Smaller scale Reddit datasets exist for gender, age and location attributes (Fabian et al., 2015; Finlay, 2014), which are unfortunately not publicly available. As far as we know, we are the first to consider hobby as a personal attribute of"
2020.lrec-1.751,P17-2076,0,0.0261746,"personalized search or recommendation. Such latent demographic attributes include age and gender (Basile et al., 2017; Bayot and Gonc¸alves, 2018; Burger et al., 2011; Fabian et al., 2015; Flekova et al., 2016a; Kim et al., 2017; Rao et al., 2010; Sap et al., 2014; Schwartz et al., 2013a; Vijayaraghavan et al., ˇ 2017), personality (Gjurkovi´c and Snajder, 2018; Schwartz et al., 2013a), regional origin (Fabian et al., 2015; Rao et al., 2010), political orientation and ethnicity (Pennacchiotti and Popescu, 2011; Preot¸iuc-Pietro et al., 2017; Preot¸iucPietro and Ungar, 2018; Rao et al., 2010; Vijayaraghavan et al., 2017), as well as occupational class mapped to income (Flekova et al., 2016b; Preot¸iuc-Pietro et al., 2015). 6118 Most prior works on automatically identifying users’ latent attributes from online communication rely on classification over hand-crafted features such as word/character n-grams (Basile et al., 2017; Burger et al., 2011; Rao et al., 2010), Linguistic Inquiry and Word Count (LIWC) (Penˇ nebaker et al., 2001) categories (Gjurkovi´c and Snajder, 2018; Preot¸iuc-Pietro et al., 2017; Preot¸iuc-Pietro and Ungar, 2018), topic distributions (Flekova et al., 2016a; Pennacchiotti and Popescu, 20"
2020.lrec-1.751,P14-2084,0,0.0289274,"sile et al., 2017; Bayot and Gonc¸alves, 2018) made use of labelled datasets published within the shared task on author profiling organized by the CLEF PAN lab (Francisco Manuel et al., 2017; Potthast et al., 2017). There has been less effort on identifying demographic attributes of Reddit users compared with the body of work that exists for Twitter users, although Reddit posts have been exploited for other purposes such as determining ˇ users’ personality (Gjurkovi´c and Snajder, 2018), mental health condition (Cohan et al., 2018), domestic abuse (Schrading et al., 2015) and irony detection (Wallace et al., 2014), among others. Thelwall and Stuart (2019) investigate how the topic of subreddit influences the gender ratio within it. The study was performed on 100 subreddits grouped by interest, gender information about the users were collected by guessing it from their usernames, which is arguably a low-precision strategy. Smaller scale Reddit datasets exist for gender, age and location attributes (Fabian et al., 2015; Finlay, 2014), which are unfortunately not publicly available. As far as we know, we are the first to consider hobby as a personal attribute of interest to be identified from online commu"
2021.acl-demo.5,Q17-1017,0,0.0118756,"ile others use commonsense knowledge for (re-)training language models (Hwang et al., 2021; Ilievski et al., 2021; Ma et al., 2021; Mitra et al., 2020), to the best of our knowledge, our demo system is the first to visualize the effect of priming vanilla language models, i.e., without task-specific retraining. Related work CSKB construction. Cyc (Lenat, 1995) is the first attempt to build a large-scale commonsense knowledge base. Since then, there have been a number of other CSKB construction projects, notably ConceptNet (Speer and Havasi, 2012), WebChild (Tandon et al., 2014, 2017), TupleKB (Mishra et al., 2017), and more recently Quasimodo (Romero et al., 2019), Dice (Chalier et al., 2020), Atomic (Sap et al., 2019), and CSKG (Ilievski et al., 2020). The early approach to building a CSKB is based on human annotation (e.g., Cyc with expert annotation and ConceptNet with crowdsourcing annotation). Later projects tend to use automated methods based on open information extraction to collect CSK from texts (e.g., WebChild, TupleKB and Quasimodo). Lately, CSKG is an attempt to combine various commonsense knowledge resources into a single KB. The common thread of these CSKB is that they are all based on SP"
2021.acl-demo.5,2021.ccl-1.108,0,0.0212985,"Missing"
2021.acl-demo.5,P17-4020,1,0.87486,"Missing"
2021.acl-demo.5,Q17-1027,0,0.0593289,"Missing"
2021.eacl-main.85,D19-1210,0,0.0282698,"ultimodal embeddings (Frome et al., 2013; Vendrov et al., 2016; Faghri et al., 2018; Wu et al., 2019; Wang et al., 2019; Liu et al., 2019). Visual-Semantic-Embeddings (VSE) has been used for generating captions for whole images (Faghri et al., 2018), or to associate text with image regions (Karpathy and Li, 2015). Color, geometry, aspect-ratio have been used to align image regions to nouns (“chair”), attributes (“big”), and pronouns (“it”) in corresponding text (Kong et al., 2014). Recent work train on document-level co-occurrences and predict links between images and sentences in a document (Hessel et al., 2019; Chu and Kao, 2017). However, alignment of small image regions to text snippets or linking images to single sentences play little role in jointly interpreting the correlation between images and a larger body of text. We focus on the latter in this work. 990 Image Image Ground Truth Paragraph Ground Truth Paragraph . . . Table Mountain Cableway. The revolving car provides 360 degree views as you ascend this mesmerising 60-million-year-old mountain. From the upper cableway station. . . . . . On the east flank of the hill is the old Muslim quarter of the Bo-Kaap; have your camera ready to captur"
2021.eacl-main.85,W16-1311,1,0.753238,"ch API3 to incorporate such tags. This API allows to search by image, and suggests tags based on visually similar images in the vast web image repository. These tags depict popular places, such as “Savarmati Ashram”, or “Mexico City insect market”, and thus constitute “Natural names objects”, “Man-made named objects”, as well as “Geographic locations” from Table 1. To further improve the semantic characterization of an image, we extend the tag set of an image by related commonsense knowledge concepts. Commonsense Knowledge (CSK). CSK can bridge the gap between visual and textual concepts (Nag Chowdhury et al., 2016). CV, BD, and MAN tags are enriched with CSK from the following ConceptNet relations – used for, has property, causes, at location, located near, conceptually related to. E.g., for the left image in Figure 3, we add CSK concept “show talent” from CV tag “stage” from the assertion hstage, used for, show talenti. 3 www.google.com/searchbyimage CSK concepts cover multiple classes from Table 1. Owing to the noise and subjectivity in ConceptNet, only concepts which are informative for a given image are retained. If the top-10 web search results of a CSK concept are semantically similar to the image"
2021.emnlp-main.380,K19-1010,0,0.0243177,"rary On the other hand, screenplays or scripts of thetexts. Moreover, predicting relationships is often atre plays, movies or TV series are more similar modeled as a binary task of sentiment classifica- to real-life conversations. Nalisnick and Baird tion (i.e., person A is positive or negative about (2013) explored Shakespeare plays to analyze the person B). Prior works on conversational data are polarity and intensity of emotions of characters restricted to small-scale data (Yu et al., 2020), or towards each other. The same data is used in merely handle coarse labels of relationship aspects Azab et al. (2019), where fine-grained relationship (Rashid and Blanco, 2018; Qamar et al., 2021). classes adopted from Massey et al. (2015) are preMost approaches use general models for text classi- dicted by applying a logistic regression classifier on fication (Chen et al., 2020; Jia et al., 2021), which a pair of learned character embeddings. However, disregard the particularities of conversational set- such approach predicts relationships solely based tings. on characters’ latent attributes without considering any conversational context. We present Approach and Contributions. Rashid and Blanco (2018) inves"
2021.emnlp-main.380,2020.lrec-1.76,0,0.504873,"rson A is positive or negative about (2013) explored Shakespeare plays to analyze the person B). Prior works on conversational data are polarity and intensity of emotions of characters restricted to small-scale data (Yu et al., 2020), or towards each other. The same data is used in merely handle coarse labels of relationship aspects Azab et al. (2019), where fine-grained relationship (Rashid and Blanco, 2018; Qamar et al., 2021). classes adopted from Massey et al. (2015) are preMost approaches use general models for text classi- dicted by applying a logistic regression classifier on fication (Chen et al., 2020; Jia et al., 2021), which a pair of learned character embeddings. However, disregard the particularities of conversational set- such approach predicts relationships solely based tings. on characters’ latent attributes without considering any conversational context. We present Approach and Contributions. Rashid and Blanco (2018) investigated the prePRIDE, a neural multi-label classifier for diction of interpersonal dimensions (Wish et al., Predicting Relationships In DialoguE. PRIDE 1976) of utterances in the Friends series, where makes inference among 12 fine-grained directed SVM classifiers"
2021.emnlp-main.380,N19-1067,0,0.0264154,"and personal interests (e.g., playing guitar, Justin Bieber) inferable from User B’s social media (exemplified in Figure 1), a system will be able to provide user A with relevant personalized recommendations for a query “birthday present ideas for my daughter”. 2 Related work Relationship Prediction. There is only limited research on relationship prediction in dialogues, as most studies focus on literary texts. The relationships in novels are often predicted on the coarse granularity (positive or negative sentiment) (Chaturvedi et al., 2016), modelled as emotionrelated classes (anger, fear) (Kim and Klinger, 2019), or described in a topic-modelling manner Prior Work and its Limitations. There has been (Iyyer et al., 2016; Chaturvedi et al., 2017). While considerable research on extracting relationships fictional texts often contain dialogues, they are inbetween characters in literary texts such as novels terleaved with narratives, where the language is less (Chaturvedi et al., 2016, 2017). These methods colloquial and more descriptive, which aids explicit are inappropriate for conversational data, though, extraction of fictional characters’ relationships. which is colloquial and less structured than li"
2021.emnlp-main.380,P13-2085,0,0.0753317,"Missing"
2021.emnlp-main.380,D18-1470,0,0.332566,"texts. Moreover, predicting relationships is often atre plays, movies or TV series are more similar modeled as a binary task of sentiment classifica- to real-life conversations. Nalisnick and Baird tion (i.e., person A is positive or negative about (2013) explored Shakespeare plays to analyze the person B). Prior works on conversational data are polarity and intensity of emotions of characters restricted to small-scale data (Yu et al., 2020), or towards each other. The same data is used in merely handle coarse labels of relationship aspects Azab et al. (2019), where fine-grained relationship (Rashid and Blanco, 2018; Qamar et al., 2021). classes adopted from Massey et al. (2015) are preMost approaches use general models for text classi- dicted by applying a logistic regression classifier on fication (Chen et al., 2020; Jia et al., 2021), which a pair of learned character embeddings. However, disregard the particularities of conversational set- such approach predicts relationships solely based tings. on characters’ latent attributes without considering any conversational context. We present Approach and Contributions. Rashid and Blanco (2018) investigated the prePRIDE, a neural multi-label classifier for"
2021.lantern-1.3,N18-2121,0,0.025467,"ommon word (ignoring stop words) between caption and comments. These variants are suffixed overlap – e.g. +NE-overlap. We report experimental results on all of these variants, adopting a 30,000/8,000/8,000 train/val/test split for each of them. Related Work Image Captioning. Prior work on captioning conditioned only on images (Farhadi et al., 2010; Vinyals et al., 2015; Karpathy and Li, 2015; Krause et al., 2017) has been successful for descriptive captions with explicit grounding to image objects. Recently, captions with sentimental and abstract concepts have been explored (Gan et al., 2017; Chandrasekaran et al., 2018; Park et al., 2017; Liu et al., 2018; Shuster et al., 2019). Although external knowledge bases like DBpedia (factual knowledge) (Wu et al., 2018) and ConceptNet (commonsense knowledge) (Zhou et al., 2019) have been leveraged, all prior work ignores the knowledge present in the text surrounding images in social media and other domains. Contextual Image Captioning leverages the latter kind of knowledge. Multimodal Summarization. Research on multimodal embeddings (Laina et al., 2019; Xia et al., 2020; Scialom et al., 2020) has facilitated studying image–text data. Summarization of multimodal doc"
C10-1103,D09-1017,0,0.358653,"the book is hard to follow and the chapter titles are not very helpful, so going back and trying to find information is quite 913 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 913–921, Beijing, August 2010 difficult. We note that there are many subjective words (hard, helpful, difficult) modified by opinion modifiers such as (very, quite) and negation words like (not). For rating prediction, considering opinion modifiers is crucial; very helpful is a much stronger sentiment than helpful. Negation words also need attention. As pointed out by Liu and Seneff (2009) we cannot simply reverse the polarity. For example, if we assign a higher positive score to very helpful than to helpful, simply reversing the sign of the scores would incorrectly suggest that not helpful is less negative than not very helpful. The widely used unigram (bag-of-words) model (Pang and Lee, 2005; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006; Ganu et al., 2009) cannot properly capture phrase patterns. Consider the following example: not so helpful vs. not so bad. In a unigram-based regression model each unigram gets a weight indicating its polarity and strength. High positive"
C10-1103,esuli-sebastiani-2006-sentiwordnet,0,0.0157308,". (2009) restrict the n-grams to the ones having certain POS patterns. However, the long n-grams matching the patterns still suffer from sparsity. The same seems to hold for sparse n-gram models (BCR in this paper) in the spirit of Ifrim et al. (2008). Although sparse n-gram models can explore arbitrarily large n-gram feature spaces, they can be of little help if the n-grams of interests occur sparsely in the datasets. Since our approach can be regarded as learning a domain-independent sentiment lexicon, it is related to the area of automatically building domainindependent sentiment lexicons (Esuli and Sebastiani, 2006; Godbole et al., 2007; Kim and Hovy, 2004). However, this prior work focused mainly on the opinion polarity of opinion words, neglecting the opinion strength. Recently, the lexicon based approaches were extended to learn domaindependent lexicons (Kanayama and Nasukawa, 2006; Qiu et al., 2009), but these approaches also neglect the aspect of opinion strength. Our method requires only the prior polarity of opinion roots and can thus be used on top of those methods for learning the scores of domain-dependent opinion components. The methods proposed in (Hu and Liu, 2004; Popescu and Etzioni, 2005"
C10-1103,P05-1015,0,0.955868,"t techniques for review rating prediction. Gerhard Weikum Max-Planck Institute for Informatics weikum@mpii.mpg.de Introduction Motivation Opinion mining and sentiment analysis has become a hot research area (Pang and Lee, 2008). There is ample work on analyzing the sentiments of online-review communities where users comment on products (movies, books, consumer electronics, etc.), implicitly expressing their opinion polarities (positive, negative, neutral), and also provide numeric ratings of products (Titov and McDonald, 2008b; Lerman et al., 2009; Hu and Liu, 2004; Titov and McDonald, 2008a; Pang and Lee, 2005; Popescu and Etzioni, 2005a). Although ratings are more informative than polarities, most prior work focused on classifying text fragments (phrases, sentences, entire reviews) by polarity. However, a product receiving mostly 5star reviews exhibits better customer purchase behavior compared to a product with mostly 4-star reviews. In this paper we address the learning and prediction of numerical ratings from review texts, and we model this as a metric regression problem over an appropriately defined feature space. Formally, the input is a set of rated documents (i.e., reviews), {xi , yi }N i=1"
C10-1103,W06-3808,0,0.0559374,"Missing"
C10-1103,W06-1642,0,0.0119082,"m models can explore arbitrarily large n-gram feature spaces, they can be of little help if the n-grams of interests occur sparsely in the datasets. Since our approach can be regarded as learning a domain-independent sentiment lexicon, it is related to the area of automatically building domainindependent sentiment lexicons (Esuli and Sebastiani, 2006; Godbole et al., 2007; Kim and Hovy, 2004). However, this prior work focused mainly on the opinion polarity of opinion words, neglecting the opinion strength. Recently, the lexicon based approaches were extended to learn domaindependent lexicons (Kanayama and Nasukawa, 2006; Qiu et al., 2009), but these approaches also neglect the aspect of opinion strength. Our method requires only the prior polarity of opinion roots and can thus be used on top of those methods for learning the scores of domain-dependent opinion components. The methods proposed in (Hu and Liu, 2004; Popescu and Etzioni, 2005b) can also be categorized into the lexicon based framework because their procedure starts with a set of seed words whose polarities are propagated to other opinion bearing words. 6 Conclusion and Future Work In this paper we show that the bag-of-opinions (BoO) representatio"
C10-1103,C04-1200,0,0.0456779,"ertain POS patterns. However, the long n-grams matching the patterns still suffer from sparsity. The same seems to hold for sparse n-gram models (BCR in this paper) in the spirit of Ifrim et al. (2008). Although sparse n-gram models can explore arbitrarily large n-gram feature spaces, they can be of little help if the n-grams of interests occur sparsely in the datasets. Since our approach can be regarded as learning a domain-independent sentiment lexicon, it is related to the area of automatically building domainindependent sentiment lexicons (Esuli and Sebastiani, 2006; Godbole et al., 2007; Kim and Hovy, 2004). However, this prior work focused mainly on the opinion polarity of opinion words, neglecting the opinion strength. Recently, the lexicon based approaches were extended to learn domaindependent lexicons (Kanayama and Nasukawa, 2006; Qiu et al., 2009), but these approaches also neglect the aspect of opinion strength. Our method requires only the prior polarity of opinion roots and can thus be used on top of those methods for learning the scores of domain-dependent opinion components. The methods proposed in (Hu and Liu, 2004; Popescu and Etzioni, 2005b) can also be categorized into the lexicon"
C10-1103,E09-1059,0,0.0183878,"that the bag-of-opinions method outperforms prior state-of-the-art techniques for review rating prediction. Gerhard Weikum Max-Planck Institute for Informatics weikum@mpii.mpg.de Introduction Motivation Opinion mining and sentiment analysis has become a hot research area (Pang and Lee, 2008). There is ample work on analyzing the sentiments of online-review communities where users comment on products (movies, books, consumer electronics, etc.), implicitly expressing their opinion polarities (positive, negative, neutral), and also provide numeric ratings of products (Titov and McDonald, 2008b; Lerman et al., 2009; Hu and Liu, 2004; Titov and McDonald, 2008a; Pang and Lee, 2005; Popescu and Etzioni, 2005a). Although ratings are more informative than polarities, most prior work focused on classifying text fragments (phrases, sentences, entire reviews) by polarity. However, a product receiving mostly 5star reviews exhibits better customer purchase behavior compared to a product with mostly 4-star reviews. In this paper we address the learning and prediction of numerical ratings from review texts, and we model this as a metric regression problem over an appropriately defined feature space. Formally, the i"
C10-1103,H05-1043,0,0.604543,"iew rating prediction. Gerhard Weikum Max-Planck Institute for Informatics weikum@mpii.mpg.de Introduction Motivation Opinion mining and sentiment analysis has become a hot research area (Pang and Lee, 2008). There is ample work on analyzing the sentiments of online-review communities where users comment on products (movies, books, consumer electronics, etc.), implicitly expressing their opinion polarities (positive, negative, neutral), and also provide numeric ratings of products (Titov and McDonald, 2008b; Lerman et al., 2009; Hu and Liu, 2004; Titov and McDonald, 2008a; Pang and Lee, 2005; Popescu and Etzioni, 2005a). Although ratings are more informative than polarities, most prior work focused on classifying text fragments (phrases, sentences, entire reviews) by polarity. However, a product receiving mostly 5star reviews exhibits better customer purchase behavior compared to a product with mostly 4-star reviews. In this paper we address the learning and prediction of numerical ratings from review texts, and we model this as a metric regression problem over an appropriately defined feature space. Formally, the input is a set of rated documents (i.e., reviews), {xi , yi }N i=1 , where xi is a sequence o"
C10-1103,N07-1038,0,0.216892,"fficult) modified by opinion modifiers such as (very, quite) and negation words like (not). For rating prediction, considering opinion modifiers is crucial; very helpful is a much stronger sentiment than helpful. Negation words also need attention. As pointed out by Liu and Seneff (2009) we cannot simply reverse the polarity. For example, if we assign a higher positive score to very helpful than to helpful, simply reversing the sign of the scores would incorrectly suggest that not helpful is less negative than not very helpful. The widely used unigram (bag-of-words) model (Pang and Lee, 2005; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006; Ganu et al., 2009) cannot properly capture phrase patterns. Consider the following example: not so helpful vs. not so bad. In a unigram-based regression model each unigram gets a weight indicating its polarity and strength. High positive/negative weights are strongly positive/negative clues. It is reasonable to assign a positive weight to helpful and a negative weight to bad. The fundamental problem of unigrams arises when assigning a weight to not. If not had a strongly negative weight, the positive weight of helpful would be strongly reduced while the negative weigh"
C10-1103,P08-1036,0,0.0940831,"on scores. Experiments show that the bag-of-opinions method outperforms prior state-of-the-art techniques for review rating prediction. Gerhard Weikum Max-Planck Institute for Informatics weikum@mpii.mpg.de Introduction Motivation Opinion mining and sentiment analysis has become a hot research area (Pang and Lee, 2008). There is ample work on analyzing the sentiments of online-review communities where users comment on products (movies, books, consumer electronics, etc.), implicitly expressing their opinion polarities (positive, negative, neutral), and also provide numeric ratings of products (Titov and McDonald, 2008b; Lerman et al., 2009; Hu and Liu, 2004; Titov and McDonald, 2008a; Pang and Lee, 2005; Popescu and Etzioni, 2005a). Although ratings are more informative than polarities, most prior work focused on classifying text fragments (phrases, sentences, entire reviews) by polarity. However, a product receiving mostly 5star reviews exhibits better customer purchase behavior compared to a product with mostly 4-star reviews. In this paper we address the learning and prediction of numerical ratings from review texts, and we model this as a metric regression problem over an appropriately defined feature"
C10-1103,H05-1044,0,0.126859,"using this feature representation, the learning problem is equivalent to: min L(β) = β βz ≤ 0 z ∈ SZ βr ≥ 0 r ∈ SR (3) where β ∈ Rd , β = [β z , β m , β r ]. β0 is the intercept of the regression function, which is estimated as the mean of the ratings in the training set. We define a new variable y˜i = yi − β0 . In order to avoid overfitting, we add an l2 norm regularizer to the loss function with the parameter λ &gt; 0. LR(β) = N λ 1 X (hβ, vi i − y˜i )2 + k β k22 2N 2 i=1 We assume that we can identify the opinion roots and negation words from a subjectivity lexicon. In this work we use MPQA (Wilson et al., 2005). In addition, the lexicon provides the prior polarity of the opinion roots. In the training phase, we are given a set of documents with ratings {xi , yi }N i=1 , and our goal is to find an optimal function f ∗ whose predictions {ˆ yi }N i=1 are as close as possibile to the original ratings {yi }N i=1 . Formally, we aim to minimize the following loss function: N 1 X (f (xi ) − yi )2 2N i=1 s.t. Learning Regression Parameters L= N 1 X (hβ, vi i + β0 − yi )2 2N (2) s.t. βz ≤ 0 z ∈ SZ βr ≥ 0 r ∈ SR (4) We solve the above optimization problem by Algorithm 1 using coordinate descent. The procedure"
C10-1103,H05-2017,0,\N,Missing
C12-2133,E06-1002,0,0.0191052,"Missing"
C12-2133,D07-1074,0,0.0185325,"classification. HYENA exploits gazetteer features and accounts for the joint evidence for types at different levels. Experiments and an extrinsic study on NED demonstrate the practical viability of HYENA. KEYWORDS: Fine-grained entity types, multi-labeling, hierarchical classification, metaclassification. Proceedings of COLING 2012: Posters, pages 1361–1370, COLING 2012, Mumbai, December 2012. 1361 1 Introduction Motivation: Web contents such as news, blogs, etc. are full of named entities. Recognizing them and disambiguating them has been intensively studied (see, e.g., (Finkel et al., 2005; Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011)). Each entity belongs to one or more lexical types associated with it. For instance, an entity such as Bob Dylan should be assigned labels of type Singer, Musician, Poet, etc., and also the corresponding supertype(s) (hypernyms) in a type hierarchy, in this case Person. Such fine-grained typing of entities can be a great asset for various NLP tasks, e.g. semantic role labeling. Most notably, named entity disambiguation (NED) can be boosted by knowing or inferring a mention’s lexical types. For example, noun phrases such as “"
C12-2133,W10-2415,0,0.0519846,"“Google founder Page”, or “rock legend Page” can be easily mapped to the entities Bob Dylan, Larry Page, and Jimmy Page if their respective types Singer, BusinessPerson, and Guitarist are available. Problem Statement: State-of-the-art tools for named entity recognition like the Stanford NER Tagger (Finkel et al., 2005) compute such lexical tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). There is little literature on fine-grained typing of entity mentions (Fleischman and Hovy, 2002; Ekbal et al., 2010; Rahman and Ng, 2010; Ling and Weld, 2012), and these approaches are pretty much limited to flat sets of several dozens of types. Because of the relatively small number of types, an entity or mention is typically mapped to one type only. The goal that we address in this paper is to extend such methods by automatically computing lexical types for entity mentions, using a large set of types from a hierarchical taxonomy with multiple levels. In this setting, many entities naturally belong to multiple types. So we face a hierarchical multi-label classification problem (Tsoumakas et al., 2012). Co"
C12-2133,D11-1142,0,0.040813,"VD-based latent topic model with a semantic kernel that captures word proximities. The method was applied to a set of 21 different types; each mention is assigned to exactly one type. The work of (Ling and Weld, 2012) considered a two-level taxonomy with 112 tags taken from the Freebase knowledge base, forming a two-level hierarchy with top-level topics and 112 types (with entity instances). (Ling and Weld, 2012) trained a CRF for the joint task of recognizing entity mentions and inferring type tags. The feature set included the ones used in earlier work (see above) plus patterns from ReVerb (Fader et al., 2011). 7 Conclusions We presented HYENA for fine-grained type classification of entity mentions. In contrast to prior methods, we can deal with hundreds of types in a multi-level hierarchy, and consider that a mention can have many different types. In experiments, HYENA outperformed state-of-the-art competitors even on their original datasets and improved efficiency of NED by reducing the search space. Acknowledgements This work is supported by the 7 th Framework IST programme of the European Union through the focused research project (STREP) on Longitudinal Analytics of Web Archive data (LAWA) – c"
C12-2133,P05-1045,0,0.179125,"i-label hierarchical classification. HYENA exploits gazetteer features and accounts for the joint evidence for types at different levels. Experiments and an extrinsic study on NED demonstrate the practical viability of HYENA. KEYWORDS: Fine-grained entity types, multi-labeling, hierarchical classification, metaclassification. Proceedings of COLING 2012: Posters, pages 1361–1370, COLING 2012, Mumbai, December 2012. 1361 1 Introduction Motivation: Web contents such as news, blogs, etc. are full of named entities. Recognizing them and disambiguating them has been intensively studied (see, e.g., (Finkel et al., 2005; Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011)). Each entity belongs to one or more lexical types associated with it. For instance, an entity such as Bob Dylan should be assigned labels of type Singer, Musician, Poet, etc., and also the corresponding supertype(s) (hypernyms) in a type hierarchy, in this case Person. Such fine-grained typing of entities can be a great asset for various NLP tasks, e.g. semantic role labeling. Most notably, named entity disambiguation (NED) can be boosted by knowing or inferring a mention’s lexical types. For example, noun p"
C12-2133,C02-1130,0,0.041128,"uch as “songwriter Dylan”, “Google founder Page”, or “rock legend Page” can be easily mapped to the entities Bob Dylan, Larry Page, and Jimmy Page if their respective types Singer, BusinessPerson, and Guitarist are available. Problem Statement: State-of-the-art tools for named entity recognition like the Stanford NER Tagger (Finkel et al., 2005) compute such lexical tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). There is little literature on fine-grained typing of entity mentions (Fleischman and Hovy, 2002; Ekbal et al., 2010; Rahman and Ng, 2010; Ling and Weld, 2012), and these approaches are pretty much limited to flat sets of several dozens of types. Because of the relatively small number of types, an entity or mention is typically mapped to one type only. The goal that we address in this paper is to extend such methods by automatically computing lexical types for entity mentions, using a large set of types from a hierarchical taxonomy with multiple levels. In this setting, many entities naturally belong to multiple types. So we face a hierarchical multi-label classification problem (Tsoumak"
C12-2133,W09-1125,0,0.0421452,"of the Person class, and developed a decision-tree classifier. (Ekbal et al., 2010) developed a maximum entropy classifier using word-level features from the mention contexts, but experimental results are flagged as non-reproducible in the ACL Anthology. (Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordinal, cardinal, etc. The method uses a rich set of features, including WordNet senses of noun-phrase head words in mention contexts. (Giuliano, 2009) proposed an SVD-based latent topic model with a semantic kernel that captures word proximities. The method was applied to a set of 21 different types; each mention is assigned to exactly one type. The work of (Ling and Weld, 2012) considered a two-level taxonomy with 112 tags taken from the Freebase knowledge base, forming a two-level hierarchy with top-level topics and 112 types (with entity instances). (Ling and Weld, 2012) trained a CRF for the joint task of recognizing entity mentions and inferring type tags. The feature set included the ones used in earlier work (see above) plus patterns"
C12-2133,D11-1072,1,0.89928,"features and accounts for the joint evidence for types at different levels. Experiments and an extrinsic study on NED demonstrate the practical viability of HYENA. KEYWORDS: Fine-grained entity types, multi-labeling, hierarchical classification, metaclassification. Proceedings of COLING 2012: Posters, pages 1361–1370, COLING 2012, Mumbai, December 2012. 1361 1 Introduction Motivation: Web contents such as news, blogs, etc. are full of named entities. Recognizing them and disambiguating them has been intensively studied (see, e.g., (Finkel et al., 2005; Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011)). Each entity belongs to one or more lexical types associated with it. For instance, an entity such as Bob Dylan should be assigned labels of type Singer, Musician, Poet, etc., and also the corresponding supertype(s) (hypernyms) in a type hierarchy, in this case Person. Such fine-grained typing of entities can be a great asset for various NLP tasks, e.g. semantic role labeling. Most notably, named entity disambiguation (NED) can be boosted by knowing or inferring a mention’s lexical types. For example, noun phrases such as “songwriter Dylan”, “Google founder Page”, or “"
C12-2133,C10-1105,0,0.0751332,"”, or “rock legend Page” can be easily mapped to the entities Bob Dylan, Larry Page, and Jimmy Page if their respective types Singer, BusinessPerson, and Guitarist are available. Problem Statement: State-of-the-art tools for named entity recognition like the Stanford NER Tagger (Finkel et al., 2005) compute such lexical tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). There is little literature on fine-grained typing of entity mentions (Fleischman and Hovy, 2002; Ekbal et al., 2010; Rahman and Ng, 2010; Ling and Weld, 2012), and these approaches are pretty much limited to flat sets of several dozens of types. Because of the relatively small number of types, an entity or mention is typically mapped to one type only. The goal that we address in this paper is to extend such methods by automatically computing lexical types for entity mentions, using a large set of types from a hierarchical taxonomy with multiple levels. In this setting, many entities naturally belong to multiple types. So we face a hierarchical multi-label classification problem (Tsoumakas et al., 2012). Contribution: This pape"
C12-2133,P11-1138,0,0.0312624,"for the joint evidence for types at different levels. Experiments and an extrinsic study on NED demonstrate the practical viability of HYENA. KEYWORDS: Fine-grained entity types, multi-labeling, hierarchical classification, metaclassification. Proceedings of COLING 2012: Posters, pages 1361–1370, COLING 2012, Mumbai, December 2012. 1361 1 Introduction Motivation: Web contents such as news, blogs, etc. are full of named entities. Recognizing them and disambiguating them has been intensively studied (see, e.g., (Finkel et al., 2005; Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011)). Each entity belongs to one or more lexical types associated with it. For instance, an entity such as Bob Dylan should be assigned labels of type Singer, Musician, Poet, etc., and also the corresponding supertype(s) (hypernyms) in a type hierarchy, in this case Person. Such fine-grained typing of entities can be a great asset for various NLP tasks, e.g. semantic role labeling. Most notably, named entity disambiguation (NED) can be boosted by knowing or inferring a mention’s lexical types. For example, noun phrases such as “songwriter Dylan”, “Google founder Page”, or “rock legend Page” can b"
C12-2133,A97-1030,0,0.0340419,"ion Prec. 0.0 −0.5 −1.5 −2.5 AIDA 49.2 45.7 28.8 17.7 0 16.1 12.3 4.7 2.2 0 0.659 0.738 0.791 0.802 0.82 0.639 0.713 0.779 0.798 0.823 Table 8: Impact of Varying Type Prediction Confidence Threshold on NED Results 6 Related Work There is little prior work on the task of classifying named entities, given in the form of (still ambiguous) noun phrases, onto fine-grained lexical types. (Fleischman and Hovy, 2002) has been the first work to address type granularities that are finer than the handful of tags used in classical NER work (person, organization, location, date, money, other – see, e.g., (Wacholder et al., 1997; Alfonseca and Manandhar, 2002; Cunningham, 2002; Finkel et al., 2005)). It considered 8 sub-classes of the Person class, and developed a decision-tree classifier. (Ekbal et al., 2010) developed a maximum entropy classifier using word-level features from the mention contexts, but experimental results are flagged as non-reproducible in the ACL Anthology. (Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordinal, cardinal, etc. The method"
C14-1207,P13-1122,0,0.0423761,"cyclic Graph (DAG) with 616,792 hypernymy links. Our empirical assessment, indicates that the alignment links between Patty and WordNet have high accuracy, with Mean Reciprocal Rank (MRR) score 0.7 and Normalized Discounted Cumulative Gain (NDCG) score 0.73. As an additional extrinsic value, HARPY provides fine-grained lexical types for the arguments of verb senses in WordNet. 1 Introduction Motivation: This paper addresses the task of discovering and organizing paraphrases of relations between entities (Lin and Pantel, 2001; Fader et al., 2011; Nakashole et al., 2012; Moro and Navigli, 2012; Alfonseca et al., 2013). This task involves understanding that the phrases “travels to”, “visits” and “on her tour through” (relating a person and a country) are synonymous and that “leader of” and “works with” (relating a person and an organization) are in a hypernymy relation: the former is subsumed by the latter. This kind of lexical knowledge can be harnessed for advanced tasks like question answering (Fader et al., 2013), search over web tables (Gupta et al., 2014), or event mining over news (Alfonseca et al., 2013). Work along these lines has developed large repositories of relational paraphrases, most notably"
C14-1207,W04-2214,0,0.0212648,"y edges, with weights derived from the similarity computation. • The remaining edges connect phrases or verbs with their respective feature vertices. There are 6 kinds of such vertex-feature edges, explained next. 2197 Verb Features: The following features are associated with verb senses. A lemma edge connects a verb sense with one or more surface-verb vertices, as given in WordNet glosses. A domain edge edge connects a verb sense with noun senses that describe the usage domain of the verb (e.g. literature, politics). This information is retrieved from WordNet and the WordNet Domains project (Bentivogli et al., 2004). While the latter does not provide sense-disambiguated information, we need to add a mechanism which maps domain information to its WordNet noun sense counterpart. Therefore, we map domain surface nouns to their most frequent senses. In addition, we harness the WordNet links of type derivationally related form to construct further edges between verb senses and noun-sense features in our CAG. The last type of edges for verb-sense features are sentence frame edges, between verb vertices and feature vertices of type sentence frame. WordNet for each verb sense provides information about its sente"
C14-1207,J06-1003,0,0.0279524,"ude the alignments between FrameNet and WordNet (Ferr´andez et al., 2010), VerbNet and PropBank (Palmer, 2009), Wikionary and WordNet (Meyer and Gurevych, 2012), and across multilingual WordNets and/or Wikipedia editions (e.g., (de Melo and Weikum, 2009; Navigli and Ponzetto, 2012)). For aligning ontologies based on OWL and RDF logics, there is a series of annual benchmark competitions (Grau et al., 2013). Most approaches are based on relatedness measures and context similarities between words or concepts and their neighborhoods in the respective resources (e.g., (Banerjee and Pedersen, 2003; Budanitsky and Hirst, 2006; Gabrilovich and Markovitch, 2007)). Algorithmically, this translates into a nearest-neighbor (most-similar) assignment between entries of different resources. More sophisticated methods use similarities merely to assign weights to relatedness edges in a graph, and then employ random walks on such a graph (e.g., (Pilehvar et al., 2013)). The prevalent method of this kind uses Personalized Page Rank (Haveliwala, 2002)), computing stationary probabilities for reaching nodes in one resource when starting random walks on a given node of the other resources (with randomized restarts). 2196 Computi"
C14-1207,D11-1142,0,0.773915,"s, coined HARPY, contains 20,812 synsets organized into a Directed Acyclic Graph (DAG) with 616,792 hypernymy links. Our empirical assessment, indicates that the alignment links between Patty and WordNet have high accuracy, with Mean Reciprocal Rank (MRR) score 0.7 and Normalized Discounted Cumulative Gain (NDCG) score 0.73. As an additional extrinsic value, HARPY provides fine-grained lexical types for the arguments of verb senses in WordNet. 1 Introduction Motivation: This paper addresses the task of discovering and organizing paraphrases of relations between entities (Lin and Pantel, 2001; Fader et al., 2011; Nakashole et al., 2012; Moro and Navigli, 2012; Alfonseca et al., 2013). This task involves understanding that the phrases “travels to”, “visits” and “on her tour through” (relating a person and a country) are synonymous and that “leader of” and “works with” (relating a person and an organization) are in a hypernymy relation: the former is subsumed by the latter. This kind of lexical knowledge can be harnessed for advanced tasks like question answering (Fader et al., 2013), search over web tables (Gupta et al., 2014), or event mining over news (Alfonseca et al., 2013). Work along these lines"
C14-1207,P13-1158,0,0.047802,"s paper addresses the task of discovering and organizing paraphrases of relations between entities (Lin and Pantel, 2001; Fader et al., 2011; Nakashole et al., 2012; Moro and Navigli, 2012; Alfonseca et al., 2013). This task involves understanding that the phrases “travels to”, “visits” and “on her tour through” (relating a person and a country) are synonymous and that “leader of” and “works with” (relating a person and an organization) are in a hypernymy relation: the former is subsumed by the latter. This kind of lexical knowledge can be harnessed for advanced tasks like question answering (Fader et al., 2013), search over web tables (Gupta et al., 2014), or event mining over news (Alfonseca et al., 2013). Work along these lines has developed large repositories of relational paraphrases, most notably, the collections ReVerb (Fader et al., 2011), Patty (Nakashole et al., 2012), and WiSeNet (Moro and Navigli, 2012). The largest of these, Patty, contains ca. 350,000 synsets of phrases, each annotated with ontological types of their two arguments (e.g., person × country, or politician × political party). However, the subsumption hierarchy of Patty is very sparse. It contains only 8,000 hypernymy links"
C14-1207,ferrandez-etal-2010-aligning,0,0.0542464,"Missing"
C14-1207,D09-1122,0,0.0518009,"Missing"
C14-1207,S10-1006,0,0.0620137,"Missing"
C14-1207,P10-1150,0,0.0178287,"iktionary entries onto WordNet senses). Thus, the huge body of work on word sense disambiguation (WSD) is relevant, too. Methodologically, this research also relies, to a large extent, on relatedness/similarity measures and random walks on appropriately constructed graphs. See (Navigli, 2009) for an extensive survey. There is remotely related work on several other tasks in computational linguistics and text mining. These include semantic relatedness between concepts or words (e.g., (Gabrilovich and Markovitch, 2007; Pilehvar et al., 2013)), type inference for the arguments of a phrase (e.g., (Kozareva and Hovy, 2010; Nakashole et al., 2013)), and entailment among verbs (e.g., (Hashimoto et al., 2009)). The SemEval-2010 task on classification of semantic relations (Hendrickx et al., 2010) addressed the problem of predicting the relation for a given sentence and pair of nominals, but was limited to a small prespecified set of relations. 3 Constructing a Candidate Alignment Graph The general idea of the main algorithm is to align phrase synsets from the Patty taxonomy with verb synsets in WordNet. To this end, we first construct a directed candidate alignment graph (CAG). Section 4 will then discuss the act"
C14-1207,D12-1048,0,0.0778653,"lated Work With the proliferation of knowledge bases, like Freebase (Google Knowledge Graph), DBpedia, YAGO, or ConceptNet, there is a wealth of resources about entities and semantic classes (i.e., unary predicates and their instances). In contrast, the systematic compilation of paraphrases for relations (i.e., binary predicates) has received much less attention. Some of the knowledge-base projects, especially those that center on Open Information Extraction, make intensive use of surface patterns (e.g., verbal phrases) that indicate relations (e.g., (Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012; Speer and Havasi, 2012; Wu et al., 2012)); however, they do not organize these patterns into a WordNet-style taxonomy. Prior work towards such taxonomies go back to the projects DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), and VerbNet (Kipper et al., 2008). However, the resulting resources were mostly restricted to single verbs. ReVerb (Fader et al., 2011) extended these approaches by automatically mining entire phrases from Web contents, but still with focus on verbal structures. Patty (Nakashole et al., 2012) used sequence mining algorithms for gathering a general c"
C14-1207,P04-1036,0,0.0209461,"se where exactly one list item is correct, we use quality measures geared for such rankings: Mean Reciprocal Rank (M RR) and Normalized Discounted Cumulative Gain (N DCG). In addition, we report on the precision for top-k results, for small k (1, 3, or 5). Here, a top-k result is considered good if the correct verb senses appears among the top-k alignments, for a given phrase. Results: The results are shown in Table 4. Our method outperforms all baselines. Among the competitors, MFS shows the best performance. This is not so surprising; MFS is rarely outperformed in word sense disambiguation (McCarthy et al., 2004; Navigli and Lapata, 2010). Our gains over MFS are remarkable. In total, HARPY aligned 20,812 phrases to 4,789 verb senses, and also obtained 616,792 hypernymy links between phrases. The evaluation process led to high inter-judge agreement, with Cohen’s Kappa around 0.678. The number of samples, 261, was large enough for statistical significance: we performed a paired t-test for M RR, N DCG and P recision@1 of the SimRank results against each of the baselines, and obtained p-values below 0.05. MRR NDCG Precision@1 Precision@3 Precision@5 SimRank 0.698 0.733 0.571 0.793 0.874 MFS 0.664 0.705 0"
C14-1207,C12-1108,0,0.0257012,"tures. WiseNet (Moro and Navigli, 2012) harnessed phrases from Wikipedia articles and clustered them into synsets of relational phrases. All of these works are fairly limited in their coverage of subsumptions (hypernymy) between relational phrases. There is ample work on computing alignments among different kinds of lexical thesauri, dictionaries, taxonomies, ontologies, and other forms of linguistic or semantic resources. Prominent cases along these lines include the alignments between FrameNet and WordNet (Ferr´andez et al., 2010), VerbNet and PropBank (Palmer, 2009), Wikionary and WordNet (Meyer and Gurevych, 2012), and across multilingual WordNets and/or Wikipedia editions (e.g., (de Melo and Weikum, 2009; Navigli and Ponzetto, 2012)). For aligning ontologies based on OWL and RDF logics, there is a series of annual benchmark competitions (Grau et al., 2013). Most approaches are based on relatedness measures and context similarities between words or concepts and their neighborhoods in the respective resources (e.g., (Banerjee and Pedersen, 2003; Budanitsky and Hirst, 2006; Gabrilovich and Markovitch, 2007)). Algorithmically, this translates into a nearest-neighbor (most-similar) assignment between entri"
C14-1207,D12-1104,1,0.935325,"tains 20,812 synsets organized into a Directed Acyclic Graph (DAG) with 616,792 hypernymy links. Our empirical assessment, indicates that the alignment links between Patty and WordNet have high accuracy, with Mean Reciprocal Rank (MRR) score 0.7 and Normalized Discounted Cumulative Gain (NDCG) score 0.73. As an additional extrinsic value, HARPY provides fine-grained lexical types for the arguments of verb senses in WordNet. 1 Introduction Motivation: This paper addresses the task of discovering and organizing paraphrases of relations between entities (Lin and Pantel, 2001; Fader et al., 2011; Nakashole et al., 2012; Moro and Navigli, 2012; Alfonseca et al., 2013). This task involves understanding that the phrases “travels to”, “visits” and “on her tour through” (relating a person and a country) are synonymous and that “leader of” and “works with” (relating a person and an organization) are in a hypernymy relation: the former is subsumed by the latter. This kind of lexical knowledge can be harnessed for advanced tasks like question answering (Fader et al., 2013), search over web tables (Gupta et al., 2014), or event mining over news (Alfonseca et al., 2013). Work along these lines has developed large rep"
C14-1207,P13-1146,1,0.845069,"rdNet senses). Thus, the huge body of work on word sense disambiguation (WSD) is relevant, too. Methodologically, this research also relies, to a large extent, on relatedness/similarity measures and random walks on appropriately constructed graphs. See (Navigli, 2009) for an extensive survey. There is remotely related work on several other tasks in computational linguistics and text mining. These include semantic relatedness between concepts or words (e.g., (Gabrilovich and Markovitch, 2007; Pilehvar et al., 2013)), type inference for the arguments of a phrase (e.g., (Kozareva and Hovy, 2010; Nakashole et al., 2013)), and entailment among verbs (e.g., (Hashimoto et al., 2009)). The SemEval-2010 task on classification of semantic relations (Hendrickx et al., 2010) addressed the problem of predicting the relation for a given sentence and pair of nominals, but was limited to a small prespecified set of relations. 3 Constructing a Candidate Alignment Graph The general idea of the main algorithm is to align phrase synsets from the Patty taxonomy with verb synsets in WordNet. To this end, we first construct a directed candidate alignment graph (CAG). Section 4 will then discuss the actual alignment algorithm."
C14-1207,P13-1132,0,0.030208,"e is a series of annual benchmark competitions (Grau et al., 2013). Most approaches are based on relatedness measures and context similarities between words or concepts and their neighborhoods in the respective resources (e.g., (Banerjee and Pedersen, 2003; Budanitsky and Hirst, 2006; Gabrilovich and Markovitch, 2007)). Algorithmically, this translates into a nearest-neighbor (most-similar) assignment between entries of different resources. More sophisticated methods use similarities merely to assign weights to relatedness edges in a graph, and then employ random walks on such a graph (e.g., (Pilehvar et al., 2013)). The prevalent method of this kind uses Personalized Page Rank (Haveliwala, 2002)), computing stationary probabilities for reaching nodes in one resource when starting random walks on a given node of the other resources (with randomized restarts). 2196 Computing alignments between resources can sometimes be viewed as a task of disambiguation words or concepts in one resource by mapping them to the other resource (e.g., mapping Wiktionary entries onto WordNet senses). Thus, the huge body of work on word sense disambiguation (WSD) is relevant, too. Methodologically, this research also relies,"
C14-1207,P10-1154,0,0.0354555,"nsitive to the exact choice of the random-jump parameter. Filtering and Candidate Pruning: The target of our alignment is the WordNet verb hierarchy, but not all relational phrases can be mapped into this target space. Therefore, we restrict ourselves to a subset of relational phrases that contain exactly one verb. This eliminates noun phrases (e.g. “father of”) and phrases that contain multiple verbs (e.g. “succeed and died”, “succeeded in persuading”). Noun phrases should be aligned to the WordNet noun hierarchy and it should be treated as a different task (using e.g. state-of-the-art work (Ponzetto and Navigli, 2010)). Multi-verb phrases often pose semantic difficulties. Note that the verbs in these phrases are always transitive verbs, as Patty is derived from subject-phraseobject structures in large corpora. We also used the cardinalities of the support sentences in Patty for pruning the noisy tail of phrases, by dropping all phrases that have only a single instance. To avoid computing SimRank scores for every pair of vertices, we prune the search space as follows. We consider only pairs of relational phrases and verb senses which contain the same surface verb (with lemmatization). Deriving Hypernymy Lin"
C14-1207,speer-havasi-2012-representing,0,0.0210309,"roliferation of knowledge bases, like Freebase (Google Knowledge Graph), DBpedia, YAGO, or ConceptNet, there is a wealth of resources about entities and semantic classes (i.e., unary predicates and their instances). In contrast, the systematic compilation of paraphrases for relations (i.e., binary predicates) has received much less attention. Some of the knowledge-base projects, especially those that center on Open Information Extraction, make intensive use of surface patterns (e.g., verbal phrases) that indicate relations (e.g., (Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012; Speer and Havasi, 2012; Wu et al., 2012)); however, they do not organize these patterns into a WordNet-style taxonomy. Prior work towards such taxonomies go back to the projects DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), and VerbNet (Kipper et al., 2008). However, the resulting resources were mostly restricted to single verbs. ReVerb (Fader et al., 2011) extended these approaches by automatically mining entire phrases from Web contents, but still with focus on verbal structures. Patty (Nakashole et al., 2012) used sequence mining algorithms for gathering a general class of relational phras"
C14-1207,P98-1013,0,\N,Missing
C14-1207,C98-1013,0,\N,Missing
C14-1207,W04-3205,0,\N,Missing
D11-1072,P10-1097,1,0.193595,"Missing"
D11-1072,E06-1002,0,\N,Missing
D11-1072,W03-0419,0,\N,Missing
D11-1072,D07-1074,0,\N,Missing
D11-1072,P05-1045,0,\N,Missing
D12-1014,D09-1062,0,0.320411,"ining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exp"
D12-1014,C10-2036,0,0.0183428,"weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 B"
D12-1014,W06-3808,0,0.0291344,"o their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 150 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of"
D12-1014,P07-1055,0,0.819669,"ntence-level polarity classification. 150 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within senten"
D12-1014,P04-1035,0,0.0557025,"y. Our kernel takes the relative positions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 150 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coar"
D12-1014,P05-1015,0,0.0708475,"ons of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 150 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when avai"
D12-1014,W02-1011,0,0.0115011,"the relative positions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 150 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training"
D12-1014,C10-1103,1,0.938211,"ratings: Large negative/positive ratings indicate a strong negative/positive orientation. A key challenge in finegrained rating prediction is that fine-grained training data for both polarity and strength is hard to obtain. We thus focus on a weakly supervised setting in which only coarse-level training data (such as document ratings and subjectivity lexicons) and, optionally, a small amount of fine-grained training data (such as sentence polarities) is available. A number of lexicon-based approaches for phraselevel rating prediction has been proposed in the literature (Taboada et al., 2011; Qu et al., 2010). These methods utilize a subjectivity lexicon of words along with information about their semantic orientation; they focus on phrases that contain words from the lexicon. A key advantage of sentence-level methods is that they are able to cover all sentences in a review and that phrase identification is avoided. To the best of our knowledge, the problem of rating prediction at the sentence level has not been addressed in the literature. A naive approach would be to simply average phrase-level ratings. Such an approach performs 1 We assign polarity other to text fragments that are off-topic or"
D12-1014,J11-2001,0,0.810659,"ping them to numerical ratings: Large negative/positive ratings indicate a strong negative/positive orientation. A key challenge in finegrained rating prediction is that fine-grained training data for both polarity and strength is hard to obtain. We thus focus on a weakly supervised setting in which only coarse-level training data (such as document ratings and subjectivity lexicons) and, optionally, a small amount of fine-grained training data (such as sentence polarities) is available. A number of lexicon-based approaches for phraselevel rating prediction has been proposed in the literature (Taboada et al., 2011; Qu et al., 2010). These methods utilize a subjectivity lexicon of words along with information about their semantic orientation; they focus on phrases that contain words from the lexicon. A key advantage of sentence-level methods is that they are able to cover all sentences in a review and that phrase identification is avoided. To the best of our knowledge, the problem of rating prediction at the sentence level has not been addressed in the literature. A naive approach would be to simply average phrase-level ratings. Such an approach performs 1 We assign polarity other to text fragments that"
D12-1014,P11-2100,0,0.193681,"Missing"
D12-1014,H05-1044,0,0.0454425,"ur experiments. 3.3 Bag-of-Opinions Rating Predictor We leverage the bag-of-opinion (BoO) model of Qu et al. (2010) as a base predictor for phrase-level ratings. The BoO model was trained from a large generic corpus without fine-grained annotations. In BoO, an opinion consists of three components: an SO-carrying word (e.g., “good”), a set of intensifiers (e.g., “very”) and a set of negators (e.g., “not”). Each opinion is scored based on these words (represented as a boolean vector b) and the polarity of the SO-carrying word (represented as sgn(r) ∈ {−1, 1}) as indicated by the MPQA lexicon of Wilson et al. (2005). In particular, the score is computed as sgn(r)ω T b, where ω is the learned weight vector. The sign function sgn(r) ensures consistent weight assignment for intensifiers and negators. For example, an intensifier like “very” can obtain a large positive or a large negative weight depending on whether it is used with a positive or negative SO-carrying word, respectively. 3.4 then given by ˆ β) ∝ P (r |X, Y, SO-CAL Rating Predictor The Semantic Orientation Calculator (SO-CAL) of Taboada et al. (2011) also predicts phrase-level ratings via a scoring function similar to the one of BoO. The SO-CAL"
D12-1035,de-marneffe-etal-2006-generating,0,0.018999,"Missing"
D12-1035,D11-1142,0,0.0606477,"e use a detector that works against a phrase-concept dictionary which looks as follows: {‘Rome’,‘eternal city’} → Rome {‘Casablanca’} → Casablanca (film) We experimented with using third-party named entity recognizers but the results were not satisfactory. This dictionary was mostly constructed as part of the knowledge base, independently of the questionto-query translation task in the form of instances of the means relation in Yago2, an example of which is shown in Figure 1 For relation detection, we experimented with various approaches. We mainly rely on a relation detector based on ReVerb (Fader et al., 2011) with additional POS tag patterns, in addition to our own which looks for patterns in dependency parses. 3.2 Phrase Mapping After phrases are detected, each phrase is mapped to a set of semantic items. The mapping of concept phrases also relies on the phrase-concept dictionary. To map relation phrases, we rely on a corpus of textual patterns to relation mappings of the form: {‘play’,‘star in’,‘act’,‘leading role’} {‘married’, ‘spouse’,‘wife’} → → actedIn 3.3 Dependency Parsing & Q-Unit Generation Dependency parsing identifies triples of tokens, or triploids, htrel , targ1 , targ2 i, where trel"
D12-1035,D11-1072,1,0.0761028,"Missing"
D12-1035,D12-1104,1,0.191146,"Missing"
D12-1035,spitkovsky-chang-2012-cross,0,0.00951002,"tten, 2008; Kulkarni et al., 2009; Hoffart et al., 2011a) and WSD (Navigli, 2009). In contrast to this prior work on related problems, our graph construction and 388 constraints are more complex, as we address the joint mapping of arbitrary phrases onto entities, classes, or relations. Moreover, instead of graph algorithms or factor-graph learning, we use an ILP for solving the ambiguity problem. This way, we can accommodate expressive constraints, while being able to disambiguate all phrases in a few seconds. DEANNA uses dictionaries of names and phrases for entities, classes, and relations. Spitkovsky and Chang (2012) recently released a huge dictionary of pairs of phrases and Wikipedia links, derived from Google’s Web index. For relations, Nakashole et al. (2012) released PATTY, a large taxonomy of patterns with semantic types. 7 Conclusions and Future Work We presented a method for translating naturallanguage questions into structured queries. The novelty of this method lies in modeling several mapping stages as a joint ILP problem. We harness type signatures and other information from large-scale knowledge bases. Although our model, in principle, leads to high combinatorial complexity, we observed that"
D12-1104,D09-1122,0,0.00978041,"Missing"
D12-1104,D11-1134,0,0.0821399,"Missing"
D12-1104,D11-1135,0,0.0393357,"Missing"
D12-1104,de-marneffe-etal-2006-generating,0,\N,Missing
D12-1104,D11-1095,0,\N,Missing
D12-1104,W09-2415,0,\N,Missing
D12-1104,P10-1023,0,\N,Missing
D12-1104,P11-1062,0,\N,Missing
D12-1104,P10-1150,0,\N,Missing
D12-1104,P10-1030,0,\N,Missing
D12-1104,P08-1003,0,\N,Missing
D12-1104,P08-1052,0,\N,Missing
D12-1104,D11-1142,0,\N,Missing
D12-1104,I05-7009,0,\N,Missing
D12-1104,nastase-etal-2010-wikinet,0,\N,Missing
D12-1104,W04-3205,0,\N,Missing
D12-1104,P09-1070,0,\N,Missing
D14-1042,E09-1005,0,0.0297606,"ome external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algo"
D14-1042,P13-1120,0,0.119268,"ntic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break hbody part-1i”. In contrast to existing resources, our VOS repository disambiguates both the verb sense and the senses of its arguments. WSD is a classification task where for"
D14-1042,E12-1059,0,0.0124141,"ur work, we also incorporate syntactic and semantic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break hbody part-1i”. In contrast to existing resources, our VOS repository disambiguates both the verb sense and the senses of its"
D14-1042,P98-1013,0,0.748474,"builds on a newly created resource of pairs of senses for verbs and their object arguments. For example, the WordNet verb sense hplay-1i (i.e., the 1st sense of 1 The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by taking the semantic types of direct objects into"
D14-1042,P03-1054,0,0.00395449,"method above with a threshold of 1. This procedure increased the performance in all cases. Evaluation Dataset. We tested Werdy on the SemEval-2007 coarse-grained dataset.6 It consists of five senseannotated documents; the sense annotations refer to a coarse-grained version of WordNet. In addition to sense annotations, the corpus also provides the corresponding KB entries (henceforth termed “gold entries”) as well as a POS tag. We restrict our evaluation to verbs that act as clause heads. In total, 461 such verbs were recognized by ClausIE (Del Corro and Gemulla, 2013) and the Stanford Parser (Klein and Manning, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee a"
D14-1042,S07-1054,0,0.0139905,"missing from our repository, we may prune the correct sense of the verb. If, however, there is an additional, incorrect 4 http://wordnet.princeton.edu/ glosstag.shtml 5 http://web.eecs.umich.edu/˜mihalcea/ downloads.html 379 argument in the repository, the correct verb sense is retained but pruning may be less effective. 7 We used a fixed threshold of 1 and vary the search depth in range 1–20. We used the candidate senses of all nouns and verbs in a sentence as context. It-Makes-Sense (IMS). A state-of-the-art, publicly available supervised system (Zhong and Ng, 2010) and a refined version of Chan et al. (2007), which ranked first in the SemEval-2007 coarse grained task. We modified the code to accept KB entries and their candidate senses. We tested both in WordNet-2.1 and 3.0; for the later we mapped Werdy’s set of candidates to WordNet-2.1. Most Frequent Sense (MFS). Selects the most frequent sense (according to WordNet frequencies) among the set of candidate senses of an entry. If there is a tie, we do not label. Note that this procedure differs slightly from the standard of picking the entry with the smallest sense id. We do not follow this approach since it cannot handle well overlapping entrie"
D14-1042,E14-1008,0,0.0398287,"y of a set of alternative, overlapping entries. Syntactic pruning did not prune the correct sense in most cases. In 16 cases (with gold entries), however, the correct sense was pruned. Five of these senses were pruned due to incorrect dependency parses, which led to incorrect frame identification. In two cases, the sense was not annotated with the recognized frame in WordNet, although it seemed adequate. In the remaining cases, a general frame from WordNet was incorrectly omitted. Improvements to WordNet’s frame annotations may thus make syntactic pruning even 382 8 Related Work Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and frames. Dligach and Palmer (2008) generated a corpus of verb and their arguments (both surface forms), which was used to incorporate a semantic feature to the superv"
D14-1042,C12-1109,0,0.0775202,"s can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems c"
D14-1042,P05-1006,0,0.803938,"overlapping entries. Syntactic pruning did not prune the correct sense in most cases. In 16 cases (with gold entries), however, the correct sense was pruned. Five of these senses were pruned due to incorrect dependency parses, which led to incorrect frame identification. In two cases, the sense was not annotated with the recognized frame in WordNet, although it seemed adequate. In the remaining cases, a general frame from WordNet was incorrectly omitted. Improvements to WordNet’s frame annotations may thus make syntactic pruning even 382 8 Related Work Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and frames. Dligach and Palmer (2008) generated a corpus of verb and their arguments (both surface forms), which was used to incorporate a semantic feature to the supervised system. In our work, w"
D14-1042,P13-4007,0,0.0452856,"is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004),8 which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled. Degree Centrality. Proposed by Navigli and Lapata (2010). The method collects all paths connecting each candidate sense of an entry to the set of candidate senses of the words the entry’s context. The candidate sense with the highest degree in the resulting subgraph is selected. We implemented this algorithm using the Neo4j library.9 Methodology. The disambiguation was performed with respect to coarse-grained sense clusters. The score of a cluster is the sum of the individual scores of"
D14-1042,D12-1104,1,0.743357,"orate syntactic and semantic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break hbody part-1i”. In contrast to existing resources, our VOS repository disambiguates both the verb sense and the senses of its arguments. WSD is a cla"
D14-1042,W11-0805,0,0.0311269,"cal KB entries in text is that entries are not restricted to single words. In addition to named entities (such as people, places, etc.), KB’s contain multi-word expressions. For example, WordNet-3.0 contains entries such as take place (verb), let down (verb), take into account (verb), be born (verb), high school (noun), fiscal year (noun), and Prime Minister (noun). Note that each individual word in a multi-word entry is usually also an entry by itself, and can even be part of several multi-word entries. To ensure correct disambiguation, all potential multi-word entries need to be recognized (Finlayson and Kulkarni, 2011), even when they do not appear as consecutive words in a sentence. Werdy addresses these challenges by exploring the syntactic structure of both the input sen2 conj root We use the notation hWordNet entry-sense numberi. 376 Pattern Clause type Example WN frame example [frame number] SVi SVe A SVc C SVmt O SVdt Oi O SVct OA SVct OC SV SVA SVC SVO SVOO SVOA SVOC AE died. AE remained in Princeton. AE is smart. AE has won the Nobel Prize. RSAS gave AE the Nobel Prize. The doorman showed AE to his office. AE declared the meeting open. Somebody verb [2] Somebody verb PP [22] Somebody verb adjective"
D14-1042,P10-1154,0,0.0781827,"Missing"
D14-1042,S07-1006,0,0.0346347,"disambiguation (WERD) with a focus on verbs and 374 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 374–385, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics the verb entry “play”) selects as direct object the noun sense hsport-1i. We refer to this novel resource as the VO Sense Repository, or VOS repository for short.1 It is constructed from the WordNet gloss-tags corpus, the SemCor dataset, and a small set of manually created VO sense pairs. We evaluated Werdy on the SemEval-2007 coarse-grained WSD task (Navigli et al., 2007), both with and without automatic recognition of entries. We found that our techniques boost state-ofthe-art WSD methods and obtain high-quality results. Werdy significantly increases the precision and recall of the best performing baselines. The rest of the paper is organized as follows. Section 2 gives an overview of Werdy components. Section 3 presents the entry recognition, and Sections 4 and 5 discuss our novel syntactic and semantic pruning techniques. Section 6 presents the Semantic VO Repository and how we constructed it. Section 7 gives the results of our evaluation. Section 8 discuss"
D14-1042,S13-1003,0,0.0136363,"ts arguments. WSD is a classification task where for every word there is a set of possible senses given by some external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since i"
D14-1042,J05-1004,0,0.2583,"eated resource of pairs of senses for verbs and their object arguments. For example, the WordNet verb sense hplay-1i (i.e., the 1st sense of 1 The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by taking the semantic types of direct objects into account. Similarly to"
D14-1042,P10-4014,0,0.527391,"iscuss our novel syntactic and semantic pruning techniques. Section 6 presents the Semantic VO Repository and how we constructed it. Section 7 gives the results of our evaluation. Section 8 discusses related work. disambiguates each entry against its possible senses. State-of-the-art methods for WSD (Navigli, 2009) work fairly well for nouns and noun phrases. However, the disambiguation of verbs and verbal phrases has received much less attention in the literature. WSD methods can be roughly categorized into (i) methods that are based on supervised training over sense-annotated corpora (e.g., Zhong and Ng (2010)), and (ii) methods that harness KB’s to assess the semantic relatedness among word senses for mapping entries to senses (e.g., Ponzetto and Navigli (2010)). For these methods, mapping verbs to senses is a difficult task since verbs tend to have more senses than nouns. In WordNet, including monosemous words, there are on average 1.24 senses per noun and 2.17 per verb. To disambiguate verbs and verbal phrases, Werdy proceeds in multiple steps. First, Werdy obtains the set of candidate senses for each recognized entry from the KB. Second, it reduces the set of candidate entries using novel synta"
D14-1042,C98-1013,0,\N,Missing
D14-1042,J14-1003,0,\N,Missing
D14-1042,W14-0111,0,\N,Missing
D15-1101,P98-1012,0,0.788406,"& Klein, 2010; Ng, 2010; Lee et al., 2013) is essentially a clustering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups. For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different from the mention Hans Albert Einstein. When CR is extended to an entire text corpus, in order to generate equivalence classes of co-referring mentions across documents, the task is known as Cross-document Co-reference Resolution (CCR) (Bagga & Baldwin, 1998; Culotta et al., 2007; Singh et al., 2011; Dutta & Weikum, 2015). Note that CCR is not the same as merely concatenating all documents in the corpus and utilizing existing CR methods. The linguistic diversity across documents and high computational cost for huge numbers of mentions in the corpus would typically make such a CR-based simulation perform poorly. Neither CR nor CCR links mention groups to corresponding KB entities. Thus, they represent both in-KB entities and out-of-KB entities (e.g., long-tail or emerging entities that do not have a Wikipedia article) in the same way. Cross-docume"
D15-1101,N09-1037,0,0.086226,"Missing"
D15-1101,D08-1029,0,0.024486,"e overhead. Cross-Document CR (CCR): Early approaches towards CCR involved the use contextual information from input documents for IR-style similarity measures (e.g., tf×idf score, KL divergence, etc.) over textual features (Bagga & Baldwin, 1998; Gooi & Allan, 2004). Probabilistic graphical models jointly learning the mappings of mentions to equivalent classes (co-referring mentions) using features similar to local CR techniques were studied in (Culotta et al., 2007; Singh et al., 2010; Singh et al., 2011), A clustering approach coupled with statistical learning of parameters was studied in (Baron & Freedman, 2008). However, such methods fail to cope with large corpora, and hence a “light-weight” streaming variant of CCR was introduced by (Rao et al., 2010). Co-occurring mentions context have been harnessed for disambiguating person names for CR in (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008). However, these methods do not use KB and depend on information extraction (IE) methods, witnessing substantial noise due to IE quality variance. A CCR framework combining co-occurring mention context with distant KB features embedded in an active hierarchical clustering pr"
D15-1101,P10-1143,0,0.0523776,"Missing"
D15-1101,N04-1002,0,0.051557,"fferent baseline variations niques by ranking the matching entities for distant labeling. However, such prior methods utilize distance labels of the current mention and considers all matching mentions making the procedure expensive. On the other hand, we extract distant features for the strongly matching (best) candidate only, reducing the performance overhead. Cross-Document CR (CCR): Early approaches towards CCR involved the use contextual information from input documents for IR-style similarity measures (e.g., tf×idf score, KL divergence, etc.) over textual features (Bagga & Baldwin, 1998; Gooi & Allan, 2004). Probabilistic graphical models jointly learning the mappings of mentions to equivalent classes (co-referring mentions) using features similar to local CR techniques were studied in (Culotta et al., 2007; Singh et al., 2010; Singh et al., 2011), A clustering approach coupled with statistical learning of parameters was studied in (Baron & Freedman, 2008). However, such methods fail to cope with large corpora, and hence a “light-weight” streaming variant of CCR was introduced by (Rao et al., 2010). Co-occurring mentions context have been harnessed for disambiguating person names for CR in (Mann"
D15-1101,E06-1002,0,0.111211,"Missing"
D15-1101,D07-1020,0,0.161414,"i-phase sieve, applying a cascade of rules for narrowing down the antecedent candidates for a mention (Raghunathan et al., 2010). Cluster ranking functions have also been proposed (Rahman & Ng, 2011; Zheng et al., 2013) to extend this paradigm for incrementally expanding and merging mention groups with preceding candidate clusters using relatedness features (Ratinov & Roth, 2012) and distant knowledge inclusion (Durrett & Klein, 2013). Person name disambiguation, a specific variation of CR, dealing with only person names, titles, nicknames, and other surface form variations was introduced in (Chen & Martin, 2007). Distant Knowledge Labels: For obtaining semantic features, additional knowledge resources such as Wikipedia, YAGO, and FrameNet have been considered (Rahman & Ng, 2011; Baker, 2012). CR methods with confidence-thresholds were proposed in (Ratinov & Roth, 2012; Lee et al., 2013), and (Zheng et al., 2013) generalized these tech853 ECB Dataset Baseline Ignored Mention Co-occurrence Link Validation (τ ) ignored Removed NEL Classification Distant KB feature dropped C3EL (Complete) CCR result NEL results Within-KB Out-of-KB C I U C I P R B3 72.5 74.4 73.4 80.2 79.0 81.4 80.2 73.2 80.7 76.8 68.9 73"
D15-1101,D13-1184,0,0.205469,"Missing"
D15-1101,N10-1061,0,0.167589,"s. For example, NER on the text Einstein won the Nobel Prize identifies the mentions “Einstein” and “Nobel Prize” and marks them as person and misc type, respectively. Named Entity Linking (NEL)1 involves the disambiguation of textual mentions, based on context and semantic information, and their mapping to proper entities in a KB (Bunescu & Pas¸ca, 2006; Cucerzan, 2007; Milne & Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cornolti et al., 2013). For example, in the above text, the mention “Einstein” is linked to the physicist Albert Einstein. Entity Co-reference Resolution (CR) (Haghighi & Klein, 2010; Ng, 2010; Lee et al., 2013) is essentially a clustering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups. For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different from the mention Hans Albert Einstein. When CR is extended to an entire text corpus, in order to generate equivalence classes of co-referring mentions across documents, the task is known as Cross-document Co-reference Resolution (CCR) (Bagga & Baldwi"
D15-1101,D13-1029,0,0.0360106,"Missing"
D15-1101,D07-1074,0,0.267623,"formatics Saarbr¨ucken, Germany weikum@mpi-inf.mpg.de Abstract & Sekine, 2007; Ratinov & Roth, 2009). This involves segmentation of token sequences to obtain mention boundaries, and mapping relevant token spans to pre-defined entity categories. For example, NER on the text Einstein won the Nobel Prize identifies the mentions “Einstein” and “Nobel Prize” and marks them as person and misc type, respectively. Named Entity Linking (NEL)1 involves the disambiguation of textual mentions, based on context and semantic information, and their mapping to proper entities in a KB (Bunescu & Pas¸ca, 2006; Cucerzan, 2007; Milne & Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cornolti et al., 2013). For example, in the above text, the mention “Einstein” is linked to the physicist Albert Einstein. Entity Co-reference Resolution (CR) (Haghighi & Klein, 2010; Ng, 2010; Lee et al., 2013) is essentially a clustering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups. For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different"
D15-1101,D11-1072,1,0.905972,"Missing"
D15-1101,N07-1011,0,0.160892,"0; Lee et al., 2013) is essentially a clustering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups. For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different from the mention Hans Albert Einstein. When CR is extended to an entire text corpus, in order to generate equivalence classes of co-referring mentions across documents, the task is known as Cross-document Co-reference Resolution (CCR) (Bagga & Baldwin, 1998; Culotta et al., 2007; Singh et al., 2011; Dutta & Weikum, 2015). Note that CCR is not the same as merely concatenating all documents in the corpus and utilizing existing CR methods. The linguistic diversity across documents and high computational cost for huge numbers of mentions in the corpus would typically make such a CR-based simulation perform poorly. Neither CR nor CCR links mention groups to corresponding KB entities. Thus, they represent both in-KB entities and out-of-KB entities (e.g., long-tail or emerging entities that do not have a Wikipedia article) in the same way. Cross-document co-reference resolu"
D15-1101,H05-1013,0,0.093172,"Missing"
D15-1101,D13-1203,0,0.046042,"Missing"
D15-1101,Q14-1037,0,0.316221,"sambiguation (Hoffart et al., 2011; Milne & Witten, 2008; Kulkarni et al., 2009; Ratinov et al., 2011). However, in the absence of CR mention groups, NEL has limited context and is bound to miss out on certain kinds of difficult cases. Although NER, CR, CCR and NEL involve closely related tasks and their tighter integration has been shown to be promising (Chen & Roth, 2013; Zheng et al., 2013), they have mostly been explored in isolation. Recently, several joint models have been proposed for CR-NER (Haghighi & Klein, 2010; Singh et al., 2013), CR-NEL (Hajishirzi et al., 2013), and NER-CR-NEL (Durrett & Klein, 2014). However, to the best of our knowledge, no method exists for jointly handling CCR and NEL on large text corpora. Input Corpus Doc 1 Doc 2 Doc 3 When Hugh played Logan, Ava Eliot was always with him. When Hugh played Wolverine, his daughter Ava accompanied him on the set. Hugh and Nicole played together in Australia. Knowledge Base Hugh Jackman Wolverine (character) Hugh Hefner Hugh Grant Mount Logan Logan Thomas Nicole Kidman Hurricane Nicole Nicolas Cage Nicole Murphy Australia (film) Australia Ava Gardner Eva Green Figure 1: Joint CCR-NEL Example (Green KB entries connected via arrows denot"
D15-1101,Q15-1002,1,0.231773,"tering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups. For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different from the mention Hans Albert Einstein. When CR is extended to an entire text corpus, in order to generate equivalence classes of co-referring mentions across documents, the task is known as Cross-document Co-reference Resolution (CCR) (Bagga & Baldwin, 1998; Culotta et al., 2007; Singh et al., 2011; Dutta & Weikum, 2015). Note that CCR is not the same as merely concatenating all documents in the corpus and utilizing existing CR methods. The linguistic diversity across documents and high computational cost for huge numbers of mentions in the corpus would typically make such a CR-based simulation perform poorly. Neither CR nor CCR links mention groups to corresponding KB entities. Thus, they represent both in-KB entities and out-of-KB entities (e.g., long-tail or emerging entities that do not have a Wikipedia article) in the same way. Cross-document co-reference resolution (CCR) computes equivalence classes ove"
D15-1101,W09-1119,0,0.108444,"Missing"
D15-1101,D12-1045,0,0.0919595,"arypis & Kumar, 1999)2 , to partition noncoreferent mentions groups. The Bayesian Information Criterion (BIC) (Schwarz, 1978; Hourdakis et al., 2010), a Bayesian variant of Minimum Description Length (Gr¨unwald, 2007), is used as the cluster split stopping criterion, and the context summaries within each final cluster are merged. CCR aims to process heterogeneous corpora that go beyond a single domain and style, such as Web collections. of 5447 mentions corresponding to 1068 distinct named-entities. Entity co-reference annotations (across documents within each topic cluster) were provided by (Lee et al., 2012), and we performed manual examination of the annotations for KB linking of the entities to Wikipedia entries, if present; thus providing ground truth for both CCR and NEL. • ClueWeb2009 FACC1 dataset4 (Gabrilovich et al., 2013): provides machine automated entity-linkage annotations of the ClueWeb09 corpus (ca. 1 Billion crawled Web pages) with Freebase entries5 . The corpus contains many topical domains and highly diverse documents from news, movie reviews, people home pages to blogs and other social media posts. We randomly select 500K documents containing 4.64 Million mentions associated wit"
D15-1101,P11-1138,0,0.813354,"ct & Sekine, 2007; Ratinov & Roth, 2009). This involves segmentation of token sequences to obtain mention boundaries, and mapping relevant token spans to pre-defined entity categories. For example, NER on the text Einstein won the Nobel Prize identifies the mentions “Einstein” and “Nobel Prize” and marks them as person and misc type, respectively. Named Entity Linking (NEL)1 involves the disambiguation of textual mentions, based on context and semantic information, and their mapping to proper entities in a KB (Bunescu & Pas¸ca, 2006; Cucerzan, 2007; Milne & Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cornolti et al., 2013). For example, in the above text, the mention “Einstein” is linked to the physicist Albert Einstein. Entity Co-reference Resolution (CR) (Haghighi & Klein, 2010; Ng, 2010; Lee et al., 2013) is essentially a clustering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups. For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different from the mention Hans Albert Einstein. When CR is extended to an e"
D15-1101,D12-1113,0,0.140645,"enote individual entities. 846 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 846–856, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. State-of-the-Art and its Limitations: Established CR methods rely on rule-based methods or supervised learning techniques on syntactic paths between mentions, semantic compatibility, and other linguistic features (Haghighi & Klein, 2009), with additional use of distant features from KBs (Lee et al., 2013). Modern cluster-ranking (Rahman & Ng, 2011) and multi-sieve methods (Ratinov & Roth, 2012) involve incremental expansion of mention groups by considering semantic types and Wikipedia categories. CCR methods utilize transitivity-aware clustering techniques (Singh et al., 2011), by considering mention-mention similarities (Bagga & Baldwin, 1998) along with features extracted from external KBs (Dutta & Weikum, 2015). NEL methods often harness the semantic similarity between mentions and entities and also among candidate entities for different mentions (in Wikipedia or other KBs) for contextualization and coherence disambiguation (Hoffart et al., 2011; Milne & Witten, 2008; Kulkarni et"
D15-1101,H05-1004,0,0.128076,"Missing"
D15-1101,W03-0405,0,0.0760331,"004). Probabilistic graphical models jointly learning the mappings of mentions to equivalent classes (co-referring mentions) using features similar to local CR techniques were studied in (Culotta et al., 2007; Singh et al., 2010; Singh et al., 2011), A clustering approach coupled with statistical learning of parameters was studied in (Baron & Freedman, 2008). However, such methods fail to cope with large corpora, and hence a “light-weight” streaming variant of CCR was introduced by (Rao et al., 2010). Co-occurring mentions context have been harnessed for disambiguating person names for CR in (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008). However, these methods do not use KB and depend on information extraction (IE) methods, witnessing substantial noise due to IE quality variance. A CCR framework combining co-occurring mention context with distant KB features embedded in an active hierarchical clustering procedure (Dutta & Weikum, 2015) was recently shown to perform efficiently, and provides inspiration for parts of our proposed C3EL approach. Named Entity Linking (NEL): Named entity resolution and linking stems from SemTag (Dill et al., 2003), and similar framew"
D15-1101,P11-1080,0,0.0397836,"Missing"
D15-1101,P10-1142,0,0.146989,"he text Einstein won the Nobel Prize identifies the mentions “Einstein” and “Nobel Prize” and marks them as person and misc type, respectively. Named Entity Linking (NEL)1 involves the disambiguation of textual mentions, based on context and semantic information, and their mapping to proper entities in a KB (Bunescu & Pas¸ca, 2006; Cucerzan, 2007; Milne & Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cornolti et al., 2013). For example, in the above text, the mention “Einstein” is linked to the physicist Albert Einstein. Entity Co-reference Resolution (CR) (Haghighi & Klein, 2010; Ng, 2010; Lee et al., 2013) is essentially a clustering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups. For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different from the mention Hans Albert Einstein. When CR is extended to an entire text corpus, in order to generate equivalence classes of co-referring mentions across documents, the task is known as Cross-document Co-reference Resolution (CCR) (Bagga & Baldwin, 1998; C"
D15-1101,P04-1076,0,0.0396111,"phical models jointly learning the mappings of mentions to equivalent classes (co-referring mentions) using features similar to local CR techniques were studied in (Culotta et al., 2007; Singh et al., 2010; Singh et al., 2011), A clustering approach coupled with statistical learning of parameters was studied in (Baron & Freedman, 2008). However, such methods fail to cope with large corpora, and hence a “light-weight” streaming variant of CCR was introduced by (Rao et al., 2010). Co-occurring mentions context have been harnessed for disambiguating person names for CR in (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008). However, these methods do not use KB and depend on information extraction (IE) methods, witnessing substantial noise due to IE quality variance. A CCR framework combining co-occurring mention context with distant KB features embedded in an active hierarchical clustering procedure (Dutta & Weikum, 2015) was recently shown to perform efficiently, and provides inspiration for parts of our proposed C3EL approach. Named Entity Linking (NEL): Named entity resolution and linking stems from SemTag (Dill et al., 2003), and similar frameworks like GLOW, Wi"
D15-1101,N06-1025,0,0.0486718,"raditional intradocument CR methods involve syntactic and semantic feature combination for identifying the best antecedent (preceding name or phrase) for a mention. CR methods employ rules or supervised learning techniques based on linguistic features such as syntactic paths and mention distances to assess semantic compatibility (Haghighi & Klein, 2009; Raghunathan et al., 2010; Rahman & Ng, 2011), while syntactic features are derived by deep parsing of sentences and noun group parsing. Semantic features from background knowledge resources like encyclopedia were used in (Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007). The use of Wikipedia and structured knowledge bases (such as YAGO) to obtain mention-type relation and fine-grained mention attributes was explored by (Haghighi & Klein, 2009; Rahman & Ng, 2011). An overview of CR methods is given in (Ng, 2010). Recent methods involve the use of multi-phase sieve, applying a cascade of rules for narrowing down the antecedent candidates for a mention (Raghunathan et al., 2010). Cluster ranking functions have also been proposed (Rahman & Ng, 2011; Zheng et al., 2013) to extend this paradigm for incrementally expanding and merging mention groups with"
D15-1101,D10-1048,0,0.165311,"proper entity in the KB. To this end, C3EL consists of 3 algorithmic stages: (i) Pre-Processing, (ii) Interleaved NEL and CCR, and (iii) Finalization. 2.1 Pre-Processing Stage HTML pages in the input corpus C are transformed into plain text using standard tools like jsoup. org. Recognition and markup of mentions are performed using the Stanford CoreNLP toolkit (nlp. stanford.edu), and a coarse-grained lexical type for each mention (e.g., person, location, organization, etc.) is obtained from the Stanford NER Tagger (Finkel et al., 2005). The multi-pass sieve algorithm for single-document CR (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013) then computes mention co-reference chains per document, and a head mention is chosen for each of the mention groups (chains). The head mention is typically represented by the most explicit denotation of the entity (e.g., person’s full name with title, location name with country, etc.). For each of the mention groups Mi , C3EL then constructs a context summary using: • Sentences – all sentences in the document that contain mentions of group Mi ; and • Co-occurrence – all sentences for other mention groups that contain mentions co-occurring in any of the sen"
D15-1101,W13-3517,0,0.14259,"ta & Weikum, 2015). NEL methods often harness the semantic similarity between mentions and entities and also among candidate entities for different mentions (in Wikipedia or other KBs) for contextualization and coherence disambiguation (Hoffart et al., 2011; Milne & Witten, 2008; Kulkarni et al., 2009; Ratinov et al., 2011). However, in the absence of CR mention groups, NEL has limited context and is bound to miss out on certain kinds of difficult cases. Although NER, CR, CCR and NEL involve closely related tasks and their tighter integration has been shown to be promising (Chen & Roth, 2013; Zheng et al., 2013), they have mostly been explored in isolation. Recently, several joint models have been proposed for CR-NER (Haghighi & Klein, 2010; Singh et al., 2013), CR-NEL (Hajishirzi et al., 2013), and NER-CR-NEL (Durrett & Klein, 2014). However, to the best of our knowledge, no method exists for jointly handling CCR and NEL on large text corpora. Input Corpus Doc 1 Doc 2 Doc 3 When Hugh played Logan, Ava Eliot was always with him. When Hugh played Wolverine, his daughter Ava accompanied him on the set. Hugh and Nicole played together in Australia. Knowledge Base Hugh Jackman Wolverine (character) Hugh"
D15-1101,P11-1082,0,0.0359699,"Missing"
D15-1101,C10-2121,0,\N,Missing
D15-1101,W11-1902,0,\N,Missing
D15-1101,D09-1120,0,\N,Missing
D15-1101,J13-4004,0,\N,Missing
D15-1101,P05-1045,0,\N,Missing
D15-1103,E09-1005,0,0.0207125,"Missing"
D15-1103,P98-1013,0,0.0728708,"tching type was found. We then proceed differently: if a KB lookup fails, we add the complete set T to the candidate set and continue to the next extractor. As mentioned above, FINET can also be run without any KB lookups; the corresponding extractors then do not have a stopping condition. In Sec. 4, we experimented with both variants and found that KB lookups generally helped. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include hrepresentative-1i, hrepresentative-2i, or hpolitician-1i. Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”). To exploit it, we apply a set of morphologic"
D15-1103,P05-1006,0,0.0951494,"Missing"
D15-1103,D12-1082,0,0.0474996,"rbr¨ucken, Germany {delcorro, abujabal, weikum}@mpi-inf.mpg.de † Universit¨at Mannheim, Mannheim, Germany rgemulla@uni-mannheim.de base1 (KB) (Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entity is introduced or expected to be unknown to readers,"
D15-1103,D14-1042,1,0.845536,"ral type. We then also run subsequent extractors. Tab. 1 displays a summary of the extractors and stopping conditions. All so-found type candidates are passed to the candidate selection phase (Sec. 3). 2.1 Preprocessing Preprocessing consists of 5 steps: (i) dependency parsing (Socher et al., 2013); (ii) co-reference (Recasens et al., 2013); (iii) named entity recogni869 tion (NER) (Finkel et al., 2005) with the detection of coarse-grained types (i.e., person, organization, location); (iv) clause identification (Del Corro and Gemulla, 2013); (v) word and multi-word expression recognition (Del Corro et al., 2014). FINET restricts its candidates to the hyponmys of the coarse-grained (CG) type of the NER system. Named entities with the same CG type in a coordinating relation (e.g., “Messi and Ronaldo are soccer players”) and identical mentions share the candidate set; the latter is reasonable in short input. FINET extractors operate either on the sentence or the clause level. A clause is a part of a sentence that expresses a statement and is thus a suitable unit for automatic text processing (Del Corro and Gemulla, 2013). Finally, we identify multiword explicit type mentions such as Prime Minister or Se"
D15-1103,P05-1045,0,0.0291681,"hen the condition is not met, we enrich the set of candidate types of the extractor with their hypernyms; we expect types to be overly specific and want to allow the selection phase to be able to select a more general type. We then also run subsequent extractors. Tab. 1 displays a summary of the extractors and stopping conditions. All so-found type candidates are passed to the candidate selection phase (Sec. 3). 2.1 Preprocessing Preprocessing consists of 5 steps: (i) dependency parsing (Socher et al., 2013); (ii) co-reference (Recasens et al., 2013); (iii) named entity recogni869 tion (NER) (Finkel et al., 2005) with the detection of coarse-grained types (i.e., person, organization, location); (iv) clause identification (Del Corro and Gemulla, 2013); (v) word and multi-word expression recognition (Del Corro et al., 2014). FINET restricts its candidates to the hyponmys of the coarse-grained (CG) type of the NER system. Named entities with the same CG type in a coordinating relation (e.g., “Messi and Ronaldo are soccer players”) and identical mentions share the candidate set; the latter is reasonable in short input. FINET extractors operate either on the sentence or the clause level. A clause is a part"
D15-1103,C12-1109,0,0.019804,"tep can be used as-is; it is not trained on any domain- or corpus-specific data. 3.1 Obtaining context All WSD systems take a set of candidate types and contextual information as input. The key challenge lies in the construction of candidate types (Sec. 2) as well as context. For each entity, we consider entity-oblivious context (from the input sentence) as well as entity-specific context (using lexical expansions). We take all words in the sentence as entityoblivious context. To construct entity-specific context, we make use of lexical expansions, which have been successfully applied in WSD (Miller et al., 2012). Its goal is to enrich contextual information to boost disambiguation. In our case, it also helps to differentiate between multiple entities in a sentence. We build the entity-specific context using word vectors. As in the corpus-based extractor, we construct a set of queries for the entity but in this case we take as context all so-obtained words that do not correspond to a named entity. For instance, the entity-specific context for the entity mention “Maradona” for query “Maradona South Africa” is: “coach”, “cup”, “striker”, “midfielder”, and “captain”. The full context for “Maradona” in “M"
D15-1103,P13-1120,0,0.0200621,"adding the direct object as a noun modifier of the deverbal noun (e.g., From “Messi plays soccer”, we form “soccer player”). If it exists in WordNet, we consider the respective types as potential candidates as well. A more indirect way of exploiting the verb-type semantic concordance is via a corpus of frequent (verb, type)-pairs, where the type refers to possible types of the verb’s subject or object. As stated above, the set of argument types compatible with a verb is limited. For instance, “treat” is usually followed by hcondition-1i, hdisease-1i, or hpatient1i. FINET, uses the corpora of Flati and Navigli (2013) and Del Corro et al. (2014). Given a verb and an entity, we search for frequent candidate types (depending on whether the entity acts as a subject or object). For example, from “Messi was treated in the hospital,” we obtain hpatient-1i. Once potential candidates have been collected, we perform a KB lookup to decide how to proceed. 2.6 resentation of a phrase and represents the semantic context in which the phrase occurs. Phrases that are semantically related, and thus appear in similar contexts, are close to each other in the word vector space. For instance, if “Messi” and “Cristiano Ronaldo”"
D15-1103,C02-1130,0,0.718644,"od that uses a hierarchical classifier. Its features include the words in the named entity mention, in sentence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories hartifact-1i, hevent1i, hperson-1i, hlocation-1i, and horganization1i. Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without th"
D15-1103,D12-1104,1,0.575482,"Gerhard Weikum∗ ∗ Max-Planck-Institut f¨ur Informatik, Saarbr¨ucken, Germany {delcorro, abujabal, weikum}@mpi-inf.mpg.de † Universit¨at Mannheim, Mannheim, Germany rgemulla@uni-mannheim.de base1 (KB) (Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entit"
D15-1103,C92-2082,0,0.407489,"et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entity is introduced or expected to be unknown to readers, but often are not explicitly stated. The NET problem differs from taxonomy induction in that (1) the type system is prespecified, (2) types are disambiguated, and (3) types are associated with each occurrence of named entity in context. Our FINET system makes use of explicit type extractions whenever possible. But even when types are not explicitly mentioned, sentences may give clues to"
D15-1103,P13-1146,1,0.916373,"S tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories hartifact-1i, hevent1i, hperson-1i, hlocation-1i, and horganization1i. Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This allows us to estimate the extent to which referring to a KB helps. Note that the corpus-based extractor makes use"
D15-1103,J13-3007,0,0.0129906,"tor, which may produce a large number of candidates and thus makes type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since hgraduate-1i or hregion-1i are highly frequent in the KB, many persons (locations) were incorrectly typed as hgraduate-1i (hregion-1i). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both hperson-1i and hlocation-1i). 5 Related Work The NET problem is related to taxonomy induction (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implic"
D15-1103,J05-1004,0,0.0244512,"d. We then proceed differently: if a KB lookup fails, we add the complete set T to the candidate set and continue to the next extractor. As mentioned above, FINET can also be run without any KB lookups; the corresponding extractors then do not have a stopping condition. In Sec. 4, we experimented with both variants and found that KB lookups generally helped. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include hrepresentative-1i, hrepresentative-2i, or hpolitician-1i. Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”). To exploit it, we apply a set of morphological transformations to"
D15-1103,P10-1154,0,0.0285599,"Missing"
D15-1103,D14-1038,0,0.0369825,"atasets. When FG labels were correct, the pre875 Last used extractor Pattern-based Mention-based Verb-based Corpus-based Entities P 180 219 47 205 71.11 82.65 48.94 64.39 individually using an existing type system. FINET draws from ideas used in taxonomy induction or KB construction. Existing systems are either based on patterns or the distributional hypothesis; these two approaches are discussed and compared in (Shi et al., 2010). In FINET, we make use of patterns (such as the ones of Hearst (1992)) in most of our extractors and of the distributional hypothesis in our corpus-based extractor. Yahya et al. (2014) developed a semisupervised method to extract facts such as “president”(“Barack Obama”, “US”), in which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by sup"
D15-1103,C10-1105,0,0.101754,"classifier. Its features include the words in the named entity mention, in sentence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories hartifact-1i, hevent1i, hperson-1i, hlocation-1i, and horganization1i. Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This all"
D15-1103,N13-1071,0,0.0345124,"Missing"
D15-1103,P13-4023,1,0.948561,"ntence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories hartifact-1i, hevent1i, hperson-1i, hlocation-1i, and horganization1i. Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This allows us to estimate the extent to which referring to a KB helps. Note that the"
D15-1103,P10-4014,0,0.0647026,"Missing"
D15-1103,C10-1112,0,0.0105075,"verb-based extractor, which may produce a large number of candidates and thus makes type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since hgraduate-1i or hregion-1i are highly frequent in the KB, many persons (locations) were incorrectly typed as hgraduate-1i (hregion-1i). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both hperson-1i and hlocation-1i). 5 Related Work The NET problem is related to taxonomy induction (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explici"
D15-1103,P06-1101,0,0.0322698,"Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entity is introduced or expected to be unknown to readers, but often are not explicitly stated. The NET problem differs from taxonomy induction in that (1) the type system is prespecified, (2) types are"
D15-1103,P13-1045,0,0.0143979,"ype selection phase. The reasoning behind this approach is to bias FINET towards the most explicit types. When the condition is not met, we enrich the set of candidate types of the extractor with their hypernyms; we expect types to be overly specific and want to allow the selection phase to be able to select a more general type. We then also run subsequent extractors. Tab. 1 displays a summary of the extractors and stopping conditions. All so-found type candidates are passed to the candidate selection phase (Sec. 3). 2.1 Preprocessing Preprocessing consists of 5 steps: (i) dependency parsing (Socher et al., 2013); (ii) co-reference (Recasens et al., 2013); (iii) named entity recogni869 tion (NER) (Finkel et al., 2005) with the detection of coarse-grained types (i.e., person, organization, location); (iv) clause identification (Del Corro and Gemulla, 2013); (v) word and multi-word expression recognition (Del Corro et al., 2014). FINET restricts its candidates to the hyponmys of the coarse-grained (CG) type of the NER system. Named entities with the same CG type in a coordinating relation (e.g., “Messi and Ronaldo are soccer players”) and identical mentions share the candidate set; the latter is reasona"
D15-1103,N06-2015,0,\N,Missing
D15-1103,C98-1013,0,\N,Missing
D15-1103,W03-0419,0,\N,Missing
D15-1103,J14-1003,0,\N,Missing
D15-1103,C12-2133,1,\N,Missing
D15-1113,P98-1013,0,0.227157,"Missing"
D15-1113,D11-1142,0,0.489676,"Missing"
D15-1113,P14-1098,0,0.0533311,"Missing"
D15-1113,N13-1092,0,0.116603,"Missing"
D15-1113,P11-1062,0,0.43906,"ocuments. We collected a dataset consisting of movie plot summaries from two different websites, Wikipedia and the Internet Movie Database (IMDB). We chose plot synopses from 25 James Bond movies and 23 movies based on the Marvel Comics characters. For each plot synopsis, we have two plot descriptions: one from Wikipedia and another from IMDB. Given a query in the form of an anonymized plot description from one website, the task is to rank the anonymized plot descriptions Entailment Graph Induction We compared the performance of PSL against the Integer Linear Programming (ILP) formulation by (Berant et al., 2011). The comparison was performed on the task of creating entailment graphs as described in (Berant et al., 2011). This task is strongly related to finding hypernyms of relational phrases. The experiments were executed on the dataset of 10 manually annotated graphs. In total this dataset contains 3,427 positive and 35,585 negative examples. Our model uses the transitivity rule (entails(A, B) ∧ entails(B, C) ⇒ entails(A, C)). We also include the local entailment scores (score(A, B) ⇒ entails(A, B)) which were released by (Berant et al., 2011). Table 5 presents micro-averaged precision, recall and"
D15-1113,P12-1013,0,0.0681677,"on person marry daughter person person had paid person athlete played for team Table 5: Results for Entailment graphs induction Berant et al. (2011) PSL Prec. Rec. F1 0.422 0.461 0.434 0.435 0.428 0.447 Hypernym relational phrase Text pattern Range resigns as person accused person joins person interacted with person played for organization ods used for this problem. To compare efficiency we measured the run-time of our method. Without any graph decomposition it took on average 232 seconds. The experiments were performed on a multi-core 2.67GHz server with 32GB of RAM. The methods reported in (Berant et al., 2012), which did not utilize graph decomposition method, had run-time above 5000 seconds. with 0.9-confidence Wilson score interval for a random sample of 100 examples. Next, we show how global constraints on the hypernymy graph such as anti-symmetry and acyclicity improve the quality of the hypernymy graph. Since the relational phrases generated by PATTY are clustered to find synonymous relations, these global constraints prevent RELLY from merging clusters. When the anti-symmetry and acyclicity rules were removed from the model, the resulting hypernymy graph included approximately 500 additional"
D15-1113,C14-1207,1,0.874489,"projects address the challenge of understanding relational phrases, but existing linguistic resources are often limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 relations. A number of manuallycurated relational taxonomies such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998) also offer highprecision hierarchies with limited coverage. In this paper, we introduce RELLY, a method for producing a hypernymy graph that has both high coverage and p"
D15-1113,W04-3205,0,0.0607116,"ype &lt;book>. Several projects from the Open Information Extraction community have addressed the task of finding synonyms of relational phrases using clustering algorithms. The biggest collection of relational phrases and their synonyms is currently the PATTY project (Nakashole et al., 2012), with around 350,000 semantically typed relational phrases. Prominent alternatives are WiseNet (Moro and Navigli, 2012), which offers 40,000 synsets of relational phrases, PPDB (Ganitkevitch et al., 2013), which contains over 220 million paraphrase pairs, as well as DIRT and VerbOcean (Lin and Pantel, 2001; Chklovski and Pantel, 2004) which inspired the approach and results pursued here. Relational phrases can be further organized into a hierarchical structure according to their hypernymy (subsumption) relationships. For example, “&lt;person> moves to &lt;country>” is a hypernym of the relational phrase “&lt;musician> emigrates to &lt;country>.” Of the aforementioned collections, only PATTY attempts to automatically create a subsumption hierarchy for the extracted relational phrases. The authors of the HARPY system argue that the sparseness of PATTY’s graph comes from the lack of general phrases in the source corpus. As a solution, th"
D15-1113,W14-1610,0,0.0414228,"t H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” cluster phrases based on the similarity of translations to other languages. Of these systems, only PATTY attempted to create a hierarchy of relations and the result was very sparse. HARPY aimed to overcome this problem by disambiguating and aligning relational phrases with WordNet, and performing a simple reconstruction of the WordNet hierarchy on top of relational phrases from PATTY. A very similar problem was addressed in the entailment graph project (Levy et al., 2014). The authors automatically created graphs of entailments between propositions, using Integer Linear Programing as one of the main components. Propositions can be encoded as triples of form (subject, relation, object). Edges in the entailment graph occur between these triples, whereas edges connect typed relations in PATTY and HARPY. Moreover, the relations in the propositions were mainly limited to single verbs, whereas in our case we also consider longer relational phrases. Relations with semantic types were also used in typed entailment graphs (Berant et al., 2011). However, the type hierar"
D15-1113,N12-1019,0,0.0733402,"Missing"
D15-1113,N13-1018,0,0.0213866,"erbNet and FrameNet. WordNet has 13,767 verb synsets, which are organized into a hierarchy with 13,239 hypernymy links. Automatic construction of taxonomies of named entities or noun phrases has received much more attention than organization of verbs or relations. In (Snow et al., 2006), the WordNet taxonomy was extended by 10,000 novel noun synsets with hypernym-hyponym links. In (Bansal et al., 2014), the authors reconstructed WordNet’s noun hypernymy/hyponymy hierarchy from scratch using a probabilistic graphical model formulation. Another method of organizing noun phrases was proposed in (Mehdad et al., 2013), where an entailment graph of noun phrases was constructed. Acknowledgments: This work was partially supported by National Science Foundation (NSF) grant IIS1218488 and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D12PC00337. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily repr"
D15-1113,D12-1104,1,0.96432,"ed to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understanding relational phrases, but existing linguistic resources are often limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 relations. A number of manuallycurated relational taxonomies such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998) a"
D15-1113,P06-1101,0,0.0492998,"to infer multi-verb or noun relational phrases. The RELLY resource is publicly available at www.mpi-inf.mpg.de/yago-naga/patty/. Although there is a scarcity of automatically created taxonomies of relations, there exist several manually curated taxonomies. Manually crafted verb or relation hierarchies are available in WordNet, VerbNet and FrameNet. WordNet has 13,767 verb synsets, which are organized into a hierarchy with 13,239 hypernymy links. Automatic construction of taxonomies of named entities or noun phrases has received much more attention than organization of verbs or relations. In (Snow et al., 2006), the WordNet taxonomy was extended by 10,000 novel noun synsets with hypernym-hyponym links. In (Bansal et al., 2014), the authors reconstructed WordNet’s noun hypernymy/hyponymy hierarchy from scratch using a probabilistic graphical model formulation. Another method of organizing noun phrases was proposed in (Mehdad et al., 2013), where an entailment graph of noun phrases was constructed. Acknowledgments: This work was partially supported by National Science Foundation (NSF) grant IIS1218488 and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior Nation"
D15-1113,W03-1011,0,0.044823,"arkov logic networks, the atoms in each logical rule take values in the 973 Table 1: PSL predicates; R1, R2 are relational phrases, V b1, V b2 WordNet verb senses and T L1, T R1, T 1, T 2 YAGO types PSL predicate Type Description weedsInclusion(R1, R2) statistical pattySubsumption(R1, R2) harpy(R1, V b1) statistical statistical wordnetHyponym(V b1, V b2) lT ype(R1, T L1) rT ype(R1, T R1) yagoHyponym(T 1, T 2) candidateHyponym(R1, R2) semantic semantic semantic semantic output hyponym(R1, R2) output degree of inclusion of sets of argument pairs of re|ArgsR1| lations defined as |ArgsR1∩ArgsR2| (Weeds and Weir, 2003) PATTY subsumption (Nakashole et al., 2012) alignment links between relational phrases and WordNet verb senses (Grycner and Weikum, 2014) hyponymy link between WordNet verb senses left (domain) type of arguments of a relational phrase right (range) type of arguments of a relational phrase T 1 is a subtype of T 2 in YAGO hierarchy relational phrase R1 is more specific than R2 (without enforcing consistent argument types) relational phrase R1 is more specific than R2 R2 may be a hypernym of R1. We use two measures of argument overlap, weedsInclusion and pattySubsumption, in rules 1 and 2, respec"
D15-1113,D12-1035,1,0.895613,"Missing"
D15-1113,C98-1013,0,\N,Missing
D16-1236,P98-1013,0,0.0116569,"nse Disambiguation (Moro et al., 2014), Named Entity Disambiguation (Hoffart et al., 2011), Question Answering (Fader et al., 2014), and Textual Entailment (Sha et al., 2015). Widely used KBs are DBpedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), YAGO (Mahdisoltani et al., 2015), Wikidata (Vrandecic and Kr¨otzsch, 2014) and the Google Knowledge Vault (Dong et al., 2014). KBs have rich information about named entities, but are pretty sparse on relations. In the latter regard, manually created resources such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008) or FrameNet (Baker et al., 1998) are much richer, but still face the limitation of labor-intensive input and human curation. The paradigm of Open Information Extraction (OIE) was developed to overcome the weak coverage of relations in automatically constructed KBs. OIE methods process natural language texts to produce triples of surface forms for the arguments and relational phrase of binary relations. The first large-scale approach along these lines, TextRunner (Banko et al., 2007), was later improved by ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012). The focus of these methods has been on verbal phrases as rel"
D16-1236,P05-1074,0,0.149281,"sequence mining algorithm to extract relational phrases with semantic type signatures, and organized them into synonymy sets and hypernymy hierarchies. WiseNet (Moro and Navigli, 2012) tapped Wikipedia categories for a similar way of organizing relational paraphrases. DEFIE (Bovi et al., 2015) went even further and used word sense disambiguation, anchored in WordNet, to group phrases with the same meanings. Translation models have previously been used for paraphrase detection. Barzilay and McKeown (2001) utilized multiple English translations of the same source text for paraphrase extraction. Bannard and Callison-Burch (2005) used the bilingual pivoting method on parallel corpora for the same task. Similar methods were performed at a much bigger scale by the Paraphrase Database (PPDB) project (Pavlick et al., 2015). Unlike POLY, the focus of these projects was not on paraphrases of binary relations. Moreover, POLY considers the semantic type signatures of relations, which is missing in PPDB. Research on OIE for languages other than English has received little attention. Kim et al. (2011) uses Korean-English parallel corpora for cross-lingual projection. Gamallo et al. (2012) developed an OIE system for Spanish and"
D16-1236,P01-1008,0,0.117941,"t large repositories of relational paraphrases are PATTY, WiseNet and DEFIE. PATTY (Nakashole et al., 2012) devised a sequence mining algorithm to extract relational phrases with semantic type signatures, and organized them into synonymy sets and hypernymy hierarchies. WiseNet (Moro and Navigli, 2012) tapped Wikipedia categories for a similar way of organizing relational paraphrases. DEFIE (Bovi et al., 2015) went even further and used word sense disambiguation, anchored in WordNet, to group phrases with the same meanings. Translation models have previously been used for paraphrase detection. Barzilay and McKeown (2001) utilized multiple English translations of the same source text for paraphrase extraction. Bannard and Callison-Burch (2005) used the bilingual pivoting method on parallel corpora for the same task. Similar methods were performed at a much bigger scale by the Paraphrase Database (PPDB) project (Pavlick et al., 2015). Unlike POLY, the focus of these projects was not on paraphrases of binary relations. Moreover, POLY considers the semantic type signatures of relations, which is missing in PPDB. Research on OIE for languages other than English has received little attention. Kim et al. (2011) uses"
D16-1236,D13-1160,0,0.034399,"n language. • POLY ALL: using synonyms of relational phrases derived from the 61 languages. Since DEFIE’s relational phrases are represented by BabelNet (Navigli and Ponzetto, 2012) word sense identifiers, we generated all possible lemmas for each identifier. We ran the paraphrase-enhanced QA system for three benchmark sets of questions: • TREC: the set of questions used for the evaluation of information retrieval QA systems (Voorhees and Tice, 2000) • WikiAnswers: a random subset of questions from WikiAnswers (Fader et al., 2013). • WebQuestions: the set of questions about Freebase entities (Berant et al., 2013). From these question sets, we kept only those questions which can be parsed by one of the 10 question parsing templates and have a correct answer in the gold-standard ground truth. In total, we executed 451 questions for TREC, 516 for WikiAnswers and 1979 for WebQuestions. For every question, each paraphrasing system generates a set of answers. We measured for how many questions we could obtain at least one correct answer. Table 7 shows the results. The best results were obtained by POLY ALL. We performed a paired t-test for the results of POLY DE and POLY ALL against all other systems. The d"
D16-1236,Q15-1038,0,0.462551,"s are an asset for a variety of tasks, including information extraction, textual entailment, and question answering. This paper presents a new method for systematically organizing a large set of such phrases. We aim to construct equivalence classes of synonymous phrases, analogously to how WordNet organizes State of the Art and its Limitations. Starting with the seminal work on DIRT (Lin and Pantel, 2001), there have been various attempts on building comprehensive resources for relational phrases. Recent works include PATTY (Nakashole et al., 2012), WiseNet (Moro and Navigli, 2012) and DEFIE (Bovi et al., 2015). Out of these DEFIE is the cleanest resource. However, the equivalence classes tend to be small, prioritizing precision over recall. On the other hand, PPDB (Ganitkevitch et al., 2013) offers the largest repository of paraphrases. However, the paraphrases are not relation-centric and they are not semantically typed. So it misses out on the opportunity of using types to distinguish identical phrases with different semantics, for example, performance in with argument types musician and song versus performance in with types athlete and competition. Our Approach. We start with a large collection"
D16-1236,D11-1142,0,0.388706,"question words to semantic types. For example, the word who is mapped to person, where to location, when to abstract entity and the rest of the question words are mapped to type entity. We harness synonyms and hyponyms of relational phrases to paraphrase the predicate of the query. The paraphrases must be compatible with the semantic type of the question word. In the end, we use the original query, as well as found paraphrases, to query a database of subject, predicate, object triples. As the knowledge graph for this experiment we used the union of collections: a triples database from OpenIE (Fader et al., 2011), Freebase (Bollacker et al., 2008), Probase (Wu et al., 2012) and NELL (Carlson et al., 2010). In total, this knowledge graph contained more than 900 Million triples. We compared six systems for paraphrasing semantically typed relational phrases: • Basic: no paraphrasing at all, merely using the originally generated query. • DEFIE: using the taxonomy of relational phrases by Bovi et al. (2015). • PATTY: using the taxonomy of relational phrases by Nakashole et al. (2012). • RELLY: using the subset of the PATTY taxonomy with additional entailment relationships between phrases (Grycner et al., 2"
D16-1236,P13-1158,0,0.0368806,"al., 2015). • POLY DE: using synonyms of relational phrases derived from the German language. • POLY ALL: using synonyms of relational phrases derived from the 61 languages. Since DEFIE’s relational phrases are represented by BabelNet (Navigli and Ponzetto, 2012) word sense identifiers, we generated all possible lemmas for each identifier. We ran the paraphrase-enhanced QA system for three benchmark sets of questions: • TREC: the set of questions used for the evaluation of information retrieval QA systems (Voorhees and Tice, 2000) • WikiAnswers: a random subset of questions from WikiAnswers (Fader et al., 2013). • WebQuestions: the set of questions about Freebase entities (Berant et al., 2013). From these question sets, we kept only those questions which can be parsed by one of the 10 question parsing templates and have a correct answer in the gold-standard ground truth. In total, we executed 451 questions for TREC, 516 for WikiAnswers and 1979 for WebQuestions. For every question, each paraphrasing system generates a set of answers. We measured for how many questions we could obtain at least one correct answer. Table 7 shows the results. The best results were obtained by POLY ALL. We performed a pa"
D16-1236,N15-1151,0,0.181872,"zing precision over recall. On the other hand, PPDB (Ganitkevitch et al., 2013) offers the largest repository of paraphrases. However, the paraphrases are not relation-centric and they are not semantically typed. So it misses out on the opportunity of using types to distinguish identical phrases with different semantics, for example, performance in with argument types musician and song versus performance in with types athlete and competition. Our Approach. We start with a large collection of relational triples, obtained by shallow information extraction. Specifically, we use the collection of Faruqui and Kumar (2015), obtained by combining the OLLIE tool with Google Translate and projecting multilingual sentences back to English. Note that the task addressed in that work is relational triple extraction, which is orthogonal to our problem of organizing the relational phrases in these triples into synonymy sets. We canonicalize the subject and object arguments 2183 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2183–2192, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of triples by applying named entity disambiguation and wo"
D16-1236,W12-0702,0,0.0487537,"Missing"
D16-1236,N13-1092,0,0.202887,"Missing"
D16-1236,D15-1113,1,0.847624,"der et al., 2011), Freebase (Bollacker et al., 2008), Probase (Wu et al., 2012) and NELL (Carlson et al., 2010). In total, this knowledge graph contained more than 900 Million triples. We compared six systems for paraphrasing semantically typed relational phrases: • Basic: no paraphrasing at all, merely using the originally generated query. • DEFIE: using the taxonomy of relational phrases by Bovi et al. (2015). • PATTY: using the taxonomy of relational phrases by Nakashole et al. (2012). • RELLY: using the subset of the PATTY taxonomy with additional entailment relationships between phrases (Grycner et al., 2015). • POLY DE: using synonyms of relational phrases derived from the German language. • POLY ALL: using synonyms of relational phrases derived from the 61 languages. Since DEFIE’s relational phrases are represented by BabelNet (Navigli and Ponzetto, 2012) word sense identifiers, we generated all possible lemmas for each identifier. We ran the paraphrase-enhanced QA system for three benchmark sets of questions: • TREC: the set of questions used for the evaluation of information retrieval QA systems (Voorhees and Tice, 2000) • WikiAnswers: a random subset of questions from WikiAnswers (Fader et al"
D16-1236,D11-1072,1,0.871001,"Missing"
D16-1236,I11-1083,0,0.0131144,"lay and McKeown (2001) utilized multiple English translations of the same source text for paraphrase extraction. Bannard and Callison-Burch (2005) used the bilingual pivoting method on parallel corpora for the same task. Similar methods were performed at a much bigger scale by the Paraphrase Database (PPDB) project (Pavlick et al., 2015). Unlike POLY, the focus of these projects was not on paraphrases of binary relations. Moreover, POLY considers the semantic type signatures of relations, which is missing in PPDB. Research on OIE for languages other than English has received little attention. Kim et al. (2011) uses Korean-English parallel corpora for cross-lingual projection. Gamallo et al. (2012) developed an OIE system for Spanish and Portuguese using rules over shallow dependency parsing. The recent work of Faruqui and Kumar (2015) extracted relational phrases from Wikipedia in 61 languages using crosslingual projection. Lewis and Steedman (2013) clustered semantically equivalent English and French phrases, based on the arguments of relations. 7 Conclusions We presented POLY, a method for clustering semantically typed English relational phrases using a multilingual corpus, resulting in a reposit"
D16-1236,D13-1064,0,0.0521369,"Missing"
D16-1236,P14-5010,0,0.00445357,"o was shot by director Matthew Rolston. arg1 was shot by David Fincher A second movie was filmed by David Fincher. director Matthew Rolston rel fue filmado por arg2 Un segundo video fue filmado por David Fincher. Translation El video fue filmado por el direcor Matthew Rolston. Translation The video rel A second movie arg2 was filmed by arg1 rel David Fincher arg2 Figure 1: Multilingual input sentences and triples noun phrases. As output, we produce a ranked list of entity mentions and common nouns. To create this ranking, we perform POS tagging and noun phrase chunking using Stanford CoreNLP (Manning et al., 2014) and Apache OpenNLP 2 . For head noun extraction, we use the YAGO Javatools3 and a set of manually crafted regular expressions. Since the input sentences result from machine translation, we could not use dependency parsing, because sentences are often ungrammatical. Finally, we extract all noun phrases which contain the same head noun. These noun phrases are then sorted according to their lengths. For example, for input phrase contemporary British director who also created “Inception”, our method would yield contemporary British director, British director, director in decreasing order. 3.2 Arg"
D16-1236,D12-1048,0,0.568345,"onal phrase typing and relational phrase clustering. In Section 3, we explain how we infer semantic types of the arguments of a relational phrase. In Section 4, we present the model for computing synonyms of relational phrases (i.e., paraphrases) and organizing them into clusters. A major asset for our approach is a large corpus of multilingual sentences from the work of Faruqui and Kumar (2015). That dataset contains sentences from Wikipedia articles in many languages. Each sentence has been processed by an Open Information Extraction method (Banko et al., 2007), specifically the OLLIE tool (Mausam et al., 2012), which produces a triple of surface phrases that correspond to a relational phrase candidate and its two arguments (subject and object). Each non-English sentence has been translated into English using Google Translate, thus leveraging the rich statistics that Google has obtained from all kinds of parallel multilingual texts. Altogether, the data from Faruqui and Kumar (2015) provides 135 million triples in 61 languages and in English (from the translations of the corresponding sentences). This is the noisy input to our 1 www.mpi-inf.mpg.de/yago-naga/poly/ 2184 method. Figure 1 shows two Span"
D16-1236,Q14-1019,0,0.0167242,"combination of all of the described datasets and all of the described datasets without POLY. The difference between these two versions suggest that POLY contains many paraphrases which are available in none of the competing resources. TREC WikiAnswers WebQuestions Basic DEFIE RELLY PATTY POLY DE POLY ALL 193 197 208 213 232 238 144 147 150 155 163 173 365 394 424 475 477 530 All All / POLY 246 218 176 157 562 494 Questions 451 516 1979 Table 7: Number of questions with correct answer. 2190 6 Related Work Knowledge bases (KBs) contribute to many NLP tasks, including Word Sense Disambiguation (Moro et al., 2014), Named Entity Disambiguation (Hoffart et al., 2011), Question Answering (Fader et al., 2014), and Textual Entailment (Sha et al., 2015). Widely used KBs are DBpedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), YAGO (Mahdisoltani et al., 2015), Wikidata (Vrandecic and Kr¨otzsch, 2014) and the Google Knowledge Vault (Dong et al., 2014). KBs have rich information about named entities, but are pretty sparse on relations. In the latter regard, manually created resources such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008) or FrameNet (Baker et al., 1998) are much richer, b"
D16-1236,D12-1104,1,0.926888,"Missing"
D16-1236,P15-2070,0,0.0728946,"Missing"
D16-1236,D15-1204,0,0.0134569,"phrases as relations, and there is little effort to determine lexical synonymy among them. The first notable effort to build up a resource for relational paraphrases is DIRT (Lin and Pantel, 2001), based on Harris’ Distributional Hypothesis to cluster syntactic patterns. RESOLVER (Yates and Etzioni, 2009) introduced a probabilistic relational model for predicting synonymy. Yao et al. (2012) incorporated latent topic models to resolve the ambiguity of relational phrases. Other probabilistic approaches employed matrix factorization for finding entailments between relations (Riedel et al., 2013; Petroni et al., 2015) or used probabilistic graphical models to find clusters of relations (Grycner et al., 2014). All of these approaches rely on the cooccurrence of the arguments of the relation. Recent endeavors to construct large repositories of relational paraphrases are PATTY, WiseNet and DEFIE. PATTY (Nakashole et al., 2012) devised a sequence mining algorithm to extract relational phrases with semantic type signatures, and organized them into synonymy sets and hypernymy hierarchies. WiseNet (Moro and Navigli, 2012) tapped Wikipedia categories for a similar way of organizing relational paraphrases. DEFIE (B"
D16-1236,N13-1008,0,0.0457861,"s has been on verbal phrases as relations, and there is little effort to determine lexical synonymy among them. The first notable effort to build up a resource for relational paraphrases is DIRT (Lin and Pantel, 2001), based on Harris’ Distributional Hypothesis to cluster syntactic patterns. RESOLVER (Yates and Etzioni, 2009) introduced a probabilistic relational model for predicting synonymy. Yao et al. (2012) incorporated latent topic models to resolve the ambiguity of relational phrases. Other probabilistic approaches employed matrix factorization for finding entailments between relations (Riedel et al., 2013; Petroni et al., 2015) or used probabilistic graphical models to find clusters of relations (Grycner et al., 2014). All of these approaches rely on the cooccurrence of the arguments of the relation. Recent endeavors to construct large repositories of relational paraphrases are PATTY, WiseNet and DEFIE. PATTY (Nakashole et al., 2012) devised a sequence mining algorithm to extract relational phrases with semantic type signatures, and organized them into synonymy sets and hypernymy hierarchies. WiseNet (Moro and Navigli, 2012) tapped Wikipedia categories for a similar way of organizing relationa"
D16-1236,D15-1185,0,0.0153839,"uggest that POLY contains many paraphrases which are available in none of the competing resources. TREC WikiAnswers WebQuestions Basic DEFIE RELLY PATTY POLY DE POLY ALL 193 197 208 213 232 238 144 147 150 155 163 173 365 394 424 475 477 530 All All / POLY 246 218 176 157 562 494 Questions 451 516 1979 Table 7: Number of questions with correct answer. 2190 6 Related Work Knowledge bases (KBs) contribute to many NLP tasks, including Word Sense Disambiguation (Moro et al., 2014), Named Entity Disambiguation (Hoffart et al., 2011), Question Answering (Fader et al., 2014), and Textual Entailment (Sha et al., 2015). Widely used KBs are DBpedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), YAGO (Mahdisoltani et al., 2015), Wikidata (Vrandecic and Kr¨otzsch, 2014) and the Google Knowledge Vault (Dong et al., 2014). KBs have rich information about named entities, but are pretty sparse on relations. In the latter regard, manually created resources such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008) or FrameNet (Baker et al., 1998) are much richer, but still face the limitation of labor-intensive input and human curation. The paradigm of Open Information Extraction (OIE) was develope"
D16-1236,P12-1075,0,0.0211012,"relations. The first large-scale approach along these lines, TextRunner (Banko et al., 2007), was later improved by ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012). The focus of these methods has been on verbal phrases as relations, and there is little effort to determine lexical synonymy among them. The first notable effort to build up a resource for relational paraphrases is DIRT (Lin and Pantel, 2001), based on Harris’ Distributional Hypothesis to cluster syntactic patterns. RESOLVER (Yates and Etzioni, 2009) introduced a probabilistic relational model for predicting synonymy. Yao et al. (2012) incorporated latent topic models to resolve the ambiguity of relational phrases. Other probabilistic approaches employed matrix factorization for finding entailments between relations (Riedel et al., 2013; Petroni et al., 2015) or used probabilistic graphical models to find clusters of relations (Grycner et al., 2014). All of these approaches rely on the cooccurrence of the arguments of the relation. Recent endeavors to construct large repositories of relational paraphrases are PATTY, WiseNet and DEFIE. PATTY (Nakashole et al., 2012) devised a sequence mining algorithm to extract relational p"
D16-1236,C98-1013,0,\N,Missing
D17-2011,D14-1067,0,0.0363968,"cture in the utterance to the semantic predicate-argument structure of the query. Limitations of past work. Prior template-based approaches rely on a set of manually defined rules or templates to handle user questions (Berant et al., 2013; Fader et al., 2013, 2014; Unger et al., 2012; Yahya et al., 2013; Yao and Durme, 2014; Zou et al., 2014). The main drawback of these approaches is the limited coverage of templates, making them brittle when it comes to unconventional question formulations. In contrast, QUINT automatically learns templates from question-answer pairs. Embedding-based methods (Bordes et al., 2014; Dong et al., 2015; Yang et al., 2014; Xu et al., 2016) map questions, KB entities, and subgraphs to a shared space for KB-QA without explicitly generating a semantic representation. This makes it difficult for such systems to generate finegrained explanations to users. Other approaches to KB-QA (Bast and Haussmann, 2015; Yih et al., 2015) over-generate query candidates for a given utterance with no fine-grained alignments to map natural language phrases in a question onto different KB items, making explainability challenging. Contribution. The key contribution of this demo paper is a live on"
D17-2011,P13-1042,0,0.0156995,"users through brief and detailed explanations, respectively. We use the question “Where was Martin Luther raised?” to drive this section. 3.1 Question and Answers Figure 5 shows the main window of QUINT where the starting point for a user is the question box or one of the sample questions provided. An expert user can make several configuration choices (Fig. 5, bottom right): • Model to load: a model includes a set of templates learned offline, and a corresponding learned query ranking function. The choices correspond to the training part of the WebQuestions (Berant et al., 2013) and Free917 (Cai and Yates, 2013) datasets. Answering phase • Whether to add answer type to queries: most KB-QA systems do not capture fine-grained types, while QUINT does so by design. When the trained system receives a new utterance u0 from the user, the dependency parse of u0 is matched against the utterance templates in T . For every match, the paired query template is instantiated using the alignment information together with the underlying lexicons. Thus, a set of candidate queries are obtained which are then ranked using the learning-to-rank framework. Finally, the answer of the top-ranked query is shown to the user. •"
D17-2011,P15-1026,0,0.0663717,"e to the semantic predicate-argument structure of the query. Limitations of past work. Prior template-based approaches rely on a set of manually defined rules or templates to handle user questions (Berant et al., 2013; Fader et al., 2013, 2014; Unger et al., 2012; Yahya et al., 2013; Yao and Durme, 2014; Zou et al., 2014). The main drawback of these approaches is the limited coverage of templates, making them brittle when it comes to unconventional question formulations. In contrast, QUINT automatically learns templates from question-answer pairs. Embedding-based methods (Bordes et al., 2014; Dong et al., 2015; Yang et al., 2014; Xu et al., 2016) map questions, KB entities, and subgraphs to a shared space for KB-QA without explicitly generating a semantic representation. This makes it difficult for such systems to generate finegrained explanations to users. Other approaches to KB-QA (Bast and Haussmann, 2015; Yih et al., 2015) over-generate query candidates for a given utterance with no fine-grained alignments to map natural language phrases in a question onto different KB items, making explainability challenging. Contribution. The key contribution of this demo paper is a live online KB-QA system t"
D17-2011,P13-1158,0,0.0113776,"consists of an utterance template based on a dependency parse pattern and a corresponding query template based on the SPARQL query language. The template (i) specifies how to chunk an utterance into phrases, (ii) guides how these phrases map to KB primitives by specifying their semantic roles as predicates, entities, or types, and (iii) aligns syntactic structure in the utterance to the semantic predicate-argument structure of the query. Limitations of past work. Prior template-based approaches rely on a set of manually defined rules or templates to handle user questions (Berant et al., 2013; Fader et al., 2013, 2014; Unger et al., 2012; Yahya et al., 2013; Yao and Durme, 2014; Zou et al., 2014). The main drawback of these approaches is the limited coverage of templates, making them brittle when it comes to unconventional question formulations. In contrast, QUINT automatically learns templates from question-answer pairs. Embedding-based methods (Bordes et al., 2014; Dong et al., 2015; Yang et al., 2014; Xu et al., 2016) map questions, KB entities, and subgraphs to a shared space for KB-QA without explicitly generating a semantic representation. This makes it difficult for such systems to generate fi"
D17-2011,D11-1072,1,0.74754,"Missing"
D17-2011,Q14-1030,0,0.0148175,"vation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the derivation provides valuable insights towards reformulating the question. 1 Introduction Motivation. A KB-QA system takes a natural language utterance as input and produces one or more crisp answers as output (Bast and Haussmann, 2015; Berant et al., 2013; Reddy et al., 2014; Yih et al., 2015). This is usually done through semantic parsing: translating the utterance to a formal query in a language such as SPARQL, and executing this query over a KB like Freebase (Bollacker et al., 2008) or YAGO (Suchanek et al., 2007) to return one or more answer entities. In addition to answering questions, a KB-QA system should ideally be able to explain how an answer was derived i.e., how the system understood the users’ questions. While rapid progress is being made on the KB-QA task, the quality of answers obtained from KB-QA systems are far from 2 Work done while at the Max P"
D17-2011,P16-1220,0,0.0304494,"tructure of the query. Limitations of past work. Prior template-based approaches rely on a set of manually defined rules or templates to handle user questions (Berant et al., 2013; Fader et al., 2013, 2014; Unger et al., 2012; Yahya et al., 2013; Yao and Durme, 2014; Zou et al., 2014). The main drawback of these approaches is the limited coverage of templates, making them brittle when it comes to unconventional question formulations. In contrast, QUINT automatically learns templates from question-answer pairs. Embedding-based methods (Bordes et al., 2014; Dong et al., 2015; Yang et al., 2014; Xu et al., 2016) map questions, KB entities, and subgraphs to a shared space for KB-QA without explicitly generating a semantic representation. This makes it difficult for such systems to generate finegrained explanations to users. Other approaches to KB-QA (Bast and Haussmann, 2015; Yih et al., 2015) over-generate query candidates for a given utterance with no fine-grained alignments to map natural language phrases in a question onto different KB items, making explainability challenging. Contribution. The key contribution of this demo paper is a live online KB-QA system that visualizes the derivation steps f"
D17-2011,D14-1071,0,0.0346248,"redicate-argument structure of the query. Limitations of past work. Prior template-based approaches rely on a set of manually defined rules or templates to handle user questions (Berant et al., 2013; Fader et al., 2013, 2014; Unger et al., 2012; Yahya et al., 2013; Yao and Durme, 2014; Zou et al., 2014). The main drawback of these approaches is the limited coverage of templates, making them brittle when it comes to unconventional question formulations. In contrast, QUINT automatically learns templates from question-answer pairs. Embedding-based methods (Bordes et al., 2014; Dong et al., 2015; Yang et al., 2014; Xu et al., 2016) map questions, KB entities, and subgraphs to a shared space for KB-QA without explicitly generating a semantic representation. This makes it difficult for such systems to generate finegrained explanations to users. Other approaches to KB-QA (Bast and Haussmann, 2015; Yih et al., 2015) over-generate query candidates for a given utterance with no fine-grained alignments to map natural language phrases in a question onto different KB items, making explainability challenging. Contribution. The key contribution of this demo paper is a live online KB-QA system that visualizes the"
D17-2011,P14-1090,0,0.0582216,"Missing"
D17-2011,P15-1128,0,0.045034,"the natural language utterance to the final answer. The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the derivation provides valuable insights towards reformulating the question. 1 Introduction Motivation. A KB-QA system takes a natural language utterance as input and produces one or more crisp answers as output (Bast and Haussmann, 2015; Berant et al., 2013; Reddy et al., 2014; Yih et al., 2015). This is usually done through semantic parsing: translating the utterance to a formal query in a language such as SPARQL, and executing this query over a KB like Freebase (Bollacker et al., 2008) or YAGO (Suchanek et al., 2007) to return one or more answer entities. In addition to answering questions, a KB-QA system should ideally be able to explain how an answer was derived i.e., how the system understood the users’ questions. While rapid progress is being made on the KB-QA task, the quality of answers obtained from KB-QA systems are far from 2 Work done while at the Max Planck Institute for"
D17-2011,D13-1160,0,0.733178,"zes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the derivation provides valuable insights towards reformulating the question. 1 Introduction Motivation. A KB-QA system takes a natural language utterance as input and produces one or more crisp answers as output (Bast and Haussmann, 2015; Berant et al., 2013; Reddy et al., 2014; Yih et al., 2015). This is usually done through semantic parsing: translating the utterance to a formal query in a language such as SPARQL, and executing this query over a KB like Freebase (Bollacker et al., 2008) or YAGO (Suchanek et al., 2007) to return one or more answer entities. In addition to answering questions, a KB-QA system should ideally be able to explain how an answer was derived i.e., how the system understood the users’ questions. While rapid progress is being made on the KB-QA task, the quality of answers obtained from KB-QA systems are far from 2 Work don"
D18-1003,S17-2082,0,0.0779831,"Missing"
D18-1003,P14-1095,0,0.0480828,"(2016, 2017) employs stylistic lan29 References the deeper semantic aspects of language, however. Wiebe and Riloff (2005); Lin et al. (2011); Recasens et al. (2013) study the problem of detecting bias in language, but do not consider credibility. Truth discovery: Prior approaches for truth discovery (Yin et al., 2008; Dong et al., 2009, 2015; Li et al., 2011, 2014, 2015; Pasternack and Roth, 2011, 2013; Ma et al., 2015; Zhi et al., 2015; Gao et al., 2015; Lyu et al., 2017) have focused on structured data with the goal of addressing the problem of conflict resolution amongst multisource data. Nakashole and Mitchell (2014) proposed a method to extract conflicting values from the Web in the form of Subject-Predicate-Object (SPO) triplets and uses language objectivity analysis to determine the true value. Like the other truth discovery approaches, however, this approach is mainly suitable for use with structured data. Credibility analysis in social media: Mukherjee et al. (2014); Mukherjee and Weikum (2015) propose PGM based approaches to jointly infer a statement’s credibility and the reliability of sources using language specific features. Approaches like (Castillo et al., 2011; Qazvinian et al., 2011; Yang et"
D18-1003,D14-1162,0,0.0851866,"Missing"
D18-1003,I11-1129,0,0.0318027,"reliability and claim correctness. Vydiswaran et al. (2011) proposes an iterative algorithm which jointly learns the veracity of textual claims and trustworthiness of the sources. These approaches do not consider Related Work Our work is closely related to the following areas: Credibility analysis of Web claims: Our work builds upon approaches for performing credibility analysis of natural language claims in an opendomain Web setting. The approach proposed in Popat et al. (2016, 2017) employs stylistic lan29 References the deeper semantic aspects of language, however. Wiebe and Riloff (2005); Lin et al. (2011); Recasens et al. (2013) study the problem of detecting bias in language, but do not consider credibility. Truth discovery: Prior approaches for truth discovery (Yin et al., 2008; Dong et al., 2009, 2015; Li et al., 2011, 2014, 2015; Pasternack and Roth, 2011, 2013; Ma et al., 2015; Zhi et al., 2015; Gao et al., 2015; Lyu et al., 2017) have focused on structured data with the goal of addressing the problem of conflict resolution amongst multisource data. Nakashole and Mitchell (2014) proposed a method to extract conflicting values from the Web in the form of Subject-Predicate-Object (SPO) trip"
D18-1003,D11-1147,0,0.601442,"Missing"
D18-1003,D17-1317,0,0.108431,"p Popat1 , Subhabrata Mukherjee2 , Andrew Yates1 , Gerhard Weikum1 1 Max Planck Institute for Informatics, Saarbr¨ucken, Germany 2 Amazon Inc., Seattle, USA {kpopat,ayates,weikum}@mpi-inf.mpg.de, subhomj@amazon.com Abstract State of the Art and Limitations: Prior work on “truth discovery” (see Li et al. (2016) for survey)1 largely focused on structured facts, typically in the form of subject-predicate-object triples, or on social media platforms like Twitter, Sina Weibo, etc. Recently, methods have been proposed to assess the credibility of claims in natural language form (Popat et al., 2017; Rashkin et al., 2017; Wang, 2017), such as news headlines, quotes from speeches, blog posts, etc. The methods geared for general text input address the problem in different ways. On the one hand, methods like Rashkin et al. (2017); Wang (2017) train neural networks on labeled claims from sites like PolitiFact.com, providing credibility assessments without any explicit feature modeling. However, they use only the text of questionable claims and no external evidence or interactions that provide limited context for credibility analysis. These approaches also do not offer any explanation of their verdicts. On the oth"
D18-1003,P13-1162,0,0.0364468,"im correctness. Vydiswaran et al. (2011) proposes an iterative algorithm which jointly learns the veracity of textual claims and trustworthiness of the sources. These approaches do not consider Related Work Our work is closely related to the following areas: Credibility analysis of Web claims: Our work builds upon approaches for performing credibility analysis of natural language claims in an opendomain Web setting. The approach proposed in Popat et al. (2016, 2017) employs stylistic lan29 References the deeper semantic aspects of language, however. Wiebe and Riloff (2005); Lin et al. (2011); Recasens et al. (2013) study the problem of detecting bias in language, but do not consider credibility. Truth discovery: Prior approaches for truth discovery (Yin et al., 2008; Dong et al., 2009, 2015; Li et al., 2011, 2014, 2015; Pasternack and Roth, 2011, 2013; Ma et al., 2015; Zhi et al., 2015; Gao et al., 2015; Lyu et al., 2017) have focused on structured data with the goal of addressing the problem of conflict resolution amongst multisource data. Nakashole and Mitchell (2014) proposed a method to extract conflicting values from the Web in the form of Subject-Predicate-Object (SPO) triplets and uses language o"
D18-1003,S17-2087,0,0.0303723,"Missing"
D18-1003,P17-2102,0,0.204224,"Missing"
D18-1003,P17-2067,0,0.19883,"Mukherjee2 , Andrew Yates1 , Gerhard Weikum1 1 Max Planck Institute for Informatics, Saarbr¨ucken, Germany 2 Amazon Inc., Seattle, USA {kpopat,ayates,weikum}@mpi-inf.mpg.de, subhomj@amazon.com Abstract State of the Art and Limitations: Prior work on “truth discovery” (see Li et al. (2016) for survey)1 largely focused on structured facts, typically in the form of subject-predicate-object triples, or on social media platforms like Twitter, Sina Weibo, etc. Recently, methods have been proposed to assess the credibility of claims in natural language form (Popat et al., 2017; Rashkin et al., 2017; Wang, 2017), such as news headlines, quotes from speeches, blog posts, etc. The methods geared for general text input address the problem in different ways. On the one hand, methods like Rashkin et al. (2017); Wang (2017) train neural networks on labeled claims from sites like PolitiFact.com, providing credibility assessments without any explicit feature modeling. However, they use only the text of questionable claims and no external evidence or interactions that provide limited context for credibility analysis. These approaches also do not offer any explanation of their verdicts. On the other hand, Popa"
D18-1003,C12-2131,0,0.0537526,"Missing"
D18-1129,W04-3252,0,0.280511,"xpressing a single proposition (Del Corro and Gemulla, 2013). This helps to avoid working with sentences that might express more than one proposition or arbitrary chunking the input text. Compression is also more principled at a fact level as the fact hierarchical structure is clearly defined (Gashteovski et al., 2017). Additionally, we exploit the fact structure to promote diversity. Several supervised and unsupervised methods have been used in text summarization to determine the relative prominence of the atomic units. For instance, two of the top performer systems Durrett et al. (2016) and Mihalcea and Tarau (2004), which we include in our extrinsic evaluation, are based on ILP and an unsupervised graph algorithm respectively. Other approaches include LDA (Pouriyeh et al., 2017), ontology-based (Baralis et al., 2013) or clustering (Yang et al., 2014), and more recently neural-based methods (See et al., 2017). As in Mihalcea and Tarau (2004) or Erkan and Radev (2004) we use PageRank to establish the relative prominence of the atomic units (Sec. 4.1). However, we weight the graph edges using word vectors to allow more expressive semantics, avoiding the sparsity of frequency-based methods. Different approa"
D18-1129,P09-1113,0,0.0149287,"achine-understandable format. Machine-readable information is usually structured in the form of facts, in which a given relation links a set of arguments [e.g., (“US”, “withdraws from”, “Iran nuclear deal”)]. Facts are at the core of several natural language understanding applications such as knowledge-base (KB) construction (Nguyen et al., 2017), question answering (Abujabal et al., 2018), structured search (Bast et al., 2014), or entity-linking (Cheng and Roth, 2013). Different approaches aim to discover facts from natural language text. In the extremes of the spectrum, relation extraction (Mintz et al., 2009) looks for all facts linkable to a KB, whereas open information extraction (Banko et al., 2007) extracts facts over an unconstrained set of arguments and relations. In this paper, we aim to additionally score facts according to their prominence. Fact salience is closely related to automatic text summarization (Erkan and Radev, 2004) as both try to capture the essential information in a document. However, fact salience output is required to be interpreted by machines to a certain extent. Text summarization, on the contrary, is meant to be understood by humans alone; it is often composed by ungr"
D18-1129,D14-1162,0,0.0818994,"ing. We want related facts to get a higher weight assuming that the most relevant facts will be those more central. We weight each edge (u, v) with the semantic similarity between u and v as the cosine between the centroid of the word embeddings in the facts. Stanovsky et al. (2015) have shown that learning word embeddings with open facts allows the generation of higher quality vectors . The assumption is that the relatedness of words within a fact is stronger than with words outside. This provides the basis for more accurate contextualization. Accordingly, in our implementation we use GloVe (Pennington et al., 2014) trained on the Wikipedia corpus using open facts extracted by M IN IE for co-occurence context. Step 3 – Relevance Prior. We introduce a prior for each fact by computing a score used to instantiate the PageRank’s teleport vector. The assumption is that authors tend to express the most relevant facts at the beginning. We instantiated xi each fact teleport as f actP rior(i) = X , where xi = |V |− i and i is the fact index. This is important especially for news where the lead paragraph is the most important part of the article. That’s why the positional baseline is so strong in tasks as text sum"
D18-1129,I17-1032,0,0.0225902,"g the input text. Compression is also more principled at a fact level as the fact hierarchical structure is clearly defined (Gashteovski et al., 2017). Additionally, we exploit the fact structure to promote diversity. Several supervised and unsupervised methods have been used in text summarization to determine the relative prominence of the atomic units. For instance, two of the top performer systems Durrett et al. (2016) and Mihalcea and Tarau (2004), which we include in our extrinsic evaluation, are based on ILP and an unsupervised graph algorithm respectively. Other approaches include LDA (Pouriyeh et al., 2017), ontology-based (Baralis et al., 2013) or clustering (Yang et al., 2014), and more recently neural-based methods (See et al., 2017). As in Mihalcea and Tarau (2004) or Erkan and Radev (2004) we use PageRank to establish the relative prominence of the atomic units (Sec. 4.1). However, we weight the graph edges using word vectors to allow more expressive semantics, avoiding the sparsity of frequency-based methods. Different approaches have also been explored to promote diversity. Xiong and Luo (2014), for example, use LSA, and Chien and Chang (2013) rely on topic models. In our case, we generat"
D18-1129,D13-1184,0,0.034871,"compress information. 1 Introduction Automatic knowledge acquisition at large scale requires the transformation of human-readable knowledge into a machine-understandable format. Machine-readable information is usually structured in the form of facts, in which a given relation links a set of arguments [e.g., (“US”, “withdraws from”, “Iran nuclear deal”)]. Facts are at the core of several natural language understanding applications such as knowledge-base (KB) construction (Nguyen et al., 2017), question answering (Abujabal et al., 2018), structured search (Bast et al., 2014), or entity-linking (Cheng and Roth, 2013). Different approaches aim to discover facts from natural language text. In the extremes of the spectrum, relation extraction (Mintz et al., 2009) looks for all facts linkable to a KB, whereas open information extraction (Banko et al., 2007) extracts facts over an unconstrained set of arguments and relations. In this paper, we aim to additionally score facts according to their prominence. Fact salience is closely related to automatic text summarization (Erkan and Radev, 2004) as both try to capture the essential information in a document. However, fact salience output is required to be interpr"
D18-1129,N13-1136,0,0.0317215,"resentation from text summarization output is difficult. This output can be incomplete or ungrammatical, given the use of compression techniques (Zajic et al., 2007), or the inclusion of keywords or short unconnected phrases with topical information (Hasan and Ng, 2014). Open information 1 https://github.com/mponza/SalIE extractors will most likely fail to generate meaningful facts in these circumstances. However, text summarization techniques to score the atomic units can be exploited for fact salience. Open facts have been already used in text summarization for redundancy, using synonymity (Christensen et al., 2013) or as input for a classifier (Christensen et al., 2014). In this case, we use facts as atomic units. Working at the fact level provides a natural framework to detect essential information in a text document, since facts are minimal comprehensive atomic units expressing a single proposition (Del Corro and Gemulla, 2013). This helps to avoid working with sentences that might express more than one proposition or arbitrary chunking the input text. Compression is also more principled at a fact level as the fact hierarchical structure is clearly defined (Gashteovski et al., 2017). Additionally, we"
D18-1129,P17-1099,0,0.0515993,"et al., 2017). Additionally, we exploit the fact structure to promote diversity. Several supervised and unsupervised methods have been used in text summarization to determine the relative prominence of the atomic units. For instance, two of the top performer systems Durrett et al. (2016) and Mihalcea and Tarau (2004), which we include in our extrinsic evaluation, are based on ILP and an unsupervised graph algorithm respectively. Other approaches include LDA (Pouriyeh et al., 2017), ontology-based (Baralis et al., 2013) or clustering (Yang et al., 2014), and more recently neural-based methods (See et al., 2017). As in Mihalcea and Tarau (2004) or Erkan and Radev (2004) we use PageRank to establish the relative prominence of the atomic units (Sec. 4.1). However, we weight the graph edges using word vectors to allow more expressive semantics, avoiding the sparsity of frequency-based methods. Different approaches have also been explored to promote diversity. Xiong and Luo (2014), for example, use LSA, and Chien and Chang (2013) rely on topic models. In our case, we generate diversity by exploiting the fact structure (Sec. 4.2). We cluster facts in terms of their subjects as a way to have the most relev"
D18-1129,P14-1085,0,0.0239169,"This output can be incomplete or ungrammatical, given the use of compression techniques (Zajic et al., 2007), or the inclusion of keywords or short unconnected phrases with topical information (Hasan and Ng, 2014). Open information 1 https://github.com/mponza/SalIE extractors will most likely fail to generate meaningful facts in these circumstances. However, text summarization techniques to score the atomic units can be exploited for fact salience. Open facts have been already used in text summarization for redundancy, using synonymity (Christensen et al., 2013) or as input for a classifier (Christensen et al., 2014). In this case, we use facts as atomic units. Working at the fact level provides a natural framework to detect essential information in a text document, since facts are minimal comprehensive atomic units expressing a single proposition (Del Corro and Gemulla, 2013). This helps to avoid working with sentences that might express more than one proposition or arbitrary chunking the input text. Compression is also more principled at a fact level as the fact hierarchical structure is clearly defined (Gashteovski et al., 2017). Additionally, we exploit the fact structure to promote diversity. Several"
D18-1129,P15-2050,0,0.28444,"Missing"
D18-1129,P16-1188,0,0.361615,"mprehensive atomic units expressing a single proposition (Del Corro and Gemulla, 2013). This helps to avoid working with sentences that might express more than one proposition or arbitrary chunking the input text. Compression is also more principled at a fact level as the fact hierarchical structure is clearly defined (Gashteovski et al., 2017). Additionally, we exploit the fact structure to promote diversity. Several supervised and unsupervised methods have been used in text summarization to determine the relative prominence of the atomic units. For instance, two of the top performer systems Durrett et al. (2016) and Mihalcea and Tarau (2004), which we include in our extrinsic evaluation, are based on ILP and an unsupervised graph algorithm respectively. Other approaches include LDA (Pouriyeh et al., 2017), ontology-based (Baralis et al., 2013) or clustering (Yang et al., 2014), and more recently neural-based methods (See et al., 2017). As in Mihalcea and Tarau (2004) or Erkan and Radev (2004) we use PageRank to establish the relative prominence of the atomic units (Sec. 4.1). However, we weight the graph edges using word vectors to allow more expressive semantics, avoiding the sparsity of frequency-b"
D18-1129,D17-1278,1,0.873514,"using synonymity (Christensen et al., 2013) or as input for a classifier (Christensen et al., 2014). In this case, we use facts as atomic units. Working at the fact level provides a natural framework to detect essential information in a text document, since facts are minimal comprehensive atomic units expressing a single proposition (Del Corro and Gemulla, 2013). This helps to avoid working with sentences that might express more than one proposition or arbitrary chunking the input text. Compression is also more principled at a fact level as the fact hierarchical structure is clearly defined (Gashteovski et al., 2017). Additionally, we exploit the fact structure to promote diversity. Several supervised and unsupervised methods have been used in text summarization to determine the relative prominence of the atomic units. For instance, two of the top performer systems Durrett et al. (2016) and Mihalcea and Tarau (2004), which we include in our extrinsic evaluation, are based on ILP and an unsupervised graph algorithm respectively. Other approaches include LDA (Pouriyeh et al., 2017), ontology-based (Baralis et al., 2013) or clustering (Yang et al., 2014), and more recently neural-based methods (See et al., 2"
D18-1129,P14-1119,0,0.0320206,"tion system splits the text into atomic units (usually sentences) that are scored and ranked (Allahyari et al., 2017). Diversity is generally guaranteed by clustering them in topics and selecting the most representative members from each cluster. Once selected, the atomic units are compressed to ensure minimality. Generating a machine-readable representation from text summarization output is difficult. This output can be incomplete or ungrammatical, given the use of compression techniques (Zajic et al., 2007), or the inclusion of keywords or short unconnected phrases with topical information (Hasan and Ng, 2014). Open information 1 https://github.com/mponza/SalIE extractors will most likely fail to generate meaningful facts in these circumstances. However, text summarization techniques to score the atomic units can be exploited for fact salience. Open facts have been already used in text summarization for redundancy, using synonymity (Christensen et al., 2013) or as input for a classifier (Christensen et al., 2014). In this case, we use facts as atomic units. Working at the fact level provides a natural framework to detect essential information in a text document, since facts are minimal comprehensiv"
D19-1583,N18-3010,0,0.0417637,"Missing"
D19-1583,P09-1113,0,0.0113242,"Missing"
D19-1583,D12-1104,1,0.788461,"Missing"
D19-1583,W16-1307,0,0.0605909,"Missing"
D19-1583,N13-1008,0,0.01929,"Missing"
D19-1583,N18-1081,0,0.0225275,"Missing"
D19-1583,P15-1150,0,0.0604148,"Missing"
D19-1583,D15-1059,0,0.0134656,"ineup lineup hpnamei consists hpnamei hnumi tour vocals hpropnamei educated hpnamei briefly attended attended hpnamei left graduating hpnamei left famous subjects frequently have obscure objects, e.g., none of Bill Gates’ children has a Wikipedia page. NED tools consequently often failed to correctly resolve related mentions. In the present work we thus opted for lexical matching, trading a higher recall against a lower precision. • Time-variance. While some KB relations are quite stable (e.g., children), others are more volatile, and may both grow or shrink over time (e.g., band membership) (Wijaya et al., 2015). Such dynamicity adds complexity to the recall assessment, as recall may then be specific to certain time points. Table 3: Selected important paragraph-level bigrams indicating completeness for SVMs. Sentence LSTM score He was the father of actor Pierre Renoir (1885-1952), filmmaker Jean Renoir (1894-1979) and ceramic artist Claude Renoir (1901-1969). 0.54 His daughter Julie Gavras and his son Romain Gavras are also filmmakers. 0.46 Genghis Khan was aware of the friction between his sons (particularly between Chagatai and Jochi) and worried of possible conflict between them if he died. 0.42 “"
D19-1583,D15-1237,0,0.0710501,"Missing"
D19-1675,W11-1701,0,0.115147,"ws reports, rumors, etc. People express their perspectives about these controversial claims through various channels like editorials, blog posts, social media, and discussion forums. To achieve a deeper understanding of these claims, we need to understand users’ perspectives and stance towards the claims. Recent research (FNC-1, 2016; Baly et al., 2018; Chen et al., 2019) has shown stance classification to be a critical step for information credibility and automated fact-checking. Prior Work and Limitations: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) hav"
D19-1675,D16-1084,0,0.0818496,"Missing"
D19-1675,N18-2004,0,0.0478736,"Missing"
D19-1675,E17-1024,0,0.0505466,"); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) propose classifiers based on hand-crafted lexicons to identify important phrases in perspectives and their consistency with the claim to predict the stance However, their model critically relies on manual lexicons and assumes that the important phrases in claims are already identified. Neural-network-based approaches for stance classification learn the claim and perspective representations separately and later combine them with conditional LSTM encoding (Augenstein et al., 2016), attention mechanisms (Du et al., 2017) or memory networks (Mohtarami et al., 2018). Some neural network models als"
D19-1675,P17-1152,0,0.0402947,".com, debatewise.org, and procon.org. Each claim has different perspectives along with the stance (supporting or opposing the claim). We 1 BERT implementation: https://git.io/fhbJQ Baselines LSTM: A long short-term memory (LSTM) model, in which we pass the claim and perspective word representations (using GloVE-6B word embeddings of size 300) through a bidirectional LSTM. Then we concatenate the final hidden states of the claim and perspective, and pass it through dense layers with ReLU activations. ESIM: An enhanced sequential inference model (ESIM) for natural language inference proposed in Chen et al. (2017). MLP: Multi-layer perceptron (MLP) based model using lexical and similarity-based features – presented as a simple but tough-to-beat baseline for stance detection in Riedel et al. (2017). WordAttn: Our implementation of word-by-word attention-based model using long short-term memory networks (Rockt¨aschel et al., 2016). LangFeat: A random forest classifier using linguistic lexicons like NRC lexicon (Mohammad and Turney, 2010), hedges (e.g., possibly, might, etc.), positive/negative sentiment words (Hu and Liu, 2004), MPQA subjective lexicon (Wilson et al., 2005) and bias lexicon (Recasens et"
D19-1675,N19-1053,0,0.0612607,"Missing"
D19-1675,C16-1154,0,0.217157,"ion proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) propose classifiers based on hand-crafted lexicons to identify important phrases in perspectives and their consistency with the claim to predict the stance However, their model critically relies on manual lexicons and assumes that the important phrases in claims are already identified. Neural-network-based approaches for stance classification learn the claim and perspective representations separately and later combine them with conditional LSTM encoding (Augenstein et al., 20"
D19-1675,N19-1423,0,0.105818,"tics Claim Tokens Claim Tokens [CLS] Dense Perspective Layer Rep. Perspective Tokens [SEP] [SEP] [CLS] Stance [SEP] BERT Dense Perspective Layer Rep. Perspective Tokens Stance [SEP] BERT Claim Tokens [CLS] [SEP] Cosine Similarity Consistency Score Claim Rep. (a) BERTBASE : Fine-tuning BERT for stance classification. (b) BERTCONS : Enhancing BERT using the joint loss (lossce for stance classification and losscos for consistency). Figure 1: BERT-based methods for determining the stance of the perspective with respect to the claim. Encoder Representations from Transformers) neural network model (Devlin et al., 2019). BERT is trained on huge text corpora and serves as background knowledge. We fine-tune BERT for our task which also allows us to jointly model claims and perspectives. Furthermore, we enhance our model by augmenting it with a novel consistency constraint to capture agreement between the claim and perspective. Key contributions of this paper are: classification token ([CLS]) and a separator token ([SEP]): [CLS] Ctoks [SEP] Ptoks [SEP]. The input sequences are tokenized using WordPiece tokenization. The final hidden state representation corresponding to the [CLS] token is used as X P |C ∈ RH ."
D19-1675,N16-1138,0,0.0205363,"ards the claims. Recent research (FNC-1, 2016; Baly et al., 2018; Chen et al., 2019) has shown stance classification to be a critical step for information credibility and automated fact-checking. Prior Work and Limitations: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) propose classifiers based on hand-crafted lexicons to identify important phrases in perspectives and their consistency with the claim to predict the stance However, their model critically relies on manual lexicons and assumes t"
D19-1675,C18-1158,0,0.229698,"o identify important phrases in perspectives and their consistency with the claim to predict the stance However, their model critically relies on manual lexicons and assumes that the important phrases in claims are already identified. Neural-network-based approaches for stance classification learn the claim and perspective representations separately and later combine them with conditional LSTM encoding (Augenstein et al., 2016), attention mechanisms (Du et al., 2017) or memory networks (Mohtarami et al., 2018). Some neural network models also incorporate lexical features (Riedel et al., 2017; Hanselowski et al., 2018). None of these approaches leverage knowledge acquired from massive external corpora. Approach and Contributions: To overcome the limitations of prior works, we present STANCY, a neural network model for stance classification. Given an input pair of a claim and a user’s perspective, our model predicts whether the perspective is supporting or opposing the claim. For example, the claim “You have nothing to worry about surveillance, if you have done nothing wrong” is supported by the user perspective “Information gathered through surveillance could be used to fight terrorism” and opposed by anoth"
D19-1675,I13-1191,0,0.1472,"ir perspectives about these controversial claims through various channels like editorials, blog posts, social media, and discussion forums. To achieve a deeper understanding of these claims, we need to understand users’ perspectives and stance towards the claims. Recent research (FNC-1, 2016; Baly et al., 2018; Chen et al., 2019) has shown stance classification to be a critical step for information credibility and automated fact-checking. Prior Work and Limitations: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tw"
D19-1675,D14-1083,0,0.0456021,"Missing"
D19-1675,S17-2083,0,0.069629,"ions: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) propose classifiers based on hand-crafted lexicons to identify important phrases in perspectives and their consistency with the claim to predict the stance However, their model critically relies on manual lexicons and assumes that the important phrases in claims are already identified. Neural-network-based approaches for stance classification learn the claim and perspective representations separately and later combine them with conditio"
D19-1675,P16-2064,0,0.0132592,"asundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) propose classifiers based on hand-crafted lexicons to identify important phrases in perspectives and their consistency with the claim to predict the stance However, their model critically relies on manual lexicons and assumes that the important phrases in claims are already identified. Neural-network-based approaches for stance classification learn the claim and perspective representations separately and later combine them with conditional LSTM encoding (Augenstein et al., 2016), attention mechani"
D19-1675,S16-1003,0,0.30897,"Missing"
D19-1675,W10-0204,0,0.0417386,"the claim and perspective, and pass it through dense layers with ReLU activations. ESIM: An enhanced sequential inference model (ESIM) for natural language inference proposed in Chen et al. (2017). MLP: Multi-layer perceptron (MLP) based model using lexical and similarity-based features – presented as a simple but tough-to-beat baseline for stance detection in Riedel et al. (2017). WordAttn: Our implementation of word-by-word attention-based model using long short-term memory networks (Rockt¨aschel et al., 2016). LangFeat: A random forest classifier using linguistic lexicons like NRC lexicon (Mohammad and Turney, 2010), hedges (e.g., possibly, might, etc.), positive/negative sentiment words (Hu and Liu, 2004), MPQA subjective lexicon (Wilson et al., 2005) and bias lexicon (Recasens et al., 2013) along with sentiment scores as features. BERTBASE : Approach proposed in Chen et al. (2019) (as described in Section 2.1). Human: Human performance on this task as reported in Chen et al. (2019). 4 Results and Discussion Stance classification performance of our model and the baselines on the test split of the Perspectrum dataset are presented in Table 2. Our consistency-aware model BERTCONS outperforms all the other"
D19-1675,N18-1070,0,0.155215,"Missing"
D19-1675,P13-1162,0,0.0174765,"t al. (2017). MLP: Multi-layer perceptron (MLP) based model using lexical and similarity-based features – presented as a simple but tough-to-beat baseline for stance detection in Riedel et al. (2017). WordAttn: Our implementation of word-by-word attention-based model using long short-term memory networks (Rockt¨aschel et al., 2016). LangFeat: A random forest classifier using linguistic lexicons like NRC lexicon (Mohammad and Turney, 2010), hedges (e.g., possibly, might, etc.), positive/negative sentiment words (Hu and Liu, 2004), MPQA subjective lexicon (Wilson et al., 2005) and bias lexicon (Recasens et al., 2013) along with sentiment scores as features. BERTBASE : Approach proposed in Chen et al. (2019) (as described in Section 2.1). Human: Human performance on this task as reported in Chen et al. (2019). 4 Results and Discussion Stance classification performance of our model and the baselines on the test split of the Perspectrum dataset are presented in Table 2. Our consistency-aware model BERTCONS outperforms all the other baselines. It achieves a performance improvement of about 2 points in F1-score over the strong baseline corresponding to the BERTBASE model (p-value of 4.985e−4 as per the McNemar"
D19-1675,E17-2088,0,0.0985046,"010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) propose classifiers based on hand-crafted lexicons to identify important phrases in perspectives and their consistency with the claim to predict the stance However, their model critically relies on manual lexicons and assumes that the important phrases in claims are already identified. Neural-network-based approaches for stance classification learn the claim and perspective representations separately and later combine them with conditional LSTM encoding (Augenstein et al., 2016), attention mechanisms (Du et al., 2017) o"
D19-1675,W10-0214,0,0.0436913,"nts from politicians, biased news reports, rumors, etc. People express their perspectives about these controversial claims through various channels like editorials, blog posts, social media, and discussion forums. To achieve a deeper understanding of these claims, we need to understand users’ perspectives and stance towards the claims. Recent research (FNC-1, 2016; Baly et al., 2018; Chen et al., 2019) has shown stance classification to be a critical step for information credibility and automated fact-checking. Prior Work and Limitations: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobh"
D19-1675,P15-1012,0,0.236663,"controversial claims through various channels like editorials, blog posts, social media, and discussion forums. To achieve a deeper understanding of these claims, we need to understand users’ perspectives and stance towards the claims. Recent research (FNC-1, 2016; Baly et al., 2018; Chen et al., 2019) has shown stance classification to be a critical step for information credibility and automated fact-checking. Prior Work and Limitations: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) p"
D19-1675,C18-1203,0,0.51741,"hrough various channels like editorials, blog posts, social media, and discussion forums. To achieve a deeper understanding of these claims, we need to understand users’ perspectives and stance towards the claims. Recent research (FNC-1, 2016; Baly et al., 2018; Chen et al., 2019) has shown stance classification to be a critical step for information credibility and automated fact-checking. Prior Work and Limitations: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determining stance only in Tweets. Bar-Haim et al. (2017) propose classifiers"
D19-1675,N12-1072,0,0.0571404,"tc. People express their perspectives about these controversial claims through various channels like editorials, blog posts, social media, and discussion forums. To achieve a deeper understanding of these claims, we need to understand users’ perspectives and stance towards the claims. Recent research (FNC-1, 2016; Baly et al., 2018; Chen et al., 2019) has shown stance classification to be a critical step for information credibility and automated fact-checking. Prior Work and Limitations: Prior approaches for stance classification proposed in Somasundaran and Wiebe (2010); Anand et al. (2011); Walker et al. (2012); Hasan and Ng (2013, 2014); Sridhar et al. (2015); Sun et al. (2018) rely on various linguistic features, e.g., n-grams, dependency parse tree, opinion lexicons, and sentiment to determine the stance of perspectives regarding controversial topics. Ferreira and Vlachos (2016) further incorporate natural language claims and propose a logistic regression model using the lexical and semantic features of claims and perspectives. SemEval tasks (Mohammad et al., 2016; Kochkina et al., 2017) and other approaches (Chen and Ku, 2016; Lukasik et al., 2016; Sobhani et al., 2017) have focused on determini"
D19-1675,H05-1044,0,0.0269025,"l language inference proposed in Chen et al. (2017). MLP: Multi-layer perceptron (MLP) based model using lexical and similarity-based features – presented as a simple but tough-to-beat baseline for stance detection in Riedel et al. (2017). WordAttn: Our implementation of word-by-word attention-based model using long short-term memory networks (Rockt¨aschel et al., 2016). LangFeat: A random forest classifier using linguistic lexicons like NRC lexicon (Mohammad and Turney, 2010), hedges (e.g., possibly, might, etc.), positive/negative sentiment words (Hu and Liu, 2004), MPQA subjective lexicon (Wilson et al., 2005) and bias lexicon (Recasens et al., 2013) along with sentiment scores as features. BERTBASE : Approach proposed in Chen et al. (2019) (as described in Section 2.1). Human: Human performance on this task as reported in Chen et al. (2019). 4 Results and Discussion Stance classification performance of our model and the baselines on the test split of the Perspectrum dataset are presented in Table 2. Our consistency-aware model BERTCONS outperforms all the other baselines. It achieves a performance improvement of about 2 points in F1-score over the strong baseline corresponding to the BERTBASE mode"
de-melo-weikum-2008-mapping,C92-2082,0,\N,Missing
de-melo-weikum-2008-mapping,H94-1025,0,\N,Missing
de-melo-weikum-2010-providing,W09-4407,1,\N,Missing
de-melo-weikum-2010-providing,buscaldi-rosso-2008-geo,0,\N,Missing
de-melo-weikum-2010-providing,P06-2037,0,\N,Missing
de-melo-weikum-2010-providing,bond-etal-2008-boot,0,\N,Missing
I17-2038,C02-1150,0,0.223759,"and puma footwear founded?” Table 4: Questions that could be answered only when TIPI was used as a plug-in. et al., 2016). Answer typing was mostly limited to considering coarse-grained types (Bast and Haussmann, 2015; Lally et al., 2012) and lexical answer types (Berant and Liang, 2015; Abujabal et al., 2017). Both such modes fail when the answer type is not explicit. More recently, Yavuz et al. (2016) exploit more implicit type cues for KBQA: but their method of creating training data is context-agnostic, which we remedy in our work. An early line of work deals with question classification (Li and Roth, 2002; Blunsom et al., 2006; Huang et al., 2008), but they were designed for a handful of TREC types and is not really relevant for KB-QA with thousands of distinct classes. Finally, this work is the first to harness answer types for compositional KB-QA. 6 Conclusion We presented TIPI, a mechanism for enabling KBQA systems to answer compositional questions using answer type prediction. TIPI relies on a finegrained answer typing module, that respects question context and type hierarchy. Experiments on a recent benchmark show that TIPI achieves stateof-the-art performance under single-query execution"
I17-2038,W12-3016,0,0.0166267,"le sub-questions “Who won a Nobel Prize in Physics?” and “Who was born in Bavaria?”, using dependency parse patterns (Xu et al., 2016; Bao et al., 2014). These patterns can handle several kinds of compositional questions with multiple entities or relations (e.g., questions with relative clauses and coordinating conjunctions). Question decomposition has also been applied to the IBM Watson system (Boguraev et al., 2014), 223 to a set of semantic types (KB-types) using lexicons (Berant et al., 2013). Such lexicons can be constructed by mining entity-annotated relational phrases in C LUE W EB 09 (Lin et al., 2012). For example, the lexical type ‘physicist’ may map to the semantic types Scientist, Theoretical physicist and Quantum physicist. Type Selection. From the candidate collection phase, we receive a set of candidate types, for each of which we run a binary classifier to get a confidence score. If no lexical answer type could be found in the question (like “Where did Einstein study?”) or there were no lexicon entries, the classifier makes predictions on all types from our type system. The hierarchical classifier works as follows: starting at the root of the type system we predict probabilities for"
I17-2038,C16-1236,0,0.510362,"m}@mpi-inf.mpg.de Abstract that are prevalent in popular KB-QA benchmarks are generally out of scope for most state-of-theart KB-QA systems (Yih et al., 2015; Dong et al., 2015; Berant and Liang, 2015). However, questions like “Who won a Nobel Prize in Physics and was born in Bavaria?”, can be decomposed into a set of simpler questions “Who won a Nobel Prize in Physics?” and “Who was born in Bavaria?”. We refer to such questions as compositional questions, and these are the focus of this work. Limitations of state-of-the-art. A few past approaches that can handle such compositional questions (Bao et al., 2016; Xu et al., 2016; Abujabal et al., 2017) generate and execute candidate SPARQL queries for each sub-question separately and/or use the intersection as the final answer (Werner Heisenberg, among others). This creates the challenge of deciding which queries from the different sub-questions fit together. Past efforts use information about answers to all generated SPARQL queries, and retrospectively choose a query pair from among these whose answer intersection is non-empty. However, this mode of operation is highly inefficient, since it necessitates execution of all generated queries, followed b"
I17-2038,P14-5010,0,0.00264933,"ty typing (Del Corro et al., 2015; Yosef et al., 2012)1 : (1) candidate collection, using lexicosyntactic patterns to identify lexical types which can be mapped to a set of semantic types using lexicons; (2) type selection, using a hierarchical classifier to disambiguate among the candidates. Note that in absence of explicit clues for candidates in the question, our method proceeds directly to the second stage. Candidate Collection. To extract the lexical answer type from a question, we use simple POS patterns (examples in Table 1), utilizing Stanford CoreNLP for tokenization and POS-tagging (Manning et al., 2014). In a second step, the lexical type extracted from the question is mapped 4 4.1 Experiments Setup Dataset. We use the very recent dataset of 150 compositional questions2 created by Abujabal et al. (2017) which were sampled from the 2013 2 Available for download at http://people. mpi-inf.mpg.de/˜abujabal/publications/ quint/complex-questions-wikiasnwers-150. json, Accessed 22 September 2017. 1 While the task in named entity typing is somewhat similar to answer typing, the crucial difference is that in the latter the entity (answer) itself is missing. 224 WikiAnswers resource (Fader et al., 201"
I17-2038,P14-1091,0,0.0195452,"hierarchy- and context-aware fine-grained answer typing module that can be used as a plug-in by any KB-QA system. 2 Answering Compositional Questions We now explain the steps by which TIPI handles the answering of a compositional question, taking the running example of “Who won a Nobel Prize in Physics and was born in Bavaria?”. A simplified workflow is shown in Figure 1. Question decomposition. Given a compositional question, TIPI first decomposes it into simple sub-questions “Who won a Nobel Prize in Physics?” and “Who was born in Bavaria?”, using dependency parse patterns (Xu et al., 2016; Bao et al., 2014). These patterns can handle several kinds of compositional questions with multiple entities or relations (e.g., questions with relative clauses and coordinating conjunctions). Question decomposition has also been applied to the IBM Watson system (Boguraev et al., 2014), 223 to a set of semantic types (KB-types) using lexicons (Berant et al., 2013). Such lexicons can be constructed by mining entity-annotated relational phrases in C LUE W EB 09 (Lin et al., 2012). For example, the lexical type ‘physicist’ may map to the semantic types Scientist, Theoretical physicist and Quantum physicist. Type"
I17-2038,D13-1160,0,0.854941,"rkflow is shown in Figure 1. Question decomposition. Given a compositional question, TIPI first decomposes it into simple sub-questions “Who won a Nobel Prize in Physics?” and “Who was born in Bavaria?”, using dependency parse patterns (Xu et al., 2016; Bao et al., 2014). These patterns can handle several kinds of compositional questions with multiple entities or relations (e.g., questions with relative clauses and coordinating conjunctions). Question decomposition has also been applied to the IBM Watson system (Boguraev et al., 2014), 223 to a set of semantic types (KB-types) using lexicons (Berant et al., 2013). Such lexicons can be constructed by mining entity-annotated relational phrases in C LUE W EB 09 (Lin et al., 2012). For example, the lexical type ‘physicist’ may map to the semantic types Scientist, Theoretical physicist and Quantum physicist. Type Selection. From the candidate collection phase, we receive a set of candidate types, for each of which we run a binary classifier to get a confidence score. If no lexical answer type could be found in the question (like “Where did Einstein study?”) or there were no lexicon entries, the classifier makes predictions on all types from our type system"
I17-2038,Q15-1039,0,0.0762418,"mance. Finally, representative questions that could only be answered when TIPI was used as plug-in, are shown in Table 4. 5 “who is the president of the us who played in bedtime for bonzo?” “who played for ac milan and inter milan?” “what movie did russell crowe and denzel washington work on together?” “which country were the adidas and puma footwear founded?” Table 4: Questions that could be answered only when TIPI was used as a plug-in. et al., 2016). Answer typing was mostly limited to considering coarse-grained types (Bast and Haussmann, 2015; Lally et al., 2012) and lexical answer types (Berant and Liang, 2015; Abujabal et al., 2017). Both such modes fail when the answer type is not explicit. More recently, Yavuz et al. (2016) exploit more implicit type cues for KBQA: but their method of creating training data is context-agnostic, which we remedy in our work. An early line of work deals with question classification (Li and Roth, 2002; Blunsom et al., 2006; Huang et al., 2008), but they were designed for a handful of TREC types and is not really relevant for KB-QA with thousands of distinct classes. Finally, this work is the first to harness answer types for compositional KB-QA. 6 Conclusion We pres"
I17-2038,P02-1006,0,0.0746551,"Missing"
I17-2038,P16-1220,0,0.376668,"Abstract that are prevalent in popular KB-QA benchmarks are generally out of scope for most state-of-theart KB-QA systems (Yih et al., 2015; Dong et al., 2015; Berant and Liang, 2015). However, questions like “Who won a Nobel Prize in Physics and was born in Bavaria?”, can be decomposed into a set of simpler questions “Who won a Nobel Prize in Physics?” and “Who was born in Bavaria?”. We refer to such questions as compositional questions, and these are the focus of this work. Limitations of state-of-the-art. A few past approaches that can handle such compositional questions (Bao et al., 2016; Xu et al., 2016; Abujabal et al., 2017) generate and execute candidate SPARQL queries for each sub-question separately and/or use the intersection as the final answer (Werner Heisenberg, among others). This creates the challenge of deciding which queries from the different sub-questions fit together. Past efforts use information about answers to all generated SPARQL queries, and retrospectively choose a query pair from among these whose answer intersection is non-empty. However, this mode of operation is highly inefficient, since it necessitates execution of all generated queries, followed by a ranking or ag"
I17-2038,D15-1103,1,0.800418,"are removed. All surviving query pairs are combined, and are then scored by the sum of the inverses of the ranks they had from the underlying KBQA model. The pair that maximizes this rank inversion score is finally chosen for execution by the system: PhysicsNobelPrize wonBy x? . Bavaria peopleBornHere x? for our example. 3 Predicting Answer Types Our strategy for answer type prediction is to harness explicit clues available in the question, and to resort to more implicit ones only when such signals are not present. We thus use a twostage approach, inspired by work on named entity typing (Del Corro et al., 2015; Yosef et al., 2012)1 : (1) candidate collection, using lexicosyntactic patterns to identify lexical types which can be mapped to a set of semantic types using lexicons; (2) type selection, using a hierarchical classifier to disambiguate among the candidates. Note that in absence of explicit clues for candidates in the question, our method proceeds directly to the second stage. Candidate Collection. To extract the lexical answer type from a question, we use simple POS patterns (examples in Table 1), utilizing Stanford CoreNLP for tokenization and POS-tagging (Manning et al., 2014). In a secon"
I17-2038,P15-1026,0,0.0303878,"Missing"
I17-2038,D16-1015,0,0.0148486,"5 “who is the president of the us who played in bedtime for bonzo?” “who played for ac milan and inter milan?” “what movie did russell crowe and denzel washington work on together?” “which country were the adidas and puma footwear founded?” Table 4: Questions that could be answered only when TIPI was used as a plug-in. et al., 2016). Answer typing was mostly limited to considering coarse-grained types (Bast and Haussmann, 2015; Lally et al., 2012) and lexical answer types (Berant and Liang, 2015; Abujabal et al., 2017). Both such modes fail when the answer type is not explicit. More recently, Yavuz et al. (2016) exploit more implicit type cues for KBQA: but their method of creating training data is context-agnostic, which we remedy in our work. An early line of work deals with question classification (Li and Roth, 2002; Blunsom et al., 2006; Huang et al., 2008), but they were designed for a handful of TREC types and is not really relevant for KB-QA with thousands of distinct classes. Finally, this work is the first to harness answer types for compositional KB-QA. 6 Conclusion We presented TIPI, a mechanism for enabling KBQA systems to answer compositional questions using answer type prediction. TIPI"
I17-2038,P13-1158,0,0.0793096,"ing et al., 2014). In a second step, the lexical type extracted from the question is mapped 4 4.1 Experiments Setup Dataset. We use the very recent dataset of 150 compositional questions2 created by Abujabal et al. (2017) which were sampled from the 2013 2 Available for download at http://people. mpi-inf.mpg.de/˜abujabal/publications/ quint/complex-questions-wikiasnwers-150. json, Accessed 22 September 2017. 1 While the task in named entity typing is somewhat similar to answer typing, the crucial difference is that in the latter the entity (answer) itself is missing. 224 WikiAnswers resource (Fader et al., 2013). Every question in this dataset was constrained to have more than one named entity or relational phrase (e.g., “Who directed Braveheart and Paparazzi?” and “Who directed Braveheart and played in Mad Max?”). We use Freebase as the underlying KB. Feature Sets Surface Surface + DP Surface + DP + w2v Surface + DP + w2v + QLen Prec (Auto) Prec (Human) 71.9 72.2 73.2 73.9 65.6 65.9 67.1 67.3 Table 2: Intrinsic evaluation of type prediction, showing effects of feature ablation. Baselines. We use the following baselines: the open-source AQQU system (Bast and Haussmann, 2015) (best performing public K"
I17-2038,P15-1128,0,0.0185094,"re general case. 1 Introduction Motivation. Question answering over knowledge bases (KB-QA) has gained attention, facilitated by the rise of large-scale knowledge bases such as Freebase (Bollacker et al., 2007), DBPedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007). The key challenge for KB-QA is to align mentions and relational phrases in a natural language question to semantic items in the KB (entities and predicates), and to construct a valid SPARQL query that is then executed over the KB to retrieve crisp answers (Abujabal et al., 2017; Bast and Haussmann, 2015; Yahya et al., 2013; Yih et al., 2015). Questions going beyond simple factoid questions (like “Who won a Nobel Prize in Physics?”) 222 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 222–227, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where the authors separated cases as being either parallel or nested. This work deals with parallel decomposition, and handling nesting like “which daughter of john f. kennedy studied at radcliffe college?” is future work. Answer Type prediction. Next, we predict a set of expected answer types for each sub-question using an answer type"
I17-2038,D08-1097,0,0.0304684,"tions that could be answered only when TIPI was used as a plug-in. et al., 2016). Answer typing was mostly limited to considering coarse-grained types (Bast and Haussmann, 2015; Lally et al., 2012) and lexical answer types (Berant and Liang, 2015; Abujabal et al., 2017). Both such modes fail when the answer type is not explicit. More recently, Yavuz et al. (2016) exploit more implicit type cues for KBQA: but their method of creating training data is context-agnostic, which we remedy in our work. An early line of work deals with question classification (Li and Roth, 2002; Blunsom et al., 2006; Huang et al., 2008), but they were designed for a handful of TREC types and is not really relevant for KB-QA with thousands of distinct classes. Finally, this work is the first to harness answer types for compositional KB-QA. 6 Conclusion We presented TIPI, a mechanism for enabling KBQA systems to answer compositional questions using answer type prediction. TIPI relies on a finegrained answer typing module, that respects question context and type hierarchy. Experiments on a recent benchmark show that TIPI achieves stateof-the-art performance under single-query execution, and substantial query reduction when the"
I17-2038,C12-2133,1,0.815505,"rviving query pairs are combined, and are then scored by the sum of the inverses of the ranks they had from the underlying KBQA model. The pair that maximizes this rank inversion score is finally chosen for execution by the system: PhysicsNobelPrize wonBy x? . Bavaria peopleBornHere x? for our example. 3 Predicting Answer Types Our strategy for answer type prediction is to harness explicit clues available in the question, and to resort to more implicit ones only when such signals are not present. We thus use a twostage approach, inspired by work on named entity typing (Del Corro et al., 2015; Yosef et al., 2012)1 : (1) candidate collection, using lexicosyntactic patterns to identify lexical types which can be mapped to a set of semantic types using lexicons; (2) type selection, using a hierarchical classifier to disambiguate among the candidates. Note that in absence of explicit clues for candidates in the question, our method proceeds directly to the second stage. Candidate Collection. To extract the lexical answer type from a question, we use simple POS patterns (examples in Table 1), utilizing Stanford CoreNLP for tokenization and POS-tagging (Manning et al., 2014). In a second step, the lexical t"
N19-1027,C16-1236,0,0.359762,"rmation needs by covering various question phenomena and the wide lexical and syntactic variety in expressing these information needs. The 1 The main part of this work was carried out when the author was at the Max Planck Institute for Informatics. 307 Proceedings of NAACL-HLT 2019, pages 307–317 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Dataset ComQA (This paper) Free917 (Cai and Yates, 2013) WebQuestions (Berant et al., 2013) SimpleQuestions (Bordes et al., 2015) QALD (Usbeck et al., 2017) LC-QuAD (Trivedi et al., 2017) ComplexQuestions (Bao et al., 2016) GraphQuestions (Su et al., 2016) ComplexWebQuestions (Talmor and Berant, 2018) TREC (Voorhees and Tice, 2000) Large scale (> 5K) Real Information Needs Complex Questions Question Paraphrases 3 7 3 3 7 3 7 3 3 7 3 7 3 7 7 7 3 7 7 3 3 7 7 7 3 3 3 3 3 3 3 7 7 7 7 7 7 3 7 7 Table 1: Comparison of ComQA with existing QA datasets over various dimensions. ters with answers. ComQA answers are primarily Wikipedia entity URLs. This has two motivations: (i) it builds on the example of search engines that use Wikipedia entities as answers for entitycentric queries (e.g., through knowledge cards), and (ii"
N19-1027,D13-1160,0,0.696182,"search in QA in a manner consistent with the needs of end users, it is important to have access to datasets that reflect real user information needs by covering various question phenomena and the wide lexical and syntactic variety in expressing these information needs. The 1 The main part of this work was carried out when the author was at the Max Planck Institute for Informatics. 307 Proceedings of NAACL-HLT 2019, pages 307–317 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Dataset ComQA (This paper) Free917 (Cai and Yates, 2013) WebQuestions (Berant et al., 2013) SimpleQuestions (Bordes et al., 2015) QALD (Usbeck et al., 2017) LC-QuAD (Trivedi et al., 2017) ComplexQuestions (Bao et al., 2016) GraphQuestions (Su et al., 2016) ComplexWebQuestions (Talmor and Berant, 2018) TREC (Voorhees and Tice, 2000) Large scale (> 5K) Real Information Needs Complex Questions Question Paraphrases 3 7 3 3 7 3 7 3 3 7 3 7 3 7 7 7 3 7 7 3 3 7 7 7 3 3 3 3 3 3 3 7 7 7 7 7 7 3 7 7 Table 1: Comparison of ComQA with existing QA datasets over various dimensions. ters with answers. ComQA answers are primarily Wikipedia entity URLs. This has two motivations: (i) it builds on the"
N19-1027,P14-1133,0,0.0292031,"s of commercial search engines and QA systems. Questions in our dataset exhibit a wide range of interesting aspects such as the need for temporal reasoning (Figure 1, cluster 1), comparison (Figure 1, cluster 2), compositionality (multiple subquestions with multiple entities and relations) (Figure 1, cluster 3), and unanswerable questions (Figure 1, cluster 4). ComQA is the result of a carefully designed large-scale crowdsourcing effort to group questions into paraphrase clusters and pair them with answers. Past work has demonstrated the benefits of paraphrasing for QA (Abujabal et al., 2018; Berant and Liang, 2014; Dong et al., 2017; Fader et al., 2013). Motivated by this, we judiciously use crowdsourcing to obtain clean paraphrase clusters from WikiAnswers’ noisy ones, resulting in ones like those shown in Figure 1, with both lexical and syntactic variations. The only other dataset to provide such clusters is that of Su et al. (2016), but that is based on synthetic information needs. • We present a dataset of 11,214 real user questions collected from a community QA website. The questions exhibit a range of aspects that are important for users and challenging for existing QA systems. Using crowdsourcin"
N19-1027,Q15-1039,0,0.024157,"Missing"
N19-1027,P13-1042,0,0.020794,"ena. ComQA questions are grouped into 4,834 paraphrase clusters through a large-scale crowdsourcing effort, which capture lexical and syntactic variety. Crowdsourcing is also used to pair paraphrase clusters with answers to serve as a supervision signal for training and as a basis for evaluation. Table 1 contrasts ComQA with publicly available QA datasets. The foremost issue that ComQA tackles is ensuring research is driven by information needs formulated by real users. Most largescale datasets resort to highly-templatic synthetically generated natural language questions (Bordes et al., 2015; Cai and Yates, 2013; Su et al., Introduction Factoid QA is the task of answering natural language questions whose answer is one or a small number of entities (Voorhees and Tice, 2000). To advance research in QA in a manner consistent with the needs of end users, it is important to have access to datasets that reflect real user information needs by covering various question phenomena and the wide lexical and syntactic variety in expressing these information needs. The 1 The main part of this work was carried out when the author was at the Max Planck Institute for Informatics. 307 Proceedings of NAACL-HLT 2019, pa"
N19-1027,chang-manning-2012-sutime,0,0.0981484,"hy’, or (ii) contain words like (dis)similarities, differences, (dis)advantages, etc. Questions matching these filters are out of scope as they require a narrative answer. We also removed questions with less than three or more than twenty words, as we found these to be typically noisy or non-factoid questions. This left us with about 21M questions belonging to 6.1M clusters. To further focus on factoid questions, we automatically classified questions into one or more of the following four classes: (1) temporal, (2) comparison, (3) single entity, and (4) multi-entity questions. We used SUTime (Chang and Manning, 2012) to identify temporal questions and the Stanford named entity recognizer (Finkel et al., 2005) to detect named entities. We used part-ofspeech patterns to identify comparatives, superlatives, and ordinals. Clusters which did not have questions belonging to any of the above classes were discarded from further consideration. Although these clusters contain false negatives e.g., “What official position did Mendeleev hold until his death?” due to errors by the tagging tools, • Entity: ComQA entities are grounded in Wikipedia. However, Wikipedia is inevitably incomplete, so answers that cannot be g"
N19-1027,N18-2047,0,0.0132129,"ifferent from factoid QA, where the goal is to answer questions from a large repository of data (be it textual or structured), and not a single paragraph. A recent direction in RC is dealing with unanswerable questions from the underlying data (Rajpurkar et al., 2018). ComQA includes such questions to allow tackling the same problem in the context of factoid QA. QA over KBs. Recent efforts have focused on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Fader et al., 2013; Mohammed et al., 2018; Reddy et al., 2014; Yang et al., 2014; Yao and Durme, 2014; Yahya 3 Overview In this work, a factoid question is a question whose answer is one or a small number of entities or literal values (Voorhees and Tice, 2000) e.g., “Who were the secretaries of state under Barack Obama?” and “When was Germany’s first postwar chancellor born?”. 3.1 Questions in ComQA A question in our dataset can exhibit one or more of the following phenomena: • Simple: questions about a single property of an entity (e.g., “Where was Einstein born?”) • Compositional: A question is compositional if answering it require"
N19-1027,D17-1091,0,0.0170821,"ngines and QA systems. Questions in our dataset exhibit a wide range of interesting aspects such as the need for temporal reasoning (Figure 1, cluster 1), comparison (Figure 1, cluster 2), compositionality (multiple subquestions with multiple entities and relations) (Figure 1, cluster 3), and unanswerable questions (Figure 1, cluster 4). ComQA is the result of a carefully designed large-scale crowdsourcing effort to group questions into paraphrase clusters and pair them with answers. Past work has demonstrated the benefits of paraphrasing for QA (Abujabal et al., 2018; Berant and Liang, 2014; Dong et al., 2017; Fader et al., 2013). Motivated by this, we judiciously use crowdsourcing to obtain clean paraphrase clusters from WikiAnswers’ noisy ones, resulting in ones like those shown in Figure 1, with both lexical and syntactic variations. The only other dataset to provide such clusters is that of Su et al. (2016), but that is based on synthetic information needs. • We present a dataset of 11,214 real user questions collected from a community QA website. The questions exhibit a range of aspects that are important for users and challenging for existing QA systems. Using crowdsourcing, questions are gr"
N19-1027,P04-1073,0,0.109622,"structured resources such as KBs (Berant et al., 2013; Unger et al., 2012), with answers being semantic entities. Recent work demonstrated that the two variants are complementary, and a combination of the two results in the best performance (Sun et al., 2018; Xu et al., 2016). QA over textual corpora. QA has a long tradition in IR and NLP, including benchmarking tasks in TREC (Voorhees and Tice, 2000; Dietz and Gamari, 2017) and CLEF (Magnini et al., 2004; Herrera et al., 2004). This has predominantly focused on retrieving answers from textual sources (Ferrucci, 2012; Harabagiu et al., 2006; Prager et al., 2004; Saquete et al., 2004; Yin et al., 2015). In IBM Watson (Ferrucci, 2012), structured data played a role, but text was the main source for answers. The TREC QA evaluation series provide hundreds of questions to be answered over documents, which have become widely adopted benchmarks for answer sentence selection (Wang and Nyberg, 2015). ComQA is orders of magnitude larger than TREC QA. Reading comprehension (RC) is a recently introduced task, where the goal is to answer a question from a given textual paragraph (Kocisk´y et al., 2017; Lai et al., 2017; Rajpurkar et al., 2016; Trischler et al.,"
N19-1027,P18-2124,0,0.147053,"nswer sentence selection (Wang and Nyberg, 2015). ComQA is orders of magnitude larger than TREC QA. Reading comprehension (RC) is a recently introduced task, where the goal is to answer a question from a given textual paragraph (Kocisk´y et al., 2017; Lai et al., 2017; Rajpurkar et al., 2016; Trischler et al., 2017; Yang et al., 2015). This setting is different from factoid QA, where the goal is to answer questions from a large repository of data (be it textual or structured), and not a single paragraph. A recent direction in RC is dealing with unanswerable questions from the underlying data (Rajpurkar et al., 2018). ComQA includes such questions to allow tackling the same problem in the context of factoid QA. QA over KBs. Recent efforts have focused on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Fader et al., 2013; Mohammed et al., 2018; Reddy et al., 2014; Yang et al., 2014; Yao and Durme, 2014; Yahya 3 Overview In this work, a factoid question is a question whose answer is one or a small number of entities or literal values (Voorhees and Tice, 2000) e.g., “Who were the secreta"
N19-1027,P13-1158,0,0.514499,"ms. Questions in our dataset exhibit a wide range of interesting aspects such as the need for temporal reasoning (Figure 1, cluster 1), comparison (Figure 1, cluster 2), compositionality (multiple subquestions with multiple entities and relations) (Figure 1, cluster 3), and unanswerable questions (Figure 1, cluster 4). ComQA is the result of a carefully designed large-scale crowdsourcing effort to group questions into paraphrase clusters and pair them with answers. Past work has demonstrated the benefits of paraphrasing for QA (Abujabal et al., 2018; Berant and Liang, 2014; Dong et al., 2017; Fader et al., 2013). Motivated by this, we judiciously use crowdsourcing to obtain clean paraphrase clusters from WikiAnswers’ noisy ones, resulting in ones like those shown in Figure 1, with both lexical and syntactic variations. The only other dataset to provide such clusters is that of Su et al. (2016), but that is based on synthetic information needs. • We present a dataset of 11,214 real user questions collected from a community QA website. The questions exhibit a range of aspects that are important for users and challenging for existing QA systems. Using crowdsourcing, questions are grouped into 4,834 para"
N19-1027,P02-1006,0,0.366037,"for improvement (e.g., to obtain better lexicons, or train better NER systems). Because of this, ComQA provides semantically grounded reference answers in Wikipedia (without committing to Wikipedia as an answering resource). For numerical quantities and dates, ComQA adopts the International System of Units and TIMEX3 standards, respectively. There are two main variants of the factoid QA task, with the distinction tied to the underlying answering resources and the nature of answers. Traditionally, QA has been explored over large textual corpora (Cui et al., 2005; Harabagiu et al., 2001, 2003; Ravichandran and Hovy, 2002; Saquete et al., 2009) with answers being textual phrases. Recently, it has been explored over large structured resources such as KBs (Berant et al., 2013; Unger et al., 2012), with answers being semantic entities. Recent work demonstrated that the two variants are complementary, and a combination of the two results in the best performance (Sun et al., 2018; Xu et al., 2016). QA over textual corpora. QA has a long tradition in IR and NLP, including benchmarking tasks in TREC (Voorhees and Tice, 2000; Dietz and Gamari, 2017) and CLEF (Magnini et al., 2004; Herrera et al., 2004). This has predo"
N19-1027,P05-1045,0,0.0182229,"tching these filters are out of scope as they require a narrative answer. We also removed questions with less than three or more than twenty words, as we found these to be typically noisy or non-factoid questions. This left us with about 21M questions belonging to 6.1M clusters. To further focus on factoid questions, we automatically classified questions into one or more of the following four classes: (1) temporal, (2) comparison, (3) single entity, and (4) multi-entity questions. We used SUTime (Chang and Manning, 2012) to identify temporal questions and the Stanford named entity recognizer (Finkel et al., 2005) to detect named entities. We used part-ofspeech patterns to identify comparatives, superlatives, and ordinals. Clusters which did not have questions belonging to any of the above classes were discarded from further consideration. Although these clusters contain false negatives e.g., “What official position did Mendeleev hold until his death?” due to errors by the tagging tools, • Entity: ComQA entities are grounded in Wikipedia. However, Wikipedia is inevitably incomplete, so answers that cannot be grounded in Wikipedia are represented as plain text. For example, the answer for “What is the n"
N19-1027,Q14-1030,0,0.021523,"A, where the goal is to answer questions from a large repository of data (be it textual or structured), and not a single paragraph. A recent direction in RC is dealing with unanswerable questions from the underlying data (Rajpurkar et al., 2018). ComQA includes such questions to allow tackling the same problem in the context of factoid QA. QA over KBs. Recent efforts have focused on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Fader et al., 2013; Mohammed et al., 2018; Reddy et al., 2014; Yang et al., 2014; Yao and Durme, 2014; Yahya 3 Overview In this work, a factoid question is a question whose answer is one or a small number of entities or literal values (Voorhees and Tice, 2000) e.g., “Who were the secretaries of state under Barack Obama?” and “When was Germany’s first postwar chancellor born?”. 3.1 Questions in ComQA A question in our dataset can exhibit one or more of the following phenomena: • Simple: questions about a single property of an entity (e.g., “Where was Einstein born?”) • Compositional: A question is compositional if answering it requires answering more pri"
N19-1027,P01-1037,0,0.0277187,"Missing"
N19-1027,P04-1072,0,0.136251,"Missing"
N19-1027,D14-1117,0,0.0180071,"curated by Fader et al. contains 763M questions. Questions in the crawl are grouped into 30M paraphrase clusters based on feedback from WikiAnswers users. This clustering has a low accuracy (Fader et al., 2014). Extracting factoid questions and cleaning the clusters are thus essential for a high-quality dataset. • Comparison: We consider three types of comparison questions: comparatives (“Which rivers in Europe are longer than the Rhine?”), superlatives (“What is the population of the largest city in Egypt?”), and ordinal questions (“What was the name of Elvis’s first movie?”). • Telegraphic (Joshi et al., 2014): These are short questions formulated in an informal manner similar to keyword queries (“First president India?”). Systems that rely on linguistic analysis often fail on such questions. • Answer tuple: Where an answer is a tuple of connected entities as opposed to a single entity (“When and where did George H. Bush go to college, and what did he study?”). 3.2 Dataset Construction Answers in ComQA Recent work has shown that the choice of answering resource, or the combination of resources significantly affects answering performance (Savenkov and Agichtein, 2016; Sun et al., 2018; Xu et al., 20"
N19-1027,D18-1455,0,0.304159,"is that of Su et al. (2016), but that is based on synthetic information needs. • We present a dataset of 11,214 real user questions collected from a community QA website. The questions exhibit a range of aspects that are important for users and challenging for existing QA systems. Using crowdsourcing, questions are grouped into 4,834 paraphrase clusters that are annotated with answers. ComQA is available at: http://qa. mpi-inf.mpg.de/comqa. For answering, recent research has shown that combining various resources for answering significantly improves performance (Savenkov and Agichtein, 2016; Sun et al., 2018; Xu et al., 2016). Therefore, we do not pair ComQA with a specific knowledge base (KB) or text corpus for answering. We call on the research community to innovate in combining different answering sources to tackle ComQA and advance research in QA. We use crowdsourcing to pair paraphrase clus• We present an extensive analysis and quantify the various difficulties in ComQA. We also present the results of state-of-the art QA systems on ComQA, and a detailed error analysis. 1 2 308 http://www.timeml.org https://en.wikipedia.org/wiki/SI 2 Related Work et al., 2013). Over the past five years, many"
N19-1027,N18-1059,0,0.120117,"l and syntactic variety in expressing these information needs. The 1 The main part of this work was carried out when the author was at the Max Planck Institute for Informatics. 307 Proceedings of NAACL-HLT 2019, pages 307–317 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Dataset ComQA (This paper) Free917 (Cai and Yates, 2013) WebQuestions (Berant et al., 2013) SimpleQuestions (Bordes et al., 2015) QALD (Usbeck et al., 2017) LC-QuAD (Trivedi et al., 2017) ComplexQuestions (Bao et al., 2016) GraphQuestions (Su et al., 2016) ComplexWebQuestions (Talmor and Berant, 2018) TREC (Voorhees and Tice, 2000) Large scale (> 5K) Real Information Needs Complex Questions Question Paraphrases 3 7 3 3 7 3 7 3 3 7 3 7 3 7 7 7 3 7 7 3 3 7 7 7 3 3 3 3 3 3 3 7 7 7 7 7 7 3 7 7 Table 1: Comparison of ComQA with existing QA datasets over various dimensions. ters with answers. ComQA answers are primarily Wikipedia entity URLs. This has two motivations: (i) it builds on the example of search engines that use Wikipedia entities as answers for entitycentric queries (e.g., through knowledge cards), and (ii) most modern KBs ground their entities in Wikipedia. Wherever the answers are"
N19-1027,W17-2623,0,0.0522617,"rager et al., 2004; Saquete et al., 2004; Yin et al., 2015). In IBM Watson (Ferrucci, 2012), structured data played a role, but text was the main source for answers. The TREC QA evaluation series provide hundreds of questions to be answered over documents, which have become widely adopted benchmarks for answer sentence selection (Wang and Nyberg, 2015). ComQA is orders of magnitude larger than TREC QA. Reading comprehension (RC) is a recently introduced task, where the goal is to answer a question from a given textual paragraph (Kocisk´y et al., 2017; Lai et al., 2017; Rajpurkar et al., 2016; Trischler et al., 2017; Yang et al., 2015). This setting is different from factoid QA, where the goal is to answer questions from a large repository of data (be it textual or structured), and not a single paragraph. A recent direction in RC is dealing with unanswerable questions from the underlying data (Rajpurkar et al., 2018). ComQA includes such questions to allow tackling the same problem in the context of factoid QA. QA over KBs. Recent efforts have focused on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast an"
N19-1027,P15-2116,0,0.0659102,"Missing"
N19-1027,P16-1220,0,0.153593,"al. (2016), but that is based on synthetic information needs. • We present a dataset of 11,214 real user questions collected from a community QA website. The questions exhibit a range of aspects that are important for users and challenging for existing QA systems. Using crowdsourcing, questions are grouped into 4,834 paraphrase clusters that are annotated with answers. ComQA is available at: http://qa. mpi-inf.mpg.de/comqa. For answering, recent research has shown that combining various resources for answering significantly improves performance (Savenkov and Agichtein, 2016; Sun et al., 2018; Xu et al., 2016). Therefore, we do not pair ComQA with a specific knowledge base (KB) or text corpus for answering. We call on the research community to innovate in combining different answering sources to tackle ComQA and advance research in QA. We use crowdsourcing to pair paraphrase clus• We present an extensive analysis and quantify the various difficulties in ComQA. We also present the results of state-of-the art QA systems on ComQA, and a detailed error analysis. 1 2 308 http://www.timeml.org https://en.wikipedia.org/wiki/SI 2 Related Work et al., 2013). Over the past five years, many datasets were intr"
N19-1027,D14-1071,0,0.0153424,"to answer questions from a large repository of data (be it textual or structured), and not a single paragraph. A recent direction in RC is dealing with unanswerable questions from the underlying data (Rajpurkar et al., 2018). ComQA includes such questions to allow tackling the same problem in the context of factoid QA. QA over KBs. Recent efforts have focused on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Fader et al., 2013; Mohammed et al., 2018; Reddy et al., 2014; Yang et al., 2014; Yao and Durme, 2014; Yahya 3 Overview In this work, a factoid question is a question whose answer is one or a small number of entities or literal values (Voorhees and Tice, 2000) e.g., “Who were the secretaries of state under Barack Obama?” and “When was Germany’s first postwar chancellor born?”. 3.1 Questions in ComQA A question in our dataset can exhibit one or more of the following phenomena: • Simple: questions about a single property of an entity (e.g., “Where was Einstein born?”) • Compositional: A question is compositional if answering it requires answering more primitive questions an"
N19-1027,D15-1237,0,0.0336017,"Missing"
N19-1027,P14-1090,0,0.0758798,"Missing"
N19-1027,I17-2038,1,0.879298,"Missing"
P10-1087,W09-1604,0,0.103146,"c extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consistDatabase of Named Entities The partitioning heuristics allowed us to process all entries in the complete set of Wikipedia dumps and produce a clean output set of connected components where each"
P10-1087,wentland-etal-2008-building,0,0.0108153,"ny smaller, more densely connected subgraphs. We thus investigated graph partitioning heuristics to decompose larger graphs into smaller parts that can more easily be handled with our algorithm. The METIS algorithms (Karypis and Kumar, 1998) can decompose graphs with hundreds of thousands of nodes almost instantly, but favour equally sized clusters over lower cut costs. We obtained partitionings with costs orders of magnitude lower using the heuristic by Dhillon et al. (2007). 4.5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1 , which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg a"
P12-2046,D08-1073,0,0.014949,"tion is PRAVDA (Wang et al., 2011), which utilizes label propagation as a semi-supervised learning strategy, but does not incorporate constraints. Similarly, TOB is an approach of extracting temporal businessrelated facts from free text, which requires deep parsing and does not apply constraints as well (Zhang et al., 2008). In contrast, CoTS (Talukdar et al., 2012) introduces a constraint-based approach of coupled semi-supervised learning for IE, however not focusing on the extraction part. Building on TimeML (Pustejovsky et al., 2003) several works (Verhagen et al., 2005; Mani et al., 2006; Chambers and Jurafsky, 2008; Verhagen et al., 2009; Yoshikawa et al., 2009) identify temporal relationships in free text, but don’t focus on fact extraction. 3 Framework Facts and Observations. We aim to extract factual knowledge transient over time from free text. More specifically, we assume time T = [0, Tmax ] to be a finite sequence of time-points with yearly granularity. Furthermore, a fact consists of a relation with two typed arguments and a timeinterval defining its validity. For instance, we write worksForClub(Beckham, RMadrid )@[2003, 2008) to express that Beckham played for Real Madrid from 2003 to 2007. Sinc"
P12-2046,D11-1072,1,0.723725,"Missing"
P12-2046,P06-1095,0,0.0375296,"emporal fact extraction is PRAVDA (Wang et al., 2011), which utilizes label propagation as a semi-supervised learning strategy, but does not incorporate constraints. Similarly, TOB is an approach of extracting temporal businessrelated facts from free text, which requires deep parsing and does not apply constraints as well (Zhang et al., 2008). In contrast, CoTS (Talukdar et al., 2012) introduces a constraint-based approach of coupled semi-supervised learning for IE, however not focusing on the extraction part. Building on TimeML (Pustejovsky et al., 2003) several works (Verhagen et al., 2005; Mani et al., 2006; Chambers and Jurafsky, 2008; Verhagen et al., 2009; Yoshikawa et al., 2009) identify temporal relationships in free text, but don’t focus on fact extraction. 3 Framework Facts and Observations. We aim to extract factual knowledge transient over time from free text. More specifically, we assume time T = [0, Tmax ] to be a finite sequence of time-points with yearly granularity. Furthermore, a fact consists of a relation with two typed arguments and a timeinterval defining its validity. For instance, we write worksForClub(Beckham, RMadrid )@[2003, 2008) to express that Beckham played for Real M"
P12-2046,W04-2401,0,0.105399,"Missing"
P12-2046,P05-3021,0,0.0350061,"rn-based approach for temporal fact extraction is PRAVDA (Wang et al., 2011), which utilizes label propagation as a semi-supervised learning strategy, but does not incorporate constraints. Similarly, TOB is an approach of extracting temporal businessrelated facts from free text, which requires deep parsing and does not apply constraints as well (Zhang et al., 2008). In contrast, CoTS (Talukdar et al., 2012) introduces a constraint-based approach of coupled semi-supervised learning for IE, however not focusing on the extraction part. Building on TimeML (Pustejovsky et al., 2003) several works (Verhagen et al., 2005; Mani et al., 2006; Chambers and Jurafsky, 2008; Verhagen et al., 2009; Yoshikawa et al., 2009) identify temporal relationships in free text, but don’t focus on fact extraction. 3 Framework Facts and Observations. We aim to extract factual knowledge transient over time from free text. More specifically, we assume time T = [0, Tmax ] to be a finite sequence of time-points with yearly granularity. Furthermore, a fact consists of a relation with two typed arguments and a timeinterval defining its validity. For instance, we write worksForClub(Beckham, RMadrid )@[2003, 2008) to express that Beckha"
P12-2046,P09-1046,0,0.0134854,"abel propagation as a semi-supervised learning strategy, but does not incorporate constraints. Similarly, TOB is an approach of extracting temporal businessrelated facts from free text, which requires deep parsing and does not apply constraints as well (Zhang et al., 2008). In contrast, CoTS (Talukdar et al., 2012) introduces a constraint-based approach of coupled semi-supervised learning for IE, however not focusing on the extraction part. Building on TimeML (Pustejovsky et al., 2003) several works (Verhagen et al., 2005; Mani et al., 2006; Chambers and Jurafsky, 2008; Verhagen et al., 2009; Yoshikawa et al., 2009) identify temporal relationships in free text, but don’t focus on fact extraction. 3 Framework Facts and Observations. We aim to extract factual knowledge transient over time from free text. More specifically, we assume time T = [0, Tmax ] to be a finite sequence of time-points with yearly granularity. Furthermore, a fact consists of a relation with two typed arguments and a timeinterval defining its validity. For instance, we write worksForClub(Beckham, RMadrid )@[2003, 2008) to express that Beckham played for Real Madrid from 2003 to 2007. Since sentences containing a fact and its full time-"
P12-3026,P98-1013,0,0.00908371,"ile frame semantics instead emphasizes the cognitive relatedness of words like ‘happy’, ‘unhappy’, ‘astonished’, and ‘amusement’, and explains that typical participants include an experiencer who experiences the emotions and external stimuli that evoke them. There have been individual systems that made use of both forms of knowledge (Shi and Mihalcea, 2005; Coppola and others, 2009), but due to their very different nature, there is currently no simple way to accomplish this feat. Our system addresses this by seamlessly integrating frame semantic knowledge into the system. We draw on FrameNet (Baker et al., 1998), the most well-known computational instantiation of frame semantics. While the FrameNet project is generally well-known, its use in practical applications has been limited due to the lack of easy-to-use APIs and because FrameNet alone does not cover as many words as WordNet. Our API simultaneously provides access to both sources. Language information For a given language, this extension provides information such as relevant writing systems, geographical regions, identification codes, and names in many different languages. These are all integrated into WordNet’s hypernym hierarchy, i.e. from l"
P12-3026,P10-1087,1,0.891386,"Missing"
P12-3026,2007.mtsummit-papers.24,0,0.0349635,"08). Further uses of lexical knowledge include data cleaning (Kedad and Métais, 2002), visual object recognition (Marszałek and Schmid, 2007), and biomedical data analysis (Rubin and others, 2006). Many of these applications have used Englishlanguage resources like WordNet (Fellbaum, 1998). Gerhard Weikum Max Planck Institute for Informatics weikum@mpi-inf.mpg.de However, a more multilingual resource equipped with an easy-to-use API would not only enable us to perform all of the aforementioned tasks in additional languages, but also to explore cross-lingual applications like cross-lingual IR (Etzioni et al., 2007) and machine translation (Chatterjee et al., 2005). This paper describes a new API that makes lexical knowledge about millions of items in over 200 languages available to applications, and a corresponding online user interface for users to explore the data. We first describe link prediction techniques used to create the multilingual core of the knowledge base with word sense information (Section 2). We then outline techniques used to incorporate named entities and specialized concepts (Section 3) and other types of knowledge (Section 4). Finally, we describe how the information is made accessi"
P12-3026,E12-1059,0,0.0281895,"Missing"
P12-3026,P10-1023,0,0.0593318,"Missing"
P12-3026,C98-1013,0,\N,Missing
P12-3026,I11-2001,0,\N,Missing
P13-1146,E06-1002,0,0.0679488,"Missing"
P13-1146,D07-1074,0,0.230166,"Missing"
P13-1146,D11-1142,0,0.0187797,"Missing"
P13-1146,P05-1045,0,0.114888,"Missing"
P13-1146,C02-1130,0,0.0648219,"Missing"
P13-1146,D11-1072,1,0.525236,"Missing"
P13-1146,D11-1011,0,0.0382117,"Missing"
P13-1146,D12-1104,1,0.809682,"Missing"
P13-1146,nastase-etal-2010-wikinet,0,0.0256286,"Missing"
P13-1146,C10-1105,0,0.0643738,"wn and unknown entities. We implemented the algorithm in (Lin 2012) in our framework, using the relational patterns of PATTY (Nakashole 2012) for comparability. For assessment we sampled from the top-5 highest ranked types for each entity. In our experiments, our implementation of NNPLB achieved precision values comparable to those reported in (Lin 2012). ii). HYENA (Hierarchical tYpe classification for Entity NAmes), the method of (Yosef 2012), based on a feature-rich classifier for fine-grained, hierarchical type tagging. This is a state-of-the-art representative of similar methods such as (Rahman 2010; Ling 2012). Evaluation Task. To evaluate the quality of types assigned to emerging entities, we presented turkers with sentences from the news tagged with outof-KB entities and the types inferred by the methods under test. The turkers task was to assess the correctness of types assigned to an entity mention. To make it easy to understand the task for the turkers, we combined the extracted entity and type into a sentence. For example if PEARL inferred that Brussels Summit is an political event, we generate and present the sentence: Brussels Summit is an event. We allowed four possible assessm"
P13-1146,P10-1149,0,0.0395258,"Missing"
P13-1146,C12-2133,1,0.813713,"Missing"
P13-1146,D12-1082,0,\N,Missing
P13-4023,C10-1105,0,0.142327,"rlying knowledge base. Yet, only very few knowledge bases have comprehensive class labeling of entities. Even more, in the best case, coverage drops sharply for relatively uncommon entities. (Ekbal et al., 2010) considered 141 subtypes of WordNet class PERSON and developed a maximum entropy classifier exploiting the words surrounding the mentions together with their POS tags and other contextual features. Their type hierarchy is finegrained, but still limited to sub classes of PERSON. In addition, their experimental results have been flagged as non-reproducible in the ACL Anthology. (Altaf ur Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordinal, cardinal, etc. They incorporated a hierarchical classifier using a rich feature set and made use of WordNet sense tagging. However, the latter requires human interception, which is not suitable for ad-hoc processing of outof-domain texts. (Ling and Weld, 2012) developed FIGER, which classifies entity mentions onto a two-level taxonomy based on the Freebase knowledge base (Bollacker et al., 2008). This"
P13-4023,W10-2415,0,0.0160837,"D is an inherently hard problem, especially with highly ambiguous mentions. As a consequence, accurate NED systems come at a high computation costs. 2. NED only works for those mentions that correspond to a canonical entity within a knowledge base. However, this fails for all out-ofknowledge-base entities like unregistered persons, start-up companies, etc. 3. NED heavily depends on the quality of the underlying knowledge base. Yet, only very few knowledge bases have comprehensive class labeling of entities. Even more, in the best case, coverage drops sharply for relatively uncommon entities. (Ekbal et al., 2010) considered 141 subtypes of WordNet class PERSON and developed a maximum entropy classifier exploiting the words surrounding the mentions together with their POS tags and other contextual features. Their type hierarchy is finegrained, but still limited to sub classes of PERSON. In addition, their experimental results have been flagged as non-reproducible in the ACL Anthology. (Altaf ur Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordi"
P13-4023,P05-1045,0,0.0070551,"rm entity classification into subtypes of PERSON. They developed a decisiontree classifier based on contextual features that can be automatically extracted from the text. In order to account for scarcity of labeled training data, they tapped on WordNet synonyms to achieve higher coverage. While their approach is fundamentally suitable, their type system is very restricted. In order to account for more fine-grained classes, more features need to be added to their feature set. Entity Type Classification Systems State-of-the-art tools for named entity recognition such as the Stanford NER Tagger (Finkel et al., 2005) compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). However, we are not aware of any online tool that performs fine-grained typing of entity mentions. The most common workaround to perform entity classification is a two-stage process: in first applying an online tool for Named-Entity Disambiguation (NED), such as DBpedia Spotlight (Mendes et al., 2011) or AIDA (Yosef et al., 2011; Hoffart et al., 2011), in order to map the mentions onto canonical entities and subsequen"
P13-4023,C02-1130,0,0.15289,"boring words and bi133 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 133–138, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics grams, part-of-speech tags, and also phrases from a large gazetteer derived from state-of-the-art knowledge bases. In order to perform “live” entity type classification based on ad-hoc text inputs, several performance optimizations have been undertaken to operate under real-time conditions. 2 We considered five systems. In the rest of this section we will briefly describe each of them. (Fleischman and Hovy, 2002) is one of the earliest approaches to perform entity classification into subtypes of PERSON. They developed a decisiontree classifier based on contextual features that can be automatically extracted from the text. In order to account for scarcity of labeled training data, they tapped on WordNet synonyms to achieve higher coverage. While their approach is fundamentally suitable, their type system is very restricted. In order to account for more fine-grained classes, more features need to be added to their feature set. Entity Type Classification Systems State-of-the-art tools for named entity re"
P13-4023,D11-1072,1,0.876105,"Missing"
P13-4023,P11-1138,0,0.037323,"ed typing of entity mentions. The most common workaround to perform entity classification is a two-stage process: in first applying an online tool for Named-Entity Disambiguation (NED), such as DBpedia Spotlight (Mendes et al., 2011) or AIDA (Yosef et al., 2011; Hoffart et al., 2011), in order to map the mentions onto canonical entities and subsequently query the knowledge base for their types. In fact, (Ling and Weld, 2012) followed this approach when comparing their entity classification system results against those obtained by an adoption of the Illinois’ Named-Entity Linking system (NEL) (Ratinov et al., 2011) and reached the conclusion that while NEL performed decently for prominent entities, it could not scale to cover long tail ones. Specifically, entity typing via NED has three major drawbacks: 1. NED is an inherently hard problem, especially with highly ambiguous mentions. As a consequence, accurate NED systems come at a high computation costs. 2. NED only works for those mentions that correspond to a canonical entity within a knowledge base. However, this fails for all out-ofknowledge-base entities like unregistered persons, start-up companies, etc. 3. NED heavily depends on the quality of th"
P13-4023,C12-2133,1,0.725982,"Missing"
P16-4004,C14-2017,0,0.0656282,"Missing"
P16-4004,I08-2140,0,0.0243176,"r interactive real-time exploration and analytics. Contrary to the systems aforementioned, PolySearch2 (Liu et al., 2015) goes beyond scientific publications, but its search and exploration interface is not entity-aware. Besides scientific publications, bio-surveillance systems aggregate and analyze news articles to identify health threats, such as disease outbreaks and food hazards. HealthMap (Freifeld et al., 2008) and EpiSpider (Keller et al., 2009) rely on user created ProMED reports and do not process documents automatically. Contrary to these userbased approaches, Global Health Monitor (Don et al., 2008) and the Medical Information System (MedISys) (Rortais et al., 2010) in combination with PULS (Steinberger et al., 2008) automatically extract entities and events from relevant medical news. However, the amount of entities both systems can distinguish is limited. Pang et al. (2015) emphasize the need for better exploratory search capabilities for health content, • integrating large knowledge bases like the Unified Medical Language System (UMLS) 20 Genre Clinical Trials Encyclopedic Articles News Scientific Publications Social Media Total Sources Documents Entity Occurrences Distinct Entities 2"
P17-2055,P14-5010,0,0.00475848,"Missing"
P17-2055,D12-1048,0,0.0877664,"relations. This can be harnessed in QA for cases like “Who won the most Olympic medals?” An important application of relation cardinalities is KB curation. KBs are notoriously incomplete, contain erroneous triples, and are limited in keeping up with the pace of real-world changes. For example, a KB may contain only 10 of the 28 Olympic medals that Phelps has won, or may incorrectly list 3 children for Obama. Extracting the cardinalities of relations for given subject entities can address all of these issues. Relation cardinalities are disregarded by virtually all IE methods. Open IE methods (Mausam et al., 2012; Del Corro and Gemulla, 2013) capture triples (or quadruples) such as hObama, has, two childreni. However, there is no way to interpret the numeric expression in the O slot of this triple. While IE methods that hinge on prespecified relations for KB population (e.g., NELL (Mitchell et al., 2015)) can already capture numeric values for explicitly stated attributes such as hBerlin2016attack, hasNumOfVictims, 32i, they are currently not able to learn them. This paper addresses the novel task of extracting relation cardinalities. For a given subject entity s and predicate p, we aim to infer the c"
P17-2055,P09-1113,0,0.0349811,"edge Bases and Information Extraction Automated KB construction is a major effort for quite a while. Some approaches, such as YAGO (Suchanek et al., 2007) or DBpedia (Auer et al., 2007), focus on structured parts of Wikipedia, while other approaches such as OLLIE (Mausam et al., 2012), ClauseIE (Del Corro and Gemulla, 2013) or NELL (Mitchell et al., 2015), focus on unstructured contents across the whole Web. In the latter, usually the schema is also not predefined, thus such approaches are called Open IE. Most state-of-the-art systems now rely on distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). Despite all efforts, KBs are immensely incomplete. For instance, the average number of children per person in Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) is just 0.02 (Razniewski et al., 2016). 3 Relation Cardinalities Definition We define a mention that expresses relation cardinalities as the following: “A cardinal number that states the number of objects that stand in a specific relation with a certain subject.” Using this definition, we analyzed how often relation cardinalities occur in Wikipedia. Relying on the part-of-speech (PoS) tagger of Stanford CoreNLP (Manning et al., 2014), we ext"
P17-2055,P16-1055,0,0.171136,"Missing"
P17-2055,S10-1071,0,0.102332,"Missing"
P17-2055,W16-1308,1,\N,Missing
P18-2039,D15-1058,0,0.131421,"ries. Our results indicate that the amount of knowledge is highly correlated with NER performance. The configurations with more external knowledge systematically outperform the more agnostic ones. 2 Knowledge Augmented NER In the following section, we describe the four knowledge categories in detail. Table 1 gives an overview of the features on the categories that use external knowledge. The features were used to train a linear chain CRF, a simple and explainable method, proven to work well for NER (Finkel et al., 2005; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015). 2.1 Knowledge Agnostic (A) This category contains the “local” features, which can be extracted directly from text without any external knowledge. They are mostly of a lexical, syntactic or linguistic nature and have been wellstudied in literature. We implement most of the features described in (Finkel et al., 2005): (1) The current word and words in a window of size 2; (2) Word shapes of the current word and words in a window of size 2; (3) POS tags in a window of size 2; (4) Prefixes (length three and four) and Suffixes (length one to four); (5) Presence of the current word in a window of s"
P18-2039,Q16-1026,0,0.302022,"otations on three types of entities (PER, ORG, LOC). 3.2 Incremental knowledge Here we analyze the impact of incrementally adding external knowledge. Fig. 1a shows four variants. Each contains the features corresponding to a given category plus all those from the lighter categories to the left. In all cases adding knowledge boosts F1 performance. The effect is particularly strong for MUC-7-test which registered an overall increment of almost 10 points. In both datasets, the biggest boost is registered when the KB-based features are added. As a reference point, one of the best systems to date (Chiu and Nichols, 2016) (neural-based) achieves F1 91.62 on CoNLL2013-test, while our full-knowledge CRF reaches F1 91.12. Fig. 1c shows the performance for each entity type on CoNLL2003. Again, there is a boost in all cases, especially organizations. Persons also improve significantly: At first they perform similar to 243 100 100 CoNLL2003-test CoNLL2003-dev 95 95 MUC-7-test MUC-7-dev 90 90 85 85 80 80 75 A Name KB 75 Entity CoNLL2003-test CoNLL2003-dev MUC-7-test MUC-7-dev A Name (a) NER F1 Score. 100 100 FPER FORG 95 95 FLOC FLOC 90 85 80 80 A FPER FALL FALL 85 75 Entity FORG FMISC 90 KB (b) Span-based F1 score N"
P18-2039,W09-1119,0,0.191671,"res automatically generated from different information sources, such as a knowledgebase, a list of names, or document-specific semantic annotations. Further, we show the effects on performance when incrementally adding deeper knowledge and discuss effectiveness/efficiency trade-offs. 1 Introduction Named Entity Recognition (NER) is the task of detecting named entity mentions in text and assigning them to their corresponding type. It is a crucial component in a wide range of natural language understanding tasks, such as named entity disambiguation (NED), question answering, etc. Previous work (Ratinov and Roth, 2009) argued that NER is a knowledge-intensive task and used prior knowledge with outstanding results. In this work, we attempt to quantify to which extent external knowledge influences NER performance. Even though recent approaches have excelled in end-to-end neural methods, this paper aims to give transparency and user-comprehensible explainability. This is especially significant for industrial sectors (e.g., those heavily regulated) that require the use of transparent methods for which a particular decision is explainable. We perform the study by devising a simple modular framework to exploit di"
P18-2039,W15-3904,0,0.06292,"Missing"
P18-2039,Q14-1037,0,0.0177492,"inability that neural approaches cannot yet provide. Previous work has already regarded NER as a knowledge intensive task (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). Most of these works incorporate background knowledge in the form of entity-type gazetteers (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014). Others, used external knowledge by exploiting the association between NER and NED (Durrett and Klein, 2014; Radford et al., 2015; Luo et al., 2015; Nguyen et al., 2016). In this study, we attempt to bring more light on the issue by quantifying the effect of different degrees of external knowledge. Our modular framework allows to test this intuition via novel feature sets that reflect the degree of knowledge contained in available knowledge sources. 5 Conclusion We investigated the importance of external knowledge for performing Named Entity Recognition by defining four feature categories, each of which conveys a different amount of knowledge. In addition to commonly used features in existing liter"
P18-2039,P05-1045,0,0.333788,"rm our experiments on two standard datasets by testing various combinations of knowledge categories. Our results indicate that the amount of knowledge is highly correlated with NER performance. The configurations with more external knowledge systematically outperform the more agnostic ones. 2 Knowledge Augmented NER In the following section, we describe the four knowledge categories in detail. Table 1 gives an overview of the features on the categories that use external knowledge. The features were used to train a linear chain CRF, a simple and explainable method, proven to work well for NER (Finkel et al., 2005; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015). 2.1 Knowledge Agnostic (A) This category contains the “local” features, which can be extracted directly from text without any external knowledge. They are mostly of a lexical, syntactic or linguistic nature and have been wellstudied in literature. We implement most of the features described in (Finkel et al., 2005): (1) The current word and words in a window of size 2; (2) Word shapes of the current word and words in a window of size 2; (3) POS tags in a window of size 2; (4) Prefixes (length thr"
P18-2039,W02-2024,0,0.182301,"3000 Figure 3: NER F1 for German on CoNLL2003g dataset and Spanish on CoNLL2002 dataset. 2500 2000 1500 1000 500 A Name KB Entity Figure 2: Timing experiments for CoNLL2003etest in average milliseconds per document 3.5 Multilingualism In order to demonstrate the general applicability of our approach, we implement our NER system for two additional languages, namely German and Spanish. All features for the Name, KB and Entity knowledge classes are derived from the respective language’s Wikipedia. Performance is evaluated on CoNLL2003g (Sang and Meulder, 2003) for German and CoNLL2002 (Tjong Kim Sang, 2002) for Spanish. Results can be found in Figure 3. Similar to the performance on English data, we can see that adding more external knowledge improves performance. For reference, we found that performance is close to the state-of-the art in both languages. Our system lags only 1.56 F1 points on (Lample et al., 2016) in German and 1.98 F1 points on (Yang et al., 2016) in Spanish. 4 Related Work NER is a widely studied problem. Most of previous work rely on the use of CRFs (Finkel et al., 2005; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al"
P18-2039,W03-0425,0,0.0230752,"05; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). A recent trend has achieved particularly good results modeling NER as an end-to-end task using neural networks (dos Santos and Guimar˜aes, 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2016). While this constitutes a big step forward, certain applications (e.g., in heavily regulated sectors) require a degree of explainability that neural approaches cannot yet provide. Previous work has already regarded NER as a knowledge intensive task (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). Most of these works incorporate background knowledge in the form of entity-type gazetteers (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014). Others, used external knowledge by exploiting the association between NER and NED (Durrett and Klein, 2014; Radford et al., 2015; Luo et al., 2015; Nguyen et al., 2016). In this study, we attempt to bring more light on the iss"
P18-2039,N16-1155,0,0.0143606,"tate-of-the art in both languages. Our system lags only 1.56 F1 points on (Lample et al., 2016) in German and 1.98 F1 points on (Yang et al., 2016) in Spanish. 4 Related Work NER is a widely studied problem. Most of previous work rely on the use of CRFs (Finkel et al., 2005; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). A recent trend has achieved particularly good results modeling NER as an end-to-end task using neural networks (dos Santos and Guimar˜aes, 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2016). While this constitutes a big step forward, certain applications (e.g., in heavily regulated sectors) require a degree of explainability that neural approaches cannot yet provide. Previous work has already regarded NER as a knowledge intensive task (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). Most of these works incorporate background knowledge in the form of entity-type gazetteers (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; R"
P18-2039,W03-0434,0,0.0121409,"awa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). A recent trend has achieved particularly good results modeling NER as an end-to-end task using neural networks (dos Santos and Guimar˜aes, 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2016). While this constitutes a big step forward, certain applications (e.g., in heavily regulated sectors) require a degree of explainability that neural approaches cannot yet provide. Previous work has already regarded NER as a knowledge intensive task (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). Most of these works incorporate background knowledge in the form of entity-type gazetteers (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014). Others, used external knowledge by exploiting the association between NER and NED (Durrett and Klein, 2014; Radford et al., 2015; Luo et al., 2015; Nguyen et al., 2016). In this study, we attempt to bring more light on the issue by quantifying the eff"
P18-2039,D11-1072,1,0.646125,"Missing"
P18-2039,D07-1073,0,0.142355,"Missing"
P18-2039,N16-1030,0,0.0999433,"implement our NER system for two additional languages, namely German and Spanish. All features for the Name, KB and Entity knowledge classes are derived from the respective language’s Wikipedia. Performance is evaluated on CoNLL2003g (Sang and Meulder, 2003) for German and CoNLL2002 (Tjong Kim Sang, 2002) for Spanish. Results can be found in Figure 3. Similar to the performance on English data, we can see that adding more external knowledge improves performance. For reference, we found that performance is close to the state-of-the art in both languages. Our system lags only 1.56 F1 points on (Lample et al., 2016) in German and 1.98 F1 points on (Yang et al., 2016) in Spanish. 4 Related Work NER is a widely studied problem. Most of previous work rely on the use of CRFs (Finkel et al., 2005; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). A recent trend has achieved particularly good results modeling NER as an end-to-end task using neural networks (dos Santos and Guimar˜aes, 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2016). While this constitutes a big step forward, certain applications (e.g., i"
P18-2039,P09-1116,0,0.0314662,"Luo et al., 2015). A recent trend has achieved particularly good results modeling NER as an end-to-end task using neural networks (dos Santos and Guimar˜aes, 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2016). While this constitutes a big step forward, certain applications (e.g., in heavily regulated sectors) require a degree of explainability that neural approaches cannot yet provide. Previous work has already regarded NER as a knowledge intensive task (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). Most of these works incorporate background knowledge in the form of entity-type gazetteers (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014). Others, used external knowledge by exploiting the association between NER and NED (Durrett and Klein, 2014; Radford et al., 2015; Luo et al., 2015; Nguyen et al., 2016). In this study, we attempt to bring more light on the issue by quantifying the effect of different degrees of external knowledge. Our modular framework a"
P18-2039,D15-1104,0,0.302433,"h first name) and “UPerson” (i.e., last name without first name). The probabilities for all other classes would be close to zero. In comparison, the word “box” should have high probability for class “O” and close to zero for all others, since we would not expect it to occur in many named entities. 2.4 Entity-Based Knowledge (Entity) This category encodes document-specific knowledge about the entities found in text to exploit the association between NER and NED. Previous work showed that the flow of information between these generates significant performance improvements (Radford et al., 2015; Luo et al., 2015). Comparatively, this module needs significantly more computational resources. It requires a first run of NED to generate document specific features, based on the disambiguated named entities. These features are used in a second run of NER. Following (Radford et al., 2015), after the first run of NED, we create a set of document-specific gazetteers derived from the disambiguated entities. This information helps in the second round to find new named entities that were previously missed. Take the sentence “Some citizens of the European Union working in the United Kingdom do not meet visa require"
P18-2039,Q16-1016,1,0.835842,"k has already regarded NER as a knowledge intensive task (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015). Most of these works incorporate background knowledge in the form of entity-type gazetteers (Florian et al., 2003; Zhang and Johnson, 2003; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014). Others, used external knowledge by exploiting the association between NER and NED (Durrett and Klein, 2014; Radford et al., 2015; Luo et al., 2015; Nguyen et al., 2016). In this study, we attempt to bring more light on the issue by quantifying the effect of different degrees of external knowledge. Our modular framework allows to test this intuition via novel feature sets that reflect the degree of knowledge contained in available knowledge sources. 5 Conclusion We investigated the importance of external knowledge for performing Named Entity Recognition by defining four feature categories, each of which conveys a different amount of knowledge. In addition to commonly used features in existing literature, we defined four novel features that we incorporated int"
P18-2039,W14-1609,0,0.337739,"s of knowledge categories. Our results indicate that the amount of knowledge is highly correlated with NER performance. The configurations with more external knowledge systematically outperform the more agnostic ones. 2 Knowledge Augmented NER In the following section, we describe the four knowledge categories in detail. Table 1 gives an overview of the features on the categories that use external knowledge. The features were used to train a linear chain CRF, a simple and explainable method, proven to work well for NER (Finkel et al., 2005; Jun’ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015). 2.1 Knowledge Agnostic (A) This category contains the “local” features, which can be extracted directly from text without any external knowledge. They are mostly of a lexical, syntactic or linguistic nature and have been wellstudied in literature. We implement most of the features described in (Finkel et al., 2005): (1) The current word and words in a window of size 2; (2) Word shapes of the current word and words in a window of size 2; (3) POS tags in a window of size 2; (4) Prefixes (length three and four) and Suffixes (length one to four); (5) Presence of the curren"
P18-2109,E06-1002,0,0.206092,"Missing"
P18-2109,D13-1184,0,0.0209997,"could be studied for further languages. NED methods and tools have been greatly advanced and become mature. Many systems use a combination of (i) local features like string similarities, lexico-syntactic characteristics and context between mentions and candidate entities and (ii) global features like the coherence among a set of selected entities. The inference over this feature space is typically performed by probabilistic graphical models, learning-to-rank techniques or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches"
P18-2109,D17-1277,0,0.0317026,"Missing"
P18-2109,Q15-1011,0,0.0173202,"reatly advanced and become mature. Many systems use a combination of (i) local features like string similarities, lexico-syntactic characteristics and context between mentions and candidate entities and (ii) global features like the coherence among a set of selected entities. The inference over this feature space is typically performed by probabilistic graphical models, learning-to-rank techniques or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs."
P18-2109,P16-1059,0,0.0194545,"ring similarities, lexico-syntactic characteristics and context between mentions and candidate entities and (ii) global features like the coherence among a set of selected entities. The inference over this feature space is typically performed by probabilistic graphical models, learning-to-rank techniques or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs. In our experiments, the Neural TextEntity Encoder by Yamada et al. (2016) serves as state-of"
P18-2109,D07-1074,0,0.189022,"son entities benefit most could be that person entities have comparably short life spans and are thus most time-sensitive. Inter-system Comparison In Table 3, we compare the time-aware NED approach diaNED-2 to various NED tools available via GERBIL (v. 1.2.5) (Usbeck et al., 2015) and to the recent work by Gupta et al. (2017). As all systems are used with standard settings, we also trained diaNED-2 on standard NED training data (CoNLL-AIDA) with the temporal context of entity mentions being the respective article’s year 6 Related Work Starting with the early work of Bunescu and Pas¸ca (2006), Cucerzan (2007), Mihalcea and Csomai (2007), and Milne and Witten (2008), 689 strated in our experiments, this time-awareness improves NED quality over diachronic texts that span long time periods. The diaNED dataset and the temporal signatures of entities are publicly available.5 Currently, we integrate a strategy for handling out-of-KB entities to determine how temporal affinity may help in the nil detection problem. Furthermore, we plan large-scale experiments with distant supervision data which will also allow to evaluate the effectiveness of considering temporal expressions in the context of the entity"
P18-2109,D17-1284,0,0.11116,"nd without the timeawareness feature. As can be seen in the table, adding the temporal feature improves the results significantly in each setting on both sets, which demonstrates that even state-of-the-art systems can be improved by making them time-aware. 5.2 HN xLisa-NGRAM (Zhang and Rettinger, 2014) WAT (Ferragina and Scaiella, 2012) PBOH (Ganea et al., 2016) FREME NER (Dojchinovski and Kliegr, 2013) FRED (Consoli and Recupero, 2015) FOX (Speck and Ngomo, 2014) Dexter (Ceccarelli et al., 2013) DBpedia Spotlight(Mendes et al., 2011) AIDA (Hoffart et al., 2011) AGDISTIS (Usbeck et al., 2014) Gupta et al. (2017) of publication. However, due to the differences in what kind of entities the systems consider and what kind of candidate entity lookup dictionaries they use, the systems are not directly comparable and the performance differences should be interpreted with a grain of salt. Nevertheless, timeawareness further increases the distance between (Yamada et al., 2016) and the second best system significantly, which demonstrates its usefulness for NED. 5.3 Type-based Analysis To gain further insights about the importance of time-awareness, we analyzed the results of diaNED-2 with and without temporal"
P18-2109,Q14-1037,0,0.0209841,"hods and tools have been greatly advanced and become mature. Many systems use a combination of (i) local features like string similarities, lexico-syntactic characteristics and context between mentions and candidate entities and (ii) global features like the coherence among a set of selected entities. The inference over this feature space is typically performed by probabilistic graphical models, learning-to-rank techniques or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neu"
P18-2109,P13-2006,0,0.0139109,"es or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs. In our experiments, the Neural TextEntity Encoder by Yamada et al. (2016) serves as state-of-the-art baseline. While temporal information was used as a global feature to compute coherence between entity lifespans (Hoffart et al., 2013), no prior work on named entity disambiguation made explicit use of temporal information as a local feature. However, the value of time has been shown in"
P18-2109,K17-1008,0,0.0197276,"-syntactic characteristics and context between mentions and candidate entities and (ii) global features like the coherence among a set of selected entities. The inference over this feature space is typically performed by probabilistic graphical models, learning-to-rank techniques or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs. In our experiments, the Neural TextEntity Encoder by Yamada et al. (2016) serves as state-of-the-art baseline. Wh"
P18-2109,D11-1072,1,0.904042,"Missing"
P18-2109,P05-1045,0,0.0371404,"hmark containing documents with heterogeneous temporal context. As in Microposts-2014, we limit documents to single sentences and headlines from HistoryNet.com (HN) and The New York Times corpus (NYT). For the annotation process, we followed the entity annotation guidelines, which have been used for annotating CoNLL-AIDA (Hoffart et al., 2011). HN is an online resource of world history with information on popular historical topics. Its section Today in History contains short texts on what happened on a specific day with a total of 7,061 facts/events (excluding born today). Using Stanford NER (Finkel et al., 2005), we extracted 13,773 entity mentions and randomly selected 350 of them. We annotated all entity mentions in respective sentences with their Wikipedia ids. After removing NER errors and out-of-KB entities, the dataset contains 865 gold entity mentions in 334 sentences. Examples are: “Conrad II claims the throne in France” from 1032 or “The Old Pretender, son of James III dies” from 1766. NYT contains more than 1.5 million documents published between 1987 and 2007. After apply5 Evaluation To evaluate the importance of temporal information in NED, we focus in our analysis on the newly created di"
P18-2109,N16-1150,0,0.0118968,"ls (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs. In our experiments, the Neural TextEntity Encoder by Yamada et al. (2016) serves as state-of-the-art baseline. While temporal information was used as a global feature to compute coherence between entity lifespans (Hoffart et al., 2013), no prior work on named entity disambiguation made explicit use of temporal information as a local feature. However, the value of time has been shown in a variety of other information extraction tasks,"
P18-2109,Q15-1023,0,0.0149606,"lem. Schumacher convinced to win on Sunday. When this news headline is fed into modern tools for Named Entity Disambiguation (NED), virtually all of them would map the mention Schumacher onto the former Formula One champion Michael Schumacher, as the best-fitting entity from a Wikipedia-centric knowledge base (KB). However, knowing that Sunday refers to August 14, 1949, i.e., ignoring the surface form but exploiting normalized information, it becomes clear that the text actually refers to the German politician Kurt Schumacher. State-of-the-art NED methods (see surveys by Hachey et al. (2013), Ling et al. (2015), Shen et al. (2015)) tend to miss this because they are designed and trained for temporally focused input corpora such as current news, and do not cope well with longitudinal archives and other diachronic corpora that span decades. Standard NED benchmarks from CoNLL and TAC do not reflect this difficulty either. 1 The diaNED corpus and the temporal signatures of entities are publicly available: https://www.mpi-inf. mpg.de/yago-naga/dianed/. 686 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 686–693 c Melbourne, Australia, July 15"
P18-2109,S10-1071,1,0.852795,"Missing"
P18-2109,D15-1063,1,0.907059,"Missing"
P18-2109,C16-1007,0,0.0174263,"et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs. In our experiments, the Neural TextEntity Encoder by Yamada et al. (2016) serves as state-of-the-art baseline. While temporal information was used as a global feature to compute coherence between entity lifespans (Hoffart et al., 2013), no prior work on named entity disambiguation made explicit use of temporal information as a local feature. However, the value of time has been shown in a variety of other information extraction tasks, such as relation extraction (UzZaman et al., 2013; Mirza and Tonelli, 2016), event extraction (Kuzey et al., 2016; Spitz and Gertz, 2016), and slot filling (Ji et al., 2011; Surdeanu et al., 2011; Surdeanu, 2013), as well as in the context of information retrieval (Berberich et al., 2010; Agarwal and Str¨otgen, 2017) and fact checking (Popat et al., 2017). In this paper, inspired by the importance of temporal information for many NLP tasks, we analyzed its value for NED. 7 Acknowledgments The authors thank the anonymous reviewers for their valuable comments and suggestions and Ikuya Yamada for providing the word-entity embeddings and his help in the implementation of"
P18-2109,Q16-1016,1,0.84472,"local features like string similarities, lexico-syntactic characteristics and context between mentions and candidate entities and (ii) global features like the coherence among a set of selected entities. The inference over this feature space is typically performed by probabilistic graphical models, learning-to-rank techniques or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada et al. (2016), Gupta et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs. In our experiments, the Neural TextEntity Encoder by Yamada et al."
P18-2109,P11-1138,0,0.0395737,"ngual temporal tagger (Str¨otgen and Gertz, 2015), the value of time for NED could be studied for further languages. NED methods and tools have been greatly advanced and become mature. Many systems use a combination of (i) local features like string similarities, lexico-syntactic characteristics and context between mentions and candidate entities and (ii) global features like the coherence among a set of selected entities. The inference over this feature space is typically performed by probabilistic graphical models, learning-to-rank techniques or algorithms related to such models (see, e.g., Ratinov et al. (2011), Hoffart et al. (2011), Ferragina and Scaiella (2012), Cheng and Roth (2013), Guo and Barbosa (2014), Durrett and Klein (2014), Chisholm and Hachey (2015), Pershina et al. (2015), Lazic et al. (2015), Nguyen et al. (2016), Globerson et al. (2016), Eshel et al. (2017), and Ganea and Hofmann (2017)). The GERBIL framework (Usbeck et al., 2015) provides a unified way of evaluating a wide variety of NED tools and services. A recent line of work uses representational learning to characterize contexts through embeddings (e.g., He et al. (2013), Sun et al. (2015), Francis-Landau et al. (2016), Yamada"
P18-2109,S13-2001,0,0.0350775,"et al. (2017), Yamada et al. (2017)). These approaches naturally lend themselves towards inference by neural networks such as LSTMs. In our experiments, the Neural TextEntity Encoder by Yamada et al. (2016) serves as state-of-the-art baseline. While temporal information was used as a global feature to compute coherence between entity lifespans (Hoffart et al., 2013), no prior work on named entity disambiguation made explicit use of temporal information as a local feature. However, the value of time has been shown in a variety of other information extraction tasks, such as relation extraction (UzZaman et al., 2013; Mirza and Tonelli, 2016), event extraction (Kuzey et al., 2016; Spitz and Gertz, 2016), and slot filling (Ji et al., 2011; Surdeanu et al., 2011; Surdeanu, 2013), as well as in the context of information retrieval (Berberich et al., 2010; Agarwal and Str¨otgen, 2017) and fact checking (Popat et al., 2017). In this paper, inspired by the importance of temporal information for many NLP tasks, we analyzed its value for NED. 7 Acknowledgments The authors thank the anonymous reviewers for their valuable comments and suggestions and Ikuya Yamada for providing the word-entity embeddings and his hel"
P18-2109,K16-1025,0,0.13506,"y enriching both with temporal signatures and contexts. diaNED-1, as basic NED system, uses a mention-entity prior reflecting entity prominence and a keyphrase-based language model for the similarity of mention and entity contexts (as suggested by Hoffart et al. (2011)). These components are cast into edge weights for a graph over which the final disambiguation is computed. Hyper-parameters for the relative influence of the two components are tuned using an SVM. We added the temporal dimension to the feature set and retrained the model accordingly to get new feature weights. diaNED-2 based on Yamada et al. (2016): This is a learning-to-rank-based model. Besides mention-entity priors and string-similarity features, it uses word and entity embeddings trained in a joint vector space to model context and coherence. The intuition is that a good candidate entity vector must be close to the word and entity vectors appearing in the same context. 2 In an analysis of temporal expressions extracted with HeidelTime from the Wikipedia corpus (August 2016 dump), we find that there are on average 18.500 expressions per year value (with year values ranging from 0001 AD to 2050 AD) in contrast to only 9.64 expressions"
P19-1023,P15-1034,0,0.0239598,"of Open Information Extraction (Open IE) and proposed a pipeline that consists of three stages: learner, extractor, and assessor. The learner uses dependency-parsing information to learn patterns for extraction, in an unsupervised way. The extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates. The assessor assigns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of each token of the input. The work most 231 D"
P19-1023,P18-2065,0,0.0183315,", 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB predicates necessitates another mapping step, typically aided by paraphrase dictionaries. This two-stage architecture is inherently prone to error propagation across its two stages: NED errors may cause extraction errors (and vice versa) that lead to inaccurate relationships being • We propose an end-to-end model for extract230 related to ours is Neural Open IE (Cui et al., 2018), which proposed an encoder-decoder with attention model to extract triples. However, this work is not geared for extracting relations of canonicalized entities. Another line of studies use neural learning for semantic role labeling (He et al., 2018), but the goal here is to recognize the predicate-argument structure of a single input sentence – as opposed to extracting relations from a corpus. All of these methods generate triples where the head and tail entities and the predicate stay in their surface forms. Therefore, different names and phrases for the same entities result in multiple trip"
P19-1023,D11-1142,0,0.0476115,"nko et al. (2007) introduced the paradigm of Open Information Extraction (Open IE) and proposed a pipeline that consists of three stages: learner, extractor, and assessor. The learner uses dependency-parsing information to learn patterns for extraction, in an unsupervised way. The extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates. The assessor assigns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of ea"
P19-1023,N13-1092,0,0.0649546,"Missing"
P19-1023,D17-1278,0,0.142936,"ntities and predicate of the first extracted triple, including NYU, instance of, and Private University, are mapped to their unique IDs Q49210, P31, and Q902104, respectively, to comply with the semantic space of the KB. Previous studies on relation extraction have employed both unsupervised and supervised approaches. Unsupervised approaches typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted e"
P19-1023,D16-1236,1,0.942465,"fold. First, the embeddings capture the relationship between words and entities, which is essential for named entity disambiguation. Second, the entity embeddings preserve the relationships between entities, which help to build a highly accurate classifier to filter the invalid extracted triples. To cope with the lack of fully labeled training data, we adapt distant supervision to generate aligned pairs of sentence and triple as the training data. We augment the process with co-reference resolution (Clark and Manning, 2016) and dictionary-based paraphrase detection (Ganitkevitch et al., 2013; Grycner and Weikum, 2016). The co-reference resolution helps extract sentences with implicit entity names, which enlarges the set of candidate sentences to be aligned with existing triples in a KB. The paraphrase detection helps filter sentences that do not express any relationships between entities. The main contributions of this paper are: also the predicate &quot;instance of&quot; is in the set of predefined predicates we are interested in, but the relationship of hNYU, instance of, Private Universityi does not exist in the KB. We aim to add this relationship to our KB. This is the typical situation for KB enrichment (as opp"
P19-1023,P18-2058,0,0.0250184,"sitates another mapping step, typically aided by paraphrase dictionaries. This two-stage architecture is inherently prone to error propagation across its two stages: NED errors may cause extraction errors (and vice versa) that lead to inaccurate relationships being • We propose an end-to-end model for extract230 related to ours is Neural Open IE (Cui et al., 2018), which proposed an encoder-decoder with attention model to extract triples. However, this work is not geared for extracting relations of canonicalized entities. Another line of studies use neural learning for semantic role labeling (He et al., 2018), but the goal here is to recognize the predicate-argument structure of a single input sentence – as opposed to extracting relations from a corpus. All of these methods generate triples where the head and tail entities and the predicate stay in their surface forms. Therefore, different names and phrases for the same entities result in multiple triples, which would pollute the KG if added this way. The only means to map triples to uniquely identified entities in a KG is by post-processing via entity linking (NED) methods (Shen et al., 2015) or by clustering with subsequent mapping (Gal´arraga e"
P19-1023,P82-1020,0,0.77071,"Missing"
P19-1023,D11-1072,1,0.814752,"Missing"
P19-1023,P10-1030,0,0.0897818,"Missing"
P19-1023,D16-1245,0,0.161323,"013) and TransE (Bordes et al., 2013). The advantages of our jointly learned embeddings are twofold. First, the embeddings capture the relationship between words and entities, which is essential for named entity disambiguation. Second, the entity embeddings preserve the relationships between entities, which help to build a highly accurate classifier to filter the invalid extracted triples. To cope with the lack of fully labeled training data, we adapt distant supervision to generate aligned pairs of sentence and triple as the training data. We augment the process with co-reference resolution (Clark and Manning, 2016) and dictionary-based paraphrase detection (Ganitkevitch et al., 2013; Grycner and Weikum, 2016). The co-reference resolution helps extract sentences with implicit entity names, which enlarges the set of candidate sentences to be aligned with existing triples in a KB. The paraphrase detection helps filter sentences that do not express any relationships between entities. The main contributions of this paper are: also the predicate &quot;instance of&quot; is in the set of predefined predicates we are interested in, but the relationship of hNYU, instance of, Private Universityi does not exist in the KB. We"
P19-1023,N18-2053,0,0.0517372,"Missing"
P19-1023,K18-1050,0,0.0272964,"ord and entity embeddings to capture the relationship between words and entities for named entity disambiguation. We further propose a modified beam search and a triple classifier to generate high-quality triples. • We evaluate the proposed model over two real-world datasets. We adapt distant supervision with co-reference resolution and paraphrase detection to obtain high-quality training data. The experimental results show that our model consistently outperforms a strong baseline for neural relation extraction (Lin et al., 2016) coupled with state-of-the-art NED models (Hoffart et al., 2011; Kolitsas et al., 2018). 2 2.1 2.2 Entity-aware Relation Extraction Inspired by the work of Brin (1998), state-of-theart methods employ distant supervision by leveraging seed facts from an existing KG (Mintz et al., 2009; Suchanek et al., 2009; Carlson et al., 2010). These methods learn extraction patterns from seed facts, apply the patterns to extract new fact candidates, iterate this principle, and finally use statistical inference (e.g., a classifier) for reducing the false positive rate. Some of these methods hinge on the assumption that the co-occurrence of a seed fact’s entities in the same sentence is an indi"
P19-1023,P17-1004,0,0.0380074,"Missing"
P19-1023,W15-1506,0,0.0685961,"Missing"
P19-1023,P16-1200,0,0.565133,"typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted entities (e.g., (Suchanek et al., 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB predicates necessitates another mapping step, typically aided by para"
P19-1023,W18-6501,0,0.0677595,"Missing"
P19-1023,N13-1008,0,0.144814,"Missing"
P19-1023,D12-1048,0,0.0271704,"troduced the paradigm of Open Information Extraction (Open IE) and proposed a pipeline that consists of three stages: learner, extractor, and assessor. The learner uses dependency-parsing information to learn patterns for extraction, in an unsupervised way. The extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates. The assessor assigns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of each token of the input"
P19-1023,D17-1188,0,0.0765332,"our problem. man (2015) proposed Convolution Networks with multi-sized window kernel. Zeng et al. (2015) proposed Piecewise Convolution Neural Networks (PCNN). Lin et al. (2016, 2017) improved this approach by proposing PCNN with sentence-level attention. This method performed best in experimental studies; hence we choose it as the main baseline against which we compare our approach. Follow-up studies considered further variations: Zhou et al. (2018) proposed hierarchical attention, Ji et al. (2017) incorporated entity descriptions, Miwa and Bansal (2016) incorporated syntactic features, and Sorokin and Gurevych (2017) used background knowledge for contextualization. None of these neural models is geared for KG enrichment, as the canonicalization of entities is out of their scope. 3 3.1 Solution Framework Figure 1 illustrates the overall solution framework. Our framework consists of three components: data collection module, embedding module, and neural relation extraction module. In the data collection module (detailed in Section 3.2), we align known triples in an existing KB with sentences that contain such triples from a text corpus. The aligned pairs of sentences and triples will later be used as the tra"
P19-1023,P09-1113,0,0.858189,"th unsupervised and supervised approaches. Unsupervised approaches typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted entities (e.g., (Suchanek et al., 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB pred"
P19-1023,N18-1081,0,0.016511,"igns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of each token of the input. The work most 231 Dataset Collection Module Wikipedia article Embedding Module Neural Relation Extraction Module Joint learning skip-gram & TransE Expected output: <Q49210,P31,Q902104&gt;; <Q387638,P161,Q40026&gt; Triple classifier Wikidata Word Embeddings Distant supervision Entity Embeddings 0.2 0.4 0.1 0.2 0.1 0.1 0.5 0.1 0.4 0.2 0.1 0.5 0.1 0.1 0.2 0.1 0.5 0.1 0.5 0.1 0.2 0.3 0.3 0.3 0.2 0.2 0.3"
P19-1023,P16-1105,0,0.0159232,"d rj ∈ R. Table 1 illustrates the input and target output of our problem. man (2015) proposed Convolution Networks with multi-sized window kernel. Zeng et al. (2015) proposed Piecewise Convolution Neural Networks (PCNN). Lin et al. (2016, 2017) improved this approach by proposing PCNN with sentence-level attention. This method performed best in experimental studies; hence we choose it as the main baseline against which we compare our approach. Follow-up studies considered further variations: Zhou et al. (2018) proposed hierarchical attention, Ji et al. (2017) incorporated entity descriptions, Miwa and Bansal (2016) incorporated syntactic features, and Sorokin and Gurevych (2017) used background knowledge for contextualization. None of these neural models is geared for KG enrichment, as the canonicalization of entities is out of their scope. 3 3.1 Solution Framework Figure 1 illustrates the overall solution framework. Our framework consists of three components: data collection module, embedding module, and neural relation extraction module. In the data collection module (detailed in Section 3.2), we align known triples in an existing KB with sentences that contain such triples from a text corpus. The ali"
P19-1023,D12-1104,1,0.779299,"ining data in the form of sentence-triple pairs. Following Sorokin and Gurevych (2017), we use distant supervision (Mintz et al., 2009) to align sentences in Wikipedia1 with triples in Wikidata2 (Vrandecic and Kr¨otzsch, 2014). 1 https://dumps.wikimedia.org/enwiki/latest/enwikilatest-pages-articles.xml.bz2 2 https://dumps.wikimedia.org/wikidatawiki/entities/latestall.ttl.gz 233 In Step (2), we use a dictionary based paraphrase detection to capture relationships between entities in a sentence. First, we create a dictionary by populating predicate paraphrases from three sources including PATTY (Nakashole et al., 2012), POLY (Grycner and Weikum, 2016), and PPDB (Ganitkevitch et al., 2013) that yield 540 predicates and 24, 013 unique paraphrases. For example, predicate paraphrases for the relationAll (WIKI) Train+val Test (WIKI) Test (GEO) #pairs 255,654 225,869 29,785 1,000 #triples 330,005 291,352 38,653 1,095 #entities 279,888 249,272 38,690 124 #predicates 158 157 109 11 Table 2: Statistics of the dataset. ship &quot;place of birth&quot; are {born in, was born in, ...}. Then, we use this dictionary to filter sentences that do not express any relationships between entities. We use exact string matching to find verb"
P19-1023,D12-1042,0,0.060714,"ntz et al., 2009; Suchanek et al., 2009; Carlson et al., 2010). These methods learn extraction patterns from seed facts, apply the patterns to extract new fact candidates, iterate this principle, and finally use statistical inference (e.g., a classifier) for reducing the false positive rate. Some of these methods hinge on the assumption that the co-occurrence of a seed fact’s entities in the same sentence is an indicator of expressing a semantic relationship between the entities. This is a potential source of wrong labeling. Follow-up studies (Hoffmann et al., 2010; Riedel et al., 2010, 2013; Surdeanu et al., 2012) overcome this limitation by various means, including the use of relation-specific lexicons and latent factor models. Still, these methods treat entities by their surface forms and disregard their mapping to existing entities in the KG. Suchanek et al. (2009) and Sa et al. (2017) used probabilistic-logical inference to eliminate false positives, based on constraint solving or Monte Carlo sampling over probabilistic graphical models, respectively. These methods integrate entity linking (i.e., NED) into their models. However, both have high computational complexity and rely on modeling constrain"
P19-1023,P18-1151,1,0.857438,"Missing"
P19-1023,K16-1025,0,0.117063,"d model on a different style of text than the training data), we also collect another test dataset outside Wikipedia. We apply the same procedure to the user reviews of a travel website. First, we collect user reviews on 100 popular landmarks in Australia. Then, we apply the adapted distant supervision to the reviews and collect 1,000 sentence-triple pairs (we call it the GEO test dataset). Table 2 summarizes the statistics of our datasets. 3.3 a joint learning of word and entity embeddings that is effective to capture the similarity between words and entities for named entity disambiguation (Yamada et al., 2016). Note that our method differs from that of Yamada et al. (2016). We use joint learning by combining skip-gram (Mikolov et al., 2013) to compute the word embeddings and TransE (Bordes et al., 2013) to compute the entity embeddings (including the relationship embeddings), while Yamada et al. (2016) use Wikipedia Link-based Measure (WLM) (Milne and Witten, 2008) that does not consider the relationship embeddings. Our model learns the entity embeddings by minimizing a margin-based objective function JE : JE = X X   max 0, γ + f (tr ) − f (t0r ) (1) tr ∈Tr t0r ∈Tr0 Tr = {hh, r, ti|hh, r, ti ∈ G"
P19-1023,D15-1203,0,0.522191,"pervised approaches typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted entities (e.g., (Suchanek et al., 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB predicates necessitates another mapping step, typi"
Q14-1013,D09-1062,0,0.0259905,"tically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 System Overview This section gives an overview of the whole system for extracting sentiment-oriented relation instances. Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from senten"
Q14-1013,P10-2050,0,0.0187524,", showing that S ENTI -LSSVM model can effectively learn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems. 2 Related Work There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al."
Q14-1013,W06-1651,0,0.0376951,"VM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing s"
Q14-1013,C08-1031,0,0.179373,"aluable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode constraints to discourage an attribute to participate in a polarity relation and a comparative relation at the same time."
Q14-1013,P11-2018,0,0.108016,"cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode c"
Q14-1013,W06-0301,0,0.0142575,"sed approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences whic"
Q14-1013,P03-1054,0,0.0042962,"X Structural Inference In order to find the best eMRG for a given sentence with a well trained model, we need to determine the most likely relation type for each relation candidate and support the corresponding assertions with proper textual evidences. We formulate this task as an Integer Linear Programming (ILP). Instead of considering all constituents of a sentence, we empirically select a subset as textual evidences for each relation candidate. 6.1 Textual Evidence Candidates Selection Textual evidences are selected based on the constituent trees of sentences parsed by the Stanford parser (Klein and Manning, 2003). For each mention in a sentence, we first locate a constituent in the tree with the maximal overlap by Jaccard similarity. Starting from this constituent, we consider two types of candidates: type I candidates are constituents at the highest level which contain neither any word of another mention nor any contrast conjunctions such as “but”; type II candidates are constituents at the highest level which cover exactly two mentions of an edge and do not overlap with any other mentions. For a binary edge connecting an entity mention and an attribute mention, we consider a type I candidate startin"
Q14-1013,P09-1039,0,0.1849,"xtracts sets 158 S ENTI -LSSVM Model The task of sentiment-oriented relation extraction is to determine the most likely sSoR in a sentence. Since sSoRs are derived from the corresponding MRG s as described in Section 3, the task is reduced to find the most likely MRG for each sentence. Since an MRG is created by assigning relation types to a subset of all relation candidates, which are possible tuples of mentions with unknown relation types, the number of MRGs can be extremely high. To tackle the task, one solution is to employ an edge-factored linear model in the framework of structural SVM (Martins et al., 2009; Tsochantaridis et al., 2004). The model suggests that a bag of features should be specified for each relation candidate, and then the model predicts the most likely candidate sets along with their relation types to form the optimal MRGs. As we observed, for a relation candidate, the most informative features are the words near its entity mentions in the original text. However, if we represent a candidate by all these words, it is very likely that the instances of different relation types share overly similar features, because a mention is often involved in more than one relation candidate, a"
Q14-1013,P07-1055,0,0.0322612,"f them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathi"
Q14-1013,H05-1043,0,0.0415718,"ation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall shor"
Q14-1013,C10-1103,1,0.854867,"ng to entity A. Co-occurrence: We have mentioned the cooccurrence feature in Equation 2, indicated by Φc (a, a0 ). It captures the co-occurrence of two labeled edges incident to the same entity mention. Note that the co-occurrence feature function is considered only if there is a contrast conjunction such as “but” between the non-shared entity mentions incident to the two labeled edges. Senti-predictors: Following the idea of (Qu et al., 2012), we encode the prediction results from the rule-based phrase-level multi-relation predictor (Ding et al., 2009) and from the bag-of-opinions predictor (Qu et al., 2010) as features based on the textual evidence. The output of the first predictor is an integer value, while the output of the second predictor is a sentiment relation, such as “positive”, “negative”, “better” or “worse”. We map the relational outputs into integer values and then encode the outputs from both predictors as senti-predictor features. Others: The commonly used part-of-speech tags are also included as features. Moreover, for an edge candidate, a set of binary features are used to denote the types of the edge and its entity mentions. For instance, a binary feature indicates whether an e"
Q14-1013,D12-1014,1,0.913576,"son and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and"
Q14-1013,D12-1110,0,0.0421273,"approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistic"
Q14-1013,P09-1026,0,0.0139075,"and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 System Overview This section gives an overview of the whole system for extracting sentiment-oriented relation instances. Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from sentences. 3.1 Concepts and Definiti"
Q14-1013,J11-2001,0,0.0385546,"Missing"
Q14-1013,P10-1059,0,0.0197331,"anon 7D, textitprice). However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background. We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence. Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions. The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training. For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assigning a sentimentbearing expression to the n"
Q14-1013,P10-1042,0,0.129447,"l Linguistics. and preferred(Nikon D7000, Canon 7D, textitprice). However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background. We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence. Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions. The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training. For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assig"
Q14-1013,H05-1044,0,0.180832,"genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode constraints to discoura"
Q14-1013,D11-1123,0,0.0218512,"hod requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 Syste"
Q14-1013,D11-1016,0,0.0189865,"rn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems. 2 Related Work There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no"
Q14-1013,W11-0323,0,0.0179712,"utperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a se"
Q14-1013,H05-2017,0,\N,Missing
Q15-1002,P98-1012,0,0.528956,": X ¯ 2 + log2 s BIC(C) = log2 (xi − C) i=1,··· ,s The BIC score for a set of clusters is the microaveraged BIC of the clusters. CROCS splits a cluster C into sub-clusters C1 , . . . , Ck iff the combined BIC value of the children is greater than that of the parent, else C is marked as leaf. 7 Experimental Evaluation Benchmark Datasets: We performed experiments with the following three publicly available benchmarking datasets, thereby comparing the performance of CROCS against state-of-the-art baselines under various input characteristics. • John Smith corpus: the classical benchmark for CCR (Bagga & Baldwin, 1998) comprising 197 articles selected from the New York Times. It includes mentions of 35 different “John Smith” person entities. All mentions pertaining to John Smith within a document refer to the same person. This provides a small-scale but demanding setting for CCR, as most John Smiths are longtail entities unknown to Wikipedia or any KB. • WePS-2 collection: a set of 4,500 Web pages used in the Web People Search 2 competition (Artiles et al., 2009). The documents comprise the top 150 Web search results (using Yahoo! search (as of 2008)) for each of 30 different people (obtained from Wikipedia"
Q15-1002,D08-1029,0,0.497569,"arget considered for enrichment. It can either be restricted to the target itself or can consider co-occurring items (other mention groups connected to the target). • Match: involves mapping the target to one or more relevant items in the source, and can involve simple name queries to full-fledged NED based on relevance or score confidence. Existing methods generally consider individual mentions or local mention groups as target. Extended scopes like co-occurring entities based on automatic NER and IE techniques have been proposed (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008), but use only the input corpus as the enrichment source. Recent methods (Rahman & Ng, 2011a; Ratinov & Roth, 2012; Hajishirzi et al., 2013; Zheng et al., 2013) harness KB’s, but consider only local mention groups. Also, these methods rely on high-quality NED for mapping mentions to KB entries. In contrast, CROCS considers extended scopes that include mention groups along with cooccurring mention groups when tapping into KB’s. We make only weak assumptions on matching men18 tions against KB entities, by filtering on confidence and merely treating semsum’s as features rather than relying on per"
Q15-1002,W10-4305,0,0.0566891,"Missing"
Q15-1002,D07-1020,0,0.874502,"neighborhood of the target considered for enrichment. It can either be restricted to the target itself or can consider co-occurring items (other mention groups connected to the target). • Match: involves mapping the target to one or more relevant items in the source, and can involve simple name queries to full-fledged NED based on relevance or score confidence. Existing methods generally consider individual mentions or local mention groups as target. Extended scopes like co-occurring entities based on automatic NER and IE techniques have been proposed (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008), but use only the input corpus as the enrichment source. Recent methods (Rahman & Ng, 2011a; Ratinov & Roth, 2012; Hajishirzi et al., 2013; Zheng et al., 2013) harness KB’s, but consider only local mention groups. Also, these methods rely on high-quality NED for mapping mentions to KB entries. In contrast, CROCS considers extended scopes that include mention groups along with cooccurring mention groups when tapping into KB’s. We make only weak assumptions on matching men18 tions against KB entities, by filtering on confidence and merely treating semsum’s as features r"
Q15-1002,D07-1074,0,0.411774,"for deep question answering. To a large extent, these advances have been enabled by the construction of huge knowledge bases (KB’s) such as DBpedia, Yago, or Freebase; the latter forming the core of the Knowledge Graph. Such semantic resources provide huge collections of entities: people, places, companies, celebrities, movies, etc., along with rich knowledge about their properties and relationships. Perhaps the most important value-adding component in this setting is the recognition and disambiguation of named entities in Web and user contents. Named Entity Disambiguation (NED) (see, e.g., (Cucerzan, 2007; Milne & Witten, 2008; Cornolti et al., 2013)) maps a mention string (e.g., a person name like “Bolt” or a noun phrase like “lightning bolt”) onto its proper entity if present in a KB (e.g., the sprinter Usain Bolt). A related but different task of co-reference resolution (CR) (see, e.g., (Haghighi & Klein, 2009; Ng, 2010; Lee et al., 2013)) identifies all mentions in a given text that refer to the same entity, including anaphoras such as “the president’s wife”, “the first lady”, or “she”. This task when extended to process an entire corpus is then known as cross-document co-reference resolut"
Q15-1002,N07-1011,0,0.184725,"Missing"
Q15-1002,P05-1045,0,0.00693723,"nced graph partitioning (using the similarity metric) in a hierarchical fashion to compute the cross-document coreference equivalence classes of mentions. 3 Intra-Document CR CROCS initially pre-processes input documents to cast them into plain text (using standard tools like (https://code.google.com/p/ boilerpipe/), (www.jsoup.org), etc.). It then uses the Stanford CoreNLP tool suite to detect mentions and anaphors (http://nlp. stanford.edu/software/). The detected 17 mentions are also tagged with coarse-grained lexical types (person, organization, location, etc.) by the Stanford NER Tagger (Finkel et al., 2005). This forms the input to the intra-document CR step, where we use the state-of-the-art open-source CR tool (based on multi-pass sieve algorithm) from Stanford to compute the local mention co-reference chains (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). The tagged texts and the local coreference chains are then passed to the second stage. This local CR step may produce errors (e.g., incorrect chaining of mentions or omissions) which propagate to the later stages. However, improving intra-document CR is orthogonal to our problem and thus out of the scope of this paper. Our ex"
Q15-1002,N04-1002,0,0.699466,"Missing"
Q15-1002,D09-1120,0,0.241707,"es, celebrities, movies, etc., along with rich knowledge about their properties and relationships. Perhaps the most important value-adding component in this setting is the recognition and disambiguation of named entities in Web and user contents. Named Entity Disambiguation (NED) (see, e.g., (Cucerzan, 2007; Milne & Witten, 2008; Cornolti et al., 2013)) maps a mention string (e.g., a person name like “Bolt” or a noun phrase like “lightning bolt”) onto its proper entity if present in a KB (e.g., the sprinter Usain Bolt). A related but different task of co-reference resolution (CR) (see, e.g., (Haghighi & Klein, 2009; Ng, 2010; Lee et al., 2013)) identifies all mentions in a given text that refer to the same entity, including anaphoras such as “the president’s wife”, “the first lady”, or “she”. This task when extended to process an entire corpus is then known as cross-document co-reference resolution (CCR) (Singh et al., 2011). It takes as input a set of documents with entity mentions, and computes as output a set of equivalence classes over the entity mentions. This does not involve mapping mentions to the entities of a KB. Unlike NED, CCR can deal with long-tail or emerging entities that are not capture"
Q15-1002,N10-1061,0,0.0303614,"Missing"
Q15-1002,D13-1029,0,0.0930008,"positions) or different strings. For a given m, the basic semsum of m, Sbasic (m), is defined as Sbasic (m) = {t ∈ sentence(m0 )|m0 ∈ M (m)} ∪ {t ∈ label(m0 )|m0 ∈ M (m)} where t are text tokens (words or phrases), sentence(m0 ) is the sentence in which mention m0 occurs, and label(m0 ) is the semantic label for m0 obtained from the KB. Note that Sbasic (m) is a bag of tokens, as different mentions in M (m) can obtain the same tokens or labels and there could be multiple occurrences of the same mention string in M (m) anyway. Prior works on CR (e.g., (Rahman & Ng, 2011a; Ratinov & Roth, 2012; Hajishirzi et al., 2013; Zheng et al., 2013)) and NED (e.g., (Cucerzan, 2007; Milne & Witten, 2008; Ratinov et al., 2011; Hoffart et al., 2011; Hajishirzi et al., 2013)) have considered such form of distant features. CROCS extends these previous methods by also considering distant features for co-occurring mention groups, and not just the group at hand. We now introduce a general framework for knowledge enrichment in our CCR setting. Strategies for knowledge enrichment involve decision making along the following dimensions: • Target: items (single mentions, local mention groups, or global mention groups across docum"
Q15-1002,D11-1072,1,0.198306,"Missing"
Q15-1002,J13-4004,0,0.344429,"ng with rich knowledge about their properties and relationships. Perhaps the most important value-adding component in this setting is the recognition and disambiguation of named entities in Web and user contents. Named Entity Disambiguation (NED) (see, e.g., (Cucerzan, 2007; Milne & Witten, 2008; Cornolti et al., 2013)) maps a mention string (e.g., a person name like “Bolt” or a noun phrase like “lightning bolt”) onto its proper entity if present in a KB (e.g., the sprinter Usain Bolt). A related but different task of co-reference resolution (CR) (see, e.g., (Haghighi & Klein, 2009; Ng, 2010; Lee et al., 2013)) identifies all mentions in a given text that refer to the same entity, including anaphoras such as “the president’s wife”, “the first lady”, or “she”. This task when extended to process an entire corpus is then known as cross-document co-reference resolution (CCR) (Singh et al., 2011). It takes as input a set of documents with entity mentions, and computes as output a set of equivalence classes over the entity mentions. This does not involve mapping mentions to the entities of a KB. Unlike NED, CCR can deal with long-tail or emerging entities that are not captured in the KB or are merely in"
Q15-1002,D12-1045,0,0.0305148,"Missing"
Q15-1002,H05-1004,0,0.0121421,"ensitivity studies explore alternative setups with Yago. Evaluation Measures: We use the established measures to assess output quality of CCR methods: • B 3 F1 score (Bagga & Baldwin, 1998): measures the F1 score as a harmonic mean of precision and recall of the final equivalence classes. Precision is defined as the ratio of the number of correctly reported co-references (for each mention) to the total number; while recall computes the fraction of actual co-references correctly identified. Both the final precision and recall are computed by averaging over all mention groups. • φ3 -CEAF score (Luo, 2005): an alternate way of computing precision, recall, and F1 scores using the best 1-to-1 mapping between the equivalence classes obtained and those in the ground truth. The best mapping of ground-truth to output classes exhibits the highest mention overlap. All experiments were conducted on a 4 core Intel i5 2.50 GHz processor with 8 GB RAM running Ubuntu 12.04. 7.1 Parameter Tuning The use of external features extracted from KB’s (for mention groups) forms an integral part in the working of CROCS, and is represented by the choice of the threshold, θ. Given an input corpus, we now discuss the tu"
Q15-1002,W03-0405,0,0.810547,"Freebase, DBpedia, or Yago. • Scope: the neighborhood of the target considered for enrichment. It can either be restricted to the target itself or can consider co-occurring items (other mention groups connected to the target). • Match: involves mapping the target to one or more relevant items in the source, and can involve simple name queries to full-fledged NED based on relevance or score confidence. Existing methods generally consider individual mentions or local mention groups as target. Extended scopes like co-occurring entities based on automatic NER and IE techniques have been proposed (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008), but use only the input corpus as the enrichment source. Recent methods (Rahman & Ng, 2011a; Ratinov & Roth, 2012; Hajishirzi et al., 2013; Zheng et al., 2013) harness KB’s, but consider only local mention groups. Also, these methods rely on high-quality NED for mapping mentions to KB entries. In contrast, CROCS considers extended scopes that include mention groups along with cooccurring mention groups when tapping into KB’s. We make only weak assumptions on matching men18 tions against KB entities, by filtering on confidence and"
Q15-1002,P10-1142,0,0.154599,"etc., along with rich knowledge about their properties and relationships. Perhaps the most important value-adding component in this setting is the recognition and disambiguation of named entities in Web and user contents. Named Entity Disambiguation (NED) (see, e.g., (Cucerzan, 2007; Milne & Witten, 2008; Cornolti et al., 2013)) maps a mention string (e.g., a person name like “Bolt” or a noun phrase like “lightning bolt”) onto its proper entity if present in a KB (e.g., the sprinter Usain Bolt). A related but different task of co-reference resolution (CR) (see, e.g., (Haghighi & Klein, 2009; Ng, 2010; Lee et al., 2013)) identifies all mentions in a given text that refer to the same entity, including anaphoras such as “the president’s wife”, “the first lady”, or “she”. This task when extended to process an entire corpus is then known as cross-document co-reference resolution (CCR) (Singh et al., 2011). It takes as input a set of documents with entity mentions, and computes as output a set of equivalence classes over the entity mentions. This does not involve mapping mentions to the entities of a KB. Unlike NED, CCR can deal with long-tail or emerging entities that are not captured in the K"
Q15-1002,P04-1076,0,0.807551,"ago. • Scope: the neighborhood of the target considered for enrichment. It can either be restricted to the target itself or can consider co-occurring items (other mention groups connected to the target). • Match: involves mapping the target to one or more relevant items in the source, and can involve simple name queries to full-fledged NED based on relevance or score confidence. Existing methods generally consider individual mentions or local mention groups as target. Extended scopes like co-occurring entities based on automatic NER and IE techniques have been proposed (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008), but use only the input corpus as the enrichment source. Recent methods (Rahman & Ng, 2011a; Ratinov & Roth, 2012; Hajishirzi et al., 2013; Zheng et al., 2013) harness KB’s, but consider only local mention groups. Also, these methods rely on high-quality NED for mapping mentions to KB entries. In contrast, CROCS considers extended scopes that include mention groups along with cooccurring mention groups when tapping into KB’s. We make only weak assumptions on matching men18 tions against KB entities, by filtering on confidence and merely treating s"
Q15-1002,D10-1048,0,0.167328,"hem into plain text (using standard tools like (https://code.google.com/p/ boilerpipe/), (www.jsoup.org), etc.). It then uses the Stanford CoreNLP tool suite to detect mentions and anaphors (http://nlp. stanford.edu/software/). The detected 17 mentions are also tagged with coarse-grained lexical types (person, organization, location, etc.) by the Stanford NER Tagger (Finkel et al., 2005). This forms the input to the intra-document CR step, where we use the state-of-the-art open-source CR tool (based on multi-pass sieve algorithm) from Stanford to compute the local mention co-reference chains (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). The tagged texts and the local coreference chains are then passed to the second stage. This local CR step may produce errors (e.g., incorrect chaining of mentions or omissions) which propagate to the later stages. However, improving intra-document CR is orthogonal to our problem and thus out of the scope of this paper. Our experiments later show that CROCS is robust and produces highquality output even with moderate errors encountered during the local-CR stage. 4 Knowledge Enrichment The knowledge enrichment phase starts with the local co-reference chains"
Q15-1002,P11-1082,0,0.0290794,"Missing"
Q15-1002,C10-2121,0,0.752599,"KL divergence, etc.) on features, similar to intra-document CR. Recent works such as (Culotta et al., 2007; Singh et al., 2010; Singh et al., 2011) are based on probabilistic graphical models for jointly learning the mappings of all mentions into equivalence classes. The features for this learning task are essentially like the ones in local CR. Baron and Freedman (2008) proposed a CCR method involving full clustering coupled with statistical learning of parameters. However, this method does not scale to large corpora making it unsuitable for Web contents. A more light-weight online method by (Rao et al., 2010) performs well on large benchmark corpora. It is based on a streaming clustering algorithm, which incrementally adds mentions to clusters or merges mention groups into single clusters, and has linear time complexity; albeit with inferior clustering quality compared to advanced methods like spectral clustering. Several CCR methods have harnessed co-occurring entity mentions, especially for the task of disambiguating person names (Mann & Yarowsky, 2003; Niu et al., 2004; Chen & Martin, 2007; Baron & Freedman, 2008). However, these methods do not utilize knowledge bases, but use information extra"
Q15-1002,P11-1138,0,0.030663,"Missing"
Q15-1002,D12-1113,0,0.257211,"14; Revision batch 11/2014; Published 1/2015. 2015 Association for Computational Linguistics. ent kinds of linguistic features like syntactic paths between mentions, the distances between them, and their semantic compatibility as derived from cooccurrences in news and Web corpora (Haghighi & Klein, 2009; Lee et al., 2013). Some methods additionally use distant labels from knowledge bases (KB’s). Cluster-ranking and multi-sieve methods incrementally expand groups of mentions and exploit relatedness features derived from semantic types, alias names, and Wikipedia categories (Rahman & Ng, 2011a; Ratinov & Roth, 2012). The CCR task - computing equivalence classes across documents - is essentially a clustering problem using a similarity metric between mentions with features like those discussed above. However, standard clustering (e.g., k-means or EM variants, CLUTO, etc.) lacks awareness of the transitivity of co-reference equivalence classes and suffers from knowledge requirement of model dimensions. Probabilistic graphical models like Markov Logic networks (Richardson & Domingos, 2006; Domingos et al., 2007; Domingos & Lowd, 2009) or factor graphs (Loeliger, 2008; Koller & Friedman, 2009) take into consi"
Q15-1002,P11-1080,0,0.218036,"itten, 2008; Cornolti et al., 2013)) maps a mention string (e.g., a person name like “Bolt” or a noun phrase like “lightning bolt”) onto its proper entity if present in a KB (e.g., the sprinter Usain Bolt). A related but different task of co-reference resolution (CR) (see, e.g., (Haghighi & Klein, 2009; Ng, 2010; Lee et al., 2013)) identifies all mentions in a given text that refer to the same entity, including anaphoras such as “the president’s wife”, “the first lady”, or “she”. This task when extended to process an entire corpus is then known as cross-document co-reference resolution (CCR) (Singh et al., 2011). It takes as input a set of documents with entity mentions, and computes as output a set of equivalence classes over the entity mentions. This does not involve mapping mentions to the entities of a KB. Unlike NED, CCR can deal with long-tail or emerging entities that are not captured in the KB or are merely in very sparse form. State of the Art and its Limitations. CR methods, for co-references within a document, are generally based on rules or supervised learning using differ15 Transactions of the Association for Computational Linguistics, vol. 3, pp. 15–28, 2015. Action Editor: Hwee Tou Ng."
Q15-1002,W13-3517,0,0.317287,"trings. For a given m, the basic semsum of m, Sbasic (m), is defined as Sbasic (m) = {t ∈ sentence(m0 )|m0 ∈ M (m)} ∪ {t ∈ label(m0 )|m0 ∈ M (m)} where t are text tokens (words or phrases), sentence(m0 ) is the sentence in which mention m0 occurs, and label(m0 ) is the semantic label for m0 obtained from the KB. Note that Sbasic (m) is a bag of tokens, as different mentions in M (m) can obtain the same tokens or labels and there could be multiple occurrences of the same mention string in M (m) anyway. Prior works on CR (e.g., (Rahman & Ng, 2011a; Ratinov & Roth, 2012; Hajishirzi et al., 2013; Zheng et al., 2013)) and NED (e.g., (Cucerzan, 2007; Milne & Witten, 2008; Ratinov et al., 2011; Hoffart et al., 2011; Hajishirzi et al., 2013)) have considered such form of distant features. CROCS extends these previous methods by also considering distant features for co-occurring mention groups, and not just the group at hand. We now introduce a general framework for knowledge enrichment in our CCR setting. Strategies for knowledge enrichment involve decision making along the following dimensions: • Target: items (single mentions, local mention groups, or global mention groups across documents) for which seman"
Q15-1002,W11-1902,0,\N,Missing
Q16-1016,W04-2214,0,0.0291821,"ken Coherence. Feature f11 (ent i , ent j ) measures the coherence between the token contexts of two entity candidates enti and entj : f11 (enti , ent j ) =  WO tok-cxt(ent i ), tok-cxt(ent j ) f11 allows us to establish cross-dependencies among labels in our graphical model. For example, the two entities David Beckham and Manchester United are highly coherent as they share many tokens in their contexts, such as “champions”, “league”, “premier”, “cup”, etc. Thus, they should mapped jointly. 4.2 Domain Features We use WordNet domains, created by Miller (1995), Magnini and Cavagli (2000), and Bentivogli et al. (2004), to construct a taxonomy of 46 domains, including Politics, Economy, Sports, Science, Medicine, Biology, Art, Music, etc. We combine the domains with semantic types (classes of entities) provided by YAGO2, by assigning them to their respective domains. This is based on the manual assignment of WordNet synsets to domains, introduced by Magnini and Cavagli (2000), and Bentivogli et al. (2004), and extends to additional types in YAGO2. For example, Singer is assigned to Music, and Football Player to Football, a sub-domain of Sports. These types include the standard NER types Person (PER), Organi"
Q16-1016,E06-1002,0,0.0133756,"3). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work of Dill et al. (2003), Bunescu and Pasca (2006), Cucerzan (2007), and Milne and Witten (2008). More recent advances led to open-source tools like the Wikipedia Miner Wikifier (Milne and Witten, 2013), the Illinois Wikifier (Ratinov et al., 2011), Spotlight (Mendes et al., 2011), Semanticizer (Meij et al., 2012), TagMe (Ferragina and Scaiella, 2010; Cornolti et al., 2014), and AIDA (Hoffart et al., 2011) with its improved variant AIDA-light (Nguyen et al., 2014). We choose some, namely, Spotlight, TagMe and AIDA-light, as baselines for our experiments. These are the best-performing, publicly available systems for news and web texts. Most of"
Q16-1016,D07-1074,0,0.0792189,"e based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work of Dill et al. (2003), Bunescu and Pasca (2006), Cucerzan (2007), and Milne and Witten (2008). More recent advances led to open-source tools like the Wikipedia Miner Wikifier (Milne and Witten, 2013), the Illinois Wikifier (Ratinov et al., 2011), Spotlight (Mendes et al., 2011), Semanticizer (Meij et al., 2012), TagMe (Ferragina and Scaiella, 2010; Cornolti et al., 2014), and AIDA (Hoffart et al., 2011) with its improved variant AIDA-light (Nguyen et al., 2014). We choose some, namely, Spotlight, TagMe and AIDA-light, as baselines for our experiments. These are the best-performing, publicly available systems for news and web texts. Most of these methods co"
Q16-1016,de-marneffe-etal-2006-generating,0,0.0331227,"Missing"
Q16-1016,Q14-1037,0,0.590502,"maintains the uncertainty of both mention candidates (i.e., token spans) and entity candidates for competing mention candidates, and makes joint decisions, as opposed to fixing mentions before reasoning on their disambiguation. We present experiments with three major datasets: the CoNLL’03 collection of newswire articles, the ACE’05 corpus of news and blogs, and the ClueWeb’09-FACC1 corpus of web pages. Baselines that we compare J-NERD with include AIDAlight (Nguyen et al., 2014), Spotlight (Daiber et al., 2013), and TagMe (Ferragina and Scaiella, 2010), and the recent joint NER/NED method of Durrett and Klein (2014). J-NERD consistently outperforms these competitors in terms of both precision and recall. 2 Related Work NER: Detecting the boundaries of text spans that denote named entities has been mostly addressed by supervised CRF’s over word sequences (McCallum and Li, 2003; Finkel et al., 2005). The work of Ratinov and Roth (2009) improved these techniques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features"
Q16-1016,P05-1045,0,0.358918,"ge base. We present experiments with different kinds of texts from the CoNLL’03, ACE’05, and ClueWeb’09-FACC1 corpora. J-NERD consistently outperforms state-of-the-art competitors in end-to-end NERD precision, recall, and F1. 1 Introduction Motivation: Methods for Named Entity Recognition and Disambiguation, NERD for short, typically proceed in two stages: • At the NER stage, text spans of entity mentions are detected and tagged with coarse-grained types like Person, Organization, Location, etc. This is typically performed by a trained Conditional Random Field (CRF) over word sequences (e.g., Finkel et al. (2005)). • At the NED stage, mentions are mapped to entities in a knowledge base (KB) based on contextual similarity measures and the semantic coherence of the selected entities (e.g., Cucerzan (2014); Hoffart et al. (2011); Ratinov et al. (2011)). This two-stage approach has limitations. First, NER may produce false positives that can misguide NED. Second, NER may miss out on some entity mentions, and NED has no chance to compensate for these false negatives. Third, NED is not able to help NER, for example, by disambiguating “easy” mentions (e.g., of prominent entities with more or less unique name"
Q16-1016,C02-1130,0,0.0631354,"ditional output of the CRF’s are type tags for the recognized word spans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a K"
Q16-1016,magnini-cavaglia-2000-integrating,0,0.026281,"tok-cxt(tok i ) Entity-Entity Token Coherence. Feature f11 (ent i , ent j ) measures the coherence between the token contexts of two entity candidates enti and entj : f11 (enti , ent j ) =  WO tok-cxt(ent i ), tok-cxt(ent j ) f11 allows us to establish cross-dependencies among labels in our graphical model. For example, the two entities David Beckham and Manchester United are highly coherent as they share many tokens in their contexts, such as “champions”, “league”, “premier”, “cup”, etc. Thus, they should mapped jointly. 4.2 Domain Features We use WordNet domains, created by Miller (1995), Magnini and Cavagli (2000), and Bentivogli et al. (2004), to construct a taxonomy of 46 domains, including Politics, Economy, Sports, Science, Medicine, Biology, Art, Music, etc. We combine the domains with semantic types (classes of entities) provided by YAGO2, by assigning them to their respective domains. This is based on the manual assignment of WordNet synsets to domains, introduced by Magnini and Cavagli (2000), and Bentivogli et al. (2004), and extends to additional types in YAGO2. For example, Singer is assigned to Music, and Football Player to Football, a sub-domain of Sports. These types include the standard"
Q16-1016,W03-0430,0,0.0108238,"datasets: the CoNLL’03 collection of newswire articles, the ACE’05 corpus of news and blogs, and the ClueWeb’09-FACC1 corpus of web pages. Baselines that we compare J-NERD with include AIDAlight (Nguyen et al., 2014), Spotlight (Daiber et al., 2013), and TagMe (Ferragina and Scaiella, 2010), and the recent joint NER/NED method of Durrett and Klein (2014). J-NERD consistently outperforms these competitors in terms of both precision and recall. 2 Related Work NER: Detecting the boundaries of text spans that denote named entities has been mostly addressed by supervised CRF’s over word sequences (McCallum and Li, 2003; Finkel et al., 2005). The work of Ratinov and Roth (2009) improved these techniques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features and external dictionaries for further improvement. An alternative line of NER techniques is based on dictionaries of name-entity pairs, including nicknames, shorthand names, and paraphrases (e.g., “the first man on the moon”). The work of Ferragina and Scaiella (20"
Q16-1016,P13-1146,1,0.486139,"ed to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work of Dill et al. (2003), Bu"
Q16-1016,W14-1609,0,0.0189621,"hat are designed for the broader task of “Wikification” are not penalized by their (typically lower) performance on inputs other than proper entity mentions. 5.2 We compare J-NERD in its four variants (linear vs. tree and local vs. global) to various state-of-the-art NER/NED methods. For NER (i.e., mention boundaries and types) we use the recent version 3.4.1 of the Stanford NER Tagger7 (Finkel et al., 2005) and the recent version 2.8.4 of the Illinois Tagger8 (Ratinov and Roth, 2009) as baselines. These systems have NER benchmark results on CoNLL’03 that are as good as the result reported in Passos et al. (2014). We retrained this model by using the same corpus-specific training data that we use for J-NERD . For NED, we compared J-NERD against the following methods for which we obtained open-source software or could call a Web service: • Berkeley-entity (Durrett and Klein, 2014) uses a joint model for coreference resolution, NER and NED with linkage to Wikipedia. • AIDA-light (Nguyen et al., 2014) is an optimized variant of the AIDA system (Hoffart et al., 2011), based on YAGO2. It uses the Stanford tool for NER. • TagMe (Ferragina and Scaiella, 2010) is a Wikifier that maps mentions to entities or c"
Q16-1016,C10-1105,0,0.0148273,"s are type tags for the recognized word spans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and t"
Q16-1016,W09-1119,0,0.566404,"he ACE’05 corpus of news and blogs, and the ClueWeb’09-FACC1 corpus of web pages. Baselines that we compare J-NERD with include AIDAlight (Nguyen et al., 2014), Spotlight (Daiber et al., 2013), and TagMe (Ferragina and Scaiella, 2010), and the recent joint NER/NED method of Durrett and Klein (2014). J-NERD consistently outperforms these competitors in terms of both precision and recall. 2 Related Work NER: Detecting the boundaries of text spans that denote named entities has been mostly addressed by supervised CRF’s over word sequences (McCallum and Li, 2003; Finkel et al., 2005). The work of Ratinov and Roth (2009) improved these techniques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features and external dictionaries for further improvement. An alternative line of NER techniques is based on dictionaries of name-entity pairs, including nicknames, shorthand names, and paraphrases (e.g., “the first man on the moon”). The work of Ferragina and Scaiella (2010) and Mendes et al. (2011) are examples of dictionary-bas"
Q16-1016,P11-1138,0,0.311803,"n Motivation: Methods for Named Entity Recognition and Disambiguation, NERD for short, typically proceed in two stages: • At the NER stage, text spans of entity mentions are detected and tagged with coarse-grained types like Person, Organization, Location, etc. This is typically performed by a trained Conditional Random Field (CRF) over word sequences (e.g., Finkel et al. (2005)). • At the NED stage, mentions are mapped to entities in a knowledge base (KB) based on contextual similarity measures and the semantic coherence of the selected entities (e.g., Cucerzan (2014); Hoffart et al. (2011); Ratinov et al. (2011)). This two-stage approach has limitations. First, NER may produce false positives that can misguide NED. Second, NER may miss out on some entity mentions, and NED has no chance to compensate for these false negatives. Third, NED is not able to help NER, for example, by disambiguating “easy” mentions (e.g., of prominent entities with more or less unique names), and then using the entities and knowledge about them as enriched features for NER. Example: Consider the following sentences: David played for manu, real, and la galaxy. His wife posh performed with the spice girls. This is difficult fo"
Q16-1016,spitkovsky-chang-2012-cross,0,0.0238091,"niques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features and external dictionaries for further improvement. An alternative line of NER techniques is based on dictionaries of name-entity pairs, including nicknames, shorthand names, and paraphrases (e.g., “the first man on the moon”). The work of Ferragina and Scaiella (2010) and Mendes et al. (2011) are examples of dictionary-based NER. The work of Spitkovsky and Chang (2012) is an example of a large-scale dictionary that can be harnessed by such methods. An additional output of the CRF’s are type tags for the recognized word spans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., poli"
Q16-1016,C12-2133,1,0.759465,"ans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work"
Q16-1016,D11-1072,1,\N,Missing
Q16-1016,W10-3503,0,\N,Missing
W06-0503,W97-1002,0,0.0598499,"Missing"
W06-0503,P05-1060,0,0.0268976,"Missing"
W06-0503,P02-1006,0,0.0419221,"Missing"
W06-0503,1993.iwpt-1.22,0,0.0207396,"Missing"
W06-0503,C00-2136,0,0.0462961,"Missing"
W06-0503,C02-1154,0,0.0473363,"Missing"
W06-0503,C92-2082,0,0.0134602,"Missing"
W09-4407,J06-1003,0,0.0175676,"esult is then computed as # $ T v(s) v(t) wsd(t, s) = σ(t, s) α + (2) ||v(s) |v(t)|| Unlike standard word sense disambiguation setups, we prefer obtaining a weighted set of multiple possibly relevant senses rather than just the sense with the highest confidence score. We use α as a smoothing parameter: For higher values of α, the function tends towards a uniform distribution of scores among the relevant senses, i.e. among those with σ(t, s) = 1. Semantic Similarity For the semantic similarity measure, we do not rely on generic measures of semantic relatedness often described in the literature [1]. The purpose of this measure here is to identify only word senses that are identical or nearly identical (e.g. For each of these n-gram sets A1m , A3m , A2p , etc., we also consider the corresponding counter function a1m , a3m , a2p , etc., that counts how often the n-gram occurs in the example sentence in the respective relative  1 s1 = s2 position. Usually, this will either be 0 or 1, though     1 s1 , s2 in near-synonymy relationship an example sentence may also contain multiple occurrences of the word being described, so higher values sim(s1 , s2 ) = 1 s1 , s2 in hypernymy relations"
W09-4407,W02-0817,0,0.0300648,"ly obtain the respective output for any k "" &lt; k simply by pruning the ranked list generated for k. This can be very useful for interactive user interfaces. 4 Related Work Several means of generating example sentences for word senses have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a set of seed expressions to create appropriate queries to Web search engines. For example, for the fibre optic channel sense of word ‘channel ’, appropriate queries would be ‘optical fiber channel ’, ‘channel telephone ’, ‘transmission channel ’. This method works well when such multi-word constructions can be constructed and coul"
W09-4407,P91-1017,0,0.236739,"nd could be used to complement our approach. Another more recent approach [11] clusters words based on a dependency parse of a monolingual corpus. This means that for each word a set of similar words is available. One then tries to match example sentences from the corpus with example sentences already given in WordNet, taking into account the word similarities. Our approach uses a different strategy by relying on parallel corpora. The intuition that lexical ambiguities in parallel corpora can be resolved more easily has been used by a number of works on word sense disambiguation. Dagan et al. [3] provided an initial linguistic analysis of this hypothesis. Several studies [9, 5, etc.] then implemented this idea in word sense disambiguation algorithms. These approaches are similar to our work. They use simple heuristics on parallel corpora to arrive at sense-labelled data that can then be used for word sense disambiguation, while our approach relies on a word sense heuristic to create example sentences from a parallel corpus. With regards to the challenge of selecting the most valuable examples, Fujii et al. [8] proposed a method for choosing example sentences for word sense disambiguat"
W09-4407,W00-0801,0,0.0656467,"Missing"
W09-4407,mihalcea-2002-bootstrapping,0,0.0348802,"have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a set of seed expressions to create appropriate queries to Web search engines. For example, for the fibre optic channel sense of word ‘channel ’, appropriate queries would be ‘optical fiber channel ’, ‘channel telephone ’, ‘transmission channel ’. This method works well when such multi-word constructions can be constructed and could be used to complement our approach. Another more recent approach [11] clusters words based on a dependency parse of a monolingual corpus. This means that for each word a set of similar words is available. One then tries to match ex"
W09-4407,J03-1002,0,0.00278992,"curacy of sense-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of W"
W09-4407,S07-1016,0,0.0961,"Extraction In the example extraction step, we connect sentences from a corpus to word senses in a given sense inventory whenever we are sufficiently confident that the sentence is an example of the corresponding word being used in the respective sense. Conventional word sense disambiguation heuristics could be used to determine word senses for a monolingual text, and then the sentences in that text could be linked to the respective senses. Unfortunately, even the most sophisticated all-words disambiguation techniques are currently not reliable enough when a finegrained sense inventory is used [14]. The intuition behind our method is that, given a parallel text that has been word aligned, we can jointly look at both versions of the text and determine the most likely senses of certain words with significantly greater accuracy than for any single version of the text. After word alignment, we independently apply word sense disambiguation heuristics for each of the languages to obtain ranked lists of senses for each word. One then analyses to what degree the ranked lists for aligned words overlap. In many cases, this makes it possible to infer the sense of a word much more reliably than wit"
W09-4407,J03-3002,0,0.0431729,"d. One then analyses to what degree the ranked lists for aligned words overlap. In many cases, this makes it possible to infer the sense of a word much more reliably than with conventional disambiguation heuristics. In such a case, we can use the respective sentence in which it occurs as an example sentence for that sense. Lexical Alignment In the past, parallel corpora had been rather difficult to obtain. This has changed with the increasing multilinguality of the Web as well as the greater demand for such resources resulting from the rise of statistical machine translation. Resnik and Smith [15] showed that the Web can be mined to obtain parallel corpora, while Tiedemann [21] built such corpora from sources such as movie subtitles and manuals of open source software. To compare the senses of words in both versions of a text, such parallel corpora first need to be wordaligned. This means that occurrences of terms (individual words or possibly lexicalized multi-word expressions) in one language need to be connected to the corresponding occurrences of semantically equivalent terms in the document for the other language. This is usually accomplished by first aligning sentences, and then"
W09-4407,W98-0709,0,0.0481438,"ora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When linking words in the corpus to this inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method. The results were generalized using Wilson score intervals, and are presented in Table 1. The smoothing parameter α from Section 2"
W09-4407,shinnou-sasaki-2008-division,0,0.102864,"in the case of highly frequent words such as conjunctions), we may 43 also choose to let the algorithm run on a smaller random sample X "" ⊂ X of sentences as input. A useful feature of this greedy algorithm is that it allows emitting a ranked list of entities. Having run the algorithm for a large k, perhaps even k = ∞, we can easily obtain the respective output for any k "" &lt; k simply by pruning the ranked list generated for k. This can be very useful for interactive user interfaces. 4 Related Work Several means of generating example sentences for word senses have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a"
W09-4407,E03-1026,0,0.01357,"-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When li"
W09-4407,J98-4002,0,0.0523455,"s been used by a number of works on word sense disambiguation. Dagan et al. [3] provided an initial linguistic analysis of this hypothesis. Several studies [9, 5, etc.] then implemented this idea in word sense disambiguation algorithms. These approaches are similar to our work. They use simple heuristics on parallel corpora to arrive at sense-labelled data that can then be used for word sense disambiguation, while our approach relies on a word sense heuristic to create example sentences from a parallel corpus. With regards to the challenge of selecting the most valuable examples, Fujii et al. [8] proposed a method for choosing example sentences for word sense disambiguation systems. Unlike our approach, which aims at representative examples for end users, their approach aims at examples likely to be useful for training a disambiguation system. Their proposal selects example sentences that are hard to classify automatically due to the associated uncertainty, so particularly clear examples of a word’s use are in fact less likely to get elected. Rychly et al. [17] presented a semi-supervised selection system that learns scores based on combinations of weak classifiers. These classifiers"
W09-4407,tiedemann-nygaard-2004-opus,0,0.163714,"many cases, this makes it possible to infer the sense of a word much more reliably than with conventional disambiguation heuristics. In such a case, we can use the respective sentence in which it occurs as an example sentence for that sense. Lexical Alignment In the past, parallel corpora had been rather difficult to obtain. This has changed with the increasing multilinguality of the Web as well as the greater demand for such resources resulting from the rise of statistical machine translation. Resnik and Smith [15] showed that the Web can be mined to obtain parallel corpora, while Tiedemann [21] built such corpora from sources such as movie subtitles and manuals of open source software. To compare the senses of words in both versions of a text, such parallel corpora first need to be wordaligned. This means that occurrences of terms (individual words or possibly lexicalized multi-word expressions) in one language need to be connected to the corresponding occurrences of semantically equivalent terms in the document for the other language. This is usually accomplished by first aligning sentences, and then using global cooccurrence-based statistics to connect words of two corresponding s"
W09-4407,1992.tmi-1.9,0,0.352786,"Missing"
W12-3008,D11-1135,0,\N,Missing
W12-3008,D11-1142,0,\N,Missing
W12-3008,W04-3205,0,\N,Missing
W14-3626,E06-1002,0,0.168994,"Missing"
W14-3626,D07-1074,0,0.17167,"Missing"
W14-3626,P13-1153,0,0.176025,"Missing"
W14-3626,D11-1072,1,0.912762,"Missing"
W14-3626,P11-1138,0,0.0610444,"Missing"
W15-3224,W14-1604,0,0.035823,"Missing"
W15-3224,P02-1051,0,0.134231,"Missing"
W15-3224,attia-etal-2010-automatically,0,0.024271,"Missing"
W15-3224,P13-1153,0,0.0738556,"med the quality of the generated data. We are making EDRAK publicly available as a valuable resource to help advance research in Arabic NLP and IR tasks such as dictionary-based NamedEntity Recognition, entity classification, and entity summarization. 1 Introduction 1.1 Motivation Rich structured resources are crucial for several Information Retrieval (IR) and NLP tasks; furthermore, resources quality significantly influence the performance of those tasks. For example, building a dictionary-based Named Entity Recognition (NER) system, requires a comprehensive and accurate dictionary of names (Darwish, 2013; Shaalan, 2014). Problems like Word Sense Disambiguation (WSD) and Named Entity Disambiguation (NED) require name and context dictionaries to resolve the correct word sense or entity respectively (Weikum et al., 2012). Arabic digital content is growing very rapidly; it is among the top growing languages on the Internet 1 . However, the amount of structured or semi1 structured Arabic content is lagging behind. For example, Wikipedia is one of the main resources from where many modern Knowledge Bases (KB) are extracted. It is heavily used in the literature for IR and NLP tasks. However, the siz"
W15-3224,P10-4002,0,0.0190338,"Missing"
W15-3224,D11-1072,1,0.82888,"Missing"
W15-3224,P14-2034,0,0.0322599,"ns SMT Found? Persons SMT ALL CMUQ-Ar. Wikipedia 28,493 33,962 34,116 79,699 62,609 128,790 Both 62,455 113,815 191,399 Table 1: Entity Names SMT Training Data Size Yes Yes No Combined SMT Found? • Name-level post-processing We postprocessed the data by applying normalization and data cleaning. (e.g. removing punctuation and URLs). No Combined SMT • Mapping to EDRAK Entities We used Wikipedia pages URLs to map extracted names from GW2C to EDRAK’s Entity repository. Arabic Name Figure 1: Architecture of Type-Aware Names Translation System (Pasha et al., 2014) or Stanford Arabic Word Segmenter (Monroe et al., 2014) should be used to perform morphological-based pre-processing. Stanford Word Segmenter provides interpolatable handy Java API, hence has been used to pre-process the data. Text has been segmented by separating clitics, and normalized by Removing Tatweel, Normalizing Digits, Normalizing Alif, and Removing Diacritics. This helps achieving better coverage for our data, and computing more accurate statistics. 4.3 NON-PER External Names Dictionaries EDRAK harnesses Google-Word-to-Concept (GW2C) (Spitkovsky and Chang, 2012) multilingual resource in order to capture more names from the web. GW2C is c"
W15-3224,pasha-etal-2014-madamira,0,0.0904915,"Missing"
W15-3224,J14-2008,0,0.0250554,"of the generated data. We are making EDRAK publicly available as a valuable resource to help advance research in Arabic NLP and IR tasks such as dictionary-based NamedEntity Recognition, entity classification, and entity summarization. 1 Introduction 1.1 Motivation Rich structured resources are crucial for several Information Retrieval (IR) and NLP tasks; furthermore, resources quality significantly influence the performance of those tasks. For example, building a dictionary-based Named Entity Recognition (NER) system, requires a comprehensive and accurate dictionary of names (Darwish, 2013; Shaalan, 2014). Problems like Word Sense Disambiguation (WSD) and Named Entity Disambiguation (NED) require name and context dictionaries to resolve the correct word sense or entity respectively (Weikum et al., 2012). Arabic digital content is growing very rapidly; it is among the top growing languages on the Internet 1 . However, the amount of structured or semi1 structured Arabic content is lagging behind. For example, Wikipedia is one of the main resources from where many modern Knowledge Bases (KB) are extracted. It is heavily used in the literature for IR and NLP tasks. However, the size of the Arabic"
W15-3224,spitkovsky-chang-2012-cross,0,0.0457625,"Missing"
W15-3224,R11-1015,0,0.0593637,"Missing"
W15-3224,P13-4023,1,0.906624,"Missing"
W15-3224,W14-3626,1,0.84816,"nd useful for semantic analysis tasks. Others are purely textual dictionaries without any notion of canonical entities. 2.1 Entity-Aware Resources Wikipedia, as the largest comprehensive online encyclopedia, is the most used corpus for creating entity-aware resources such as YAGO (Hoffart et al., 2013), DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2008). Due to the limited size of Arabic Wikipedia, building strong semantic resources becomes a challenge. Several research efforts have been exerted to go beyond Arabic Wikipedia to construct a rich entity-aware resource. AIDArabic (Yosef et al., 2014) is an NED system for Arabic text that uses an entity-name dictionary and an entity-context catalog extracted from Wikipedia. They leveraged Wikipedia titles, disambiguation pages, redirects, and incoming anchor texts to populate the entity-name dictionary. In addition, Wikipedia categories, incoming Wikipedia links page titles, and outgoing anchor texts were used in building the entity-context catalog. In order to overcome the small size of Arabic Wikipedia, they proposed building an entity catalog including entities from both the English and Arabic Wikipedia’s. While their catalog was compre"
W15-3224,N13-1046,0,\N,Missing
W15-3224,C12-2133,1,\N,Missing
W15-3811,P08-3009,0,0.0337286,"rd sense disambiguation (WSD) of free text. For the general domain, the vast body of work has been surveyed by Navigli (2009), and mature software tools such as It Makes Sense (Zhong and Ng, 2010) covers most words. For the biomedical domain, the majority of previous works center around two WSD datasets (Weeber et al., 2001; Jimeno-Yepes et al., 2011) that together contain 253 ambiguous words, multi-word terms, and abbreviations. In addition, Stevenson et al. (2008), Fan et al. (2009), and Cheng et al. (2012) propose methods to generate labeled data. As for methodologies, vector space models (McInnes, 2008; Savova et al., 2008) are a common choice. Another common approach is to exploit the rich knowledge embedded in UMLS. Agirre et al. (2010) and Humphrey et al. (2006) leverage entity-entity relations and semantic type information in UMLS, respectively. Entity disambiguation is another highly relevant research area. For the general domain, most efforts focus on named entities, and software systems such as AIDA (Hoffart et al., 2011) and Wikifier (Ratinov et al., 2011) are both robust and scalable. In contrast, for the biomedical domain, existing works target restricted scopes such as species (W"
W15-3811,P11-1138,0,0.040825,"(2008), Fan et al. (2009), and Cheng et al. (2012) propose methods to generate labeled data. As for methodologies, vector space models (McInnes, 2008; Savova et al., 2008) are a common choice. Another common approach is to exploit the rich knowledge embedded in UMLS. Agirre et al. (2010) and Humphrey et al. (2006) leverage entity-entity relations and semantic type information in UMLS, respectively. Entity disambiguation is another highly relevant research area. For the general domain, most efforts focus on named entities, and software systems such as AIDA (Hoffart et al., 2011) and Wikifier (Ratinov et al., 2011) are both robust and scalable. In contrast, for the biomedical domain, existing works target restricted scopes such as species (Wang et al., 2010) and acronyms (Harmston et al., 2012). Although MetaMap (Aronson and Lang, 2010) covers all the diverse entities in UMLS, its entity disambiguation functionality remains limited. Table 1: Semantic types for the target words culture and degree. tions – for example, labeling “process” in “monitoring of the carcinogenic process” as body function – can in turn enhance the coverage and quality of information extraction tasks. 1.2 Approach and contribution"
W15-3811,W12-2429,0,0.0310394,"formative semantic type we propose. The problem setting closest to word usage detection is undoubtedly word sense disambiguation (WSD) of free text. For the general domain, the vast body of work has been surveyed by Navigli (2009), and mature software tools such as It Makes Sense (Zhong and Ng, 2010) covers most words. For the biomedical domain, the majority of previous works center around two WSD datasets (Weeber et al., 2001; Jimeno-Yepes et al., 2011) that together contain 253 ambiguous words, multi-word terms, and abbreviations. In addition, Stevenson et al. (2008), Fan et al. (2009), and Cheng et al. (2012) propose methods to generate labeled data. As for methodologies, vector space models (McInnes, 2008; Savova et al., 2008) are a common choice. Another common approach is to exploit the rich knowledge embedded in UMLS. Agirre et al. (2010) and Humphrey et al. (2006) leverage entity-entity relations and semantic type information in UMLS, respectively. Entity disambiguation is another highly relevant research area. For the general domain, most efforts focus on named entities, and software systems such as AIDA (Hoffart et al., 2011) and Wikifier (Ratinov et al., 2011) are both robust and scalable."
W15-3811,C08-1102,0,0.0275427,"terms, some of which essentially entail the uninformative semantic type we propose. The problem setting closest to word usage detection is undoubtedly word sense disambiguation (WSD) of free text. For the general domain, the vast body of work has been surveyed by Navigli (2009), and mature software tools such as It Makes Sense (Zhong and Ng, 2010) covers most words. For the biomedical domain, the majority of previous works center around two WSD datasets (Weeber et al., 2001; Jimeno-Yepes et al., 2011) that together contain 253 ambiguous words, multi-word terms, and abbreviations. In addition, Stevenson et al. (2008), Fan et al. (2009), and Cheng et al. (2012) propose methods to generate labeled data. As for methodologies, vector space models (McInnes, 2008; Savova et al., 2008) are a common choice. Another common approach is to exploit the rich knowledge embedded in UMLS. Agirre et al. (2010) and Humphrey et al. (2006) leverage entity-entity relations and semantic type information in UMLS, respectively. Entity disambiguation is another highly relevant research area. For the general domain, most efforts focus on named entities, and software systems such as AIDA (Hoffart et al., 2011) and Wikifier (Ratinov"
W15-3811,D11-1072,1,0.823004,"Missing"
W15-3811,P10-4014,0,0.0165534,"rature metric for bending stage in progression (e.g. second degree burn) academic degree degree of freedom in statistics edges out of a node in a graph generic, uninformative a knowledge acquisition application. Their work makes a point in analyzing “semantically poor” terms, some of which essentially entail the uninformative semantic type we propose. The problem setting closest to word usage detection is undoubtedly word sense disambiguation (WSD) of free text. For the general domain, the vast body of work has been surveyed by Navigli (2009), and mature software tools such as It Makes Sense (Zhong and Ng, 2010) covers most words. For the biomedical domain, the majority of previous works center around two WSD datasets (Weeber et al., 2001; Jimeno-Yepes et al., 2011) that together contain 253 ambiguous words, multi-word terms, and abbreviations. In addition, Stevenson et al. (2008), Fan et al. (2009), and Cheng et al. (2012) propose methods to generate labeled data. As for methodologies, vector space models (McInnes, 2008; Savova et al., 2008) are a common choice. Another common approach is to exploit the rich knowledge embedded in UMLS. Agirre et al. (2010) and Humphrey et al. (2006) leverage entity-"
W16-1311,N07-4013,0,0.0323896,"s. Moreover, the intuition behind using CSK is that humans innately interpolate visual or textual information with associated latent knowledge for analysis and understanding. Hence we believe that leveraging CSK in addition to textual and visual information would take results closer to human users’ preferences. In order to use such background knowledge, curating a CSK knowledge base is of primary importance. Since automatic acquisition of canonicalized CSK from the web can be costly, we conjecture that noisy subject-predicate-object (SPO) triples extracted through Open Information Extraction (Banko et al., 2007) may be used as CSK. We hypothesize that the combination of the noisy ingredients – CSK, object-classes, and textual descriptions – would create an ensemble effect providing for efficient search and retrieval. We describe the components of our architecture in the following sections. 3.1 Data, Knowledge and Features We consider a document x from a collection X with two kinds of features: 59 • Visual features xvj : labels of object classes recognized in the image, including their hypernyms (e.g., king cobra, cobra, snake). • Textual features xxj : words that occur in the text that accompanies th"
W16-1311,D11-1142,0,0.0257325,"Missing"
W16-1311,E12-1076,0,0.0644102,"Missing"
W16-1311,W10-0721,0,0.034833,"rch by providing additional visual cues. We exploit the domain restriction facility of Google search (query string site:domain name) to get Google search results explicitly on our dataset. The OpenIE tool ReVerb (Fader et al., 2011) run against our corpus produces around 1 million noisy SPO triples. After filtering with our basic language model we have ~22,000 moderately clean assertions. Image Dataset: For the purpose of experiments we construct our own image dataset. ~50,000 images with descriptions are collected from the following datasets: Flickr30k (Young et al., 2014), Pascal Sentences (Rashtchian et al., 2010), SBU Captioned Photo Dataset (Ordonez et al., 2011), and MSCOCO (Lin et al., 2014). The images are collected by comparing their textual descriptions with our basic language model for Tourism. An existing object detection algorithm – LSDA (Hoffman et al., 2014) – is used for object detection in the images. The detected object classes are based on the 7000 leaf nodes of ImageNet (Deng et al., 2009). We also expand these classes by adding their super-classes or hypernyms with the same confidence score. Query Benchmark: We construct a benchmark of 20 queries from co-occurring Flickr tags from the"
W16-1311,W14-5406,0,0.0266502,"arious configurations. Our approach substantially improves the query result quality. 2 Related Work Existing Commonsense Knowledge Bases: Traditionally commonsense knowledge bases were curated manually through experts (Lenat, 1995) or through crowd-sourcing (Singh et al., 2002). Modern methods of CSK acquisition are automatic, either from test corpora (Liu and Singh, 2004) or from the web (Tandon et al., 2014). Vision and NLP: Research at the intersection of Natural Language Processing and Computer Vision is in limelight in the recent past. There have been work on automatic image annotations (Wang et al., 2014), description generation (Vinyals et al., 2014; Ordonez et al., 2011; Mitchell et al., 2012), scene understanding (Farhadi et al., 2010), image retrieval through natural language queries (Malinowski and Fritz, 2014) etc. Commonsense knowledge from text and vision: There have been attempts for learning CSK from real images (Chen et al., 2013) as well as from nonphoto-realistic abstractions (Vedantam et al., 2015). Recent work have also leveraged CSK for visual verification of relational phrases (Sadeghi et al., 2015) and for non-visual tasks like fill-in-the-blanks by intelligent agents (Lin an"
W16-1311,Q14-1006,0,0.0178306,"nded Google Know2Look Google in its search by providing additional visual cues. We exploit the domain restriction facility of Google search (query string site:domain name) to get Google search results explicitly on our dataset. The OpenIE tool ReVerb (Fader et al., 2011) run against our corpus produces around 1 million noisy SPO triples. After filtering with our basic language model we have ~22,000 moderately clean assertions. Image Dataset: For the purpose of experiments we construct our own image dataset. ~50,000 images with descriptions are collected from the following datasets: Flickr30k (Young et al., 2014), Pascal Sentences (Rashtchian et al., 2010), SBU Captioned Photo Dataset (Ordonez et al., 2011), and MSCOCO (Lin et al., 2014). The images are collected by comparing their textual descriptions with our basic language model for Tourism. An existing object detection algorithm – LSDA (Hoffman et al., 2014) – is used for object detection in the images. The detected object classes are based on the 7000 leaf nodes of ImageNet (Deng et al., 2009). We also expand these classes by adding their super-classes or hypernyms with the same confidence score. Query Benchmark: We construct a benchmark of 20 qu"
W16-1311,W10-0707,0,\N,Missing
W16-2909,D13-1186,0,0.023775,"es and Aronson (2010). In terms of evaluation, two gold standards, NLM WSD (Weeber et al., 2001) and MSH WSD (Jimeno-Yepes et al., 2011), are available. When it comes to disambiguating only specific or highly specialized entities, a large body of work exists. To name a few representative specializations, there are works that disambiguate between species of genes (Harmston et al., 2012; Wang et al., 2010); chemicals (Batista-Navarro et al., 2015; Leaman et al., 2015); diseases (D’Souza and Ng, 2015); entities in clinical notes (Kang et al., 2012); and coarse entity types (Siu and Weikum, 2015; Jindal and Roth, 2013; Cohen et al., 2011). cal resources: • MEDLINE abstracts are a large corpus indexed with rich, manually assigned MeSH (Medical Subject Heading) terms; we safely consider all MeSH terms to be accurate. In addition, since abstracts are very compactly written, their content rarely strays away from the biomedical domain. In other words, nonbiomedical entities occur only rarely. • UMLS is the authoritative and comprehensive knowledge base of the biomedical domain covering all aspects of the domain, with a vast collection of entities plus their lexical variations, semantic types, and inter-relation"
W16-2909,W15-3811,1,0.823692,"compared by JimenoYepes and Aronson (2010). In terms of evaluation, two gold standards, NLM WSD (Weeber et al., 2001) and MSH WSD (Jimeno-Yepes et al., 2011), are available. When it comes to disambiguating only specific or highly specialized entities, a large body of work exists. To name a few representative specializations, there are works that disambiguate between species of genes (Harmston et al., 2012; Wang et al., 2010); chemicals (Batista-Navarro et al., 2015; Leaman et al., 2015); diseases (D’Souza and Ng, 2015); entities in clinical notes (Kang et al., 2012); and coarse entity types (Siu and Weikum, 2015; Jindal and Roth, 2013; Cohen et al., 2011). cal resources: • MEDLINE abstracts are a large corpus indexed with rich, manually assigned MeSH (Medical Subject Heading) terms; we safely consider all MeSH terms to be accurate. In addition, since abstracts are very compactly written, their content rarely strays away from the biomedical domain. In other words, nonbiomedical entities occur only rarely. • UMLS is the authoritative and comprehensive knowledge base of the biomedical domain covering all aspects of the domain, with a vast collection of entities plus their lexical variations, semantic ty"
W16-2909,P15-2049,0,0.0336725,"Missing"
