2012.eamt-1.58,P05-1032,0,0.0202018,"ymbols that describe properties of source phrase words rather than target words. The statistical distribution of these abstract symbols in the phrase table allows for a much better choice of Huffman codes. 2 Related Work Zens and Ney (2007) describe a phrase table architecture on which the binary phrase table of Moses is based. The source phrase index consists of a prefix tree. Memory requirements are low due to ondemand loading. Disk space requirements however are substantial. Promising alternatives to the concept of fixed phrase tables are suffix-array based implementation of phrase tables (Callison-burch and Bannard, 2005; Zhang and Vogel, 2005; Lopez, 2008; Levenberg et al., 2010) that can create phrase pairs on-demand more or less directly from a parallel corpus. However, we do not compare this approach with ours, as we are not concerned with on-demand phrase table creation. Other approaches, based on phrase table filtering (Johnson et al., 2007) can be seen as a type of compression. They reduce the number of phrases in the phrase table by significance filtering and thus reduce space usage and improve translation quality at one stroke. An important advantage of this approach is that can be easily combined wi"
2012.eamt-1.58,guthrie-etal-2010-efficient,0,0.0609825,"e creation. Other approaches, based on phrase table filtering (Johnson et al., 2007) can be seen as a type of compression. They reduce the number of phrases in the phrase table by significance filtering and thus reduce space usage and improve translation quality at one stroke. An important advantage of this approach is that can be easily combined with any fixed phrase table, including ours. The architecture of the source phrase index of the discussed phrase table has been inspired by the efforts concerned with language model compression and randomized language models (Talbot and Brants, 2008; Guthrie et al., 2010). Guthrie et. al (2010) who describe a language model implementation based on a minimal perfected hash function and fingerprints generated with a random hash function is the greatest influence. The idea to use the CMPH2 library (Belazzougui et al., 2009) and MurmurHash33 for our phrase table implementation originates from that paper. The problem of parallel text compression has been addressed by only few works (NevillManning and Bell, 1992; Conley and Klein, 2008; Sanchez-Martinez et al., 2012), most other works are earlier variants of SanchezMartinez et al. (2012). Conley and Klein (2008) pro"
2012.eamt-1.58,D07-1103,0,0.19442,"(REnc), a novel method for the compression of word-aligned target language in phrase tables is presented. Combined with Huffman coding a relative size reduction of 56 percent for target phrase words and alignment data is achieved when compared to bare Huffman coding without R-Enc. In the context of the complete phrase table the size reduction is 22 percent. 1 Introduction As the size of available parallel corpora increases, the size of translation phrase tables used for statistical machine translation extracted from these corpora increases even faster. Although phrase table filtering methods (Johnson et al., 2007) have been described and physical memory as well as disk space are cheap, even current high-end systems can be pushed to their limits. The current inmemory representation of a phrase-table in Moses (Koehn et al., 2007), a widely used open-source statistical machine toolkit, is unusable for anything else but toy-size translation models or prefiltered test set data. A binary on-disk implementation of a phrase table is generally used, but its on-disk size requirements are significant.1 The goal of this paper is to describe the first steps towards a compact phrase table implementac 2012 European A"
2012.eamt-1.58,P07-2045,0,0.011466,"achieved when compared to bare Huffman coding without R-Enc. In the context of the complete phrase table the size reduction is 22 percent. 1 Introduction As the size of available parallel corpora increases, the size of translation phrase tables used for statistical machine translation extracted from these corpora increases even faster. Although phrase table filtering methods (Johnson et al., 2007) have been described and physical memory as well as disk space are cheap, even current high-end systems can be pushed to their limits. The current inmemory representation of a phrase-table in Moses (Koehn et al., 2007), a widely used open-source statistical machine toolkit, is unusable for anything else but toy-size translation models or prefiltered test set data. A binary on-disk implementation of a phrase table is generally used, but its on-disk size requirements are significant.1 The goal of this paper is to describe the first steps towards a compact phrase table implementac 2012 European Association for Machine Translation. 1 The need for a more compact phrase-table implementation arose during the author’s collaboration with the MT team at WIPO. The space requirements of the binary representations of th"
2012.eamt-1.58,N10-1062,0,0.0170109,"get words. The statistical distribution of these abstract symbols in the phrase table allows for a much better choice of Huffman codes. 2 Related Work Zens and Ney (2007) describe a phrase table architecture on which the binary phrase table of Moses is based. The source phrase index consists of a prefix tree. Memory requirements are low due to ondemand loading. Disk space requirements however are substantial. Promising alternatives to the concept of fixed phrase tables are suffix-array based implementation of phrase tables (Callison-burch and Bannard, 2005; Zhang and Vogel, 2005; Lopez, 2008; Levenberg et al., 2010) that can create phrase pairs on-demand more or less directly from a parallel corpus. However, we do not compare this approach with ours, as we are not concerned with on-demand phrase table creation. Other approaches, based on phrase table filtering (Johnson et al., 2007) can be seen as a type of compression. They reduce the number of phrases in the phrase table by significance filtering and thus reduce space usage and improve translation quality at one stroke. An important advantage of this approach is that can be easily combined with any fixed phrase table, including ours. The architecture o"
2012.eamt-1.58,C08-1064,0,0.0142548,"ther than target words. The statistical distribution of these abstract symbols in the phrase table allows for a much better choice of Huffman codes. 2 Related Work Zens and Ney (2007) describe a phrase table architecture on which the binary phrase table of Moses is based. The source phrase index consists of a prefix tree. Memory requirements are low due to ondemand loading. Disk space requirements however are substantial. Promising alternatives to the concept of fixed phrase tables are suffix-array based implementation of phrase tables (Callison-burch and Bannard, 2005; Zhang and Vogel, 2005; Lopez, 2008; Levenberg et al., 2010) that can create phrase pairs on-demand more or less directly from a parallel corpus. However, we do not compare this approach with ours, as we are not concerned with on-demand phrase table creation. Other approaches, based on phrase table filtering (Johnson et al., 2007) can be seen as a type of compression. They reduce the number of phrases in the phrase table by significance filtering and thus reduce space usage and improve translation quality at one stroke. An important advantage of this approach is that can be easily combined with any fixed phrase table, including"
2012.eamt-1.58,2011.mtsummit-plenaries.5,0,0.0620037,"Missing"
2012.eamt-1.58,P08-1058,0,0.0130022,"ith on-demand phrase table creation. Other approaches, based on phrase table filtering (Johnson et al., 2007) can be seen as a type of compression. They reduce the number of phrases in the phrase table by significance filtering and thus reduce space usage and improve translation quality at one stroke. An important advantage of this approach is that can be easily combined with any fixed phrase table, including ours. The architecture of the source phrase index of the discussed phrase table has been inspired by the efforts concerned with language model compression and randomized language models (Talbot and Brants, 2008; Guthrie et al., 2010). Guthrie et. al (2010) who describe a language model implementation based on a minimal perfected hash function and fingerprints generated with a random hash function is the greatest influence. The idea to use the CMPH2 library (Belazzougui et al., 2009) and MurmurHash33 for our phrase table implementation originates from that paper. The problem of parallel text compression has been addressed by only few works (NevillManning and Bell, 1992; Conley and Klein, 2008; Sanchez-Martinez et al., 2012), most other works are earlier variants of SanchezMartinez et al. (2012). Conl"
2012.eamt-1.58,N07-1062,0,0.017113,"ecompressed, decoded, and constructed as objects during run-time. As we show later, this does not necessarily mean that performance is negatively affected. Even better compression can be achieved with a dedicated encoding method of target words developed for translation phrase tables. Rank Encoding (R-Enc) exploits the fact that target phrase words can be reduced to abstract symbols that describe properties of source phrase words rather than target words. The statistical distribution of these abstract symbols in the phrase table allows for a much better choice of Huffman codes. 2 Related Work Zens and Ney (2007) describe a phrase table architecture on which the binary phrase table of Moses is based. The source phrase index consists of a prefix tree. Memory requirements are low due to ondemand loading. Disk space requirements however are substantial. Promising alternatives to the concept of fixed phrase tables are suffix-array based implementation of phrase tables (Callison-burch and Bannard, 2005; Zhang and Vogel, 2005; Lopez, 2008; Levenberg et al., 2010) that can create phrase pairs on-demand more or less directly from a parallel corpus. However, we do not compare this approach with ours, as we are"
2012.eamt-1.58,2005.eamt-1.39,0,0.0319122,"source phrase words rather than target words. The statistical distribution of these abstract symbols in the phrase table allows for a much better choice of Huffman codes. 2 Related Work Zens and Ney (2007) describe a phrase table architecture on which the binary phrase table of Moses is based. The source phrase index consists of a prefix tree. Memory requirements are low due to ondemand loading. Disk space requirements however are substantial. Promising alternatives to the concept of fixed phrase tables are suffix-array based implementation of phrase tables (Callison-burch and Bannard, 2005; Zhang and Vogel, 2005; Lopez, 2008; Levenberg et al., 2010) that can create phrase pairs on-demand more or less directly from a parallel corpus. However, we do not compare this approach with ours, as we are not concerned with on-demand phrase table creation. Other approaches, based on phrase table filtering (Johnson et al., 2007) can be seen as a type of compression. They reduce the number of phrases in the phrase table by significance filtering and thus reduce space usage and improve translation quality at one stroke. An important advantage of this approach is that can be easily combined with any fixed phrase tab"
2013.mtsummit-user.7,W11-2107,0,0.0253127,"Missing"
2013.mtsummit-user.7,2012.amta-papers.6,0,0.0224737,"Missing"
2013.mtsummit-user.7,W08-0509,0,0.0523461,"ng and scalability M rows Gb A tool that parallelizes all the preprocessing steps has been built. The input corpus is split into several parts and each step (tokenization, sentence alignment, filtering) is applied to each part in parallel. We set apart a collection of recent documents from the resulting parallel corpora for all language pairs. Out of these documents, we randomly select 1000 segments for testing and 2000 as a development set used later for weight optimization. As in a typical training procedure, the first step relies on computing word alignment models. For this we use MGIZA++ (Gao & Vogel 2008) to accelerate the process. Despite the multi-thread computations of MGIZA++, this step still takes about 2-3 days on a 8-core server, on average and for each language pair (Note that word alignment is slow because of generally long sentences). In the newest version of Moses, training scripts have been extensively parallelized, reducing the time needed for the remaining standard training steps to about one day. Before, this was the longest part which could take several days. Original binary versions of the phrase table and reordering model are huge; on average we observe sizes of around 20 Gb,"
2013.mtsummit-user.7,P07-1019,0,0.0414998,"tions. An example for English into Chinese is given in Table 2. We can see that instead of the initial 20 Gb, we end up with only 1.12 Gb of disk space consumed for each model. This means that during translation a similar amount of memory is being used by each translation process. Several translation directions can now be easily loaded into memory on a single server avoiding IO-bottlenecks completely. Besides making it possible to use large translation models on average-size servers, there are significant speed gains. Translation speed is further optimized by using the Cube-Pruning algorithm (Huang & Chiang, 2007) built into Moses. This (underadvertised) feature of Moses, in combination with in-memory storage, allows for increases in the average translation speed by an order of magnitude, compared to standard settings. Small losses in translation quality (fractions of one BLEU point) due to Cube-pruning are acceptable. 2.4 Architecture The training and decoding services are currently hosted on two cloud servers running instances of Linux. We have chosen to employ two Amazon Elastic Clouds servers with the so-called ""HighMemory Quadruple Extra Large Instance"" configuration.6 We first trained our models"
2013.mtsummit-user.7,ma-2006-champollion,0,0.31562,"|, |加纳""). Arabic is also quite challenging for SMT (see Soudi et al., 2012). We remove short vowels, and try to split the most common prefixes, which are usually not ambiguous when found at the beginning of a word: al-""""ال, wal- """"وال, lil- ""MNNNO"", bi- """"ب, lia- """"ال, kal- ""لQNN“آ In addition, for other prefixes which could be ambiguous (like ""ka-""), we add a finite list of words where we force the split. e.g. ""XYQZTUVW ""آdiplomat, which is decomposed in the prefix ""ka-"" - as - ("" )""كand ""XYQZTUV ""دdiplomat 2) Sentence alignment We use a home-made adaptation of Champollion (Ma, 2006) where the tool first aligns sentences and then tries to split at a lower level (segment level) when the sentence contains more than 80 characters (the further step of word alignment 4 3 For technical reason the original language and the translated language are unknown when exporting the data, therefore we do not differentiate source and target direction (ie. the French texts translated into English from the English texts translated into French). http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?c atalogId=LDC2013T06 5 http://lucene.apache.org/core/old_versioned_docs/vers ions/3_0_0/api/contri"
2014.eamt-1.44,P05-1066,0,0.165628,"Missing"
2014.eamt-1.44,E03-1076,0,0.0590892,"pre-reordering is a recent addition. 2 German Compound Words German has the particularity to join individual words into compound words. This is a challenge for SMT as it generates OOV words and data sparseness. Especially patents “suffer” from compound words, e.g. a recent German patent was c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 1 The 10 publication languages under the Patent Cooperation Treaty are Arabic, Chinese, English, French, German, Japanese, Korean, Portuguese, Russian and Spanish. 217 Related Work Koehn and Knight (2003) use parallel texts to train a compound splitter: after aligning the segments, they search for possible splits where each part has a translation as one word in the target segment. POS-information is used as a filter. Popovi´c et al. (2006) experiment with two compound splitting methods (German-English): linguistic and corpusbased and reach similar results for both methods. Junczys-Dowmunt (2008) proposes high-accuracy methods for compound splitting. At WIPO, decompounding is also used in the inhouse developed tools for patent search, CLIR and PATENTSCOPE (Pouliquen and Mazenc, 2011). As our go"
2014.eamt-1.44,P07-1091,0,0.0200966,"nts seem to favour long sentences. Thus, the meaningful verb part may appear at the end of a long sentence, many words away from the subject. Phrase-based SMT is not capable of capturing such long-distance relationships and often fails to translate the verb entirely. 3.1 Related Work Many approaches for clause restructuring exist, we only refer to a few. For German, Collins et. al (2005) describe a syntactic parsing approach with manually written reordering rules for the parsed trees. Reordering rules inferred automatically from parse trees and word alignments, have been proposed for Chinese (Li et al., 2007). Syntactic parsing is resource-hungry and timeintensive and cannot be part of our pipeline. Less demanding approaches rely on part-of-speech taggers, see Popovi´c and Ney (2006) for manually written rules or Niehues and Kolss (2009) for automatically induced reordering pattern. 3.2 Our Method Our approach is a shallow one with manually written rules that rely on POS tags. These rules are combined with selection algorithms that are based on alignment data or if alignment data is unavailable on a maximum entropy classifier. Both, partof-speech tagger and the maximum entropy classifier, are part"
2014.eamt-1.44,W09-0435,0,0.0302859,"Missing"
2014.eamt-1.44,popovic-ney-2006-pos,0,0.0605449,"Missing"
2014.eamt-1.44,2011.mtsummit-plenaries.5,1,0.899526,"automatic evaluation results for both subsystems, for the pre-reordering mechanism manual evaluation results are reported. 1 2.1 Introduction German is one of the 10 official publication languages in which a Patent application can be filed at WIPO 1 . Among the European languages, German proves to be the most challenging one for WIPO’s in-house SMT system. In contrast to French, English, or Spanish, extensive preprocessing has to be applied when German is the source language. In this paper we will illustrate fragments of the Patent SMT pipeline deployed at WIPO that deal with these problems (Pouliquen and Mazenc, 2011). Decompounding has been an established part of the WIPO pipeline, verb structure pre-reordering is a recent addition. 2 German Compound Words German has the particularity to join individual words into compound words. This is a challenge for SMT as it generates OOV words and data sparseness. Especially patents “suffer” from compound words, e.g. a recent German patent was c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 1 The 10 publication languages under the Patent Cooperation Treaty are Arabic, Chinese, English, Fre"
2015.eamt-1.28,2013.mtsummit-user.7,1,0.89271,"Missing"
2015.eamt-1.28,P96-1041,0,0.247587,"ntext of larger SMT models and use this tool to compute 200 word classes from the target language data. The target language corpora are mapped to sequences of classes and 9-gram language model are estimated. The final phrase-tables of the larger models (EnglishFrench, English-Spanish) have been significance pruned (Johnson et al. 2007) for size reduction. In our experiments significance pruning results in no quality loss while reducing translation model size by a factor of 5. The standard 5-gram language models and the 9-gram word-class models are estimated with Modified Kneser-Ney smoothing (Chen and Goodman 1996, Heafield et al. 2013). To reduce size requirements, we use heavily quantized binary models with no noticeable quality reduction. Pruning is applied to all singleton ngrams with n equal to or greater than 3. 3.2 Attempts at domain adaptation We explore two model combination methods for both, translation models and language models: linear and log-linear interpolation. Log-linear model interpolation is natively supported in Moses via its feature function framework. Translation models and language models can be log-linearly interpolated just by adding them to the Moses configuration files. Param"
2015.eamt-1.28,P13-2071,0,0.0583886,"Missing"
2015.eamt-1.28,P13-2121,0,0.0151353,"els and use this tool to compute 200 word classes from the target language data. The target language corpora are mapped to sequences of classes and 9-gram language model are estimated. The final phrase-tables of the larger models (EnglishFrench, English-Spanish) have been significance pruned (Johnson et al. 2007) for size reduction. In our experiments significance pruning results in no quality loss while reducing translation model size by a factor of 5. The standard 5-gram language models and the 9-gram word-class models are estimated with Modified Kneser-Ney smoothing (Chen and Goodman 1996, Heafield et al. 2013). To reduce size requirements, we use heavily quantized binary models with no noticeable quality reduction. Pruning is applied to all singleton ngrams with n equal to or greater than 3. 3.2 Attempts at domain adaptation We explore two model combination methods for both, translation models and language models: linear and log-linear interpolation. Log-linear model interpolation is natively supported in Moses via its feature function framework. Translation models and language models can be log-linearly interpolated just by adding them to the Moses configuration files. Parameter tuning then choose"
2015.eamt-1.28,D07-1103,0,0.0224059,"OSM) is added to the phrase-based decoder. Class-based language models seem to be a good compromise between increased n-gram length and total model size. We use automatically calculated word cluster ids as classes. We had good experience with word2vec (Mikolov et al. 2012) in the context of larger SMT models and use this tool to compute 200 word classes from the target language data. The target language corpora are mapped to sequences of classes and 9-gram language model are estimated. The final phrase-tables of the larger models (EnglishFrench, English-Spanish) have been significance pruned (Johnson et al. 2007) for size reduction. In our experiments significance pruning results in no quality loss while reducing translation model size by a factor of 5. The standard 5-gram language models and the 9-gram word-class models are estimated with Modified Kneser-Ney smoothing (Chen and Goodman 1996, Heafield et al. 2013). To reduce size requirements, we use heavily quantized binary models with no noticeable quality reduction. Pruning is applied to all singleton ngrams with n equal to or greater than 3. 3.2 Attempts at domain adaptation We explore two model combination methods for both, translation models and"
2015.eamt-1.28,J10-4005,0,0.0301167,"Missing"
2015.eamt-1.28,ma-2006-champollion,0,0.0288499,"ocuments translated between January 2000 and October 2014 (ca. 20,000 documents for English, French and Spanish, about 400 documents for Russian/Arabic, see Table 1). The provided corpora have been extracted from original Word or PDF documents, identical IDs between languages allow to align documents for each language pair. We use an in-house (WIPO) sentence aligner. The tool processes each parallel text document and produces a set of aligned sentences after applying the following steps: • Sentence splitting • Tokenization • Sentence alignment with our sentences aligned (based on Champollion (Ma 2006)) — produces an “aligned-segment-matchingscore” • filtering out whole documents with an average-segment-matching-score below a given threshold • filtering out sets of consecutive segments having a low scores • filtering out sets of consecutive segments that are sorted by alphabetical order3 • filtering out sentences having only one word or more than 80 words, or a source/target word ratio more than 9 3 3.1 SMT system Baseline system The baseline SMT system consists of an extended Moses (Koehn et al. 2007) configuration. Durrani et al. (2013) report on improvements for various language pairs wh"
2015.eamt-1.28,P02-1040,0,0.0915343,"Missing"
2015.eamt-1.28,E12-1055,0,0.0263585,"Moses configuration files. Parameter tuning then chooses the appropriate interpolation weights which are actually feature weights. Linear interpolation, though a standard method for language models, is more involved. In the case of language models, we 203 compute a new static linearly interpolated language model from IMO and UN data target language data. Interpolation weights are optimized on the dev set. In the case of translation models we use a new feature function available in Moses that allows for setting up virtual phrase tables that are in fact linearly interpolated translation models (Sennrich 2012). We use the same interpolation weights as previously determined for linear language model interpolation. The two interpolated translation models are the original IMO and UN translation models as used in stand-alone translators. Results are mixed, we report the best results for our experiments (see Table 2, Section 5.1). Log-linear interpolation is downright harmful (and therefore omitted), for the larger language pairs (en-fr and en-es) any of the interpolation methods seem to be unhelpful, improvements for en-es are within the range of optimizer instability. For the smaller models (en-ar, en"
2016.amta-researchers.4,W16-2211,1,0.878391,"Missing"
2016.amta-researchers.4,J07-2003,0,0.664132,"ection 4. We conclude in the last section and discuss possible future work. 1.1 Prior Work Most prior work on increasing decoding speed look to optimizing speciﬁc components of the decoder or the decoding algorithm. Heaﬁeld (2011) and Yasuhara et al. (2013) describes fast, efﬁcient datastructures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model. Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heaﬁeld et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern´andez et al. (2016) was concerned with multi-core speed bu"
2016.amta-researchers.4,W11-2123,0,0.779462,"or existing users. The source code is available in the existing Moses repository1 . The rest of the paper will be broken up into the following sections. The rest of this section will discuss prior work and an outline of the phrase-based model. Section 2 will then describe the modiﬁcations to improve decoding speed. We describe the experiment setup in Section 3 and present results Section 4. We conclude in the last section and discuss possible future work. 1.1 Prior Work Most prior work on increasing decoding speed look to optimizing speciﬁc components of the decoder or the decoding algorithm. Heaﬁeld (2011) and Yasuhara et al. (2013) describes fast, efﬁcient datastructures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model. Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying"
2016.amta-researchers.4,P14-2022,0,0.504816,"aded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model. Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heaﬁeld et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern´andez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework. There are a number of phrase-based decoding implementations, many of which implements the extensions to the phrase-based model described above. The most well known is Moses (Koehn et al., 2007) which implements a number of speed optimizations, including cubepruning. It is widely used for MT research and commercial use. Joshua (Li et al., 2009) also s"
2016.amta-researchers.4,P07-2045,1,0.0124459,"ve operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heaﬁeld et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern´andez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework. There are a number of phrase-based decoding implementations, many of which implements the extensions to the phrase-based model described above. The most well known is Moses (Koehn et al., 2007) which implements a number of speed optimizations, including cubepruning. It is widely used for MT research and commercial use. Joshua (Li et al., 2009) also supports cube-pruning for phrase-based models. Phrasal (Spence Green and Manning, 2014) supports a number of variants of the phrase-based model. Jane (Wuebker et al., 2012a) supports the language model look-ahead described in Wuebker et al. (2012b) in addition to other tools to speed up decoding such as having a separate fast, lightweight decoder. mtplz is a specialized decoder developed to implement the incremental decoding described in"
2016.amta-researchers.4,W09-0424,1,0.804557,"o Heaﬁeld et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern´andez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework. There are a number of phrase-based decoding implementations, many of which implements the extensions to the phrase-based model described above. The most well known is Moses (Koehn et al., 2007) which implements a number of speed optimizations, including cubepruning. It is widely used for MT research and commercial use. Joshua (Li et al., 2009) also supports cube-pruning for phrase-based models. Phrasal (Spence Green and Manning, 2014) supports a number of variants of the phrase-based model. Jane (Wuebker et al., 2012a) supports the language model look-ahead described in Wuebker et al. (2012b) in addition to other tools to speed up decoding such as having a separate fast, lightweight decoder. mtplz is a specialized decoder developed to implement the incremental decoding described in Heaﬁeld et al. (2014). The Moses, Joshua and Phrasal decoders implement multithreading, however, they all report scalability problems, either in the pap"
2016.amta-researchers.4,W01-1408,0,0.065408,"eptably high memory usage so object queues are available for high-churn objects which allows the decoder to re-cycle unused objects before the reset event. This also increase speed as LIFO queues are used so that the most recently accessed memory are used, increasing CPU cache hits. 2.2 Stack Conﬁgurations The most popular stack conﬁguration for phrase-based models is coverage cardinality, that is, hypotheses that have translated the same number of source words are stored in the same stack. This is implemented in Pharaoh, Moses and Joshua. However, there are alternatives to this conﬁguration. Och et al. (2001) uses a single stack for all hypotheses, Brown et al. (1993) uses coverage stacks (ie. one stack per unique coverage vector) while Wuebker et al. (2012a) and Zens and Ney (2008) apply both coverage and cardinality pruning. While useful, these prior works present only one particular stack conﬁguration each. Ortiz-Mart´ınez et al. (2006) explore a range of stack conﬁgurations by deﬁning a granularity parameter which controls the maximum number of stacks required to decode a sentence. We shall re-visit the question of stack conﬁguration with a particular emphasis on how they can help improve the"
2016.amta-researchers.4,W06-3109,0,0.0838393,"Missing"
2016.amta-researchers.4,W14-3311,0,0.0468555,"Missing"
2016.amta-researchers.4,tiedemann-2012-parallel,0,0.0145478,"r-en 5.8 1.8 0.6 Table 2: Model sizes in GB ar-en fr-en For speed testing Set name Subset of training data # sentences 800k 200k # words 5.8m 5.9m Avg words/sent 7.3 29.7 For model score testing Set name OpenSubtitles newstest2011 # sentences 2000 3003 # words 14,620 86,162 Avg words/sent 7.3 28.7 Table 3: Test sets were not published. We will compare results with using a separate model in this paper. 3 Experimental Setup We trained a phrase-based system using the Moses toolkit with standard settings. The training data consisted of most of the publicly available Arabic-English data from Opus (Tiedemann, 2012) containing over 69 million parallel sentences, and tuned on a held out set. The phrase-table was then pruned, keeping only the top 100 entries per source phrase, according to p(t|s). All model ﬁles were then binarized; the language models were binarized using KenLM (Heaﬁeld, 2011), the phrase table using the Probing phrase-table, lexicalized reordering model using the compact data structure (Junczys-Dowmunt, 2012). These binary formats were chosen for their best-in-class multithreaded performance. Table 2 gives details of the resultant sizes of the model ﬁles. For testing decoding speed, we u"
2016.amta-researchers.4,C12-3061,0,0.28382,"of the decoder or the decoding algorithm. Heaﬁeld (2011) and Yasuhara et al. (2013) describes fast, efﬁcient datastructures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model. Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heaﬁeld et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern´andez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework. There are a number of phrase-based decoding implementations, many of which implements the extensions to the ph"
2016.amta-researchers.4,P12-2006,0,0.203839,"of the decoder or the decoding algorithm. Heaﬁeld (2011) and Yasuhara et al. (2013) describes fast, efﬁcient datastructures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model. Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heaﬁeld et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern´andez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework. There are a number of phrase-based decoding implementations, many of which implements the extensions to the ph"
2016.amta-researchers.4,D13-1023,0,0.0200433,"The source code is available in the existing Moses repository1 . The rest of the paper will be broken up into the following sections. The rest of this section will discuss prior work and an outline of the phrase-based model. Section 2 will then describe the modiﬁcations to improve decoding speed. We describe the experiment setup in Section 3 and present results Section 4. We conclude in the last section and discuss possible future work. 1.1 Prior Work Most prior work on increasing decoding speed look to optimizing speciﬁc components of the decoder or the decoding algorithm. Heaﬁeld (2011) and Yasuhara et al. (2013) describes fast, efﬁcient datastructures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model. Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensi"
2016.amta-researchers.4,N07-1062,0,0.577873,"er will be broken up into the following sections. The rest of this section will discuss prior work and an outline of the phrase-based model. Section 2 will then describe the modiﬁcations to improve decoding speed. We describe the experiment setup in Section 3 and present results Section 4. We conclude in the last section and discuss possible future work. 1.1 Prior Work Most prior work on increasing decoding speed look to optimizing speciﬁc components of the decoder or the decoding algorithm. Heaﬁeld (2011) and Yasuhara et al. (2013) describes fast, efﬁcient datastructures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model. Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score"
2016.amta-researchers.4,2008.iwslt-papers.8,0,0.031175,"speed as LIFO queues are used so that the most recently accessed memory are used, increasing CPU cache hits. 2.2 Stack Conﬁgurations The most popular stack conﬁguration for phrase-based models is coverage cardinality, that is, hypotheses that have translated the same number of source words are stored in the same stack. This is implemented in Pharaoh, Moses and Joshua. However, there are alternatives to this conﬁguration. Och et al. (2001) uses a single stack for all hypotheses, Brown et al. (1993) uses coverage stacks (ie. one stack per unique coverage vector) while Wuebker et al. (2012a) and Zens and Ney (2008) apply both coverage and cardinality pruning. While useful, these prior works present only one particular stack conﬁguration each. Ortiz-Mart´ınez et al. (2006) explore a range of stack conﬁgurations by deﬁning a granularity parameter which controls the maximum number of stacks required to decode a sentence. We shall re-visit the question of stack conﬁguration with a particular emphasis on how they can help improve the tradeoff between speed and translation quality. We will do so in the context of the cube-pruning algorithm, the algorithm that we will be using and which was not available to ma"
2021.emnlp-main.539,2020.acl-main.747,0,0.030344,"975). The state-of-the-art approach to this task is based on the Predictor-Estimator (PredEst) architecture (Kim et al., 2017; Kepler et al., 2019). At a very high level, the predictor training uses a crosslingual masked language model (MLM) objective, which trains the model to predict a word in the target sentence given the source and both the left and right context on the target side. An estimator is then finetuned from the predictor model to predict word-level QE tags. In recent years, the top-ranking systems also incorporate large-scale pre-trained crosslingual encoder such as XLMRoBERTa (Conneau et al., 2020), glass-box features (Moura et al., 2020) and pseudo post-editing data augmentation (Wei et al., 2020; Lee, 2020). The Levenshtein Transformer (LevT, Gu et al., 2019) is a neural network architecture that can iteratively generate sequences in a non-autoregressive manner. Unlike normal autoregressive sequence models that have only one prediction head Aw to predict the next output words, LevT has two extra prediction heads Adel and Ains that predicts deletion and insertion operations based on the output sequence from the previous iteration. For translation generation, at the k-th iteration durin"
2021.emnlp-main.539,2020.wmt-1.117,0,0.0681255,"Missing"
2021.humeval-1.11,W19-5301,1,0.887319,"Missing"
2021.humeval-1.11,2020.wmt-1.137,0,0.128088,". Castilho et al. (2020) have examined the necessary context span needed for evaluation across different domains, and for relatively short documents like news articles, the authors recommend presenting the whole document during the assessment of individual segments. Using document context has also been recommended by Toral (2020) who reported that this information was needed for evaluators to rank systems in a contrastive evaluation setting. Having the text available during the assessment of fluency or adequacy might be essential for some evaluators who spend more time reading than assessing (Castilho, 2020). Although the literature is consistent about the need of document context in human evaluation of MT, little research has been done on the impact of experimental design and user interfaces on annotator productivity and the reliability of assessments in this context. The existing research on experimental designs for machine translation evaluation focuses on contrasting direct assessments with pairwise rankings (Novikova et al., 2018; Sakaguchi and Van Durme, 2018) and not on the optimal presentation of the document-level information. However, Document context in human evaluation of MT outputs R"
2021.humeval-1.11,2020.lrec-1.461,0,0.0578943,"Missing"
2021.humeval-1.11,2004.iwslt-evaluation.1,0,0.137608,"Missing"
2021.humeval-1.11,C18-2019,1,0.838243,"translation more favourably if they cannot identify errors related to 98 even the simple UI design decision of aligning document translations on the sentence level impacts efficiency of some evaluators (Popovi´c, 2020). With this work, we want to promote that direction of research. 3 Statistic All L4 Document-level human evaluation campaigns at WMT During the WMT evaluation campaigns of 2019 and 2020, segment and document-level assessments of document translations were collected, but using different methods and thus user interfaces. Both were implemented in the Appraise evaluation framework (Federmann, 2018) as a source-based direct assessment task (Graham et al., 2013; Cettolo et al., 2017), i.e. all segments and entire documents were judged on a continuous scale between 0 and 100 by bilingual annotators. 3.1 WMT20 Annotators Seg. judgements Doc. judgements cs, de, fi, gu kk, lt, ru, zh 1,271 207,916 12,907 cs, de, iu, jp pl, ru, ta, zh 1,213 186,813 13,790 Languages Annotators Seg. judgements Doc. judgements cs, de, ru, zh 779 127,178 7,894 cs, de, ru, zh 746 115,571 10,019 Languages Table 1: Statistics of data from the WMT19 and WMT20 campaigns, including languages, the total number of annotat"
2021.humeval-1.11,W13-2305,0,0.0295728,"related to 98 even the simple UI design decision of aligning document translations on the sentence level impacts efficiency of some evaluators (Popovi´c, 2020). With this work, we want to promote that direction of research. 3 Statistic All L4 Document-level human evaluation campaigns at WMT During the WMT evaluation campaigns of 2019 and 2020, segment and document-level assessments of document translations were collected, but using different methods and thus user interfaces. Both were implemented in the Appraise evaluation framework (Federmann, 2018) as a source-based direct assessment task (Graham et al., 2013; Cettolo et al., 2017), i.e. all segments and entire documents were judged on a continuous scale between 0 and 100 by bilingual annotators. 3.1 WMT20 Annotators Seg. judgements Doc. judgements cs, de, fi, gu kk, lt, ru, zh 1,271 207,916 12,907 cs, de, iu, jp pl, ru, ta, zh 1,213 186,813 13,790 Languages Annotators Seg. judgements Doc. judgements cs, de, ru, zh 779 127,178 7,894 cs, de, ru, zh 746 115,571 10,019 Languages Table 1: Statistics of data from the WMT19 and WMT20 campaigns, including languages, the total number of annotators and collected segment-level and document-level scores, aft"
2021.humeval-1.11,2020.emnlp-main.6,0,0.125284,"ity of assessments in this context. The existing research on experimental designs for machine translation evaluation focuses on contrasting direct assessments with pairwise rankings (Novikova et al., 2018; Sakaguchi and Van Durme, 2018) and not on the optimal presentation of the document-level information. However, Document context in human evaluation of MT outputs Recent research emphasized the importance of document context in human evaluation of machine translation, especially in terms of accessing potential human parity or super-human performance (L¨aubli et al., 2018; Toral et al., 2018; Graham et al., 2020; Toral, 2020). Several works have compiled sets of recommendations for document-level evaluation. For example, Laubli et al. (2020) recommend evaluation of documents instead of independent sentences as translators tend to judge machine translation more favourably if they cannot identify errors related to 98 even the simple UI design decision of aligning document translations on the sentence level impacts efficiency of some evaluators (Popovi´c, 2020). With this work, we want to promote that direction of research. 3 Statistic All L4 Document-level human evaluation campaigns at WMT During the W"
2021.humeval-1.11,D18-1512,0,0.0376965,"Missing"
2021.humeval-1.11,N18-2012,0,0.0299627,"Missing"
2021.humeval-1.11,2020.coling-main.444,0,0.0372345,"Missing"
2021.humeval-1.11,P18-1020,0,0.0249078,"Missing"
2021.humeval-1.11,2020.eamt-1.20,0,0.189489,"ingle document translated by the same MT system were provided sequentially to evaluators in the order as they appear in the document, only one segment shown at a time (Fig. 1a), followed by the entire document comprised of already scored segments (Fig. 1b). WMT 2020 (Barrault et al., 2020) implemented a more document-centric approach, displaying the full translated document on a single screen (Fig. 1c) for most of the out-of-English language pairs. While the change was primarily about the user interface (UI), we believe it can impact the quality of document-level evaluation to a large extent. Toral (2020) has noticed potential issues arising from the limited inter-sentential context in the WMT19 method, in which the evaluator does not have continuous access to all segments from the document. Unable to revisit previous sentences and never seeing subsequent sentences, the evaluator might forget or lack access to important details necessary to rate the current segment. On the other hand, displaying a long document on a screen can notably increase cognitive load, potentially lowering reliability of assessments over time (Gonzalez et al., 2011), and increase annotation time and costs, especially at"
2021.naacl-main.92,Q19-1038,0,0.0211529,"Missing"
2021.naacl-main.92,W05-0909,0,0.0770283,"ection The HP detection algo- token vocabulary of 10K is applied over lowerrithm used is presented as algorithm 2. In practice, cased tokenized text. The NMT model is a sixalgorithm 2 is a specific instance of the algorithm layer Transformer model with embedding size 512, from Lee et al. (2018), wherein we make the fol- FFN layer dimension 1024 and 4 attention heads lowing three changes: (42M parameters), and the checkpoint with the best validation BLEU (detokenized, with beam=5) is 1 In practice, other MT metrics such as METEOR or BERTselected. In each case, a batch size of 4K tokens, Score (Banerjee and Lavie, 2005; Zhang et al., 2019) could also be used as empirical extensions of MVE for sequences, dropout of 0.3 and tied encoder-decoder embedhowever, word/character n-gram overlap provides a stronger dings is used. Then, the MVE (algorithm 1) is indication of memorization than soft-overlap methods like BERT-Score. applied on the training samples using the above t 1174 trained models to compute the memorization values, mem for each source sample i. For further analysis, we do not consider any sample which hasn’t been excluded from the random training sets at least twice. To generate HP we use algorithm"
2021.naacl-main.92,W19-5361,0,0.0180576,"perturbations, we compare the cross-attention heads of the last layer of the decoder for the Random and Memorized sets. Table 4 presents a comparison of the average entropy of the attention matrix, averaged diagonal attention and the average attention paid to the last source token, aggregated over the entire sets. The results show that the two sets differ considerably in terms of the attention distribution, with the memorized set having more fixed (lower-entropy) average attention distributions. Although this result is known for hallucinated translations (Lee et al., 2018; Voita et al., 2020; Berard et al., 2019), which have a tendency of producing deficient attention maps, the fact that this phenomenon extends to memorized samples as well further helps establish the link between memorization and hallucination under perturbation. Data-Type Attention Entropy Diagonal Attention Entropy Average Last Token Attention Memorized 1.85 1.48 0.35 Random 2.70 2.21 0.25 Table 4: Attention Statistics Comparison for Random vs Memorized Sets. 4.2 Natural Hallucinations Hypothesis 2 (H2) Corpus-level noise patterns (comprised of invalid source-target pairs) dictate the type of natural hallucinations generated by the"
2021.naacl-main.92,P19-1425,0,0.0179116,"inguistics: Human Language Technologies, pages 1172–1183 June 6–11, 2021. ©2021 Association for Computational Linguistics this section, we briefly survey the two areas. 2.1 Hallucinations in NMT The phenomena of hallucinations in NMT lack clear categorical definitions. Lee et al. (2018) define hallucinations as the model producing a vastly different (inadequate) output when the source is perturbed under a specific noise model and present an algorithm to detect such cases. Subsequently, approaches to making NMT models more robust to small perturbations in the input have been actively explored (Cheng et al., 2019), however, no coherent theory to explain the phenomena of hallucinations has been empirically validated in the existing literature. Our work differs from Lee et al. (2018) in that we not only study hallucinations under source side perturbations but also under corpus-level noise. Further, we build on their work by filling in the gap for a plausible hypothesis that explains various types of hallucinations. Wang and Sennrich (2020) consider hallucinations as outputs detached from the source, and demonstrate that NMT models are more prone to hallucinations under out-of-domain settings by manually"
2021.naacl-main.92,D18-1045,0,0.444982,"terized as being decoupled from the source sequence, despite being (fully or moderately) fluent in the target language (Müller et al., 2020). Two main hallucination phenomena have been reported in the existing literature: 2. We introduce corpus-level noise into NMT parallel corpora and show that specific noise patterns interact with sequence to sequence training dynamics in different ways to generate the prominent hallucination patterns reported in the literature (Lee et al., 2018). 3. We demonstrate the phenomenon of hallucination amplification in the outputs generated using Backtranslation (Edunov et al., 2018) and Knowledge Distillation (Kim and Rush, 2016), two widely used data generation algorithms for MT. 2 Related Work Our work connects hallucinations in NMT to the 1. NMT models tend to generate hallucinated problem of generalization in Deep Learning. In 1172 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1172–1183 June 6–11, 2021. ©2021 Association for Computational Linguistics this section, we briefly survey the two areas. 2.1 Hallucinations in NMT The phenomena of hallucinations in NMT lack"
2021.naacl-main.92,W18-6478,1,0.850528,"ples Table 9 presents some examples from the most memorized training samples, thereby representing the samples from the long-tail of the data that is likely to have been memorized by the model. Qualitatively, the examples appear to be different (in terms of source/target syntax) from a random subset of training samples (e.g. in Appendix A, Table 10), although we leave further quantitative analysis of the differences to future work. Similarly, the link between out-of-domain and memorized samples needs to be ascertained quantitatively. Corpus-Level Filtering Incorporating heuristics or filters (Junczys-Dowmunt, 2018; Zhang et al., 2020) to remove invalid source-target pairs, especially the noise patterns explored in section 4.2 (or to remove bitext indeterminacy in general) could be effective in reducing natural hallucinations. 7 Conclusion In this work we demonstrated that memorized training samples are far more likely to hallucinate under perturbation than non-memorized samples, under an extension of the Memory Value Estimator proposed in Feldman and Zhang (2020). We also 6.2 Preventing Hallucinations showed that specific noise patterns in the training In this subsection, we discuss a few methods that"
2021.naacl-main.92,2020.acl-main.66,0,0.022883,"pora; which, according to the cenAmeliorating Memorization During Learning tral thesis of the long-tail theory (Feldman, 2020), Robust learning algorithms e.g. Robust Early learnwould lead to memorizations. Similarly, noise in ing (Xia et al., 2021) that are designed to prevent the form of invalid references is an artifact of the memorization specifically are likely to prevent perscale at which web-based corpora are collected and turbation based hallucinations. given that both hallucinations under perturbations and natural hallucinations are widely reported in Robust Learning on Noisy Samples Kang and Hashimoto (2020) propose a loss-truncation ap- large-scale NMT systems, our insights should be directly applicable to larger-scale models as well. proach to reduce the impact of noisy references in sequence-to-sequence training, using the intermeWe hope that our work serves as a useful step diate model’s loss as a sample quality estimator towards a detailed understanding of hallucinations and test their algorithm on a summarization task. in NMT and in other sequence to sequence modLi et al. (2021) present a modification to Expected els. Among the numerous interesting directions for Risk Minimization (ERM), na"
2021.naacl-main.92,D16-1139,0,0.135414,"ce, despite being (fully or moderately) fluent in the target language (Müller et al., 2020). Two main hallucination phenomena have been reported in the existing literature: 2. We introduce corpus-level noise into NMT parallel corpora and show that specific noise patterns interact with sequence to sequence training dynamics in different ways to generate the prominent hallucination patterns reported in the literature (Lee et al., 2018). 3. We demonstrate the phenomenon of hallucination amplification in the outputs generated using Backtranslation (Edunov et al., 2018) and Knowledge Distillation (Kim and Rush, 2016), two widely used data generation algorithms for MT. 2 Related Work Our work connects hallucinations in NMT to the 1. NMT models tend to generate hallucinated problem of generalization in Deep Learning. In 1172 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1172–1183 June 6–11, 2021. ©2021 Association for Computational Linguistics this section, we briefly survey the two areas. 2.1 Hallucinations in NMT The phenomena of hallucinations in NMT lack clear categorical definitions. Lee et al. (2018"
2021.naacl-main.92,W17-3204,0,0.0342606,"nds of hallucinations, studying them through the lens of generalization, memorization and optimization in sequence to sequence models. Our key contributions are as follows: 1. We extend the Memorization Value Estimator proposed in Feldman and Zhang (2020) to the sequence to sequence setting and demonstrate that hallucinations under source-side perturbations could be explained through the long-tail theory they propose. Introduction Neural Machine Translation (NMT) enjoys tremendous success, far surpassing the performance of previous statistical approaches in high-to-moderate resource settings (Koehn and Knowles, 2017). However, NMT suffers from well known pathologies such as coverage (Tu et al., 2016), mistranslation of named entities (Ugawa et al., 2018), etc. In terms of adequacy of the generated output (Martindale et al., 2019), hallucinations are egregious mistakes that lie at the extreme end of NMT pathologies. Such hallucinated outputs are characterized as being decoupled from the source sequence, despite being (fully or moderately) fluent in the target language (Müller et al., 2020). Two main hallucination phenomena have been reported in the existing literature: 2. We introduce corpus-level noise in"
2021.naacl-main.92,W18-6459,0,0.0691665,"consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results at https://github.com/vyraun/ hallucinations. 1 outputs under certain cases of source perturbation (Lee et al., 2018). 2. NMT models have a propensity to hallucinate more frequently under out-of-domain inputs (Müller et al., 2020). However, a plausible theory to explain the generation of different types of hallucinations, including the above two results is still lacking in the NMT literature. Lee et al. (2018) posited that hallucinations could be happening due to decoder instability, however, their experiments to engineer solutions based on this proved inconclusive. In this work, we present a systematic study of different kinds of hallucinations, studying them through the lens of generalization, memorization"
2021.naacl-main.92,W19-6623,0,0.0140808,"r proposed in Feldman and Zhang (2020) to the sequence to sequence setting and demonstrate that hallucinations under source-side perturbations could be explained through the long-tail theory they propose. Introduction Neural Machine Translation (NMT) enjoys tremendous success, far surpassing the performance of previous statistical approaches in high-to-moderate resource settings (Koehn and Knowles, 2017). However, NMT suffers from well known pathologies such as coverage (Tu et al., 2016), mistranslation of named entities (Ugawa et al., 2018), etc. In terms of adequacy of the generated output (Martindale et al., 2019), hallucinations are egregious mistakes that lie at the extreme end of NMT pathologies. Such hallucinated outputs are characterized as being decoupled from the source sequence, despite being (fully or moderately) fluent in the target language (Müller et al., 2020). Two main hallucination phenomena have been reported in the existing literature: 2. We introduce corpus-level noise into NMT parallel corpora and show that specific noise patterns interact with sequence to sequence training dynamics in different ways to generate the prominent hallucination patterns reported in the literature (Lee et"
2021.naacl-main.92,2020.amta-research.14,0,0.514898,"ominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results at https://github.com/vyraun/ hallucinations. 1 outputs under certain cases of source perturbation (Lee et al., 2018). 2. NMT models have a propensity to hallucinate more frequently under out-of-domain inputs (Müller et al., 2020). However, a plausible theory to explain the generation of different types of hallucinations, including the above two results is still lacking in the NMT literature. Lee et al. (2018) posited that hallucinations could be happening due to decoder instability, however, their experiments to engineer solutions based on this proved inconclusive. In this work, we present a systematic study of different kinds of hallucinations, studying them through the lens of generalization, memorization and optimization in sequence to sequence models. Our key contributions are as follows: 1. We extend the Memoriza"
2021.naacl-main.92,N19-4009,0,0.0316621,"okens T from the most common tokens in the token dictionary computed over the training corpus, for obtaining the most plausible perturbations. Algorithm 2: Hallucination under Perturbation Data: NMT Model, Parallel Corpus (X, Y), Token Set T Result: Hallucinated Samples H for x, y in X, Y do y 0 = Model(x) if adjusted-bleu(y 0 , y) > 0.09 then for t in T do x e = put t at the beginning of the input x ye = Model(e x) if adjusted-bleu(e y , y 0 ) < 0.01 then add x to H 4.1.1 Experiments and Results To compute the memorization values, mem in algorithm 1, we train t = 10 NMT models using fairseq (Ott et al., 2019) on different randomly selected subsets of sentence pairs (each about 101K samples) from the IWSLT-2014 De-En dataset (160K samples). BPE (Sennrich et al., 2016) with a joint Hallucination Detection The HP detection algo- token vocabulary of 10K is applied over lowerrithm used is presented as algorithm 2. In practice, cased tokenized text. The NMT model is a sixalgorithm 2 is a specific instance of the algorithm layer Transformer model with embedding size 512, from Lee et al. (2018), wherein we make the fol- FFN layer dimension 1024 and 4 attention heads lowing three changes: (42M parameters),"
2021.naacl-main.92,P02-1040,0,0.109225,"ing any generality. 4 Origins of Hallucinations In this section, we propose and empirically validate two hypotheses in order to explain the two categories of hallucinations described in section 3. 4.1 Hallucinations under Perturbations Hypothesis 1 (H1) The samples memorized by a NMT model are most likely to generate hallucinations when perturbed. To validate H1, we adapt the Memorization Value Estimator (MVE) proposed by Feldman and Zhang (2020) to the sequence to sequence setting, by replacing the accuracy metric they use with a sequence overlap metric such as chrF (Popovi´c, 2015) or BLEU (Papineni et al., 2002)1 . We then compare the hallucination behaviour under perturbation of the most-memorized samples with random samples using the hallucination detection algorithm proposed in Lee et al. (2018). Memorization Value Estimation The modified Memorization Value Estimator (MVE) is described in algorithm 1. MVE computes the memorization value of a sample as the change in average prediction metric M (for which we use metrics such as chrF, BLEU) for the given sample between the models trained with the sample included in the training set and the models trained with the sample excluded. Algorithm 1: Memoriz"
2021.naacl-main.92,W15-3049,0,0.0457929,"Missing"
2021.naacl-main.92,2020.findings-emnlp.276,1,0.792798,"from memorization of the samples in the long-tail of the dataset (Feld- level knowledge distillation. man, 2020), a simple iterative solution could be Due to the compute-intensive algorithms into analyze the long-tail (using Algorithm 1), and volved in our analysis, we conduct most of our implement data-augmentations specific to the char- experiments using the IWSLT 2014 corpus. Howacteristics of such samples (e.g. as in Table 9), with ever, long-tailed phenomena are a characteristic of the goal of bringing such samples out of the long- natural language and even scaling the size of the tail (Raunak et al., 2020). Further work is required corpus doesn’t alleviate the characteristic Zipfian to determine the dynamics of such transition. distribution of the occurrence of words/tokens in the NMT corpora; which, according to the cenAmeliorating Memorization During Learning tral thesis of the long-tail theory (Feldman, 2020), Robust learning algorithms e.g. Robust Early learnwould lead to memorizations. Similarly, noise in ing (Xia et al., 2021) that are designed to prevent the form of invalid references is an artifact of the memorization specifically are likely to prevent perscale at which web-based corpor"
2021.naacl-main.92,P18-2037,0,0.0213946,"Missing"
2021.naacl-main.92,P16-1162,0,0.0861462,"Hallucination under Perturbation Data: NMT Model, Parallel Corpus (X, Y), Token Set T Result: Hallucinated Samples H for x, y in X, Y do y 0 = Model(x) if adjusted-bleu(y 0 , y) > 0.09 then for t in T do x e = put t at the beginning of the input x ye = Model(e x) if adjusted-bleu(e y , y 0 ) < 0.01 then add x to H 4.1.1 Experiments and Results To compute the memorization values, mem in algorithm 1, we train t = 10 NMT models using fairseq (Ott et al., 2019) on different randomly selected subsets of sentence pairs (each about 101K samples) from the IWSLT-2014 De-En dataset (160K samples). BPE (Sennrich et al., 2016) with a joint Hallucination Detection The HP detection algo- token vocabulary of 10K is applied over lowerrithm used is presented as algorithm 2. In practice, cased tokenized text. The NMT model is a sixalgorithm 2 is a specific instance of the algorithm layer Transformer model with embedding size 512, from Lee et al. (2018), wherein we make the fol- FFN layer dimension 1024 and 4 attention heads lowing three changes: (42M parameters), and the checkpoint with the best validation BLEU (detokenized, with beam=5) is 1 In practice, other MT metrics such as METEOR or BERTselected. In each case, a b"
2021.naacl-main.92,P16-1008,0,0.0235819,"imization in sequence to sequence models. Our key contributions are as follows: 1. We extend the Memorization Value Estimator proposed in Feldman and Zhang (2020) to the sequence to sequence setting and demonstrate that hallucinations under source-side perturbations could be explained through the long-tail theory they propose. Introduction Neural Machine Translation (NMT) enjoys tremendous success, far surpassing the performance of previous statistical approaches in high-to-moderate resource settings (Koehn and Knowles, 2017). However, NMT suffers from well known pathologies such as coverage (Tu et al., 2016), mistranslation of named entities (Ugawa et al., 2018), etc. In terms of adequacy of the generated output (Martindale et al., 2019), hallucinations are egregious mistakes that lie at the extreme end of NMT pathologies. Such hallucinated outputs are characterized as being decoupled from the source sequence, despite being (fully or moderately) fluent in the target language (Müller et al., 2020). Two main hallucination phenomena have been reported in the existing literature: 2. We introduce corpus-level noise into NMT parallel corpora and show that specific noise patterns interact with sequence"
2021.naacl-main.92,C18-1274,0,0.0194916,"ntributions are as follows: 1. We extend the Memorization Value Estimator proposed in Feldman and Zhang (2020) to the sequence to sequence setting and demonstrate that hallucinations under source-side perturbations could be explained through the long-tail theory they propose. Introduction Neural Machine Translation (NMT) enjoys tremendous success, far surpassing the performance of previous statistical approaches in high-to-moderate resource settings (Koehn and Knowles, 2017). However, NMT suffers from well known pathologies such as coverage (Tu et al., 2016), mistranslation of named entities (Ugawa et al., 2018), etc. In terms of adequacy of the generated output (Martindale et al., 2019), hallucinations are egregious mistakes that lie at the extreme end of NMT pathologies. Such hallucinated outputs are characterized as being decoupled from the source sequence, despite being (fully or moderately) fluent in the target language (Müller et al., 2020). Two main hallucination phenomena have been reported in the existing literature: 2. We introduce corpus-level noise into NMT parallel corpora and show that specific noise patterns interact with sequence to sequence training dynamics in different ways to gene"
2021.naacl-main.92,2020.acl-main.326,0,0.0305696,"el and present an algorithm to detect such cases. Subsequently, approaches to making NMT models more robust to small perturbations in the input have been actively explored (Cheng et al., 2019), however, no coherent theory to explain the phenomena of hallucinations has been empirically validated in the existing literature. Our work differs from Lee et al. (2018) in that we not only study hallucinations under source side perturbations but also under corpus-level noise. Further, we build on their work by filling in the gap for a plausible hypothesis that explains various types of hallucinations. Wang and Sennrich (2020) consider hallucinations as outputs detached from the source, and demonstrate that NMT models are more prone to hallucinations under out-of-domain settings by manually ascertaining whether an output generated is hallucinated or not. Manual detection of hallucinations, however, is an impediment for fast experimental cycles, and in this work, besides explaining the generation of such natural hallucinations (i.e. hallucinations generated without any source perturbation), we also propose an approximate corpus level hallucination detection algorithm to aid faster analysis. 2.2 Generalization in Dee"
2021.naacl-main.92,2020.acl-main.756,0,0.0194509,"ome examples from the most memorized training samples, thereby representing the samples from the long-tail of the data that is likely to have been memorized by the model. Qualitatively, the examples appear to be different (in terms of source/target syntax) from a random subset of training samples (e.g. in Appendix A, Table 10), although we leave further quantitative analysis of the differences to future work. Similarly, the link between out-of-domain and memorized samples needs to be ascertained quantitatively. Corpus-Level Filtering Incorporating heuristics or filters (Junczys-Dowmunt, 2018; Zhang et al., 2020) to remove invalid source-target pairs, especially the noise patterns explored in section 4.2 (or to remove bitext indeterminacy in general) could be effective in reducing natural hallucinations. 7 Conclusion In this work we demonstrated that memorized training samples are far more likely to hallucinate under perturbation than non-memorized samples, under an extension of the Memory Value Estimator proposed in Feldman and Zhang (2020). We also 6.2 Preventing Hallucinations showed that specific noise patterns in the training In this subsection, we discuss a few methods that corpora lead to speci"
D15-1052,2012.iwslt-papers.5,0,0.139107,"udged more than one time by the same annotator. The agreement numbers in Table 2 are in the lower range of values reported during WMT. However, it should be noted that judges never saw the repeated outputs within one ranking which probably decreases agreement compared to the MTspecific task. 5.1 Ranking methods We adapt two ranking methods applied during WMT13 and WMT14 to GEC evaluation: the Expected Wins method and a version of TrueSkill. Expected Wins. Expected Wins (EW) has been introduced for WMT13 (Bojar et al., 2013) and is based on an underlying model of “relative ability” proposed in Koehn (2012). One advantage of this method is its intuitiveness; the scores reflect the probability that a system Si will be ranked better than another system that has been randomly chosen from a pool of opponents {Sj : j 6= i}. Defining the function win(A, B) as the number of times system A is ranked better than system B, Bojar et 464 M20.5 # Score Range System # 0.397 0.417 0.416 0.345 0.350 0.331 0.312 0.322 0.301 0.700 0.112 0.307 0.000 0.301 0.248 0.214 0.217 0.188 0.149 0.144 0.136 0.051 0.017 0.028 0.013 0.000 0.373 0.367 0.350 0.308 0.299 0.266 0.253 0.253 0.151 0.078 0.071 0.059 0.000 1 0.628 1 A"
D15-1052,W13-2202,0,0.0253297,"Missing"
D15-1052,W14-3336,0,0.0322309,"Missing"
D15-1052,W08-0309,0,0.127321,"Missing"
D15-1052,P11-2089,0,0.101018,"., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments. Therefore, manual evaluation of the system outputs are conducted and their results are reported as the final rankings of the workshops. These human 2 Evaluation of GEC systems Madnani et al. (2011) addresses two problems of GEC evaluation: 1) a lack of informative metrics and 2) an inability to directly compare the performance of systems developed by different researchers. Two evaluation methodologies are presented, both based on crowdsourcing which are used to grade types of errors rather than system performance as presented in this work. Chodorow et al. (2012) draw attention to the many evalua1 During the camera-ready preparation phase, we learned about similar research by Napoles et al. (2015). After contacting the authors, it was agreed to treat both works as fully concurrent. Futur"
D15-1052,C12-1038,0,0.432056,"ty are an imperfect substitute for human assessments. Therefore, manual evaluation of the system outputs are conducted and their results are reported as the final rankings of the workshops. These human 2 Evaluation of GEC systems Madnani et al. (2011) addresses two problems of GEC evaluation: 1) a lack of informative metrics and 2) an inability to directly compare the performance of systems developed by different researchers. Two evaluation methodologies are presented, both based on crowdsourcing which are used to grade types of errors rather than system performance as presented in this work. Chodorow et al. (2012) draw attention to the many evalua1 During the camera-ready preparation phase, we learned about similar research by Napoles et al. (2015). After contacting the authors, it was agreed to treat both works as fully concurrent. Future work will compare the results. 461 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461–470, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. tion issues in error detection which make it hard to compare different approaches. The lack of consensus is due to the nature of the error de"
D15-1052,W14-3605,0,0.0372759,"or grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments. Therefore, manual evaluation of the system outputs are conducted and their results are reported as the final rankings of the workshops. These human 2 Evaluation of GEC systems Madnani"
D15-1052,N12-1067,0,0.253035,"in English essays written by second language learners of English. Training and test data was annotated with 28 error types. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use publicly available resources for training. Twenty-five student non-native speakers of English were recruited to write essays to be used as test data. Each student wrote two essays. The 50 test essays were error-annotated by two English native speakers. The essays and error annotations were made available after the task. The MaxMatch (M2 ) scorer (Dahlmeier and Ng, 2012) has been used as the official shared task evaluation metric. 4 4.1 100 Figure 1: Frequencies of distinct corrected sentences produced by 13 systems per input sentence. instead a parametrized distribution that favors diverse sets of outputs. The probability pi for a set of outputs Oi is calculated as follows: N is the number of systems to be evaluated, M is the maximum number of sentences presented to the evaluator in a single ranking (we use M = 5). The set of system outputs to be evaluated E = {O1 , . . . , On } ∀1≤i≤n |Oi |= N , consists of n (= 1312) sets Oi of N output sentences each. Eve"
D15-1052,W11-2838,0,0.130796,"ion evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments. Therefore, manual evaluation of the system output"
D15-1052,W13-3601,0,0.12606,"the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is"
D15-1052,W12-2006,0,0.178382,"ve been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments. Therefore, manual evaluation of the system outputs are conducted and"
D15-1052,W14-1701,0,0.36702,"ut of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of"
D15-1052,W11-2107,0,0.010484,"ation values have been calculated for the ranking presented in Felice and Briscoe (2015). where H and M are the vectors of human and met¯ and M ¯ are corresponding means. ric scores, H 6.2 Spearman’s ρ Metrics The inventory of evaluation metrics for GEC is significantly smaller than for MT. We hope that making our data available will fuel the interest in this area. The following metrics are assessed: Machine translation evaluation metrics. Basing most of our results on findings from MT, we also take a look at two machine translation evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011). In order to use the CoNLL-2014 gold standard with these metrics, the edit-based annotation has been converted into two plain text files, one per annotator. MaxMatch (M2 ). Due to its adoption as the main evaluation metric of the CoNLL shared tasks and the QALB shared tasks (Mohit et al., 2014), the M2 metric (Dahlmeier and Ng, 2012) can be seen as a de facto standard. Being an Fβ -score, M2 results are most influenced by the choice of β. Between the CoNLL-2013 and CoNLL-2014 shared tasks, the organizers changed β from 1.0 to 0.5, and motivate this with intuition alone. The QALB shared tasks"
D15-1052,P02-1040,0,0.109626,"] are possible. The reported correlation values have been calculated for the ranking presented in Felice and Briscoe (2015). where H and M are the vectors of human and met¯ and M ¯ are corresponding means. ric scores, H 6.2 Spearman’s ρ Metrics The inventory of evaluation metrics for GEC is significantly smaller than for MT. We hope that making our data available will fuel the interest in this area. The following metrics are assessed: Machine translation evaluation metrics. Basing most of our results on findings from MT, we also take a look at two machine translation evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011). In order to use the CoNLL-2014 gold standard with these metrics, the edit-based annotation has been converted into two plain text files, one per annotator. MaxMatch (M2 ). Due to its adoption as the main evaluation metric of the CoNLL shared tasks and the QALB shared tasks (Mohit et al., 2014), the M2 metric (Dahlmeier and Ng, 2012) can be seen as a de facto standard. Being an Fβ -score, M2 results are most influenced by the choice of β. Between the CoNLL-2013 and CoNLL-2014 shared tasks, the organizers changed β from 1.0 to 0.5, and motivate this with"
D15-1052,federmann-2010-appraise,0,0.0204732,"for GEC judgment. (b) Overlapping rankings. Figure 2: Displayed ranking and corresponding overlapping rankings. the entire set of output sets we obtain the probability pi of sampling the i-th set of outputs as p0 pi = P|E|i 0 j=1 pj 4.2 lowing sentence. Identical corrections are collapsed into one output, system names with the same output are recorded internally. Edited fragments are highlighted, blue for insertions and substitutions, pale blue and crossed-out for deletions. . Collecting system rankings 4.3 The sets of outputs sampled with the described method have been prepared for Appraise (Federmann, 2010) and presented to the judges. Judges were asked to rank sentences from best to worse. Ties are allowed. Judges were aware that the absolute ranks bear no relevance as ranks are later turned into relative pairwise judgments. No notion of “better” or “worse” was imposed by the authors, we relied on the judges to develop their own intuition. All eight judges are English native speakers and have extensive backgrounds in linguistics. Figure 2a displays a screen shot of Appraise with a judged sentence. Several modifications to the Appraise framework2 were implemented to account for the specific natu"
D15-1052,W14-3301,0,0.135051,"Missing"
D15-1052,N15-1060,0,0.67792,"ting this lack of confidence by presenting the results of the first1 large-scale human evaluation of automatic grammatical error correction systems submitted to the CoNLL-2014 shared task. Most of our inspiration is drawn from the recent WMT edition (Bojar et al., 2014) and its metrics task (Macháˇcek and Bojar, 2014). We also provide an analysis of correlation between the standard metrics in GEC and human judgment and show that the commonly used parameters for standard metrics in the shared task may not be optimal. The uncertainty about metrics quality leads to proposals of new metrics, with Felice and Briscoe (2015) being a recent example. Based on human judgments we can show that this proposed metric maybe less useful than hoped. The paper presents the results of the first large-scale human evaluation of automatic grammatical error correction (GEC) systems. Twelve participating systems and the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for gra"
D15-1052,W14-3302,0,\N,Missing
D15-1052,W13-2201,0,\N,Missing
D15-1052,P15-2097,0,\N,Missing
D16-1161,P06-1032,0,0.222743,"Association for Computational Linguistics M2 50.0 baseline (u) 40.0 baseline (r) 30.0 20.0 10.0 this work Su sa Co nt NL o L Ch e ol Xi t al 201 la e e . ( 4 m t 2 (u pa a 01 ) H tt e l. (2 4) oa t ( ng al. 016 r) ) ( et 20 (u al 16 ) .( ) 20 (r 16 ) )( r) de ns sp e (r ar ) s de e (r ns ) sp e (u ar ) se (u ) 0.0 Figure 1: Comparison with previous work on the CoNLL-2014 task, trained on publicly available data. Dashed lines mark results for our baseline systems with restricted (r) and unrestricted (u) data. 2 Previous Work While machine translation has been used for GEC in works as early as Brockett et al. (2006), we start our discussion with the CoNLL-2014 shared task (Ng et al., 2014) where for the first time an unrestricted set of errors had to be fully corrected. Previous work, most notably during the CoNLL sharedtask 2013 (Ng et al., 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well. The goal of the CoNLL-2014 shared task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 ty"
D16-1161,P15-1068,0,0.119932,"Missing"
D16-1161,buck-etal-2014-n,0,0.0235977,"Missing"
D16-1161,2011.mtsummit-papers.1,0,0.0889645,"able to beat systems that did not tune for the task metric. We reinvestigated these ideas with radically better results, re-implemented the M2 metric in C++ and added it as a scorer to the Moses parameter optimization framework. Due to this integration we can now tune parameter weights with MERT, PRO or Batch Mira. The inclusion of the latter two enables us to experiment with sparse features. Based on Clark et al. (2011) concerning the effects of optimizer instability, we report results averaged over five tuning runs. Additionally, we compute parameter weight vector centroids as suggested by Cettolo et al. (2011). They showed that parameter vector centroids averaged over several tuning runs yield similar to or better than average results and reduce variance. We generally confirm this for M2 -based tuning. 3.2 Dense features The standard features in SMT have been chosen to help guiding the translation process. In a GEC setting the most natural units seem to be minimal edit operations that can be either counted or modeled in context with varying degrees of generalization. That way, the decoder can be informed on several levels source phrase target phrase a short time . a situation a supermarket . a supe"
D16-1161,N12-1047,0,0.0236994,"s We saw that introducing finer-grained edit operations improved performance. The natural evolution of that idea are features that describe specific correction operations with and without context. This can be accomplished with sparse features, but tuning sparse features according to the M2 metric poses unexpected problems. 4.1 Optimizing for M2 with PRO and Mira The MERT tool included in Moses cannot handle parameter tuning with sparse feature weights and one of the other optimizers available in Moses has to be used. We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets (Table 3). Experiments with Mira hyper-parameters allowed to counter these effects. We first change the Optimizer 2013 2014 M2 MERT PRO Mira 33.50 33.68 29.19 42.85 40.34 34.13 30.0 31.06 33.86 43.88 42.91 -model-bg -D 0.001 25.0 20.0 Table 3: Tuning with different optimizers with dense features only, results are given for the CoNLL-2013 and CoNLL-2014 test set background BLEU approximation meth"
D16-1161,P11-2031,0,0.0107449,"re weights of their two SMT-based systems with BLEU on the CoNLL2013 test set and report state-of-the-art results. Despite tuning with M2 , in Junczys-Dowmunt and Grundkiewicz (2014) we were not able to beat systems that did not tune for the task metric. We reinvestigated these ideas with radically better results, re-implemented the M2 metric in C++ and added it as a scorer to the Moses parameter optimization framework. Due to this integration we can now tune parameter weights with MERT, PRO or Batch Mira. The inclusion of the latter two enables us to experiment with sparse features. Based on Clark et al. (2011) concerning the effects of optimizer instability, we report results averaged over five tuning runs. Additionally, we compute parameter weight vector centroids as suggested by Cettolo et al. (2011). They showed that parameter vector centroids averaged over several tuning runs yield similar to or better than average results and reduce variance. We generally confirm this for M2 -based tuning. 3.2 Dense features The standard features in SMT have been chosen to help guiding the translation process. In a GEC setting the most natural units seem to be minimal edit operations that can be either counted"
D16-1161,N12-1067,0,0.651517,"ected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well. The goal of the CoNLL-2014 shared task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 types were targeted. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use additional publicly available data. The corrected system outputs were evaluated blindly using the MaxMatch (M2 ) metric (Dahlmeier and Ng, 2012). Thirteen system submissions took part in the shared task. Among the 1547 top-three positioned systems, two submissions — CAMB (Felice et al., 2014) and AMU (JunczysDowmunt and Grundkiewicz, 2014)1 — were partially or fully based on SMT. The second system, CUUI (Rozovskaya et al., 2014), was a classifierbased approach, another popular paradigm in GEC. After the shared task, Susanto et al. (2014) published work on GEC systems combinations. They combined the output from a classification-based system and a SMT-based system using MEMT (Heafield and Lavie, 2010), reporting new state-ofthe-art resu"
D16-1161,W13-1703,0,0.331599,"on source and target sides are substitutions; insertions and deletions are interpreted in the same way as for SMT. Gaps, jumps, and other operations typical for OSMs do not appear as we disabled reordering. Word-class language model. The monolingual Wikipedia data has been used create a 9-gram wordclass language model with 200 word-classes produced by word2vec (Mikolov et al., 2013). This features allows to capture possible long distance dependencies and semantical aspects. 3.3 Training and Test Data The training data provided in both shared tasks is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013). NUCLE consists of 1,414 essays written by Singaporean students who are nonnative speakers of English. The essays cover topics, such as environmental pollution, health care, etc. The grammatical errors in these essays have been hand-corrected by professional English teachers and annotated with one of the 28 predefined error type. Another 50 essays, collected and annotated similarly as NUCLE, were used in both CoNLL GEC shared tasks as blind test data. The CoNLL-2014 test set has been annotated by two human annotators, the CoNLL-2013 by one annotator. Many participants of CoNLL-2014 shared tas"
D16-1161,P13-2071,0,0.0317283,"an look at translation hypotheses outside their own span and take advantage of the constructed target context. The most typical stateful features are language models. In this section, we discuss LM-like features over edit operations. 3 We believe this is important information that currently has not yet been mastered in neural encoder-decoder approaches. 1549 Wikipedia CommonCrawl (u) Sentences Tokens 57.15 K 1.38 K 1.31 K 2.23 M 3.72 M 1.15 M 29.07 K 30.11 K 30.03 M 51.07 M 213.08 M 59.13 G 3.37 G 975.63 G Table 2: Parallel (above line) and monolingual training data. Operation Sequence Model. Durrani et al. (2013) introduce Operation Sequence Models in Moses. These models are Markov translation models that in our setting can be interpreted as Markov edition models. Translations between identical words are matches, translations that have different words on source and target sides are substitutions; insertions and deletions are interpreted in the same way as for SMT. Gaps, jumps, and other operations typical for OSMs do not appear as we disabled reordering. Word-class language model. The monolingual Wikipedia data has been used create a 9-gram wordclass language model with 200 word-classes produced by wo"
D16-1161,W14-1702,0,0.469378,"red task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 types were targeted. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use additional publicly available data. The corrected system outputs were evaluated blindly using the MaxMatch (M2 ) metric (Dahlmeier and Ng, 2012). Thirteen system submissions took part in the shared task. Among the 1547 top-three positioned systems, two submissions — CAMB (Felice et al., 2014) and AMU (JunczysDowmunt and Grundkiewicz, 2014)1 — were partially or fully based on SMT. The second system, CUUI (Rozovskaya et al., 2014), was a classifierbased approach, another popular paradigm in GEC. After the shared task, Susanto et al. (2014) published work on GEC systems combinations. They combined the output from a classification-based system and a SMT-based system using MEMT (Heafield and Lavie, 2010), reporting new state-ofthe-art results for the CoNLL-2014 test set. Xie et al. (2016) presented a neural networkbased approach to GEC. Their method relies on a character-level encoder-"
D16-1161,W08-0509,0,0.0209984,"Missing"
D16-1161,W11-2123,0,0.021439,"from the CoNLL-2014 were free to use any publicly available data, for instance in Junczys-Dowmunt and Grundkiewicz (2014), we made use of an n-gram language model trained from Common Crawl. Xie et al. (2016) reach the best published result for the task (before this work) by integrating a similar n-gram language model with their neural approach. We filter the English resources made available by Buck et al. (2014) with cross-entropy filtering (Moore and Lewis, 2010) using the corrected NUCLE corpus as seed data. We keep all sentence with a negative cross-entropy score and compute a 5gram KenLM (Heafield, 2011) language model with heavy pruning. This step produces roughly 300G of compressed text and a manageable 21G binary model (available for download). Table 4 summarizes the best results reported in 1553 this paper for the CoNLL-2014 test set (column 2014) before and after adding the Common Crawl n-gram language model. The vanilla Moses baseline with the Common Crawl model can be seen as a new simple baseline for unrestricted settings and is ahead of any previously published result. The combination of sparse features and web-scale monolingual data marks our best result, outperforming previously pu"
D16-1161,D11-1125,0,0.0378013,"resented in this work. 4 Sparse Features We saw that introducing finer-grained edit operations improved performance. The natural evolution of that idea are features that describe specific correction operations with and without context. This can be accomplished with sparse features, but tuning sparse features according to the M2 metric poses unexpected problems. 4.1 Optimizing for M2 with PRO and Mira The MERT tool included in Moses cannot handle parameter tuning with sparse feature weights and one of the other optimizers available in Moses has to be used. We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets (Table 3). Experiments with Mira hyper-parameters allowed to counter these effects. We first change the Optimizer 2013 2014 M2 MERT PRO Mira 33.50 33.68 29.19 42.85 40.34 34.13 30.0 31.06 33.86 43.88 42.91 -model-bg -D 0.001 25.0 20.0 Table 3: Tuning with different optimizers with dense features only, results are given for the CoNLL-2013 and CoNLL-2014 te"
D16-1161,W14-1703,1,0.507736,"h to GEC. Their method relies on a character-level encoder-decoder recurrent neural network with an attention mechanism. They use data from the public Lang-8 corpus and combine their model with an n-gram language model trained on web-scale Common Crawl data. More recent results are Chollampatt et al. (2016) and Hoang et al. (2016) which also rely on MT systems with new features (a feed-forward neural translation model) and n-best list re-ranking methods. However, most of the improvement over the CoNLL-2014 shared task of these works stems from using the parameter tuning tools we introduced in Junczys-Dowmunt and Grundkiewicz (2014). In Figure 1 we give a graphical overview of the published results for the CoNLL-2014 test set in comparison to the results we will discuss in this work. Positions marked with (r) use only restricted data which corresponds to the data set used by Susanto et al. (2014). Positions with (u) make use of web-scale data, this corresponds to the resources used in Xie et al. (2016). We marked the participants of the CoNLL-2014 shared task as unrestricted as some participants made use of Common Crawl data or Google n-grams. The visible plateau for results 1 Junczys-Dowmunt and Grundkiewicz (2014) is o"
D16-1161,C12-2084,0,0.304908,"f error-rate-adapted NUCLE Figure 3: Results on the CoNLL-2014 test set for different optimization settings (5 runs for each system) and different feature sets, the “All dense” entry includes OSM, the word class language model, and edit operations). The small circle marks results for averaged weights vectors and is chosen as the final result. ing similar training data as Susanto et al. (2014). We refer to this setting that as the “resticted-data setting” (r). Parallel data for translation model training is adapted from the above mentioned NUCLE corpus and the publicly available Lang-8 corpus (Mizumoto et al., 2012), this corpus is distinct from the non-public web-crawled data described in Section 6. Uncorrected sentences serve as source data, corrected counterparts as target data. For language modeling, the target language sentences of both parallel resources are used, additionally we extract all text from the English Wikipedia. Phrase-based SMT makes it ease to scale up in terms of training data, especially in the case of ngram language models. To demonstrate the ease of data integration we propose an “unrestricted setting” (u) based on the data used in Junczys-Dowmunt and Grundkiewicz (2014), one of t"
D16-1161,P10-2041,0,0.020199,"the best fine-tuning. 5 Adding a web-scale language model Until now we restricted our experiments to data used by Susanto et al. (2014). However, systems from the CoNLL-2014 were free to use any publicly available data, for instance in Junczys-Dowmunt and Grundkiewicz (2014), we made use of an n-gram language model trained from Common Crawl. Xie et al. (2016) reach the best published result for the task (before this work) by integrating a similar n-gram language model with their neural approach. We filter the English resources made available by Buck et al. (2014) with cross-entropy filtering (Moore and Lewis, 2010) using the corrected NUCLE corpus as seed data. We keep all sentence with a negative cross-entropy score and compute a 5gram KenLM (Heafield, 2011) language model with heavy pruning. This step produces roughly 300G of compressed text and a manageable 21G binary model (available for download). Table 4 summarizes the best results reported in 1553 this paper for the CoNLL-2014 test set (column 2014) before and after adding the Common Crawl n-gram language model. The vanilla Moses baseline with the Common Crawl model can be seen as a new simple baseline for unrestricted settings and is ahead of an"
D16-1161,W13-3601,0,0.0414049,"0 (r 16 ) )( r) de ns sp e (r ar ) s de e (r ns ) sp e (u ar ) se (u ) 0.0 Figure 1: Comparison with previous work on the CoNLL-2014 task, trained on publicly available data. Dashed lines mark results for our baseline systems with restricted (r) and unrestricted (u) data. 2 Previous Work While machine translation has been used for GEC in works as early as Brockett et al. (2006), we start our discussion with the CoNLL-2014 shared task (Ng et al., 2014) where for the first time an unrestricted set of errors had to be fully corrected. Previous work, most notably during the CoNLL sharedtask 2013 (Ng et al., 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well. The goal of the CoNLL-2014 shared task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 types were targeted. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use additional publicly available data. The corrected system outputs were evaluated blindly us"
D16-1161,W14-1701,0,0.568442,"30.0 20.0 10.0 this work Su sa Co nt NL o L Ch e ol Xi t al 201 la e e . ( 4 m t 2 (u pa a 01 ) H tt e l. (2 4) oa t ( ng al. 016 r) ) ( et 20 (u al 16 ) .( ) 20 (r 16 ) )( r) de ns sp e (r ar ) s de e (r ns ) sp e (u ar ) se (u ) 0.0 Figure 1: Comparison with previous work on the CoNLL-2014 task, trained on publicly available data. Dashed lines mark results for our baseline systems with restricted (r) and unrestricted (u) data. 2 Previous Work While machine translation has been used for GEC in works as early as Brockett et al. (2006), we start our discussion with the CoNLL-2014 shared task (Ng et al., 2014) where for the first time an unrestricted set of errors had to be fully corrected. Previous work, most notably during the CoNLL sharedtask 2013 (Ng et al., 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well. The goal of the CoNLL-2014 shared task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 types were targeted. Participating teams were given training data with manual"
D16-1161,P03-1021,0,0.0404133,"is work seem to confirm our claims about missing strong baselines. Rozovskaya and Roth (2016) introduce a SMTclassifier pipeline with state-of-the-art results. Unfortunately, these results are reported for a training set that is not publicly available (data crawled from the Lang-8 website)2 . Figure 2 compares our results for this resource to Rozovskaya and Roth (2016). See Section 6 for details. 3 Dense feature optimization Moses comes with tools that can tune parameter vectors according to different MT tuning metrics. Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al., 2002). BLEU was never designed for grammatical error correction; we find that directly optimizing for M2 works far better. 2 We shared this resource that has been crawled by us for use in Junczys-Dowmunt and Grundkiewicz (2014) privately with Rozovskaya and Roth (2016), but originally were not planning to report results for this resource in the future. We now provide a comparison to Rozovskaya and Roth (2016), but discourage any further use of this unofficial data due to reproducibility issues. 1548 Tuning towards M2 The M2 metric (Dahlmeier and Ng, 2012) is an"
D16-1161,P02-1040,0,0.111083,"ur claims about missing strong baselines. Rozovskaya and Roth (2016) introduce a SMTclassifier pipeline with state-of-the-art results. Unfortunately, these results are reported for a training set that is not publicly available (data crawled from the Lang-8 website)2 . Figure 2 compares our results for this resource to Rozovskaya and Roth (2016). See Section 6 for details. 3 Dense feature optimization Moses comes with tools that can tune parameter vectors according to different MT tuning metrics. Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al., 2002). BLEU was never designed for grammatical error correction; we find that directly optimizing for M2 works far better. 2 We shared this resource that has been crawled by us for use in Junczys-Dowmunt and Grundkiewicz (2014) privately with Rozovskaya and Roth (2016), but originally were not planning to report results for this resource in the future. We now provide a comparison to Rozovskaya and Roth (2016), but discourage any further use of this unofficial data due to reproducibility issues. 1548 Tuning towards M2 The M2 metric (Dahlmeier and Ng, 2012) is an FScore, based on the edits extracted"
D16-1161,P16-1208,0,0.506924,"Moses comes with tools that can tune parameter vectors according to different MT tuning metrics. Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al., 2002). BLEU was never designed for grammatical error correction; we find that directly optimizing for M2 works far better. 2 We shared this resource that has been crawled by us for use in Junczys-Dowmunt and Grundkiewicz (2014) privately with Rozovskaya and Roth (2016), but originally were not planning to report results for this resource in the future. We now provide a comparison to Rozovskaya and Roth (2016), but discourage any further use of this unofficial data due to reproducibility issues. 1548 Tuning towards M2 The M2 metric (Dahlmeier and Ng, 2012) is an FScore, based on the edits extracted from a Levenshtein distance matrix. For the CoNLL-2014 shared task, the β-parameter was set to 0.5, putting two times more weight on precision than on recall. In Junczys-Dowmunt and Grundkiewicz (2014) we have shown that tuning with BLEU is counterproductive in a setting where M2 is the evaluation metric. For inherently weak systems this can result in all correction attempts to be disabled, MERT then lea"
D16-1161,D14-1102,0,0.232436,"manually annotated corrections of grammatical errors and were allowed to use additional publicly available data. The corrected system outputs were evaluated blindly using the MaxMatch (M2 ) metric (Dahlmeier and Ng, 2012). Thirteen system submissions took part in the shared task. Among the 1547 top-three positioned systems, two submissions — CAMB (Felice et al., 2014) and AMU (JunczysDowmunt and Grundkiewicz, 2014)1 — were partially or fully based on SMT. The second system, CUUI (Rozovskaya et al., 2014), was a classifierbased approach, another popular paradigm in GEC. After the shared task, Susanto et al. (2014) published work on GEC systems combinations. They combined the output from a classification-based system and a SMT-based system using MEMT (Heafield and Lavie, 2010), reporting new state-ofthe-art results for the CoNLL-2014 test set. Xie et al. (2016) presented a neural networkbased approach to GEC. Their method relies on a character-level encoder-decoder recurrent neural network with an attention mechanism. They use data from the public Lang-8 corpus and combine their model with an n-gram language model trained on web-scale Common Crawl data. More recent results are Chollampatt et al. (2016)"
D16-1161,W13-3607,0,0.038497,"-2014 task, trained on publicly available data. Dashed lines mark results for our baseline systems with restricted (r) and unrestricted (u) data. 2 Previous Work While machine translation has been used for GEC in works as early as Brockett et al. (2006), we start our discussion with the CoNLL-2014 shared task (Ng et al., 2014) where for the first time an unrestricted set of errors had to be fully corrected. Previous work, most notably during the CoNLL sharedtask 2013 (Ng et al., 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well. The goal of the CoNLL-2014 shared task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 types were targeted. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use additional publicly available data. The corrected system outputs were evaluated blindly using the MaxMatch (M2 ) metric (Dahlmeier and Ng, 2012). Thirteen system submissions took part in the shared task. Among the 1547 top-"
D16-1161,W14-1704,0,\N,Missing
D18-1332,D17-1045,1,0.807552,"even larger batches by processing multiple mini-batches and summing their gradients locally without sending them to the optimizer. This still increases speed because communication is reduced (Table 1). We introduce parameter τ , which is the number of iterations a GPU performs locally before communicating externally as if it had run one large batch. The Words1 The Tesla P100 has 16 GB of GPU memory and we opt to use 10 GBs of mini-batches and the rest is used to store model parameters, shards, optimizers and additional system specific elements such as the cache vectors for gradient dropping (Aji and Heafield, 2017). Warmup Lowering initial learning rate Goyal et al. (2017) lower the initial learning rate and gradually increase it over a number of minibatches until it reaches a predefined maximum. This technique is also adopted in the work of Vaswani et al. (2017). This is the canonical way to perform warmup for neural network training. 2.2.2 Local optimizers We propose an alternative warm up strategy and compare it with the canonical method. Since we emulate large batches by running multiple smaller batches, it makes sense to consider whether to optimize locally between each batch by adapting the concep"
D18-1332,P18-4020,1,0.871141,"Missing"
D18-1332,P18-2051,0,0.0651113,"Missing"
D18-1332,W16-2323,0,0.0310632,"nt and reducing the frequency of gradient communicaVRAM 3 GB 7 GB 10 GB 20* GB 30* GB 40* GB τ 1 1 1 2 2 4 Words 3080 7310 10448 20897 31345 41794 WPS 19.5k 36.6k 40.2k 44.2k 46.0k 47.6k Table 1: Relationship between the GPU Memory (VRAM) budget for batches (* means emulated by summing τ smaller batches), number of source words processed in each batch and wordsper-second (WPS) measured on a shallow model. 2 Experiments This section introduces each optimization along with an intrinsic experiment on the WMT 2016 Romanian→English task (Bojar et al., 2016). The translation system is equivalent to Sennrich et al. (2016), which was the first place constrained system (and tied for first overall in the WMT16 2991 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2991–2996 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics shared task.). The model is a shallow bidirectional GRU (Bahdanau et al., 2014) encoder-decoder trained on 2.6 million parallel sentences. Due to variable-length sentences, machine translation systems commonly fix a memory budget then pack as many sentences as possible into a dynamicallysized batch. The"
D19-5546,W18-6111,0,0.103631,"nsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require any amount of annotated sentences for tuning. Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in unsupervised and low-resource GEC (Rozovskaya et al., 2017; Bryant and Briscoe, 2018; Boyd, 2018; Rozovskaya and Roth, 2019), which recently culminated in the low-resource track of the Building Educational Application (BEA) shared task (Bryant et al., 2019).1 We present Minimally-Augmented Grammatical Error Correction (MAGEC), a simple but effective approach to unsupervised and low-resource GEC which does not require any authentic error-labelled training data. A neural sequence-to-sequence model is trained on clean and synthetically noised sentences alone. The noise is automatically created from confusion sets. Additionally, if labelled data Our minimally-augmented GEC approach uses synt"
D19-5546,W18-0529,0,0.0455897,"s that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require any amount of annotated sentences for tuning. Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in unsupervised and low-resource GEC (Rozovskaya et al., 2017; Bryant and Briscoe, 2018; Boyd, 2018; Rozovskaya and Roth, 2019), which recently culminated in the low-resource track of the Building Educational Applicati"
D19-5546,W19-4406,0,0.088522,"19), but it does not require any amount of annotated sentences for tuning. Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in unsupervised and low-resource GEC (Rozovskaya et al., 2017; Bryant and Briscoe, 2018; Boyd, 2018; Rozovskaya and Roth, 2019), which recently culminated in the low-resource track of the Building Educational Application (BEA) shared task (Bryant et al., 2019).1 We present Minimally-Augmented Grammatical Error Correction (MAGEC), a simple but effective approach to unsupervised and low-resource GEC which does not require any authentic error-labelled training data. A neural sequence-to-sequence model is trained on clean and synthetically noised sentences alone. The noise is automatically created from confusion sets. Additionally, if labelled data Our minimally-augmented GEC approach uses synthetic noise as its primary source of training data. We generate erroneous sentences from monolingual texts via random word perturbations selected from automatica"
D19-5546,P17-1074,0,0.1325,"Missing"
D19-5546,N12-1067,0,0.238196,"Missing"
D19-5546,W13-1703,0,0.0824767,"systems (Grundkiewicz et al., 2019) from the BEA shared task trained on publicly available errorannotated corpora (Table 4a). It is difficult to compare with the top low-resource system from the shared task, because it uses additional parallel data from Wikipedia (Grundkiewicz and JunczysDowmunt, 2014), larger ensemble, and n-best list re-ranking with right-to-left models, which can be also implemented in this work. MAGEC systems are generally on par with the results achieved by a recent unsupervised contribution based on finite state transducers by Stahlberg et al. (2019) on the CoNLL-2014 (Dahlmeier et al., 2013) and JFLEG test sets (Napoles et al., 2017) (Table 5). 360 12 The weight of the language model is grid-searched on the development set. Lang. EN DE RU P Spell.+punc. R F0.5 28.8 24.1 27.68 54.8 71.4 57.43 26.7 75.0 30.70 P and Russian benchmarks, trained with labelled data, by a large margin. For future work, we plan to evaluate MAGEC on more languages and experiment with more diversified confusion sets created with additional unsupervised generation methods. Other errors R F0.5 33.5 16.8 27.93 63.6 55.8 61.83 14.6 19.7 15.37 Table 6: Performance of single MAGEC w/ LM models on two groups of e"
D19-5546,N19-1423,0,0.00827394,"ost other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require any amount of annotated sentences for tuning. Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in u"
D19-5546,E14-3013,0,0.0178403,"ell-checkers. In low-resource settings, we outperform the current state-ofthe-art results for German and Russian GEC tasks by a large margin without using any real error-annotated training data. When combined with labelled data, our method can serve as an efficient pre-training technique. 1 Introduction is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique. The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training exa"
D19-5546,P18-1097,0,0.0328543,"erve as an efficient pre-training technique. 1 Introduction is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique. The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require any amount of annotated sentence"
D19-5546,W19-4427,1,0.856099,"his large-scale unsupervised approach can perform better than training on small authentic error corpora. A large amount of training examples increases the chance that synthetic errors resemble real error patterns and results in better language modelling properties. If any small amount of error-annotated learner data is available, it can be used to fine-tune the pre-trained model and further boost its performance. Pre-training of decoders of GEC models from language models has been introduced by Junczys-Dowmunt et al. (2018b), we pretrain the full encoder-decoder models instead, as proposed by Grundkiewicz et al. (2019). Table 1: Examples of spell-broken confusion sets for English, German and Russian. Edit distance Confusion sets consist of words with the shortest Levenshtein distance (Levenshtein, 1966) to the selected confused word. Word embeddings Confusion sets contain the most similar words to the confused word based on the cosine similarity of their word embedding vectors (Mikolov et al., 2013). Spell-breaking Confusion sets are composed of suggestions from a spell-checker; a suggestion list is extracted for the confused word regardless of its actual correctness. These methods can be used to build conf"
D19-5546,P19-2020,0,0.0234574,"Missing"
D19-5546,W19-4449,0,0.203798,"for German and Russian GEC tasks by a large margin without using any real error-annotated training data. When combined with labelled data, our method can serve as an efficient pre-training technique. 1 Introduction is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique. The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed"
D19-5546,N18-1055,1,0.932602,"then ensembled with a language model. Being limited only by the amount of clean monolingual data, this large-scale unsupervised approach can perform better than training on small authentic error corpora. A large amount of training examples increases the chance that synthetic errors resemble real error patterns and results in better language modelling properties. If any small amount of error-annotated learner data is available, it can be used to fine-tune the pre-trained model and further boost its performance. Pre-training of decoders of GEC models from language models has been introduced by Junczys-Dowmunt et al. (2018b), we pretrain the full encoder-decoder models instead, as proposed by Grundkiewicz et al. (2019). Table 1: Examples of spell-broken confusion sets for English, German and Russian. Edit distance Confusion sets consist of words with the shortest Levenshtein distance (Levenshtein, 1966) to the selected confused word. Word embeddings Confusion sets contain the most similar words to the confused word based on the cosine similarity of their word embedding vectors (Mikolov et al., 2013). Spell-breaking Confusion sets are composed of suggestions from a spell-checker; a suggestion list is extracted f"
D19-5546,D18-1541,0,0.127827,"te-ofthe-art results for German and Russian GEC tasks by a large margin without using any real error-annotated training data. When combined with labelled data, our method can serve as an efficient pre-training technique. 1 Introduction is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique. The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alter"
D19-5546,D18-2012,0,0.0535139,"Missing"
D19-5546,D17-1156,0,0.0245694,"DE RU P Spell.+punc. R F0.5 28.8 24.1 27.68 54.8 71.4 57.43 26.7 75.0 30.70 P and Russian benchmarks, trained with labelled data, by a large margin. For future work, we plan to evaluate MAGEC on more languages and experiment with more diversified confusion sets created with additional unsupervised generation methods. Other errors R F0.5 33.5 16.8 27.93 63.6 55.8 61.83 14.6 19.7 15.37 Table 6: Performance of single MAGEC w/ LM models on two groups of errors on respective development sets. All unsupervised systems benefit from domainadaptation via fine-tuning on authentic labelled data (Miceli Barone et al., 2017). The more authentic high-quality and in-domain training data is used, the greater the improvement, but even as few as ~2,000 sentences are helpful (Fig. 1). We found that fine-tuning on a 2:1 mixture of synthetic and oversampled authentic data prevents the model from over-fitting. This is particularly visible for English which has the largest fine-tuning set (34K sentences), and the difference of 5.2 F0.5 between finetuning with and without synthetic data is largest. Spelling and punctuation errors The GEC task involves detection and correction of all types of error in written texts, includin"
D19-5546,E17-2037,0,0.0305574,"BEA shared task trained on publicly available errorannotated corpora (Table 4a). It is difficult to compare with the top low-resource system from the shared task, because it uses additional parallel data from Wikipedia (Grundkiewicz and JunczysDowmunt, 2014), larger ensemble, and n-best list re-ranking with right-to-left models, which can be also implemented in this work. MAGEC systems are generally on par with the results achieved by a recent unsupervised contribution based on finite state transducers by Stahlberg et al. (2019) on the CoNLL-2014 (Dahlmeier et al., 2013) and JFLEG test sets (Napoles et al., 2017) (Table 5). 360 12 The weight of the language model is grid-searched on the development set. Lang. EN DE RU P Spell.+punc. R F0.5 28.8 24.1 27.68 54.8 71.4 57.43 26.7 75.0 30.70 P and Russian benchmarks, trained with labelled data, by a large margin. For future work, we plan to evaluate MAGEC on more languages and experiment with more diversified confusion sets created with additional unsupervised generation methods. Other errors R F0.5 33.5 16.8 27.93 63.6 55.8 61.83 14.6 19.7 15.37 Table 6: Performance of single MAGEC w/ LM models on two groups of errors on respective development sets. All u"
D19-5546,W17-5032,0,0.0740885,"rm the current state-ofthe-art results for German and Russian GEC tasks by a large margin without using any real error-annotated training data. When combined with labelled data, our method can serve as an efficient pre-training technique. 1 Introduction is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique. The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approa"
D19-5546,D10-1094,0,0.0326701,"roach to unsupervised and low-resource GEC which does not require any authentic error-labelled training data. A neural sequence-to-sequence model is trained on clean and synthetically noised sentences alone. The noise is automatically created from confusion sets. Additionally, if labelled data Our minimally-augmented GEC approach uses synthetic noise as its primary source of training data. We generate erroneous sentences from monolingual texts via random word perturbations selected from automatically created confusion sets. These are traditionally defined as sets of frequently confused words (Rozovskaya and Roth, 2010). We experiment with three unsupervised methods for generating confusion sets: 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st 2 GNU Aspell supports more than 160 languages: http: //aspell.net/man-html/Supported.html 2 Minimally-augmented grammatical error correction 357 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 357–363 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics Word Confusion set had night then hard head hand gad has ha ad hat knight naught nought nights bight might nightie them the hen ten than thin thee"
D19-5546,Q19-1001,0,0.130228,"approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require any amount of annotated sentences for tuning. Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in unsupervised and low-resource GEC (Rozovskaya et al., 2017; Bryant and Briscoe, 2018; Boyd, 2018; Rozovskaya and Roth, 2019), which recently culminated in the low-resource track of the Building Educational Application (BEA) shared task (Bryant et al., 2019).1 We present Minimally-Augmented Grammatical Error Correction (MAGEC), a simple but effective approach to unsupervised and low-resource GEC which does not require any authentic error-labelled training data. A neural sequence-to-sequence model is trained on clean and synthetically noised sentences alone. The noise is automatically created from confusion sets. Additionally, if labelled data Our minimally-augmented GEC approach uses synthetic noise as its primary s"
D19-5546,J17-4002,0,0.0212988,"sk-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require any amount of annotated sentences for tuning. Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in unsupervised and low-resource GEC (Rozovskaya et al., 2017; Bryant and Briscoe, 2018; Boyd, 2018; Rozovskaya and Roth, 2019), which recently culminated in the low-resource track of the Building Educational Application (BEA) shared task (Bryant et al., 2019).1 We present Minimally-Augmented Grammatical Error Correction (MAGEC), a simple but effective approach to unsupervised and low-resource GEC which does not require any authentic error-labelled training data. A neural sequence-to-sequence model is trained on clean and synthetically noised sentences alone. The noise is automatically created from confusion sets. Additionally, if labelled data Our mini"
D19-5546,N19-1406,0,0.120144,"replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require any amount of annotated sentences for tuning. Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in unsupervised and low-resource GEC (Rozovskaya et al., 2017; Bryant and Briscoe, 2018; Boyd, 2018; Rozovskaya and Roth, 2019), which recently culminated in the low-resource track of the Building Educational Application (BEA) shared task (Bry"
D19-5546,N18-1057,0,0.0545179,"When combined with labelled data, our method can serve as an efficient pre-training technique. 1 Introduction is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique. The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it"
D19-5546,N19-1014,0,0.0506757,"labelled data, our method can serve as an efficient pre-training technique. 1 Introduction is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique. The proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017; Kasewa et al., 2018; Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018; Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary.2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018; Stahlberg et al., 2019), but it does not require an"
D19-5632,W18-2716,1,0.798396,"more than 4x faster than last year’s fastest submission at more than 3 points higher BLEU. Our fastest GPU model at 1.5 seconds translation time is slightly faster than last year’s fastest RNN-based submissions, but outperforms them by more than 4 BLEU and 10 BLEU points respectively. 1 Introduction This paper describes the submissions of the “Marian” team to the Workshop on Neural Generation and Translation (WNGT 2019) efficiency shared task (Hayashi et al., 2019). The goal of the task is to build NMT systems on CPUs and GPUs placed on the Pareto Frontier of efficiency and accuracy. Marian (Junczys-Dowmunt et al., 2018a) is an efficient neural machine translation (NMT) toolkit written in pure C++ based on dynamic computational graphs.1 Marian is a research tool which can ∗ 1 First authors with equal contribution. https://github.com/marian-nmt/marian be used to define state-of-the-art systems that at the same time can produce truly deployment-ready models across different devices. This is accomplished within a single execution engine that does not require specialized, inference-only decoders. Our submissions to last year’s edition of the same shared task defined the Pareto frontiers for translation quality v"
D19-5632,D16-1139,0,0.0240319,"ss stated differently, our student is a single model that follows the Transformer-base configuration (model size 512, filter size 2048, 6 blocks) with modifications. See Section 3 for details. For all models, we use the same vocabulary of 32,000 subwords, computed with SentencePiece (Kudo and Richardson, 2018). The training data is provided by the shared task organizers and restricted to about 4 Million sentences from the WMT news translation task for English-German. Use of other data is not permitted. We again implement the interpolated sequencelevel knowledge distillation method proposed by Kim and Rush (2016): The teacher ensemble is used to forward-translate the training data and collect 8-best lists for each sentence. Choosing the best translation for each sentence based on sentence-level BLEU compared to the original target, we create a synthetic target data. The student is trained on the original source and this synthetic forward translated target. Table 1 contains BLEU scores of the teacher ensemble (T) and a student model distilled from this teacher (Student ← T). The gap is 2.4 BLEU. 2.1 Knowledge distillation with noisy backward-forward translation In our experience, student training benef"
D19-5632,D18-2012,0,0.0346385,"ain four forward (ende) and four inverse (de-en) teacher models according to the Transformer-big configuration (model size 1024, filter size 4096, 6 blocks, file size 813 MiB) from Vaswani et al. (2017). We think of a teacher as the set of all models that have been used to create the artificial training data. Unless stated differently, our student is a single model that follows the Transformer-base configuration (model size 512, filter size 2048, 6 blocks) with modifications. See Section 3 for details. For all models, we use the same vocabulary of 32,000 subwords, computed with SentencePiece (Kudo and Richardson, 2018). The training data is provided by the shared task organizers and restricted to about 4 Million sentences from the WMT news translation task for English-German. Use of other data is not permitted. We again implement the interpolated sequencelevel knowledge distillation method proposed by Kim and Rush (2016): The teacher ensemble is used to forward-translate the training data and collect 8-best lists for each sentence. Choosing the best translation for each sentence based on sentence-level BLEU compared to the original target, we create a synthetic target data. The student is trained on the ori"
D19-5632,D17-1300,0,0.0189122,"been computed with a setup from our WNMT2018 submission (Junczys-Dowmunt et al., 2018b). On batchlevel, a shortlist selects the 75 most common target words and up to 75 most probable translations per input-batch word. This set of words is used to create an output vocabulary matrix over a couple of hundred words instead of 32,000 which reduces the computational load with no loss in quality (compare with GPU-bound BLEU scores in Figure 1b). For systems left of the dotted black line, matrix multiplication is executed with mixed 32-bit (Intel’s MKL library) and 16-bit (own implementation based on Devlin (2017)) kernels. All systems right of the dotted line, are the same model as “SSRUTied” without re-training, but executed with different runtime optimizations. In this section we discuss new runtime optimizations which will be available in Marian v1.9. 4.1 8-bit matrix multiplication with packing The AWS m5.large target platform for CPU-bound decoding is equipped with an Intel Xeon Platinum 8175 CPU. This CPU supports 8-bit integer instructions with AVX-512 (Advanced Vector eXtensions512) which can be used to accelerate deep neural network models (Wu et al., 2016; Rodriguez et al., 2018; Bhandare et"
D19-5632,D18-1045,0,0.0296904,"EU compared to the original target, we create a synthetic target data. The student is trained on the original source and this synthetic forward translated target. Table 1 contains BLEU scores of the teacher ensemble (T) and a student model distilled from this teacher (Student ← T). The gap is 2.4 BLEU. 2.1 Knowledge distillation with noisy backward-forward translation In our experience, student training benefits from forward-translated data that was not seen during teacher training. Since we do not have access to additional monolingual source data, we generate noisy back-translated sentences (Edunov et al., 2018), one set per inverse teacher model. Noisy sentences are generated by sampling from the output softmax distribution via added Gumbel noise. We then use the forward (en-de) teacher ensemble to translate the sampled English sentences into German and choose the best output from the 8-best list measured against the original target. This increases the training corpus 5-fold. Training on this new data reduces the gap to the teacher to 1.3 BLEU; a single teacher model is only 0.4 BLEU better. BLEU Teacher (T) Single teacher model 28.9 28.0 Student without teacher Student ← T Student ← T with 4×NBFT 2"
D19-5632,P18-4020,1,0.842655,"Missing"
D19-5632,D18-1477,0,0.0270981,"der parameters in L2-cache during translation. Moving from SSRU to SSRU-Tied in Figure 1a, we see a 1.39x speed up and a small drop of 0.2 BLEU. The GPU is largely unaffected since cachelocality is less of an issue here. 2 AANs parallelize better during training since the average can be computed non-recurrently, however for the small student models the increase in training time is negligible. 3 We are describing the SRU based on V1 of the preprint on Arxiv from September 2017. Subsequent updates and publications seem to have changed the implementation, resulting in more complex variants, e.g. Lei et al. (2018). We implemented the SRU at time of publication of V1 and missed the updates, but our variant seems to work just fine. 283 4 The transformer uses ReLU non-linearities everywhere. It seems however that student-sized models trained from scratch behave worse when using either SRU or SSRU compared to all the alternatives. There is however no difference between the SRU and SSRU which seems to confirm that the reset-gate rt can be dropped when the additive skipconnection is already present. 6 It does when projections of the encoder into decoder space for purpose of applying cross-attention can be ca"
D19-5632,W18-2715,0,0.0173407,"marized in Table 3. We report model configurations, architectures, dimensions, depth, number of parameters, file sizes in MiB for CPU and GPU, translation speed in words per second and BLEU for newstest2014, omitting newstest2015. Time has been measured by the shared-task organizers on AWS m5.large (CPU) and p3.x2large (GPU) instances. Until this moment, we kept model dimensions and decoder depth constant while optimizing a configuration that corresponds to the Microsoft inproduction models (bold row in Table 3). For the final shared task submissions, we vary model dimensions and — similar to Senellart et al. (2018) — decoder depth in order to explore the trade-offs between quality and speed on the Pareto frontier. We train a shallower base-configuration “(1) Base” with two tied decoder layers and small loss in BLEU compared to the 6-layer version. The speedup is significant on both device types. To cover higher-quality models, we add a “(2) Large” configuration with improved BLEU but slower translation speed. As in the previous year, we do not submit models below 26 BLEU, but due to the improved teacher-student training, we can cut down model size drastically before that threshold is reached. We are see"
D19-5632,P18-1166,0,0.0611285,"discuss the influence of self-regression mechanisms in Section 3.1 and parameter tying in Section 3.2. Architecture-indepedent but devicespecific optimizations for the CPU are detailed in Section 4 and for the GPU in Section 5. More general optimizations are outlined in Section 4.2. Performance has been measured with Marian v1.7, measurements are self-reported by Marian. 3.1 SSRU instead of self-attention or AAN In previous work (Junczys-Dowmunt et al., 2018b) and later experiments, we found that replacing the self-attention mechanims in Transformer decoders with an Average Attention Network (Zhang et al., 2018) or modern RNN variants does not affect student quality while resulting in faster decoding on GPU and CPU. This is mainly caused by reducing the decoder complexity from O(n2 ) to O(n) over the number of output tokens n. In Figure 1 we see how switching from a vanilla Transformer-base variant to a student with AAN improves speed on both devices, but more so on the GPU. While we had good results for AANs in JunczysDowmunt et al. (2018b), we feel somewhat uneasy about the flat element-wise average used to accumulate over inputs. RNNs share the linear computational complexity of the AAN over input"
E17-3017,W14-3346,0,0.0173175,"2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes fe"
E17-3017,W11-2107,0,0.0197012,"r a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes features aimed towards facilitating ex"
E17-3017,W14-3354,0,0.0551932,"Missing"
E17-3017,E17-2025,0,0.133573,"Missing"
E17-3017,W16-2209,1,0.131218,"axout before the softmax layer. • In both encoder and decoder word embedding layers, we do not use additional biases. • Compared to Look, Generate, Update decoder phases in Bahdanau et al. (2015), we implement Look, Update, Generate which drastically simplifies the decoder implementation (see Table 1). • Optionally, we perform recurrent Bayesian dropout (Gal, 2015). • Instead of a single word embedding at each source position, our input representations allows multiple features (or “factors”) at each time step, with the final embedding being the concatenation of the embeddings of each feature (Sennrich and Haddow, 2016). • We allow tying of embedding matrices (Press and Wolf, 2017; Inan et al., 2016). We will here describe some differences in more detail: available at https://github.com/rsennrich/nematus https://github.com/nyu-dl/dl4mt-tutorial 65 Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 65–68 c 2017 Association for Computational Linguistics z0j being the reset and update gate activations. In this formulation, W0 , U0 , Wr0 , U0r , Wz0 , U0z are trained model parameters; σ is the logistic sigmoid activation function. The attention mechanism, ATT, inputs the"
E17-3017,W16-2323,1,0.216734,"Sutskever et al., 2014) has recently established itself as a new state-of-the art in machine translation. We present Nematus1 , a new toolkit for Neural Machine Translation. Nematus has its roots in the dl4mt-tutorial.2 We found the codebase of the tutorial to be compact, simple and easy to extend, while also producing high translation quality. These characteristics make it a good starting point for research in NMT. Nematus has been extended to include new functionality based on recent research, and has been used to build top-performing systems to last year’s shared translation tasks at WMT (Sennrich et al., 2016) and IWSLT (Junczys-Dowmunt and Birch, 2016). Nematus is implemented in Python, and based on the Theano framework (Theano Development Team, 2016). It implements an attentional encoder–decoder architecture similar to Bahdanau et al. (2015). Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus. 1 2 Neural Network Architecture • In the decoder, we use a feedforward hidden layer with tanh non-linearity rathe"
E17-3017,P16-1159,0,0.0772511,"ll documented toolkit to support their research. The toolkit is by no means limited to research, and has been used to train MT systems that are currently in production (WIPO, 2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT2"
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
I17-1013,2011.mtsummit-papers.35,0,0.802875,"Missing"
I17-1013,C12-1014,0,0.0347328,"Missing"
I17-1013,P17-1175,0,0.0686833,"Missing"
I17-1013,P15-2026,0,0.128451,"and conclude in Section 7. 2 Previous work 3 Before the application of neural sequence-tosequence models to APE, most APE systems would rely on phrase-based SMT following a monolingual approach first introduced by Simard et al. (2007). Béchara et al. (2011) proposed a “source-context aware” variant of this approach where automatically created word alignments were used to create a new source language which consisted of joined MT output and source token pairs. The inclusion of source-language information in that form was shown to improve the automatic post-editing results (Béchara et al., 2012; Chatterjee et al., 2015). The quality of the used word alignments plays an important role for this methods, as demonstrated for instance by Pal et al. (2015). During the WMT-2016 APE shared task two systems relied on neural models, the CUNI system (Libovický et al., 2016) and the shared task winner, the system submitted by the AMU team (Junczys-Dowmunt and Grundkiewicz, 2016). This submission explored the application of neural translation models to the APE problem and achieved good results by treating different models as components in a log-linear model, allowing for multiple inputs (the source src and the translated"
I17-1013,P02-1040,0,0.0989136,"1,839 36.63 25.28 Table 1: Statistics for artificial data sets in comparison to official training and development data. Adapted from Junczys-Dowmunt and Grundkiewicz (2016). ,  sj =GRU2 s0j , cj . This could be easily extended to an arbitrary number of encoders with different architectures. During training, this model is fed with a triparallel corpus, and during translation both input sequences are processed simultaneously to produce the corrected output. This model is denoted as M - CGRU. post-edited data. The main task metric is TER (Snover et al., 2006) — the lower the better — with BLEU (Papineni et al., 2002) as a secondary metric. To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) — the authors of the best WMT-2016 APE shared 4.4 Hard Attention with Soft Dual-Attention task system — generated large amounts of artifiAnalogously to the procedure described in seccial data via round-trip translations. The artificial tion 4.2, we can extend the doubly-attentive cGRU data has been filtered to match the HTER statistics to take the hard-attended encoder context as addiof the training and development data for the shared tional input: 6  task and was made availabl"
I17-1013,E17-3017,1,0.792903,"ad not been seen in mt. The influence of the components on the final result was tuned with Minimum Error Rate Training (Och, 2003) with regard to the task metric TER. Following the WMT-2016 APE shared task, Pal et al. (2017) published work on another neural APE system that integrated precomputed wordalignment features into the neural structure and enAttentional Encoder-Decoder Implementations of all models explored in this paper are available in the Marian1 toolkit (JunczysDowmunt et al., 2016). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017). The model differs from the standard model introduced by Bahdanau et al. (2015) by several aspects, the most important being the conditional GRU with attention. The summary provided in this section is based on the description in Sennrich et al. (2017). Given the raw MT output sequence (x1 , . . . , xTx ) of length Tx and its manually post-edited equivalent (y1 , . . . , yTy ) of length Ty , we construct the encoder-decoder model using the following formulations. Encoder context A single forward encoder state → − h i is calculated as: → − → − h i = GRU( h i−1 , F[xi ]), where F is the encoder"
I17-1013,W16-2378,1,0.884691,"decoder models with soft attention. We focus on neural end-to-end models that combine both inputs mt and src in a single neural architecture, modeling {mt, src} → pe directly. Apart from that, we investigate the influence of hard-attention models, which seem to be well-suited for monolingual tasks. Finally, we create combinations of both architectures. We report results on data sets provided during the WMT-2016 shared task on automatic postediting (Bojar et al., 2016) and compare our performance against the shared task winner, the system submitted by the Adam Mickiewicz University (AMU) team (Junczys-Dowmunt and Grundkiewicz, 2016), and a more recent system by Pal et al. (2017) with the previously best published results on the same test set. Our main contributions are: (1) we perform a thorough comparison of end-to-end neural approaches to APE during which (2) we demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model achieve the best reported results for the WMT-2016 APE task, and (3) show that models with a hard-attention mechanism reach competitive results although they execute fewer edits than models relying only on soft attention. The remainder of the paper i"
I17-1013,P16-1162,0,0.0682225,"r consample the original training data 20 times and add text Cmt is attended to by the hard monotonic atboth artificial data sets. This results in a total of tention mechanism. The target training data conslightly more than 5M training triplets. We valsists of the step/token sequences used for all preidate on the development set for early stopping vious hard-attention models. We call this model and report results on the WMT-2016 test set. The M - CGRU - HARD . data is already tokenized. Additionally we truecase all files and apply segmentation into BPE sub5 Experiments and Results word units (Sennrich et al., 2016). We reuse the subword units distributed with the artificial data 5.1 Training, Development, and Test Data set. For the hard-attention models, we create tar5 We perform all our experiments with the official get training and development files following the WMT-2016 (Bojar et al., 2016) automatic postLCS-based procedure outlined in section 4.1. editing data and the respective development and test sets. The training data consists of a small 5.2 Training parameters set of 12,000 post-editing triplets (src, mt, pe), where src is the original English text, mt is All models are trained on the same tr"
I17-1013,P07-2045,0,0.0161484,"ith Asynchronous SGD (Adam) on three to four GPUs. 5.3 • We train all models until convergence (earlystopping with a patience of 10 based on development set cross-entropy cost), saving model checkpoints every 10,000 minibatches. For different models we observed early stopping to be triggered between 600,000 and 900,000 mini-batch updates or between 8 and 11 epochs. Evaluation Table 2 contains relevant results for the WMT2016 APE shared task — during the task and afterwards. WMT-2016 BASELINE -1 is the raw uncorrected MT output. BASELINE -2 is the result of a vanilla phrase-based Moses system (Koehn et al., 2007) trained only on the official 12,000 sentences. Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal 125 Model TER-pe TER-mt Model Mod. Imp. Det. CGRU 22.27 12.01 CGRU 1575 871 399 GRU - HARD 22.72 22.10 9.48 11.57 GRU - HARD 1479 1564 783 897 362 371 M - CGRU 20.69 19.92 15.98 15.41 M - CGRU 1668 1612 1020 1037 379 322 20.87 20.34 13.62 13.34 M - CGRU - HARD 1688 1672 1044 1074 388 341 CGRU - HARD M - CGRU ×4 M - CGRU - HARD M - CGRU - HARD ×4 CGRU - HARD M - CGRU M - CGRU - HARD Table 4: TER w.r.t. the reference compared to TER w.r.t. the input on test 2016. Low"
I17-1013,N07-1064,0,0.322605,"ed their results even further. None of their systems, however, integrated information from src, all modeled mt → pe. nisms are: hard-attention in Section 4.1, a combination of hard attention and soft attention in Section 4.2, dual soft attention in Section 4.3 and a combination of hard attention and dual soft attention in Section 4.4. We describe experiments and results in Section 5 and conclude in Section 7. 2 Previous work 3 Before the application of neural sequence-tosequence models to APE, most APE systems would rely on phrase-based SMT following a monolingual approach first introduced by Simard et al. (2007). Béchara et al. (2011) proposed a “source-context aware” variant of this approach where automatically created word alignments were used to create a new source language which consisted of joined MT output and source token pairs. The inclusion of source-language information in that form was shown to improve the automatic post-editing results (Béchara et al., 2012; Chatterjee et al., 2015). The quality of the used word alignments plays an important role for this methods, as demonstrated for instance by Pal et al. (2015). During the WMT-2016 APE shared task two systems relied on neural models, th"
I17-1013,P17-2031,0,0.0572649,"ing. This model is called CGRU - HARD. 1. The end-of-sentence symbol can only be generated if the hard attention mechanism has reached the end of the input sequence, enforcing full coverage; 4.3 2. The hSTEPi symbol cannot be generated once the end-of-sentence position in the source has been reached. It is however still possible to generate content tokens. Soft Dual-Attention Neural multi-source models (Zoph and Knight, 2016) seem to be a natural fit for the APE task as raw MT output and original source language input are available. Although applications to the APE problem have been reported (Libovický and Helcl, 2017), state-of-the-art results seem to be missing. In this section we give details about our dualsource model implementation. We rename the existing encoder C to Cmt to signal that the first encoder consumes the raw MT output and introduce a structurally identical second encoder C src = src {hsrc 1 , . . . , hTsrc } over the source language. To compute the decoder start state s0 for the multiencoder model we concatenate the averaged encoder contexts before mapping them into the decoder state space: This model requires a target sequence with correctly inserted hSTEPi symbols. For the described APE"
I17-1013,2006.amta-papers.25,0,0.182085,"ntences TER 12,000 1,000 2,000 26.22 24.81 – 4,335,715 531,839 36.63 25.28 Table 1: Statistics for artificial data sets in comparison to official training and development data. Adapted from Junczys-Dowmunt and Grundkiewicz (2016). ,  sj =GRU2 s0j , cj . This could be easily extended to an arbitrary number of encoders with different architectures. During training, this model is fed with a triparallel corpus, and during translation both input sequences are processed simultaneously to produce the corrected output. This model is denoted as M - CGRU. post-edited data. The main task metric is TER (Snover et al., 2006) — the lower the better — with BLEU (Papineni et al., 2002) as a secondary metric. To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) — the authors of the best WMT-2016 APE shared 4.4 Hard Attention with Soft Dual-Attention task system — generated large amounts of artifiAnalogously to the procedure described in seccial data via round-trip translations. The artificial tion 4.2, we can extend the doubly-attentive cGRU data has been filtered to match the HTER statistics to take the hard-attended encoder context as addiof the training and development data"
I17-1013,W16-2361,0,0.045269,". (2011) proposed a “source-context aware” variant of this approach where automatically created word alignments were used to create a new source language which consisted of joined MT output and source token pairs. The inclusion of source-language information in that form was shown to improve the automatic post-editing results (Béchara et al., 2012; Chatterjee et al., 2015). The quality of the used word alignments plays an important role for this methods, as demonstrated for instance by Pal et al. (2015). During the WMT-2016 APE shared task two systems relied on neural models, the CUNI system (Libovický et al., 2016) and the shared task winner, the system submitted by the AMU team (Junczys-Dowmunt and Grundkiewicz, 2016). This submission explored the application of neural translation models to the APE problem and achieved good results by treating different models as components in a log-linear model, allowing for multiple inputs (the source src and the translated sentence mt) that were decoded to the same target language (post-edited translation pe). Two systems were considered, one using src as the input (src → pe) and another using mt as the input (mt → pe). A simple string-matching penalty integrated wi"
I17-1013,N16-1004,0,0.0197407,"get:    sj = cGRUatt sj−1 , E[yj−1 ]; haj , C . (4) The rest of the model is unchanged; the translation process is the same as before and we use the same target step/token sequence for training. This model is called CGRU - HARD. 1. The end-of-sentence symbol can only be generated if the hard attention mechanism has reached the end of the input sequence, enforcing full coverage; 4.3 2. The hSTEPi symbol cannot be generated once the end-of-sentence position in the source has been reached. It is however still possible to generate content tokens. Soft Dual-Attention Neural multi-source models (Zoph and Knight, 2016) seem to be a natural fit for the APE task as raw MT output and original source language input are available. Although applications to the APE problem have been reported (Libovický and Helcl, 2017), state-of-the-art results seem to be missing. In this section we give details about our dualsource model implementation. We rename the existing encoder C to Cmt to signal that the first encoder consumes the raw MT output and introduce a structurally identical second encoder C src = src {hsrc 1 , . . . , hTsrc } over the source language. To compute the decoder start state s0 for the multiencoder mode"
I17-1013,P03-1021,0,0.0267105,"g for multiple inputs (the source src and the translated sentence mt) that were decoded to the same target language (post-edited translation pe). Two systems were considered, one using src as the input (src → pe) and another using mt as the input (mt → pe). A simple string-matching penalty integrated within the log-linear model was used to control for higher faithfulness with regard to the raw MT output. The penalty fired if the APE system proposed a word in its output that had not been seen in mt. The influence of the components on the final result was tuned with Minimum Error Rate Training (Och, 2003) with regard to the task metric TER. Following the WMT-2016 APE shared task, Pal et al. (2017) published work on another neural APE system that integrated precomputed wordalignment features into the neural structure and enAttentional Encoder-Decoder Implementations of all models explored in this paper are available in the Marian1 toolkit (JunczysDowmunt et al., 2016). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017). The model differs from the standard model introduced by Bahdanau et al. (2015) by several aspects, the mos"
I17-1013,E17-2056,0,0.102383,"Missing"
I17-1013,W15-3026,0,0.0393867,"Missing"
L16-1561,chen-eisele-2012-multiun,0,0.0193426,"e UN translation toolkit. DGACM publishes documents in the six official UN languages and additionally in German2 and is running major translation operations in various locations. The global DGACM translation output for 2014 alone was 231 million words. The translated documents are hosted on the Official Document System3 (ODS) and are publicly available. Historically, this parallel data has been a major resource for SMT and NLP research, and has resulted in various (unofficial) corpora, most of them incomplete due to resorting to scraping ODS (Rafalovitch and Dale, 2009; Eisele and Chen, 2010; Chen and Eisele, 2012). Other resources are also available from the Linguistic Data Consortium (LDC)4 . Depending on the language pair, the present corpus is between two (e.g. en-fr) to four times (e.g. en-ru) larger than data published by (Chen and Eisele, 2012); half of the documents are available for all six languages. The scope of documents used for the SMT models has continuously expanded as additional United Nations documents have become available. The present corpus is the result of this going collection process. The sharing of technology, expertise, and data has proven to be a crucial factor in enabling the"
L16-1561,P13-2071,0,0.0351676,"-house Moses (Koehn et al., 2007) systems that were trained on the described data. Sentences longer than 100 words were discarded. To speed up the word alignment procedure, we split the training corpora into four equally sized parts that are aligned with MGIZA++ (Gao and Vogel, 2008), running 5 iterations of Model 1 and the HMM model on each part.10 We use a 5gram language model trained from the target parallel data, with 3-grams or higher order being pruned if they occur only once. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al., 2013) (3-grams and 4-grams are pruned if they occur only once, 5-grams and 6-grams if they occur only twice, etc.), both trained using KenLM (Heafield et al., 2013). To reduce the phrase-table size, we apply significance pruning (Johnson et al., 2007) and use the compact phrase-table and reordering data structures (Junczys-Dowmunt, 2012). During decoding, we use the cube-pruning algorithm with stack size and cube-pruning pop limits of 1,000. All scores ar"
L16-1561,eisele-chen-2010-multiun,0,0.0108529,"tical global tool in the UN translation toolkit. DGACM publishes documents in the six official UN languages and additionally in German2 and is running major translation operations in various locations. The global DGACM translation output for 2014 alone was 231 million words. The translated documents are hosted on the Official Document System3 (ODS) and are publicly available. Historically, this parallel data has been a major resource for SMT and NLP research, and has resulted in various (unofficial) corpora, most of them incomplete due to resorting to scraping ODS (Rafalovitch and Dale, 2009; Eisele and Chen, 2010; Chen and Eisele, 2012). Other resources are also available from the Linguistic Data Consortium (LDC)4 . Depending on the language pair, the present corpus is between two (e.g. en-fr) to four times (e.g. en-ru) larger than data published by (Chen and Eisele, 2012); half of the documents are available for all six languages. The scope of documents used for the SMT models has continuously expanded as additional United Nations documents have become available. The present corpus is the result of this going collection process. The sharing of technology, expertise, and data has proven to be a crucia"
L16-1561,W08-0509,0,0.0133029,"gned document pairs 1,727,539 Documents 86,307 (b) Document statistics Lines 11,365,709 English Tokens 334,953,817 (c) Statistics for fully aligned subcorpus Table 1: Statistics for the United Nations Corpus v1.0 (1990 – 2014) 8. Machine Translation Baselines Based on the described test sets we also provide baseline results for our in-house Moses (Koehn et al., 2007) systems that were trained on the described data. Sentences longer than 100 words were discarded. To speed up the word alignment procedure, we split the training corpora into four equally sized parts that are aligned with MGIZA++ (Gao and Vogel, 2008), running 5 iterations of Model 1 and the HMM model on each part.10 We use a 5gram language model trained from the target parallel data, with 3-grams or higher order being pruned if they occur only once. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al., 2013) (3-grams and 4-grams are pruned if they occur only once, 5-grams and 6-grams if they occur only twice, etc.), both t"
L16-1561,P13-2121,0,0.00862001,"ns of Model 1 and the HMM model on each part.10 We use a 5gram language model trained from the target parallel data, with 3-grams or higher order being pruned if they occur only once. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al., 2013) (3-grams and 4-grams are pruned if they occur only once, 5-grams and 6-grams if they occur only twice, etc.), both trained using KenLM (Heafield et al., 2013). To reduce the phrase-table size, we apply significance pruning (Johnson et al., 2007) and use the compact phrase-table and reordering data structures (Junczys-Dowmunt, 2012). During decoding, we use the cube-pruning algorithm with stack size and cube-pruning pop limits of 1,000. All scores are provided for lowercased data; the data was tokenized with the Moses tokenizer. For Chinese segmentation we used Jieba11 before applying the Moses tokenizer. Full Data into and from English At DGACM, translation is mainly done between English and the remaining lan10 We confirmed that there seemed to be"
L16-1561,D07-1103,0,0.0619939,"ry/fr/2010/cd/1890.doc Extract text Convert to TEI Extract text Convert to TEI Xml/en/2010/cd/1890.xml Xml/fr/2010/cd/1890.xml Split paragraphs into sentences Extract text Tokenize Lowercase Extract text Tokenize Lowercase tokenized/en/2010/cd/1890.tok domly select a subset of 10,000 document pairs and align them using Hunalign (Varga et al., 2005), selecting only 11 alignments that are themselves surrounded by 1-1 alignments. This small lower-quality parallel corpus is used to train an SMT system with Moses (Koehn et al., 2007). Following Sennrich and Volk (2011) we use significance pruning (Johnson et al., 2007) to filter out noise resulting from alignment errors. Next, our monolingual sentence aligner BLEU-Champ9 is applied. BLEU-Champ relies on smoothed sentence level BLEU-2 as a similarity metric between sentences and uses the Champollion algorithm (Ma, 2006) with that metric. In order to avoid computational bottlenecks for long documents, first a path consisting only of 0-1, 1-0, 1-1 alignments is calculated. In a second step, the search is restricted to a 10-sentence-wide corridor around the best path allowing for all alignment combinations up to 4-4 alignments. This procedure avoids search erro"
L16-1561,P07-2045,0,0.0603749,"g/en/docs/symbols 7 http://lib-thesaurus.un.org/ 3531 Binary/en/2010/cd/1890.doc Binary/fr/2010/cd/1890.doc Extract text Convert to TEI Extract text Convert to TEI Xml/en/2010/cd/1890.xml Xml/fr/2010/cd/1890.xml Split paragraphs into sentences Extract text Tokenize Lowercase Extract text Tokenize Lowercase tokenized/en/2010/cd/1890.tok domly select a subset of 10,000 document pairs and align them using Hunalign (Varga et al., 2005), selecting only 11 alignments that are themselves surrounded by 1-1 alignments. This small lower-quality parallel corpus is used to train an SMT system with Moses (Koehn et al., 2007). Following Sennrich and Volk (2011) we use significance pruning (Johnson et al., 2007) to filter out noise resulting from alignment errors. Next, our monolingual sentence aligner BLEU-Champ9 is applied. BLEU-Champ relies on smoothed sentence level BLEU-2 as a similarity metric between sentences and uses the Champollion algorithm (Ma, 2006) with that metric. In order to avoid computational bottlenecks for long documents, first a path consisting only of 0-1, 1-0, 1-1 alignments is calculated. In a second step, the search is restricted to a 10-sentence-wide corridor around the best path allowing"
L16-1561,ma-2006-champollion,0,0.0232044,"subset of 10,000 document pairs and align them using Hunalign (Varga et al., 2005), selecting only 11 alignments that are themselves surrounded by 1-1 alignments. This small lower-quality parallel corpus is used to train an SMT system with Moses (Koehn et al., 2007). Following Sennrich and Volk (2011) we use significance pruning (Johnson et al., 2007) to filter out noise resulting from alignment errors. Next, our monolingual sentence aligner BLEU-Champ9 is applied. BLEU-Champ relies on smoothed sentence level BLEU-2 as a similarity metric between sentences and uses the Champollion algorithm (Ma, 2006) with that metric. In order to avoid computational bottlenecks for long documents, first a path consisting only of 0-1, 1-0, 1-1 alignments is calculated. In a second step, the search is restricted to a 10-sentence-wide corridor around the best path allowing for all alignment combinations up to 4-4 alignments. This procedure avoids search errors and is fast enough to use the Champollion algorithm with documents consisting of thousands of sentences. Given the English tokenized text and the translated French text, BLEU-Champ produces a ladder file (Hunalign’s numeric alignment format) which even"
L16-1561,2011.eamt-1.2,1,0.588161,"e archive of parallel documents from its own translation operations. Multilingualism is a strategic priority for the United Nations, as an essential factor in harmonious communication among peoples. The official publication of this corpus is a reaction to the growing importance of statistical machine translation (SMT) within the UN Department for General Assembly and Conference Management (DGACM) translation services. In 2011, a research project — in cooperation with the World Intellectual Property Organization (WIPO) — to explore a prototype SMT system based on the TAPTA system used at WIPO (Pouliquen et al., 2011) for the language pair English-Spanish (Pouliquen et al., 2012) was spearheaded by the Spanish Translation Service (STS) in New York and quickly noticed by other United Nations language services. Further development underlined the good performance of the SMT approach and its applicability to UN translation services. The system was expanded (Pouliquen et al., 2013) to a total of 10 language pairs, resulting in a productiongrade cloud-based SMT service called TAPTA4UN. Especially since its integration with the in-house computer assisted translation (CAT) tool eLUNa, TAPTA4UN has become a critica"
L16-1561,2012.eamt-1.4,1,0.808434,"ions. Multilingualism is a strategic priority for the United Nations, as an essential factor in harmonious communication among peoples. The official publication of this corpus is a reaction to the growing importance of statistical machine translation (SMT) within the UN Department for General Assembly and Conference Management (DGACM) translation services. In 2011, a research project — in cooperation with the World Intellectual Property Organization (WIPO) — to explore a prototype SMT system based on the TAPTA system used at WIPO (Pouliquen et al., 2011) for the language pair English-Spanish (Pouliquen et al., 2012) was spearheaded by the Spanish Translation Service (STS) in New York and quickly noticed by other United Nations language services. Further development underlined the good performance of the SMT approach and its applicability to UN translation services. The system was expanded (Pouliquen et al., 2013) to a total of 10 language pairs, resulting in a productiongrade cloud-based SMT service called TAPTA4UN. Especially since its integration with the in-house computer assisted translation (CAT) tool eLUNa, TAPTA4UN has become a critical global tool in the UN translation toolkit. DGACM publishes do"
L16-1561,2013.mtsummit-user.7,1,0.860036,"Missing"
L16-1561,W15-4927,1,0.738983,") to four times (e.g. en-ru) larger than data published by (Chen and Eisele, 2012); half of the documents are available for all six languages. The scope of documents used for the SMT models has continuously expanded as additional United Nations documents have become available. The present corpus is the result of this going collection process. The sharing of technology, expertise, and data has proven to be a crucial factor in enabling the adoption of machine translation (MT) at the UN. In the past, DGACM has successfully shared its translation models with other organizations such as WIPO, IMO (Pouliquen et al., 2015), FAO and ILO. Consequently, in order to facilitate research into and the adoption and development of SMT, DGACM is making available a more complete corpus of its parallel documents in a reusable format, including sentence level alignments. 2. License and Availability The UN parallel corpus is composed of official records and other parliamentary documents of the United Nations that are in the public domain. The UN corpus will be made available for download at http:// conferences.unite.un.org/UNCorpus. The following disclaimer5 , an integral part of the corpus, shall be respected with regard to"
L16-1561,2009.mtsummit-posters.15,0,0.140942,"a, TAPTA4UN has become a critical global tool in the UN translation toolkit. DGACM publishes documents in the six official UN languages and additionally in German2 and is running major translation operations in various locations. The global DGACM translation output for 2014 alone was 231 million words. The translated documents are hosted on the Official Document System3 (ODS) and are publicly available. Historically, this parallel data has been a major resource for SMT and NLP research, and has resulted in various (unofficial) corpora, most of them incomplete due to resorting to scraping ODS (Rafalovitch and Dale, 2009; Eisele and Chen, 2010; Chen and Eisele, 2012). Other resources are also available from the Linguistic Data Consortium (LDC)4 . Depending on the language pair, the present corpus is between two (e.g. en-fr) to four times (e.g. en-ru) larger than data published by (Chen and Eisele, 2012); half of the documents are available for all six languages. The scope of documents used for the SMT models has continuously expanded as additional United Nations documents have become available. The present corpus is the result of this going collection process. The sharing of technology, expertise, and data ha"
L16-1561,W11-4624,0,0.149124,"thesaurus.un.org/ 3531 Binary/en/2010/cd/1890.doc Binary/fr/2010/cd/1890.doc Extract text Convert to TEI Extract text Convert to TEI Xml/en/2010/cd/1890.xml Xml/fr/2010/cd/1890.xml Split paragraphs into sentences Extract text Tokenize Lowercase Extract text Tokenize Lowercase tokenized/en/2010/cd/1890.tok domly select a subset of 10,000 document pairs and align them using Hunalign (Varga et al., 2005), selecting only 11 alignments that are themselves surrounded by 1-1 alignments. This small lower-quality parallel corpus is used to train an SMT system with Moses (Koehn et al., 2007). Following Sennrich and Volk (2011) we use significance pruning (Johnson et al., 2007) to filter out noise resulting from alignment errors. Next, our monolingual sentence aligner BLEU-Champ9 is applied. BLEU-Champ relies on smoothed sentence level BLEU-2 as a similarity metric between sentences and uses the Champollion algorithm (Ma, 2006) with that metric. In order to avoid computational bottlenecks for long documents, first a path consisting only of 0-1, 1-0, 1-1 alignments is calculated. In a second step, the search is restricted to a 10-sentence-wide corridor around the best path allowing for all alignment combinations up t"
L16-1561,steinberger-etal-2006-jrc,1,0.210322,"Missing"
N18-1055,P06-1032,0,0.12591,"ethods from statistical machine translation (SMT), especially the phrase-based variant. For the CoNLL 2014 benchmark on grammatical error correction (Ng et al., 2014), Junczys-Dowmunt and Grundkiewicz (2016) established a set of methods for GEC by SMT that remain state-of-the-art. Systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017) that improve on results by Junczys-Dowmunt and Grundkiewicz (2016) use their set-up as a backbone for more complex systems. The view that GEC can be approached as a machine translation problem by translating from erroneous to correct text originates from Brockett et al. (2006) and resulted in many systems (e.g. Felice et al., 2014; Susanto et al., 2014) that represented the current state-of-the-art at the time. In the field of machine translation proper, the emergence of neural sequence-to-sequence methods and their impressive results have lead to a paradigm shift away from phrase-based SMT towards neural machine translation (NMT). During WMT 2017 (Bojar et al., 2017) authors of pure phrase-based systems offered “unconditional surrender”1 to NMT-based methods. Based on these developments, one would expect to see a rise of state-of-the-art neural methods for GEC, bu"
N18-1055,2011.mtsummit-papers.1,0,0.0502984,"n the paper. We did not see any differences compared to smaller beams. – 70.8 – 9.5 32.9 30.9 47.2 47.0 52.1 52.5 44.0 Average of 4 Ensemble of 4 40.0 t-S rc ou +D ro p +D om Optimizer instability Junczys-Dowmunt and Grundkiewicz (2016) noticed that discriminative parameter tuning for GEC by phrase-based SMT leads to unstable M2 results between tuning runs. This is a well-known effect for SMT parameter tuning and Clark et al. (2011) recommend reporting results for multiple tuning runs. Junczys-Dowmunt and Grundkiewicz (2016) perform four tuning runs and calculate parameter centroids following Cettolo et al. (2011). Neural sequence-to-sequence training is discriminative optimization and as such prone to instability. We already try to alleviate this by averaging over eight best checkpoints, but as seen in Table 3, results for M2 remain unstable for runs with differently initialized weights. An amplitude of 3 points M2 on the CoNLL-2014 test set is larger than most improvements reported in recent papers. None of the recent works on neural GEC account for instability, hence it is unclear if observed outcomes are actual improvements or lucky picks among byproducts of instability. We therefore strongly sugge"
N18-1055,W17-5037,0,0.471992,"we segment training and test/dev data accordingly. Segmentation is reversed before evaluation. Table 2: Statistics for test and development data. 2.1 Training and test data To make our results comparable to state-of-the-art results in the field of GEC, we limit our training data strictly to public resources. In the case of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our met"
N18-1055,P11-2031,0,0.0393378,"thub.com/grammatical/ neural-naacl2018 5 https://github.com/marian-nmt/marian 6 We used a larger beam-size than usual due to experiments with re-ranking of n-best lists not included in the paper. We did not see any differences compared to smaller beams. – 70.8 – 9.5 32.9 30.9 47.2 47.0 52.1 52.5 44.0 Average of 4 Ensemble of 4 40.0 t-S rc ou +D ro p +D om Optimizer instability Junczys-Dowmunt and Grundkiewicz (2016) noticed that discriminative parameter tuning for GEC by phrase-based SMT leads to unstable M2 results between tuning runs. This is a well-known effect for SMT parameter tuning and Clark et al. (2011) recommend reporting results for multiple tuning runs. Junczys-Dowmunt and Grundkiewicz (2016) perform four tuning runs and calculate parameter centroids following Cettolo et al. (2011). Neural sequence-to-sequence training is discriminative optimization and as such prone to instability. We already try to alleviate this by averaging over eight best checkpoints, but as seen in Table 3, results for M2 remain unstable for runs with differently initialized weights. An amplitude of 3 points M2 on the CoNLL-2014 test set is larger than most improvements reported in recent papers. None of the recent"
N18-1055,N12-1067,0,0.398337,"nd Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beg"
N18-1055,W13-1703,0,0.405661,"C, however, an analysis on advantages of word versus sub-word or character level segmentation is beyond the scope of this paper. A set of 50,000 monolingual BPE units is trained on the error-annotated data and we segment training and test/dev data accordingly. Segmentation is reversed before evaluation. Table 2: Statistics for test and development data. 2.1 Training and test data To make our results comparable to state-of-the-art results in the field of GEC, we limit our training data strictly to public resources. In the case of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores"
N18-1055,W17-3203,0,0.207254,"aining objective for GEC. We investigate how to leverage monolingual data for neural GEC by transfer learning in Section 4 and experiment with language model ensembling in Section 5. Section 6 explores deep NMT architectures. In Section 7, we provide an overview of the experiments and how results relate to the JFLEG benchmark. We also recommend a model-independent toolbox for neural GEC. 2 A trustable baseline for neural GEC In this section, we combine insights from JunczysDowmunt and Grundkiewicz (2016) for grammatical error correction by phrase-based statistical machine translation and from Denkowski and Neubig (2017) for trustable results in neural machine translation to propose a trustable baseline for neural grammatical error correction. 596 Test/Dev set Sent. Annot. Metric CoNLL-2013 test CoNLL-2014 test JFLEG dev JFLEG test 1,381 1,312 754 747 1 2 4 4 M2 M2 GLEU GLEU large-vocabulary problem of NMT. This is a well established procedure in neural machine translation and has been demonstrated to be generally superior to UNK-replacement methods. It has been largely ignored in the field of grammatical error correction even when word segmentation issues have been explored (Ji et al., 2017; Schmaltz et al.,"
N18-1055,W17-4724,0,0.0793217,"al methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) published impressive new results for neural GEC with some overlap with our methods. However, our results stay ahead on all benchmarks while using simpler models. 595 Proceedings of NAACL-HLT 2018, pages 595–606 c New Orleans, Louisiana, June 1 - 6,"
N18-1055,N13-1073,0,0.0285722,"NUCLE corpus ten times to the training corpus. This can also be seen as similar to Junczys-Dowmunt and Grundkiewicz (2016) who tune phrase-based SMT parameters on the entire NUCLE corpus. Respectable improvements on both CoNLL test sets (+Domain-Adapt. in Table 4) are achieved. 3.3 Λ L(x, y, a) = − Ty X t=1 λ(xat , yt ) = λ(xat , yt ) log P (yt |x, y&lt;t ),  Λ if xat 6= yt , 1 otherwise where (x, y) is a training sentence pair and a is a word alignment at ∈ {0, 1, . . . , Tx } such that source token xat generates target token yt . Alignments are computed for each sentence pair with fast-align (Dyer et al., 2013). 599 7 Output embeddings are encoded in the last output layer of a neural language or translation model. output embedding Pre-trained embeddings Pre-trained decoder parameters Randomly initialized parameters first output layer second cGRU block attention mechanism bidirectional GRU first cGRU block source embedding target embedding encoder decoder Figure 2: Parameters pretrained on monolingual data are marked with colors. Blue indicates pre-trained embeddings with word2vec, red parameters have been pre-trained with the GRU-based language model only. All embedding layers have tied parameters."
N18-1055,W14-1702,0,0.498932,"Missing"
N18-1055,P17-1070,0,0.497382,"ards neural machine translation (NMT). During WMT 2017 (Bojar et al., 2017) authors of pure phrase-based systems offered “unconditional surrender”1 to NMT-based methods. Based on these developments, one would expect to see a rise of state-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) publ"
N18-1055,D16-1161,1,0.919021,"kens. Among these the Lang-8 corpus is quite noisy and of low quality. The Cambridge Learner Corpus (CLC) by Nicholls (2003) — probably the best resource in this list — is non-public and we would strongly discourage reporting results that include it as training data as this makes comparisons difficult. Contrasting this with Fig. 1, we see that for about 20M tokens NMT systems start outperforming SMT models without additional large language models. Current state-of-the-art GEC systems based on SMT, however, all include large-scale indomain language models either following the steps outlined in Junczys-Dowmunt and Grundkiewicz (2016) or directly re-using their domain-adapted Common-Crawl language model. It seems that the current state of neural methods in GEC reflects the behavior for NMT systems trained on smaller data sets. Based on this, we conclude that we can think of GEC as a lowresource, or at most mid-resource, machine translation problem. This means that techniques proposed for low-resource (neural) MT should be applicable to improving neural GEC results. In this work we show that adapting techniques from low-resource (neural) MT and SMT-based GEC methods allows neural GEC systems to catch up to and outperform SM"
N18-1055,P18-4020,1,0.882714,"Missing"
N18-1055,W17-3204,0,0.025851,"te-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) published impressive new results for neural GEC with some overlap with our methods. However, our results stay ahead on all benchmarks while using simpler models. 595 Proceedings of NAACL-HLT 2018, pages 595–606 c New Orleans, Louisiana, June 1 - 6,"
N18-1055,D17-1156,0,0.0234167,"nd the objective function were modified. In this section we investigate if these techniques can be generalized to deeper or different architectures. 6.1 Architectures Dev Prec. Rec. Test +Pretrain-Dec. +GRU-LM 40.3 41.6 65.2 62.2 32.2 36.6 54.1 54.6 +Deep-RNN +Deep-RNN-LM 41.1 41.9 64.3 61.3 35.2 40.2 55.2 55.5 +Transformer +Transformer-LM 41.5 42.9 63.0 61.9 38.9 40.2 56.1 55.8 Table 8: Shallow (Pretrain-Dec.) versus deep ensembles, with and without corresponding language models. We consider two state-of-the-art NMT architectures implemented in Marian: Deep RNN A deep RNN-based model (Miceli Barone et al., 2017) proposed by Sennrich et al. (2017a) for their WMT 2017 submissions. This model is based on the shallow model we used until now. It has single layer RNNs in the encoder and decoder, but increases depth by stacking multiple GRU-style blocks inside one RNN cell. A single RNN step passes through all blocks before recursion. The encoder RNN contains 4 stacked GRU blocks, the decoder 8 (1 + 7 due to the conditional GRU). Following Sennrich et al. (2017a), we enable layer-normalization in the RNN-layers. State and embedding dimensions used throughout this work and in Sennrich et al. (2017a) are the"
N18-1055,W17-4710,0,0.0254019,"nd the objective function were modified. In this section we investigate if these techniques can be generalized to deeper or different architectures. 6.1 Architectures Dev Prec. Rec. Test +Pretrain-Dec. +GRU-LM 40.3 41.6 65.2 62.2 32.2 36.6 54.1 54.6 +Deep-RNN +Deep-RNN-LM 41.1 41.9 64.3 61.3 35.2 40.2 55.2 55.5 +Transformer +Transformer-LM 41.5 42.9 63.0 61.9 38.9 40.2 56.1 55.8 Table 8: Shallow (Pretrain-Dec.) versus deep ensembles, with and without corresponding language models. We consider two state-of-the-art NMT architectures implemented in Marian: Deep RNN A deep RNN-based model (Miceli Barone et al., 2017) proposed by Sennrich et al. (2017a) for their WMT 2017 submissions. This model is based on the shallow model we used until now. It has single layer RNNs in the encoder and decoder, but increases depth by stacking multiple GRU-style blocks inside one RNN cell. A single RNN step passes through all blocks before recursion. The encoder RNN contains 4 stacked GRU blocks, the decoder 8 (1 + 7 due to the conditional GRU). Following Sennrich et al. (2017a), we enable layer-normalization in the RNN-layers. State and embedding dimensions used throughout this work and in Sennrich et al. (2017a) are the"
N18-1055,C12-2084,0,0.387332,"Missing"
N18-1055,W17-5039,0,0.317175,"Missing"
N18-1055,E17-2037,0,0.359355,"al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Foll"
N18-1055,W14-1701,0,0.581413,"results in the field of GEC, we limit our training data strictly to public resources. In the case of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preproc"
N18-1055,W13-3601,0,0.321815,"of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization"
N18-1055,D17-1039,0,0.0626855,"Missing"
N18-1055,Q16-1013,0,0.0340052,"zysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enc"
N18-1055,D14-1102,0,0.08616,"Missing"
N18-1055,I17-2062,0,0.083227,"Missing"
N18-1055,D17-1298,0,0.400682,"ring WMT 2017 (Bojar et al., 2017) authors of pure phrase-based systems offered “unconditional surrender”1 to NMT-based methods. Based on these developments, one would expect to see a rise of state-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) published impressive new results for neural GEC with"
N18-1055,W17-4739,1,0.892616,"included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enchant3 spell-checker to the JFLEG data before evaluation. No spellchecking is used for the CoNLL test sets. We follow the recommendation by Denkowski and Neubig (2017) to use byte-pair encoding (BPE) sub-word units (Sennrich et al., 2016b) to solve the 3 2.3 Model and training procedure Implementations of all models explored in this work4 are available in the Marian5 toolkit (JunczysDowmunt et al., 2018). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017b). The model differs from the model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention for which Sennrich et al. (2017b) provide a concise description. All embedding vectors consist of 512 units; the RNN states of 1024 units. The number of BPE segments determines the size of the vocabulary of our models, i.e. 50,000 entries. Source and target side use the same vocabulary. To avoid overfitting, we use variational dropout (Gal and Ghahramani, 2016) over GRU steps and input embeddings with probability 0.2. We optimize with Adam (K"
N18-1055,E17-3017,1,0.840934,"Missing"
N18-1055,W16-2309,0,0.0297254,"t) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enchant3 spell-checker to the JFLEG data before evaluation. No spellchecking is used for the CoNLL test sets. We follow the recommendation by Denkowski and Neubig (2017) to use byte-pair encoding (BPE) sub-word units (Sennrich et al., 2016b) to solve the 3 2.3 Model and training procedure Implementations of all models explored in this work4 are available in the Marian5 toolkit (JunczysDowmunt et al., 2018). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017b). The model differs from the model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention for which Sennrich et al. (2017b) provide a concise description. All embedding vectors consist of 512 units; the RNN states of 1024 units. The number of B"
N18-1055,P16-1162,0,0.0866913,"t) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enchant3 spell-checker to the JFLEG data before evaluation. No spellchecking is used for the CoNLL test sets. We follow the recommendation by Denkowski and Neubig (2017) to use byte-pair encoding (BPE) sub-word units (Sennrich et al., 2016b) to solve the 3 2.3 Model and training procedure Implementations of all models explored in this work4 are available in the Marian5 toolkit (JunczysDowmunt et al., 2018). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017b). The model differs from the model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention for which Sennrich et al. (2017b) provide a concise description. All embedding vectors consist of 512 units; the RNN states of 1024 units. The number of B"
N18-1055,P11-1019,0,0.723877,"Public 57.1K 1.9M 30.9K 1.9M 1.2M 25.0M 0.5M 29.2M Yes Yes Yes No Table 1: Statistics for existing GEC training data sets. Data sets marked with * are used in this work. starts low for small corpora, outperforms SMT at a corpus size of about 15 million words, and with increasing size beats SMT with a large in-domain language model. Table 1 lists existing training resources for the English as-a-second-language (ESL) grammatical error correction task. Publicly available resources, NUS Corpus of Learner English (NUCLE) by Dahlmeier et al. (2013), Lang-8 NAIST (Mizumoto et al., 2012) and CLC FCE (Yannakoudakis et al., 2011) amount to about 27M tokens. Among these the Lang-8 corpus is quite noisy and of low quality. The Cambridge Learner Corpus (CLC) by Nicholls (2003) — probably the best resource in this list — is non-public and we would strongly discourage reporting results that include it as training data as this makes comparisons difficult. Contrasting this with Fig. 1, we see that for about 20M tokens NMT systems start outperforming SMT models without additional large language models. Current state-of-the-art GEC systems based on SMT, however, all include large-scale indomain language models either following"
N18-1055,D17-1297,0,0.140577,"ed in transfer learning. In general, one first trains a neural model on high-resource data and then uses the resulting parameters to initialize parameters of a new model meant to be trained on lowresource data only. Various settings are possible, e.g. initializing from models trained on large outof-domain data and continuing on in-domain data (Miceli Barone et al., 2017) or using related language pairs (Zoph et al., 2016). Models can also be partially initialized by pre-training monolingual language models (Ramachandran et al., 2017) or only word-embeddings (Gangi and Federico, 2017). In GEC, Yannakoudakis et al. (2017) apply pretrained monolingual word-embeddings as initializations for error-detection models to re-rank SMT n-best lists. Approaches based on pre-training with monolingual data appear to be particularly wellsuited to the GEC task. Junczys-Dowmunt and Grundkiewicz (2016) published 300GB of compressed monolingual data used in their work to create a large domain-adapted Common-Crawl ngram language model.8 We use the first 100M lines. Preprocessing follows section 2.2 including BPE segmentation. 4.1 Pre-training embeddings Similarly to Gangi and Federico (2017) or Yannakoudakis et al. (2017), we us"
N18-1055,D16-1163,0,0.025615,"Missing"
N18-1055,W16-2323,0,\N,Missing
N18-2046,P04-3031,0,0.12699,"2012). We also report results on JFLEG (Napoles et al., 2017) with the 284 Proceedings of NAACL-HLT 2018, pages 284–290 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Corpus Sentences Tokens NUCLE Lang-8 NAIST CoNLL-2013 (dev) CoNLL-2014 (test) JFLEG Dev JFLEG Test 57,151 1,943,901 1,381 1,312 754 747 1,162K 25,026K 29K 30K 14K 13K System SMT Dense + Sparse + Char. ops GLEU metric (Napoles et al., 2015). The data set is provided with a development and test set split. All data sets are listed in Table 1. We preprocess Lang-8 with the NLTK tokenizer (Bird and Loper, 2004) and preserve the original tokenization in NUCLE and JFLEG. Sentences are truecased with scripts from Moses (Koehn et al., 2007). For dealing with out-of-vocabulary words, we split tokens into 50k subword units using Byte Pair Encoding (BPE) by Sennrich et al. (2016b). BPE codes are extracted only from correct sentences from Lang-8 and NUCLE. SMT systems For our SMT-based systems, we follow recipes proposed by Junczys-Dowmunt and Grundkiewicz (2016), and use a phrase-based SMT system with a log-linear combination of task-specific features. We use word-level Levenshtein distance and edit operat"
N18-2046,P15-1068,0,0.0425773,"Missing"
N18-2046,buck-etal-2014-n,0,0.125672,"Missing"
N18-2046,P17-1070,0,0.66566,"ce than any other GEC system reported so far. 1 NMT 2 55 50 45 R& JD R’1 & 6 Th G’ is 16 w or k Y Sc & h. B’ & 1 Ji&al.’ 6 Th al. 17 is ’17 w or k Y Ch &al .& .’1 N 7 Th g’ is 17 w or k 40 Figure 1: Comparison of SMT, NMT and hybrid GEC systems on the CoNLL-2014 test set (M2 ). Introduction Currently, the most effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017). Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set). However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017). In this work, we combine both MT flavors within a hybrid GEC system. Such a GEC system preserves the accuracy of SMT output and at the same time generates more fluent sentences achieving new state-of-the-art results on two different benchmarks: the annotationbased CoNLL-2014 and the"
N18-2046,W17-5037,0,0.643535,"nerates more fluent sentences as it typical for NMT. Our analysis shows that the created systems are closer to reaching human-level performance than any other GEC system reported so far. 1 NMT 2 55 50 45 R& JD R’1 & 6 Th G’ is 16 w or k Y Sc & h. B’ & 1 Ji&al.’ 6 Th al. 17 is ’17 w or k Y Ch &al .& .’1 N 7 Th g’ is 17 w or k 40 Figure 1: Comparison of SMT, NMT and hybrid GEC systems on the CoNLL-2014 test set (M2 ). Introduction Currently, the most effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017). Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set). However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017). In this work, we combine both MT flavors within a hybrid GEC system. Such a GEC system preserves the accuracy of SMT output and at the same time generat"
N18-2046,D16-1161,1,0.938487,"es et al., 2015). The data set is provided with a development and test set split. All data sets are listed in Table 1. We preprocess Lang-8 with the NLTK tokenizer (Bird and Loper, 2004) and preserve the original tokenization in NUCLE and JFLEG. Sentences are truecased with scripts from Moses (Koehn et al., 2007). For dealing with out-of-vocabulary words, we split tokens into 50k subword units using Byte Pair Encoding (BPE) by Sennrich et al. (2016b). BPE codes are extracted only from correct sentences from Lang-8 and NUCLE. SMT systems For our SMT-based systems, we follow recipes proposed by Junczys-Dowmunt and Grundkiewicz (2016), and use a phrase-based SMT system with a log-linear combination of task-specific features. We use word-level Levenshtein distance and edit operation counts as dense features (Dense), and correction patterns on words with one word left/right context on Word Classes (WC) as sparse features (Sparse). We also experiment with additional character-level dense features (Char. ops). All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014). Expe"
N18-2046,N12-1067,0,0.44782,"(§ 3) and NMT (§ 4) baseline systems. Then, we experiment with system combinations through pipelining and reranking (§ 5). Finally, we compare the performance with human annotations and identify issues with current state-of-the-art systems (§ 6). 2 Data and preprocessing Our main training data is NUCLE (Dahlmeier et al., 2013). English sentences from the publicly available Lang-8 Corpora (Mizumoto et al., 2012) serve as additional training data. We use official test sets from two CoNLL shared tasks from 2013 and 2014 (Ng et al., 2013, 2014) as development and test data, and evaluate using M2 (Dahlmeier and Ng, 2012). We also report results on JFLEG (Napoles et al., 2017) with the 284 Proceedings of NAACL-HLT 2018, pages 284–290 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Corpus Sentences Tokens NUCLE Lang-8 NAIST CoNLL-2013 (dev) CoNLL-2014 (test) JFLEG Dev JFLEG Test 57,151 1,943,901 1,381 1,312 754 747 1,162K 25,026K 29K 30K 14K 13K System SMT Dense + Sparse + Char. ops GLEU metric (Napoles et al., 2015). The data set is provided with a development and test set split. All data sets are listed in Table 1. We preprocess Lang-8 with the NLTK tokenizer (Bird a"
N18-2046,W13-1703,0,0.382881,"y-based JFLEG benchmark. Moreover, comparison with human gold standards shows that the created systems are closer to reaching human-level performance than any other GEC system described in the literature so far. Using consistent training data and preprocessing (§ 2), we first create strong SMT (§ 3) and NMT (§ 4) baseline systems. Then, we experiment with system combinations through pipelining and reranking (§ 5). Finally, we compare the performance with human annotations and identify issues with current state-of-the-art systems (§ 6). 2 Data and preprocessing Our main training data is NUCLE (Dahlmeier et al., 2013). English sentences from the publicly available Lang-8 Corpora (Mizumoto et al., 2012) serve as additional training data. We use official test sets from two CoNLL shared tasks from 2013 and 2014 (Ng et al., 2013, 2014) as development and test data, and evaluate using M2 (Dahlmeier and Ng, 2012). We also report results on JFLEG (Napoles et al., 2017) with the 284 Proceedings of NAACL-HLT 2018, pages 284–290 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Corpus Sentences Tokens NUCLE Lang-8 NAIST CoNLL-2013 (dev) CoNLL-2014 (test) JFLEG Dev JFLEG Test"
N18-2046,P11-1105,0,0.0349701,"racted only from correct sentences from Lang-8 and NUCLE. SMT systems For our SMT-based systems, we follow recipes proposed by Junczys-Dowmunt and Grundkiewicz (2016), and use a phrase-based SMT system with a log-linear combination of task-specific features. We use word-level Levenshtein distance and edit operation counts as dense features (Dense), and correction patterns on words with one word left/right context on Word Classes (WC) as sparse features (Sparse). We also experiment with additional character-level dense features (Char. ops). All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014). Experiment settings Translation models are trained with Moses (Koehn et al., 2007), wordalignment models are produced with MGIZA++ (Gao and Vogel, 2008), and no reordering models are used. Language models are built using KenLM (Heafield, 2011), while word classes are trained with word2vec1 . We tune the systems separately for M2 and GLEU metrics. MERT (Och, 2003) is used for tuning dense features and Batch Mira (Cherry and Foster, 2012) for sparse feature"
N18-2046,W08-0509,0,0.0575988,"nce and edit operation counts as dense features (Dense), and correction patterns on words with one word left/right context on Word Classes (WC) as sparse features (Sparse). We also experiment with additional character-level dense features (Char. ops). All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014). Experiment settings Translation models are trained with Moses (Koehn et al., 2007), wordalignment models are produced with MGIZA++ (Gao and Vogel, 2008), and no reordering models are used. Language models are built using KenLM (Heafield, 2011), while word classes are trained with word2vec1 . We tune the systems separately for M2 and GLEU metrics. MERT (Och, 2003) is used for tuning dense features and Batch Mira (Cherry and Foster, 2012) for sparse features. For M2 tunning 1 https://github.com/dav/word2vec CoNLL R M2 JFLEG GLEU 56.91 30.25 48.38 54.68 60.28 29.40 49.82 55.25 60.27 30.21 50.27 55.79 Table 2: Results for SMT baseline systems on the CoNLL-2014 (M2 ) and JFLEG Test (GLEU) sets. Table 1: Statistics for training and testing data set"
N18-2046,W11-2123,0,0.0420493,"ne word left/right context on Word Classes (WC) as sparse features (Sparse). We also experiment with additional character-level dense features (Char. ops). All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014). Experiment settings Translation models are trained with Moses (Koehn et al., 2007), wordalignment models are produced with MGIZA++ (Gao and Vogel, 2008), and no reordering models are used. Language models are built using KenLM (Heafield, 2011), while word classes are trained with word2vec1 . We tune the systems separately for M2 and GLEU metrics. MERT (Och, 2003) is used for tuning dense features and Batch Mira (Cherry and Foster, 2012) for sparse features. For M2 tunning 1 https://github.com/dav/word2vec CoNLL R M2 JFLEG GLEU 56.91 30.25 48.38 54.68 60.28 29.40 49.82 55.25 60.27 30.21 50.27 55.79 Table 2: Results for SMT baseline systems on the CoNLL-2014 (M2 ) and JFLEG Test (GLEU) sets. Table 1: Statistics for training and testing data sets. 3 P we follow the 4-fold cross-validation on NUCLE with adapted error rate recommended b"
N18-2046,P18-4020,1,0.878813,"Missing"
N18-2046,C12-2084,0,0.240194,"Missing"
N18-2046,P15-2097,0,0.50309,"Missing"
N18-2046,E17-2037,0,0.691121,"st effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017). Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set). However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017). In this work, we combine both MT flavors within a hybrid GEC system. Such a GEC system preserves the accuracy of SMT output and at the same time generates more fluent sentences achieving new state-of-the-art results on two different benchmarks: the annotationbased CoNLL-2014 and the fluency-based JFLEG benchmark. Moreover, comparison with human gold standards shows that the created systems are closer to reaching human-level performance than any other GEC system described in the literature so far. Using consistent training data and preprocessing (§ 2), we first create strong SMT (§ 3) and NMT"
N18-2046,W14-1701,0,0.633477,"Missing"
N18-2046,W13-3601,0,0.215185,"onsistent training data and preprocessing (§ 2), we first create strong SMT (§ 3) and NMT (§ 4) baseline systems. Then, we experiment with system combinations through pipelining and reranking (§ 5). Finally, we compare the performance with human annotations and identify issues with current state-of-the-art systems (§ 6). 2 Data and preprocessing Our main training data is NUCLE (Dahlmeier et al., 2013). English sentences from the publicly available Lang-8 Corpora (Mizumoto et al., 2012) serve as additional training data. We use official test sets from two CoNLL shared tasks from 2013 and 2014 (Ng et al., 2013, 2014) as development and test data, and evaluate using M2 (Dahlmeier and Ng, 2012). We also report results on JFLEG (Napoles et al., 2017) with the 284 Proceedings of NAACL-HLT 2018, pages 284–290 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Corpus Sentences Tokens NUCLE Lang-8 NAIST CoNLL-2013 (dev) CoNLL-2014 (test) JFLEG Dev JFLEG Test 57,151 1,943,901 1,381 1,312 754 747 1,162K 25,026K 29K 30K 14K 13K System SMT Dense + Sparse + Char. ops GLEU metric (Napoles et al., 2015). The data set is provided with a development and test set split. All d"
N18-2046,P03-1021,0,0.0152748,"dense features (Char. ops). All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014). Experiment settings Translation models are trained with Moses (Koehn et al., 2007), wordalignment models are produced with MGIZA++ (Gao and Vogel, 2008), and no reordering models are used. Language models are built using KenLM (Heafield, 2011), while word classes are trained with word2vec1 . We tune the systems separately for M2 and GLEU metrics. MERT (Och, 2003) is used for tuning dense features and Batch Mira (Cherry and Foster, 2012) for sparse features. For M2 tunning 1 https://github.com/dav/word2vec CoNLL R M2 JFLEG GLEU 56.91 30.25 48.38 54.68 60.28 29.40 49.82 55.25 60.27 30.21 50.27 55.79 Table 2: Results for SMT baseline systems on the CoNLL-2014 (M2 ) and JFLEG Test (GLEU) sets. Table 1: Statistics for training and testing data sets. 3 P we follow the 4-fold cross-validation on NUCLE with adapted error rate recommended by JunczysDowmunt and Grundkiewicz (2016). Models evaluated on GLEU are optimized on JFLEG Dev using the GLEU scorer, which"
N18-2046,P16-1208,0,0.175374,"tem preserves the accuracy of SMT output and, at the same time, generates more fluent sentences as it typical for NMT. Our analysis shows that the created systems are closer to reaching human-level performance than any other GEC system reported so far. 1 NMT 2 55 50 45 R& JD R’1 & 6 Th G’ is 16 w or k Y Sc & h. B’ & 1 Ji&al.’ 6 Th al. 17 is ’17 w or k Y Ch &al .& .’1 N 7 Th g’ is 17 w or k 40 Figure 1: Comparison of SMT, NMT and hybrid GEC systems on the CoNLL-2014 test set (M2 ). Introduction Currently, the most effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017). Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set). However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017). In this work, we combine both MT flavors within a hybrid GEC system. Such a GEC system"
N18-2046,I17-2062,0,0.317264,"Missing"
N18-2046,D17-1298,0,0.552183,"g human-level performance than any other GEC system reported so far. 1 NMT 2 55 50 45 R& JD R’1 & 6 Th G’ is 16 w or k Y Sc & h. B’ & 1 Ji&al.’ 6 Th al. 17 is ’17 w or k Y Ch &al .& .’1 N 7 Th g’ is 17 w or k 40 Figure 1: Comparison of SMT, NMT and hybrid GEC systems on the CoNLL-2014 test set (M2 ). Introduction Currently, the most effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017). Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set). However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017). In this work, we combine both MT flavors within a hybrid GEC system. Such a GEC system preserves the accuracy of SMT output and at the same time generates more fluent sentences achieving new state-of-the-art results on two different benchmarks: the annotationbased C"
N18-2046,E17-3017,1,0.833676,"st sets. A result of 55.79 GLEU on JFLEG Test is already 2 points better than the GLEU-tuned NMT system of Sakaguchi et al. (2017) and only 1 point worse than the best reported result by Chollampatt and Ng (2017) with their M2 -tuned SMT system, even though no additional spelling correction has been used at this point. We experiment with specialized spell-checking methods in later sections. 4 NMT systems The model architecture we choose for our NMTbased systems is an attentional encoder-decoder model with a bidirectional single-layer encoder and decoder, both using GRUs as their RNN variants (Sennrich et al., 2017). A similar architecture has been already tested for the GEC task by Sakaguchi et al. (2017), but we use different hyperparameters. To improve the performance of our NMT models, similarly to Xie et al. (2016) and Ji et al. (2017), we combine them with an additional large-scale language model. In contrast to previous studies, which use an n-gram probabilistic LM, we build a 2-layer Recurrent Neural Network Language Model (RNN 285 50.08 56.04 50.30 56.74 56.7 55.8 Table 3: Results for NMT systems on the CoNLL-2014 (M2 ) and JFLEG Test (GLEU) sets. 56 45 Results A single NMT model achieves lower"
N18-2046,W16-2323,0,0.198125,"(dev) CoNLL-2014 (test) JFLEG Dev JFLEG Test 57,151 1,943,901 1,381 1,312 754 747 1,162K 25,026K 29K 30K 14K 13K System SMT Dense + Sparse + Char. ops GLEU metric (Napoles et al., 2015). The data set is provided with a development and test set split. All data sets are listed in Table 1. We preprocess Lang-8 with the NLTK tokenizer (Bird and Loper, 2004) and preserve the original tokenization in NUCLE and JFLEG. Sentences are truecased with scripts from Moses (Koehn et al., 2007). For dealing with out-of-vocabulary words, we split tokens into 50k subword units using Byte Pair Encoding (BPE) by Sennrich et al. (2016b). BPE codes are extracted only from correct sentences from Lang-8 and NUCLE. SMT systems For our SMT-based systems, we follow recipes proposed by Junczys-Dowmunt and Grundkiewicz (2016), and use a phrase-based SMT system with a log-linear combination of task-specific features. We use word-level Levenshtein distance and edit operation counts as dense features (Dense), and correction patterns on words with one word left/right context on Word Classes (WC) as sparse features (Sparse). We also experiment with additional character-level dense features (Char. ops). All systems use a 5-gram Language"
N18-2046,P16-1162,0,0.493367,"(dev) CoNLL-2014 (test) JFLEG Dev JFLEG Test 57,151 1,943,901 1,381 1,312 754 747 1,162K 25,026K 29K 30K 14K 13K System SMT Dense + Sparse + Char. ops GLEU metric (Napoles et al., 2015). The data set is provided with a development and test set split. All data sets are listed in Table 1. We preprocess Lang-8 with the NLTK tokenizer (Bird and Loper, 2004) and preserve the original tokenization in NUCLE and JFLEG. Sentences are truecased with scripts from Moses (Koehn et al., 2007). For dealing with out-of-vocabulary words, we split tokens into 50k subword units using Byte Pair Encoding (BPE) by Sennrich et al. (2016b). BPE codes are extracted only from correct sentences from Lang-8 and NUCLE. SMT systems For our SMT-based systems, we follow recipes proposed by Junczys-Dowmunt and Grundkiewicz (2016), and use a phrase-based SMT system with a log-linear combination of task-specific features. We use word-level Levenshtein distance and edit operation counts as dense features (Dense), and correction patterns on words with one word left/right context on Word Classes (WC) as sparse features (Sparse). We also experiment with additional character-level dense features (Char. ops). All systems use a 5-gram Language"
N18-2046,D17-1297,0,0.329037,"he NMT ensemble with or without RNN LM5 . In this case the NMT system serves as an automatic post-editing system. Pipelining improves the results on both test sets by increasing recall (Table 4). As the performance of the NMT system without a RNN LM is much lower than the performance of the SMT system alone, this implies that both approaches produce complementary corrections. Rescoring with NMT Rescoring of an n-best list obtained from one system by another is a commonly used technique in GEC, which allows to combine multiple different systems or even different approaches (Hoang et al., 2016; Yannakoudakis et al., 2017; Chollampatt and Ng, 2017; Ji et al., 2017). In our experiments, we generate a 1000 n-best list with the SMT system and add separate scores from each neural component. Scores of NMT models and the RNN LM are added in the form of probabilities in negative log space. The re-scored weights are obtained from a single run of the Batch Mira algorithm (Cherry and Foster, 2012) on the development set. As opposed to pipelining, rescoring improves precision at the expense of recall and is more effective for the CoNLL data resulting in up to 54.95 M2 . On JFLEG, rescoring only with the RNN LM 4 The best"
N18-2046,N16-1042,0,0.100761,"the created systems are closer to reaching human-level performance than any other GEC system reported so far. 1 NMT 2 55 50 45 R& JD R’1 & 6 Th G’ is 16 w or k Y Sc & h. B’ & 1 Ji&al.’ 6 Th al. 17 is ’17 w or k Y Ch &al .& .’1 N 7 Th g’ is 17 w or k 40 Figure 1: Comparison of SMT, NMT and hybrid GEC systems on the CoNLL-2014 test set (M2 ). Introduction Currently, the most effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017). Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set). However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017). In this work, we combine both MT flavors within a hybrid GEC system. Such a GEC system preserves the accuracy of SMT output and at the same time generates more fluent sentences achieving new state-of-the-art results on two di"
N18-2046,W02-0109,0,\N,Missing
N18-2046,N12-1047,0,\N,Missing
P16-1161,D07-1007,0,0.312333,"econd issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only help with semantics but also to improve morphological and syntactic coherence. For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and Wu, 2007), (Gimpel and Smith, 2008) inter alia. Consider the first set of examples in Figure 1, produced by a strong baseline PBMT system. The English word “shooting” has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word “film” is close, the phrase-based model is able to use it in one phrase with the ambiguous “shooting”, disambiguating correctly the translation. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source context information is re"
P16-1161,2012.eamt-1.60,0,0.0164676,"test set and we test on the WMT14 set. We use TreeTagger (Schmid, 1994) to lemmatize and tag the German data. English-Polish has not been included in WMT shared tasks so far, but was present as a language pair for several IWSLT editions which concentrate on TED talk translation. Full test sets are only available for 2010, 2011, and 2012. The references for 2013 and 2014 were not made public. We use the development set and test set from 2010 as development data for parameter tuning. The remaining two test sets (2011, 2012) are our test data. We train on the concatenation of Europarl and WIT3 (Cettolo et al., 2012), ca. 750 thousand sentence pairs. The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi´orkowski, 2009). English-Romanian was added in WMT16. We train our system using the available parallel data – Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official development set and we test on the WMT16 test set. We use the online tagger by Tufis et al. (2008) to preprocess the data. Table 3 shows the obtained results. Similarly to English-C"
P16-1161,P11-2031,0,0.0533509,"– our classifier with source-context features only, • +target – our classifier with both sourcecontext and target-context features. For each of these settings, we vary the size of the training data for our classifier, the phrase table and the LM. We experiment with three different sizes: small (200 thousand sentence pairs), medium (5 million sentence pairs), and full (the whole CzEng corpus, over 14.8 million sentence pairs). For each setting, we run system weight optimization (tuning) using minimum error rate training (Och, 2003) five times and report the average BLEU score. We use MultEval (Clark et al., 2011) to compare the systems and to determine whether the differences in results are statistically significant. We always compare the baseline with +source and +source with +target. Table 2 shows the obtained results. Statistically significant differences (α=0.01) are marked in bold. The source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging result. Target-side context information allows our model to push the translation quality further: even for the small data setting, i"
P16-1161,P14-1129,0,0.036134,"We look at phrase counts and cooccurrence counts in the training data, we subtract one from the number of occurrences for the current source phrase, target phrase and the phrase pair. If the count goes to zero, we skip the training example. Without this technique, the classifier might learn to simply trust very long phrase pairs which were extracted from the same training sentence. For target-side context features, we simply use the true (gold) target context. This leads to training which is similar to language model estimation; this model is somewhat similar to the neural joint model for MT (Devlin et al., 2014), but in our case implemented using a linear (maximumentropy-like) model. 2.4 Training We use Vowpal Wabbit in the --csoaa ldf mc setting which reduces our multi-class problem to one-against-all binary classification. We use the logistic loss as our objective. We experimented with various settings of L2 regularization but were not able to get an improvement over not using regularization at all. We train each model with 10 iterations over the data. We evaluate all of our models on a held-out set. We use the same dataset as for MT system tuning because it closely matches the domain of our test s"
P16-1161,W08-0302,0,0.0657345,"th morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only help with semantics but also to improve morphological and syntactic coherence. For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and Wu, 2007), (Gimpel and Smith, 2008) inter alia. Consider the first set of examples in Figure 1, produced by a strong baseline PBMT system. The English word “shooting” has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word “film” is close, the phrase-based model is able to use it in one phrase with the ambiguous “shooting”, disambiguating correctly the translation. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source context information is required for correct disambi"
P16-1161,2010.amta-papers.33,0,0.107227,"nt phrases. They used a strong feature set originally developed for word sense disambiguation. Gimpel and Smith (2008) also used wider source-context information but did not train a classifier; instead, the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the “discriminative word lexicon” and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and assumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphological agreement required by individual rules (such as explicit rules enforcing subject-ve"
P16-1161,D07-1091,0,0.151245,"perimented with various settings of L2 regularization but were not able to get an improvement over not using regularization at all. We train each model with 10 iterations over the data. We evaluate all of our models on a held-out set. We use the same dataset as for MT system tuning because it closely matches the domain of our test set. We evaluate model accuracy after each pass over the training data to detect over-fitting and we select the model with the highest held-out accuracy. 2.5 Feature Set Our feature set requires some linguistic processing of the data. We use the factored MT setting (Koehn and Hoang, 2007) and we represent each type of information as an individual factor. On the source side, we use the word surface form, its lemma, morphological tag, analytical function (such as Subj for subjects) and the lemma of the parent node in the dependency parse tree. On the target side, we only use word lemmas and morphological tags. Table 1 lists our feature sets for each language pair. We implemented indicator features for both the source and target side; these are simply concatenations of the words in the current phrase into a single feature. Internal features describe words within the current phras"
P16-1161,2005.mtsummit-papers.11,0,0.0967476,"ur model. A baseline which always chooses the most frequent phrasal translation obtains accuracy of 51.5. For the sourcecontext model, the held-out accuracy was 66.3, while the target context model achieved accuracy of 74.8. Note that this high difference is somewhat misleading because in this setting, the targetcontext model has access to the true target context (i.e., it is cheating). 4.2 Additional Language Pairs We experiment with translation from English into German, Polish, and Romanian. Our English-German system is trained on the data available for the WMT14 translation task: Europarl (Koehn, 2005) and the Common Crawl corpus,3 roughly 4.3 million sentence pairs altogether. We tune the system on the WMT13 test set and we test on the WMT14 set. We use TreeTagger (Schmid, 1994) to lemmatize and tag the German data. English-Polish has not been included in WMT shared tasks so far, but was present as a language pair for several IWSLT editions which concentrate on TED talk translation. Full test sets are only available for 2010, 2011, and 2012. The references for 2013 and 2014 were not made public. We use the development set and test set from 2010 as development data for parameter tuning. The"
P16-1161,D09-1022,0,0.0271419,"have been proposed before. Carpuat and Wu (2007) trained a maximum entropy classifier for each source phrase type which used source context information to disambiguate its translations. The models did not capture target-side information and they were independent; no parameters were shared between classifiers for different phrases. They used a strong feature set originally developed for word sense disambiguation. Gimpel and Smith (2008) also used wider source-context information but did not train a classifier; instead, the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the “discriminative word lexicon” and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the predi"
P16-1161,W11-2124,0,0.0242953,"mplemented indicator features for both the source and target side; these are simply concatenations of the words in the current phrase into a single feature. Internal features describe words within the current phrase. Context features are extracted either from a window of a fixed size around the current phrase (on the source side) or from a limited left-hand side context (on the target side). Bilingual context features are concatenations of target-side context words and their sourceside counterparts (according to word alignment); these features are similar to bilingual tokens in bilingual LMs (Niehues et al., 2011). Each of our feature types can be configured to look at any individual factors or their combinations. The features in Table 1 are divided into three sets. The first set contains label-independent (=shared) features which only depend on the source sentence. The second set contains shared features which depend on target-side context; these can only be used when VW is applied during decoding. We use target context size two in all our experiments.2 Finally, the third set contains label-dependent features which describe the currently predicted phrasal translation. 2 In preliminary experiments we f"
P16-1161,P03-1021,0,0.038966,"English to Czech translation. To verify that our method is 1709 • +source – our classifier with source-context features only, • +target – our classifier with both sourcecontext and target-context features. For each of these settings, we vary the size of the training data for our classifier, the phrase table and the LM. We experiment with three different sizes: small (200 thousand sentence pairs), medium (5 million sentence pairs), and full (the whole CzEng corpus, over 14.8 million sentence pairs). For each setting, we run system weight optimization (tuning) using minimum error rate training (Och, 2003) five times and report the average BLEU score. We use MultEval (Clark et al., 2011) to compare the systems and to determine whether the differences in results are statistically significant. We always compare the baseline with +source and +source with +target. Table 2 shows the obtained results. Statistically significant differences (α=0.01) are marked in bold. The source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging result. Target-side context information allows ou"
P16-1161,P14-5003,0,0.0928089,"Missing"
P16-1161,P11-1024,0,0.0179208,"the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the “discriminative word lexicon” and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and assumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphological agreement required by individual rules (such as explicit rules enforcing subject-verb agreement). Our algorithm only assumes that hypotheses are constructed left to right and provides a general way for including target context information in the classifier, regardless of the type"
P16-1161,tufis-etal-2008-racais,0,0.132569,"11, 2012) are our test data. We train on the concatenation of Europarl and WIT3 (Cettolo et al., 2012), ca. 750 thousand sentence pairs. The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi´orkowski, 2009). English-Romanian was added in WMT16. We train our system using the available parallel data – Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official development set and we test on the WMT16 test set. We use the online tagger by Tufis et al. (2008) to preprocess the data. Table 3 shows the obtained results. Similarly to English-Czech experiments, BLEU scores are av1710 3 http://commoncrawl.org/ input: baseline: +source: +target: the most intensive mining took place there from 1953 to 1962 . nejv´ıce intenzivn´ı tˇezˇ ba doˇslo tam z roku 1953 , aby 1962 . the most intensive miningnom there occurred there from 1953 , in order to 1962 . nejv´ıce intenzivn´ı tˇezˇ by m´ısto tam z roku 1953 do roku 1962 . the most intensive mininggen place there from year 1953 until year 1962 . nejv´ıce intenzivn´ı tˇezˇ ba prob´ıhala od roku 1953 do roku 1"
P16-1161,H05-1097,0,0.0554915,"iminative lexicon. The second issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only help with semantics but also to improve morphological and syntactic coherence. For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and Wu, 2007), (Gimpel and Smith, 2008) inter alia. Consider the first set of examples in Figure 1, produced by a strong baseline PBMT system. The English word “shooting” has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word “film” is close, the phrase-based model is able to use it in one phrase with the ambiguous “shooting”, disambiguating correctly the translation. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source c"
P16-1161,W14-3302,1,\N,Missing
P18-4020,P07-2045,1,0.0306875,"Missing"
P18-4020,E17-2025,0,0.0357645,"83.9 73.0 67.9 54.8 46.6 35.3 40 23.5 20 12.4 60 0 100 Deep RNN 80 60 40 20 7.8 42.5 33.437.1 28.2 22.8 17.2 13.1 0 100 Transformer 80 54.9 49.0 42.9 37.6 30.4 23.4 16.4 60 40 20 0 9.1 1 2 3 4 5 6 7 8 Number of GPUs Figure 1: Training speed in thousands of source tokens per second for shallow RNN, deep RNN and Transformer model. Dashed line projects linear scale-up based on single-GPU performance. ory to maximize speed and memory usage. This guarantees that a chosen memory budget will not be exceeded during training. All models use tied embeddings between source, target and output embeddings (Press and Wolf, 2017). Contrary to Sennrich et al. (2017a) or Vaswani et al. (2017), we do not average checkpoints, but maintain a continuously updated exponentially averaged model over the entire training run. Following Vaswani et al. (2017), the learning rate is set to 0.0003 and decayed as the inverse square root of the number of updates after 16,000 updates. When training the transformer model, a linearly growing learning rate is used during the first 16,000 iterations, starting with 0 until the base learning rate is reached. 118 W a¨ Si hl e e n ei n Ta en s be tatu - rfe h ss la im tz M e fe nu¨ s . tleg e E"
P18-4020,W17-4774,1,0.824066,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,E17-3017,1,0.852816,"Missing"
P18-4020,I17-1013,1,0.851844,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,P16-1162,1,0.624087,"work, we implemented many efficient meta- ple scripts at https://github.com/marian-nmt/ algorithms. These include multi-device (GPU or marian-examples. 117 test2017 UEdin WMT17 (single) +Ensemble of 4 +R2L Reranking 33.9 35.1 36.2 27.5 28.3 28.3 Deep RNN (single) +Ensemble of 4 +R2L Reranking 34.3 35.3 35.9 27.7 28.2 28.7 Transformer (single) +Ensemble of 4 +R2L Reranking 35.6 36.4 36.8 28.8 29.4 29.5 Source tokens per second ×103 test2016 Source tokens per second ×103 System • preprocessing of training data, tokenization, true-casing4 , vocabulary reduction to 36,000 joint BPE subword units (Sennrich et al., 2016) with a separate tool.5 • training of a shallow model for backtranslation on parallel WMT17 data; • translation of 10M German monolingual news sentences to English; concatenation of artificial training corpus with original data (times two) to produce new training data; • training of four left-to-right (L2R) deep models (either RNN-based or Transformer-based); • training of four additional deep models with right-to-left (R2L) orientation; 6 • ensemble-decoding with four L2R models resulting in an n-best list of 12 hypotheses per input sentence; • rescoring of n-best list with four R2L models, a"
P18-4020,N18-1055,1,0.879647,"Missing"
P18-4020,P17-4012,0,0.0756084,"lconquers-patent-translation-in-majorwipo-roll-out/ Marian has minimal dependencies (only Boost and CUDA or a BLAS library) and enables barrierfree optimization at all levels: meta-algorithms such as MPI-based multi-node training, efficient batched beam search, compact implementations of new models, custom operators, and custom GPU kernels. Intel has contributed and is optimizing a CPU backend. Marian grew out of a C++ re-implementation of Nematus (Sennrich et al., 2017b), and still maintains binary-compatibility for common models. Hence, we will compare speed mostly against Nematus. OpenNMT (Klein et al., 2017), perhaps one of the most popular toolkits, has been reported to have training speed competitive to Nematus. Marian is distributed under the MIT license and available from https://marian-nmt. github.io or the GitHub repository https: //github.com/marian-nmt/marian. 2 Design Outline We will very briefly discuss the design of Marian. Technical details of the implementations will be provided in later work. 2.1 Custom Auto-Differentiation Engine The deep-learning back-end included in Marian is based on reverse-mode auto-differentiation with dynamic computation graphs and among the established mach"
Q17-1015,W13-3520,0,0.0303314,"Missing"
Q17-1015,W08-0330,0,0.0181167,"tem and the TER-tuned APE ensemble are much weaker in terms of F1MULT . This is less surprising in the case of the full ensemble, as it has been tuned towards TER for the APE task specifically. However, we can obtain even better APEbased QE systems for both shared task settings by tuning the full APE ensembles towards F1MULT , the official WMT16 QE metric, and towards F1BAD for WMT15.12 With this approach, we produce our new best stand-alone QE-systems for both shared tasks, which we denote as A PE QE. 11 Note that this system resembles other QE approaches which use pseudo-reference features (Albrecht and Hwa, 2008; Soricut and Narsale, 2012; Shah et al., 2013), since the s → p is essentially an “alternative” MT system. 12 Using again MERT and executing 7 iterations on the official development set with an n-best list size of 12. F1BAD dev F1BAD test Best system in WMT15 43.1 43.12 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 43.68 43.51 44.68 46.44 47.61 42.50 43.35 43.70 46.05 47.08 F1MULT dev F1MULT test Best system in WMT16 49.25 49.52 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 46.11 46.80 49.16 54.95 56.80 46.16 47.29 50.27 55.68 57.47 Table 10: Performance of the sev"
Q17-1015,W12-3108,0,0.0237159,"potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the"
Q17-1015,W13-2241,0,0.0141301,"atically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the related task of aut"
Q17-1015,W15-3035,0,0.0424635,"Missing"
Q17-1015,W13-2242,0,0.0744382,"Missing"
Q17-1015,C04-1046,0,0.0567254,"Missing"
Q17-1015,W14-3340,0,0.0500303,"Missing"
Q17-1015,2015.mtsummit-users.5,0,0.0860722,"Missing"
Q17-1015,W15-3036,0,0.100031,"Missing"
Q17-1015,P15-1174,0,0.0310394,"word-level QE task); see Figure 1. Besides these datasets, for training the APE system we make use of artificial roundtrip translations; this will be detailed in §4. Evaluation. For all experiments, we report the official evaluation metrics of each dataset’s year. For WMT15, the official metric for the word-level QE task is the F1 score of the BAD labels (F1BAD ). For WMT16, it is the product of the F1 scores for the OK and BAD labels (denoted F1MULT ). For sentencelevel QE, we report the Pearson’s r correlation for HTER prediction and the Spearman’s ρ correlation score for sentence ranking (Graham, 2015). From post-edited sentences to quality labels. In the datasets above, the word quality labels are obtained automatically by aligning the translated and the post-edited sentence with the TERCOM software tool (Snover et al., 2006)2 , with the default settings (tokenized, case insensitive, exact matching only, shifts disabled). This tool computes the HTER (the normalized edit distance) between the translated and post-edited sentence. As a by-product, it aligns the words in the two sentences, identifying substitution errors, word deletions (i.e. words omitted by the translation system), and inser"
Q17-1015,W15-3014,0,0.0133895,"s predictions to avoid overfitting the training set. This is done by splitting the training set in K folds (we set K = 10) and training K different instances of the first system, where each instance is trained on K − 1 folds and makes predictions for the left-out fold. The concatenation of all the predictions yields an unbiased training set for the second classifier. Neural intra-ensembles. We also evaluate the performance of intra-ensembled neural systems. We train independent instances of N EURAL QE with different random initializations and different data shuffles, following the approach of Jean et al. (2015) in neural MT. In Tables 5–6, we report the performance on the WMT15 and WMT16 datasets of systems ensembling 5 and 15 of these instances, called respectively N EURAL QE-5 and N EURAL QE-15. The in210 F1BAD dev F1BAD test Best system in WMT15 QUETCH+ (2nd best) 43.1– 43.12 43.05 L INEAR QE N EURAL QE N EURAL QE-5 N EURAL QE-15 S TACKED QE 43.68 43.51 44.21 44.11 44.68 42.50 43.35 43.54 43.93 43.70 Table 5: Performance of the pure QE systems on the WMT15 datasets. The best performing system in the WMT15 competition was by Espl`a-Gomis et al. (2015), followed by Kreutzer et al. (2015)’s QUETCH+,"
Q17-1015,W16-2378,1,0.791496,"improvement in the development set, but a degradation in the test set. In WMT16, however, stacking is clearly beneficial, with a boost of about 2 points over the best intraensembled neural system and 3–4 points above the linear system, both in the development and test partitions. For the remainder of this paper, we will take S TACKED QE as our pure QE system. 4 APE-Based Quality Estimation Now that we have described a pure QE system, we move on to an APE-based QE system (A PE QE). Our starting point is the system submitted by the Adam Mickiewicz University (AMU) team to the APE task of WMT16 (Junczys-Dowmunt and Grundkiewicz, 2016). They explored the application of neural translation models to the APE problem and achieved good results by treating different models as components in a log-linear model, allowing for multiple inputs (the source s and the translated sentence t) that were decoded to the same target language (post-edited translation p). Two systems were considered, one using s as the input (s → p) and another using t as the input (t → p). A simple string-matching penalty integrated within the loglinear model was used to control for higher faithfulness with regard to the raw MT output. The penalty fires if the A"
Q17-1015,W16-2384,0,0.130055,"11). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the related task of automatic post-editing (APE; Simard et al. (2007)), which aims to au205 T"
Q17-1015,W16-2385,0,0.177565,"al., 2013). This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a"
Q17-1015,W15-3037,0,0.640642,"omes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the related task of automatic post-editing (APE; Simard et al. (2007)), which aims to au205 Transactions of the Association for Computational Linguistics, vol. 5, pp."
Q17-1015,W15-3038,0,0.0367556,"Missing"
Q17-1015,W14-3342,0,0.0457533,"sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system"
Q17-1015,D08-1017,1,0.671671,"1.20) Table 4: Effect of architectural changes in N EURAL QE on the WMT16 development set. without POS tags as input performs almost 2.5 points worse. Finally, varying the size of the hidden layers and the depth of the network hurts the final model’s performance, albeit more slightly. 3.3 Stacking Neural and Linear Models We now stack the N EURAL QE system (§3.2) into the L INEAR QE system (§3.1) as an ensemble strategy; we call the resulting system S TACKED QE. Stacking architectures (Wolpert, 1992; Breiman, 1996) have proved effective in structured NLP problems (Cohen and de Carvalho, 2005; Martins et al., 2008). The underlying idea is to combine two systems by letting the prediction of the first system be used as an input feature for the second system. During training, it is necessary to jackknife the first system’s predictions to avoid overfitting the training set. This is done by splitting the training set in K folds (we set K = 10) and training K different instances of the first system, where each instance is trained on K − 1 folds and makes predictions for the left-out fold. The concatenation of all the predictions yields an unbiased training set for the second classifier. Neural intra-ensembles"
Q17-1015,P13-2109,1,0.813822,"Missing"
Q17-1015,W16-2387,1,0.607823,"al., 2004; Specia et al., 2013). This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE syste"
Q17-1015,P03-1021,0,0.0756007,"heckpoints of each training run are averaged element-wise (JunczysDowmunt et al., 2016) resulting in new single models with generally improved performance. To verify the quality of the APE system, we ensemble the 8 resulting models (4 times s → p and 4 times t → p) and add the APE penalty described in Junczys-Dowmunt and Grundkiewicz (2016). This large ensemble across folds is only used during test time. For creating the jackknifed training data, only the models from the corresponding fold are used. Since we combine models of different types, we tune weights on the development set with MERT9 (Och, 2003) towards TER, yielding the model denoted as “APE TER-tuned”. Results are listed in Table 7 for the APE shared task (WMT 16). For the purely s → p and t → p ensembles, models are weighted equally. We achieve slightly better results in terms of TER, the main task metric, than the original system, using less data. For completeness, we also apply this procedure to WMT15 data, generating a similar resource of 500K artificial English-Spanish-Spanish postediting triplets via roundtrip translation.10 The training, jackknifing and ensembling methods are the same as for the WMT16 setting. For the WMT15"
Q17-1015,W12-3117,0,0.0273681,"an on false negatives (cFP ∈ {0.5, 0.55, . . . , 0.95}, cFN = 1 − cFP ), to account for the existence of fewer BAD labels than OK labels in the data. These values are tuned on the development set. Results and feature contribution. Table 3 shows the performance of the L INEAR QE system. To help understand the contribution of each group of features, we evaluated different variants of the L INEAR QE system on the development sets of WMT15/16. As expected, the use of bigrams improves the simple unigram model, and the syntac4 While syntactic features have been used previously in sentence-level QE (Rubino et al., 2012), they have never been applied to the finer-grained word-level variant tackled here. 5 http://www.cs.cmu.edu/˜ark/TurboParser. 208 Features WMT15 (F1BAD ) WMT16 (F1MULT ) 41.77 42.20 42.80 43.68 40.05 40.63 43.65 46.11 unigrams only +simple bigram +rich bigrams +syntactic (full) Table 3: Performance on the WMT15 (En-Es) and WMT16 (En-De) development sets of several configurations of L INEAR QE. We report the official metric for these shared tasks, F1BAD for WMT15 and F1MULT for WMT16. tic features help even further. The impact of these features is more prominent in WMT16: the rich bigram featu"
Q17-1015,2013.mtsummit-papers.21,0,0.432432,"ppealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture"
Q17-1015,W07-0728,0,0.17708,"Missing"
Q17-1015,2006.amta-papers.25,0,0.0932633,"evaluation metrics of each dataset’s year. For WMT15, the official metric for the word-level QE task is the F1 score of the BAD labels (F1BAD ). For WMT16, it is the product of the F1 scores for the OK and BAD labels (denoted F1MULT ). For sentencelevel QE, we report the Pearson’s r correlation for HTER prediction and the Spearman’s ρ correlation score for sentence ranking (Graham, 2015). From post-edited sentences to quality labels. In the datasets above, the word quality labels are obtained automatically by aligning the translated and the post-edited sentence with the TERCOM software tool (Snover et al., 2006)2 , with the default settings (tokenized, case insensitive, exact matching only, shifts disabled). This tool computes the HTER (the normalized edit distance) between the translated and post-edited sentence. As a by-product, it aligns the words in the two sentences, identifying substitution errors, word deletions (i.e. words omitted by the translation system), and insertions (redundant words in the translation). Words in the MT output that need to be edited are marked by the BAD quality labels. The fact that the quality labels are automatically obtained from the post-edited sentences is not jus"
Q17-1015,W12-3121,0,0.0213076,"E ensemble are much weaker in terms of F1MULT . This is less surprising in the case of the full ensemble, as it has been tuned towards TER for the APE task specifically. However, we can obtain even better APEbased QE systems for both shared task settings by tuning the full APE ensembles towards F1MULT , the official WMT16 QE metric, and towards F1BAD for WMT15.12 With this approach, we produce our new best stand-alone QE-systems for both shared tasks, which we denote as A PE QE. 11 Note that this system resembles other QE approaches which use pseudo-reference features (Albrecht and Hwa, 2008; Soricut and Narsale, 2012; Shah et al., 2013), since the s → p is essentially an “alternative” MT system. 12 Using again MERT and executing 7 iterations on the official development set with an n-best list size of 12. F1BAD dev F1BAD test Best system in WMT15 43.1 43.12 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 43.68 43.51 44.68 46.44 47.61 42.50 43.35 43.70 46.05 47.08 F1MULT dev F1MULT test Best system in WMT16 49.25 49.52 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 46.11 46.80 49.16 54.95 56.80 46.16 47.29 50.27 55.68 57.47 Table 10: Performance of the several word-level QE systems"
Q17-1015,P13-4014,0,0.0326539,"Missing"
Q17-1015,2011.eamt-1.12,0,0.059343,"Introduction The goal of quality estimation (QE) is to evaluate a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2013). This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim"
Q17-1015,P14-1067,0,0.140631,"Missing"
Q17-1015,J07-1003,0,0.168213,"changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of featur"
Q17-1015,W16-2301,0,\N,Missing
W14-1703,P06-1032,0,0.83101,"Missing"
W14-1703,buck-etal-2014-n,0,0.010178,"Missing"
W14-1703,2011.mtsummit-papers.1,0,0.193426,"Missing"
W14-1703,N12-1047,0,0.035236,"Missing"
W14-1703,P11-2031,0,0.0521429,"Missing"
W14-1703,D12-1052,0,0.112336,"timization needs to be performed according to relevant metrics. This paper constitutes the description of the Adam Mickiewicz University (AMU) submission to the CoNLL-2014 Shared Task on Grammatical Error Correction. We explore the interaction of large-scale data, parameter optimization, and taskspecific features in a Moses-based system. Related work is presented in the next section, the system setup is shortly described in Section 3. Sections 4 to 7 contain our main contributions. In Section 4, we describe our implementation of feature weights tuning according to the MaxMatch (M2 ) metric by Dahlmeier and Ng (2012b) which is the evaluation metric of the current CoNLL2014 Shared Task. Sections 5 and 6 deal with the data-intensive aspects of our paper. We start by extending the baseline system with a Wikipediabased language model and finish with a web-scale language model estimated from CommonCrawl data. Uncorrected/corrected data from the social language learner’s platform Lang-8 is used to extend the translation models of our system. Task-specific dense and sparse features are introduced in Section 7. These features are meant to raise the “awareness” of the decoder for grammatical error correction. In"
W14-1703,W13-1703,0,0.405331,"Missing"
W14-1703,I11-1017,0,0.12169,"Missing"
W14-1703,2012.iwslt-papers.17,0,0.0136481,"he source phrase, t is the target phrase. The exponential function is used because Moses relies on a log-linear model. In the log-linear model, the edit distances of all phrase pairs used to translate a sentence sum to the total number of edits that have been applied to produce the target sentence. Note that the Lang-8 data has not been processed for noise-reduction, this feature should take care of the problem and penalize sentences that have diverged to much from the source. Table 7 contains examples of phrase pairs 7.2 Sparse Features Sparse features are a relatively new addition to Moses (Hasler et al., 2012). Unlike dense features, they are optional and unrestricted in number, thousands of different sparse features may be used. A verbose version of the above mentioned LD feature is implemented as a sparse feature. Each edit operation is annotated with the operation type and the words that take part in the operation. The decoder can now learn to favor or penalize specific edits during tuning. As before in the case of error adaption patterns from Section 6.2, we generalize substitution operations if common substrings of a length equal to or greater than three characters appear in corresponding sour"
W14-1703,C12-2084,0,0.291377,"Missing"
W14-1703,P13-2121,0,0.040696,"Missing"
W14-1703,W13-3601,0,0.168567,"ata originates from the social learner’s platform Lang-8. We use similar resources. Very interesting work is presented by Dahlmeier and Ng (2012a). A custom beamsearch decoder for grammatical error correction is introduced that incorporates discriminative classifiers for specific error categories such as articles and prepositions. The authors perform parameter tuning and find PRO to work better with M21 than MERT1 . The specialized decoder tuned with M21 is compared to Moses that has been tuned with BLEU. As we show in Section 4.2, this cannot be a fair comparison. The CoNLL-2013 Shared Task (Ng et al., 2013) saw a number of systems based entirely or partially on translation approaches. Most notable are Yuan and Felice (2013) and Yoshimoto et al. (2013). Yuan and Felice (2013) apply Moses to all five error types of the shared task and extend the provided training data by adding other learner’s corpora. They also experiment with generating artificial errors. Improvement over the baseline are small, but their approach to generate errors shows promise. We successfully re-implement their baseline. Yoshimoto et al. (2013) use Moses for two error classes, prepositions and determiners, for other classes"
W14-1703,W11-2123,0,0.0123912,"Missing"
W14-1703,W14-1701,0,0.201225,"Missing"
W14-1703,D11-1125,0,0.0368368,"Missing"
W14-1703,P03-1021,0,0.0468536,"d with the full training data to translate the CoNLL-2013 test set and the blind CoNLL-2014 test set. As it turns out, averaging parameter vectors across all parts has a consistently positive effect for M2 . This is shown in Table 2, systems mentioned in the table are introduced in Section 5 and Section 7.2. Tuning Metric We refer to F0.5 computed by the M2 metric as M20.5 . Moses is bundled with several tuning tools that can tune parameter vectors according to different MT tuning metrics. The most widely used is BLEU (Papineni et al., 2002). We first attempt minimum error rate tuning (MERT) (Och, 2003) with BLEU, results are shown in Table 1. While BLEU scores increase on both, 4×2-CV and ST2013, the effect on M20.5 is catastrophic3 though not surprising. The baseline is so weak that it introduces more errors than corrections, thus lowering the similarity of the output and the reference below the level of the similarity of the input and the reference. MERT learns parameter weights that disable nearly all correction attempts. The obvious solution is to tune directly with 2 M . M2 provides per-sentence sufficient statistics and can easily4 be integrated with MERT. We retune with M2 and see an"
W14-1703,W12-3139,0,0.0417064,"dvanced systems, tuned parameters do generally better. 4.3 Average Table 2: Effects of parameter weight smoothing on three selected systems for 4×2-CV (CoNLL2014) Table 1: Tuning with BLEU and M2 4.2 Concat. 4.4 Tuning Sparse Feature Weights Tuning sparse features (Section 7.2) with M2 poses an unexpected challenge. Moses implements two methods for feature-rich tuning: PRO (Hopkins and May, 2011) and Batch k-best MIRA (kb-MIRA) (Cherry and Foster, 2012) that both function as drop-in replacements for MERT. MERT cannot be used directly with sparse features. When BLEU is used as a tuning metric, Koehn and Haddow (2012) report results for PRO on a par with MERT for a system with only dense features. Unfortunately, this cannot be confirmed for M2 ; we consistently see worse results than for MERT using PRO or kb-MIRA. PRO and kb-MIRA operate on sentence-level while MERT computes M2 for the complete corpus. Similar to Dahlmeier and Ng (2012a), we use sentence-level M2 as an approximation. We suspect that M2 might not be distinctive enough in a Parameter Smoothing Based on the results of Clark et al. (2011), it has become good practice to tune systems between three and five times and report average results in or"
W14-1703,P02-1040,0,0.104239,"training data with this parameter vector. The same parameters are later used with the full training data to translate the CoNLL-2013 test set and the blind CoNLL-2014 test set. As it turns out, averaging parameter vectors across all parts has a consistently positive effect for M2 . This is shown in Table 2, systems mentioned in the table are introduced in Section 5 and Section 7.2. Tuning Metric We refer to F0.5 computed by the M2 metric as M20.5 . Moses is bundled with several tuning tools that can tune parameter vectors according to different MT tuning metrics. The most widely used is BLEU (Papineni et al., 2002). We first attempt minimum error rate tuning (MERT) (Och, 2003) with BLEU, results are shown in Table 1. While BLEU scores increase on both, 4×2-CV and ST2013, the effect on M20.5 is catastrophic3 though not surprising. The baseline is so weak that it introduces more errors than corrections, thus lowering the similarity of the output and the reference below the level of the similarity of the input and the reference. MERT learns parameter weights that disable nearly all correction attempts. The obvious solution is to tune directly with 2 M . M2 provides per-sentence sufficient statistics and ca"
W14-1703,N03-1017,0,0.00786845,"l error correction. 7.1 Source (s) and their Levenshtein distance feature. We extend the currently best system NUCLE+CCLM+L8AT with the Levenshtein distance feature. Results are shown in Table 8 (+LD). For 4×2-CV small improvements can be observed, the effect is more significant for ST2013. It can be concluded that this very simple modification of the standard translation model is a beneficial extension of SMT for grammatical correction. Dense Features In Moses, translation models are described by a set of dense features: phrase translation probabilities, lexical scores, and a phrase penalty (Koehn et al., 2003). In the grammatical error correction scenario where source and target phrases are often identical or similar, it might be useful to inform the decoder about the differences in a phrase pair. We extend translation models with a wordbased Levenshtein distance feature (Levenshtein, 1966) that captures the number of edit operations required to turn the source phrase into the target phrase. Each phrase pair in the phrase table is scored with ed(s,t) where d is the word-based distance function, s is the source phrase, t is the target phrase. The exponential function is used because Moses relies on"
W14-1703,W13-3607,0,0.546766,"Missing"
W14-1703,P07-2045,0,\N,Missing
W14-1703,N12-1067,0,\N,Missing
W14-1703,W13-3604,0,\N,Missing
W15-4927,2013.mtsummit-user.7,1,0.474169,"Missing"
W15-4927,P96-1041,0,0.289931,"e context of larger SMT models and use this tool to compute 200 word classes from the target language data. The target language corpora are mapped to sequences of classes and 9-gram language model are estimated. The ﬁnal phrase-tables of the larger models (EnglishFrench, English-Spanish) have been signiﬁcance pruned (Johnson et al. 2007) for size reduction. In our experiments signiﬁcance pruning results in no quality loss while reducing translation model size by a factor of 5. The standard 5-gram language models and the 9-gram word-class models are estimated with Modiﬁed Kneser-Ney smoothing (Chen and Goodman 1996, Heaﬁeld et al. 2013). To reduce size requirements, we use heavily quantized binary models with no noticeable quality reduction. Pruning is applied to all singleton ngrams with n equal to or greater than 3. 3.2 Attempts at domain adaptation We explore two model combination methods for both, translation models and language models: linear and log-linear interpolation. Log-linear model interpolation is natively supported in Moses via its feature function framework. Translation models and language models can be log-linearly interpolated just by adding them to the Moses conﬁguration ﬁles. Paramete"
W15-4927,P13-2071,0,0.202326,"Missing"
W15-4927,P13-2121,0,0.0399403,"models and use this tool to compute 200 word classes from the target language data. The target language corpora are mapped to sequences of classes and 9-gram language model are estimated. The ﬁnal phrase-tables of the larger models (EnglishFrench, English-Spanish) have been signiﬁcance pruned (Johnson et al. 2007) for size reduction. In our experiments signiﬁcance pruning results in no quality loss while reducing translation model size by a factor of 5. The standard 5-gram language models and the 9-gram word-class models are estimated with Modiﬁed Kneser-Ney smoothing (Chen and Goodman 1996, Heaﬁeld et al. 2013). To reduce size requirements, we use heavily quantized binary models with no noticeable quality reduction. Pruning is applied to all singleton ngrams with n equal to or greater than 3. 3.2 Attempts at domain adaptation We explore two model combination methods for both, translation models and language models: linear and log-linear interpolation. Log-linear model interpolation is natively supported in Moses via its feature function framework. Translation models and language models can be log-linearly interpolated just by adding them to the Moses conﬁguration ﬁles. Parameter tuning then chooses"
W15-4927,D07-1103,0,0.126306,"(OSM) is added to the phrase-based decoder. Class-based language models seem to be a good compromise between increased n-gram length and total model size. We use automatically calculated word cluster ids as classes. We had good experience with word2vec (Mikolov et al. 2012) in the context of larger SMT models and use this tool to compute 200 word classes from the target language data. The target language corpora are mapped to sequences of classes and 9-gram language model are estimated. The ﬁnal phrase-tables of the larger models (EnglishFrench, English-Spanish) have been signiﬁcance pruned (Johnson et al. 2007) for size reduction. In our experiments signiﬁcance pruning results in no quality loss while reducing translation model size by a factor of 5. The standard 5-gram language models and the 9-gram word-class models are estimated with Modiﬁed Kneser-Ney smoothing (Chen and Goodman 1996, Heaﬁeld et al. 2013). To reduce size requirements, we use heavily quantized binary models with no noticeable quality reduction. Pruning is applied to all singleton ngrams with n equal to or greater than 3. 3.2 Attempts at domain adaptation We explore two model combination methods for both, translation models and la"
W15-4927,J10-4005,0,0.0445388,"Missing"
W15-4927,ma-2006-champollion,0,0.0835602,"ocuments translated between January 2000 and October 2014 (ca. 20,000 documents for English, French and Spanish, about 400 documents for Russian/Arabic, see Table 1). The provided corpora have been extracted from original Word or PDF documents, identical IDs between languages allow to align documents for each language pair. We use an in-house (WIPO) sentence aligner. The tool processes each parallel text document and produces a set of aligned sentences after applying the following steps: • Sentence splitting • Tokenization • Sentence alignment with our sentences aligned (based on Champollion (Ma 2006)) — produces an “aligned-segment-matchingscore” • ﬁltering out whole documents with an average-segment-matching-score below a given threshold • ﬁltering out sets of consecutive segments having a low scores • ﬁltering out sets of consecutive segments that are sorted by alphabetical order3 • ﬁltering out sentences having only one word or more than 80 words, or a source/target word ratio more than 9 3 3.1 SMT system Baseline system The baseline SMT system consists of an extended Moses (Koehn et al. 2007) conﬁguration. Durrani et al. (2013) report on improvements for various language pairs when an"
W15-4927,P02-1040,0,0.0934803,"Missing"
W15-4927,E12-1055,0,0.0242787,"e Moses conﬁguration ﬁles. Parameter tuning then chooses the appropriate interpolation weights which are actually feature weights. Linear interpolation, though a standard method for language models, is more involved. In the case of language models, we 203 compute a new static linearly interpolated language model from IMO and UN data target language data. Interpolation weights are optimized on the dev set. In the case of translation models we use a new feature function available in Moses that allows for setting up virtual phrase tables that are in fact linearly interpolated translation models (Sennrich 2012). We use the same interpolation weights as previously determined for linear language model interpolation. The two interpolated translation models are the original IMO and UN translation models as used in stand-alone translators. Results are mixed, we report the best results for our experiments (see Table 2, Section 5.1). Log-linear interpolation is downright harmful (and therefore omitted), for the larger language pairs (en-fr and en-es) any of the interpolation methods seem to be unhelpful, improvements for en-es are within the range of optimizer instability. For the smaller models (en-ar, en"
W15-4927,P07-2045,0,\N,Missing
W16-2316,P13-2071,0,0.0139652,"Adding multiple models as separate features becomes thus similar to ensemble translation with pure neural models. In this section we give algorithmic details about integrating GPU-based soft-attention neural translation models into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects: Phrase-Based baseline systems We base our set-up on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013). We perform no language-specific adaptations or modifications. The two systems differ only 1. While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models. 3 In experiments not described in this paper, we tried BPE encoding for the English-German language pair and found subword units to cope well with German compound nouns when used for phrase-based SMT. 4 This artificial data has not been used for the creation of the phrase-based system, but it might be worthwhile to explore this possib"
W16-2316,E14-4029,0,0.0220874,"th phrase-based decoding. Experiments have been conducted for the English-Russian language pair in both translation directions. For these experiments we re-implemented the inference step of the models described in Bahdanau et al. (2015) (more exactly the DL4MT1 variant also present in Nematus2 ) in efficient 1 https://github.com/nyu-dl/ dl4mt-tutorial 2 https://github.com/rsennrich/nematus 319 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 319–325, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mechanisms (Durrani et al., 2014). Resorting to subword units eliminates the need for these.3 3 with respect to translation direction and the available (monolingual) training data. For domainadaptation, we rely solely on parameter tuning with Batch-Mira (Cherry and Foster, 2012) and on-line log-linear interpolation. Binary domainindicators for each separate parallel corpus are introduced to the phrase-tables (four indicators) and a separate language model per parallel and monolingual resource is trained (en:16 and ru:12). All language models are 5-gram models with Modified Kneser-Ney smoothing and without pruning thresholds ("
W16-2316,P13-2121,0,0.0373563,"Missing"
W16-2316,P07-2045,0,0.00567367,"rectly with our code. One or multiple models can be added to the Moses log-linear model as different instances of the same feature, which during tuning can be separately weighted. Adding multiple models as separate features becomes thus similar to ensemble translation with pure neural models. In this section we give algorithmic details about integrating GPU-based soft-attention neural translation models into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects: Phrase-Based baseline systems We base our set-up on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013). We perform no language-specific adaptations or modifications. The two systems differ only 1. While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models. 3 In experiments not described in this paper, we tried BPE encoding for the English-German language pair and found subword units to cope well with German c"
W16-2316,J10-4005,0,0.0110406,"Moses to a minimum, we propose two-pass stack decoding where the first pass is a hypothesis and expansions collection step and the second pass is the original expansion and scoring step. Between the two steps we pre-calculate perhypothesis scores with the procedure described above. The data structure introduced in Figure 1 is then reused for probability look-up during the scoring phrase of stack decoding as if individual hypotheses where scored on-the-fly. Figure 3 contains our complete proposal for two-pass stack decoding, a modification of the original stack decoding algorithm described in Koehn (2010). We dissect stack decoding into smaller reusable pieces that can be passed functors to perform different tasks for the same sets of hypotheses. The main reason for this is the small word “applicable” in line 12, which hides a complicated set of target phrase choices based on reordering limits and coverage vectors which should 5.3 Stack rescoring The previous approach cannot be used with lazy decoding algorithms — like cube pruning — 5 Large matrix sizes, however, do slow-down translation speed significantly. 322 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: procedure S TACK R ESCORING Place"
W16-2316,W15-3034,0,0.135613,"ed a C++/CUDA version of the inference step for the neural models trained with DL4MT or Nematus, which can be used directly with our code. One or multiple models can be added to the Moses log-linear model as different instances of the same feature, which during tuning can be separately weighted. Adding multiple models as separate features becomes thus similar to ensemble translation with pure neural models. In this section we give algorithmic details about integrating GPU-based soft-attention neural translation models into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects: Phrase-Based baseline systems We base our set-up on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013). We perform no language-specific adaptations or modifications. The two systems differ only 1. While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models. 3 In experiments not described in this pa"
W16-2316,N12-1047,0,0.347327,"exactly the DL4MT1 variant also present in Nematus2 ) in efficient 1 https://github.com/nyu-dl/ dl4mt-tutorial 2 https://github.com/rsennrich/nematus 319 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 319–325, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mechanisms (Durrani et al., 2014). Resorting to subword units eliminates the need for these.3 3 with respect to translation direction and the available (monolingual) training data. For domainadaptation, we rely solely on parameter tuning with Batch-Mira (Cherry and Foster, 2012) and on-line log-linear interpolation. Binary domainindicators for each separate parallel corpus are introduced to the phrase-tables (four indicators) and a separate language model per parallel and monolingual resource is trained (en:16 and ru:12). All language models are 5-gram models with Modified Kneser-Ney smoothing and without pruning thresholds (Heafield et al., 2013). We treat different years of the News Crawl data as different domains to take advantage of possible recency-based effects. During parameter tuning on the newstest2014 test set, we can unsurprisingly observe that weights for"
W16-2316,P16-1162,1,\N,Missing
W16-2316,P16-1009,1,\N,Missing
W16-2378,N13-1000,0,0.218453,"Missing"
W16-2378,2011.mtsummit-papers.35,0,0.877772,"Missing"
W16-2378,C12-1014,0,0.0869539,"Missing"
W16-2378,P13-2121,0,0.0346039,"Missing"
W16-2378,P07-2045,0,0.011945,"e copies, an important aspect for APE. We refer the reader to Bahdanau et al. (2015) for a detailed description of the discussed models. At the time of writing, no APE systems relying on neural translation models seem to have been published.1 3. All other parallel English-German bilingual data admissible during the WMT-16 news translation task; 4. The German monolingual Common Crawl corpus admissible for the WMT-16 news translation and IT translation tasks. 3.2 Pre- and post-processing The provided triplets have already been tokenized, the tokenization scheme seems to correspond to the Moses (Koehn et al., 2007) tokenizer without escaped special characters, so we re-apply escaping. All other data is tokenized with the Moses tokenizer with standard settings per language. We truecase the data with the Moses truecaser. To deal with the limited ability of neural translation models to handle out-of-vocabulary words we split tokens into subword units, following Sennrich et al. (2015b). Subword units were learned using a modified version of the byte pair encoding (BPE) compression algorithm (Gage, 1994). Sennrich et al. (2015b) modified the algorithm to work on character level instead of on bytes. The most"
W16-2378,W15-3025,0,0.13309,"2 2.1 Related work Post-Editing State-of-the-art APE systems follow a monolingual approach firstly proposed by Simard et al. (2007) who trained a phrase-based SMT system on machine translation output and its post-edited versions. B´echara et al. (2011) proposed a “sourcecontext aware” variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT-output and source token pairs. The inclusion of source-language information in that form is shown to be useful to improve the automatic postediting results (B´echara et al., 2012; Chatterjee et al., 2015b). The quality of the word alignments plays an important role for this methods, as shown for instance by Pal et al. (2015). A number of techniques have been developed to improve PB-SMT-based APE systems, e.g. approaches relying on phrase-table filtering techniques and specialized features. Chatterjee et al. (2015a) propose a pipeline where the best language model and pruned phrase table are selected through task-specific dense features. The goal was to overcome data sparsity issues. Introduction This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic"
W16-2378,P10-2041,0,0.0218352,"nd-trip translation results are the new uncorrected MT-output. Artificial post-editing data The provided post-editing data is orders of magnitude too small to train our neural models, and even with the in-domain training data from the IT translation task, we quickly see overfitting effects for a first English-German translation system. Inspired by Sennrich et al. (2015a) — who use backtranslated monolingual data to enrich bilingual training corpora — we decided to create artificial training triplets. 4.1 Round-trip translation Bootstrapping monolingual data We applied cross-entropy filtering (Moore and Lewis, 2010) to the German Common Crawl corpus performing the following steps: • We filtered the corpus for “well-formed” lines which start with a capital Unicode letter character and end in an end-of-sentence punctuation mark. We require the line to contain at least 30 Unicode letters. 4.3 Filtering for TER We hope that a round-trip translation process produces literal translations that may be more-orless similar to post-edited triplets, where the distance between MT-output and post-edited text is generally smaller than between MT-output and human-produced translations of the same source. Having that muc"
W16-2378,P15-2026,0,0.434525,"2 2.1 Related work Post-Editing State-of-the-art APE systems follow a monolingual approach firstly proposed by Simard et al. (2007) who trained a phrase-based SMT system on machine translation output and its post-edited versions. B´echara et al. (2011) proposed a “sourcecontext aware” variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT-output and source token pairs. The inclusion of source-language information in that form is shown to be useful to improve the automatic postediting results (B´echara et al., 2012; Chatterjee et al., 2015b). The quality of the word alignments plays an important role for this methods, as shown for instance by Pal et al. (2015). A number of techniques have been developed to improve PB-SMT-based APE systems, e.g. approaches relying on phrase-table filtering techniques and specialized features. Chatterjee et al. (2015a) propose a pipeline where the best language model and pruned phrase table are selected through task-specific dense features. The goal was to overcome data sparsity issues. Introduction This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic"
W16-2378,W15-3026,0,0.0718563,"Missing"
W16-2378,P02-1040,0,0.0999086,"uned phrase table are selected through task-specific dense features. The goal was to overcome data sparsity issues. Introduction This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. Following the APE shared task from WMT 2015 (Bojar et al., 2015), the aim is to test methods for correcting errors produced by an unknown machine translation system in a black-box scenario. The organizers provide training data with human postedits, evaluation is carried out part-automatically using TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), and part-manually. We explore the application of neural translation models to the APE task and investigate a number of aspects that seem to lead to good results: 751 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 751–758, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The authors of the Abu-MaTran system (no publication, see Bojar et al. (2015)) incorporate sentence-level classifiers in a post-processing step which choose between the given MT output or an automatic post-edition coming from a PB-SMT APE"
W16-2378,N12-1047,0,0.0311161,"output. Since once the input sentence has been provided to a NMT model it essentially turns into a language model, this can be achieved without much effort. In theory an unlimited number of inputs can be combined in this way without the need of specialized multi-input training procedures (Zoph and Knight, 2016).6 In NMT ensembles, homogeneous models are typically weighted equally. Here we combine different models and equal weighting does not work. Instead, we treat each ensemble component as a feature in a traditional log-linear model and perform weighting as parameter tuning with BatchMira (Cherry and Foster, 2012). AmuNMT can produce Moses-compatible n-best lists and we devised an iterative optimization process similar to the one available in Moses. We tune the weights on the development set towards lower TER scores; two iterations seem to be enough. When ensembling one mt→pe model and one src→pe model, the assigned weights correspond roughly to 0.8 and 0.2 respectively. The linear combination of all eight models (mt→pe×4 / src→pe×4) improves quality by 0.9 TER and 1.2 BLEU, however, weights were tuned on the same data. MT-output to post-editing We started training the monolingual MT-PE model with the"
W16-2378,P16-1009,0,0.0419666,"Missing"
W16-2378,D14-1179,0,0.0421994,"Missing"
W16-2378,N07-1064,0,0.583281,"ystem submitted to the shared-task by a large margin. 1 • Log-linear combination of monolingual and bilingual models in an ensemble-like manner; • Addition of task-specific features in a loglinear model that allow to control for faithfulness of the automatic post-editing output with regard to the input, otherwise a weakness of neural translation models. According to the automatic evaluation metrics used for the task, our system is ranked first among all submission to the shared task. 2 2.1 Related work Post-Editing State-of-the-art APE systems follow a monolingual approach firstly proposed by Simard et al. (2007) who trained a phrase-based SMT system on machine translation output and its post-edited versions. B´echara et al. (2011) proposed a “sourcecontext aware” variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT-output and source token pairs. The inclusion of source-language information in that form is shown to be useful to improve the automatic postediting results (B´echara et al., 2012; Chatterjee et al., 2015b). The quality of the word alignments plays an important role for this methods, as shown for instance by Pal"
W16-2378,2006.amta-papers.25,0,0.454008,"the best language model and pruned phrase table are selected through task-specific dense features. The goal was to overcome data sparsity issues. Introduction This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. Following the APE shared task from WMT 2015 (Bojar et al., 2015), the aim is to test methods for correcting errors produced by an unknown machine translation system in a black-box scenario. The organizers provide training data with human postedits, evaluation is carried out part-automatically using TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), and part-manually. We explore the application of neural translation models to the APE task and investigate a number of aspects that seem to lead to good results: 751 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 751–758, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The authors of the Abu-MaTran system (no publication, see Bojar et al. (2015)) incorporate sentence-level classifiers in a post-processing step which choose between the given MT output or an automatic post-"
W16-2378,W15-3027,0,0.0303559,"Missing"
W16-2378,W15-3001,0,\N,Missing
W16-2378,P16-1162,0,\N,Missing
W16-2378,N13-1073,0,\N,Missing
W17-4707,W07-0702,1,0.671821,"oposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurrent neural networks to learn representations that generate both words and CCG supertags, conditioned on the entire lexical and syntactic target history. ments, and also tense and morphological aspects of the word in a given context. Consider the sentence in Figure 1. This sentence contains two PP attachments and could lead to several disambiguation possibilities (“in” can attach to “Netanyahu” or “receives”, and “of” can attach to “capit"
W17-4707,J07-2003,0,0.0156397,"ining. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. Th"
W17-4707,W14-4012,0,0.028354,"Missing"
W17-4707,P17-2021,0,0.0761778,"ework to show source and target syntax provide complementary information. Applying more tightly coupled linguistic factors on the target for NMT has been previously investigated. Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based model"
W17-4707,D14-1179,0,0.00956885,"Missing"
W17-4707,N16-1024,0,0.0146616,"al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurr"
W17-4707,P16-1231,0,0.0172413,"butions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselines, table 2 compares the scores obtained by ou"
W17-4707,P16-1078,0,0.0552534,"ntence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: Obama IOB: O CCG: NP Target-side receives O ((S[dcl]NP)/PP)/NP Net+ B NP an+ I NP yahu E NP in O PP/NP the O NP/N capital O N of O (NPNP)/NP USA"
W17-4707,D16-1025,0,0.0250595,"Missing"
W17-4707,N04-1035,0,0.0228682,"ht coupling of target words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned wi"
W17-4707,W16-2208,0,0.0269734,"Missing"
W17-4707,P02-1040,0,0.120603,"of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_885"
W17-4707,D13-1176,0,0.032968,"urther improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: O"
W17-4707,W05-0908,0,0.0147936,"lish and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_8859 5 72 guage pairs and use bootstrap resampling (Riezler and Maxwell, 2005) to test statistical significance. We compute BLEU with multi-bleu.perl over tokenized sentences both on the development sets, for early stopping, and on the test sets for evaluating our systems. Words are segmented into sub-units that are learned jointly for source and target using BPE (Sennrich et al., 2016b), resulting in a vocabulary size of 85,000. The vocabulary size for CCG supertags was 500. For the experiments with source-side features we use the BPE sub-units and the IOB tags as baseline features. We keep the total word embedding size fixed to 500 dimensions. We allocate 10 dimension"
W17-4707,Q15-1013,1,0.864408,"n the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntacti"
W17-4707,D15-1169,0,0.0163034,"DE→EN EN→DE RO→EN EN→RO1 Experimental Setup and Evaluation 4.1 dev 2,986 1,984 Table 1: Number of sentences in the training, development and test sets. We use EasySRL to label the English side of the parallel corpus with CCG supertags1 instead of using a corpus with gold annotations as in Luong et al. (2016). 4 train 4,468,314 605,885 Data and methods We train the neural MT systems on all the parallel data available at WMT16 (Bojar et al., 2016) for the German↔English and Romanian↔English language pairs. The English side of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016"
W17-4707,E17-3017,1,0.894752,"Missing"
W17-4707,W16-2209,1,0.896448,") show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of how best to incorporate this information. For language pairs where syntactic resources are available on both the source and target-side, we show that approaches to incorporate source syntax and target syntax are complementary. We propose a method for tightly coupling words and syntax by interleaving the target syntactic representation"
W17-4707,W07-0701,0,0.202057,"words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories,"
W17-4707,W16-2323,1,0.729608,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,W16-2204,1,0.813749,"signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word"
W17-4707,P16-1162,1,0.46286,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,R13-1079,1,0.793726,"1 and T2 . This results in two probability distributions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselin"
W17-4707,D16-1159,0,0.0475338,"Alexandra Birch1 1 School of Informatics, University of Edinburgh 2 Adam Mickiewicz University 3 Dep. of Computer Science, Johns Hopkins University {m.nadejde,siva.reddy, rico.sennrich, a.birch}@ed.ac.uk {t.dwojak,junczys}@amu.edu.pl, phi@jhu.edu Abstract tured by these models. In a detailed analysis, Bentivogli et al. (2016) show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of h"
W17-4707,W12-3150,1,0.846453,"erleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indi"
W17-4774,I17-1013,1,0.671277,"ennrich et al., 2017). The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention. The summary provided in this section is based on the description in Sennrich et al. (2017). More details on the specific architectures in this shared 1 639 https://github.com/marian-nmt/marian Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 639–646 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics task submission are given in Junczys-Dowmunt and Grundkiewicz (2017). Given the raw MT output sequence (x1 , . . . , xTx ) of length Tx and its manually post-edited equivalent (y1 , . . . , yTy ) of length Ty , we construct the encoder-decoder model using the following formulations. Layer GRU1 generates an intermediate representation s0j from the previous hidden state sj−1 and the embedding of the previous decoded symbol E[yj−1 ]: s0j = GRU1 (sj−1 , E[yj−1 ]) . The attention mechanism, ATT, inputs the entire context set C along with intermediate hidden state s0j in order to compute the context vector cj as follows: Encoder context A single forward encoder stat"
W17-4774,P07-2045,0,0.0120054,"set Model dev 2016 TER↓ BLEU↑ test 2016 TER↓ BLEU↑ E NC D EC - ATT 22.01 68.11 22.27 66.90 E NC D EC - HARD E NC D EC - HARD + ATT 22.72 22.11 66.82 67.82 22.72 22.10 65.86 67.15 E NC D EC - DOUBLE - ATT E NC D EC - DOUBLE - ATT × 4 20.79 20.10 69.28 70.24 20.69 19.92 68.56 69.40 20.83 20.08 69.02 70.05 20.87 20.34 68.14 68.96 E NC D EC - HARD + DOUBLE - ATT E NC D EC - HARD + DOUBLE - ATT × 4 Table 4: Post-submission results, the main task metric is TER (the lower the better) provement. This one-time looped system is our primary submission P RIMARY. 5 of a vanilla phrase-based Moses system (Koehn et al., 2007) trained only on the official 12,000 sentences. Junczys-Dowmunt and Grundkiewicz (2016) is the best system at the shared task. Pal et al. (2017) S YMMETRIC is the currently best reported result on the WMT16 APE test set for a single neural model (single source), whereas Pal et al. (2017) R ERANKING – the overall best reported result on the test set – is a system combination of Pal et al. (2017) S YMMETRIC with phrase-based models via n-best list re-ranking. Post-submission analysis This section is based on the work in JunczysDowmunt and Grundkiewicz (2017). After the submission we performed a"
W17-4774,W16-2301,0,0.0370485,"Missing"
W17-4774,P17-2031,0,0.0117818,"n constraints: The rest of the model is unchanged; the translation process is the same as before and we use the same target step/token sequence for training. This model is called E NC D EC - HARD - ATT. 2.4 1. The end-of-sentence symbol can only be generated if the hard attention mechanism has reached the end of the input sequence, enforcing full coverage; Soft Double-Attention Neural multi-source models (Zoph and Knight, 2016) seem to be natural fit for the APE task, as raw MT output and original source language input are available. Although application to the APE problem have been reported (Libovický and Helcl, 2017), state-of-the-art results seem to be missing. In this section we give details about our doublesource model implementation. We rename the existing encoder C to Cmt to signal that the first encoder consumes the raw MT output and introduce 2. The hSTEPi symbol cannot be generated once the end-of-sentence position in the source has been reached. It is however still possible to generate content tokens. Obviously, this model requires a target sequence with correctly inserted hSTEPi symbols. For the described APE task, using the Longest 3 641 Similar to GNU wdiff. a structurally identical second enc"
W17-4774,W16-2361,0,0.0145689,"xplore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs mt and src in a single neural architecture, modeling {mt, src} → pe directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. 1 Roman Grundkiewicz School of Informatics University of Edinburgh rgrundki@exseed.ed.ac.uk Introduction During the WMT 2016 APE two systems relied on neural models, the CUNI system (Libovický et al., 2016) and the shared task winner, the system submitted by the Adam Mickiewicz University (AMU) team (Junczys-Dowmunt and Grundkiewicz, 2016). This submission explored the application of neural translation models to the APE problem and achieved good results by treating different models as components in a log-linear model, allowing for multiple inputs (the source src and the translated sentence mt) that were decoded to the same target language (post-edited translation pe). Two systems were considered, one using src as the input (src → pe) and another using mt as the input (mt → pe). A simple stringma"
W17-4774,P17-1175,0,0.0270631,"Missing"
W17-4774,P03-1021,0,0.163172,"ng for multiple inputs (the source src and the translated sentence mt) that were decoded to the same target language (post-edited translation pe). Two systems were considered, one using src as the input (src → pe) and another using mt as the input (mt → pe). A simple stringmatching penalty integrated within the log-linear model was used to control for higher faithfulness with regard to the raw MT output. The penalty fires if the APE system proposes a word in its output that has not been seen in mt. The influence of the components on the final result was tuned with Minimum Error Rate Training (Och, 2003) with regard to the task metric TER. 2 Encoder-Decoder Models with APE-specific Attention Models 2.1 Standard Attentional Encoder-Decoder The attentional encoder-decoder model in Marian1 is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017). The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention. The summary provided in this section is based on the description in Sennrich et al. (2017). More details on the specific architectures in this shared 1 639 https://github.com/ma"
W17-4774,E17-2056,0,0.0263614,"Missing"
W17-4774,P02-1040,0,0.119385,"mbols, otherwise 50. editing data and the respective development and test sets. The training data consists of a small set of 23,000 post-editing triplets (src, mt, pe), where src is the original English text, mt is the raw MT output generated by an English-toGerman system, and pe is the human post-edited MT output. The MT system used to produce the raw MT output is unknown, so is the original training data. The task consist of automatically correcting the MT output so that it resembles human postedited data. The main task metric is TER (Snover et al., 2006) – the lower the better – with BLEU (Papineni et al., 2002) as a secondary metric. Table 1 summarizes the data sets used in this work. To produce our final training data set we oversample the original training data 20 times and add all three artificial data sets (they may overlap). This results in a total of slightly more than 21M training triplets. We keep the development set as a validation set for early stopping and report results on the WMT16 test set. The data is already tokenized, additionally we truecase all files and apply segmentation into BPE subword units. We reuse the subword units distributed with the artificial data set. For the hard-att"
W17-4774,E17-3017,1,0.812401,"mt → pe). A simple stringmatching penalty integrated within the log-linear model was used to control for higher faithfulness with regard to the raw MT output. The penalty fires if the APE system proposes a word in its output that has not been seen in mt. The influence of the components on the final result was tuned with Minimum Error Rate Training (Och, 2003) with regard to the task metric TER. 2 Encoder-Decoder Models with APE-specific Attention Models 2.1 Standard Attentional Encoder-Decoder The attentional encoder-decoder model in Marian1 is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017). The model differs from the standard model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention. The summary provided in this section is based on the description in Sennrich et al. (2017). More details on the specific architectures in this shared 1 639 https://github.com/marian-nmt/marian Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 639–646 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics task submission are given in Junczys-Dowmunt and G"
W17-4774,W16-2378,1,0.949286,"in APE scenarios are incomplete or unsatisfying in terms of performance. In this work, we explore a number of singlesource and double-source neural architectures which we believe to be better fits to the APE task than vanilla encoder-decoder models with soft attention. We focus on neural end-to-end models that combine both inputs mt and src in a single neural architecture, modeling {mt, src} → pe directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks. Finally, we create combinations of both architectures. Following (Junczys-Dowmunt and Grundkiewicz, 2016), we also attempt to generate more artificial data for the task. Instead of relying on filtering towards specific error rates, we generate text with fitting error rates from the start which allows us to retain more data. This work describes the AMU-UEdin submission to the WMT 2017 shared task on Automatic Post-Editing. We explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs mt and src in a single neural architecture, modeling {mt, src} → pe directly. Apart from that, we"
W17-4774,2006.amta-papers.25,0,0.146012,"tted system results to accommodate the additional step symbols, otherwise 50. editing data and the respective development and test sets. The training data consists of a small set of 23,000 post-editing triplets (src, mt, pe), where src is the original English text, mt is the raw MT output generated by an English-toGerman system, and pe is the human post-edited MT output. The MT system used to produce the raw MT output is unknown, so is the original training data. The task consist of automatically correcting the MT output so that it resembles human postedited data. The main task metric is TER (Snover et al., 2006) – the lower the better – with BLEU (Papineni et al., 2002) as a secondary metric. Table 1 summarizes the data sets used in this work. To produce our final training data set we oversample the original training data 20 times and add all three artificial data sets (they may overlap). This results in a total of slightly more than 21M training triplets. We keep the development set as a validation set for early stopping and report results on the WMT16 test set. The data is already tokenized, additionally we truecase all files and apply segmentation into BPE subword units. We reuse the subword units"
W17-4774,N16-1004,0,0.0788017,"Missing"
W18-2105,W18-2100,0,0.0694082,"Missing"
W18-2716,D17-1300,0,0.0989181,"Missing"
W18-2716,N13-1073,0,0.041288,"2, states size 1024). This model corresponds to the University of Edinburgh submission to WMT 2017 (Sennrich et al., 2017a). 4 Optimizing for the CPU Most of our effort was concentrated on improving CPU computation in Marian. Apart from improvements from code profiling and bottleneck identification, we worked towards integrating integer-based matrix products into Marian’s computation graphs. 4.1 Shortlist A simple way to improve CPU-bound NMT efficiency is to restrict the final output matrix multiplication to a small subset of translation candidates. We use a shortlist created with fastalign (Dyer et al., 2013). For every mini-batch we restrict the output 130 vocabulary to the union of the 100 most frequent target words and the 100 most probable translations for every source word in a batch. All CPU results are computed with a shortlist. 4.2 Model Quantization and integer products Previously, Marian tensors would only work with 32-bit floating point numbers. We now support tensors with underlying types corresponding to the standard numerical types in C++. We focus on integer tensors. Some of our submissions replaced 32-bit floating-point matrix multiplication with 16-bit or 8-bit signed integers. Fo"
W18-2716,E17-3017,1,0.706471,"mprovements. The GPU-bound University of Edinburgh 10 Crichton Street Edinburgh, Scotland, EU computations in Marian are already highly optimized and we mostly concentrate on modeling aspects and beam-search hyper-parameters. The weak baselines (at 16.9 BLEU on newstest2014 at least 12 BLEU points below the stateof-the-art) could promote approaches that happily sacrifice quality for speed. We choose a quality cut-off of around 26 BLEU for the first test set (newstest2014) and do not spend much time on systems below that threshold.3 This threshold was chosen based on the semi-official Sockeye (Hieber et al., 2017) baseline (27.6 BLEU on newstest2014) referenced on the shared task page.4 We believe our CPU implementation of the Transformer model (Vaswani et al., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transf"
W18-2716,P18-4020,1,0.877611,"Missing"
W18-2716,D16-1139,0,0.267486,"BLEU for the first test set (newstest2014) and do not spend much time on systems below that threshold.3 This threshold was chosen based on the semi-official Sockeye (Hieber et al., 2017) baseline (27.6 BLEU on newstest2014) referenced on the shared task page.4 We believe our CPU implementation of the Transformer model (Vaswani et al., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transformer-big model (model size 1024, filter size 4096, file size 813 MiB) from Vaswani et al. (2017) for ensembling. We use 36,000 BPE joint subwords (Sennrich et al., 2016) and a joint vocabulary with tied source, target, and output embeddings. One model is trained until convergence for eight days on four P40 GPUs. See tables 3 and 4 for BLEU scores of an overview of BLEU scores for models trained in this work. 1 See the shared ta"
W18-2716,P16-1162,0,0.119163,"., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transformer-big model (model size 1024, filter size 4096, file size 813 MiB) from Vaswani et al. (2017) for ensembling. We use 36,000 BPE joint subwords (Sennrich et al., 2016) and a joint vocabulary with tied source, target, and output embeddings. One model is trained until convergence for eight days on four P40 GPUs. See tables 3 and 4 for BLEU scores of an overview of BLEU scores for models trained in this work. 1 See the shared task description: https://sites. google.com/site/wnmt18/shared-task 2 https://marian-nmt.github.io 3 We added smaller post-submission systems to demonstrate that our approach outperforms systems by other participants when we take part in the race to the quality bottom. 4 https://github.com/awslabs/sockeye/ tree/wnmt18/wnmt18 129 Proceedin"
W18-2716,P18-1166,0,0.172105,"er-parameters. The weak baselines (at 16.9 BLEU on newstest2014 at least 12 BLEU points below the stateof-the-art) could promote approaches that happily sacrifice quality for speed. We choose a quality cut-off of around 26 BLEU for the first test set (newstest2014) and do not spend much time on systems below that threshold.3 This threshold was chosen based on the semi-official Sockeye (Hieber et al., 2017) baseline (27.6 BLEU on newstest2014) referenced on the shared task page.4 We believe our CPU implementation of the Transformer model (Vaswani et al., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transformer-big model (model size 1024, filter size 4096, file size 813 MiB) from Vaswani et al. (2017) for ensembling. We use 36,000 BPE joint subwords (Sennrich et al., 2016) and a joint vocabulary with tied source"
W18-6415,W18-6478,1,0.856232,"at noisy parallel resource: Paracrawl. First experiments with shallow RNN models (chosen for fast experimentation) indicated that adding this data without a rigorous data filtering scheme would lead to catastrophic loss in quality (compare WMT+back-trans and Paracrawl-32M in Table 2). We therefore experiment with data selection and weighting. 4.1 Dual conditional cross-entropy filtering The scoring method introduced in this section is our main contribution to the WMT2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018), details are provided in our corresponding system submission (Junczys-Dowmunt, 2018). For a sentence pair (x, y) we calculate a score: |HA (y|x) − HB (x|y)| 1 + (HA (y|x) + HB (x|y)) 2 (1) where A and B are translation models trained on the same data but in inverse directions, and HM (·|·) is the word-normalized conditional cross-entropy of the probability distribution PM (·|·) for a model M: HM (y|x) = − 1 log PM (y|x) |y| |y| 1 X =− log PM (yt |y<t , x). |y| t=1 The score (denoted as dual conditional cross-entropy) has two components with different functions: the absolute difference |HA (y|x) − HB (x|y) |measures the agreement between the two conditional probability distrib"
W18-6415,W18-6453,0,0.0277264,"ng advantage of Paracrawl This year’s shared task included a new, large but somewhat noisy parallel resource: Paracrawl. First experiments with shallow RNN models (chosen for fast experimentation) indicated that adding this data without a rigorous data filtering scheme would lead to catastrophic loss in quality (compare WMT+back-trans and Paracrawl-32M in Table 2). We therefore experiment with data selection and weighting. 4.1 Dual conditional cross-entropy filtering The scoring method introduced in this section is our main contribution to the WMT2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018), details are provided in our corresponding system submission (Junczys-Dowmunt, 2018). For a sentence pair (x, y) we calculate a score: |HA (y|x) − HB (x|y)| 1 + (HA (y|x) + HB (x|y)) 2 (1) where A and B are translation models trained on the same data but in inverse directions, and HM (·|·) is the word-normalized conditional cross-entropy of the probability distribution PM (·|·) for a model M: HM (y|x) = − 1 log PM (y|x) |y| |y| 1 X =− log PM (yt |y<t , x). |y| t=1 The score (denoted as dual conditional cross-entropy) has two components with different functions: the absolute difference |HA (y|"
W18-6415,P10-2041,0,0.0608548,"agreement between the two conditional probability distributions, assuming that (word-normalized) translation probabilities of sentence pairs in both directions should be roughly equal. We want disagreement to be low, hence this value should be close to 0. adq(x, y) = exp(−(|HA (y|x) − HB (x|y)| 1 + (HA (y|x) + HB (x|y))). 2 We score the entire Paracrawl data with this score and keep the scores. We further assign a value of 1 to all original WMT parallel sentences. That way we have a score for every sentence. 4.2 Cross-entropy difference filtering We treated cross-entropy filtering proposed by Moore and Lewis (2010) as another score. Crossentropy filtering or Moore-Lewis filtering uses the quantity HI (x) − HN (x) (2) where I is an in-domain model, N is a non-domainspecific model and HM is the word-normalized cross-entropy of a probability distribution PM defined by a model M : 427 HM (x) = − 1 log PM (x) |x| |x| 1 X =− log PM (xt |x<t ). |x| t=1 Sentences scored with this method and selected when their score is below a chosen threshold are likely to be more in-domain according to model I and less similar to data used to train N than sentences above that threshold. We chose WMT German news data from the"
W18-6415,E17-2025,0,0.017723,"models and Transformerbase models with synchronous Adam on 8 NVIDIA Titan X Pascal GPUs with 12GB RAM for 10 epochs each. The back-translation model is trained with asynchronous Adam on 8 GPUs. The transformer-big models are trained until convergence on four NVIDIA P40 GPUs with 24GB RAM. We do not specify a batch size as Marian adjusts the batch based on available memory to maximize speed and memory usage. This guarantees that a chosen memory budget will not be exceeded during training and uses maximal batch sizes. All models use tied embeddings between source, target and output embeddings (Press and Wolf, 2017). Contrary to Sennrich et al. (2017) or Vaswani et al. (2017), we do not average checkpoints, but maintain a continuously updated exponentially averaged model over the entire training run. Following Vaswani et al. (2017), the learning rate is set to 0.0003 (0.0002 for Transformer-big) and decayed as the inverse square root of the number of updates after 16,000 updates. When training the Transformer model, a linearly growing learning rate is used during the first 16,000 iterations, starting with 0 until the base learning rate is reached. Table 1 contains our results for WMT2017 training data wi"
W18-6415,P16-1162,0,0.024394,"024. We use layer-normalization (Ba et al., 2016) and variational drop-out with p = 0.1 (Gal and Ghahramani, 2016) inside GRU-blocks and attention. 3.2 Transformer architectures We very closely follow the architecture described in Vaswani et al. (2017) and their “base” and “big” models. 3.3 Training recipe Modeled after the description from Sennrich et al. (2017), we reuse the example available at https://github.com/marian-nmt/ marian-examples and perform the following steps: • preprocessing of training data, tokenization, true-casing1 , vocabulary reduction to 36,000 joint BPE subword units (Sennrich et al., 2016) with a separate tool.2 • training of a shallow model for backtranslation on parallel WMT17 data; 1 Proprocessing was performed using scripts from Moses (Koehn et al., 2007). 2 https://github.com/rsennrich/ subword-nmt At this stage we did not update to WMT2018 parallel or monolingual training data. This might put us at a slight disadvantage, but we could reuse models and back-translated data that was produced earlier. We train the deep RNN models and Transformerbase models with synchronous Adam on 8 NVIDIA Titan X Pascal GPUs with 12GB RAM for 10 epochs each. The back-translation model is tra"
W18-6415,W18-6401,0,\N,Missing
W18-6415,W17-4739,0,\N,Missing
W18-6467,W16-2378,1,0.749868,"data consists of a small set of post-editing triplets (src, mt, pe), where src is the original English text, mt is the raw MT output generated by an Englishto-German system, and pe is the human post-edited MT output. The MT system used to produce the raw MT output is unknown, as is the original training data. The task consists of automatically correcting the MT output so that it resembles human postedited data. The main task metric is TER (Snover et al., 2006) — the lower the better — with BLEU (Papineni et al., 2002) as a secondary metric. To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) — the authors of the best WMT-2016 APE shared task system — generated large amounts of artificial data via round-trip translations. The artificial data has been filtered to match the HTER statistics of the training and development data for the shared task and was made available for download. The organizers also made available a large new resource for APE training, the eSCAPE corpus (Negri et al., 2018), which contains triplets generated from SMT and NMT systems in separate data sets. To produce our final training data set we oversample the original training data 20 times and add both artifici"
W18-6467,W17-4774,1,0.805207,"s the SMT postediting sub-task establishing the new state-ofthe-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on the rather weak results in the NMT sub-task, we hypothesize that neural-on-neural APE might not be actually useful. 1 2 Introduction This paper describes the Microsoft (MS) and University of Edinburgh (UEdin) submission to the Automatic Post-editing shared task at WMT2018 (Chatterjee et al., 2018). Based on training data and systems from the WMT2017 shared task (Bojar et al., 2017), we re-implement our own models from the last shared task (Junczys-Dowmunt and Grundkiewicz, 2017a,b) and introduce a few small improvements based on extensive parameter sharing. Next, we experiment with our implementation of dual-source transformer models which have been available in our NMT toolkit Marian (Junczys-Dowmunt et al., 2018) since version v1.0 (November 2017). We believe this is one of the first descriptions of such an architectures for Automatic Post-Editing (APE) purposes, but similar approaches have been used for two-step decoding, for instance in Hassan et al. (2018). We further extend this model to share parameters across encoders with improved results for APE. Training,"
W18-6467,I17-1013,1,0.855433,"s the SMT postediting sub-task establishing the new state-ofthe-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on the rather weak results in the NMT sub-task, we hypothesize that neural-on-neural APE might not be actually useful. 1 2 Introduction This paper describes the Microsoft (MS) and University of Edinburgh (UEdin) submission to the Automatic Post-editing shared task at WMT2018 (Chatterjee et al., 2018). Based on training data and systems from the WMT2017 shared task (Bojar et al., 2017), we re-implement our own models from the last shared task (Junczys-Dowmunt and Grundkiewicz, 2017a,b) and introduce a few small improvements based on extensive parameter sharing. Next, we experiment with our implementation of dual-source transformer models which have been available in our NMT toolkit Marian (Junczys-Dowmunt et al., 2018) since version v1.0 (November 2017). We believe this is one of the first descriptions of such an architectures for Automatic Post-Editing (APE) purposes, but similar approaches have been used for two-step decoding, for instance in Hassan et al. (2018). We further extend this model to share parameters across encoders with improved results for APE. Training,"
W18-6467,P10-2041,0,0.0374373,"ion factors potentially beneficial for ensembling. Experiments with eSCAPE So far, we only trained on data that was available during WMT2017. This year, the task organizers added a new large corpus created for automatic post-editing across many domains. We experimented with domain selection algorithms for this corpus and tried to find subsets that would be better suited to the given IT domain. We trained an 5-gram language model on a 10M words randomly sampled subset of the German IT training data and a similarly size language model on the eSCAPE data. Next we applied cross-entropy filtering (Moore and Lewis, 2010) to produce domain scores. We sorted eSCAPE by these scores and selected different sizes of subsets. Smaller subsets 3.4 The NMT APE sub-task So far we reported only results for the SMT APE sub-task. For the NMT system we trained our transformer-base model on eSCAPE NMT data only. Including SMT-specific data seemed to be harmful. In the end we only applied an ensemble of 4 such models observing moderate improvements on the development data. It seemed that our system was quite good at correcting errors due to hallucinated BPE words. We believe that our shared embeddings/encoders were helpful he"
W18-6467,L18-1004,0,0.183045,"sk metric is TER (Snover et al., 2006) — the lower the better — with BLEU (Papineni et al., 2002) as a secondary metric. To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) — the authors of the best WMT-2016 APE shared task system — generated large amounts of artificial data via round-trip translations. The artificial data has been filtered to match the HTER statistics of the training and development data for the shared task and was made available for download. The organizers also made available a large new resource for APE training, the eSCAPE corpus (Negri et al., 2018), which contains triplets generated from SMT and NMT systems in separate data sets. To produce our final training data set we oversample the original training data 20 times and add both artificial data sets. This results in a total of 1 We did not make the models available, but researchers interested in reproducing these results are encouraged to contact one or both of the authors. We will be happy to help. The used architectures are available in Marian: https: //marian-nmt.github.io 822 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 822–8"
W18-6467,P02-1040,0,0.10156,"al WMT-2018 automatic post-editing data and the respective development and test sets. The training data consists of a small set of post-editing triplets (src, mt, pe), where src is the original English text, mt is the raw MT output generated by an Englishto-German system, and pe is the human post-edited MT output. The MT system used to produce the raw MT output is unknown, as is the original training data. The task consists of automatically correcting the MT output so that it resembles human postedited data. The main task metric is TER (Snover et al., 2006) — the lower the better — with BLEU (Papineni et al., 2002) as a secondary metric. To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) — the authors of the best WMT-2016 APE shared task system — generated large amounts of artificial data via round-trip translations. The artificial data has been filtered to match the HTER statistics of the training and development data for the shared task and was made available for download. The organizers also made available a large new resource for APE training, the eSCAPE corpus (Negri et al., 2018), which contains triplets generated from SMT and NMT systems in separate data"
W18-6467,P16-1162,0,0.125447,"ppy to help. The used architectures are available in Marian: https: //marian-nmt.github.io 822 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 822–826 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64094 slightly more than 5M training triplets. We validate on the development set for early stopping and report results on the WMT-2016 APE test set. The data is already tokenized. Additionally we truecase all files and apply segmentation into BPE subword units (Sennrich et al., 2016). We reuse the subword units distributed with the artificial data set. 3 Softmax Linear Add & Norm Feed Forward Add & Norm Multi-Head Attention Experiments Add & Norm During the WMT2017 APE shared task we submitted a dual-source model with soft and hard attention which placed second right after a very similar dualsource model by the FBK team. We include the performance of those models based on the shared task descriptions in Table 1, systems WMT17:FBK and WMT17:AMU (ours). We mostly worked on the APE sub-task for automatic post-editing for the SMT system. The system in the NMT sub-task seemed"
W18-6467,2006.amta-papers.25,0,0.117664,"test data We perform all our experiments with the official WMT-2018 automatic post-editing data and the respective development and test sets. The training data consists of a small set of post-editing triplets (src, mt, pe), where src is the original English text, mt is the raw MT output generated by an Englishto-German system, and pe is the human post-edited MT output. The MT system used to produce the raw MT output is unknown, as is the original training data. The task consists of automatically correcting the MT output so that it resembles human postedited data. The main task metric is TER (Snover et al., 2006) — the lower the better — with BLEU (Papineni et al., 2002) as a secondary metric. To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) — the authors of the best WMT-2016 APE shared task system — generated large amounts of artificial data via round-trip translations. The artificial data has been filtered to match the HTER statistics of the training and development data for the shared task and was made available for download. The organizers also made available a large new resource for APE training, the eSCAPE corpus (Negri et al., 2018), which contains tr"
W18-6478,D11-1033,0,0.660449,"can filter harmful sentence pairs from these large resources. In this paper, we introduce dual conditional cross-entropy filtering, a simple but effective data selection method for noisy parallel corpora. We think of it as the missing adequacy component to the fluency aspects of cross-entropy difference filtering by Moore and Lewis (2010). Similar to Moore-Lewis filtering for monolingual data, we 1 https://paracrawl.eu directly select samples that have the potential to improve perplexity (and in our case translation performance) of models trained with the filtered data. This is different from Axelrod et al. (2011) who simply expand Moore and Lewis filtering to both sides of the parallel corpus. We use conditional probability distributions and enforce agreement between inverse translation directions. In most cases, neural translation models are trained to minimize perplexity (or cross-entropy) on a training set. Our selection criterion includes the optimization criterion of neural machine translation which we approximate by using neural translation models pre-trained on clean seed data. We evaluated our method in the context of the WMT2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018) an"
W18-6478,W18-2709,0,0.0606905,"ontext of the WMT2018 shared task on parallel corpus filtering and achieve the overall highest ranking scores of the shared task, scoring top in three out of four subtasks. 1 Introduction Recently, large web-crawled parallel corpora which are meant to rival non-public resources held by popular machine translation providers have been made publicly available to the research community in form of the Paracrawl corpus.1 At the same time, it has been shown that neural translation models are far more sensitive to noisy parallel training data than phrase-based statistical machine translation methods (Khayrallah and Koehn, 2018; Belinkov and Bisk, 2017). This creates the need for data selection methods that can filter harmful sentence pairs from these large resources. In this paper, we introduce dual conditional cross-entropy filtering, a simple but effective data selection method for noisy parallel corpora. We think of it as the missing adequacy component to the fluency aspects of cross-entropy difference filtering by Moore and Lewis (2010). Similar to Moore-Lewis filtering for monolingual data, we 1 https://paracrawl.eu directly select samples that have the potential to improve perplexity (and in our case translat"
W18-6478,W18-6453,0,0.0876973,"Axelrod et al. (2011) who simply expand Moore and Lewis filtering to both sides of the parallel corpus. We use conditional probability distributions and enforce agreement between inverse translation directions. In most cases, neural translation models are trained to minimize perplexity (or cross-entropy) on a training set. Our selection criterion includes the optimization criterion of neural machine translation which we approximate by using neural translation models pre-trained on clean seed data. We evaluated our method in the context of the WMT2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018) and submitted our best method to the task. Although we only optimized for one of the four subtasks of the shared task, our submission scored highest for three out of four subtasks and third for the fourth subtask; there were 48 submissions to each subtask in total. 2 WMT 2018 shared task on parallel corpus filtering We quote the shared task description provided by the organizers on the task website2 and add citations where appropriate: The organizers “provide a very noisy 1 billion word (English token count) German-English corpus crawled from the web as part of the Paracrawl project” and “ask"
W18-6478,P10-2041,0,0.843173,"same time, it has been shown that neural translation models are far more sensitive to noisy parallel training data than phrase-based statistical machine translation methods (Khayrallah and Koehn, 2018; Belinkov and Bisk, 2017). This creates the need for data selection methods that can filter harmful sentence pairs from these large resources. In this paper, we introduce dual conditional cross-entropy filtering, a simple but effective data selection method for noisy parallel corpora. We think of it as the missing adequacy component to the fluency aspects of cross-entropy difference filtering by Moore and Lewis (2010). Similar to Moore-Lewis filtering for monolingual data, we 1 https://paracrawl.eu directly select samples that have the potential to improve perplexity (and in our case translation performance) of models trained with the filtered data. This is different from Axelrod et al. (2011) who simply expand Moore and Lewis filtering to both sides of the parallel corpus. We use conditional probability distributions and enforce agreement between inverse translation directions. In most cases, neural translation models are trained to minimize perplexity (or cross-entropy) on a training set. Our selection c"
W18-6478,P18-2037,0,0.0361804,"5 de-en·adq·dom0.50 35.5 36.0 35.4 30.5 31.0 30.6 de-en·sim de-en·sim·dom0.25 de-en·adq·sim·dom0.25 34.5 35.5 35.5 29.6 30.6 30.7 Table 2: Results on development data. We only train neural models for the 100M sub-task. We did not optimize for any of the other three sub-tasks. the selection process or experiment with conditional probability distributions (translation models) for domain filtering. 3.5 Cosine similarity of sentence embeddings We further experimented with sentence embedding similarity to contrast this method with our crossentropy based approach. Recently, Hassan et al. (2018) and Schwenk (2018) used cosine similarities of sentence embeddings in a common multilingual space to select translation pairs for neural machine translation. Both these approaches rely on creating a multi-lingual translation model across all available translation directions and then using the accumulated encoder representations (after summing or max-pooling contextual word-level embeddings across the time dimension) of sentences in a pair to compute similarity scores. Following Hassan et al. (2018), we train a new multi-lingual translation model on WMT18 parallel data (excluding Paracrawl) by joining GermanEngl"
W19-4427,D18-1332,1,0.811303,"s with the Marian toolkit11 (Junczys-Dowmunt et al., 2018a), and generally follow the configuration proposed by JunczysDowmunt et al. (2018b). Transformer models are trained using Adam (Kingma and Ba, 2014) with a learning rate of 0.0003 and linear warm-up for the first 16k updates, followed by inverted squared decay. For the larger models, we decrease the learning rate to 0.0002 and warm-up to 8k first updates. We train with synchronous SGD (Adam) and dynamically sized mini-batches fitted into 48GB GPU RAM memory across 4 GPUs, accumulating gradients for 3 iterations before making an update (Bogoychev et al., 2018). This results in mini-batches consisting of ca. 2,700 sentences. The maximum length of a training 11 sentence is limited to 150 subword units. Strong regularization via dropout (Gal and Ghahramani, 2016) is used to dissuade the model from simply copying the input: we use a dropout probability between transformer layers of 0.3, for transformer self-attention and filters of 0.1, and for source and target words of 0.3 and 0.1 respectively. For source and target words we dropout entire embedding vectors, not just single neurons. We also use label smoothing with a weight of 0.1, and exponential av"
W19-4427,W18-0529,0,0.127845,"Missing"
W19-4427,W19-4406,0,0.408145,"vated by the problems identified in these papers but concerned by the complexity of their methods, we sought simpler and more effective approaches to both challenges. For data sparsity, we propose an unsupervised synthetic parallel data generation method exploiting confusion sets from a spellchecker to augment training data used for pre-training sequence-to-sequence models. For multi-pass decoding, we use right-to-left models in rescoring, similar to competitive neural machine translation systems. In the Building Educational Application (BEA) 2019 Shared Task on Grammatical Error Correction1 (Bryant et al., 2019), our GEC systems ranked first in the restricted and low-resource tasks.2 This confirms the effectiveness of the proposed methods in scenarios with and without readily-available large amounts of error-annotated data. The rest of the paper is organized as follows: 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ 2 Incidentally, our restricted system also outperformed all submissions to the unrestricted task to which we did not submit. 252 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 252–263 c Florence, Italy, August 2, 2019. 2019"
W19-4427,P17-1074,0,0.139008,"ed W&I+LOCNESS datasets. No restriction was placed on publicly available unannotated data or NLP tools such as spellcheckers. The low-resource track was limited to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track allowed unrestricted data. The performance of participating systems was evaluated using the ERRANT scorer (Bryant et al., 2017) which reports a F0.5 over span-based corrections. 3 Related work plied their model with noisy examples synthesized from clean sentences. Junczys-Dowmunt et al. (2018b) utilized a large amount of monolingual data by pre-training decoder parameters with a language model, and Lichtarge et al. (2018, 2019), on the other hand, used a large-scale out-of-domain parallel corpus extracted from Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by mea"
W19-4427,P18-1097,0,0.482717,"where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F0.5 in the restricted and low-resource tracks respectively, both on the W&I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-theart results of 64.16 M2 for the submitted system, and 61.30 M2 for the constrained system trained on the NUCLE and Lang-8 data. 1 Multi-pass decoding: the correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018"
W19-4427,W17-5037,0,0.0194999,"e correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018b). These two papers established a number of techniques for neural GEC, such as transfer learning from monolingual data, strong regularization, model ensembling, and using a large-scale language model. Subsequent work highlighted two challenges in neural GEC, data sparsity and multi-pass decoding: Motivated by the problems identified in these papers but concerned by the complexity of their methods, we sought simpler and more ef"
W19-4427,N12-1067,0,0.517964,"0.4 0.49 0.65 0.58 0.75 0.73 0.53 0.38 0.47 0.37 0.26 0.25 0.53 0.44 0.71 0.61 0.28 0.21 0.6 0.57 0.59 0.56 0.57 0.49 0.29 0.2 0.4 0.22 0.22 0.29 0.19 0.6 0.7 0.66 Low-resource Restricted 0.8 0.69 0.66 F0.5 0.2 W O PR EP PR O PU N N CT SP EL L V V ER ER B B: FO V RM E V RB: ER S B: VA TE N SE M O RP H N NO O U UN N :N N U O M U N :P O SS O RT OT H H ER D ET N J CO DV A A D J 0.0 Figure 2: Comparison of restricted and low-resource systems (F0.5 ) on a selection of error types from ERRANT on W&I+LOCNESS Dev. the CoNLL 2014 test set (Dahlmeier et al., 2013) calculated with the official M2Scorer (Dahlmeier and Ng, 2012). We also report results on the JFLEG test set (Napoles et al., 2017) using GLEU (Napoles et al., 2015). Following other works (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018b), we correct spelling errors in JFLEG using Enchant before decoding. On CoNLL-2014, our best GEC system achieves 64.16 M2 , which is the highest score reported on this test set so far, including the systems trained on non-publicly available resources (Ge et al., 2018a,b). Although comparing to prior work, the improvement is impressive, our submitted system uses the public FCE corpus and the new W&I Train sets and s"
W19-4427,W13-1703,0,0.692334,"phic errors. The shared task introduced two new annotated datasets for development and evaluation: Cambridge English Write & Improve (W&I) and the LOCNESS corpora (Bryant et al., 2019; Granger, 1998). These represent a more diverse cross-section of English language levels and domains than previous datasets. There were three tracks that varied in the amount of admissible annotated learner data for system development. In the restricted track, participants were provided with four learner corpora containing 1.2 million sentences in total: the public FCE corpus (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013), Lang-8 Corpus of Learner English (Mizumoto et al., 2012), and the mentioned W&I+LOCNESS datasets. No restriction was placed on publicly available unannotated data or NLP tools such as spellcheckers. The low-resource track was limited to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track allowed unrestricted data. The p"
W19-4427,E14-3013,0,0.343628,"m Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterati"
W19-4427,W09-2112,0,0.0638537,"rallel corpus extracted from Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. ("
W19-4427,N18-2046,1,0.867139,"64.24 F0.5 in the restricted and low-resource tracks respectively, both on the W&I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-theart results of 64.16 M2 for the submitted system, and 61.30 M2 for the constrained system trained on the NUCLE and Lang-8 data. 1 Multi-pass decoding: the correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018b). These two papers established a number of techniques for neural GEC, such as transfer learning from monolingual data, strong regu"
W19-4427,W11-2123,1,0.636646,"ng examples. Our final training set in the restricted setting contains 1,953,554 sentences, assembled from the cleaned Lang-8 corpus and oversampled remaining corpora: FCE and the training portion of W&I are oversampled 10 times, NUCLE 5 times. Table 3 summarizes all data sets used for training. W&I+LOCNESS Dev is used solely as a development set in both tracks. an average score from two language models: an n-gram probabilistic word-level language model estimated from target sentences, and a simplified operation sequence model built on edits between source and target sentences.8 We use KenLM (Heafield, 2011) to build 5-gram language models. The top 2 million sentence pairs with the highest scores are used as training data in place of the errorannotated ESL learner data to train models for the low-resource system. Monolingual data We use News Crawl6 — a publicly available corpus of monolingual texts extracted from online newspapers released for the WMT series of shared tasks (Bojar et al., 2018) — as our primary monolingual data source. We uniformly sampled 100 million English sentences from de-duplicated crawls in years 2007 to 2018 to produce synthetic parallel data for model pretraining. Anothe"
W19-4427,D16-1161,1,0.882952,"d Lang-8 data. 1 Multi-pass decoding: the correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018b). These two papers established a number of techniques for neural GEC, such as transfer learning from monolingual data, strong regularization, model ensembling, and using a large-scale language model. Subsequent work highlighted two challenges in neural GEC, data sparsity and multi-pass decoding: Motivated by the problems identified in these papers but concerned by the complexity of their methods, we"
W19-4427,N18-1055,1,0.673652,"ed to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track allowed unrestricted data. The performance of participating systems was evaluated using the ERRANT scorer (Bryant et al., 2017) which reports a F0.5 over span-based corrections. 3 Related work plied their model with noisy examples synthesized from clean sentences. Junczys-Dowmunt et al. (2018b) utilized a large amount of monolingual data by pre-training decoder parameters with a language model, and Lichtarge et al. (2018, 2019), on the other hand, used a large-scale out-of-domain parallel corpus extracted from Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previo"
W19-4427,D18-1541,0,0.341544,"n explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two models display unique adva"
W19-4427,P18-1007,0,0.0309372,"ference filtering by Moore and Lewis (2010). W&I+LOCNESS Dev is used as an in-domain seed corpus. All sentence pairs in WikEd are sorted w.r.t 5.2 Data preprocessing Following the preprocessing methods of the data provided in the shared task, we tokenize other data sets with spaCy.9 We also normalize Unicode punctuation to ASCII with a script included in the Moses SMT toolkit10 (Koehn et al., 2007). To handle the open vocabulary issue, we split tokens into 32,000 subword units trained on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Richardson, 2018). Model architecture We experiment with different variants of Transformer models (Vaswani et al., 2017). The “Transformer Base” architecture has 6 blocks of selfattention/feed forward sub-layers in the encoder and decoder, 8-head self-attention layers, and embeddings vector size of 512. The ReLU activation function (Nair and Hinton, 2010) is used between filters of size 2048. We tie output layer, decoder and encoder embeddings (Press and Wolf, 2017). We choose the “Transformer Big” architecture as our final models for the restricted track. They di"
W19-4427,D18-2012,0,0.0339514,"ewis (2010). W&I+LOCNESS Dev is used as an in-domain seed corpus. All sentence pairs in WikEd are sorted w.r.t 5.2 Data preprocessing Following the preprocessing methods of the data provided in the shared task, we tokenize other data sets with spaCy.9 We also normalize Unicode punctuation to ASCII with a script included in the Moses SMT toolkit10 (Koehn et al., 2007). To handle the open vocabulary issue, we split tokens into 32,000 subword units trained on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Richardson, 2018). Model architecture We experiment with different variants of Transformer models (Vaswani et al., 2017). The “Transformer Base” architecture has 6 blocks of selfattention/feed forward sub-layers in the encoder and decoder, 8-head self-attention layers, and embeddings vector size of 512. The ReLU activation function (Nair and Hinton, 2010) is used between filters of size 2048. We tie output layer, decoder and encoder embeddings (Press and Wolf, 2017). We choose the “Transformer Big” architecture as our final models for the restricted track. They differ from Transformer Base by the number of hea"
W19-4427,N19-1333,0,0.397602,"Missing"
W19-4427,D17-1156,0,0.028521,"ence. embedding similarities (Mikolov et al., 2013). Corpus 4.3 FCE Train NUCLE Lang-8 W&I Train W&I+LOCNESS Dev WikEd R R R R L,R L 28,350 57,113 1,041,409 34,308 4,384 2,000,000 News Crawl L,R 100M Model pre-training and fine-tuning We generate synthetic errors from 100 million sentences sampled from the English part of the WMT News Crawl corpus (Bojar et al., 2018) and use pairs of synthetic and authentic sentences exclusively to pre-train transformer models. A pre-trained model can be used with the actual indomain error-annotated data by fine-tuning (Hinton and Salakhutdinov, 2006; Miceli Barone et al., 2017). We experimented with two fine-tuning strategies: 1. Initialising the neural network weights with the pre-trained model and starting a new training run on new data. This resets learning rate scheduling and optimizer parameters. We further refer to this procedure as re-training. 2. Continuing training the existing model with new data preserving the learning rate, optimizer parameters and historic weights for exponential smoothing. We refer to this scheme as fine-tuning. The main difference between re-training and fine-tuning is resetting the training state after pretraining. The latter strateg"
W19-4427,C12-2084,0,0.243296,"Missing"
W19-4427,P10-2041,0,0.0291478,"produce synthetic parallel data for model pretraining. Another subset of 2 million sentences was selected to augment the training data during fine-tuning. The Enchant spellchecker7 with the Aspell backend and a British English dictionary were used to generate confusion sets. 5.3 Wikipedia edits In the low-resource setting, we use a filtered subset of the WikEd corpus (Grundkiewicz and Junczys-Dowmunt, 2014). The original corpus contains 56 million automatically extracted edited sentences from Wikipedia revisions and is quite noisy. We clean the data using cross-entropy difference filtering by Moore and Lewis (2010). W&I+LOCNESS Dev is used as an in-domain seed corpus. All sentence pairs in WikEd are sorted w.r.t 5.2 Data preprocessing Following the preprocessing methods of the data provided in the shared task, we tokenize other data sets with spaCy.9 We also normalize Unicode punctuation to ASCII with a script included in the Moses SMT toolkit10 (Koehn et al., 2007). To handle the open vocabulary issue, we split tokens into 32,000 subword units trained on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Ri"
W19-4427,P15-2097,0,0.215119,"56 0.57 0.49 0.29 0.2 0.4 0.22 0.22 0.29 0.19 0.6 0.7 0.66 Low-resource Restricted 0.8 0.69 0.66 F0.5 0.2 W O PR EP PR O PU N N CT SP EL L V V ER ER B B: FO V RM E V RB: ER S B: VA TE N SE M O RP H N NO O U UN N :N N U O M U N :P O SS O RT OT H H ER D ET N J CO DV A A D J 0.0 Figure 2: Comparison of restricted and low-resource systems (F0.5 ) on a selection of error types from ERRANT on W&I+LOCNESS Dev. the CoNLL 2014 test set (Dahlmeier et al., 2013) calculated with the official M2Scorer (Dahlmeier and Ng, 2012). We also report results on the JFLEG test set (Napoles et al., 2017) using GLEU (Napoles et al., 2015). Following other works (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018b), we correct spelling errors in JFLEG using Enchant before decoding. On CoNLL-2014, our best GEC system achieves 64.16 M2 , which is the highest score reported on this test set so far, including the systems trained on non-publicly available resources (Ge et al., 2018a,b). Although comparing to prior work, the improvement is impressive, our submitted system uses the public FCE corpus and the new W&I Train sets and should not be directly contrasted with systems trained on the NUCLE and Lang-8 corpora only. In contrast"
W19-4427,E17-2037,0,0.193977,"71 0.61 0.28 0.21 0.6 0.57 0.59 0.56 0.57 0.49 0.29 0.2 0.4 0.22 0.22 0.29 0.19 0.6 0.7 0.66 Low-resource Restricted 0.8 0.69 0.66 F0.5 0.2 W O PR EP PR O PU N N CT SP EL L V V ER ER B B: FO V RM E V RB: ER S B: VA TE N SE M O RP H N NO O U UN N :N N U O M U N :P O SS O RT OT H H ER D ET N J CO DV A A D J 0.0 Figure 2: Comparison of restricted and low-resource systems (F0.5 ) on a selection of error types from ERRANT on W&I+LOCNESS Dev. the CoNLL 2014 test set (Dahlmeier et al., 2013) calculated with the official M2Scorer (Dahlmeier and Ng, 2012). We also report results on the JFLEG test set (Napoles et al., 2017) using GLEU (Napoles et al., 2015). Following other works (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018b), we correct spelling errors in JFLEG using Enchant before decoding. On CoNLL-2014, our best GEC system achieves 64.16 M2 , which is the highest score reported on this test set so far, including the systems trained on non-publicly available resources (Ge et al., 2018a,b). Although comparing to prior work, the improvement is impressive, our submitted system uses the public FCE corpus and the new W&I Train sets and should not be directly contrasted with systems trained on the NUCLE an"
W19-4427,P03-1021,0,0.0189038,"exts, so can be more capable of correcting errors of different types. We adapt the re-ranking technique. We first generate n-best lists using the ensemble of standard left-to-right models and the language model, then re-score sentence pairs with right-to-left models using length-normalized scores, and re-rank the hypotheses. We have experimented with different weighting strategies during re-scoring, but found that weighting all sequence-to-sequence models equally with 1.0 and grid-searching the weight of the language model again works best. Tuning all ensemble weights independently with MERT (Och, 2003) lead to overfitting to the development set. 5 5.1 Right-to-left re-ranking Experiments Datasets Error-annotated data The restricted models are trained on data provided in the shared task: the A common approach to improve the performance of NMT systems is re-ranking with right-to-left 255 FCE corpus (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013), W&I+LOCNESS data sets (Bryant et al., 2019; Granger, 1998), and a preprocessed version of the Lang-8 Corpus of Learner English (Mizumoto et al., 2012). We clean Lang-8 using regular expressions5 to 1) filter out sentences with a low rati"
W19-4427,E17-2025,0,0.0261162,"on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Richardson, 2018). Model architecture We experiment with different variants of Transformer models (Vaswani et al., 2017). The “Transformer Base” architecture has 6 blocks of selfattention/feed forward sub-layers in the encoder and decoder, 8-head self-attention layers, and embeddings vector size of 512. The ReLU activation function (Nair and Hinton, 2010) is used between filters of size 2048. We tie output layer, decoder and encoder embeddings (Press and Wolf, 2017). We choose the “Transformer Big” architecture as our final models for the restricted track. They differ from Transformer Base by the number of heads in multi-head attention components (16 heads instead for 8), larger embeddings vector size of 1024 and filter size of 4096. The architecture of the language models corresponds to the structure of the decoder of the sequence-to-sequence model, either Transformer Base or Big. 5 Cleaning Lang-8 led to minor improvements during the preliminary experiments when no pre-training was used. 6 http://data.statmt.org/news-crawl/ 7 https://abiword.github.io/"
W19-4427,D17-1039,0,0.0727316,"Missing"
W19-4427,W17-5032,0,0.354327,"ial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two mode"
W19-4427,E14-1038,0,0.204522,"upervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by"
W19-4427,D10-1094,0,0.165207,"etic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further exten"
W19-4427,I17-2062,0,0.0756667,"Missing"
W19-4427,W17-4739,1,0.866064,"tated data without and with transfer learning from the language model. Surprisingly, for the restricted system, pre-training the decoder parameters (Baseline + LM pretraining) does not yield much improvement. A major improvement is achieved, however, by pre-training of the entire neural network on the synthetic data (Re-training). The fine-tuning strategy generally leads to better results than re-training, mostly due to increased precision. Adding 2 million of synthetic sentences to the error-annotated data — resulting approximately in an 1:1 ratio of genuine and artificial training examples (Sennrich et al., 2017) — further improves the performance. Ensembling eight Transformer models with a language model and re-ranking the n-best lists with four right-to-left models leads to consistent improvements. The quality of the language model is important as using a stronger language model (LM Big) generally improves the scores. The systems with bigger models (Ensemble Big×4 + LM Big) have a higher precision and thus perform better on both datasets. Interestingly, reranking using smaller and relatively weaker rightto-left Transformer Base models is still beneficial. We have found that re-ranking works best for"
W19-4427,W16-2309,0,0.0334077,"g model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two models display unique advantages for specific error types as they decode with different contexts. Inspired by this finding, we adapt a common technique from NMT (Sennrich et al., 2016, 2017) that reranks with a right-to-left model, but without multiple rounds. We contend that multiple rounds are only necessary if the system has low recall. Many recent advances in neural GEC aim at overcoming the mentioned data sparsity problem. Ge et al. (2018a) proposed fluency-boost learning that generates additional training examples during training from an independent backward model or the forward model being trained. Xie et al. (2018) sup253 4 4.1 System overview Transformer models Our neural GEC systems are based on Transformer models (Vaswani et al., 2017) that have been recently ad"
W19-4427,N19-1406,0,0.132521,"Missing"
W19-4427,N18-1057,0,0.244346,"(2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two models display unique advantages for specific error types as they decode with differ"
W19-4427,P11-1019,0,0.817167,"g grammatical, lexical, and orthographic errors. The shared task introduced two new annotated datasets for development and evaluation: Cambridge English Write & Improve (W&I) and the LOCNESS corpora (Bryant et al., 2019; Granger, 1998). These represent a more diverse cross-section of English language levels and domains than previous datasets. There were three tracks that varied in the amount of admissible annotated learner data for system development. In the restricted track, participants were provided with four learner corpora containing 1.2 million sentences in total: the public FCE corpus (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013), Lang-8 Corpus of Learner English (Mizumoto et al., 2012), and the mentioned W&I+LOCNESS datasets. No restriction was placed on publicly available unannotated data or NLP tools such as spellcheckers. The low-resource track was limited to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track"
W19-4427,N19-1014,0,0.240749,"Missing"
W19-5321,D18-1045,0,0.408662,"r main focus is document-level neural machine translation with deep transformer models. We first explore strong sentence-level systems, trained on large-scale data created via data-filtering and noisy back-translation and investigate the interaction of both with the translation direction Sentence-Level Baselines Before moving on to building our document-level systems, we first start with a baseline sentencelevel system. We try to combine the strengths of last year’s two dominating systems for the English-German news translation task – FAIR’s submission with large-scale noisy back-translation (Edunov et al., 2018) and our own, based on dual cross-entropy data-filtering (Junczys-Dowmunt, 2018b,a). For the current WMT19 shared task for 225 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 225–233 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics English-German, evaluation is carried out on a test set where the source side consists of original English content only, the target side is a translation. To inform our system choices, we create a similar dev set out of test2016, test2017 and test2018 by splitting the te"
W19-5321,W18-6478,1,0.840629,"r models. We first explore strong sentence-level systems, trained on large-scale data created via data-filtering and noisy back-translation and investigate the interaction of both with the translation direction Sentence-Level Baselines Before moving on to building our document-level systems, we first start with a baseline sentencelevel system. We try to combine the strengths of last year’s two dominating systems for the English-German news translation task – FAIR’s submission with large-scale noisy back-translation (Edunov et al., 2018) and our own, based on dual cross-entropy data-filtering (Junczys-Dowmunt, 2018b,a). For the current WMT19 shared task for 225 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 225–233 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics English-German, evaluation is carried out on a test set where the source side consists of original English content only, the target side is a translation. To inform our system choices, we create a similar dev set out of test2016, test2017 and test2018 by splitting the test sets by original language and concatenating the respective splits, each abou"
W19-5321,W18-6415,1,0.861549,"r models. We first explore strong sentence-level systems, trained on large-scale data created via data-filtering and noisy back-translation and investigate the interaction of both with the translation direction Sentence-Level Baselines Before moving on to building our document-level systems, we first start with a baseline sentencelevel system. We try to combine the strengths of last year’s two dominating systems for the English-German news translation task – FAIR’s submission with large-scale noisy back-translation (Edunov et al., 2018) and our own, based on dual cross-entropy data-filtering (Junczys-Dowmunt, 2018b,a). For the current WMT19 shared task for 225 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 225–233 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics English-German, evaluation is carried out on a test set where the source side consists of original English content only, the target side is a translation. To inform our system choices, we create a similar dev set out of test2016, test2017 and test2018 by splitting the test sets by original language and concatenating the respective splits, each abou"
W19-5321,W18-6467,1,0.782414,"Missing"
W19-5321,P18-4020,1,0.878861,"Missing"
W19-5321,D18-2012,0,0.045166,"and initialize residual layers with Glorot uniform weights (Glorot and Bengio, 2010) multiplied by √ 1/ i where i is the number of the i-th layer √ from the bottom. Radford et al. (2019) used 1/ d where 1 BLEU+case.mixed+lang.en-de+numrefs.1 +smooth.exp+test.wmt18+tok.13a +version.1.3.0 d is the total depth of the transformer stack. We found that their method helped with perplexity, but hurt BLEU. We did not see detrimental effects for our progressive multiplier. Omitting the multiplier led to problems with convergence for deep models. We use the same SentencePiece vocabulary for all models (Kudo and Richardson, 2018). For the purpose of the task, we extended the Marian toolkit with fp16 training, BERT-models (Devlin et al., 2018) and multi-task training. Similar to Edunov et al. (2018) we use mixed-precision training with fp16, an optimizer delay of 16 before updating the gradients. We train on 8 Voltas with 16GB each. Training of one model takes between 2 and 4 days on a single machine. In terms of words per second we reach about 180K target words per second for 6-layer sentence-level systems and 120K target labels for 6-layer document-level systems with long sequences. 2.2 Data-Filtering Table 1 summari"
W19-5321,W18-6319,0,0.03018,"t 1-2, 2019. 2019 Association for Computational Linguistics English-German, evaluation is carried out on a test set where the source side consists of original English content only, the target side is a translation. To inform our system choices, we create a similar dev set out of test2016, test2017 and test2018 by splitting the test sets by original language and concatenating the respective splits, each about 4500 sentences. We report results on both splits of our new dev set as well as on the joint dev set. We further report results on the original test sets for comparison. We use SacreBLEU1 (Post, 2018) for all reported scores. It is currently not quite clear to us how to interpret results on the split test sets. One would assume that improvements on the original source language indicate actual translation quality improvements, but here we might be suffering from reference bias towards non-native target content. This might indicate higher adequacy but effectively penalize more fluent output. Conversely, higher results on the split with original target language might indicate higher fluency, but the reduced complexity of the non-native source language might make the translation task easier an"
W19-5321,W17-4811,0,0.0799164,"acceptable losses on the originally-English side. We empirically choose a weight of 0.3 for type (a) models, using a weight of 1 for type (c) models. In hindsight, an ensemble of 8 models of type (c) might have been the better choice, however, we did not train that many models of type (c). Our final sentence-level model is the 0.3 · (4×a) + 1.0 · (4×c) ensemble; we submit this model as our pure sentence-level model. 3 Document-Level Systems Our work is inspired rather by recent results on long-sequence language modelling than by previous document-level machine translation approaches. However, Tiedemann and Scherrer (2017) needs to be emphasized as an important precursor to this paper. They explore the influence of a limited number of context sentences by simply concatenating up to two sentences in source or target. We drop the limits and consume full documents if their total length stays below 1000 subword units. These sequences can easily consist of 20 or more sentences. Recent work by Devlin et al. (2018) and Radford et al. (2019) have shown significant impact by training deeper models on large data sets with long-sequence context. In terms of architecture, the language modeling work relies on standard trans"
W19-5321,N19-1423,0,\N,Missing
W19-5321,W19-5301,0,\N,Missing
