2020.lrec-1.613,baccianella-etal-2010-sentiwordnet,0,0.0139889,"2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity (Cer et al., 2018). It was observed that cosine similarity values were very close to each other, making it difficult to clearly separate domains. Since Angular Similarity dis"
2020.lrec-1.613,W06-1615,0,0.226339,"ch is based on the hypothesis that if source and target domains are similar, their CDSA accuracy should also be higher given all other conditions (such as data size) are the same. The rest of the paper is organized as follows. We describe related work in Section 2. We then introduce our sentiment classifier in Section 3. and the similarity metrics in Section 4. The results are presented in Section 5. followed by a discussion in Section 6. Finally, we conclude the paper in Section 7. 2. Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domai"
2020.lrec-1.613,Q17-1010,0,0.0320651,"ing D1 in D2 and ∆E indicates percentage change in entropy before and after mixing of source and target domains. Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is"
2020.lrec-1.613,D18-2029,0,0.0254947,"three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity (Cer et al., 2018). It was observed that cos"
2020.lrec-1.613,N19-1149,0,0.0182451,"int sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Recent work by Dai et al. (2019) uses similarity metrics to select the domain from which pre-trained embeddings should be obtained for named entity recognition. Similarly, Schultz et al. (2018) present a method for source domain selection as a weighted sum of similarity metrics. They use statistical classifiers such as logistic regression and support vector machines. However, the similarity measures used are computationally intensive. To the best of our knowledge, this is the first work at this scale that compares different costeffective similarity metrics with the performance of CDSA. 3. 1. Labelled Data: Here, each review"
2020.lrec-1.613,P07-1033,0,0.0418056,"Missing"
2020.lrec-1.613,L16-1041,0,0.0167249,"all our metrics in detail later in this section. These 11 metrics can also be classified into two categories: • Symmetric Metrics - The metrics which consider domain-pairs (D1 , D2 ) and (D2 , D1 ) as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings. Sentiment Classifier The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset (Dragoni et al., 2016), which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1. To ensure that the datasets are balanced across all domains, we randomly select 5000 positive and 5000 negative reviews from each domain. The length of the reviews ranges from 5 words to 1654 words across all domains, with an average length ranging from 71 words to 125 words per domain. We point the reader to the original paper for detailed dataset statistics. We normalize the dataset by removing numerical values, punctuations, stop words, and c"
2020.lrec-1.613,P11-1013,0,0.0355759,"Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Re"
2020.lrec-1.613,P12-1043,0,0.0274583,"aume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Recent work by Dai et al. (2019) uses similarity metrics to select the domain from which pre-trained embeddings should be obtained for named entity recognition. Similarly, Schultz et al. (2018) present a metho"
2020.lrec-1.613,J81-4005,0,0.668614,"Missing"
2020.lrec-1.613,P18-2064,1,0.804252,"just need to search for them in the target domain to find out common significant words. LM2: Symmetric KL-Divergence (SKLD) KL Divergence can be used to compare the probabilistic distribution of polar words in two domains (Kullback and Leibler, 1951). A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains 4984 is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by Murthy et al. (2018) and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as ‘polar’ for a domain if, |P − N |>= 0.5 (2) where P is the probability of a word appearing in a review which is labelled positive and N is the probability of a word appearing in a review which is labelled negative. SKLD of a polar word for domain-pair (D1 , D2 ) is calculated as: ! ! P1 N1 + P1 ∗ log (3) A = N1 ∗ log N2 P2 N2 B = N2 ∗ log N1 ! P2 + P2 ∗ log P1 (4) A+B (5) 2 where Pi and Ni are probabilities of a word appearing under positively labelled and negatively labelle"
2020.lrec-1.613,D14-1162,0,0.0821559,"Missing"
2020.lrec-1.613,N18-1202,0,0.00906568,"tage change in entropy before and after mixing of source and target domains. Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain"
2020.lrec-1.613,I13-1076,1,0.806812,"main-pairs is a reason for poor performance. To mitigate this, we compute a confidence term for a domain-pair (D1 , D2 ) using the Jaccard Similarity Coefficient which is calculated as follows: C W1 + W2 − C |P1 − P2 |+ |N1 − N2 | (8) The overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs. (L1 Distance)avg + 1 J (9) Domain pairs are ranked in increasing order of final value. ! SKLD = J= which change their polarity across domains (Sharma and Bhattacharyya, 2013). The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc. For every common polar word between two domains, L1 Distance between two vectors [P1 , N1 ] and [P2 , N2 ] is calculated as; LM4: Entropy Change Entropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution"
2020.lrec-1.613,P18-1089,1,0.855125,"for each source domain can be highly intensive both in terms of time and resources. This makes it important to devise easy-to-compute metrics that use labelled data in the source and target domains. When target domain data is labelled, we use the following four metrics for comparing and ranking source domains for a particular target domain: LM1: Significant Words Overlap All words in a domain are not significant for sentiment expression. For example, comfortable is significant in the ‘Clothing’ domain but not as significant in the ‘Movie’ domain. In this metric, we build upon existing work by Sharma et al. (2018) and extract significant words from 4983 D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12 D13 D14 D15 D16 D17 D18 D19 D20 D1 84.84 76.34 74.47 75.66 81.14 74.19 75.57 73.83 75.39 82.75 77.11 78.31 76.00 74.28 71.91 72.15 77.14 77.15 78.83 79.08 D2 70.15 83.24 75.00 74.32 69.81 73.87 77.93 73.29 72.90 72.69 65.46 79.11 79.46 77.31 72.34 75.18 77.22 80.04 71.26 70.15 D3 72.58 77.71 85.78 80.31 70.86 71.99 75.67 78.39 78.70 73.83 66.81 78.49 77.00 80.29 71.26 76.59 77.27 76.21 75.33 71.98 D4 73.94 77.83 80.16 84.49 73.09 76.37 75.08 79.82 76.93 73.59 72.53 78.69 78.42 78.66 75.29 75.44 77.06 79.09 76.18 73"
2020.lrec-1.613,P09-1043,0,0.0453219,"e and target domains are similar, their CDSA accuracy should also be higher given all other conditions (such as data size) are the same. The rest of the paper is organized as follows. We describe related work in Section 2. We then introduce our sentiment classifier in Section 3. and the similarity metrics in Section 4. The results are presented in Section 5. followed by a discussion in Section 6. Finally, we conclude the paper in Section 7. 2. Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been ex"
ar-etal-2012-cost,C10-1063,1,\N,Missing
ar-etal-2012-cost,D11-1100,1,\N,Missing
ar-etal-2012-cost,P96-1042,0,\N,Missing
ar-etal-2012-cost,P02-1053,0,\N,Missing
ar-etal-2012-cost,W02-1011,0,\N,Missing
ar-etal-2012-cost,W10-1808,0,\N,Missing
C12-2008,D08-1014,0,0.0167976,"synset-based features perform better than word-based features for sentiment analysis. Here, we carry out our study on two widely spoken Indian languages: Hindi and Marathi. These languages belong to the Indo-Aryan subgroup of the Indo-European language family. For these two languages, we first verify the superiority of sense-based features over word-based features for SA. Thereafter we proceed to verify the efficacy of the sense-based approach for cross-lingual sentiment analysis for these two languages. This work differs from existing works(Brooke et al., 2009; Wan, 2009; Wei and Pal, 2010; Banea et al., 2008) on CLSA in two aspects: (i) our focus is not necessarily to use a resource-rich language to help a resource-scarce language but can be applied to any two languages which share a common sense space (by using WordNets with matching synset identifiers); (ii) our work is an alternative to MT-based cross-lingual sentiment analysis for languages which do not have an MT system between them. 2 Background Study: Word Senses for SA In our previous work (Balamurali et al., 2011), we showed that word senses act as better features than lexeme-based features for document level SA. We termed this feature sp"
C12-2008,bhattacharyya-2010-indowordnet,1,0.681629,"used to predict the sentiment of documents in another language (call it L t est ). Machine Translation is often employed for CLSA (Wan, 2009; Wei and Pal, 2010). A document in L t est is translated into L t r ain and is checked for polarity using the classifier trained on the polarity marked documents of L t r ain . However, MT is resource-intensive and does not exist for most pairs of languages. WordNet (Fellbaum, 1998) is a widely used lexical resource in the NLP community and is present in many languages.1 Most of the WordNets are developed using the expansion based approach (Vossen, 1998; Bhattacharyya, 2010) wherein a new WordNet for a target language (L t ) is created by adding words which represent the corresponding synsets in the source language (Ls ) WordNet. As a consequence, corresponding concepts in Ls and L t have the same synset (concept) identifier. Our work leverages this fact, and uses WordNet senses as features for building a classifier in L t r ain . The document to be tested for polarity is preprocessed by replacing words in this document with the corresponding synset identifiers. This step eliminates the distinction between L t r ain and L t est as far as the document is concerned"
C12-2008,R09-1010,0,0.168504,"lamurali et al., 2011) where we showed that WordNet synset-based features perform better than word-based features for sentiment analysis. Here, we carry out our study on two widely spoken Indian languages: Hindi and Marathi. These languages belong to the Indo-Aryan subgroup of the Indo-European language family. For these two languages, we first verify the superiority of sense-based features over word-based features for SA. Thereafter we proceed to verify the efficacy of the sense-based approach for cross-lingual sentiment analysis for these two languages. This work differs from existing works(Brooke et al., 2009; Wan, 2009; Wei and Pal, 2010; Banea et al., 2008) on CLSA in two aspects: (i) our focus is not necessarily to use a resource-rich language to help a resource-scarce language but can be applied to any two languages which share a common sense space (by using WordNets with matching synset identifiers); (ii) our work is an alternative to MT-based cross-lingual sentiment analysis for languages which do not have an MT system between them. 2 Background Study: Word Senses for SA In our previous work (Balamurali et al., 2011), we showed that word senses act as better features than lexeme-based featur"
C12-2008,N10-1120,0,0.0497777,"Missing"
C12-2008,W02-1011,0,0.0159694,"Missing"
C12-2008,schulz-etal-2010-multilingual,0,0.0970841,"Missing"
C12-2008,P09-1027,0,0.839269,"an opinion in a text. Though the majority of the work in SA is for English, there has been work in other languages as well such as Chinese, Japanese, German and Spanish (Seki et al., 2007; Nakagawa et al., 2010; Schulz et al., 2010). To perform SA on these languages, cross-lingual approaches are often used due to the lack of annotated content in these languages. In Cross-Lingual Sentiment Analysis (CLSA), the training corpus in one language (call it L t r ain ) is used to predict the sentiment of documents in another language (call it L t est ). Machine Translation is often employed for CLSA (Wan, 2009; Wei and Pal, 2010). A document in L t est is translated into L t r ain and is checked for polarity using the classifier trained on the polarity marked documents of L t r ain . However, MT is resource-intensive and does not exist for most pairs of languages. WordNet (Fellbaum, 1998) is a widely used lexical resource in the NLP community and is present in many languages.1 Most of the WordNets are developed using the expansion based approach (Vossen, 1998; Bhattacharyya, 2010) wherein a new WordNet for a target language (L t ) is created by adding words which represent the corresponding synsets"
C12-2008,P10-2048,0,0.800561,"in a text. Though the majority of the work in SA is for English, there has been work in other languages as well such as Chinese, Japanese, German and Spanish (Seki et al., 2007; Nakagawa et al., 2010; Schulz et al., 2010). To perform SA on these languages, cross-lingual approaches are often used due to the lack of annotated content in these languages. In Cross-Lingual Sentiment Analysis (CLSA), the training corpus in one language (call it L t r ain ) is used to predict the sentiment of documents in another language (call it L t est ). Machine Translation is often employed for CLSA (Wan, 2009; Wei and Pal, 2010). A document in L t est is translated into L t r ain and is checked for polarity using the classifier trained on the polarity marked documents of L t r ain . However, MT is resource-intensive and does not exist for most pairs of languages. WordNet (Fellbaum, 1998) is a widely used lexical resource in the NLP community and is present in many languages.1 Most of the WordNets are developed using the expansion based approach (Vossen, 1998; Bhattacharyya, 2010) wherein a new WordNet for a target language (L t ) is created by adding words which represent the corresponding synsets in the source langu"
C12-2008,D11-1100,1,\N,Missing
C16-1234,bakliwal-etal-2012-hindi,1,0.717687,"ion (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan,"
C16-1234,W14-3902,0,0.259828,"nglish speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Reco"
C16-1234,P07-1056,0,0.0156888,"tive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan, December 11-17 2016. Sentence variations Trailer dhannnsu hai bhai Dhannnsu trailer hai bhai Bhai trailer dhannnsu hai Bhai dhannnsu"
C16-1234,W14-3908,0,0.026059,"g interest owing to the rising amount of non-English speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tas"
C16-1234,W10-3208,0,0.0476344,"al. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan, December 11-17 2016. Sentence variations Trailer dhannnsu hai bhai Dhannnsu trailer hai bhai Bhai trailer dhannnsu hai Bhai dhannnsu trailer hai Table 1: Illustra"
C16-1234,W15-3904,0,0.0635959,"Missing"
C16-1234,esuli-sebastiani-2006-sentiwordnet,0,0.0460907,"ng data into 80-20 split to get the final training, validation and testing data. As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset. As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data. The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence. The architecture of the proposed system (Subword-LSTM) is described in Figure 2. We compare it with a character-level LSTM (Char-LSTM) following the same architecture without the convolutional and maxpooling layers. We use Adamax (Kingma and Ba, 2014) (a variant of Adam based on infinity norm) optimizer to train this setup in an end-to-end fashion using batch size of 128. We use very simplistic architectures because of the constraint on the size of"
C16-1234,N13-1090,0,0.0191484,"esome, brother. Sentiment Polarity Positive Negative Positive Table 4: Examples of Hi-En Code Mixed Comments from the dataset. Our dataset and code is freely available for download 2 to encourage further exploration in this domain. 2 https://github.com/DrImpossible/Sub-word-LSTM 2484 3 Learning Compositionality Our target is to perform sentiment analysis on the above presented dataset. Most commonly used statistical approaches learn word-level feature representations. We start our exploration for suitable algorithms from models having word-based representations. 3.1 Word-level models Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al., 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al., 2014). They are theoretically sound since language consists of inherently arbitrary mappings between ideas and words. Eg: The words person(English) and insaan(Hindi) do not share any priors in their construction and neither do their constructions have any relationship with the semantic concept of a person. Hence, popular approaches consider lexical units to"
C16-1234,D13-1170,0,0.0727234,"kers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimar˜aes, 2015). LSTMs have been observed to outperform baselines for language modelling (Kim et al., 2015) and classification (Zhou et al., 2015). In a recent work, (Bojanowski et al., 2016) proposed a skip-gram based"
C16-1234,W14-3907,0,0.0805565,"ted final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimar˜aes, 2015). LSTMs have been observed to outperform baselines"
C16-1234,J11-2001,0,0.0210151,"t − C + B = Bat” lacks any linguistic basis. But, groups of characters may serve semantic functions. This is illustrated by U n + Holy = U nholy or Cat + s = Cats which is semantically interpretable by a human. Since sub-word level representations can generate meaningful lexical representations and individually carry semantic weight, we believe that sub-word level representations consisting composition of characters might allow generation of new lexical structures and serve as better linguistic units than characters. 3.3 Sub-word level representations Lexicon based approaches for the SA task (Taboada et al., 2011; Sharma et al., 2015) perform a dictionary look up to obtain an individual score for words in a given sentence and combine these scores to get the sentiment polarity of a sentence. We however want to use intermediate sub-word feature representations learned by the filters during convolution operation. Unlike traditional approaches that add sentiment scores of individual words, we propagate relevant information with LSTM and compute final sentiment of the sentence as illustrated in Figure 1. Hypothesis: We propose that incorporating sub-word level representations into the design of our models"
C16-1234,W13-3512,0,0.0154115,"Table 4: Examples of Hi-En Code Mixed Comments from the dataset. Our dataset and code is freely available for download 2 to encourage further exploration in this domain. 2 https://github.com/DrImpossible/Sub-word-LSTM 2484 3 Learning Compositionality Our target is to perform sentiment analysis on the above presented dataset. Most commonly used statistical approaches learn word-level feature representations. We start our exploration for suitable algorithms from models having word-based representations. 3.1 Word-level models Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al., 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al., 2014). They are theoretically sound since language consists of inherently arbitrary mappings between ideas and words. Eg: The words person(English) and insaan(Hindi) do not share any priors in their construction and neither do their constructions have any relationship with the semantic concept of a person. Hence, popular approaches consider lexical units to be independent entities. However, operating on the lexical"
C16-1234,D14-1105,0,0.0461119,"ing amount of non-English speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging"
C16-1234,P12-2018,0,0.0555154,"ches: Hashtags, User Mentions, Emoticons etc. may not exist in the data. 2486 Figure 1: Illustration of the proposed methodology Figure 3: Training accuracy and loss variation. Figure 2: Schematic overview of the architecture. 4.2 Experimental Setup Our dataset is divided into 3 splits- Training, validation and testing. We first divide the data into randomized 80-20 train test split, then further randomly divide the training data into 80-20 split to get the final training, validation and testing data. As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset. As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data. The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence. The architecture of th"
D11-1100,D09-1020,0,0.229494,"benefit of a word sense-based feature space to supervised sentiment classification. However, a word sense-based feature space is feasible subject to verification of the hypothesis that sentiment and word senses are related. Towards this, Wiebe and Mihalcea (2006) conduct a study on hu1082 man annotation of 354 words senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnecessary as used in Martn-Wanton et al. (2010). Rentoumi et al. (2009) suggest using word senses to detect sentence level polarity of news headlines. The authors use graph similarity to detect polarity of senses. To predict sentence level polarity, a HMM is trained on word sense and POS as the observation. The authors report that word senses particularl"
D11-1100,P04-1035,0,0.0423852,"ose are limited to affective classes. This restricts the size of the feature space to a limited set of labels. As opposed to this, we construct feature vectors that map to a larger sense-based space. In order to do so, we use synset offsets as representation of sense-based features. Akkaya et al. (2009), Martn-Wanton et al. (2010) and Carrillo de Albornoz et al. (2010) perform sentiment classification of individual sentences. However, we consider a document as a unit of sentiment classification i.e. our goal is to predict a document on the whole as positive or negative. This is different from Pang and Lee (2004) which suggests that sentiment is associated only with subjective content. A document in its entirety is represented using sensebased features in our experiments. Carrillo de Albornoz et al. (2010) suggests expansion using WordNet relations which we also follow. This is a benefit that can be achieved only in a sense-based space. 3 Features based on WordNet Senses In their original form, documents are said to be in lexical space since they consist of words. When the words are replaced by their corresponding senses, the resultant document is said to be in semantic space. WordNet 2.1 (Fellbaum, 1"
D11-1100,W02-1011,0,0.0298014,"address the problem of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline. 1 Introduction Sentiment Analysis (SA) is the task of prediction of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary 1081 classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semant"
D11-1100,N04-3012,0,0.0435663,"Missing"
D11-1100,R09-1067,0,0.0838769,"ords senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnecessary as used in Martn-Wanton et al. (2010). Rentoumi et al. (2009) suggest using word senses to detect sentence level polarity of news headlines. The authors use graph similarity to detect polarity of senses. To predict sentence level polarity, a HMM is trained on word sense and POS as the observation. The authors report that word senses particularly help understanding metaphors in these sentences. Our work differs in terms of the corpus and document sizes in addition to generating a general purpose classifier. Another supervised approach of creating an emotional intensity classifier using concepts as features has been reported by Carrillo de Albornoz et al."
D11-1100,P02-1053,0,0.0127258,"of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline. 1 Introduction Sentiment Analysis (SA) is the task of prediction of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary 1081 classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervi"
D11-1100,P06-1134,0,0.0426931,"the similaritybased replacement technique using WordNet synsets in section 4. Our experiments have been described in section 5. In section 6, we present our results and related discussions. Section 7 analyzes some of the causes for erroneous classification. Finally, section 8 concludes the paper and points to future work. 2 Related Work This work studies the benefit of a word sense-based feature space to supervised sentiment classification. However, a word sense-based feature space is feasible subject to verification of the hypothesis that sentiment and word senses are related. Towards this, Wiebe and Mihalcea (2006) conduct a study on hu1082 man annotation of 354 words senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources u"
D16-1104,W14-2608,0,0.0423397,"): We first compute similarity scores for all pairs of words (except stop words). We then return four feature values per sentence.3 : 2. Gonz´alez-Ib´anez et al. (2011a): They propose two sets of features: unigrams and dictionary-based. The latter are words from a lexical resource called LIWC. We use words from LIWC that have been annotated as emotion and psychological process words, as described in the original paper. • • • • Maximum score of most similar word pair Minimum score of most similar word pair Maximum score of most dissimilar word pair Minimum score of most dissimilar word pair 3. Buschmeier et al. (2014): In addition to unigrams, they propose features such as: (a) Hyperbole (captured by three positive or negative words in a row), (b) Quotation marks and ellipsis, (c) Positive/Negative Sentiment words followed by an exclamation mark or question mark, (d) Positive/Negative Sentiment Scores followed by ellipsis (represented by a ‘...’), (e) Punctuation, (f) Interjections, and (g) Laughter expressions. For example, in case of the first feature, we consider the most similar word to every word in the sentence, and the corresponding similarity scores. These most similar word scores for each word are"
D16-1104,D15-1116,0,0.214199,"Missing"
D16-1104,P11-2102,0,0.245391,"Missing"
D16-1104,P15-2124,1,0.893523,"ight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection. 1 Can word embedding-based features when augmented to features reported in prior work improve the performance of sarcasm detection? Introduction Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Linguistic studies show that the notion of context incongruity is at the heart of sarcasm (Ivanko and Pexman, 2003). A popular trend in automatic sarcasm detection is semi-supervised extraction of patterns that capture the underlying context incongruity (Davidov et al., 2010; Joshi et al., 2015; Riloff et al., 2013). However, techniques to extract these patterns rely on sentiment-bearing words and may not capture nuanced forms of sarcasm. Consider the sentence ‘With a sense of humor like that, you could make a living as a garbage man anywhere in the country.1 ’ The speaker makes a subtle, contemptuous remark about the 1 All examples in this paper are actual instances from our dataset. To the best of our knowledge, this is the first attempt that uses word embedding-based features to detect sarcasm. In this respect, the paper makes a simple increment to state-of-the-art but opens up a"
D16-1104,W15-2905,1,0.883789,"Missing"
D16-1104,W07-0101,0,0.125028,"and ‘apple’ with similarity score of 0.1414. The sarcasm in this sentence can be understood only in context of the complete conversation that it is a part of. 3. Metaphors in non-sarcastic text: Figurative language may compare concepts that are not directly related but still have low similarity. Consider the nonsarcastic quote ‘Oh my love, I like to vanish in you like a ripple vanishes in an ocean - slowly, silently and endlessly’. Our system incorrectly predicts this as sarcastic. 8 Related Work Early sarcasm detection research focused on speech (Tepperman et al., 2006) and lexical features (Kreuz and Caucci, 2007). Several other features have been proposed 1010 Conclusion This paper shows the benefit of features based on word embedding for sarcasm detection. We experiment with four past works in sarcasm detection, where we augment our word embedding-based features to their sets of features. Our features use the similarity score values returned by word embeddings, and are of two categories: similarity-based (where we consider maximum/minimum similarity score of most similar/dissimilar word pair respectively), and weighted similarity-based (where we weight the maximum/minimum similarity scores of most si"
D16-1104,P14-2050,0,0.0405182,"Features given in paper X + S+WS (i.e., weighted and unweighted similarity features) 1008 67.2 64.6 67.6 67 78.8 75.2 51.2 52.8 72.53 69.49 58.26 59.05 using embeddings from Word2Vec We experiment with four types of word embeddings: 1. LSA: This approach was reported in Landauer and Dumais (1997). We use pre-trained word embeddings based on LSA5 . The vocabulary size is 100,000. 2. GloVe: We use pre-trained vectors avaiable from the GloVe project6 . The vocabulary size in this case is 2,195,904. 3. Dependency Weights: We use pre-trained vectors7 weighted using dependency distance, as given in Levy and Goldberg (2014). The vocabulary size is 174,015. Experiment Setup 1. Features given in paper X F Table 2: Performance of unigrams versus our similarity-based features These are computed similar to unweighted similarity features. We create a dataset consisting of quotes on GoodReads 4 . GoodReads describes itself as ‘the world’s largest site for readers and book recommendations.’ The website also allows users to post quotes from books. These quotes are snippets from books labeled by the user with tags of their choice. We download quotes with the tag ‘sarcastic’ as sarcastic quotes, and the ones with ‘philosop"
D16-1104,W13-1605,0,0.0630179,"Missing"
D16-1104,D13-1066,0,0.438285,"Missing"
D16-1104,P14-2084,0,0.0583793,"Missing"
D16-1104,P15-1100,0,0.0518009,"Missing"
I13-2006,D07-1091,0,\N,Missing
I13-2006,P07-2045,0,\N,Missing
I13-2006,N03-1017,0,\N,Missing
K16-1015,P10-2050,0,0.00902686,"tional random fields. Zhang et al. (2014) deal with emotion classification. Using a dataset of children’s stories manually annotated at the sentence level, they employ HMM to identify sequential structure and a classifier to predict emotion in a particular sentence. Mao and Lebanon (2006) present a isotonic CRF that predicts global and local sentiment of documents, with additional mechanism for author-specific distributions and smoothing sentiment curves. Yessenalina et al. (2010) present a joint learning algorithm for sentencelevel subjectivity labeling and document-level sentiment labeling. Choi and Cardie (2010) deal with sequence learning to jointly identify scope of opinion polarity expressions, and polarity labels. Taking inspiration from use of sequence labeling for sarcasm detection, our work takes the first step to show if sequence labeling techniques are helpful at all. They experiment with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of di"
K16-1015,W10-2914,0,0.127959,"ent with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of different types of sarcasm that were correctly labeled by sequence labeling but incorrectly labeled by classification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a reddit14 comment. (Rajadesingan et al., 20"
K16-1015,P11-2102,0,0.111854,"Missing"
K16-1015,W15-2905,1,0.153096,"assification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a reddit14 comment. (Rajadesingan et al., 2015) present a suite of classifier features that capture different kinds of context: context related to the author, conversation, etc. The new trend in sarcasm detection is, thus, to look at additional context beyond the text where sarcasm is to be predicted. The work closest to ours is by Wang et al. (2015). They use a labeled dataset of 1500 tweets, the labels for which are obtained automatically. Due to their autom"
K16-1015,W14-2608,0,0.0951159,"from Prior Work We also compare our results with features presented in two prior works9 : 1. Features given in Gonz´alez-Ib´anez et al. (2011): These features are: (a) Interjections, (b) Punctuations, (c) Pragmatic features (where we include action words as well), (d) Sentiment lexicon-based features from LIWC (Pennebaker et al., 2001) (where they include counts of linguistic process words, positive/negative emotion words, etc.). Thus, we wish to validate our hypothesis in case of: 1. Our data-derived features as given in Section 4.1. 2. Past features from Gonz´alez-Ib´anez et al. (2011) and Buschmeier et al. (2014) as given in Section 4.2. 2. Features given in Buschmeier et al. (2014): In addition to unigrams, the features used by them are: (a) Hyperbole (captured by three positive or negative words in a row), (b) Quotation marks and ellipsis, (c) Positive/Negative Sentiment Scores followed by punctuation (this includes more than one positive or negative words with an exclamation mark or question mark at the end), (d) Positive/Negative Sentiment Scores followed by ellipsis (this includes more than one positive or negative words with a ‘...’ at the end, (e) Punctuation, (f) Interjections, and (g) Laughte"
K16-1015,W07-0101,0,0.161201,"techniques are helpful at all. They experiment with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of different types of sarcasm that were correctly labeled by sequence labeling but incorrectly labeled by classification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a r"
K16-1015,L16-1147,0,0.0280848,"Missing"
K16-1015,D10-1102,0,0.0133469,"roaches for sequence labeling in sentiment classification have been studied. Zhao et al. (2008) perform sentiment classification using conditional random fields. Zhang et al. (2014) deal with emotion classification. Using a dataset of children’s stories manually annotated at the sentence level, they employ HMM to identify sequential structure and a classifier to predict emotion in a particular sentence. Mao and Lebanon (2006) present a isotonic CRF that predicts global and local sentiment of documents, with additional mechanism for author-specific distributions and smoothing sentiment curves. Yessenalina et al. (2010) present a joint learning algorithm for sentencelevel subjectivity labeling and document-level sentiment labeling. Choi and Cardie (2010) deal with sequence learning to jointly identify scope of opinion polarity expressions, and polarity labels. Taking inspiration from use of sequence labeling for sarcasm detection, our work takes the first step to show if sequence labeling techniques are helpful at all. They experiment with MPQA corpus that is labeled at the sentence level for polarity as well as intensity. Specialized sequence labeling techniques like these are the next step to our first ste"
K16-1015,D08-1013,0,0.0430497,"Missing"
K16-1015,P14-2084,0,0.111454,"the next step to our first step: showing if sequence labeling techniques are helpful at all, for sarcasm detection of dialogue. Table 9: Proportion of utterances of different types of sarcasm that were correctly labeled by sequence labeling but incorrectly labeled by classification techniques 8 Related Work Sarcasm detection approaches using different features have been reported (Tepperman et al., 2006; Kreuz and Caucci, 2007; Tsur et al., 2010; Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Joshi et al., 2015; Buschmeier et al., 2014). However, Wallace et al. (2014) show how context beyond the target text (i.e., extra-textual context) is necessary for humans as well as machines, in order to identify sarcasm. Following this, the new trend in sarcasm detection is to explore the use of such extratextual context (Khattri et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015; Wallace, 2015). (Wallace, 2015) uses meta-data about reddits to predict sarcasm in a reddit14 comment. (Rajadesingan et al., 2015) present a suite of classifier features that capture different kinds of context: context related to the author, conversation, etc. The new trend in"
K16-1015,P15-1100,0,0.0664233,"be verbal: her date says, ‘They’ve taken 40 minutes to bring our appetizers’ to which the speaker responds ‘I absolutely love this restaurant’. Both these examples point to the intuition that for dialogue (i.e., data where more than one speaker participates in a discourse), conversational context is often a clue for sarcasm. For such dialogue, prior work in sarcasm detection (determining whether a text is sarcastic or not) captures context in the form of classifier features such as the topic’s probability of evoking sarcasm, or the author’s tendency to use sarcasm (Rajadesingan et al., 2015; Wallace, 2015). In this paper, we present an alternative hypothesis: sarcasm detection of dialogue is better formulated as a sequence labeling task, instead of classification task. The central message of our work is the efficacy of using sequence labeling as a learning mechanism for sarcasm detection in dialogue, and not in the set of features that we propose for sarcasm detection although we experiment with three feature sets. For our experiments, we create a manually labeled dataset of dialogues from TV series ‘Friends’. Each dialogue is considered to be a sequence of utterances, and every utterance is an"
K16-1015,P15-2124,1,\N,Missing
L16-1349,P98-1004,0,0.030152,"chine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignmen"
L16-1349,P03-1012,0,0.038474,"t multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in l"
L16-1349,P08-1112,0,0.0233195,"oarse lexical resource using parallel topics obtained from multilingual topic models. We observe that for a machine translation system for English-Hindi, these coarse alignments do fine! In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A statistical machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover ali"
L16-1349,C04-1005,0,0.0538573,"ments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation lexical resource can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Sánchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the wo"
L16-1349,P10-1155,1,0.6771,"nlike so many one to one Cartesian product alignments, our approach keeps them in the same sentence, thus reducing the chances of the system learning non synonymous candidate translations. 2200 Figure 3: Parallel English-Hindi topics as generated by the topic model for the health dataset Thus, for T topics, and K top words, sentential approach results in a coarse lexical resource of T X K pseudo-parallel sentences. The coarse lexical resource for varying values of T is available freely for download. 3.1. Experiment Setup To generate the topics, we use corpora from health and tourism domain by Khapra et al. (2010). These datasets contain approximately 25000 parallel sentences for English - Hindi language pair. We implement the multilingual topic model in Java. Our implementation uses Gibbs sampling as described in the original paper. 3.3. Quantitative Evaluation Two human annotators evaluated the quality of the output obtained. Each word was marked as whether or not a translation in the other language was present in the same topic. The two annotators, A1 and A2, are native speakers of Hindi, and have had 15+ years of academic instruction in English. The inter-annotator agreement between them and their"
L16-1349,P07-2045,0,0.033916,"cal machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in noncomparable corpora. They then use MLTM to create bilingual lexicons for low resource language pairs, and provided candidate translations for more computationFigure 1: Our Sentent"
L16-1349,J00-2004,0,0.0962513,"the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation lexical resource can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data"
L16-1349,D09-1092,0,0.536026,"tem is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in noncomparable corpora. They then use MLTM to create bilingual lexicons for low resource language pairs, and provided candidate translations for more computationFigure 1: Our Sentential Approach to create pseudoparallel data ally intense alignment processes without the sentencealigned translations"
L16-1349,P00-1056,0,0.447221,"op terms for a text classification task. They observe that parallel topics perform better than topic words that are translated into the target language. Approaches that do not rely on parallel corpus have also been reported. Jagarlamudi and Daumé III (2010) use a bilingual lexical resource, and a comparable corpora to estimate a model called JointLDA. Boyd-Graber and Blei (2009) use unaligned corpus and extract multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Bar"
L16-1349,J03-1002,0,0.00587388,"approach to use a coarse lexical resource using parallel topics obtained from multilingual topic models. We observe that for a machine translation system for English-Hindi, these coarse alignments do fine! In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A statistical machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is p"
L16-1349,tufis-barbu-2002-lexical,0,0.0758505,"nd Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy,"
L16-1349,J97-3002,0,0.0670303,"ment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word se"
L16-1349,C98-1004,0,\N,Missing
L18-1424,P06-4018,0,0.0340434,"with epsilon e=0.5 and RBF kernel3 . We set C=1000 for tweets and C=1500 for snippets. We report our results on four-fold cross validation for both datasets. Note that we convert individual sentences into words. Therefore, the dataset in the case of book snippets has 6377 instances, while the one of tweets has 6610 instances. The four folds for cross-validation are created over these instances. With a word as instance, the task is binary classification: 1 indicating that the word is a sarcasm target and 0 indicating that it is not. For rules in the rule-based extractor, we use tools in NLTK (Bird, 2006), wherever necessary. We consider two baselines with which our hybrid approach is compared: 1. Baseline 1: All Objective Words: As the first baseline, we design a na¨ıve approach for our task: include all words of the sentence which are not stop words, and have neutral sentiment polarity, as the predicted sarcasm target. 2. Baseline 2: Sequence labeling has been reported for opinion target identification (Jin et al., 2009). Therefore, we use SVM-HMM (Altun et al., 2003) with default parameters as the second baseline. We report performance using two metrics: Exact Match Accuracy and Dice Score."
L18-1424,P11-2102,0,0.0396278,"Missing"
L18-1424,P15-2124,1,0.950205,"approach for sarcasm target identification. It is based on a combination of two types of extractors: one based on rules, and another consisting of a statistical classifier. Our introductory approach establishes the viability of sarcasm target identification, and will serve as a baseline for future work. Keywords: Sentiment Analysis, Computational Sarcasm, Aspect Extraction 1. Introduction Sarcasm is a form of verbal irony that is intended to express contempt or ridicule (Source: The Free Dictionary). While several approaches have been reported for sarcasm detection (Rajadesingan et al., 2015; Joshi et al., 2015; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011), no past work, to the best of our knowledge, has attempted to identify a crucial component of sarcasm: the target of ridicule (Campbell and Katz, 2012). This is important because the sentiment of the sarcastic text needs to be attributed to this target of ridicule. Towards this motivation, we introduce ‘sarcasm target identification’: the task of extracting the target of ridicule (i.e., sarcasm target) of a sarcastic text. The input is a sarcastic text while the output is either (a) a subset of words in the sentence that point to the sarcasm"
L18-1424,D16-1104,1,0.882368,"e for future work. Sarcasm target identification can benefit natural language generation and sentiment analysis systems. Being able to recognize the entity towards which the negative sentiment was intended, a natural language generation system will have more context to generate a response. Similarly, a sentiment analysis system will be able to attribute the negative sentiment in a sarcastic text towards the correct aspect of a product or the appropriate entity. 2. Related work Computational sarcasm primarily focuses on sarcasm detection: classification of a text as sarcastic or non-sarcastic. Joshi et al. (2016a) present a survey of sarcasm detection approaches. They observe three trends in sarcasm detection: semi-supervised extraction of sarcastic patterns, use of hashtag-based supervision, and use of contextual information for sarcasm detection (Tsur et al., 2010; Davidov et al., 2010; Joshi et al., 2015). However, to the best of our knowledge, no past work aims to identify phrases in a sarcastic sentence that indicate the target of ridicule in the sentence. Related to sarcasm target identification is sentiment target identification. Sentiment target identification deals with identifying the entit"
L18-1424,W15-2905,1,0.829118,"f the lists are empty, the output is returned as ‘Outside’. 2. Hybrid AND : In this configuration, the integrator predicts the set of words that occur in the output of both the two extractors as the sarcasm target. If the intersection of the lists is empty, the output is returned as ‘Outside’. accuracy because it accounts for missing words and extra words in the target. Let the two lists (predicted and actual) be X and Y. Dice score is given by (2X ∩ Y )/(X + Y ). 5.3. Rule R1 R2 R3 R4 R5 R6 R7 R8 R9 The idea of using two configurations OR and AND is based on a rule-based sarcasm detector by (Khattri et al., 2015). While AND is intuitive, the second configuration OR is necessary because our extractors individually may not capture all forms of sarcasm target. This is intuitive because our rules may not cover all forms of sarcasm targets. 5.2. Experiment Setup We use SVM Perf (Joachims, 2006) to train the classifiers, optimized for F-score with epsilon e=0.5 and RBF kernel3 . We set C=1000 for tweets and C=1500 for snippets. We report our results on four-fold cross validation for both datasets. Note that we convert individual sentences into words. Therefore, the dataset in the case of book snippets has 6"
L18-1424,P12-1036,0,0.0170892,"n order to achieve sentiment summarization. Lu et al. (2011) perform multi-aspect sentiment analysis using a topic model. 2676 Example Target Love when you don’t have two minutes to send me a quick text. Don’t you just love it when Microsoft tells you that you’re spelling your own name wrong. I love being ignored. He is as good at coding as Tiger Woods is at avoiding controversy. Oh, and I suppose the apple ate the cheese. you Microsoft being ignored He, Tiger Woods Outside Table 1: Examples of sarcasm targets Several other topic model-based approaches to aspect extraction have been reported (Mukherjee and Liu, 2012). To the best of our knowledge, ours is the first work that deals with sarcasm target identification. 3. that started off the day. We refer to such cases as the ‘Outside’ cases. 4. 4.1. Formulation Sarcasm is a well-known challenge to sentiment analysis (Pang et al., 2008). Consider the sarcastic sentence ‘My cell phone has an awesome battery that lasts 20 minutes’. This sentence mocks the battery of the cell phone. Aspectbased sentiment analysis deals with identifying sentiment expressed towards different aspects or dimensions of an entity. Therefore, aspect-based sentiment analysis needs to"
L18-1424,P05-1015,0,0.156733,"hat uses a word-level classifier for every word in the sentence, to predict if the word will constitute the sarcasm target). Since this is the first work in sarcasm target detection, no past work exists to be used as a baseline. Hence, we devise two baselines to validate the strength of our work. The first is a simple, intuitive baseline to show if our approach (which is computationally more intensive than this simple baseline) holds value. In absence of past work, using simple and obvious techniques to solve a problem have been considered as baselines in sentiment analysis (Tan et al., 2011; Pang and Lee, 2005). As the second baseline, we use a technique reported for sentiment/opinion target identification since sentiment target identification appears to be related to sarcasm target identification, on the surface. Our manually labeled datasets are available for download at: https://github.com/Pranav-Goel/ 1 This label is necessary because the sarcasm target may not be present as a word, as discussed in Section 2. Sarcasm-Target-Detection. Each unit consists of a piece of text (either book snippet or tweet) with the annotation as the sarcasm target where the sarcasm target is a subset of words in the"
L18-1424,J11-1002,0,0.046851,"hey observe three trends in sarcasm detection: semi-supervised extraction of sarcastic patterns, use of hashtag-based supervision, and use of contextual information for sarcasm detection (Tsur et al., 2010; Davidov et al., 2010; Joshi et al., 2015). However, to the best of our knowledge, no past work aims to identify phrases in a sarcastic sentence that indicate the target of ridicule in the sentence. Related to sarcasm target identification is sentiment target identification. Sentiment target identification deals with identifying the entity towards which sentiment is expressed in a sentence. Qiu et al. (2011) present an approach to extract opinion words and targets collectively from a dataset. Aspect identification for sentiment has also been studied. This deals with extracting aspects of an entity (for example, color, weight, battery in case of a cell phone). Probabilistic topic models have been commonly used for the same. Titov et al. (2008) present a probabilistic topic model that jointly estimates sentiment and aspect in order to achieve sentiment summarization. Lu et al. (2011) perform multi-aspect sentiment analysis using a topic model. 2676 Example Target Love when you don’t have two minute"
L18-1424,D13-1066,0,0.163533,"Missing"
L18-1424,P08-1036,0,0.163879,"Missing"
P11-4022,esuli-sebastiani-2006-sentiwordnet,0,0.0393282,"content. This is on the basis of predictions by each resource by weighting them according to their accuracies. These weights have been assigned to each resource based on experimental results. For each resource, the following scores are determined. negscore[r] = objscore[r] = m X i=1 m X i=1 m X 3.4 Resources Sentiment-based lexical resources annotate words/concepts with polarity. The completeness of these resources individually remains a question. To achieve greater coverage, we use four different sentiment-based lexical resources for C-Feel-It. They are described as follows. 1. SentiWordNet (Esuli and Sebastiani, 2006) assigns three scores to synsets of WordNet: positive score, negative score and objective score. When a word is looked up, the label corresponding to maximum of the three scores is returned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It Version 2 posscore[r] = We normalize these scores to get the final positive, negative and objective pertaining to search string r. These scores are represented in form of percentage. 2. Subjectivity lexicon (Wiebe et al., 2004) i"
P11-4022,P04-1035,0,0.0911454,"Missing"
P11-4022,P05-1015,0,0.0513541,"Missing"
P11-4022,J04-3002,0,0.0110993,"and Sebastiani, 2006) assigns three scores to synsets of WordNet: positive score, negative score and objective score. When a word is looked up, the label corresponding to maximum of the three scores is returned. For multiple synsets of a word, the output label returned by majority of the synsets becomes the prediction of the resource. Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It Version 2 posscore[r] = We normalize these scores to get the final positive, negative and objective pertaining to search string r. These scores are represented in form of percentage. 2. Subjectivity lexicon (Wiebe et al., 2004) is a resource that annotates words with tags like parts-ofspeech, prior polarity, magnitude of prior polarity (weak/strong), etc. The prior polarity can be positive, negative or neutral. For prediction using this resource, we use this prior polarity. 3. Inquirer (Stone et al., 1966) is a list of words marked as positive, negative and neutral. We use these labels to use Inquirer resource for our prediction. 4. Taboada (Taboada and Grieve, 2004) is a word-list that gives a count of collocations with positive and negative seed words. A word closer to a positive seed word is predicted to be posit"
P11-4022,W00-1308,0,\N,Missing
P14-2007,P13-2062,1,0.62415,"Missing"
P14-2007,D09-1019,0,0.0429007,"Missing"
P14-2007,carl-2012-translog,0,0.0130471,"values to a scale of 1-10 using min-max normalization. To understand how the formula records sentiment annotation complexity, consider the SACs of examples in section 2. The sentence “it is messy , uncouth , incomprehensible , vicious and absurd” has a SAC of 3.3. On the other hand, the SAC for the sarcastic sentence “it’s like an all-star salute to disney’s cheesy commercialism.” is 8.3. 2. While the annotator reads the sentence, a remote eye-tracker (Model: Tobii TX 300, Sampling rate: 300Hz) records the eyemovement data of the annotator. The eyetracker is linked to a Translog II software (Carl, 2012) in order to record the data. A snapshot of the software is shown in figure 1. The dots and circles represent position of eyes and fixations of the annotator respectively. 3. The experiment then continues in modules of 50 sentences at a time. This is to prevent fatigue over a period of time. Thus, each annotator participates in this experiment over a number of sittings. We ensure the quality of our dataset in different ways: (a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment. (b) To minimize noise due to head movements fur"
P14-2007,P05-1015,0,0.150353,"king device that measures the “duration of eye-fixations1 ”. Another attribute recorded by the eye-tracker that may have been used is “saccade duration2 ”. However, saccade duration is not significant for annotation of short text, as in our case. Hence, the SAC labels of our dataset are fixation durations with appropriate normalization. It may be noted that the eye-tracking device is used only to annotate training data. The actual prediction of SAC is done using linguistic features alone. 3.1 Eye-tracking Experimental Setup We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http://help.sentiment140. com/for-students. A total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected. We then obtain two kinds of annotation from five paid annotators: (a) sentiment (positive, negative and objective), (b) eye-movement as recorded 1 A long stay of the visual gaze on a single location. A rapid movement of the eyes between positions of rest on the sentence. 2 37 a fixation duration recorded for each sentenceannotator pair3 The multi-rater kappa IAA for sentiment annotation is 0.686. 3.2 Figure 1: Gaze-data recording using Transl"
P14-2007,esuli-sebastiani-2006-sentiwordnet,0,0.0691661,"Missing"
P14-2007,P13-2149,1,0.884599,"Missing"
P14-2007,C12-1055,0,0.0668226,"Missing"
P14-2007,N13-1088,1,0.300018,"Missing"
P14-2007,P11-2102,0,0.0817206,"Missing"
P14-2007,I13-1195,0,0.0644871,"Missing"
P14-2007,C96-2123,0,\N,Missing
P14-2007,C08-1031,0,\N,Missing
P14-2007,W11-1707,0,\N,Missing
P15-2100,E12-1049,0,0.0337048,"Dataset H (193 drunk, 317 sober): A separate dataset is created where drunk tweets are downloaded using drunk hashtags, as above. The set of sober tweets is collected using both the approaches above. The resultant is the held-out test set Dataset-H that contains no tweets in common with Datasets 1 and 2. The drunk tweets for Datasets 1 and 2 are the same. Figure 1 shows a word-cloud for these drunk tweets (with stop words and forms of the word ‘drunk’ removed), created using Dataset Creation We use hashtag-based supervision to create our datasets, similar to tasks like emotion classification (Purver and Battersby, 2012). The tweets are downloaded using Twitter API (https://dev. 4 This is a rigid criterion, but we observe that tweets with hyperlinks are likely to be promotional in nature. 605 Feature Unigram & Bigram (Presence) Unigram & Bigram (Count) LDA unigrams (Presence/Count) POS Ratio #Named Entity Mentions #Discourse Connectors Spelling errors Repeated characters Capitalisation Length Emoticon (Presence/Count) Sentiment Ratio Description N-gram Features Boolean features indicating unigrams and bigrams Real-valued features indicating unigrams and bigrams Stylistic Features Boolean & real-valued feature"
P15-2100,D13-1133,0,0.0262903,"witter.com/). We remove non-Unicode characters, and eliminate tweets that contain hyperlinks4 and also tweets that are shorter than 6 words in length. Finally, hashtags used to indicate drunk or sober tweets are removed so that they provide labels, but do not act as features. The dataset is available on request. As a result, we create three datasets, each using a different strategy for sober tweets, as follows: health issues, suicidal nature, criminal status, etc. (Pennebaker, 1993; Pennebaker, 1997). NLP techniques have been used in the past to address social safety and mental health issues (Resnik et al., 2013). 3 Definition and Challenges Drunk-texting prediction is the task of classifying a text as drunk or sober. For example, a tweet ‘Feeling buzzed. Can’t remember how the evening went’ must be predicted as ‘drunk’, whereas, ‘Returned from work late today, the traffic was bad’ must be predicted as ‘sober’. The challenges are: 1. More than topic categorisation: Drunktexting prediction is similar to topic categorisation (that is, classification of documents into a set of categories such as ‘news’, ‘sports’, etc.). However, Borrill et al. (1987) show that alcohol abusers have more pronounced emotion"
P15-2100,H05-1044,0,0.0102358,"sed for dataset creation. For example, timestamps were a good option to account for time at which a tweet was posted. However, this could not be used because user’s local times was not available, since very few users had geolocation enabled. 5 happpy. Since drunk-texting is often associated with emotional expression, we also incorporate a set of sentiment-based features. These features include: count/presence of emoticons and sentiment ratio. Sentiment ratio is the proportion of positive and negative words in the tweet. To determine positive and negative words, we use the sentiment lexicon in Wilson et al. (2005). To identify a more refined set of words that correspond to the two classes, we also estimated 20 topics for the dataset by estimating an LDA model (Blei et al., 2003). We then consider top 10 words per topic, for both classes. This results in 400 LDA-specific unigrams that are then used as features. Feature Design The complete set of features is shown in Table 1. There are two sets of features: (a) N-gram features, and (b) Stylistic features. We use unigrams and bigrams as N-gram features- considering both presence and count. Table 1 shows the complete set of stylistic features of our predic"
P15-2100,P06-4018,0,0.0189092,"Missing"
P15-2124,W14-2609,0,0.563262,"troduce inter-sentential incongruity for sarcasm detection, that expands context of a discussion forum post by including the previous post (also known as the ‘elicitor’ post) in the discussion thread. Introduction Sarcasm is defined as ‘a cutting, often ironic remark intended to express contempt or ridicule’1 . Sarcasm detection is the task of predicting a text as sarcastic or non-sarcastic. The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014), (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013), a positive verb being followed by a negative situation (Riloff et al., 2013), or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010). Thus, the past work detects sarcasm with specific indicators. However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories. In this paper, we use one such linguistic theory: context incongruity. Although the past work exploits incongruity, it does so piecemeal; we take a more well-rou"
P15-2124,walker-etal-2012-corpus,0,0.128027,"ach to create a sarcasm-annotated dataset was employed in Gonzalez-Ibanez et al. (2011). As an additional quality check, a rough glance through the tweets is done, and the ones found to be wrong are removed. The hashtags mentioned above are removed from the text so that they act as labels but not as features. 2. Tweet-B (2278 tweets, 506 sarcastic): This dataset was manually labeled for Riloff et al. (2013). Some tweets were unavailable, due to deletion or privacy settings. 3. Discussion-A (1502 discussion forum posts, 752 sarcastic): This dataset is created from the Internet Argument Corpus (Walker et al., 2012) that contains manual annota• Number of sentiment incongruities: The number of times a positive word is followed by a negative word, and vice versa • Largest positive/negative subsequence: The length of the longest series of contiguous positive/negative words • Number of positive and negative words • Lexical Polarity: The polarity based purely on the basis of lexical features, as determined by Lingpipe SA system (Alias-i, 2008). Note that the ‘native polarity’ need not be correct. However, a tweet that is strongly positive on the surface is more likely to be sarcastic than a tweet that seems t"
P15-2124,W10-2914,0,0.680174,"ended to express contempt or ridicule’1 . Sarcasm detection is the task of predicting a text as sarcastic or non-sarcastic. The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014), (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013), a positive verb being followed by a negative situation (Riloff et al., 2013), or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010). Thus, the past work detects sarcasm with specific indicators. However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories. In this paper, we use one such linguistic theory: context incongruity. Although the past work exploits incongruity, it does so piecemeal; we take a more well-rounded view of incongruity and place it center-stage for our work. 1 Rest of the paper is organized as follows. We first discuss related work in Section 2. We introduce context incongruity in Section 3. Feature design for explicit incongruity is presented in Section 3.1,"
P15-2124,P11-2102,0,0.679454,"ts’ as well as long ‘discussion forum posts’. • We introduce inter-sentential incongruity for sarcasm detection, that expands context of a discussion forum post by including the previous post (also known as the ‘elicitor’ post) in the discussion thread. Introduction Sarcasm is defined as ‘a cutting, often ironic remark intended to express contempt or ridicule’1 . Sarcasm detection is the task of predicting a text as sarcastic or non-sarcastic. The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014), (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013), a positive verb being followed by a negative situation (Riloff et al., 2013), or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010). Thus, the past work detects sarcasm with specific indicators. However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories. In this paper, we use one such linguistic theory: context incongruity. Although the past work exploits incongruity"
P15-2124,W13-1605,0,0.460601,"Missing"
P15-2124,maynard-greenwood-2014-cares,0,0.564505,"Missing"
P15-2124,P13-2149,1,0.910944,"et of Dutch tweets that contain sarcasmrelated hashtags and implement a classifier to predict sarcasm. A recent work by ?) takes the output of sarcasm detection as an input to sentiment classification. They present a rule-based system that uses the pattern: if the sentiment of a tokenized hashtag does not agree with sentiment in rest of the tweet, the tweet is sarcastic, in addition to other rules. Our approach is architecturally similar to Tsur et al. (2010b) who use a semi-supervised pattern acquisition followed by classification. Our feature engineering is based on Riloff et al. (2013) and Ramteke et al. (2013). Riloff et al. (2013) state that sarcasm is a contrast between positive sentiment word and a negative situation. They implement a rule-based system that uses phrases of positive verb phrases and negative situations extracted from a corpus of sarcastic tweets. Ramteke et al. (2013) present a novel approach to detect thwarting: the phenomenon where sentiment in major portions of text is reversed by sentiment in smaller, conclusive portions. 3 3.1 Explicit incongruity Explicit incongruity is overtly expressed through sentiment words of both polarities (as in the case of ‘I love being ignored’ wh"
P15-2124,D13-1066,0,0.740627,"Missing"
P19-1108,W18-3713,0,0.0129492,"and the tweet is predicted to not be a PHM. If the figurative usage prediction is literal, then the prediction from the PHM detection module is returned. We refer to this approach as ‘+Pipeline’. 2. Feature Augmentation Approach augments PHM detection with figurative usage features. Therefore, the figurative label and the linguistic features from figurative usage detection are concatenated as figurative usage features ad passed through a convolution layer. The two are then concatenated in a dense layer to make the prediction. The approach is illustrated in Figure 3. This approach is based on Dasgupta et al. (2018), where they augment additional features to word embeddings of words in a document. We refer to this approach as ‘+FeatAug’. In +Pipeline, the figurative label guides whether or not PHM detection will be called. In +FeatAug, the label becomes one of the features. For both the approaches, the figurative label is determined by producing the literal usage score and then applying an empirically determined threshold. We experimentally determine if using the literal usage score performs better than using the LDA-based estimator (See Section 4.3). 4.1 Experiment Setup Dataset We report our results on"
P19-1108,N15-1184,0,0.0491512,"Missing"
P19-1108,W15-3821,0,0.060689,"Missing"
P19-1108,N15-1169,0,0.0296133,"Missing"
P19-1108,N13-1097,0,0.0939878,"Missing"
P19-1108,D18-1199,0,0.228364,"report that either the author or someone they know is experiencing a health condition or a symptom (Lamb et al., 2013). For example, the sentence ‘I have been coughing since morning’ is a PHM, while ‘Having a cough for three weeks or more could be a sign of cancer’ is not. The former reports that the author has a cough while, in the latter, the author provides information about coughs in general. Past work in PHM detection uses classification-based approaches with human-engineered features (Lamb To address the question, we use a state-ofthe-art approach that detects idiomatic usage of words (Liu and Hwa, 2018). Given a word and a sentence, the approach identifies if the word is used in a figurative or literal sense in the sentence. We refer to this module as ‘figurative usage detection’. We experiment with alternative ways to combine figurative usage detection with PHM detection, and report results on a manually labeled dataset of tweets. 2 Motivation As the first step, we ascertain if the volume of figurative usage of symptom words warrants such attention. Therefore, we randomly selected 200 tweets (with no duplicates and retweets) posted in November 2018, each containing either ‘cough’ or ‘breath"
P19-1108,P14-5010,0,0.00247159,"peline approach. figurative/literal distribution which indicates the probability of a word to be either figurative or literal, and a document-figurative/literal distribution which gives a predictive score for a document to be literal or figurative. To obtain the literal usage score, we generate the literal usage representation using word2vec similarity learned from the Sentiment140 tweet dataset (Go et al., 2009). We use two sets of linguistic features, as reported in Liu and Hwa (2018): the presence of subordinate clauses and part-of-speech tags of neighbouring words, using Stanford CoreNLP (Manning et al., 2014). We adapt the abstractness feature in their paper to health-relatedness (i.e., the presence of health-related words). The intuition is that tweets which contain more health-related words are more likely to be using the symptom words in a literal sense instead of figurative. Therefore, the abstractness feature in the original paper is converted to domain relatedness and captured using the presence of health-related words. We consider the symptom word as the target word. It must be noted that we do not have or use figurative labels in the dataset except for the sample used to report the efficac"
P19-1108,D13-1145,0,0.0702514,"Missing"
P19-1108,D14-1162,0,0.080737,"Missing"
P19-1108,E09-1086,0,0.044109,"Missing"
S15-2098,W11-0705,0,0.0287093,"linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging. Recent publication from (Socher et al., 2013) has further"
S15-2098,baccianella-etal-2010-sentiwordnet,0,0.0902886,"Missing"
S15-2098,W12-3704,1,0.854817,"ed Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features"
S15-2098,P05-1045,0,0.0235427,"Missing"
S15-2098,S14-2076,0,0.0139472,"e SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of e"
S15-2098,S13-2053,0,0.0201563,"numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 201"
S15-2098,S13-2052,0,0.0312022,"ervices can gauge the public sentiment towards the new product or service they launched, political parties can estimate their chances of winning the upcoming elections by monitoring what people are saying on Twitter about them, and so on. In spite of the availability of huge amount of data and the huge promises they entail, working with social media data is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗ The first two authors made equal contribution to this work Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple"
S15-2098,pak-paroubek-2010-twitter,0,0.0600791,"a data is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗ The first two authors made equal contribution to this work Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes b"
S15-2098,S14-2004,0,0.0805018,"Missing"
S15-2098,S14-2009,0,0.0154444,"nt towards the new product or service they launched, political parties can estimate their chances of winning the upcoming elections by monitoring what people are saying on Twitter about them, and so on. In spite of the availability of huge amount of data and the huge promises they entail, working with social media data is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗ The first two authors made equal contribution to this work Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses pr"
S15-2098,S15-2078,0,0.0148721,"Eval too and attracted an unprecedented number of participations. This task comprises of four sub-tasks. We participated in subtask 2 — Message polarity classification. Although we lie a few notches down from the top system, we present a very simple yet effective approach to handle this problem that can be implemented in a single day! 1 2 Introduction Social media not only acts as a proxy for the real world society, it also offers a treasure trove of data for different types of analyses like Trend Analysis, Event Detection and Sentiment Analysis, to name a few. SemEval 2015 Task 10 subtask B (Rosenthal et al., 2015) specifically deals with the task of Sentiment Analysis in Twitter. Sentiment Analysis in social media in general and Twitter in particular has a wide range of applications — Companies/services can gauge the public sentiment towards the new product or service they launched, political parties can estimate their chances of winning the upcoming elections by monitoring what people are saying on Twitter about them, and so on. In spite of the availability of huge amount of data and the huge promises they entail, working with social media data is far more challenging than regular text data. Being use"
S15-2098,D13-1170,0,0.00367562,"features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging. Recent publication from (Socher et al., 2013) has further raised the bar for Sentiment Analysis in general, but it is not specifically designed to tackle tweets data. 3 3.1 Approach Preprocessing We acquire a list of acronyms and their expanded forms 1 . We use this list as a look-up table and replace all occurrences of acronyms in our data by their expanded forms. We normalize all numbers that find a place in our data by replacing them with the string ‘0’. We do not remove stop words because they often contribute heavily towards expressing sentiment/emotion. We do not also stem the words because stemming leads to the loss of the parts o"
S15-2098,H05-1044,0,0.165906,"of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver"
S15-2098,P06-4018,0,\N,Missing
S15-2129,W12-3704,1,0.861206,"Missing"
S15-2129,S14-2149,0,0.0529627,"Missing"
S15-2129,S14-2145,0,0.0428515,"Missing"
S15-2129,S14-2135,0,0.0386611,"Missing"
S15-2129,P05-1045,0,0.0133495,"Missing"
S15-2129,D10-1101,0,0.102655,"Missing"
S15-2129,S14-2076,0,0.0461655,"Missing"
S15-2129,W02-0109,0,0.0947969,"Missing"
S15-2129,S13-2053,0,0.0588788,"Missing"
S15-2129,P04-1035,0,0.0418711,"Missing"
S15-2129,S14-2004,0,0.0462049,"Missing"
S15-2129,S15-2082,0,0.0548958,"Missing"
S15-2129,S14-2009,0,0.0478206,"Missing"
S15-2129,D13-1170,0,0.0121678,"Missing"
S15-2129,S14-2038,0,0.0429146,"Missing"
S15-2129,S14-2036,0,0.0380869,"Missing"
S15-2129,H05-1044,0,0.119918,"Missing"
S15-2129,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S15-2129,P06-4018,0,\N,Missing
U16-1013,W14-2608,0,0.0180467,"example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we investigate sarcasm versus irony classification. To do so, we compare sarcasm versus irony classification with sarcasm versus philosophy classification. In case of former, the two classe"
U16-1013,W10-2914,0,0.0204562,"hand, sarcasm is a form of verbal irony that is intended to express contempt or ridicule. In other words, sarcasm has an element of ridicule and a target of ridicule, which is absent in irony (Lee and Katz, 1998). For example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is"
U16-1013,P11-2102,0,0.0231454,"that is intended to express contempt or ridicule. In other words, sarcasm has an element of ridicule and a target of ridicule, which is absent in irony (Lee and Katz, 1998). For example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we i"
U16-1013,P15-2124,1,0.872736,"new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we investigate sarcasm versus irony classification. To do so, we compare sarcasm versus irony classification with sarcasm versus philosophy classification. In case of former, the two classes are similar (where"
U16-1013,D16-1104,1,0.796821,"r sarcasm versus irony. 2 Related Work Several approaches have been proposed for sarcasm detection, with context incongruity as the basis of sarcasm detection. Joshi et al. (2015) present features based on explicit and implicit incongruity for sarcasm detection. Maynard and Greenwood (2014) use contrasting sentiment between hashtag and text of a tweet as an indicator of sarcasm. Davidov et al. (2010) rely on Wallace and Do Kook Choe (2014) use properties of reddit comments to add contextual information. Recent work uses deep learning-based techniques for sarcasm detection (Poria et al., 2016; Joshi et al., 2016). The work closest to ours is by Ling and Klinger Aditya Joshi, Vaibhav Tripathi, Pushpak Bhattacharyya, Mark Carman, Meghna Singh, Jaya Saraswati and Rajita Shukla. 2016. How Challenging is Sarcasm versus Irony Classification?: A Study With a Dataset from English Literature. In Proceedings of Australasian Language Technology Association Workshop, pages 123−127. Original Labels A1 Sarcasm Philosophy Irony Cannot say Original Labels Sarcasm Philosophy Irony 27 5 2 13 18 222 7 24 6 17 12 14 A3 Table 1: Confusion matrix for Annotator 1 A2 Sarcasm Philosophy Irony Cannot say Philosophy Irony 30 9"
U16-1013,W07-0101,0,0.0607824,"result1 . On the other hand, sarcasm is a form of verbal irony that is intended to express contempt or ridicule. In other words, sarcasm has an element of ridicule and a target of ridicule, which is absent in irony (Lee and Katz, 1998). For example, ‘He invented a new cure for a heart disease but later, died of the same disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm ver"
U16-1013,W13-1104,0,0.0130965,"tures for the task. However, our analysis from both human and computational perspectives is novel, along with our observations. Another novelty of this work is the domain of our dataset. Majority of the past works in sarcasm detection use tweets. Riloff et al. (2013a) and Maynard and Greenwood (2014) label these tweets manually whereas Bamman and Smith (2015) and Davidov et al. (2010) rely on hashtags to produce annotations. Some works in the past also explore long text for the task of sarcasm detection. Wallace and Do Kook Choe (2014) download posts from Reddit 2 for irony detection, whereas Lukin and Walker (2013) work with reviews. One past work by Tepperman et al. (2006) performs sarcasm detection on spoken dialogues as well. There are past works using literary quotes corpora for a variety of other NLP problems. Elson and McKeown (2010) extract quotes from popular literary work, for the task of quote attribution3 . Søgaard (2012) perform the task of detecting famous quotes in literary works, gathered from the Gutenberg Corpus. Skabar and Abdalgader (2010) cluster famous quotations by improving sentence similarity measurements. This dataset consists of quotes from a quotes website. In terms of a datas"
U16-1013,maynard-greenwood-2014-cares,0,0.275207,"e disease himself ’ is ironic but not sarcastic. However, ‘I didn’t make it big in Hollywood because I don’t write bad enough’ is sarcastic where the target of ridicule is Hollywood. Past work in sarcasm detection considers it as a sarcastic versus non-sarcastic classification (Kreuz and Caucci, 2007; Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011). Alternately, Reyes et al. (2012) consider classification of irony/sarcasm versus humor. In many past approaches, sarcasm and irony are treated interchangeably (Buschmeier et al., 2014; Joshi et al., 2015; 1 Source: The Cambridge Dictionary Maynard and Greenwood, 2014). However, since sarcasm has a target that is being ridiculed, it is crucial that sarcasm be distinguished from mere irony. This is because when the target is identified, the sentiment of the target can be appropriately assigned. Owing to the two above reasons, sarcasm versus irony detection is a useful task. In this paper, we investigate sarcasm versus irony classification. To do so, we compare sarcasm versus irony classification with sarcasm versus philosophy classification. In case of former, the two classes are similar (where sarcasm is hurtful/contemptuous). In case of sarcasm versus phil"
U16-1013,C16-1151,0,0.0193489,"y but not as much for sarcasm versus irony. 2 Related Work Several approaches have been proposed for sarcasm detection, with context incongruity as the basis of sarcasm detection. Joshi et al. (2015) present features based on explicit and implicit incongruity for sarcasm detection. Maynard and Greenwood (2014) use contrasting sentiment between hashtag and text of a tweet as an indicator of sarcasm. Davidov et al. (2010) rely on Wallace and Do Kook Choe (2014) use properties of reddit comments to add contextual information. Recent work uses deep learning-based techniques for sarcasm detection (Poria et al., 2016; Joshi et al., 2016). The work closest to ours is by Ling and Klinger Aditya Joshi, Vaibhav Tripathi, Pushpak Bhattacharyya, Mark Carman, Meghna Singh, Jaya Saraswati and Rajita Shukla. 2016. How Challenging is Sarcasm versus Irony Classification?: A Study With a Dataset from English Literature. In Proceedings of Australasian Language Technology Association Workshop, pages 123−127. Original Labels A1 Sarcasm Philosophy Irony Cannot say Original Labels Sarcasm Philosophy Irony 27 5 2 13 18 222 7 24 6 17 12 14 A3 Table 1: Confusion matrix for Annotator 1 A2 Sarcasm Philosophy Irony Cannot say P"
U16-1013,D13-1066,0,0.12887,"Missing"
U16-1013,P14-2084,0,0.573573,"Missing"
U19-1008,W17-6605,0,0.0281374,"Missing"
U19-1008,D18-1403,0,0.250713,"ey are considering to purchase. This makes opinion summarisation important because it allows users to obtain aggregate key opinions about the product or service based on its reviews. Given the value of opinion summaries, automatic opinion summarisation is an active area of research with the focus of producing high-quality opinion summaries. The quality of an opinion summary would ideally be evaluated by human annotators. For example, annotators may read a summary and rate it according to quality measures such as informativeness, ability to capture sentiment polarity, coherence and redundancy (Angelidis and Lapata, 2018). However, human evaluation is resourceintensive and not scalable. This motivates automatic evaluation. In the case of automatic evaluation, for a set of product reviews, reference summaries are written by human experts apriori. Reference summaries are the ground truth summaries against which candidate summaries to be evaluated. Using reference and candidate summaries, automatic evaluation of opinion summaries adopts metrics from text summarisation (Lin, 2004) and machine translation (Papineni et al., 2002; Lavie and Denkowski, 2009). We focus on the most frequently reported metric for opinion"
U19-1008,baccianella-etal-2010-sentiwordnet,0,0.0297437,"used to compare summaries. other summary that is of same aspect but opposite polarity, Summ-Ant. This forms the second and third summaries of the triplet. This experiment controls for all the matching of the other words except for the sentiment-bearing words. Hence, we can study the impact of matching sentimentbearing words in the reference summary. To generate the synonym and antonym version of a summary, we first identify the sentimentbearing words in the summary. A sentimentbearing word is an adjective, adverb or verb and its lemmatised word form contains a sentiment score in SentiWordNet (Baccianella et al., 2010). The pre-processing steps of part-of-speech tagging, lemmatisation and looking it up in SentiWordnet was performed through python’s NLTK package (Bird et al., 2009). We obtain synonyms and antonyms3 from Wiktionary using the python package wiktionaryparser. Table 6 reports the pro3 We also experimented with WordNet (Fellbaum, 1998) to get synonym and antonym of a sentiment-bearing word, however, the pairs we obtained from WordNet had lower coverage than Wiktionary. portion of sentiment-bearing words present in the gold standard summaries and Table 7 shows examples of the synonyms and antonyms"
U19-1008,C10-1039,0,0.222919,"of lexical and sentiment similarity to capture sentiment-aware similarity between sentences. We note that past work states the limitations of ROUGE as a part of the discussion of the results of their proposed systems, while examining these limitations is the focus of our work. 3 ROUGE ROUGE measures content coverage of candidate summaries against reference summaries (Lin, 2004). Different variants of ROUGE have been proposed. For example, ROUGE-N, ROUGE-L and ROUGE-S count the number of overlapping units of n-gram, word sequences, and word pairs between the candidate summary and the reference Ganesan et al. (2010) Jayanth et al. (2015) Wang and Ling (2016) Angelidis and Lapata (2018) Kunneman et al. (2018) Amplayo and Lapata (2019) Dataset Task ROUGE Others Opinosis Movie RottenTomatoes Oposum ProductReviews RottenTomatoes Abstractive Abstractive Abstractive Extractive Abstractive Abstractive R-1,R-2,R-SU4 R-1,R-2 R-SU4 R-1,R-2,R-L No R-1 R-2,R-L,R-SU4 No Senti Corr BLEU, METEOR No Precision, Recall and F1 METEOR Table 1: Summary of automatic evaluation metrics used to evaluate opinion summaries. Reference: The rooms were neat and clean. Configuration None Stemming StopWordRemoval StopWordRemoval+Stemm"
U19-1008,D15-1013,0,0.0203267,"nth et al. (2015) observe that ROUGE is influenced by topic terms more than sentiment terms. Therefore, they report two metrics: ROUGE scores and sentiment correlation. Mackie et al. (2014) show that, for microblog summarisation, ROUGE does not correlate with human judgment as well as a more na¨ıve indicator: fraction of topic words. In addition, limitations of ROUGE to evaluate text summarisation have also been reported. Conroy and Schlesinger (2008) show that ROUGE may not correlate well with human evaluation for text summarisation, and needs to be combined with human scores. More recently, Graham (2015) present an extensive comparison of 192 variants of ROUGE, and show that the metrics have contrasting conclusions. Table 1 summarises key studies and the choice of automatic evaluation metrics used for opinion summarisation. Despite the limitations, ROUGE is the most popular metric for opinion summarisation (Moussa et al., 2018). It continues to be used as an automatic evaluation in recent papers either on its own (Anchiˆeta et al., 2017) or in combination with other automatic metrics such as METEOR (Amplayo and Lapata, 2019). Angelidis and Lapata (2018) report ROUGE for multi-document opinion"
U19-1008,D15-1017,0,0.082126,"tric for summarisation. This metric captures linguistic quality using a set of features. They conclude that syntactic features are the best indicators for linguistic quality of summaries. Following the availability of datasets with opinion summaries, ROUGE could be used for opinion summarisation. However, its value has been under1 Although the current analysis focuses on ROUGE for evaluating opinion summaries, the limitations of using word matching for evaluation is also a problem faced by text summarisation and text generation. stood to be limited for the evaluation of opinion summarisation. Jayanth et al. (2015) observe that ROUGE is influenced by topic terms more than sentiment terms. Therefore, they report two metrics: ROUGE scores and sentiment correlation. Mackie et al. (2014) show that, for microblog summarisation, ROUGE does not correlate with human judgment as well as a more na¨ıve indicator: fraction of topic words. In addition, limitations of ROUGE to evaluate text summarisation have also been reported. Conroy and Schlesinger (2008) show that ROUGE may not correlate well with human evaluation for text summarisation, and needs to be combined with human scores. More recently, Graham (2015) pre"
U19-1008,C18-1188,0,0.0518609,"Missing"
U19-1008,E09-1059,0,0.014294,"te summary is accurate to the opinion aspect and polarity in the reference summary?’ This paper makes two-fold contributions: (1) Through experiments, we demonstrate that ROUGE is not able to accurately evaluate the opinions in the candidate summary against the reference summary1 ; and (2) Our discussion provides three recommendations for further research on opinion summary evaluation. 2 Related Work Early work in opinion summarisation conducted their evaluation using metrics other than ROUGE. Pang and Lee (2004), an early work in extractive opinion summarisation, use sentence-level accuracy. Lerman et al. (2009) pre-date the existence of opinion summarisation datasets. Therefore, they use human evaluation for their systems. Pitler et al. (2010) propose an automatic metric for summarisation. This metric captures linguistic quality using a set of features. They conclude that syntactic features are the best indicators for linguistic quality of summaries. Following the availability of datasets with opinion summaries, ROUGE could be used for opinion summarisation. However, its value has been under1 Although the current analysis focuses on ROUGE for evaluating opinion summaries, the limitations of using wo"
U19-1008,W04-1013,0,0.279367,"ccording to quality measures such as informativeness, ability to capture sentiment polarity, coherence and redundancy (Angelidis and Lapata, 2018). However, human evaluation is resourceintensive and not scalable. This motivates automatic evaluation. In the case of automatic evaluation, for a set of product reviews, reference summaries are written by human experts apriori. Reference summaries are the ground truth summaries against which candidate summaries to be evaluated. Using reference and candidate summaries, automatic evaluation of opinion summaries adopts metrics from text summarisation (Lin, 2004) and machine translation (Papineni et al., 2002; Lavie and Denkowski, 2009). We focus on the most frequently reported metric for opinion summarisation, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004) and leave the analysis of other evaluation metrics to future studies. ROUGE counts the overlap of word or word units between the candidate summary and reference summary with respect to the word units in the reference summary. A higher ROUGE score means a larger overlap of word units while a lower ROUGE score means a smaller overlap of word units. The ROUGE scores can then be"
U19-1008,P04-1035,0,0.102399,"arch question is: ‘Can ROUGE scores be used to correctly compare summaries to ensure that the candidate summary is accurate to the opinion aspect and polarity in the reference summary?’ This paper makes two-fold contributions: (1) Through experiments, we demonstrate that ROUGE is not able to accurately evaluate the opinions in the candidate summary against the reference summary1 ; and (2) Our discussion provides three recommendations for further research on opinion summary evaluation. 2 Related Work Early work in opinion summarisation conducted their evaluation using metrics other than ROUGE. Pang and Lee (2004), an early work in extractive opinion summarisation, use sentence-level accuracy. Lerman et al. (2009) pre-date the existence of opinion summarisation datasets. Therefore, they use human evaluation for their systems. Pitler et al. (2010) propose an automatic metric for summarisation. This metric captures linguistic quality using a set of features. They conclude that syntactic features are the best indicators for linguistic quality of summaries. Following the availability of datasets with opinion summaries, ROUGE could be used for opinion summarisation. However, its value has been under1 Althou"
U19-1008,P02-1040,0,0.106026,"informativeness, ability to capture sentiment polarity, coherence and redundancy (Angelidis and Lapata, 2018). However, human evaluation is resourceintensive and not scalable. This motivates automatic evaluation. In the case of automatic evaluation, for a set of product reviews, reference summaries are written by human experts apriori. Reference summaries are the ground truth summaries against which candidate summaries to be evaluated. Using reference and candidate summaries, automatic evaluation of opinion summaries adopts metrics from text summarisation (Lin, 2004) and machine translation (Papineni et al., 2002; Lavie and Denkowski, 2009). We focus on the most frequently reported metric for opinion summarisation, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004) and leave the analysis of other evaluation metrics to future studies. ROUGE counts the overlap of word or word units between the candidate summary and reference summary with respect to the word units in the reference summary. A higher ROUGE score means a larger overlap of word units while a lower ROUGE score means a smaller overlap of word units. The ROUGE scores can then be used as a criterion to compare summaries and sy"
U19-1008,P10-1056,0,0.0332854,"gh experiments, we demonstrate that ROUGE is not able to accurately evaluate the opinions in the candidate summary against the reference summary1 ; and (2) Our discussion provides three recommendations for further research on opinion summary evaluation. 2 Related Work Early work in opinion summarisation conducted their evaluation using metrics other than ROUGE. Pang and Lee (2004), an early work in extractive opinion summarisation, use sentence-level accuracy. Lerman et al. (2009) pre-date the existence of opinion summarisation datasets. Therefore, they use human evaluation for their systems. Pitler et al. (2010) propose an automatic metric for summarisation. This metric captures linguistic quality using a set of features. They conclude that syntactic features are the best indicators for linguistic quality of summaries. Following the availability of datasets with opinion summaries, ROUGE could be used for opinion summarisation. However, its value has been under1 Although the current analysis focuses on ROUGE for evaluating opinion summaries, the limitations of using word matching for evaluation is also a problem faced by text summarisation and text generation. stood to be limited for the evaluation of"
U19-1008,D17-1049,0,0.131934,"for opinion summarisation (Moussa et al., 2018). It continues to be used as an automatic evaluation in recent papers either on its own (Anchiˆeta et al., 2017) or in combination with other automatic metrics such as METEOR (Amplayo and Lapata, 2019). Angelidis and Lapata (2018) report ROUGE for multi-document opinion summarisation. Alternatives to ROUGE have been proposed. Kabadjov et al. (2009) use sentiment intensity to measure sentiment summarisation. Kunneman et al. (2018) use gold standard summaries available in the forum as reference summaries, and report precision, recall and F1 scores. Poddar et al. (2017) use a combination of lexical and sentiment similarity to capture sentiment-aware similarity between sentences. We note that past work states the limitations of ROUGE as a part of the discussion of the results of their proposed systems, while examining these limitations is the focus of our work. 3 ROUGE ROUGE measures content coverage of candidate summaries against reference summaries (Lin, 2004). Different variants of ROUGE have been proposed. For example, ROUGE-N, ROUGE-L and ROUGE-S count the number of overlapping units of n-gram, word sequences, and word pairs between the candidate summary"
U19-1008,N16-1007,0,0.0420013,"Missing"
U19-1020,W16-2917,0,0.0507377,"Missing"
U19-1020,W18-5911,1,0.884519,"Missing"
U19-1020,N13-1097,0,0.0212999,"Missing"
U19-1020,E17-1015,0,0.0146851,"MTL has been applied in different kinds of tasks. Zou et al. (2018) predict influenza counts based on search counts for different geographical regions - however, they do not use a neural architecture. The task in itself is similar to Pair 1 in our experiments. Chowdhury et al. (2018) use MTL for pharmacovigilance, where each tweet is labeled with adverse drug reaction and indication labels. This is similar to the drug usage detection task in our experiments. For this, they use bi-directional LSTM as the shared layer, in addition to other task-specific layers before and after the shared layer. Benton et al. (2017) use MTL for prediction of mental health signals. Their architecture uses multi-layer perceptrons as shared layers. Bingel and Søgaard (2017) use bi-directional LSTM as the shared layer and compares different pairs of NLP tasks. In contrast, we experiment with three alternatives of shared sentence representations. The above are classification formulations for health informatics. MTL has also been used for other tasks such as biomedical entity extraction (Crichton et al., 2017), non-textual data based on medical tests to predict disease progression (Zhou et al., 2011) and so on. We use datasets"
U19-1020,P17-1001,0,0.105214,"help each other in terms of how similar the participating datasets or tasks are. In this paper, we examine the utility of Multi-Task Learning (MTL) for several pairs of health informatics tasks that are related in different ways. MTL pertains to the class of learning algorithms that jointly train predictors for more than one task. In Natural Language Processing (NLP) research, MTL using deep learning has been used either to learn shared representations for related tasks, or to deal with limited labeled datasets (Xue et al., 2007; Zhang and Yeung, 2012; Søgaard and Goldberg, 2016; Ruder, 2017; Liu et al., 2017) for a variety of NLP problems such as sentiment analysis (Huang et al., 2013; Mishra et al., 2017). Most of this work that uses MTL presents architectures utilising multiple shared and task-specific layers. In contrast, we wish to see if the benefit comes from the simplistic notion of ‘learning these classifiers together’. Therefore, we use a basic architecture for our MTL experiments consisting of a single shared layer and single task-specific layers, and experiment with different alternatives for the shared layer. This simplicity allows us to understand the benefit of MTL in comparison with"
U19-1020,E17-2026,0,0.0758614,"lems: one problem influences the probability of output of the other. Through our experiments with simple architectures for popular tasks in health informatics, we examine the question: ‘Does multi-task learning always help?’ 2 Related Work MTL has been applied to a variety of text classification tasks (Søgaard and Goldberg, 2016; Xue et al., 2007; Ruder, 2017; Zhang and Yeung, 2012; Liu et al., 2017). The impact of task relatedness on MTL has been explored in case of statistical prediction models (Zhang and Yeung, 2012; Ben-David and Schuller, 2003). In the case of deep learning-based models, Bingel and Søgaard (2017) show how fundamental NLP tasks (such as MWE detection, POS tagging and so on) of different complexities perform when paired. (Mishra et al., 2017) use MTL for two related tasks in opinion mining: sentiment classificaton and sarcasm classification. (Wu and Huang, 2016) use MTL for personalisation of sentiment classification where global and local classifiers are jointly learned. A survey of MTL approaches using deep learning is by (Ruder, 2017). In the context of health informatics, MTL has been applied in different kinds of tasks. Zou et al. (2018) predict influenza counts based on search cou"
U19-1020,P17-1035,0,0.0666567,"we examine the utility of Multi-Task Learning (MTL) for several pairs of health informatics tasks that are related in different ways. MTL pertains to the class of learning algorithms that jointly train predictors for more than one task. In Natural Language Processing (NLP) research, MTL using deep learning has been used either to learn shared representations for related tasks, or to deal with limited labeled datasets (Xue et al., 2007; Zhang and Yeung, 2012; Søgaard and Goldberg, 2016; Ruder, 2017; Liu et al., 2017) for a variety of NLP problems such as sentiment analysis (Huang et al., 2013; Mishra et al., 2017). Most of this work that uses MTL presents architectures utilising multiple shared and task-specific layers. In contrast, we wish to see if the benefit comes from the simplistic notion of ‘learning these classifiers together’. Therefore, we use a basic architecture for our MTL experiments consisting of a single shared layer and single task-specific layers, and experiment with different alternatives for the shared layer. This simplicity allows us to understand the benefit of MTL in comparison with Single-Task Learning (STL) for different configurations of shared layers, for task pairs that are"
U19-1020,D14-1162,0,0.0821623,"has been reported in the paper or its derivative papers, to the best of our knowledge. Since these datasets have been reported in past papers, we use Tweepy3 to download the datasets of tweets using their identifiers. To implement the deep learning models, we use Keras (Chollet, 2015), with the Adam optimiser and binary crossentropy as the loss function during training, with 3 http://www.tweepy.org/; Last accessed on 3rd September, 2019. a dropout of 0.25 and number of units for intermediate layers as 25. We use word embeddings with 200 dimensions, pre-trained on a Twitter corpus using GLoVe (Pennington et al., 2014). These embeddings have been trained on 2 billion tweets with 27 billion tokens. The general outline of our experimentation is a comparison of MTL with the equivalent singletask learning (STL) version. The corresponding STL architecture is shown in Figure 2. This architecture is identical to MTL, except that it separately learns the classifiers for the two tasks. The STL version uses one dense layer to obtain the classification output after the embedding layer and a layer to capture the semantic representation (the equivalent of the shared layer in MTL). For all our experiments, we report aver"
U19-1020,P16-2038,0,0.154212,"f these datasets or classification tasks help each other in terms of how similar the participating datasets or tasks are. In this paper, we examine the utility of Multi-Task Learning (MTL) for several pairs of health informatics tasks that are related in different ways. MTL pertains to the class of learning algorithms that jointly train predictors for more than one task. In Natural Language Processing (NLP) research, MTL using deep learning has been used either to learn shared representations for related tasks, or to deal with limited labeled datasets (Xue et al., 2007; Zhang and Yeung, 2012; Søgaard and Goldberg, 2016; Ruder, 2017; Liu et al., 2017) for a variety of NLP problems such as sentiment analysis (Huang et al., 2013; Mishra et al., 2017). Most of this work that uses MTL presents architectures utilising multiple shared and task-specific layers. In contrast, we wish to see if the benefit comes from the simplistic notion of ‘learning these classifiers together’. Therefore, we use a basic architecture for our MTL experiments consisting of a single shared layer and single task-specific layers, and experiment with different alternatives for the shared layer. This simplicity allows us to understand the b"
U19-1020,W18-5904,0,0.0677063,"Missing"
U19-1026,L18-1424,1,0.682663,"been reported for automatic detection of sarcasm in text, spanning rule-based approaches to deep neural architectures (Joshi et al., 2017). Since sarcasm is a peculiar form of sentiment expression, the target of a sarcastic text bears implications on attribution of the negative sentiment to the appropriate target. For example, for an aspect-based sentiment analysis system, the sarcasm target will be the aspect towards which a negative sentiment will be assigned. Two prior papers report approaches for sarcasm target identification. The problem of sarcasm target identification was introduced in Joshi et al. (2018). They present three kinds of methods: (a) rule-based which use heuristics to determine sarcasm targets, (b) learning-based which use a sequence labelling algorithm trained on a dataset labelled with sarcasm targets, and (c) a hybrid of the two where output of the two systems is combined to make the final predictions. More recently, Patro et al. (2019) present a deep learning-based architecture for sarcasm target identification. The semantic representation of each word is captured in terms of its context window using a bidirectional LSTM. This semantic representation is then concatenated with"
U19-1026,maynard-greenwood-2014-cares,0,0.0334276,"s posted on social media. We introduce the task, describe the data and present the results of baselines and participants. This year’s shared task was particularly challenging and no participating systems improved the results of our baseline. 1 Introduction Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Sarcastic text has been understood to be a challenge to sentiment analysis because a sarcastic text may appear to be positive on the surface but is intended to be negative. Empirical results also show that sarcastic text is detrimental to sentiment analysis (Maynard and Greenwood, 2014). Applications where sentiment understanding is important are also impacted by sarcastic text. These applications include dialogue systems where the correct prediction of sentiment is important to generate appropriate responses. Towards this, computational sarcasm has gained interest in the research community. Sentiment in a text can be understood to be composed of valence (positive/negative) and the target (Liu, 2012). The connection between sarcasm detection and sentiment analysis is the target. Sarcastic text bears a target of ridicule. It is this target that receives negative sentiment in"
U19-1026,D19-1663,0,0.02034,"ment analysis system, the sarcasm target will be the aspect towards which a negative sentiment will be assigned. Two prior papers report approaches for sarcasm target identification. The problem of sarcasm target identification was introduced in Joshi et al. (2018). They present three kinds of methods: (a) rule-based which use heuristics to determine sarcasm targets, (b) learning-based which use a sequence labelling algorithm trained on a dataset labelled with sarcasm targets, and (c) a hybrid of the two where output of the two systems is combined to make the final predictions. More recently, Patro et al. (2019) present a deep learning-based architecture for sarcasm target identification. The semantic representation of each word is captured in terms of its context window using a bidirectional LSTM. This semantic representation is then concatenated with features based on LIWC, NER, empathy and POS tags, to learn a classifier. They show an improvement over the prior work. 4 Data The data used in the 2019 ALTA Shared Task consists of 950 training samples and 544 test samples. A count of the words appearing in the targets of the training data (Figure 1) reveals that a large percentage of the data is labe"
W11-1717,P04-1035,0,0.0451257,"Missing"
W11-1717,W02-1011,0,0.0174595,"ion is a task under Sentiment Analysis (SA) that deals with automatically tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. Thus, a sentiment classifier tags the sentence ‘The movie is entertaining and totally worth your money!’ in a movie review as positive with respect to the movie. On the other hand, a sentence ‘The movie is so boring that I was dozing away through the second half.’ is labeled as negative. Finally, ‘The movie is directed by Nolan’ is labeled as neutral. For the purpose of this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary classification task for output labels as positive and negative. Lexeme-based (bag-of-words) features are commonly used for supervised sentiment classification (Pang and Lee, 2008). In addition to this, there also has been work that identifies the roles of different parts-of-speech (POS) like adjectives in sentiment classification (Pang et al., 2002; Whitelaw et 132 al., 2005). Complex features based on parse trees have been explored for modeling high-accuracy polarity classifiers (Matsumoto et al., 2005). Text parsers have also been found to be helpful in"
W11-1717,N04-3012,0,0.163923,"Missing"
W11-1717,R09-1067,0,0.0401426,"Missing"
W11-1717,P02-1053,0,0.00373858,"entiment Analysis (SA) that deals with automatically tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. Thus, a sentiment classifier tags the sentence ‘The movie is entertaining and totally worth your money!’ in a movie review as positive with respect to the movie. On the other hand, a sentence ‘The movie is so boring that I was dozing away through the second half.’ is labeled as negative. Finally, ‘The movie is directed by Nolan’ is labeled as neutral. For the purpose of this work, we follow the definition of Pang et al. (2002) & Turney (2002) and consider a binary classification task for output labels as positive and negative. Lexeme-based (bag-of-words) features are commonly used for supervised sentiment classification (Pang and Lee, 2008). In addition to this, there also has been work that identifies the roles of different parts-of-speech (POS) like adjectives in sentiment classification (Pang et al., 2002; Whitelaw et 132 al., 2005). Complex features based on parse trees have been explored for modeling high-accuracy polarity classifiers (Matsumoto et al., 2005). Text parsers have also been found to be helpful in modeling valenc"
W11-1717,P06-1134,0,0.0648413,"Missing"
W11-1717,D09-1020,0,\N,Missing
W14-2623,carl-2012-translog,0,0.0543055,"as reported by the participants are shown in Table 1. All participants correctly identified the polarity of document D0. Participant P9 reported that D1 is confusing. 4 out of 12 participants were unable to detect correct opinion in D2. Experiment Setup 3.3 This section describes the framework used for our eye-tracking experiment. A participant is given the task of annotating documents with one out of the following labels: positive, negative and objective. While she reads the document, her eyefixations are recorded. To log eye-fixation data, we use Tobii T120 remote eye-tracker with Translog(Carl, 2012). Translog is a freeware for recording eye movements and keystrokes during translation. We configure Translog for reading with the goal of sentiment. 3.1 Participant description Experiment Description We obtain two kinds of annotation from our annotators: (a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eyetracker. They are given a set of instructions beforehand and can seek clarifications. This experiment is conducted as follows: 1. A complete document is displayed on the screen. The font size and line separation are set to 17pt and 1.5 cm respectively to e"
W14-2623,N13-1088,1,0.440886,"Missing"
W14-2623,P04-1035,0,\N,Missing
W15-2905,P15-1100,0,0.116958,"le for our approach Figure 2: Architecture of our sarcasm detection approach Similarly, supervised approaches implement sarcasm as a classification task that predicts whether a piece of text is sarcastic or not (Gonzalez-Ibanez et al., 2011; Barbieri et al., 2014; Carvalho et al., 2009). The features used include unigrams, emoticons, etc. Recent work in sarcasm detection deals with a more systematic feature design. Joshi et al. (2015) use a linguistic theory called context incongruity as a basis of feature design, and describe two kinds of features: implicit and explicit incongruity features. Wallace et al. (2015) uses as features beyond the target text as features. These include features from the comments and description of forum theme. In this way, sarcasm detection using ML-based classifiers has proceeded in the direction of improving the feature design, while rule-based sarcasm detection uses rules generated from heuristics. Our paper presents a novel approach to sarcasm detection: ‘looking at historical tweets for sarcasm detection of a target tweet’. It is similar to Wallace et al. (2015) in that it considers text apart from the target text. However, while they look at comments within a thread an"
W15-2905,W14-2609,0,0.00702725,"3) predict a tweet as sarcastic if there is a sentiment contrast between a verb and a noun phrase. 25 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 25–30, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. Figure 1: A motivating example for our approach Figure 2: Architecture of our sarcasm detection approach Similarly, supervised approaches implement sarcasm as a classification task that predicts whether a piece of text is sarcastic or not (Gonzalez-Ibanez et al., 2011; Barbieri et al., 2014; Carvalho et al., 2009). The features used include unigrams, emoticons, etc. Recent work in sarcasm detection deals with a more systematic feature design. Joshi et al. (2015) use a linguistic theory called context incongruity as a basis of feature design, and describe two kinds of features: implicit and explicit incongruity features. Wallace et al. (2015) uses as features beyond the target text as features. These include features from the comments and description of forum theme. In this way, sarcasm detection using ML-based classifiers has proceeded in the direction of improving the feature d"
W15-2905,W10-2914,0,0.541361,"rk by considering sentiment contrasts beyond the target tweet. Specifically, we look at tweets generated by the same author in the past (we refer to this as ‘historical tweets’). Consider the example in Figure 1. The author USER1 wrote the tweet ‘Nicki Minaj, don’t I hate her?!’. The author’s historical tweets may tell us that he/she has spoken positively about Nicki Minaj in the past. • Implicit Contrast: The tweet contains one word of a polarity, and a phrase of the other polarity. The implicit sentiment phrases are extracted from a set of sarcastic tweets as described in Tsur et al. (2010) Davidov et al. (2010). This is similar to implicit incongruity given by Joshi et al. (2015). For example, the sentence ‘I love being ignored.’ is predicted as sarcastic since it has a positive word 26 ‘love’ and a negative word ‘ignored’. We include rules to discount contrast across conjunctions like ‘but’ 3 . 4.2 2. If the historical tweets contained sarcasm towards the target phrase, and so did the target tweet, the predictor will incorrectly mark the tweet as non-sarcastic. Historical Tweet-based Predictor 3. If an entity mentioned in the target tweet never appeared in the author’s historical tweets, then no in"
W15-2905,P11-2102,0,0.460787,"Similarly, Riloff et al. (2013) predict a tweet as sarcastic if there is a sentiment contrast between a verb and a noun phrase. 25 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 25–30, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. Figure 1: A motivating example for our approach Figure 2: Architecture of our sarcasm detection approach Similarly, supervised approaches implement sarcasm as a classification task that predicts whether a piece of text is sarcastic or not (Gonzalez-Ibanez et al., 2011; Barbieri et al., 2014; Carvalho et al., 2009). The features used include unigrams, emoticons, etc. Recent work in sarcasm detection deals with a more systematic feature design. Joshi et al. (2015) use a linguistic theory called context incongruity as a basis of feature design, and describe two kinds of features: implicit and explicit incongruity features. Wallace et al. (2015) uses as features beyond the target text as features. These include features from the comments and description of forum theme. In this way, sarcasm detection using ML-based classifiers has proceeded in the direction of"
W15-2905,P15-2124,1,0.593536,"two components: a contrast-based predictor (that identifies if there is a sentiment contrast within a target tweet), and a historical tweet-based predictor (that identifies if the sentiment expressed towards an entity in the target tweet agrees with sentiment expressed by the author towards that entity in the past). 1 Introduction Sarcasm1 is defined as ‘the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone’s feelings or to criticize something in a humorous way’2 . An example of sarcasm is ‘Being stranded in traffic is the best way to start my week’(Joshi et al., 2015). There exists a sentiment contrast between the phrases ‘being stranded’ and ‘best way’ which enables an automatic sarcasm detection approach to identify the sarcasm in this sentence. Existing approaches rely on viewing sarcasm as a contrast in sentiment (Riloff et al., 2013; Maynard and Greenwood, 2014). However, consider the sentences ‘Nicki Minaj, don’t I hate her!’ or ‘I love spending four hours cooking on a weekend!’. The sarcasm is ambiguous because of a likely hyperbole in the first sentence, and because 2 1 We use irony and sarcasm interchangeably in this paper, as has been done in pas"
W15-2905,maynard-greenwood-2014-cares,0,0.130196,"entity in the past). 1 Introduction Sarcasm1 is defined as ‘the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone’s feelings or to criticize something in a humorous way’2 . An example of sarcasm is ‘Being stranded in traffic is the best way to start my week’(Joshi et al., 2015). There exists a sentiment contrast between the phrases ‘being stranded’ and ‘best way’ which enables an automatic sarcasm detection approach to identify the sarcasm in this sentence. Existing approaches rely on viewing sarcasm as a contrast in sentiment (Riloff et al., 2013; Maynard and Greenwood, 2014). However, consider the sentences ‘Nicki Minaj, don’t I hate her!’ or ‘I love spending four hours cooking on a weekend!’. The sarcasm is ambiguous because of a likely hyperbole in the first sentence, and because 2 1 We use irony and sarcasm interchangeably in this paper, as has been done in past work. Sarcasm has an element of criticism, while irony may not. 2 http://dictionary.cambridge.org/dictionary/british/sarcasm Related work Sarcasm detection relies mostly on rule-based algorithms. For example, Maynard and Greenwood (2014) predict a tweet as sarcastic if the sentiment embedded in a hasht"
W15-2905,P04-1035,0,0.0304567,"Missing"
W15-2905,D13-1066,0,0.457719,"Missing"
W15-5912,W05-0402,0,0.0946813,"Missing"
W15-5912,S15-2136,0,0.033932,"pression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal with a specific situation: Indomain data being difficult to obtain. Research in TER mostly deals with news domain text, arguably because of availability of large corpora and abundance of temporal expressions in news documents. In recent times, TER has also been applied to other domains like medical. Approaches for medical domain TER in the past have been either rule-based (Sohn"
W15-5912,S13-2002,0,0.108708,"anner, etc. Early work in TER considers it as a part of named entity recognition (Bikel et al., 1999). TER, as a separate task, was introduced as Temporal Expression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal with a specific situation: Indomain data being difficult to obtain. Research in TER mostly deals with news domain text, arguably because of availability of large corpora and abundance of temporal expressions in news d"
W15-5912,chang-manning-2012-sutime,0,0.130476,"ssed point in time, a duration or a frequency (Wikipedia, 2014). These expressions can be used in information extraction and question-answering to (a) answer time-specific queries, (b) arrange information in a chronological manner, etc. Early work in TER considers it as a part of named entity recognition (Bikel et al., 1999). TER, as a separate task, was introduced as Temporal Expression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal wi"
W15-5912,S10-1071,0,0.0942351,"Missing"
W15-5912,S10-1062,0,0.0344268,"., 1999). TER, as a separate task, was introduced as Temporal Expression Recognition and Normalization (TERN). TER in general domain has been widely studied. Rulebased methods for specific domains were adopted by popular systems like Heideltime (Str¨otgen and Gertz, 2010), SUTime (Chang and Manning, 2012), MayoTime (Sohn et al., 2013). The rules were regular expressions over word or tokens. Su84 pervised Classifiers like SVM, CRF using linguistic features have been explored (Adafre and de Rijke, 2005; Bethard, 2013). Joint inference-based classifiers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal with a specific situation: Indomain data being difficult to obtain. Research in TER mostly deals with news domain text, arguably because of availability of large corpora and abundance of temporal expressions in news documents. In recent times, TER has also been applied to other domains like medical. Approaches for medi"
W15-5945,P98-1004,0,0.0494915,"chine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignment"
W15-5945,J93-2003,0,0.0480692,"for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently occur in the training data. Hence, a need for specialized techniques that improve alignment quality has been felt (Sanchis and Snchez, 20"
W15-5945,P03-1012,0,0.0503326,"t multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in lo"
W15-5945,P08-1112,0,0.0229563,", statistical machine translation (SMT), and automatic construction of bilingual text. Statistical Machine Translation (SMT) is a technology for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently"
W15-5945,C04-1005,0,0.051151,"nments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Snchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the word may"
W15-5945,I13-2006,1,0.856431,"form word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Snchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the word may belong. This helps in disambiguation and thus selecting word correspondences but can also give rise to increased vocabulary thus making the training data more sparse. Joshi et al. (2013) use in domain parallel data to inject additional alignment mappings for the news headline domain. Finally, Koehn et al. (2007) propose a factored translation model that can incorporate any linguistic factors including POS information in phrase-based SMT. It provides a generalized representation of a translation model, because it can map multiple source and target factors. It may help to effectively handle out-of-vocabulary (OOV) by incorporating many linguistic factors, but it still crucially relies on the initial quality of word alignment that will dominate the translation probabilities. In"
W15-5945,P10-1155,1,0.670319,"y the topic model for the tourism dataset 5 Experiment Setup 2. Cartesian product Approach: In this approach the pseudo parallel data was created using MLTM approach described earlier, and added to the training data before training the MT systems. We added the pseudo parallel data to training data using the approach indicated in Figure 3. Thus, for 50 topics and 11 top words, we add 550 pseudo-parallel sentences, each of length 1. In this section, we describe the dataset, and setup for the experiments conducted. 5.1 Dataset For our experiments, we use corpora from health and tourism domain by Khapra et al. (2010). These datasets contain approximately 25000 parallel sentences for English - Hindi language pair. We use these for both the creation of pseudo parallel data, and training Machine translation systems. We separate 500 sentences each for testing and tuning purposes. We ensure that they are not present in the training corpus. 5.2 3. Sentential Approach: We added the pseudo parallel data created using MLTM approach to the training data using the approach indicated in Figure 4. Thus, for 50 topics and 11 top words, we add 50 pseudo-parallel sentences, each of length 11. Setup We implemented the mul"
W15-5945,P07-2045,0,0.0298051,"dely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently occur in the training data. Hence, a need for specialized techniques that improve alignment quality has been felt (Sanchis and Snchez, 2008; Lee et al., 2006; Koehn et al., 2007). Mimno et al. (2009) present a multilingual topic model called PolyLDA, and apply it for Machine Translation for European and other languages such as Danish, German, Greek, English, Spanish, etc. Since multilingual topic models generate parallel topics: parallel clusters of words that are likely to be about the same theme, these topics provide coarse alignment that a Moses-like translation system can leverage on. The idea is to not rely on any external ontology such as WordNet and to rely purely on a parallel corpus to create such coarse alignments. The focus of our paper is to improve word a"
W15-5945,J00-2004,0,0.0224318,"the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanc"
W15-5945,D09-1092,0,0.0304376,"Missing"
W15-5945,P00-1056,0,0.750202,"the top terms for a text classification task. They observe that parallel topics perform better than topic words that are translated into the target language. Approaches that do not rely on parallel corpus have also been reported. Jagarlamudi and Daum´e III (2010) use a bilingual dictionary, and a comparable corpora to estimate a model called JointLDA. Boyd-Graber and Blei (2009) use unaligned corpus and extract multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barb"
W15-5945,J03-1002,0,0.00636946,"ense disambiguation, statistical machine translation (SMT), and automatic construction of bilingual text. Statistical Machine Translation (SMT) is a technology for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between"
W15-5945,tufis-barbu-2002-lexical,0,0.0496959,"nd Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy,"
W15-5945,J97-3002,0,0.384589,"nment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word se"
W15-5945,C98-1004,0,\N,Missing
W16-0415,P06-4018,0,0.00513565,"We evaluate two key components of PIE model, namely, segregation of words and hierarchy of author-group distributions. Segregation of words: A key component of PIE model is the strategy to decide whether a word is an issue word or position word. We experiment with following alternatives to do this: (a) POSbased segregation as done in Fang et al. (2012) using twitter POS tagger Bontcheva et al. (2013). We experimentally determine the optimal split as: nouns as issue words, and adjectives, verbs and adverbs as position words, (b) POS-based+PMIbased collocation handler to include n-grams, using Bird (2006), (c) Subjectivity-based segregation classifies words present in the subjectivity word list by McAuley and Leskovec (2013) as position words, (d) POS+Subjectivity-based segregation where we first categorize nouns as issue words, and then look for other words in the subjectivity word list. In order to select the best strategy of segregation, we compute topic coherence metric Cv using Palmetto by R¨oder, Both, and Hinneburg Average cosine similarity With Without Within members of the same group Demo.-Demo. 0.261 0.253 Repub.-Repub. 0.014 0.013 Within members of different groups Demo.-Repub. 0.10"
W16-0415,R13-1011,0,0.0327122,"l discover? (Section 5.2) • Once we discovered political issues, positions and group-wise distribution, can the model be used to predict political affiliation? (Section 5.3) 5.1 Impact of Model Components on Performance We evaluate two key components of PIE model, namely, segregation of words and hierarchy of author-group distributions. Segregation of words: A key component of PIE model is the strategy to decide whether a word is an issue word or position word. We experiment with following alternatives to do this: (a) POSbased segregation as done in Fang et al. (2012) using twitter POS tagger Bontcheva et al. (2013). We experimentally determine the optimal split as: nouns as issue words, and adjectives, verbs and adverbs as position words, (b) POS-based+PMIbased collocation handler to include n-grams, using Bird (2006), (c) Subjectivity-based segregation classifies words present in the subjectivity word list by McAuley and Leskovec (2013) as position words, (d) POS+Subjectivity-based segregation where we first categorize nouns as issue words, and then look for other words in the subjectivity word list. In order to select the best strategy of segregation, we compute topic coherence metric Cv using Palmett"
W16-0415,N09-1054,0,0.0743337,"Missing"
W16-0415,D10-1006,0,0.0382301,"r tweet, and divide words into categories based on POS tags. Our model improves upon these two works in the following key ways: • A richer latent variable structure. In our model, the opinion words depend on BOTH topic and opinion latent variables (as opposed to only the latter). This structure allows our model to generate topics corresponding to political positions, which is not achieved in the past work. Related Work Our model is based on Latent Dirichlet Allocation (LDA) given by Blei, Ng, and Jordan (2003). Models based on LDA by Jo and Oh (2011), Mei et al. (2007), Lin and He (2009), and Zhao et al. (2010) present approaches to extract sentiment-coherent topics in datasets. The past work in analytics related to political opinion can be broadly classified in three categories. The first category predicts the outcome of an election. Gayo-Avello, Metaxas, and Mustafaraj (2011) predict the election outcome for a pair of Democrat and Republican candidates, while Metaxas, Mustafaraj, and Gayo-Avello (2011) aggregate sentiment in tweets in order to map it to votes. Conover et al. (2011) use a network graph of authors and their known political orientations, in order to predict the political orientation."
W16-2111,P15-2124,1,0.865158,"evaluation of the quality of sarcasm annotation, and the impact of this quality on sarcasm classification. This study forms a stepping stone towards systematic evaluation of quality of these datasets annotated by non-native annotators, and can be extended to other culture combinations. 1 Introduction Sarcasm is a linguistic expression where literal sentiment of a text is different from the implied sentiment, with the intention of ridicule (Schwoebel et al., 2000). Several data-driven approaches have been reported for computational detection of sarcasm (Tsur et al., 2010; Davidov et al., 2010; Joshi et al., 2015). As is typical of supervised approaches, they rely on datasets labeled with sarcasm. We refer to the process of creating such sarcasm-labeled datasets as sarcasm annotation. Linguistic studies concerning cross-cultural dependencies of sarcasm have been reported (Boers, 2003; Thomas, 1983; Tannen, 1984; Rockwell and Theriot, 2001; Bouton, 1988; Haiman, 1998; Dress et al., 2008). 2 Why is such an evaluation of quality important? To build NLP systems, creation of annotated corpora is common. When annotators are hired, factors such as language competence are considered. However, while tasks like"
W16-2111,D13-1066,0,0.227557,"Missing"
W16-2111,P06-4018,0,0.00947142,"Missing"
W16-2111,P13-1004,0,0.0256494,"es importance. Our work is the first-of-itskind study related to sarcasm annotation. Similar studies have been reported for related tasks. Hupont et al. (2014) deal with result of cultural differences on annotation of images with emotions. Das and Bandyopadhyay (2011) describe multi-cultural observations during creation of an emotion lexicon. For example, they state that the word ‘blue’ may be correlated to sadness in some cultures but to evil in others. Similar studies to understand annotator biases have been performed for subjectivity annotation (Wiebe et al., 1999) and machine translation (Cohn and Specia, 2013). Wiebe et al. (1999) show how some annotators may have individual biases towards a certain subjective label, and devise a method to obtain bias-corrected tags. Cohn and Specia (2013) consider annotator biases for the task of assigning quality scores to machine translation output. 3.3 Experiments The annotation experiment is conducted as follows. Our annotators read a unit of text, and determine whether it is sarcastic or not. The experiment is conducted in sessions of 50 textual units, and the annotators can pause anywhere through a session. This results in datasets where each textual unit ha"
W16-2111,walker-etal-2012-corpus,0,0.0302106,"lity, (b) degradation in annotation quality, (c) impact of quality degradation on sarcasm classification, and (c) prediction of disagreement. 3.1 4.1 3 Our Annotation Experiments Datasets We use two sarcasm-labeled datasets that have been reported in past work. The first dataset is Tweet-A. This dataset, introduced by Riloff et al. (2013), consists of 2278 manually labeled tweets, out of which 506 are sarcastic. We call these annotations American1. An example of a sarcastic tweet in this dataset is ‘Back to the oral surgeon #yay’. The second dataset is DiscussionA: This dataset, introduced by Walker et al. (2012), consists of 5854 discussion forum posts, out of which 742 are sarcastic. This dataset was created using Amazon Mechanical Turk. IP addresses of Turk workers were limited to USA during the experiment1 . We call these annotations American2. An example post here is: ‘A master baiter like you should present your thesis to be taken seriously. You haven’t and you aren’t.’. 3.2 What difficulties do our Indian annotators face? Table 1 shows examples where our Indian annotators face difficulty in annotation. We describe experiences from the experiments in two parts: 1. Situations in which they were u"
W16-2111,P11-4009,0,0.014095,"hree annotators create a sarcasm-labeled dataset. the host for a meal may be perceived as polite in some cultures, but sarcastic in some others. Due to popularity of crowd-sourcing, cultural background of annotators may not be known at all. Keeping these constraints in mind, a study of non-native annotation, and its effect on the corresponding NLP task assumes importance. Our work is the first-of-itskind study related to sarcasm annotation. Similar studies have been reported for related tasks. Hupont et al. (2014) deal with result of cultural differences on annotation of images with emotions. Das and Bandyopadhyay (2011) describe multi-cultural observations during creation of an emotion lexicon. For example, they state that the word ‘blue’ may be correlated to sadness in some cultures but to evil in others. Similar studies to understand annotator biases have been performed for subjectivity annotation (Wiebe et al., 1999) and machine translation (Cohn and Specia, 2013). Wiebe et al. (1999) show how some annotators may have individual biases towards a certain subjective label, and devise a method to obtain bias-corrected tags. Cohn and Specia (2013) consider annotator biases for the task of assigning quality sc"
W16-2111,P99-1032,0,0.378434,"Missing"
W16-2111,W10-2914,0,0.222093,"Missing"
W16-5001,baccianella-etal-2010-sentiwordnet,0,0.027947,"bels are determined based on hashtags, as stated above. The total number of distinct labels (L) is 3, and the total number of distinct sentiment (S) is 2. The total number of distinct topics (Z) is experimentally determined as 50. We use block-based Gibbs sampling to estimate the distributions. The block-based sampler samples all latent variables together based on their joint distributions. We set asymmetric priors based on sentiment word-list from McAuley and Leskovec (2013b). A key parameter of the model is ηw since it drives the split of a word as a topic or a sentiment word. SentiWordNet (Baccianella et al., 2010) is used to learn the distribution ηw prior to estimating the model. We average across multiple senses of a word. Based on the SentiWordNet scores to all senses of a word, we determine this probability. 6 Results Work day morning night today work Women Women Wife Compliment(s) Fashion Love Party Jokes Weather Quote Jokes Humor Comedy Satire Snow Today Rain Weather Day School Love Politics tomorrow school work morning night love feeling break-up day/night sleep Ukraine Russia again deeply raiders life friends night drunk parties Table 2: Topics estimated when the topic model is learned on only"
W16-5001,W14-2608,0,0.0162925,"reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is based on the idea of context incongruity. In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al. (2014) train their classifiers using features such as unigrams, laughter expressions, hyperbolic expressions, etc. Liebrecht et al. (2013) experiment with unigrams, bigrams and trigrams as features. To the best of our knowledge, past approaches for sarcasm detection do not use topic modeling, which we do. 3 Motivation Topic models enable discovery of thematic structures in a large-sized corpus. The motivation behind using topic models for sarcasm detection arises from two reasons: (a) presence of sarcasm-prevalent topics, and (b) differences in s"
W16-5001,P15-2124,1,0.942754,"ast work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group. Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is based on the idea of context incongruity. In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al. (2014) train their classifiers usi"
W16-5001,W16-0415,1,0.853443,"i-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group. Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is ba"
W16-5001,K16-1015,1,0.87285,"i-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group. Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is ba"
W16-5001,W15-2905,1,0.896339,"Missing"
W16-5001,W13-1605,0,0.0775844,"Missing"
W16-5001,P12-1036,0,0.439376,"he dataset and the experiment setup. Section 6 discusses the results in three steps: qualitative results, quantitative results and application of our topic model to sarcasm detection. Section 7 concludes the paper and points to future work. 2 Related Work Topic models are popular for sentiment aspect extraction. Jo and Oh (2011) present an aspect-sentiment unification model that learns different aspects of a product, and the words that are used to express sentiment towards the aspects. In terms of using two latent variables: one for aspect and one for sentiment, they are related to our model. Mukherjee and Liu (2012a) use a semi-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model"
W16-5001,P12-1034,0,0.112964,"he dataset and the experiment setup. Section 6 discusses the results in three steps: qualitative results, quantitative results and application of our topic model to sarcasm detection. Section 7 concludes the paper and points to future work. 2 Related Work Topic models are popular for sentiment aspect extraction. Jo and Oh (2011) present an aspect-sentiment unification model that learns different aspects of a product, and the words that are used to express sentiment towards the aspects. In terms of using two latent variables: one for aspect and one for sentiment, they are related to our model. Mukherjee and Liu (2012a) use a semi-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model"
W16-5001,D09-1026,0,0.0309919,"topic z and switch =0 (topic word), with prior γ χs Distribution over words given sentiment s and switch=1 (sentiment word), with prior δ1 χsz Distribution over words given a sentiment s and topic z and switch=1 (sentiment word), with prior δ2 ψl Distribution over sentiment given a label l and switch =1 (sentiment word), with prior β1 ψzl Distribution over sentiment given a label l and topic z and switch =1 (sentiment word), with prior β2 Table 1: Glossary of Variables/Distributions used 4 Sarcasm Topic Model 4.1 Design Rationale Our topic model requires sentiment labels of tweets, as used in Ramage et al. (2009). This sentiment can be positive or negative. However, in order to incorporate sarcasm, we re-organize the two sentiment values into three: literal positive, literal negative and sarcastic. The observed variable l in our model indicates this sentiment label. For sake of simplicity, we refer to the three values of l as positive, negative and sarcastic, in rest of the paper. Every word w in a tweet is either a topic word or a sentiment word. A topic word arises due to a topic, whereas a sentiment word arises due to combination of topic and sentiment. This notion is common to several sentiment-ba"
W16-5001,D13-1066,0,0.0698258,"Missing"
W16-5001,K16-1017,0,0.148106,"Missing"
W16-5001,P14-2084,0,0.0522059,"Missing"
W18-5911,D14-1162,0,0.0816606,"ilarity: For each word, we obtain similarity with ‘receive, ‘get’ and ‘take’, and use the highest similarity as this feature. We use pre-trained embeddings from Mikolov et al. (2013). This is to allow presence of words related to the act of receiving to be used as a signal for prediction; 1. Sentence Vector: 200 dimensions; Logistic Regression. (SV) 2. Dense Neural Network: 64 dimensions, 1 inter. layer + 5 epochs (DNN) 3. BiLSTM: GloVe840B + 3 epochs + 50 lstm units + 0.25 dropout (BiLSTM) 6. Sentence Vector: A sentence vector is computed as an average of word vectors using GloVe embeddings (Pennington et al., 2014); 4. CNN: GloVe840B + 5 epochs + 50 filters + 2 filter length + 0.75 dropout (CNN) 5. LSTM-LM: We pre-train a language model on the training dataset with a 3-layer LSTM. We then build a softmax layer on top of this pretrained LSTM, and fine-tune the neural network with supervision (Howard and Ruder, 2018). 7. Length: Number of characters and words; 8. Emotion: Word counts of each emotion category as given by SenticNet (Cambria et al., 2014). The combination of classifiers, misclassification costs and features has been experimentally validated. All models are implemented using TensorFlow (Abadi"
W18-5911,W17-5801,0,0.0297903,"shi1 Xiang Dai1,2 Sarvnaz Karimi1 Ross Sparks1 C´ecile Paris1 C Raina MacIntyre3 1 CSIRO Data61, Sydney, Australia, 2 University of Sydney, Sydney, Australia 3 The University of New South Wales, Sydney, Australia {aditya.joshi,dai.dai,sarvnaz.karimi}@csiro.au {ross.sparks,cecile.paris}@csiro.au, r.macintyre@unsw.edu.au Abstract cine. On the contrary, ‘Vaccines drastically reduce risks of infection’ is negative because the tweet describes vaccines but does not report a vaccine being administered. Past work in vaccination behaviour detection uses n-grams as features of a statistical classifier (Skeppstedt et al., 2017; Huang et al., 2017). However, alternatives to n-grams have shown promise in several Natural Language Processing (NLP) tasks. Therefore, we compare three typical NLP approaches for vaccination behaviour detection: rule-based, statistical and deep learning techniques. Our submissions to the shared task use statistical and deep learning-based text classification. The systems are trained on a concatenation of the training and the validation set. The work reported in this paper ranked first among nine teams, as communicated by the shared task committee. Vaccination behaviour detection deals with"
W18-5911,P04-3031,0,0.0352858,"ount Table 1: Features of the statistical approach. 2.2 support vector machine are summarised in Table 1. These features are: Rule-based Approach Since vaccination behaviour detection may appear to be only about detecting administration of a vaccine, we implement a na¨ıve method to detect vaccination behaviour. Our rule-based approach looks for words indicating ‘receive’ (without negation) to predict vaccination behaviour as follows: 1. Uni/Bigrams: Boolean; 2. Special Characters: A feature each indicating four special characters ?, #, @, ! 3. POS: Count of each POS tag using NLTK POS tagger (Bird and Loper, 2004). This feature follows the intuition that presence of certain POS tags such as verbs may serve as signals; 1. If a tweet contains one among the words ‘give’, ‘take’, ‘taking’, ‘gave’, ‘giving’, ‘get’, ‘getting’, ‘took’, ‘receive’ or ‘received’ and no negation word, predict the tweet as positive. 4. Negation: Presence of a negation word. This is to serve as a negation feature where, although the act of receiving a vaccine is mentioned, the negation word changes the output class; 2. Else, predict the tweet as negative. 2.3 Deep Learning-based Approach We experiment with five typical deep learnin"
W18-5911,P18-1031,0,0.02039,"r: 200 dimensions; Logistic Regression. (SV) 2. Dense Neural Network: 64 dimensions, 1 inter. layer + 5 epochs (DNN) 3. BiLSTM: GloVe840B + 3 epochs + 50 lstm units + 0.25 dropout (BiLSTM) 6. Sentence Vector: A sentence vector is computed as an average of word vectors using GloVe embeddings (Pennington et al., 2014); 4. CNN: GloVe840B + 5 epochs + 50 filters + 2 filter length + 0.75 dropout (CNN) 5. LSTM-LM: We pre-train a language model on the training dataset with a 3-layer LSTM. We then build a softmax layer on top of this pretrained LSTM, and fine-tune the neural network with supervision (Howard and Ruder, 2018). 7. Length: Number of characters and words; 8. Emotion: Word counts of each emotion category as given by SenticNet (Cambria et al., 2014). The combination of classifiers, misclassification costs and features has been experimentally validated. All models are implemented using TensorFlow (Abadi et al., 2016). The parameters are experimentally determined. 44 Approach F-Score Accuracy Approach F-Score Accuracy Skeppstedt et al. (2017) Huang et al. (2017) 76.84 77.64 87.01 87.65 Statistical LSTM-LM 86.06 88.74 85.71 89.44 Statistical Rule-based SV DNN BiLSTM CNN LSTM-LM 80.75 40.48 77.87 78.74 78."
W18-5911,P11-1015,0,0.021916,"Missing"
W19-1309,K16-1017,0,0.0606252,"e impact of different features for sarcasm/irony classification. Bouazizi and Ohtsuki (2016) propose a patternbased approach to detect sarcasm on Twitter. As deep learning techniques gain popularity, Ghosh and Veale (2016) propose a neural network semantic model for sarcasm detection. They use Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a fully connected layer. Poria et al. (2016) propose a novel method to detect sarcasm using CNN. They use a pre-trained CNN for extracting sentiment, emotion and personality features for sarcasm detection. Amir et al. (2016) propose a deep-learning-based architecture to incorporate additional context for sarcasm detection. They propose an approach to learn user embeddings to provide contextual features, going beyond the lexical and syntactic cues. Finally, they use these user embeddings for sarcasm detection. Zhang et al. (2016) use a bi-directional Gated Recurrent Unit (GRU) followed by a pooling neural network to detect sarcasm. Ghosh and Veale (2017) propose a neural architecture that considers the speaker’s mood on the basis of most recent prior tweets for sarcasm detection. Far´ıas et al. (2016) propose a 1."
W19-1309,P11-2102,0,0.596948,"stic text poses to sentiment analysis has led to research interest in com∗ 1 Equal Contribution. The work was done when authors were doing their Masters at IIT-Bombay. This refers to statistical approaches that do not rely on deep learning. 2 labels: sarcastic due to number, non-sarcastic. † 72 Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 72–80 c Minneapolis, June 6, 2019. 2019 Association for Computational Linguistics 3 putational sarcasm. While several approaches to detect sarcasm have been reported (Gonz´alezIb´an˜ ez et al., 2011; Joshi et al., 2015), they may fall short in case of sarcasm expressed via numbers. Consider the following three sentences: Related Work Sarcasm and irony detection has been extensively studied in linguistics, psychology, and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years (Joshi et al., 2017). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.), prosodic and contextual cues. Carvalho et al. (2009) use simple linguistic"
W19-1309,baccianella-etal-2010-sentiwordnet,0,0.091032,"Missing"
W19-1309,C18-1156,0,0.510074,"s in Section 4. Then, in Section 5, we present two deep learning-based approaches. In Section 6, we outline the experimental setup and present the results of our experiments in Section 7. We present both qualitative as well as quantitative error analysis in Section 8. Finally, we conclude the paper and discuss future work in Section 9. Introduction Sarcasm is a challenge to sentiment analysis because it uses verbal irony to express contempt or ridicule, thereby, potentially confusing typical sentiment classifiers. Several approaches for sarcasm detection have been reported in the recent past (Hazarika et al., 2018; Joshi et al., 2017; Ghosh and Veale, 2017; Buschmeier et al., 2014; Riloff et al., 2013). In this paper, we focus on a peculiar form of sarcasm: sarcasm expressed through numbers. In other words, the goal of this paper is the classification task where a tweet containing one or more numbers is classified as sar2 Motivation The challenge that sarcastic text poses to sentiment analysis has led to research interest in com∗ 1 Equal Contribution. The work was done when authors were doing their Masters at IIT-Bombay. This refers to statistical approaches that do not rely on deep learning. 2 labels:"
W19-1309,D13-1066,0,0.21924,"Missing"
W19-1309,P15-2124,1,0.959436,"o sentiment analysis has led to research interest in com∗ 1 Equal Contribution. The work was done when authors were doing their Masters at IIT-Bombay. This refers to statistical approaches that do not rely on deep learning. 2 labels: sarcastic due to number, non-sarcastic. † 72 Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 72–80 c Minneapolis, June 6, 2019. 2019 Association for Computational Linguistics 3 putational sarcasm. While several approaches to detect sarcasm have been reported (Gonz´alezIb´an˜ ez et al., 2011; Joshi et al., 2015), they may fall short in case of sarcasm expressed via numbers. Consider the following three sentences: Related Work Sarcasm and irony detection has been extensively studied in linguistics, psychology, and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years (Joshi et al., 2017). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.), prosodic and contextual cues. Carvalho et al. (2009) use simple linguistic features like an int"
W19-1309,S18-1005,0,0.0372944,"Missing"
W19-1309,W13-1605,0,0.292471,"Missing"
W19-1309,N16-1174,0,0.109314,"Missing"
W19-1309,P17-1155,0,0.0146795,"tector. Figure 1 shows the interfacing of our module with the overall sarcasm detection system. Figure 1: Interfacing of our module with the overall sarcasm detection system 3 Ivanko and Pexman (2003) describe the relationship between incongruity and sarcasm. 73 novel model using affective features based on a wide range of lexical resources available for English for detecting irony in tweets. Sulis et al. (2016) study the difference between sarcasm and irony in tweets. They propose a novel set of sentiment, structural and psycholinguistic features for distinguishing between irony and sarcasm. Peled and Reichart (2017) and Dubey et al. (2019) model sarcasm interpretation as a monolingual machine translation task. They use Moses4 , attention networks, and pointer generator networks for the task of sarcasm interpretation. Van Hee et al. (2018) present the first shared task in irony detection in tweets. Recently, Hazarika et al. (2018) propose a hybrid approach for sarcasm detection in online social media discussions. They extract contextual information from the discourse of a discussion thread. They also use user embeddings that encode stylometric and personality features of users and content-based feature ex"
W19-1309,C16-1231,0,0.138871,"Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a fully connected layer. Poria et al. (2016) propose a novel method to detect sarcasm using CNN. They use a pre-trained CNN for extracting sentiment, emotion and personality features for sarcasm detection. Amir et al. (2016) propose a deep-learning-based architecture to incorporate additional context for sarcasm detection. They propose an approach to learn user embeddings to provide contextual features, going beyond the lexical and syntactic cues. Finally, they use these user embeddings for sarcasm detection. Zhang et al. (2016) use a bi-directional Gated Recurrent Unit (GRU) followed by a pooling neural network to detect sarcasm. Ghosh and Veale (2017) propose a neural architecture that considers the speaker’s mood on the basis of most recent prior tweets for sarcasm detection. Far´ıas et al. (2016) propose a 1. This phone has an awesome battery backup of 38 hours 2. This phone has a terrible battery backup of 2 hours 3. This phone has an awesome battery backup of 2 hours At the time of writing this paper, a battery backup of 38 hours is good for phones while a battery backup of 2 hours is bad. Therefore, sentences"
W19-1309,D14-1162,0,0.0838694,"e user embeddings that encode stylometric and personality features of users and content-based feature extractors such as CNN and show a significant improvement in classification performance on a large Reddit corpus. 4 Figure 2: Rule-Based Approach that the 14th instance in the dataset is the sarcastic tweet ‘This phone has an awesome battery backup of 2 hours’. This tweet contains two noun phrases: ‘phone’ and ‘awesome battery backup’. The words in these two noun phrases are ‘phone’, ‘awesome’, ‘battery’, ‘backup’. We first convert these words into 200-D word vectors (initialized using GloVe (Pennington et al., 2014) and finetuned on our dataset). Then we sum up word vectors of words in the noun phrase list and normalize them by the length of the noun phrase list. We call this the noun phrase vector. Given these entities, the tweet representation is: (14, Noun Phrase Vector, 2, ‘hours’). Since the tweet is sarcastic, it is stored in the sarcastic repository. In addition to tweet entries, both sarcastic and non-sarcastic repositories also maintain two dictionaries: (a) Dictionary of mean values where each entry is a key-value pair where key is the unit of measurement and value is the average of all the num"
W19-1309,C16-1151,0,0.0852134,"sentiment word and a negative situation. Joshi et al. (2015) show how sarcasm arises because of implicit or explicit incongruity in the sentence. Buschmeier et al. (2014) analyze the impact of different features for sarcasm/irony classification. Bouazizi and Ohtsuki (2016) propose a patternbased approach to detect sarcasm on Twitter. As deep learning techniques gain popularity, Ghosh and Veale (2016) propose a neural network semantic model for sarcasm detection. They use Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a fully connected layer. Poria et al. (2016) propose a novel method to detect sarcasm using CNN. They use a pre-trained CNN for extracting sentiment, emotion and personality features for sarcasm detection. Amir et al. (2016) propose a deep-learning-based architecture to incorporate additional context for sarcasm detection. They propose an approach to learn user embeddings to provide contextual features, going beyond the lexical and syntactic cues. Finally, they use these user embeddings for sarcasm detection. Zhang et al. (2016) use a bi-directional Gated Recurrent Unit (GRU) followed by a pooling neural network to detect sarcasm. Ghosh"
W19-5015,C18-1139,0,0.0246372,"attention mechanism to incorporate information about the order and the collection of words (Vaswani et al., 2017). The pre-trained model of USE that returns a vector of 512 dimensions is also available on Tensorflow Hub; (C) Neural-Net Language Model (NNLM) by Bengio et al. (2003): The model simultaneously learns representations of words and probability functions for word sequences, allowing it to capture semantics of a sentence. We use a pre-trained model available on Tensorflow Hub, that is trained on the English Google News 200B corpus, and computes a vector of 128 dimensions; (D) FLAIR by Akbik et al. (2018): This library by Zalando research3 uses character-level language models to learn contextualised representations. We use the pooling option to create sentence vectors. This is a concatenation of GloVe embeddings and the forward/backward language model. The resultant is a vector of 4196 dimensions. Table 1 refers to the four configurations as ELMo, USE, NNLM and FLAIR respectively. Table 2: Dataset statistics. pus from an unrelated domain (news and general, in the case of Word2Vec and GloVe respectively), they may not capture the semantics of the domain of the specific classification problem. T"
W19-5015,D11-1145,0,0.0126458,"ns used in our experiments. natives that have been used for several text classification problems in NLP: word-based representations and context-based representations. They are summarised in Table 1, and described in the following subsections. and Priego, 2017) and sentence similarity detection (Fu et al., 2016). In terms of the biomedical domain, word embedding-based features have been used for entity extraction in biomedical corpora (Yadav et al., 2017) or clinical information extraction (Kholghi et al., 2016). Several approaches for personal health mention classification have been reported (Aramaki et al., 2011; Lamb et al., 2013a; Yin et al., 2015). Aramaki et al. (2011) use bag-of-words as features for personal health mention classification. Lamb et al. (2013a) use linguistic features including coarse topic-based features, while Yin et al. (2015) use features based on parts-of-speech and dependencies for a statistical classifier. Feng et al. (2018) compare statistical classifiers with deep learning-based classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et a"
W19-5015,S16-1098,0,0.0354512,"Missing"
W19-5015,K15-1027,0,0.0306613,"Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 2016), sentiment analysis (Zhang et al., 2015; Tkachenko et al., 2018), co-reference resolution (Simova and Uszkoreit, 2017), grammatical error correction (Chou et al., 2016), emotion intensity determination (Buscaldi 1 Content words refers to all words except stop words. 135 Proceedings of the BioNLP 2019 workshop, pages 135–141 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Word-based Context-based Representation Details A tweet vector is the average of the vectors of the content words in the tweet. Word2Vec PreTrain, Vect"
W19-5015,W17-5236,0,0.0297775,"ov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 201"
W19-5015,W16-2917,0,0.0246815,"100) 0.070) 0.116) 0.7814 (σ: 0.02) 0.8155 (σ: 0.030) 0.7495 (σ: 0.020) 0.7896 (σ: 0.031) (B) Context-based Representations ELMo USE NNLM FLAIR 1024 512 128 4196 0.8010 (σ: 0.8164 (σ: 0.8520 (σ: 0.8000 (σ: 0.021) 0.008) 0.006) 0.021) 0.7724 (σ: 0.7790 (σ: 0.7610 (σ: 0.7667 (σ: Table 3: Comparison of five word-based representations with four context-based representations; Average accuracy with standard deviation (σ) indicated in brackets. 5 detect whether or not a tweet describes the usage of a medicinal drug (‘I took some painkillers this morning’, for example). We use the dataset provided by Jiang et al. (2016); (C) Personal Health Mention classification (PHMC): A personal health mention is a person’s report about their illness. We use the dataset provided by Robinson et al. (2015). For example ‘I have been sick for a week now’ is a personal health mention while ‘Rollercoasters can make you sick’ is not. It must be noted that IIC involves influenza while the PHMC dataset covers a set of illnesses as described later. We first present a quantitative evaluation to compare the two types of representations. Following that, we analyse sources of errors. 5.1 Quantitative Evaluation We compare word-based an"
W19-5015,D16-1104,1,0.84456,"mbeddings’) are dense, real-valued vectors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems:"
W19-5015,D18-2029,0,0.0160858,"IIC DUC PHMC 9,006 (2,306) 13,409 (3,167) 2,661 (1,304) words in the sentence, they compute a vector for sentences on the whole, by taking into account the order of words and the set of co-occurring words. We experiment with four deep contextualised vectors: (A) Embeddings from Language Models (ELMo) by Peters et al. (2018): ELMo uses character-based word representations and bidirectional LSTMs. The pre-trained model computes a contextualised vector of 1024 dimensions. ELMo is available in the Tensorflow Hub2 , a repository of machine learning modules; (B) Universal Sentence Encoder (USE) by Cer et al. (2018): The encoder uses a Transformer architecture that uses attention mechanism to incorporate information about the order and the collection of words (Vaswani et al., 2017). The pre-trained model of USE that returns a vector of 512 dimensions is also available on Tensorflow Hub; (C) Neural-Net Language Model (NNLM) by Bengio et al. (2003): The model simultaneously learns representations of words and probability functions for word sequences, allowing it to capture semantics of a sentence. We use a pre-trained model available on Tensorflow Hub, that is trained on the English Google News 200B corpus"
W19-5015,W16-4910,0,0.0292297,"e, real-valued vectors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation"
W19-5015,U16-1003,0,0.0689249,"Missing"
W19-5015,Y15-1013,0,0.021879,"s for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-trained on a corpus of tweet"
W19-5015,C18-1152,0,0.01679,"mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-trained on a corpus of tweets with 27 billion tokens"
W19-5015,N13-1097,0,0.122449,"ents. natives that have been used for several text classification problems in NLP: word-based representations and context-based representations. They are summarised in Table 1, and described in the following subsections. and Priego, 2017) and sentence similarity detection (Fu et al., 2016). In terms of the biomedical domain, word embedding-based features have been used for entity extraction in biomedical corpora (Yadav et al., 2017) or clinical information extraction (Kholghi et al., 2016). Several approaches for personal health mention classification have been reported (Aramaki et al., 2011; Lamb et al., 2013a; Yin et al., 2015). Aramaki et al. (2011) use bag-of-words as features for personal health mention classification. Lamb et al. (2013a) use linguistic features including coarse topic-based features, while Yin et al. (2015) use features based on parts-of-speech and dependencies for a statistical classifier. Feng et al. (2018) compare statistical classifiers with deep learning-based classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word"
W19-5015,N15-1184,0,0.0722086,"Missing"
W19-5015,E17-1109,0,0.124235,"ddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 2016), sentiment analysis (Zhang et al., 2015; Tkac"
W19-5015,W18-4002,0,0.0190136,"th deep learning-based classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has b"
W19-5015,W18-4414,0,0.0190502,"sed classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-traine"
W19-5015,S15-2094,0,0.030168,"ns (also known as ‘embeddings’) are dense, real-valued vectors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used fo"
W19-5015,D14-1162,0,0.0829765,"Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-trained on a corpus of tweets with 27 billion tokens, resulting in 200-dimensional vectors. Representations A tweet vector is a distributed representation of a tweet, and is computed for every tweet in the training set. The tweet vector along with the output label is then used to train the statistical classification model. The intuition is that the tweet vector captures the semantics of the tweet and, as a result, can be effectively used for classification. To obtain tweet vectors, we experiment with two alter2. Embeddings Trained on The Training Split: It may be argued tha"
W19-5015,N18-1202,0,0.0398235,"of the tweet and, as a result, can be effectively used for classification. To obtain tweet vectors, we experiment with two alter2. Embeddings Trained on The Training Split: It may be argued that, since the pretrained embeddings are learned from a cor136 Classification # tweets (# true tweets) IIC DUC PHMC 9,006 (2,306) 13,409 (3,167) 2,661 (1,304) words in the sentence, they compute a vector for sentences on the whole, by taking into account the order of words and the set of co-occurring words. We experiment with four deep contextualised vectors: (A) Embeddings from Language Models (ELMo) by Peters et al. (2018): ELMo uses character-based word representations and bidirectional LSTMs. The pre-trained model computes a contextualised vector of 1024 dimensions. ELMo is available in the Tensorflow Hub2 , a repository of machine learning modules; (B) Universal Sentence Encoder (USE) by Cer et al. (2018): The encoder uses a Transformer architecture that uses attention mechanism to incorporate information about the order and the collection of words (Vaswani et al., 2017). The pre-trained model of USE that returns a vector of 512 dimensions is also available on Tensorflow Hub; (C) Neural-Net Language Model (N"
W19-5015,simova-uszkoreit-2017-word,0,0.0311253,"ors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al."
W19-5015,P18-1112,0,0.0143197,"2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 2016), sentiment analysis (Zhang et al., 2015; Tkachenko et al., 2018), co-reference resolution (Simova and Uszkoreit, 2017), grammatical error correction (Chou et al., 2016), emotion intensity determination (Buscaldi 1 Content words refers to all words except stop words. 135 Proceedings of the BioNLP 2019 workshop, pages 135–141 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Word-based Context-based Representation Details A tweet vector is the average of the vectors of the content words in the tweet. Word2Vec PreTrain, Vectors of the content words are obtained from preGloVe PreTrain trained embeddings from Word2Vec & GloVe res"
