2020.acl-main.257,Q17-1002,0,0.0183207,"t activate selectively or more strongly for a particular function such as modalityspecific or category-specific semantics (such as objects/actions, abstract/concrete, animate/inanimate, animals, fruits/vegetables, colours, body parts, countries, flowers, etc.) (Warrington, 1975; Warrington and McCarthy, 1987; McCarthy and Warrington, 1988). This indicates a function-specific 2873 division of lower-level semantic processing. Singlespace distributional word models have been found to partially correlate to these distributed brain activity patterns (Mitchell et al., 2008; Huth et al., 2012, 2016; Anderson et al., 2017), but fail to explain the full spectrum of fine-grained word associations humans are able to make. Our work has been partly inspired by this literature. Compositional Distributional Semantics. Partially motivated by similar observations, prior work frequently employs tensor-based methods for composing separate tensor spaces (Coecke et al., 2010): there, syntactic categories are often represented by tensors of different orders based on assumptions on their relations. One fundamental difference is made between atomic types (e.g., nouns) versus compositional types (e.g., verbs). Atomic types are"
2020.acl-main.257,J10-4006,0,0.0840012,"th neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed da"
2020.acl-main.257,D08-1007,0,0.0581951,"Missing"
2020.acl-main.257,P09-1068,0,0.0571221,"additionally evaluate our models on a number of other established datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this t"
2020.acl-main.257,P10-1046,0,0.060836,"usibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc ,"
2020.acl-main.257,D14-1082,0,0.00762763,"igh similarity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,W09-0211,0,0.0610713,"Missing"
2020.acl-main.257,D14-1004,0,0.0502357,"Missing"
2020.acl-main.257,W16-1605,0,0.0209594,"ile effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enab"
2020.acl-main.257,Q17-1010,0,0.285232,"V), I(O)). The space is optimised such that vectors for plausible SVO compositions will be close. Note that one word can have several vectors, for example chicken can occur both as S and O. Introduction Word representations are in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The"
2020.acl-main.257,J10-4007,0,0.031176,"Missing"
2020.acl-main.257,P19-1318,0,0.0205895,"actorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vectors encode knowledge that is often complementary to what is coded in word vectors. Rei et al. (2018) and Vuli´c and Mrkˇsi´c (2018) described related task-dependent neural nets for mapping word embeddings into relation-specific spaces for scoring lexical entailment. In this work, we propose a task-independent approach and extend it to work with a variable number of relations. Neuroscience. Theories from cognitive linguistics and neuroscience reveal that single-space repre"
2020.acl-main.257,W15-1106,0,0.122987,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,N15-1003,0,0.119856,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,D11-1129,0,0.0855066,"Missing"
2020.acl-main.257,W11-2507,0,0.311832,"SVO study (V) researcher (S), scientist (S), subject (O), art (O) eat (V) food (O), cat (S), dog (S) need (V) help (O), implementation (S), support (O) Table 1: Nearest neighbours in a function-specific space trained for the SVO structure. In the Joint SVO space (bottom) we show nearest neighbors for verbs (V) from the two other subspaces (O and S). Mann and Ruhlen, 2011). In language, this event understanding information is typically captured by the SVO structures and, according to the cognitive science literature, is well aligned with how humans process sentences (McRae et al., 1997, 1998; Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014); it reflects the likely distinct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation"
2020.acl-main.257,P14-2050,0,0.194288,"and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vec"
2020.acl-main.257,P16-1020,0,0.018132,"nal solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enable a better model of associations between concepts and consequently improved event representations by encoding the relevant information directly into the parameters for each word during training. Word vectors offer several advantages over tensors: a large reduction in parameters and fixed dimensionality across concepts. This facilitates their reuse and transfer across different tasks. For this reason, we find our multidirection"
2020.acl-main.257,N16-1118,0,0.0466753,"Missing"
2020.acl-main.257,J15-4004,1,0.911699,"in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as i"
2020.acl-main.257,C12-2054,0,0.0226717,"tandalone: their meaning is independent from other types. On the other hand, verbs are compositional as they rely on their subjects and objects for their exact meaning. Due to this added complexity, the compositional types are often represented with more parameters than the atomic types, e.g., with a matrix instead of a vector. The goal is then to compose constituents into a semantic representation which is independent of the underlying grammatical structure. Therefore, a large body of prior work is concerned with finding appropriate composition functions (Grefenstette and Sadrzadeh, 2011a,b; Kartsaklis et al., 2012; Milajevs et al., 2014) to be applied on top of word representations. Since this approach represents different syntactic structures with tensors of varying dimensions, comparing syntactic constructs is not straightforward. This compositional approach thus struggles with transferring the learned knowledge to downstream tasks. State-of-the-art compositional models (Tilk et al., 2016; Weber et al., 2018) combine similar tensor-based approaches with neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of"
2020.acl-main.257,P08-1028,0,0.227251,"Missing"
2020.acl-main.257,K16-1008,0,0.0310574,"tablished datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this task function-specific representations a"
2020.acl-main.257,Q17-1022,1,0.909677,"Missing"
2020.acl-main.257,L16-1262,0,0.0649244,"Missing"
2020.acl-main.257,D14-1162,0,0.108435,"arity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,K15-1026,1,0.838922,"across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as illustrated by Figure 1."
2020.acl-main.257,D16-1017,0,0.246401,"nct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram"
2020.acl-main.257,N18-1103,1,0.908435,"Missing"
2020.acl-main.257,P18-2101,1,0.90757,"Missing"
2020.acl-main.257,P99-1014,0,0.453354,"Missing"
2020.acl-main.257,W16-2518,0,0.142282,"itional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspe"
2020.cl-3.4,P16-1223,0,0.0323852,"Missing"
2020.cl-3.4,P13-1025,0,0.0324815,"1995; Wardhaugh 2011), but this claim was not put to test in a realworld setting such as those we are testing in. Granted, understanding character from language and predicting actions from language are quite different. However, if it is the case that neural networks could learn a character-like context using the final action as the supervision sign, it could have substantial implications for language processing and even the social sciences. In the emerging field of computational social science, there is a substantial effort to harness linguistic signals to better answer scientific questions (Danescu-Niculescu-Mizil et al. 2013). This approach, also known as text-as-data, has led to many advancements in the prediction of stock prices (Kogan et al. 2009), understanding of political discourse (Field et al. 2018), and analysis of court decisions (Goldwasser and Daum´e III 2014; Sim, Routledge, and Smith 2016). Our work adds another facet to this literature, trying to identify textual signals that enable the prediction of actions that are not explicitly mentioned in the text. 2.3 Prediction and Analysis in Basketball Basketball is at the forefront of sports analytics. In recent decades, there have been immense efforts to"
2020.cl-3.4,N19-1423,0,0.32162,"e (Gramacy, Taddy, and Tian 2017), which constitute estimating the difference they make on final game outcomes. However, in this research we decided to focus on metrics that can be attributed to specific types of decisions and not to overall game outcomes. 669 Computational Linguistics Volume 46, Number 3 term memory (LSTM) (Hochreiter and Schmidhuber 1997) and convolutional neural network (CNN) (LeCun et al. 1998). Finally, to better model the interview structure and to take advantage of recent advancements in contextual embeddings, we also use a BERT-based architecture (Vaswani et al. 2017; Devlin et al. 2019) and explore the tradeoff between a light-weight attention mechanism and more parameter-heavy alternatives (Section 5). Our results (Sections 6, 7) suggest that our text-based models are able to learn from the interviews to predict the player’s performance metrics, while the performancebased baselines are not able to predict much better than a coin flip or the most common class, a phenomenon we try to explain in Section 7. Interestingly, the models that exploit both the textual signal and the signal from past performance metrics improve on some of the most challenging predictions. These result"
2020.cl-3.4,D18-1393,0,0.0165179,"quite different. However, if it is the case that neural networks could learn a character-like context using the final action as the supervision sign, it could have substantial implications for language processing and even the social sciences. In the emerging field of computational social science, there is a substantial effort to harness linguistic signals to better answer scientific questions (Danescu-Niculescu-Mizil et al. 2013). This approach, also known as text-as-data, has led to many advancements in the prediction of stock prices (Kogan et al. 2009), understanding of political discourse (Field et al. 2018), and analysis of court decisions (Goldwasser and Daum´e III 2014; Sim, Routledge, and Smith 2016). Our work adds another facet to this literature, trying to identify textual signals that enable the prediction of actions that are not explicitly mentioned in the text. 2.3 Prediction and Analysis in Basketball Basketball is at the forefront of sports analytics. In recent decades, there have been immense efforts to document every aspect of the game in real-time, and currently for every game there are data capturing each play’s result, player and ball movements, and even crowd generated noise. Res"
2020.cl-3.4,E14-1069,0,0.0632571,"Missing"
2020.cl-3.4,D14-1181,0,0.0112135,"ts for the fact that some words are more frequent in general, but makes the same assumptions. We report results for the RF classifier for both BoW (unigrams) and TFIDF (unigrams + bigrams) feature sets, because these consistently performed better in development data experiments, for BoW and TFIDF respectively. Finally, because these simple models were consistently outperformed by our best text-based DNN models (see Section 7), we did not attempt to incorporate any performance metrics as features into them. 5.3 Deep Neural Networks DNNs have proven effective for many text classification tasks (Kim 2014; Ziser and Reichart 2018). An appealing property of these models is that training a DNN using a supervision signal results not only in a predictive model, but also with a representation of the data in the context of the supervision signal. This is especially intriguing in our case, where the supervision signal is not clearly visible in the text, and is more related to its speaker. Moreover, the text in our task is structured as a dialog between two speakers, which entails an additional level of contextual dependence between speakers, on top of the internal linguistic structures of the utteran"
2020.cl-3.4,N09-1031,0,0.117968,"Missing"
2020.cl-3.4,E14-1056,0,0.0742442,"Missing"
2020.cl-3.4,D15-1166,0,0.066928,"Missing"
2020.cl-3.4,D18-1290,1,0.808602,"Missing"
2020.cl-3.4,P15-1159,0,0.0244213,"also extract information regarding the text author. We hope this research problem and the highlevel topic will be of interest to the NLP community. To facilitate further research we also release our data and code. 2. Related Work Previous work on the intersection of language, behavior, and sports is limited because of the rarity of relevant textual data (Xu, Yu, and Hoi 2015). However, there is an abundance of research on predicting human decision-making (e.g., Hartford, Wright, and Leyton-Brown 2016; Plonsky et al. 2017; Rosenfeld and Kraus 2018), on using language to predict human behavior (Niculae et al. 2015; Sim, Routledge, and Smith 670 Oved, Feder, and Reichart Predicting Actions from Interviews 2016), and on predicting outcomes in basketball (Cervone et al. 2014; Ganguly and Frank 2018). Because we aim to bridge the gap between the different disciplines, we survey the relevant work in each. 2.1 Prediction and Decision-Making Previous decision-making work is both theoretical—modeling the incentives individuals face and the equilibrium observed given their competing interests (Gilboa 2009), and empirical—aiming to disentangle causal relationships that can shed light on what could be driving act"
2020.cl-3.4,W02-1011,0,0.0266297,"Missing"
2020.cl-3.4,D14-1162,0,0.0820533,"Missing"
2020.cl-3.4,N18-1202,0,0.017209,"oreover, the text in our task is structured as a dialog between two speakers, which entails an additional level of contextual dependence between speakers, on top of the internal linguistic structures of the utterances produced by the individual speakers. These factors pose a difficult challenge from a modeling perspective, yet DNNs are known for their architectural flexibility that allows learning a joint representation for more than one sequence (Chen, Bolton, and Manning 2016), and have shown promising performance in different tasks where models attempt to capture nuanced phenomena in text (Peters et al. 2018). 685 Computational Linguistics Volume 46, Number 3 We consider three models that excel on text classification tasks: CNN (Kim 2014), BiLSTM (Hochreiter and Schmidhuber 1997), and BERT (Devlin et al. 2019). In order to obtain a vectorized representation of an interview’s text, we used different text embedding techniques per model, each based on different pre-trained embedding models. Below we describe the various models. 5.3.1 The CNN Model. Motivation. We implement a standard word-level CNN model for text classification (CNN-T), closely following the implementation described in Kim (2014). Th"
2020.cl-3.4,N16-3020,0,0.0866149,"Missing"
2020.cl-3.4,P18-1084,1,0.834912,"as sentiment analysis, stance classification, and intent detection that also extract information regarding the text author. We hope this research problem and the highlevel topic will be of interest to the NLP community. To facilitate further research we also release our data and code. 2. Related Work Previous work on the intersection of language, behavior, and sports is limited because of the rarity of relevant textual data (Xu, Yu, and Hoi 2015). However, there is an abundance of research on predicting human decision-making (e.g., Hartford, Wright, and Leyton-Brown 2016; Plonsky et al. 2017; Rosenfeld and Kraus 2018), on using language to predict human behavior (Niculae et al. 2015; Sim, Routledge, and Smith 670 Oved, Feder, and Reichart Predicting Actions from Interviews 2016), and on predicting outcomes in basketball (Cervone et al. 2014; Ganguly and Frank 2018). Because we aim to bridge the gap between the different disciplines, we survey the relevant work in each. 2.1 Prediction and Decision-Making Previous decision-making work is both theoretical—modeling the incentives individuals face and the equilibrium observed given their competing interests (Gilboa 2009), and empirical—aiming to disentangle cau"
2020.cl-3.4,W14-3110,0,0.0605497,"Missing"
2020.cl-3.4,D16-1178,0,0.0553739,"Missing"
2020.cl-3.4,P16-2038,0,0.0200886,"contextual embeddings of the question and the answer. This serves as an attempt to account for the subtler context a question induces over an answer, and for the role of each speaker in the dialog. These text-based models exhibit the predictive power of text alone in our prediction task. Combined models. DNNs tranform their input signals into vectors and their computations are hence based on matrix calculations. This shared representation of various input signals makes these models highly suitable to multitask and crossmodal learning, as has been shown in a variety of recent NLP works (e.g., Søgaard and Goldberg 2016; Rotman, Vuli´c, and Reichart 2018; Malca and Reichart 2018). We therefore implemented variants of our best performing LSTM and BERT text-based models that incorporate textual features from the pre-game interview with performance metrics from the previous three time steps. These models help us quantify the marginal effect of adding textual features in predicting the direction of the deviation from the player’s mean performance, over metric-based models. We next describe each of our models in details. 5.1 Metric-Based Autoregressive Models An autoregressive (AR(k)) model is a representation of"
2020.cl-3.4,N16-1174,0,0.0988981,"l dependencies within each pair of a question and its immediate answer. In future work we plan to further explore the interview structure in our modeling. Interview Representation. Our CNN model treats an interview as a single sequence of words, and our BiLSTM model treats an interview as a single sequence of sentences where each sentence is represented by the average of its word embeddings. Neither of these models take into account any other characteristics of the interview structure. Although there are CNN and LSTM-based models that aim to capture document structure, for example, hierarchy (Yang et al. 2016), adapting these to capture the subtleties of an interview structure is a non-trivial task. BERT provides a method for producing a single joint contextual representation for two related text sequences (such as Q-A pairs), which attempts to represent both texts and the relations between them. We found this feature useful for our task and a natural fit for modeling interview structure, as it allowed us to break up each interview to its Q-A pairs, input them in sequence to BERT, and produce a respective sequence of Q-A vectors. We follow a similar method for producing Q-A vectors as described in"
2020.cl-3.4,N18-1112,1,0.834201,"fact that some words are more frequent in general, but makes the same assumptions. We report results for the RF classifier for both BoW (unigrams) and TFIDF (unigrams + bigrams) feature sets, because these consistently performed better in development data experiments, for BoW and TFIDF respectively. Finally, because these simple models were consistently outperformed by our best text-based DNN models (see Section 7), we did not attempt to incorporate any performance metrics as features into them. 5.3 Deep Neural Networks DNNs have proven effective for many text classification tasks (Kim 2014; Ziser and Reichart 2018). An appealing property of these models is that training a DNN using a supervision signal results not only in a predictive model, but also with a representation of the data in the context of the supervision signal. This is especially intriguing in our case, where the supervision signal is not clearly visible in the text, and is more related to its speaker. Moreover, the text in our task is structured as a dialog between two speakers, which entails an additional level of contextual dependence between speakers, on top of the internal linguistic structures of the utterances produced by the indivi"
2020.cl-4.5,E17-1088,0,0.0206243,"-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steerin"
2020.cl-4.5,D18-1214,0,0.0617967,"Missing"
2020.cl-4.5,P18-1073,0,0.159528,"to principal component analysis) from the input distributional word vectors, since they do not contribute toward distinguishing the actual semantic meaning of different words. The method contains a single (tunable) hyperparameter ddA , which denotes the number of the dominating directions to remove from the initial representations. Previous work has verified the usefulness of ABTT in several English lexical semantic tasks such as semantic similarity, word analogies, and concept categorization, as well as in sentence-level text classification tasks (Mu, Bhat, and Viswanath 2018). (3) UNCOVEC (Artetxe et al. 2018) adjusts the similarity order of an arbitrary input word embedding space, and can emphasize either syntactic or semantic information in the transformed vectors. In short, it transforms the input space X into an adjusted space XWα through a linear map Wα controlled by a single hyperparameter α. The nth -order similarity transformation of the input word vector space X (for which n = 1) can be obtained as M n (X) = M 1 (XW (n − 1)/2 ), with Wα = QΓ α , where Q and Γ are the matrices obtained via eigendecomposition of X T X = QΓQT . Γ is a diagonal matrix containing eigenvalues of X T X; Q is an o"
2020.cl-4.5,P98-1013,0,0.103027,"Missing"
2020.cl-4.5,J82-2005,0,0.629873,"Missing"
2020.cl-4.5,L18-1618,0,0.021365,"999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 2008). For instance, the two senses that are distinguished in English as time and weather are co-lexified in Croatian: the word vrijeme is used in both cases. 854 Vuli´c et al. Multi-SimLex resolution of translat"
2020.cl-4.5,N18-1083,0,0.0292232,"ially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents"
2020.cl-4.5,J19-2006,0,0.0292702,"es to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents share the same (func"
2020.cl-4.5,E17-2036,0,0.0144067,"2) Source: SemEval-17: Task 2 (henceforth SEMEVAL-500; Camacho-Collados et al. 2017). We start from the full data set of 500 concept pairs to extract a total of 334 concept pairs for English Multi-SimLex a) which contain only single-word concepts, b) which are not named entities, c) where POS tags of the two concepts are the same, d) where both concepts occur in the top 250K most frequent word types in the English Wikipedia, and e) which do not already occur in SimLex-999. The original concepts were sampled as to span all the 34 domains available as part of BabelDomains (Camacho-Collados and Navigli 2017), which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample. 3) Source: CARD-660 (Pilehvar et al. 2018). Sixty-seven word pairs are taken from this data set focused on rare word similarity, applying the same selection criteria a to e utilized for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus (Baroni et al. 2009). CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia), and slang (2mrw), which might be difficult to tr"
2020.cl-4.5,S17-2002,0,0.0611659,"uli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 849 Computational Linguistics Volume 46, Numbe"
2020.cl-4.5,D14-1082,0,0.0211604,"Missing"
2020.cl-4.5,2020.acl-main.747,0,0.168947,"Missing"
2020.cl-4.5,D18-1269,0,0.383476,"the coverage also to languages that are resourcelean and/or typologically diverse (e.g., Welsh, Kiswahili, as in this work). Multilingual Data Sets for Natural Language Understanding. The Multi-SimLex initiative and corresponding data sets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual BERT (Devlin et al. 2019) or XLM (Conneau and Lample 2019) are typically probed on XNLI test data (Conneau et al. 2018b) for crosslingual natural language inference. XNLI was created by translating examples from the English MultiNLI data set, and projecting its sentence labels (Williams, Nangia, and Bowman 2018). Other recent multilingual data sets target the task of question answering based on reading comprehension: i) MLQA (Lewis et al. 2019) includes 7 languages; ii) XQuAD (Artetxe, Ruder, and Yogatama 2019) 10 languages; and iii) TyDiQA (Clark et al. 2020) 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English data set, TyDiQA was built independen"
2020.cl-4.5,Q19-1041,1,0.810306,"ingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 diffe"
2020.cl-4.5,C18-1323,0,0.354998,"6), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 8"
2020.cl-4.5,D19-1006,0,0.0554256,"n 2019). 875 Computational Linguistics Volume 46, Number 4 Impact of Unsupervised Post-Processing. First, the results in Table 12 suggest that applying dimension-wise mean centering to the initial vector spaces has positive impact on word similarity scores in all test languages and for all models, both static and contextualized (see the + MC rows in Table 12). Mimno and Thompson (2017) show that distributional word vectors have a tendency toward narrow clusters in the vector space (i.e., they occupy a narrow cone in the vector space and are therefore anisotropic [Mu, Bhat, and Viswanath 2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vu"
2020.cl-4.5,N15-1184,0,0.266878,"trinsic evaluations of specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This make"
2020.cl-4.5,N18-2029,1,0.762404,"Missing"
2020.cl-4.5,P19-1070,1,0.912198,"Missing"
2020.cl-4.5,Q16-1002,1,0.880245,"Missing"
2020.cl-4.5,J15-4004,1,0.93809,"Missing"
2020.cl-4.5,D18-1043,0,0.0596882,"Missing"
2020.cl-4.5,2020.acl-main.560,0,0.0211952,"guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zema"
2020.cl-4.5,D18-1330,0,0.0361587,"Missing"
2020.cl-4.5,W14-1503,0,0.0323243,"wski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney (2012), Kiela and Clark (2014), and Melamud et al. (2016) demonstrated that different choices of hyperparameters in WE algorithms (such as context window) emphasize different relations in the resulting representations. Likewise, Agirre et al. (2009) and Levy and Goldberg (2014) discovered that WEs learned from texts annotated with syntactic information mirror similarity better than simple local bag-of-words neighborhoods. The failure of WEs to capture semantic similarity, in turn, affects model performance in several NLP applications where such knowledge is crucial. In particular, Natural Language Understanding tasks such"
2020.cl-4.5,W16-1607,0,0.0601778,"Missing"
2020.cl-4.5,kipper-etal-2004-extending,0,0.0569229,"Missing"
2020.cl-4.5,D19-1279,0,0.0717479,"Missing"
2020.cl-4.5,2020.emnlp-main.363,1,0.891144,"Missing"
2020.cl-4.5,P15-1027,0,0.0827227,"Missing"
2020.cl-4.5,P14-2050,0,0.0586512,"their associated meaning confounds the two distinct relations (Hill, Reichart, and Korhonen 2015; Schwartz, Reichart, and Rappoport 2015; Vuli´c et al. 2017b). As a result, distributional methods obscure a crucial facet of lexical meaning. This limitation also reflects onto word embeddings (WEs), representations of words as low-dimensional vectors that have become indispensable for a wide range of NLP applications (Collobert et al. 2011; Chen and Manning 2014; Melamud et al. 2016, inter alia). In particular, it involves both static WEs learned from co-occurrence patterns (Mikolov et al. 2013; Levy and Goldberg 2014; Bojanowski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney"
2020.cl-4.5,2020.emnlp-main.484,0,0.0477066,"Missing"
2020.cl-4.5,D18-1521,0,0.0264898,"Missing"
2020.cl-4.5,D17-1308,0,0.0696529,"Missing"
2020.cl-4.5,N19-1386,0,0.0334837,"Missing"
2020.cl-4.5,Q17-1022,1,0.934218,"Missing"
2020.cl-4.5,L18-1381,0,0.0518909,"Missing"
2020.cl-4.5,D18-1169,0,0.15982,"me prominent English word pair data sets such as WordSim-353 (Finkelstein et al. 2002), MEN (Bruni, Tran, and Baroni 2014), or Stanford Rare Words (Luong, Socher, and Manning 2013) did not discriminate between similarity and relatedness, the importance of this distinction was established by Hill, Reichart, and Korhonen (2015) (see again the discussion in §2.1) through the creation of SimLex-999. This inspired other similar data sets that focused on different lexical properties. For instance, SimVerb-3500 (Gerz et al. 2016) provided similarity ratings for 3,500 English verbs, whereas CARD-660 (Pilehvar et al. 2018) aimed at measuring the semantic similarity of infrequent concepts. Semantic Similarity Data Sets in Other Languages. Motivated by the impact of data sets such as SimLex-999 and SimVerb-3500 on representation learning in English, a line of related work put focus on creating similar resources in other languages. The dominant approach is translating and reannotating the entire original English SimLex-999 data set, as done previously for German, Italian, and Russian (Leviant and Reichart 2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venek"
2020.cl-4.5,P19-1493,0,0.0740204,"Missing"
2020.cl-4.5,J19-3005,1,0.889644,"Missing"
2020.cl-4.5,D18-1026,1,0.925849,"Missing"
2020.cl-4.5,Q17-1020,0,0.0167721,"that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation"
2020.cl-4.5,D18-1299,0,0.0140965,"i.e., the distributional information).1 Data sets that quantify the strength of semantic similarity between concept pairs such as SimLex-999 (Hill, Reichart, and Korhonen 2015) or SimVerb-3500 (Gerz et al. 2016) have been instrumental in improving models for distributional semantics and representation learning. Discerning between semantic similarity and relatedness/association is not only crucial for theoretical studies on lexical semantics (see §2), but has also been shown to benefit a range of language understanding tasks in NLP. Examples include dialog state tracking (Mrkˇsi´c et al. 2017; Ren et al. 2018), spoken language understanding (Kim et al. 2016; Kim, de Marneffe, and Fosler-Lussier 2016), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Eve"
2020.cl-4.5,Q19-1044,1,0.813505,"elines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scal"
2020.cl-4.5,L18-1152,0,0.223909,"2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venekoski and Vankka (2017) applied this process only to a subset of 300 concept pairs from the English SimLex-999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 20"
2020.cl-4.5,P19-1072,0,0.0344182,"Missing"
2020.cl-4.5,K15-1026,1,0.904015,"Missing"
2020.cl-4.5,P18-1072,1,0.919137,"Missing"
2020.cl-4.5,N16-1161,0,0.0133804,"For instance, we have highlighted how sharing the same encoder parameters across multiple languages may harm performance. However, it remains unclear if, and to what extent, the input language embeddings present in XLM -100 but absent in 886 Vuli´c et al. Multi-SimLex M - BERT help mitigate this issue. In addition, pretrained language embeddings can be obtained both from typological databases (Littell et al. 2017) and from neural architectures (Malaviya, Neubig, and Littell 2017). Plugging these embeddings into the encoders in lieu of embeddings trained end-to-end as suggested by prior work (Tsvetkov et al. 2016; Ammar et al. 2016; Ponti et al. 2019b) might extend the coverage to more resourcelean languages. Another important follow-up analysis might involve the comparison of the performance of representation learning models on multilingual data sets for both word-level semantic similarity and sentence-level natural language understanding. In particular, Multi-SimLex fills a gap in available resources for multilingual NLP and might help understand how lexical and compositional semantics interact if put alongside existing resources such as XNLI (Conneau et al. 2018b) for natural language inference or"
2020.cl-4.5,P19-1490,1,0.894602,"Missing"
2020.cl-4.5,Q15-1025,0,0.0155434,"f specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This makes the comparison of s"
2020.cl-4.5,2020.acl-main.536,0,0.0322002,"Missing"
2020.cl-4.5,D19-1077,0,0.0931228,". Because the concept pairs in Multi-SimLex are lowercased, 12 We also tested another encoding method where we fed pairs instead of single words/concepts into the pretrained encoder. The rationale is that the other concept in the pair can be used as a disambiguation signal. However, this method consistently led to sub-par performance across all experimental runs. 873 Computational Linguistics Volume 46, Number 4 we use the uncased version of M - BERT.13 M - BERT comprises all Multi-SimLex languages, and its evident ability to perform crosslingual transfer (Pires, Schlinger, and Garrette 2019; Wu and Dredze 2019; Wang et al. 2020) also makes it a convenient baseline model for crosslingual experiments later in §8. The second multilingual model we consider, XLM -100,14 is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a 1,280-dimensional representation. In contrast to M - BERT, XLM -100 drops the next-sentence prediction objective and adds a crosslingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the first H = 4 hidden layers in all experiments.15 Besides M - BERT and XLM, covering multiple lang"
2020.cl-4.5,K18-2001,0,0.0637127,"Missing"
2020.cl-4.5,P19-1307,0,0.0177035,"2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vuli´c et al. 2019), and for modeling lexical semantic change (Schlechtweg et al. 2019). However, to the best of our knowledge, the results summarized in Table 12 are the first evidence that also confirms its importance for semantic similarity in a wide array of languages. In sum, as a general rule of thumb, we suggest always mean-centering representations for semantic tasks. The results further indicate that additional post-processing methods such as ABTT and UNCOVEC on top of mean-centered vector spaces can lead to further gains in most languages. The gains are even visible for languages t"
2020.cl-4.5,K19-1021,1,0.900219,"Missing"
2020.cl-4.5,C98-1013,0,\N,Missing
2020.cl-4.5,J10-4006,0,\N,Missing
2020.cl-4.5,P94-1019,0,\N,Missing
2020.cl-4.5,J06-1003,0,\N,Missing
2020.cl-4.5,N09-1003,0,\N,Missing
2020.cl-4.5,D14-1034,1,\N,Missing
2020.cl-4.5,W13-3512,0,\N,Missing
2020.cl-4.5,D15-1242,0,\N,Missing
2020.cl-4.5,P15-2001,0,\N,Missing
2020.cl-4.5,kamholz-etal-2014-panlex,0,\N,Missing
2020.cl-4.5,N15-1104,0,\N,Missing
2020.cl-4.5,N16-1060,1,\N,Missing
2020.cl-4.5,Q17-1010,0,\N,Missing
2020.cl-4.5,P16-1024,1,\N,Missing
2020.cl-4.5,J17-4004,1,\N,Missing
2020.cl-4.5,E17-1016,1,\N,Missing
2020.cl-4.5,E17-2002,0,\N,Missing
2020.cl-4.5,P17-1042,0,\N,Missing
2020.cl-4.5,P18-1004,1,\N,Missing
2020.cl-4.5,P18-1142,1,\N,Missing
2020.cl-4.5,D18-1027,0,\N,Missing
2020.cl-4.5,D18-1024,0,\N,Missing
2020.cl-4.5,K18-1028,0,\N,Missing
2020.cl-4.5,N19-1391,0,\N,Missing
2020.cl-4.5,N19-1131,0,\N,Missing
2020.cl-4.5,K17-1013,1,\N,Missing
2020.cl-4.5,N19-1423,0,\N,Missing
2020.cl-4.5,N18-1101,0,\N,Missing
2020.cl-4.5,P19-4007,1,\N,Missing
2020.cl-4.5,W19-4310,1,\N,Missing
2020.cl-4.5,D19-1449,1,\N,Missing
2020.cl-4.5,D19-1288,1,\N,Missing
2020.cl-4.5,D19-1226,1,\N,Missing
2020.cl-4.5,D19-1165,0,\N,Missing
2020.cl-4.5,K19-1004,1,\N,Missing
2020.cl-4.5,D19-2007,1,\N,Missing
2020.cl-4.5,W17-0228,0,\N,Missing
2020.emnlp-main.186,D19-1572,0,0.0168032,"fectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (Gerz et al., 2018; Pires et al., 2019; Artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (Dryer and Haspelmath, 2013; Wichmann et al., 2018; Ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual NLP applications (Ponti et al., 2018; Eisenschlos et al., 2019). The differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training domain, or training data size and quality (Søgaard et al., 2018; Arora et al., 2019; Vuli´c et al., 2020). In future research we also plan an in-depth study of these factors and their relation to our spectral analysis. We believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work. Thes"
2020.emnlp-main.186,P19-1070,1,0.899741,"Missing"
2020.emnlp-main.186,D18-1330,0,0.0777051,"Missing"
2020.emnlp-main.186,kamholz-etal-2014-panlex,0,0.0256814,"Missing"
2020.emnlp-main.186,D19-1167,0,0.0284714,"the first step towards the development of more robust multilingually applicable NLP technology (O’Horan et al., 2016; Bjerva et al., 2019; Ponti et al., 2019). For instance, selecting suitable source languages is a prerequisite for successful cross-lingual transfer of dependency parsers or POS taggers (Naseem et al., 2012; Ponti et al., 2018; de Lhoneux et al., 2018). In another example, with all other factors kept similar (e.g., training data size, domain similarity), the quality of machine translation also depends heavily on the properties and language proximity of the actual language pair (Kudugunta et al., 2019). In this work, we contribute to this research endeavor by proposing a suite of spectral-based measures that capture the degree of isomorphism (Søgaard et al., 2018) between the monolingual embedding spaces of two languages. Our main hypothesis is that the potential to align two embedding spaces and learn transfer functions can be estimated through the differences between the monolingual embeddings’ spectra. We therefore discuss representative statistics of the spectrum of an embedding space (i.e., the set of the singular values of the embedding matrix), such as its condition number or its sor"
2020.emnlp-main.186,D18-1543,0,0.0348147,"Missing"
2020.emnlp-main.186,E17-2002,0,0.396657,"the embedding matrix), such as its condition number or its sorted list of singular values. We then derive measures for the isomorphism between two embedding spaces based on these statistics. To validate our hypothesis, we perform an extensive empirical evaluation with a range of crosslingual NLP tasks. This analysis reveals that our proposed spectrum-based isomorphism measures better correlate and explain greater variance than previous isomorphism measures (Søgaard et al., 2018; Patra et al., 2019). In addition, our measures also outperform standard approaches based on linguistic information (Littell et al., 2017), The first part of our empirical analysis targets bilingual lexicon induction (BLI), a cross-lingual task that received plenty of attention, in particular as a case study to investigate the impact of crosslanguage variation on task performance (Søgaard et al., 2018; Artetxe et al., 2018). Its popularity stems from its simple task formulation and reduced 2377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2377–2390, c November 16–20, 2020. 2020 Association for Computational Linguistics resource requirements, which makes it widely applicable across"
2020.emnlp-main.186,P12-1066,0,0.0250075,"1 Introduction The effectiveness of joint multilingual modeling and cross-lingual transfer in cross-lingual NLP is critically impacted by the actual languages in consideration (Bender, 2011; Ponti et al., 2019). Characterizing, measuring, and understanding this cross-language variation is often the first step towards the development of more robust multilingually applicable NLP technology (O’Horan et al., 2016; Bjerva et al., 2019; Ponti et al., 2019). For instance, selecting suitable source languages is a prerequisite for successful cross-lingual transfer of dependency parsers or POS taggers (Naseem et al., 2012; Ponti et al., 2018; de Lhoneux et al., 2018). In another example, with all other factors kept similar (e.g., training data size, domain similarity), the quality of machine translation also depends heavily on the properties and language proximity of the actual language pair (Kudugunta et al., 2019). In this work, we contribute to this research endeavor by proposing a suite of spectral-based measures that capture the degree of isomorphism (Søgaard et al., 2018) between the monolingual embedding spaces of two languages. Our main hypothesis is that the potential to align two embedding spaces and"
2020.emnlp-main.186,P19-1018,0,0.418289,"re discuss representative statistics of the spectrum of an embedding space (i.e., the set of the singular values of the embedding matrix), such as its condition number or its sorted list of singular values. We then derive measures for the isomorphism between two embedding spaces based on these statistics. To validate our hypothesis, we perform an extensive empirical evaluation with a range of crosslingual NLP tasks. This analysis reveals that our proposed spectrum-based isomorphism measures better correlate and explain greater variance than previous isomorphism measures (Søgaard et al., 2018; Patra et al., 2019). In addition, our measures also outperform standard approaches based on linguistic information (Littell et al., 2017), The first part of our empirical analysis targets bilingual lexicon induction (BLI), a cross-lingual task that received plenty of attention, in particular as a case study to investigate the impact of crosslanguage variation on task performance (Søgaard et al., 2018; Artetxe et al., 2018). Its popularity stems from its simple task formulation and reduced 2377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2377–2390, c November 16–2"
2020.emnlp-main.186,P19-1493,0,0.0231134,"tistic of effective condition number in §2.1. Our study is also the first to compare language distance measures that are based on discrete linguistic information (Littell et al., 2017) with measures of isomorphism (i.e., our spectral-based measures, IS, GH), which can also be used as proxy language distance measures. Our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (Gerz et al., 2018; Pires et al., 2019; Artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (Dryer and Haspelmath, 2013; Wichmann et al., 2018; Ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual NLP applications (Ponti et al., 2018; Eisenschlos et al., 2019). The differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training"
2020.emnlp-main.186,J19-3005,1,0.889996,"Missing"
2020.emnlp-main.186,P18-1142,1,0.89642,"Missing"
2020.emnlp-main.186,W19-4328,0,0.0612639,"Missing"
2020.emnlp-main.186,P18-1072,1,0.760711,"Missing"
2020.emnlp-main.186,D19-1449,1,0.902371,"Missing"
2020.emnlp-main.186,2020.emnlp-main.257,1,0.862162,"Missing"
2020.findings-emnlp.135,W17-4767,0,0.013728,"this approach of guiding a main task with the semantic information encompassed in discourse markers, studying it in the context of sentence fusion. 2.2 Generation Evaluation Two main approaches are used to evaluate generation models against a single gold-truth reference. The first estimates the correctness of a generated text using a ‘softer’ similarity metric between the text and the reference instead of exact matching. Earlier metrics like BLEU and ROUGE (Papineni et al., 2002; Lin, 2004), considered n-gram agreement. Later metrics matched words in the two texts using their word embeddings (Lo, 2017; Clark et al., 2019). More recently, contextual similarity measures were devised for this purpose (Lo, 2019; Wieting et al., 2019; Zhao et al., 2019; Zhang et al., 2020; Sellam et al., 2020). In §7 we provide a qualitative analysis for the latter, presenting typical evaluation mistakes made by a recently-proposed contextual-similarity based metric (Zhang et al., 2020). This analysis reveals properties that characterize such methods, which make them less suitable for our task. The second approach extends the (single) reference into multiple ones, by automatically generating paraphrases of the"
2020.findings-emnlp.135,W19-5358,0,0.0113266,"g it in the context of sentence fusion. 2.2 Generation Evaluation Two main approaches are used to evaluate generation models against a single gold-truth reference. The first estimates the correctness of a generated text using a ‘softer’ similarity metric between the text and the reference instead of exact matching. Earlier metrics like BLEU and ROUGE (Papineni et al., 2002; Lin, 2004), considered n-gram agreement. Later metrics matched words in the two texts using their word embeddings (Lo, 2017; Clark et al., 2019). More recently, contextual similarity measures were devised for this purpose (Lo, 2019; Wieting et al., 2019; Zhao et al., 2019; Zhang et al., 2020; Sellam et al., 2020). In §7 we provide a qualitative analysis for the latter, presenting typical evaluation mistakes made by a recently-proposed contextual-similarity based metric (Zhang et al., 2020). This analysis reveals properties that characterize such methods, which make them less suitable for our task. The second approach extends the (single) reference into multiple ones, by automatically generating paraphrases of the reference (a.k.a pseudoreferences) (Albrecht and Hwa, 2008; Yoshimura et al., 2019; Kauchak and Barzilay, 20"
2020.findings-emnlp.135,P17-1093,0,0.0241585,"dataset. Malmi et al. (2019) further improved accuracy by introducing LaserTagger, modeling sentence fusion as a sequence tagging problem. Rothe et al. (2019) set the state-of-the-art with a BERT-based (Devlin et al., 2019) model. Related to sentence fusion is the task of predicting the discourse marker that should connect two input sentences (Elhadad and McKeown, 1990; Grote and Stede, 1998; Malmi et al., 2018). It is typically utilized as an intermediate step to improve downstream tasks, mainly for discourse relation prediction (Pitler et al., 2008; Zhou et al., 2010; Braud and Denis, 2016; Qin et al., 2017). Connective prediction was included in multi-task frameworks for discourse relation prediction (Liu et al., 2016) and unsupervised sentence embedding (Jernite et al., 2017; Nie et al., 2019). We follow this approach of guiding a main task with the semantic information encompassed in discourse markers, studying it in the context of sentence fusion. 2.2 Generation Evaluation Two main approaches are used to evaluate generation models against a single gold-truth reference. The first estimates the correctness of a generated text using a ‘softer’ similarity metric between the text and the reference"
2020.findings-emnlp.135,D19-1510,1,0.866289,"o only small labeled datasets. Therefore, they relied on hand-crafted features (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Elsner and Santhanam, 2011; Filippova, 2010; Thadani and McKeown, 2013). Recently, D ISCO F USE, a large-scale dataset for sentence fusion, was introduced by Geva et al. (2019). This dataset was generated by automatically applying hand-crafted rules for 12 different discourse phenomena to break fused text examples from two domains, Wikipedia and Sports news, into two unfused sentences, while the content of the original text is preserved. We follow prior work (Malmi et al., 2019; Rothe et al., 2019) and use the balanced version of D ISCO F USE, containing ∼16.5 million examples, where the most frequent discourse phenomena were down-sampled. With D ISCO F USE, it became possible to train data-hungry neural fusion models. Geva et al. (2019) showed that a Transformer model (Vaswani et al., 2017) outperforms an LSTM-based (Hochreiter and Schmidhuber, 1997) seq2seq model on this dataset. Malmi et al. (2019) further improved accuracy by introducing LaserTagger, modeling sentence fusion as a sequence tagging problem. Rothe et al. (2019) set the state-of-the-art with a BERT-"
2020.findings-emnlp.135,L18-1260,1,0.821563,"ble to train data-hungry neural fusion models. Geva et al. (2019) showed that a Transformer model (Vaswani et al., 2017) outperforms an LSTM-based (Hochreiter and Schmidhuber, 1997) seq2seq model on this dataset. Malmi et al. (2019) further improved accuracy by introducing LaserTagger, modeling sentence fusion as a sequence tagging problem. Rothe et al. (2019) set the state-of-the-art with a BERT-based (Devlin et al., 2019) model. Related to sentence fusion is the task of predicting the discourse marker that should connect two input sentences (Elhadad and McKeown, 1990; Grote and Stede, 1998; Malmi et al., 2018). It is typically utilized as an intermediate step to improve downstream tasks, mainly for discourse relation prediction (Pitler et al., 2008; Zhou et al., 2010; Braud and Denis, 2016; Qin et al., 2017). Connective prediction was included in multi-task frameworks for discourse relation prediction (Liu et al., 2016) and unsupervised sentence embedding (Jernite et al., 2017; Nie et al., 2019). We follow this approach of guiding a main task with the semantic information encompassed in discourse markers, studying it in the context of sentence fusion. 2.2 Generation Evaluation Two main approaches a"
2020.findings-emnlp.135,P19-1442,0,0.0204445,"BERT-based (Devlin et al., 2019) model. Related to sentence fusion is the task of predicting the discourse marker that should connect two input sentences (Elhadad and McKeown, 1990; Grote and Stede, 1998; Malmi et al., 2018). It is typically utilized as an intermediate step to improve downstream tasks, mainly for discourse relation prediction (Pitler et al., 2008; Zhou et al., 2010; Braud and Denis, 2016; Qin et al., 2017). Connective prediction was included in multi-task frameworks for discourse relation prediction (Liu et al., 2016) and unsupervised sentence embedding (Jernite et al., 2017; Nie et al., 2019). We follow this approach of guiding a main task with the semantic information encompassed in discourse markers, studying it in the context of sentence fusion. 2.2 Generation Evaluation Two main approaches are used to evaluate generation models against a single gold-truth reference. The first estimates the correctness of a generated text using a ‘softer’ similarity metric between the text and the reference instead of exact matching. Earlier metrics like BLEU and ROUGE (Papineni et al., 2002; Lin, 2004), considered n-gram agreement. Later metrics matched words in the two texts using their word"
2020.findings-emnlp.135,P02-1040,0,0.108398,"discourse relation prediction (Liu et al., 2016) and unsupervised sentence embedding (Jernite et al., 2017; Nie et al., 2019). We follow this approach of guiding a main task with the semantic information encompassed in discourse markers, studying it in the context of sentence fusion. 2.2 Generation Evaluation Two main approaches are used to evaluate generation models against a single gold-truth reference. The first estimates the correctness of a generated text using a ‘softer’ similarity metric between the text and the reference instead of exact matching. Earlier metrics like BLEU and ROUGE (Papineni et al., 2002; Lin, 2004), considered n-gram agreement. Later metrics matched words in the two texts using their word embeddings (Lo, 2017; Clark et al., 2019). More recently, contextual similarity measures were devised for this purpose (Lo, 2019; Wieting et al., 2019; Zhao et al., 2019; Zhang et al., 2020; Sellam et al., 2020). In §7 we provide a qualitative analysis for the latter, presenting typical evaluation mistakes made by a recently-proposed contextual-similarity based metric (Zhang et al., 2020). This analysis reveals properties that characterize such methods, which make them less suitable for our"
2020.findings-emnlp.135,2020.acl-main.704,0,0.0216456,"ain approaches are used to evaluate generation models against a single gold-truth reference. The first estimates the correctness of a generated text using a ‘softer’ similarity metric between the text and the reference instead of exact matching. Earlier metrics like BLEU and ROUGE (Papineni et al., 2002; Lin, 2004), considered n-gram agreement. Later metrics matched words in the two texts using their word embeddings (Lo, 2017; Clark et al., 2019). More recently, contextual similarity measures were devised for this purpose (Lo, 2019; Wieting et al., 2019; Zhao et al., 2019; Zhang et al., 2020; Sellam et al., 2020). In §7 we provide a qualitative analysis for the latter, presenting typical evaluation mistakes made by a recently-proposed contextual-similarity based metric (Zhang et al., 2020). This analysis reveals properties that characterize such methods, which make them less suitable for our task. The second approach extends the (single) reference into multiple ones, by automatically generating paraphrases of the reference (a.k.a pseudoreferences) (Albrecht and Hwa, 2008; Yoshimura et al., 2019; Kauchak and Barzilay, 2006; Edunov et al., 2018; Gao et al., 2020). Our method (§3.3) follows this paradigm"
2020.tacl-1.33,P15-1071,0,0.0147551,"methods, and the hierarchical attention transfer network (HATN; Li et al., 2018), which is one of our baselines (see below). Unsupervised DA through representation learning has followed two main avenues. The first avenue consists of works that aim to explicitly build a feature representation that bridges the gap between the domains. A seminal framework in this line is structural correspondence learning (SCL; Blitzer et al., 2006, 2007), that splits the feature space into pivot and non-pivot features. A large number of works have followed this idea (e.g., Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Li et al., 2017, 2018; Tu and Wang, 2019; Ziser and Reichart, 2017, 2018) and we discuss it below. Works in the second avenue learn cross-domain representations by training autoencoders (AEs) on the unlabeled data from the source and target domains. This way they hope to obtain a more robust representation, which is hopefully better suited for DA. Examples for such models include the stacked denoising AE (SDA; Vincent et al., 2008; Glorot et al., 2011, the marginalized SDA and its variants (MSDA; Chen et al., 2012; Yang and Eisenstein, 2014; Clinchant et al., 2016) and va"
2020.tacl-1.33,P16-2005,0,0.019497,", 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Li et al., 2017, 2018; Tu and Wang, 2019; Ziser and Reichart, 2017, 2018) and we discuss it below. Works in the second avenue learn cross-domain representations by training autoencoders (AEs) on the unlabeled data from the source and target domains. This way they hope to obtain a more robust representation, which is hopefully better suited for DA. Examples for such models include the stacked denoising AE (SDA; Vincent et al., 2008; Glorot et al., 2011, the marginalized SDA and its variants (MSDA; Chen et al., 2012; Yang and Eisenstein, 2014; Clinchant et al., 2016) and variational AE based models (Louizos et al., 2016). Recently, Ziser and Reichart (2017, 2018) and Li et al. (2018) married these approaches and presented pivot-based approaches where the representation model is based on DNN encoders (AE, long short-term memory [LSTM], or hierarchical attention networks). Because their methods outperformed the above models, we aim to extend them to models that can also exploit massive out of (source and target) domain corpora. We next elaborate on pivot-based approaches. Pivot-based Domain Adaptation Proposed by Blitzer et al. (2006, 2007) through their SC"
2020.tacl-1.33,P07-1056,0,0.470744,"ese models so that they close the gap between the source and target domains, we fine-tune their parameters using a pivot-based variant of the Masked Language Modeling (MLM) objective, optimized on unlabeled data from both the source and the target domains. We further present R-PERL (regularized PERL), which facilitates parameter sharing for pivots with similar meaning. We perform extensive experimentation in various unsupervised DA setups of the task of binary sentiment classification (§4, 5). First, for compatibility with previous work, we experiment with the legacy product review domains of Blitzer et al. (2007) (12 setups). We then experiment with more challenging setups, adapting between the above domains and the airline review domain (Nguyen, 2015) used in Ziser and Reichart (2018) (4 setups), as well as the IMDb movie review domain (Maas et al., 2011) (6 setups). We compare PERL to the best performing pivot-based methods (Ziser and Reichart, 2018; Li et al., 2018) and to DA approaches that fine-tune a massively pretrained BERT model by optimizing its standard MLM objective using target-domain unlabeled data (Lee et al., 2020; Han and Eisenstein, 2019). PERL and R-PERL substantially outperform the"
2020.tacl-1.33,N19-1423,0,0.614218,"t feature representation for the source and the target domains. Later on, Ziser and Reichart (2017, 2018), and Li et al. (2018) married the two approaches and achieved substantial improvements on a variety of DA setups. Despite their success, pivot-based DNN models still only utilize labeled data from the source domain and unlabeled data from both the source and the target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. With the recent game-changing success of contextualized word embedding models trained on such massive corpora (Devlin et al., 2019; Peters et al., 2018), it is natural to ask whether information from such corpora can enhance these DA methods, particularly that background knowledge from noncontextualized embeddings has shown useful for Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous research following this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose"
2020.tacl-1.33,D19-1424,0,0.0218666,"er-parameter configurations of each model, for 4 arbitrarily selected setups. In all 4 setups, PERL and R-PERL consistently achieve higher avg, max, and min values and lower std values compared to the other models (with the exception of PBLM achieving higher max for K → A). Moreover, the std values of PBLM and especially HATN are substantially higher than those of the models that use BERT. Yet, PERL and R-PERL demonstrate lower std values compared to BERT and Fine-tuned BERT in 3 of 4 setups, indicating that our method contributes to stability beyond the documented contribution of BERT itself Hao et al. (2019). 6.2 Design Choice Analysis Impact of Pivot Selection One design choice that impacts our results is the method through which pivots are selected. We next compare three alternatives to our pivot selection method, keeping all other aspects of PERL fixed. As above, we arbitrarily select four setups. We consider the following pivot selection methods: (a) Random-Frequent: Pivots are randomly selected from the unigrams and bigrams that appear at least 80 times in the unlabeled data of each of the domains; (b) High-MI, No Target: We select the pivots that have the highest mutual information (MI) wit"
2020.tacl-1.33,D18-1045,0,0.0219912,"rpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation learning model that extends contextualized word embedding models such as BERT (Devlin et al., 2019) with pivot-based fine-tuning. PERL outperforms strong baselines across 22 sentiment classification domain adaptation setups, improves indomain model performance, yields effective reduced-size models, and increases model stability.1 1 Introduction Natural Language Processing (NLP) algorithms are constantly improving, gradually approaching human-level performance (Dozat and Manning, 2017; Edunov et al., 2018; Radford et al., 2018). However, those algorithms often depend on the availability of large amounts of manually annotated data from the domain in which the task is performed. Unfortunately, collecting such annotated data is often costly and laborious, which substantially limits the applicability of NLP technology. Domain Adaptation (DA), training an algorithm on annotated data from a source domain so that it can be effectively applied to other target domains, is one of the ways to solve the above bottleneck. ∗ Both authors contributed equally to this work. Our code is at https://github.com/ey"
2020.tacl-1.33,P07-1034,0,0.295196,"Missing"
2020.tacl-1.33,N10-1004,0,0.0930115,"Missing"
2020.tacl-1.33,N19-1039,0,0.0956605,"method through which pivots are selected. We next compare three alternatives to our pivot selection method, keeping all other aspects of PERL fixed. As above, we arbitrarily select four setups. We consider the following pivot selection methods: (a) Random-Frequent: Pivots are randomly selected from the unigrams and bigrams that appear at least 80 times in the unlabeled data of each of the domains; (b) High-MI, No Target: We select the pivots that have the highest mutual information (MI) with the source domain label, but appear less than 10 times in the target domain unlabeled data; (c) Oracle Miller (2019): Here the pivots are selected according to our method, but the labeled data used for pivot-label MI computation is the target domain test data rather than the source domain training data. This is an upper bound on the performance of our method since it uses target domain labeled data, which is not available to us. For all methods we select 100 pivots (see above). Table 5 presents the results of the four PERL variants, and compare them to BERT and Finetuned BERT. We observe four patterns in the results. First, PERL with our pivot selection method, which emphasizes both high MI with the task la"
2020.tacl-1.33,P15-1062,0,0.0216503,"ithm on annotated data from a source domain so that it can be effectively applied to other target domains, is one of the ways to solve the above bottleneck. ∗ Both authors contributed equally to this work. Our code is at https://github.com/eyalbd2/ PERL. 1 504 Transactions of the Association for Computational Linguistics, vol. 8, pp. 504–521, 2020. https://doi.org/10.1162/tacl a 00328 Action Editor: Jimmy Lin. Submission batch: 4/2020; Revision batch: 2020; Published 8/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. DA (Plank and Moschitti, 2013; Nguyen et al., 2015). In this paper we hence propose an unsupervised DA approach that extends leading approaches based on DNNs and pivot-based ideas, so that they can incorporate information encoded in massive corpora (§3). Our model, named PERL: Pivotbased Encoder Representation of Language, builds on massively pre-trained contextualized word embedding models such as BERT (Devlin et al., 2019). To adjust the representations learned by these models so that they close the gap between the source and target domains, we fine-tune their parameters using a pivot-based variant of the Masked Language Modeling (MLM) objec"
2020.tacl-1.33,P11-1015,0,0.206648,"Missing"
2020.tacl-1.33,Q19-1044,1,0.827013,"assive out of (source and target) domain corpora. We next discuss this pre-training process, which is also known as training models for contextualized word embeddings. Contextualized Word Embedding Models Contextualized word embedding (CWE) models are trained on massive corpora (Peters et al., 2018; Radford et al., 2019). They typically utilize a language modeling objective or a closely related variant (Peters et al., 2018; Ziser and Reichart, 2018; Devlin et al., 2019; Yang et al., 2019), although in some recent papers the model is trained on a mixture of basic NLP tasks (Zhang et al., 2019; Rotman and Reichart, 2019). The contribution 506 of such models to the state-of-the-art in a variety of NLP tasks is already well-established. CWE models typically follow three steps: (1) Pre-training: Where a DNN (referred to as the encoder of the model) is first trained on massive unlabeled corpora which represent a broad domain (such as English Wikipedia); (2) Fine-tuning: An optional step, where the encoder is refined on unlabeled text of interest. As noted above, Lee et al. (2020) and Han and Eisenstein (2019) tuned BERT on unlabeled target domain data to facilitate domain adaptation; and (3) Supervised task train"
2020.tacl-1.33,D12-1131,1,0.859338,"Missing"
2020.tacl-1.33,N18-1202,0,0.357895,"ion for the source and the target domains. Later on, Ziser and Reichart (2017, 2018), and Li et al. (2018) married the two approaches and achieved substantial improvements on a variety of DA setups. Despite their success, pivot-based DNN models still only utilize labeled data from the source domain and unlabeled data from both the source and the target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. With the recent game-changing success of contextualized word embedding models trained on such massive corpora (Devlin et al., 2019; Peters et al., 2018), it is natural to ask whether information from such corpora can enhance these DA methods, particularly that background knowledge from noncontextualized embeddings has shown useful for Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous research following this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation"
2020.tacl-1.33,Q14-1002,0,0.0632692,"Missing"
2020.tacl-1.33,P13-1147,0,0.0231687,"ion (DA), training an algorithm on annotated data from a source domain so that it can be effectively applied to other target domains, is one of the ways to solve the above bottleneck. ∗ Both authors contributed equally to this work. Our code is at https://github.com/eyalbd2/ PERL. 1 504 Transactions of the Association for Computational Linguistics, vol. 8, pp. 504–521, 2020. https://doi.org/10.1162/tacl a 00328 Action Editor: Jimmy Lin. Submission batch: 4/2020; Revision batch: 2020; Published 8/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. DA (Plank and Moschitti, 2013; Nguyen et al., 2015). In this paper we hence propose an unsupervised DA approach that extends leading approaches based on DNNs and pivot-based ideas, so that they can incorporate information encoded in massive corpora (§3). Our model, named PERL: Pivotbased Encoder Representation of Language, builds on massively pre-trained contextualized word embedding models such as BERT (Devlin et al., 2019). To adjust the representations learned by these models so that they close the gap between the source and target domains, we fine-tune their parameters using a pivot-based variant of the Masked Languag"
2020.tacl-1.33,N03-1027,0,0.24422,"Missing"
2020.tacl-1.33,2020.emnlp-demos.6,0,0.0814299,"Missing"
2020.tacl-1.33,D16-1023,0,0.0191969,"chical attention transfer network (HATN; Li et al., 2018), which is one of our baselines (see below). Unsupervised DA through representation learning has followed two main avenues. The first avenue consists of works that aim to explicitly build a feature representation that bridges the gap between the domains. A seminal framework in this line is structural correspondence learning (SCL; Blitzer et al., 2006, 2007), that splits the feature space into pivot and non-pivot features. A large number of works have followed this idea (e.g., Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Li et al., 2017, 2018; Tu and Wang, 2019; Ziser and Reichart, 2017, 2018) and we discuss it below. Works in the second avenue learn cross-domain representations by training autoencoders (AEs) on the unlabeled data from the source and target domains. This way they hope to obtain a more robust representation, which is hopefully better suited for DA. Examples for such models include the stacked denoising AE (SDA; Vincent et al., 2008; Glorot et al., 2011, the marginalized SDA and its variants (MSDA; Chen et al., 2012; Yang and Eisenstein, 2014; Clinchant et al., 2016) and variational AE based m"
2020.tacl-1.33,P19-1139,0,0.0451045,"Missing"
2020.tacl-1.33,K17-1040,1,0.907844,"of deep neural network (DNN) modeling, attention has been recently focused on representation learning approaches. Within representation learning for unsupervised DA, two approaches have been shown particularly useful. In one line of work, DNN-based methods that use compress-based noise reduction to learn cross-domain features have been developed (Glorot et al., 2011; Chen et al., 2012). In another line of work, methods based on the distinction between pivot and nonpivot features (Blitzer et al., 2006, 2007) learn a joint feature representation for the source and the target domains. Later on, Ziser and Reichart (2017, 2018), and Li et al. (2018) married the two approaches and achieved substantial improvements on a variety of DA setups. Despite their success, pivot-based DNN models still only utilize labeled data from the source domain and unlabeled data from both the source and the target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. With the recent game-changing success of contextualized word embedding models trained on such massive corpora (Devlin et al., 2019; Peters et al., 2018), it is natural to ask whether information from such corp"
2020.tacl-1.33,N18-1112,1,0.481924,"objective, optimized on unlabeled data from both the source and the target domains. We further present R-PERL (regularized PERL), which facilitates parameter sharing for pivots with similar meaning. We perform extensive experimentation in various unsupervised DA setups of the task of binary sentiment classification (§4, 5). First, for compatibility with previous work, we experiment with the legacy product review domains of Blitzer et al. (2007) (12 setups). We then experiment with more challenging setups, adapting between the above domains and the airline review domain (Nguyen, 2015) used in Ziser and Reichart (2018) (4 setups), as well as the IMDb movie review domain (Maas et al., 2011) (6 setups). We compare PERL to the best performing pivot-based methods (Ziser and Reichart, 2018; Li et al., 2018) and to DA approaches that fine-tune a massively pretrained BERT model by optimizing its standard MLM objective using target-domain unlabeled data (Lee et al., 2020; Han and Eisenstein, 2019). PERL and R-PERL substantially outperform these baselines, emphasizing the additive effect of massive pre-training and pivot-based fine-tuning. As an additional contribution, we show that pivot-based learning is effective"
2020.tacl-1.33,P14-2088,0,0.0232229,"et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Li et al., 2017, 2018; Tu and Wang, 2019; Ziser and Reichart, 2017, 2018) and we discuss it below. Works in the second avenue learn cross-domain representations by training autoencoders (AEs) on the unlabeled data from the source and target domains. This way they hope to obtain a more robust representation, which is hopefully better suited for DA. Examples for such models include the stacked denoising AE (SDA; Vincent et al., 2008; Glorot et al., 2011, the marginalized SDA and its variants (MSDA; Chen et al., 2012; Yang and Eisenstein, 2014; Clinchant et al., 2016) and variational AE based models (Louizos et al., 2016). Recently, Ziser and Reichart (2017, 2018) and Li et al. (2018) married these approaches and presented pivot-based approaches where the representation model is based on DNN encoders (AE, long short-term memory [LSTM], or hierarchical attention networks). Because their methods outperformed the above models, we aim to extend them to models that can also exploit massive out of (source and target) domain corpora. We next elaborate on pivot-based approaches. Pivot-based Domain Adaptation Proposed by Blitzer et al. (200"
2020.tacl-1.33,P19-1591,1,0.881434,"Missing"
2021.acl-long.447,D19-1607,0,0.0452479,"Missing"
2021.acl-long.447,2020.emnlp-main.40,0,0.467333,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,2020.emnlp-main.369,0,0.514216,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,P19-1493,0,0.364814,"s, we make our sampled few shots publicly available.1 1 Introduction Multilingual pretrained encoders like multilingual BERT (mBERT; Devlin et al. (2019)) and XLMR (Conneau et al., 2020) are the top performers in crosslingual tasks such as natural language inference (Conneau et al., 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English f"
2021.acl-long.447,2020.acl-main.467,0,0.054412,"Missing"
2021.acl-long.447,P19-1459,0,0.0154384,"he clock didn’t even work one minute ... Visually, however, very nice.”) Pretrained multilingual encoders are shown to learn and store “language-agnostic” features (Pires et al., 2019; Zhao et al., 2020); §5.3 shows that source-training mBERT on EN substantially benefits other languages, even for difficult semantic tasks like PAWSX. Conditioning on such languageagnostic features, we expect that the buckets should lead to good understanding and reasoning capabilities for a target language. However, plain few-shot finetuning still relies heavily on unintended shallow lexical cues and shortcuts (Niven and Kao, 2019; Geirhos et al., 2020) that generalize poorly. Other open research questions for future work arise: How do we overcome this excessive reliance on lexical features? How can we leverage language-agnostic features with few shots? Our standardized buckets, baseline results, and analyses are the initial step towards researching and answering these questions. 5.5 Target-Adapting Methods SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning"
2021.acl-long.447,P19-1015,0,0.0280507,"in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for each target (i.e., non-English) language of a task to get a reliable estimation of model performance. CLS Tasks. For MLDoc and MARC, e"
2021.acl-long.447,N18-1101,0,0.0283863,"l selection in this stage. 4 Experimental Setup We consider three types of tasks requiring varying degrees of semantic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to a"
2021.acl-long.447,D19-1077,0,0.12812,"019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially im"
2021.acl-long.447,2020.repl4nlp-1.16,0,0.269393,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.emnlp-main.362,0,0.385788,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.findings-emnlp.29,0,0.094877,"Missing"
2021.acl-long.447,2020.emnlp-main.608,0,0.0774621,"Missing"
2021.acl-long.447,D19-1382,0,0.0227255,"tic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for"
2021.acl-long.447,2020.emnlp-main.660,0,0.0765103,"Missing"
2021.acl-long.447,N18-1109,0,0.0145892,"e task instead of between different tasks. Few-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images. Methods like prototypical networks (Snell et al., 2017) and modelagnostic meta-learning (MAML; Finn et al. (2017)) have also been applied to many monolingual (typically English) NLP tasks such as relation classification (Han et al., 2018; Gao et al., 2019), namedentity recognition (Hou et al., 2020a), word sense disambiguation (Holla et al., 2020), and text classification (Yu et al., 2018; Yin, 2020; Yin et al., 2020; Bansal et al., 2020; Gupta et al., 2020). However, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020). Inspired by this work, we compare various fewshot finetuning methods from computer vision in the context of FS-XLT. Task Performance Variance. Deep neural networks’ performance on NLP tasks is bound to exhibit large varian"
2021.acl-long.447,N19-1131,0,0.034028,"Missing"
2021.acl-short.10,D19-1418,0,0.110212,"gmentations.1 1 Figure 1: Predictions and attention maps of a state-ofthe-art VQA-CP model over a VQA example (left) and its augmentation (right). A robust model should use the information it utilizes in the original example to correctly answer the augmented one. that the distribution of answers per question type (e.g., “what color”, “how many”) differs between the train and test sets. Using VQA-CP, Kafle et al. (2019) demonstrated the poor out-of-distribution generalization of many VQA systems. While many models were subsequently designed to deal with the VQA-CP dataset (Cadene et al., 2019; Clark et al., 2019; Chen et al., 2020; Gat et al., 2020), aiming to solve the out-of-distribution generalization problem in VQA, they were later demonstrated to overfit the unique properties of this dataset (Teney et al., 2020). Moreover, no measures for robustness to distribution shifts have been proposed. In this work we propose a consistency-based measure that can indicate on the robustness of VQA models to distribution shifts. Our robustness measure is based on counterfactual data augmentations (CADs), which were shown useful for both training (Kaushik et al., 2019) and evaluation (Garg et al., 2019; Agarwa"
2021.acl-short.10,D19-1596,0,0.0580721,"Missing"
2021.acl-short.10,2020.acl-main.442,0,0.0602767,"Missing"
2021.acl-short.10,P16-1009,0,0.0394422,"3: Original accuracy over our proposed augmentations (Y/N C, Y/N HM, Y/N WK) and alternatives (BT, Reph, ConVQA). The rows correspond to state-of-the-art models on VQA-CP (top), VQA (middle) and Visual Dialog (bottom). Reph and ConVQA were not created for VisDial, and it does not have “what kind” questions. on scene graphs attached to each image, and CSConVQA is manually generated by annotators. Finally, back-translation, translating to another language and back, is a high-coverage although lowquality approach to text augmentation. It was used during training and shown to improve NLP models (Sennrich et al., 2016), but was not considered in VQA. We use English-German translations. of our focused intervention approach for measuring robustness. The high RAD values for BT and Reph might indicate that VQA models are indeed robust to linguistic variation, as long as the answer does not change. Interestingly, our augmentations also reveal that VQA-CP models are less robust than VQA models. This suggests that despite the attempt to design more robust models, VQA-CP models still overfit their training data. Models The VQA-CP models we consider are RUBi (Cadene et al., 2019), LMH (Clark et al., 2019) and CSS (C"
2021.eacl-main.76,Q19-1038,0,0.0283117,"k-specific labelled data shortage. To bridge this gap, we combine semi-supervised deep generative models and multi-lingual pretraining to form a pipeline for document classification task. Compared to strong supervised learning baselines, our semi-supervised classification framework is highly competitive and outperforms the state-of-the-art counterparts in lowresource settings across several languages. 1 1 Introduction Multi-lingual pretraining has been shown to effectively use unlabelled data through learning shared representations across languages that can be transferred to downstream tasks (Artetxe and Schwenk, 2019; Devlin et al., 2019; Wu and Dredze, 2019; Conneau and Lample, 2019). Nonetheless, the lack of labelled data still leads to inferior performance of the same model compared to those trained in languages with more labelled data such as English (Zeman et al., 2018; Zhu et al., 2019). Semi-supervised learning is another appealing paradigm that supplements the labelled data with unlabelled data which is easy to acquire (Blum and Mitchell, 1998; Zhou and Li, 2005; McClosky et al., 2006, inter alia). In particular, deep generative models (DGMs) such as variational autoencoder (VAE; Kingma and Wellin"
2021.eacl-main.76,Q17-1010,0,0.0290135,"al., 2017; Alemi et al., 2018). We only run one trial with fixed random seed for both pretraining and document classification. Training details can be found in the Appendix. As our supervised baselines we compare with the following two groups: (I) NXVAE-based supervised models which are pretrained NXVAE encoder with a multi-layer perceptron classifier on top (denoted by NXVAE-z1 (qφ (y|z1 )) or NXVAEh (qφ (y|h)) depending on the representation fed into the classifier; or NXVAE-z1 models initialised with different pretrained embeddings: random initialisation (RAND), mono-lingual fastText (FT; Bojanowski et al. (2017)), unsupervised cross-lingual MUSE (Lample et al., 2018b), pretrained embeddings from Wei and Deng (2017) (PEMB), and our resulting embeddings from pretrained NXVAE (NXEMB).5 (II) We also pretrain a word-based BERT (BERTW) with parameter size akin to NXVAE on the same data, and fine-tune it directly.6 For our semi-supervised experiments, we test https://github.com/google-research/ bert/blob/master/multilingual.md. 3 https://pytorch.org/. 4 https://www.statmt.org/europarl/. 5 All embeddings are pretrained on the same Europarl data. We also trained subword-based models for BERT and NXVAE, and ob"
2021.eacl-main.76,D14-1179,0,0.0501179,"Missing"
2021.eacl-main.76,N06-1020,0,0.156333,"led data through learning shared representations across languages that can be transferred to downstream tasks (Artetxe and Schwenk, 2019; Devlin et al., 2019; Wu and Dredze, 2019; Conneau and Lample, 2019). Nonetheless, the lack of labelled data still leads to inferior performance of the same model compared to those trained in languages with more labelled data such as English (Zeman et al., 2018; Zhu et al., 2019). Semi-supervised learning is another appealing paradigm that supplements the labelled data with unlabelled data which is easy to acquire (Blum and Mitchell, 1998; Zhou and Li, 2005; McClosky et al., 2006, inter alia). In particular, deep generative models (DGMs) such as variational autoencoder (VAE; Kingma and Welling (2014)) are capable of capturing complex data distributions at scale with rich latent representations, and they have been used ∗ Work done while at Microsoft Research Cambridge. Code is available at https://github.com/ cambridgeltl/mling_sdgms. 1 for semi-supervised learning in various tasks in NLP (Xu et al., 2017; Yin et al., 2018; Choi et al., 2019; Xie and Ma, 2019), as well as inducing crosslingual word embeddings (Wei and Deng, 2017), and representation learning in combina"
2021.eacl-main.76,2005.mtsummit-papers.11,0,0.107451,"der (§4.2). All documents are lowercased. We report accuracy for evaluation following Schwenk and Li (2018). For all experiments, We use Adam (Kingma and Ba, 2015) as optimiser, but with different learning rates for both settings and pretraining. We implemented the model with Pytorch3 1.10 (Paszke et al., 2019), and use GeForce GTX 1080Ti GPUs. See the Appendix for details about model configurations and training. 4.1 LSTM Encoder with VAE Pretraining Experimental Setup. For pretraining NXVAE, we use three language pairs: EN - DE, EN - FR and DE FR constructed from Europarl v7 parallel corpus (Koehn, 2005),4 where only two language pairs are available: EN-DE and EN-FR, which consist of four datasets in total: (EN, DE)EN-DE , and (EN, FR)EN-FR . For DE-FR, we pair DE EN-DE and FR EN-FR directly as pseudo parallel data. We trim all datasets into exactly the same sentence size, and preprocess them 2 E 234 238 1000 G 252 266 1030 M 244 268 979 Total 1000 1000 4000 DE 270 229 984 240 268 1026 245 266 1022 245 237 968 1000 1000 4000 FR 227 257 999 262 237 973 258 237 998 253 269 1030 1000 1000 4000 RU 261 265 1073 288 272 1121 184 204 706 267 259 1100 1000 100 4000 ZH 294 324 1169 286 300 1215 109 93"
2021.eacl-main.76,P18-1070,0,0.0200299,"ppealing paradigm that supplements the labelled data with unlabelled data which is easy to acquire (Blum and Mitchell, 1998; Zhou and Li, 2005; McClosky et al., 2006, inter alia). In particular, deep generative models (DGMs) such as variational autoencoder (VAE; Kingma and Welling (2014)) are capable of capturing complex data distributions at scale with rich latent representations, and they have been used ∗ Work done while at Microsoft Research Cambridge. Code is available at https://github.com/ cambridgeltl/mling_sdgms. 1 for semi-supervised learning in various tasks in NLP (Xu et al., 2017; Yin et al., 2018; Choi et al., 2019; Xie and Ma, 2019), as well as inducing crosslingual word embeddings (Wei and Deng, 2017), and representation learning in combination with Transformers via pretraining (Li et al., 2020). To leverage the benefits of both worlds, we propose a pipeline method by combining semisupervised DGMs (SDGMs) based on M1+M2 model (Kingma et al., 2014) with multi-lingual pretraining. The pretrained model serves as multilingual encoder, and SDGMs can operate on top of it independently of encoding architecture. To highlight such independence, we experiment with two pretraining settings: (1"
2021.emnlp-main.20,2020.tacl-1.33,1,0.906812,"for AE work. information of both feature types. Blitzer et al. (2006, 2007) learned linear nonpivot to pivot mappings that do not exploit the input structure (e.g., the structure of the review document in a sentiment classification task). A series of consecutive works alleviated these limitations. For example, Ziser and Reichart (2017) used a feedforward neural network to learn the mapping, also exploiting the semantic similarity between pivots. Later (Ziser and Reichart, 2018b, 2019) proposed PBLM, a pivot-based language model, which also exploits the structure of the input text . Recently, Ben-David et al. (2020) integrated these ideas into the BERT architecture. They changed its Masked Language Modeling task so that pivots are more often masked than non-pivots, and the model should predict if a token is a pivot or not, and then identify the token only in the former case. Despite the great success of this line of work on unsupervised DA for sentiment classification, the adaptation of the proposed ideas to sequence tagging tasks with cross-domain category shift is challenging. In this paper we solve this challenge and demonstrate that our solution yields state-ofthe-art results on unsupervised DA for A"
2021.emnlp-main.20,P07-1056,0,0.625954,"et al., 2007). DA, training models on source domain labeled data so that they can effectively generalize to different target domains, is a long-standing research challenge. While the target domain labeled data availability in DA setups ranges from little (supervised DA (Daumé III, 2007)) to none (unsupervised DA (Ramponi and Plank, 2020)), unlabeled data is typically available in both source and target domains. This paper focuses on unsupervised DA as we believe it is a more realistic and practical scenario. Due to the great success of deep learning models, DA through representation learning (Blitzer et al., 2007; Ziser and Reichart, 2017), i.e., learning a shared representation for both the source and target domains, has recently become prominent (Ziser and Reichart, 2018a; Ben-David et al., 2020). Of particular importance to this line of work are approaches that utilize pivot features (Blitzer et al., 2006) that: (a) frequently appear in both domains; and (b) have high mutual information (MI) with the task label. While pivot-based methods achieve state-of-the-art results in many text classification tasks (Ziser and Reichart, 2018a; Miller, 2019; BenDavid et al., 2020), it is not trivial to successfu"
2021.emnlp-main.20,W06-1615,0,0.613521,"classifier is stacked on top of the representation model of step (a) and fine-tuned using the source domain labeled data. The finetuned model is then applied to the target domain test data, hoping that the domain-invariant feature space would mitigate the domain gap. Many works have followed this avenue (e.g., (Chen et al., 2012; Louizos et al., 2016; Ganin et al., 2016)) and a comprehensive survey is beyond the scope of this paper. Below we discuss the line of work which is most relevant to our model as well as to most previous unsupervised DA for AE work. information of both feature types. Blitzer et al. (2006, 2007) learned linear nonpivot to pivot mappings that do not exploit the input structure (e.g., the structure of the review document in a sentiment classification task). A series of consecutive works alleviated these limitations. For example, Ziser and Reichart (2017) used a feedforward neural network to learn the mapping, also exploiting the semantic similarity between pivots. Later (Ziser and Reichart, 2018b, 2019) proposed PBLM, a pivot-based language model, which also exploits the structure of the input text . Recently, Ben-David et al. (2020) integrated these ideas into the BERT architec"
2021.emnlp-main.20,2020.emnlp-main.572,0,0.44775,"the categories least two different aspects with different sentiment rather than randomly. Further, it employs a new pre- polarities, making it more challenging compared to training task: The prediction of which categories the SemEval datasets. 220 2.2 Domain Adaptation DA is a fundamental challenge in machine learning in general and NLP in particular. In this work, we focus on unsupervised DA, in which labeled data is available from the source domain and unlabeled data is available from both the source and the target domains. DA approaches include instance re-weighting (Mansour et al., 2008; Gong et al., 2020), sub-sampling from both domains (Chen et al., 2011) and learning a shared representation for the source and target domains (Blitzer et al., 2007; Ganin et al., 2016; Ziser and Reichart, 2018b). This section focuses on the latter approach which has become prominent due to the success of deep learning. Indeed, our proposed method, as well as the previous methods and the baselines we compare to, follow this approach. Unsupervised DA via Shared Representation The shared representation approach to unsupervised DA typically consists of two major steps: (a) A representation model is trained using th"
2021.emnlp-main.20,D10-1101,0,0.063276,"highest score. Then we construct a binary vector, where each coordinate corresponds to one of the categories and its value is 1 if the score of that category is higher than β and 0 otherwise. Here again β is a hyper-parameter of the algorithm. Cross-entropy (which is also the loss function of CMLM) is a very natural loss for this task, as we are interested in a model that assigns high probabilities to aspect categories that are represented in the text and low probabilities to aspect categories that are not. 4 Experimental Setup Task and Domains We experiment with the task of cross-domain AE (Jakob and Gurevych, 2010). We consider data from the Amazon laptop reviews (L) and the Yelp restaurant reviews (R) domains. The labeled data from the L domain is taken from the SemEval 2014 task on ABSA (Pontiki et al., 2014). We follow Gong et al. (2020) and combine the SemEval 2014, 2015 and 2016 restaurant datasets (Pontiki et al., 2014, 2015, 2016) into a sin3 This is the static word embedding of the category name. 4 gle dataset, removing all duplicated examples. This We also considered schemes where a randomly selected sub-set of this list is masked, but observed no improvements. results in 3045/800 and 3845/2158"
2021.emnlp-main.20,D19-1654,0,0.0220417,"domly selected sub-set of this list is masked, but observed no improvements. results in 3045/800 and 3845/2158 train/validation 223 examples for the L and R domains, respectively. At test time the target domain validation set serves as a test set. We obtain the list of category names for each domain from Pontiki et al. (2016).5 We used 863,000 laptop reviews from the Amazon reviews dataset of McAuley et al. (2015), and 570,000 reviews from the Yelp open dataset 6 as our unlabeled dataset for both domains, respectively. To consider a more challenging setup, we experiment with the MAMS dataset (Jiang et al., 2019), consisting of 5297 labeled reviews from the restaurant domain (M). We used 4297 reviews as a training set and 1000 as a validation set, and the same unlabeled data as for the R domain. Each review in the MAMS dataset has at least two aspects with different sentiment polarities, making it harder to adapt to, as the label distribution is different from that of the SemEval datasets. We consider adaptation from the L to the M domain and vice versa, adding two source-target pairs to our experiments (the M and R domains both address the same topic, and we consider them too similar for adaptation)."
2021.emnlp-main.20,D19-1466,0,0.0140645,"s. Likewise, Pereg et al. (2020) feature mapping. This way the induced representa- incorporate syntactic knowledge from an external tion consists of the cross-domain and task-relevant parser (Dozat and Manning, 2017) into BERT via 221 its self-attention mechanism. Relying on supervised parsers, these approaches naturally suffer from the degradation of such parsers when applied to resource-poor domains (e.g., user-generated content) or languages. Moreover, the work of Wang and Pan (2018) requires additional human annotation for opinion word labels, which might not be available for new domains. Li et al. (2019) avoids the need for external resources (except from an opinion lexicon), by applying a dual memory mechanism combined with a gradient reversal layer (Ganin et al., 2016), in a model that jointly learns to predict aspect and opinion terms. Recently, Gong et al. (2020) presented the Unified Domain Adaptation (UDA) approach, the first to apply a pre-trained language encoder (BERT) to our task. Particularly, they apply selfsupervised POS and dependency relation information as an auxiliary training task in order to bias BERT towards domain-invariant representations. Then, they apply instance re-we"
2021.emnlp-main.20,D15-1168,0,0.0304334,"We hence first describe in-domain AE, and then continue with a survey of DA, focusing on pivot-based unsupervised DA. Finally, we describe works at the intersection of both problems. 2.1 Aspect Extraction Early in-domain aspect extraction works heavily rely on feature engineering, often feeding graphical models with linguistic-driven features such as partof-speech (POS) tags (Jin et al., 2009), WordNet attributes and word frequencies (Li et al., 2010). The SemEval ABSA task releases (Pontiki et al., 2014, 2015, 2016) and the rise of deep learning have pushed AE research substantially forward. Liu et al. (2015) applied Recurrent Neural Networks (RNN) combined with simple linguistic features to outperform feature-rich Conditional Random Field (CRF) models. Wang et al. (2016) showed that stacking a CRF on top of an RNN further improves the results. Recently, Xu et al. (2019) tuned BERT for AE and obtained additional improvements, demonstrating the effectiveTo overcome these limitations, we present DIL- ness of massive pre-training with unlabeled data BERT: Domain Invariant Learning with BERT, a for this task. Tian et al. (2020) proposed new precustomized fine-tuning procedure for AE in an un- training"
2021.emnlp-main.20,2021.ccl-1.108,0,0.0924444,"Missing"
2021.emnlp-main.20,N19-1039,0,0.0300737,"Missing"
2021.emnlp-main.20,P03-1054,0,0.0265526,"ions, defined in §1. The authors use the distinction between pivot and non- adapting to new, more challenging domains. pivot features (features that do not meet at least one Similarly, inspired by the pivot-based modeling of the criteria, as long as they are frequent in one approach of Blitzer et al. (2006), Wang and Pan of the domains) in order to learn a shared cross- (2018) train a recursive recurrent network to predict domain representation model. The main idea is to source and target dependency trees (obtained by utilize the pivot features to extract cross-domain an off-the-shelf parser (Klein and Manning, 2003)). and task-relevant information from non-pivot fea- Then, they jointly train the model to predict aspect tures, which is done through non-pivot to pivot and opinion words. Likewise, Pereg et al. (2020) feature mapping. This way the induced representa- incorporate syntactic knowledge from an external tion consists of the cross-domain and task-relevant parser (Dozat and Manning, 2017) into BERT via 221 its self-attention mechanism. Relying on supervised parsers, these approaches naturally suffer from the degradation of such parsers when applied to resource-poor domains (e.g., user-generated con"
2021.emnlp-main.20,2020.coling-main.158,0,0.0574563,"Missing"
2021.emnlp-main.20,C10-1074,0,0.0497675,"ackground and Previous Work While both aspect extraction and domain adaptation are active fields of research, research on their intersection is not as frequent. We hence first describe in-domain AE, and then continue with a survey of DA, focusing on pivot-based unsupervised DA. Finally, we describe works at the intersection of both problems. 2.1 Aspect Extraction Early in-domain aspect extraction works heavily rely on feature engineering, often feeding graphical models with linguistic-driven features such as partof-speech (POS) tags (Jin et al., 2009), WordNet attributes and word frequencies (Li et al., 2010). The SemEval ABSA task releases (Pontiki et al., 2014, 2015, 2016) and the rise of deep learning have pushed AE research substantially forward. Liu et al. (2015) applied Recurrent Neural Networks (RNN) combined with simple linguistic features to outperform feature-rich Conditional Random Field (CRF) models. Wang et al. (2016) showed that stacking a CRF on top of an RNN further improves the results. Recently, Xu et al. (2019) tuned BERT for AE and obtained additional improvements, demonstrating the effectiveTo overcome these limitations, we present DIL- ness of massive pre-training with unlabe"
2021.emnlp-main.20,S15-2082,0,0.104409,"rst describe in-domain AE, and then continue with a survey of DA, focusing on pivot-based unsupervised DA. Finally, we describe works at the intersection of both problems. 2.1 Aspect Extraction Early in-domain aspect extraction works heavily rely on feature engineering, often feeding graphical models with linguistic-driven features such as partof-speech (POS) tags (Jin et al., 2009), WordNet attributes and word frequencies (Li et al., 2010). The SemEval ABSA task releases (Pontiki et al., 2014, 2015, 2016) and the rise of deep learning have pushed AE research substantially forward. Liu et al. (2015) applied Recurrent Neural Networks (RNN) combined with simple linguistic features to outperform feature-rich Conditional Random Field (CRF) models. Wang et al. (2016) showed that stacking a CRF on top of an RNN further improves the results. Recently, Xu et al. (2019) tuned BERT for AE and obtained additional improvements, demonstrating the effectiveTo overcome these limitations, we present DIL- ness of massive pre-training with unlabeled data BERT: Domain Invariant Learning with BERT, a for this task. Tian et al. (2020) proposed new precustomized fine-tuning procedure for AE in an un- training"
2021.emnlp-main.20,S14-2004,0,0.136574,"action and domain adaptation are active fields of research, research on their intersection is not as frequent. We hence first describe in-domain AE, and then continue with a survey of DA, focusing on pivot-based unsupervised DA. Finally, we describe works at the intersection of both problems. 2.1 Aspect Extraction Early in-domain aspect extraction works heavily rely on feature engineering, often feeding graphical models with linguistic-driven features such as partof-speech (POS) tags (Jin et al., 2009), WordNet attributes and word frequencies (Li et al., 2010). The SemEval ABSA task releases (Pontiki et al., 2014, 2015, 2016) and the rise of deep learning have pushed AE research substantially forward. Liu et al. (2015) applied Recurrent Neural Networks (RNN) combined with simple linguistic features to outperform feature-rich Conditional Random Field (CRF) models. Wang et al. (2016) showed that stacking a CRF on top of an RNN further improves the results. Recently, Xu et al. (2019) tuned BERT for AE and obtained additional improvements, demonstrating the effectiveTo overcome these limitations, we present DIL- ness of massive pre-training with unlabeled data BERT: Domain Invariant Learning with BERT, a"
2021.emnlp-main.20,J11-1002,0,0.0705655,".3 Domain Adaptation for AE Like most recent DA works, the shared representation approach is prominent in DA for AE, where previous works align the different domains using syntactic patterns, reasoning that such patterns are robust across domains. For example, Ding et al. (2017) learn a shared representation by training an RNN to predict rule-based syntactic patterns, populated by a pre-defined sentiment lexicon. While Shared Representation Using Pivot Features Pivots features, proposed by Blitzer et al. (2006, dependency relations-based rules are known to improve in-domain aspect extraction (Qiu et al., 2011; 2007) through their structural correspondence Wang et al., 2016), using hand-crafted patterns with learning (SCL) framework, are features that meet a pre-defined sentiment lexicon heavily relies on both the domain frequency and the source-domain prior knowledge and might not be robust when task label correlation conditions, defined in §1. The authors use the distinction between pivot and non- adapting to new, more challenging domains. pivot features (features that do not meet at least one Similarly, inspired by the pivot-based modeling of the criteria, as long as they are frequent in one app"
2021.emnlp-main.20,2020.coling-main.603,0,0.0732877,"Missing"
2021.emnlp-main.20,2020.acl-main.374,0,0.0215092,"nd the rise of deep learning have pushed AE research substantially forward. Liu et al. (2015) applied Recurrent Neural Networks (RNN) combined with simple linguistic features to outperform feature-rich Conditional Random Field (CRF) models. Wang et al. (2016) showed that stacking a CRF on top of an RNN further improves the results. Recently, Xu et al. (2019) tuned BERT for AE and obtained additional improvements, demonstrating the effectiveTo overcome these limitations, we present DIL- ness of massive pre-training with unlabeled data BERT: Domain Invariant Learning with BERT, a for this task. Tian et al. (2020) proposed new precustomized fine-tuning procedure for AE in an un- training tasks based on automatically extracted sensupervised DA setup. More specifically, DILBERT timent words and aspect terms. Finally, Jiang et al. employs a variant of the BERT masked language (2019) released the Multi-Aspect Multi-Sentiment modeling (MLM) task such that hidden tokens are (MAMS) dataset, where each sentence contains at chosen by their semantic similarity to the categories least two different aspects with different sentiment rather than randomly. Further, it employs a new pre- polarities, making it more cha"
2021.emnlp-main.20,N19-1242,0,0.137568,"engineering, often feeding graphical models with linguistic-driven features such as partof-speech (POS) tags (Jin et al., 2009), WordNet attributes and word frequencies (Li et al., 2010). The SemEval ABSA task releases (Pontiki et al., 2014, 2015, 2016) and the rise of deep learning have pushed AE research substantially forward. Liu et al. (2015) applied Recurrent Neural Networks (RNN) combined with simple linguistic features to outperform feature-rich Conditional Random Field (CRF) models. Wang et al. (2016) showed that stacking a CRF on top of an RNN further improves the results. Recently, Xu et al. (2019) tuned BERT for AE and obtained additional improvements, demonstrating the effectiveTo overcome these limitations, we present DIL- ness of massive pre-training with unlabeled data BERT: Domain Invariant Learning with BERT, a for this task. Tian et al. (2020) proposed new precustomized fine-tuning procedure for AE in an un- training tasks based on automatically extracted sensupervised DA setup. More specifically, DILBERT timent words and aspect terms. Finally, Jiang et al. employs a variant of the BERT masked language (2019) released the Multi-Aspect Multi-Sentiment modeling (MLM) task such tha"
2021.emnlp-main.20,K17-1040,1,0.761963,"gap. Many works have followed this avenue (e.g., (Chen et al., 2012; Louizos et al., 2016; Ganin et al., 2016)) and a comprehensive survey is beyond the scope of this paper. Below we discuss the line of work which is most relevant to our model as well as to most previous unsupervised DA for AE work. information of both feature types. Blitzer et al. (2006, 2007) learned linear nonpivot to pivot mappings that do not exploit the input structure (e.g., the structure of the review document in a sentiment classification task). A series of consecutive works alleviated these limitations. For example, Ziser and Reichart (2017) used a feedforward neural network to learn the mapping, also exploiting the semantic similarity between pivots. Later (Ziser and Reichart, 2018b, 2019) proposed PBLM, a pivot-based language model, which also exploits the structure of the input text . Recently, Ben-David et al. (2020) integrated these ideas into the BERT architecture. They changed its Masked Language Modeling task so that pivots are more often masked than non-pivots, and the model should predict if a token is a pivot or not, and then identify the token only in the former case. Despite the great success of this line of work on"
2021.emnlp-main.20,D18-1022,1,0.915261,"ch challenge. While the target domain labeled data availability in DA setups ranges from little (supervised DA (Daumé III, 2007)) to none (unsupervised DA (Ramponi and Plank, 2020)), unlabeled data is typically available in both source and target domains. This paper focuses on unsupervised DA as we believe it is a more realistic and practical scenario. Due to the great success of deep learning models, DA through representation learning (Blitzer et al., 2007; Ziser and Reichart, 2017), i.e., learning a shared representation for both the source and target domains, has recently become prominent (Ziser and Reichart, 2018a; Ben-David et al., 2020). Of particular importance to this line of work are approaches that utilize pivot features (Blitzer et al., 2006) that: (a) frequently appear in both domains; and (b) have high mutual information (MI) with the task label. While pivot-based methods achieve state-of-the-art results in many text classification tasks (Ziser and Reichart, 2018a; Miller, 2019; BenDavid et al., 2020), it is not trivial to successfully apply them on tasks such as AE. This stems from two reasons: First, AE is a sequence tagging task with multiple labels for each input example (i.e., word-level"
2021.emnlp-main.20,N18-1112,1,0.915537,"ch challenge. While the target domain labeled data availability in DA setups ranges from little (supervised DA (Daumé III, 2007)) to none (unsupervised DA (Ramponi and Plank, 2020)), unlabeled data is typically available in both source and target domains. This paper focuses on unsupervised DA as we believe it is a more realistic and practical scenario. Due to the great success of deep learning models, DA through representation learning (Blitzer et al., 2007; Ziser and Reichart, 2017), i.e., learning a shared representation for both the source and target domains, has recently become prominent (Ziser and Reichart, 2018a; Ben-David et al., 2020). Of particular importance to this line of work are approaches that utilize pivot features (Blitzer et al., 2006) that: (a) frequently appear in both domains; and (b) have high mutual information (MI) with the task label. While pivot-based methods achieve state-of-the-art results in many text classification tasks (Ziser and Reichart, 2018a; Miller, 2019; BenDavid et al., 2020), it is not trivial to successfully apply them on tasks such as AE. This stems from two reasons: First, AE is a sequence tagging task with multiple labels for each input example (i.e., word-level"
2021.emnlp-main.20,P19-1591,1,0.851206,"In Domain M L R BERT-ID 0.33 0.77 0.41 L-R Average 1.8 1 3.95 1.1 1.26 3.11 2.17 2.29 1.39 1.41 1.76 1.85 1.39 1.47 1.85 1.88 Average 0.51 Table 4: Standard deviations for all models across the five folds of the cross-validation protocol. folds of the cross-validation protocol. DILBERT has the lowest averaged standard-deviation among all DA models and No-DA baselines. This is obviously another advantage of our proposed model, particularly that in unsupervised domain adaptation there is no labeled target domain data available for model selection, and hence stability to random seeds is crucial (Ziser and Reichart, 2019). 6 Conclusions and Future Work previously presented ones. In future work, we would like to extend our approach so that it can jointly solve the aspect extraction and sentiment analysis tasks (the ABSA task). Moreover, we would like to verify the quality of our approach in additional domains, tasks (i.e., going beyond AE to other tasks that present category shifts when domains change) and eventually even languages. Acknowledgments We would like to thank the members of the IE@Technion NLP group for their valuable feedback and advice. This research was partially funded by an ISF personal grant N"
C08-1002,P98-1013,0,0.153692,"Missing"
C08-1002,P98-1046,0,0.0308457,"where most instances were monosemous. For completeness, we compared our method to theirs2 , achieving similar results. We review related work in Section 2, and discuss the task in Section 3. Section 4 introduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ section"
C08-1002,C96-1055,0,0.370452,"Missing"
C08-1002,W01-0502,0,0.0298959,"1 >, < x2 , C2 , c2 >, ..., < xn , Cn , cn > } ⊆ (X × 2S × S)n , where n is the size of the training set. Let < xn+1 , Cn+1 >∈ (X × 2S ) be a new instance. Our task is to select which of the labels in Cn+1 is its correct label cn+1 (xn+1 does not have to be a previously observed lemma, but its lemma must appear in a VN class). The structure of the task lets us apply a learning algorithm that is especially appropriate for it. What we need is an algorithm that allows us to restrict the possible labels of each instance, both in training and in testing. The sequential model algorithm presented by Even-Zohar and Roth (2001) directly supports this requirement. We use the SNOW learning architecture for multi-class classification (Roth, 1998), which contains an implementation of that algorithm 9 . 5 Experimental Setup We used SemLink VN annotations and parse trees on sections 02-21 of the WSJ Penn Treebank for training, and section 00 as a development set, as is common in the parsing community. We performed two parallel sets of experiments, one using manually created gold standard parse trees and one using parse trees created by a state-of-the-art 9 Experiments on development data revealed that for verbs for which"
C08-1002,J01-3003,0,0.115307,"Missing"
C08-1002,P05-3014,0,0.0532961,"Missing"
C08-1002,J05-1004,0,0.151397,"in classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made available to the community. 2 Using the same sentences and instances, obtained from the authors. 10 number of classes relative to the number of types6 . A classifier may gather valuable information for all members of a certain VN class, without seeing all of its members in the training data. From this perspective the task resembles POS tagging. In both tasks there are many dozens (or more) of possible labels, while eac"
C08-1002,C00-2108,0,0.214619,"Missing"
C08-1002,kipper-etal-2006-extending,0,0.0365947,"ed work in Section 2, and discuss the task in Section 3. Section 4 introduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made"
C08-1002,H05-1111,0,0.166298,"Missing"
C08-1002,P98-1112,0,0.0739319,"Missing"
C08-1002,N07-1069,0,0.045338,"Missing"
C08-1002,W04-2606,0,0.0702308,"ntroduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made available to the community. 2 Using the same sentences and instances, obtai"
C08-1002,J03-4003,0,\N,Missing
C08-1002,C98-1046,0,\N,Missing
C08-1002,C98-1108,0,\N,Missing
C08-1002,C98-1013,0,\N,Missing
C08-1002,P05-1022,0,\N,Missing
C08-1002,J04-1003,0,\N,Missing
C08-1002,C96-2102,0,\N,Missing
C08-1091,P06-1109,0,0.121181,"NLP applications that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented"
C08-1091,W06-2912,0,0.555938,"NLP applications that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented"
C08-1091,P07-1051,0,0.0120828,"pirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (Van Zaanen, 2001), and improving language models (Chen, 1995). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank. Recently, works along this line have for the first time outperformed the right branching heuristic baseline for English. These include the constituent–context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar– based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). All of these use as input POS tag sequences, except of Seginer’s algorithm, which uses plain text. All of these papers induce unlabeled bracketing or dependencies. There are other algorithmic approaches to the problem (e.g., (Adriaans, 1992; Daelemans, 1995; Van Zaanen, 2001)). None of these had evaluated labeled bracketing on annotated corpora. In this paper we focus on the induction of labeled bracketing. Bayesian Model Merging 2 1 The"
C08-1091,A00-1031,0,0.0554477,"Missing"
C08-1091,P95-1031,0,0.0433833,"on 3 we detail our algorithm. The experimental setup and results are presented in Sections 4 and 5. 2 Previous Work Unsupervised parsing has attracted researchers for decades (see (Clark, 2001; Klein, 2005) for recent reviews). Many types of input, syntax formalisms, search procedures, and success criteria were used. Among the theoretical and practical motivations to this problem are the study of human language acquisition (in particular, an empirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (Van Zaanen, 2001), and improving language models (Chen, 1995). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank. Recently, works along this line have for the first time outperformed the right branching heuristic baseline for English. These include the constituent–context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar– based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2"
C08-1091,E03-1009,0,0.335106,"Missing"
C08-1091,P06-1111,0,0.512516,"in the grammar induction models of (Wolf, 1982; Langley and Stromsten, 2000; Petasis et al., 2004; Solan et al., 2005). The BMM model used here (Borensztajn and Zuidema, 2007) combines features of (Petasis et al., 2004) and Stolcke’s algorithm, applying the minimum description length (MDL) principle. We use it here only for initial labeling of existing bracketings. The MDL principle was also used in (Grunwald, 1994; de Marcken, 1995; Clark, 2001). There are only two previous papers we are aware of that induce labeled bracketing and evaluate on corpora annotated with a similar representation (Haghighi and Klein, 2006; Borensztajn and Zuidema, 2007). We utilize and extend the latter’s labeling algorithm. However, the evaluation done by the latter dealt only with labeling, using gold-standard (manually annotated) bracketings. Thus, we can directly compare our results only to (Haghighi and Klein, 2006), where two models (PCFG × NONE and PCFG × CCM) are fully unsupervised. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously, as opposed to our three step method3 . 3 Algorithm Our model consists of three stages: bracketing, initial labeling, and label clusterin"
C08-1091,H05-1004,0,0.0117696,"s other methods in future papers. find a (one-to-one) matching M from X to Y having a maximal weight. In our case, X is the set of model symbols, Y is the set of T or P most frequent target symbols (depending on the desired label set size used), and wij := CXi ,Yj , computed as in greedy mapping (the number of times xi and yj share a constituent). To make the graph complete, we add zero weight edges between induced and target labels that do not share any constituent. The Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) solves this problem, and we used it to perform the LL mapping (see also (Luo, 2005)). We assessed the overall quality of our algorithm, the quality of its labeling stage and the quality of the syntactic clustering (SC) stage. For the overall quality of the induced grammar (both bracketing and labeling) we compare our results with (Haghighi and Klein, 2006), using their setup10 . That setup was used for all numbers reported in this paper. Note that a random baseline would yield very poor results, so there is nothing to be gained from comparing to it. We assessed the quality of the labeling (MDL and SC) stages alone, using only the correct bracketings produced by the first sta"
C08-1091,D07-1043,0,0.0931203,"o map many induced labels to the same target label, and is therefore highly forgiving of large mismatches between the structures of the induced and target grammars. Hence, we also applied a label-to-label (LL) mapping, computed by reducing this problem to optimal assignment in a weighted complete bipartite graph, formally defined as follows. Given a weighted complete bipartite graph G = (X ∪ Y ; X × Y ) where edge (Xi , Yj ) has weight wij , 8 Excluding punctuation and null elements, according to the scheme of (Klein, 2005). 9 There are many possible methods for evaluating clustering quality (Rosenberg and Hirschberg, 2007). For our task, overall f-score is a very natural one. We will address other methods in future papers. find a (one-to-one) matching M from X to Y having a maximal weight. In our case, X is the set of model symbols, Y is the set of T or P most frequent target symbols (depending on the desired label set size used), and wij := CXi ,Yj , computed as in greedy mapping (the number of times xi and yj share a constituent). To make the graph complete, we add zero weight edges between induced and target labels that do not share any constituent. The Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) solv"
C08-1091,P07-1049,0,0.794444,"r output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented here belongs to the first group, annotati"
C08-1091,P06-1072,0,0.179426,"tions that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented here belongs to the first"
C08-1091,P02-1017,0,0.156135,"e used. Among the theoretical and practical motivations to this problem are the study of human language acquisition (in particular, an empirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (Van Zaanen, 2001), and improving language models (Chen, 1995). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank. Recently, works along this line have for the first time outperformed the right branching heuristic baseline for English. These include the constituent–context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar– based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). All of these use as input POS tag sequences, except of Seginer’s algorithm, which uses plain text. All of these papers induce unlabeled bracketing or dependencies. There are other algorithmic approaches to the problem (e.g., (Adriaans, 1992; Daelemans, 1995; Van Zaanen, 2001)). None of these had evaluated labeled bra"
C08-1091,P04-1061,0,0.431009,"language, and it can potentially assist NLP applications that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper."
C08-1091,C02-1145,0,0.0861723,"highest score: Map(bi ) = argmaxj v bv i · aj |bv ||av | i j (3) The cosine metric grows when the same coordinates (features) in both vectors have higher values. As a result, vectors with high values of the same features (corresponding to similar syntactic behavior) get high scores. 4 Experimental Setup We evaluated our algorithm on English, German and Chinese corpora: the WSJ Penn Treebank, containing economic English newspaper articles, the Brown corpus, containing various English genres, the Negra corpus (Brants, 1997) of German newspaper text, and version 5.0 of the Chinese Penn Treebank (Xue et al., 2002). In each corpus, we used the sentences of length at most 108 , numbering 7422 (WSJ10), 9117 (Brown10), 7542 (NEGRA10) and 4626 (CTB10). For each corpus the following T and P values were used: WSJ10: 26, 8; Brown10: 28, 7; NEGRA10: 22, 6; CTB10: 24, 9. Each number produces a different grammar. For labeled f-score evaluation, the induced labels should be mapped to the target labels9 . We evaluated with two different mapping schemes. For each pair (Xi , Yj ) of induced and target labels, let CXi ,Yj be the number of times they label a constituent having the same span in the same sentence. Follow"
C10-2146,A00-1031,0,0.093527,"n word Tagging. Section 3 describes our web-query based algorithm. Section 4 and Section 5 describe experimental setup and results. 2 Previous Work Most supervised POS tagging works address the issue of unknown words. While the general methods of POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interacti"
C10-2146,D07-1019,0,0.0210065,"ormance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance usin"
C10-2146,W04-3205,0,0.023381,"rb-object bigrams from the web by query1 We did follow their experimental procedure as much as we could. Like (Blitzer et al., 2006), we compare our algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data"
C10-2146,W03-0407,0,0.0743266,"to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al., 2003). 3 the words on the right. For each context type we define a web query using two common features supported by the major search engines: wild-card search, expressed using the ‘*’ character, and exact sentence search, expressed by quoted characters. The retrieved sentences contain the parts enclosed in quotes in the exact same place they appear in the query, while an asterisk can be replaced by any single word. For a word u we execute the following three queries for each of its test contexts: 1. Replacement: &quot;u−2 u−1 ∗ u+1 u+2 &quot;. This retrieves words that appear in the same context as u. 2. Lef"
C10-2146,I05-1006,0,0.0296337,"Missing"
C10-2146,J93-2004,0,0.0357365,"Missing"
C10-2146,N06-1020,0,0.0233453,"daptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al., 2003). 3 the words on the right. For each context type we define a web query us"
C10-2146,P06-1089,0,0.0216094,"ally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interactions between POS tags of unknown words with the same lexical form. Unknown word tagging has also been explored in the context of domain adaptation of POS taggers. In this context two directions were explored: a supervised method that requires a manually annotated corpus from the target domain (Daume III, 2007), and a semi-supervised method that uses an unlabeled corpus from the target domain (Blitzer et al., 2006). Both methods require the preparation of a corpus of target domain sentences and re-training the learning algorithm. Blitzer"
C10-2146,C08-1089,0,0.0122093,"serves an application working on web text). Second, preparing a corpus is time consuming, especially when it needs to be manually annotated. Our algorithm requires no corpus from the target data domain, no preprocessing step, and it doesn’t even need to know the identity of the target domain. Consequently, the problem we address here is more difficult (and arguably more useful) than that addressed in previous work1 . The domain adaptation techniques above have not been applied to languages other than English, while our algorithm is shown to perform well in seven scenarios in three languages. Qiu et al. (2008) explored Chinese unknown word POS tagging using internal component and contextual features. Their work is not directly comparable to ours since they did not test a domain adaptation scenario, and used substantially different corpora and evaluation measures in their experiments. Numerous works utilized web resources for NLP tasks. Most of them collected corpora using data mining techniques and used them offline. For example, Keller et al., (2002) and Keller and Lapata (2003) described a method to obtain frequencies for unseen adjective-noun, noun-noun and verb-object bigrams from the web by qu"
C10-2146,W02-1001,0,0.0484009,"he run time overhead is less than 0.5 seconds per an unknown word in the English and German experiments, and less than a second per unknown word in the Chinese experiments. Section 2 reviews previous work on unknown word Tagging. Section 3 describes our web-query based algorithm. Section 4 and Section 5 describe experimental setup and results. 2 Previous Work Most supervised POS tagging works address the issue of unknown words. While the general methods of POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological"
C10-2146,W96-0213,0,0.25897,"this segmentation. To obtain the words filling the ‘*’ slots in our queries, we take all possible segmentations in which the two words appears in our training data. The queries we use in our algorithm are not the only possible ones. For example, a possible query 2 http://developer.yahoo.com/search/boss/ we do not use for the word u is &quot;∗∗u−1 uu+1 u+2 &quot;. The aforementioned set of queries gave the best results in our English, German and Chinese development data and is therefore the one we used. 3.2 Final Tagging The MXPOST Tagger. We integrated our algorithm into the maximum entropy tagger of (Ratnaparkhi, 1996). The tagger uses a set h of contexts (‘history’) for each word wi (the index i is used to allow an easy notation of the previous and next words, whose lexemes and POS tags are used as features). For each such word, the tagger computes the following conditional probability for the tag tr : p(h, tr ) p(tr |h) = P (1) ′ t′r ∈T p(h, tr ) where T is the tag set, and the denominator is simply p(h). The joint probability of a history h and a tag t is defined by: p(h, t) = Z k Y f (h,t) αj j (2) j=1 where α1 , . . . , αk are the model parameters, f1 , . . . , fk are the model’s binary features (indic"
C10-2146,P07-1033,0,0.100422,"Missing"
C10-2146,P07-1078,1,0.847885,"biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al., 2003). 3 the words on the right. For each context type we define a web query using two common features supporte"
C10-2146,P07-1030,1,0.845268,"r algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagg"
C10-2146,H05-1058,0,0.0393421,"Missing"
C10-2146,I05-3005,0,0.0595225,"cy Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interactions between POS tags of unknown words with the same lexical form. Unknown word tagging has also been explored in the context of domain adaptation of POS taggers. In this context two directions were explored: a supervised method that requires a manually annotated corpus from the target domain (Daume III, 2007), and a semi-supervised method that uses an unlabeled corpus from the target domain (Blitzer et al., 2006). Both methods r"
C10-2146,W04-3206,0,0.0111541,"tal procedure as much as we could. Like (Blitzer et al., 2006), we compare our algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words"
C10-2146,W02-1030,0,0.029218,"chniques above have not been applied to languages other than English, while our algorithm is shown to perform well in seven scenarios in three languages. Qiu et al. (2008) explored Chinese unknown word POS tagging using internal component and contextual features. Their work is not directly comparable to ours since they did not test a domain adaptation scenario, and used substantially different corpora and evaluation measures in their experiments. Numerous works utilized web resources for NLP tasks. Most of them collected corpora using data mining techniques and used them offline. For example, Keller et al., (2002) and Keller and Lapata (2003) described a method to obtain frequencies for unseen adjective-noun, noun-noun and verb-object bigrams from the web by query1 We did follow their experimental procedure as much as we could. Like (Blitzer et al., 2006), we compare our algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is"
C10-2146,W00-1308,0,0.0184986,"POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interactions between POS tags of unknown words with the same lexical form. Unknown word tagging has also been explored in the context of domain adaptation of POS taggers. In this context two directions were explored: a supervised method that requires a manually annotated"
C10-2146,N03-1033,0,0.126243,"an unknown word in the English and German experiments, and less than a second per unknown word in the Chinese experiments. Section 2 reviews previous work on unknown word Tagging. Section 3 describes our web-query based algorithm. Section 4 and Section 5 describe experimental setup and results. 2 Previous Work Most supervised POS tagging works address the issue of unknown words. While the general methods of POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orth"
C10-2146,C02-1145,0,0.0503884,"Missing"
C10-2146,J03-3005,0,\N,Missing
C10-2146,W06-1615,0,\N,Missing
C10-2146,U05-1007,0,\N,Missing
C12-1141,P10-1131,0,0.016314,"algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneous"
C12-1141,D10-1117,0,0.0133099,"Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously 2 . Borensztajn and Z"
C12-1141,W06-2912,0,0.0228754,"blem and use the k-way normalized cut algorithm (Yu and Shi, 2003) to solve it. We evaluate our algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NON"
C12-1141,N09-1009,0,0.0970314,"u and Shi, 2003) to solve it. We evaluate our algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM a"
C12-1141,J02-3001,0,0.0309942,"ree nonterminals (e.g., ‘NP’, ‘VP’, ‘PP’) in an unsupervised manner. We keep the discussion of dependency parsing for future research. Many linguistic theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive"
C12-1141,P10-2036,0,0.0368109,"Missing"
C12-1141,P06-1111,0,0.338063,"s a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then clustering these categories into a ﬁnal set. In this work we focus on improving the critical last stage of narrowing down the large number of induced BMM label"
C12-1141,P07-1107,0,0.0348312,"uctures are different in nature from the phrase categories explored in this paper, our algorithm may be applicable to this case as well. We keep this question for future research. DP has been used for unsupervised syntactic acquisition tasks. Finkel and Manning (2007) Used DP for unsupervised POS induction from dependency structures. Liang et al. (2009) proposed a nonparametric Bayesian generalization of PCFG, based on the hierarchical Dirichlet Process, and applied it to supervised parsing. DP has been used for many other NLP tasks as well (e.g. (Goldwater et al., 2006; Johnson et al., 2007; Haghighi and Klein, 2007; Johnson and Goldwater, 2009)). However, we are not aware of works that explored DP as a model for creating a diverse ensemble of experts for clustering tasks. 3 Building a Diverse Clustering Ensemble As discussed, to induce phrase categories we adopt a two stage approach where we ﬁrst induce a large number of categories and then narrow this into a ﬁnal set. The ﬁrst stage, leading to a collection of BMM categories is similar to RR08. Our novelty is in taking a qualitatively different approach to the critical stage of narrowing down the large number of categories into a small informative set."
C12-1141,N09-1012,0,0.0969143,"7 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe substantially different syntactic functions. In this paper we"
C12-1141,N09-1036,0,0.0308732,"ature from the phrase categories explored in this paper, our algorithm may be applicable to this case as well. We keep this question for future research. DP has been used for unsupervised syntactic acquisition tasks. Finkel and Manning (2007) Used DP for unsupervised POS induction from dependency structures. Liang et al. (2009) proposed a nonparametric Bayesian generalization of PCFG, based on the hierarchical Dirichlet Process, and applied it to supervised parsing. DP has been used for many other NLP tasks as well (e.g. (Goldwater et al., 2006; Johnson et al., 2007; Haghighi and Klein, 2007; Johnson and Goldwater, 2009)). However, we are not aware of works that explored DP as a model for creating a diverse ensemble of experts for clustering tasks. 3 Building a Diverse Clustering Ensemble As discussed, to induce phrase categories we adopt a two stage approach where we ﬁrst induce a large number of categories and then narrow this into a ﬁnal set. The ﬁrst stage, leading to a collection of BMM categories is similar to RR08. Our novelty is in taking a qualitatively different approach to the critical stage of narrowing down the large number of categories into a small informative set. Motivation For the Ensemble A"
C12-1141,P02-1017,0,0.051686,"malize this idea as a global optimization problem and use the k-way normalized cut algorithm (Yu and Shi, 2003) to solve it. We evaluate our algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) p"
C12-1141,P03-1054,0,0.0391133,"Missing"
C12-1141,P04-1061,0,0.0404688,"Papers, pages 2307–2324, COLING 2012, Mumbai, December 2012. 2307 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe"
C12-1141,P08-1006,0,0.0158071,"c theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then clustering these categories into a ﬁnal set. In this"
C12-1141,C02-1145,0,0.114958,"Missing"
C12-1141,J08-2005,0,0.0260252,"’, ‘VP’, ‘PP’) in an unsupervised manner. We keep the discussion of dependency parsing for future research. Many linguistic theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories"
C12-1141,C08-1091,1,0.894179,"annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then clustering these categories into a ﬁnal set. In this work we focus on improving the critical last stage of narrowing down the large number of induced BMM labels into a smaller set of informative categories1 . Naively, one might think that simply replacing the s"
C12-1141,W09-1121,1,0.782673,"out performing a direct mapping to the gold standard. They are based on the observation that a good clustering reduces the uncertainty of the gold standard cluster given the induced cluster and vice-versa. Several such measures exist, we use two widely–accepted ones, the V (Rosenberg and Hirschberg, 2007) measure and the VI (Meila, 2007) measure. The V measure is deﬁned as follows: V= h=1− H(G|T ) H(G) 2hc h+ c ,c = 1− H(T |G) H(T ) For the VI measure, we report its normalized version, NVI. NVI and VI induce the same order over clusterings but NVI values for good clusterings ranges in [0, 1] (Reichart and Rappoport, 2009). The NVI measure is deﬁned to be: NV I = H(G|T ) + H(T |G) H(G) Note that V scores are in [0, 1] and the higher the score, the better the clustering. For NVI, the scores are non-negative and lower scores imply improved clustering quality. We use e as the base of the logarithm. Many other clustering evaluation measures exist. The ones we use here are well accepted in the literature. For a recent review see (Reichart and Rappoport, 2009). 7 Results In this section we demonstrate the effectiveness of our DPMM ensemble for the task of unsupervised induction of syntactic categories (parse tree non"
C12-1141,D07-1043,0,0.139262,"apping the induced labels to the gold labels and then computing the standard labeled parsing F–score 10 . While the labeling accuracy after mapping is not explicitly given, it can computed by dividing the unlabeled F–score with the labeled F–score. The IT based measures provides a way to evaluate the induced clustering without performing a direct mapping to the gold standard. They are based on the observation that a good clustering reduces the uncertainty of the gold standard cluster given the induced cluster and vice-versa. Several such measures exist, we use two widely–accepted ones, the V (Rosenberg and Hirschberg, 2007) measure and the VI (Meila, 2007) measure. The V measure is deﬁned as follows: V= h=1− H(G|T ) H(G) 2hc h+ c ,c = 1− H(T |G) H(T ) For the VI measure, we report its normalized version, NVI. NVI and VI induce the same order over clusterings but NVI values for good clusterings ranges in [0, 1] (Reichart and Rappoport, 2009). The NVI measure is deﬁned to be: NV I = H(G|T ) + H(T |G) H(G) Note that V scores are in [0, 1] and the higher the score, the better the clustering. For NVI, the scores are non-negative and lower scores imply improved clustering quality. We use e as the base of the logarithm"
C12-1141,P07-1049,0,0.369829,"ember 2012. 2307 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe substantially different syntactic func"
C12-1141,P06-1072,0,0.12929,"COLING 2012, Mumbai, December 2012. 2307 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe substantially different"
C12-1141,D11-1117,0,0.0346509,"Missing"
C12-1141,N10-1116,0,0.0130274,"n measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously 2 . Borensztajn and Zuidema (2007) presented the Bayesian Model Merging"
C12-1141,W10-2902,0,0.0201848,"n measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously 2 . Borensztajn and Zuidema (2007) presented the Bayesian Model Merging"
C12-1141,D11-1118,0,0.0429292,"Missing"
C12-1141,C04-1060,0,0.0267815,"dency parsing for future research. Many linguistic theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then cluster"
C12-2097,P04-1056,0,0.0167077,"ross-document (e.g. sentence similarity) levels, and can be flexibly applied to both fully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role"
C12-2097,P05-1045,0,0.0244601,"ce similarity) levels, and can be flexibly applied to both fully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information s"
C12-2097,P10-2036,0,0.0196981,"os, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information structure of the document. As our data is biomedical, we use the version of the AZ scheme adapted for biology by (Miz"
C12-2097,D11-1025,1,0.888857,"le to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). This comes, however, with a heavy cost of requiring thousands of manually annotated sentences to achieve good performance. Even the weakly supervised approach by (Guo and Korhonen, 2011) requires hundreds of annotated sentences for optimal performance. In this paper we focus on a primarily unsupervised approach to inferring information structure which avoids the high annotation cost of the supervised approaches. The only previous work on this topic that we are aware of is that of (Varge et al., 2012) who proposed a simple word-level Latent Dirichlet Allocation (LDA) model to the task, assuming that the phenomenon is mostly lexical. As we show in this paper, the information structure of scientific documents is governed by a number of additional factors, which calls for a more"
C12-2097,W10-1913,1,0.871954,"riments the algorithm provably finds the exact MAP assignment. 996 We compare the predictions of our model to those of argumentative zoning (AZ) – a widely used information structure scheme (Teufel and Moens, 2002) where the core categories are argued to be domain-independent and which has been used to analyse texts in various disciplines such as computational linguistics (Teufel and Moens, 2002), law (Hachey and Grover, 2006), biology (Mizuta et al., 2006) and chemistry (Teufel et al., 2009). We experiment with the only publicly available AZ corpus: the corpus of 792 biomedical abstracts by (Guo et al., 2010) which provides AZ annotations for 7886 sentences. Our experimental evaluation shows that the model outperforms traditional algorithms for both the unsupervised and the transductive setups. by a large margin Our results show that it is possible to infer high quality knowledge about the information structure of scientific documents even when only little or no human annotation effort is involved. 2 Previous Work Machine Learning for Information Structure Nearly all previous work on automatic detection of information structure has relied on supervised algorithms and, consequently, on corpora cons"
C12-2097,I08-1050,0,0.607355,"G 2012, Mumbai, December 2012. 995 1 Introduction Information structure of scientific literature (i.e. the way scientists communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retri"
C12-2097,liakata-etal-2010-corpora,0,0.121932,"earch within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). This comes, however, with a heavy cost of requiring thousands of manually annotated sentences to achieve good performa"
C12-2097,W06-3309,0,0.686947,"es 995–1006, COLING 2012, Mumbai, December 2012. 995 1 Introduction Information structure of scientific literature (i.e. the way scientists communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks s"
C12-2097,N12-1008,1,0.8603,"ully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information structure of the document. As our data is biomedical, we use the vers"
C12-2097,D12-1131,1,0.849641,"y applied to both fully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information structure of the document. As our data i"
C12-2097,J02-4002,0,0.925022,"ientific literature (i.e. the way scientists communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 200"
C12-2097,D09-1155,0,0.28462,"communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007"
C12-2097,varga-etal-2012-unsupervised,0,0.435568,"extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). This comes, however, with a heavy cost of requiring thousands of manually annotated sentences to achieve good performance. Even the weakly supervised approach by (Guo and Korhonen, 2011) requires hundreds of annotated sentences for optimal performance. In this paper we focus on a primarily unsupervised approach to inferring information structure which avoids the high annotation cost of the supervised approaches. The only previous work on this topic that we are aware of is that of (Varge et al., 2012) who proposed a simple word-level Latent Dirichlet Allocation (LDA) model to the task, assuming that the phenomenon is mostly lexical. As we show in this paper, the information structure of scientific documents is governed by a number of additional factors, which calls for a more expressive model. We propose a more sophisticated and flexible model capable of integrating different types of task knowledge, depending on the knowledge available in a real-life situation. We investigate two scenarios: (1) the fully unsupervised scenario where no manually annotated sentences are available; and (2) th"
C12-2097,W04-1202,0,\N,Missing
C12-2097,L10-1000,0,\N,Missing
C12-3023,P07-2009,0,0.0285208,"ning. The idea of active learning is to create a high-performance classifier but to minimize the cost of annotation. On the server side we tested the most popular classifiers including Naive Bayes classifier, Support Vector Machines (SVM), Maximum Entropy Model, Conditional Random Fields, among many others, and SVM is so far the best classifier for this task. The features listed below were used for classification. Most of them have been successfully used in recent related work (Teufel and Moens, 2002; Mullen et al., 2005; Merity et al., 2009; Guo et al., 2011b). The C&C POS tagger and parser (Curran et al., 2007) was used for extracting syntactic features such as grammatical relations (GR) from each sentence. Section. Normalized section names (Introduction, Methods, Results, Discussion). Location in article/section/paragraph. Each article/section/paragraph was divided into ten equal parts. Location was defined by the parts where the sentence begins and ends. Citation. The number of citations in a sentence (0, 1 or more). Table and Figure. The number of referred tables and figures in a sentence (0, 1 or more). N-gram. Any unigrams and bigrams in the corpus (an n-gram feature equals 1 if it is observed"
C12-3023,W10-1913,1,0.75623,"information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is interesting since it can facilitate easi"
C12-3023,D11-1025,1,0.706692,"ka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is interesting since it can facilitate easier porting of the techniques to new tasks. However, also approaches based on weak supervision require data annotation in real-world applications. Moreover, simulation of active learning (such as that conducted by (Guo et al., 2011b) who used a fully annotated corpus from which they restored the labels of selected sentences in each iteration) is not practical but real-time interactive annotati"
C12-3023,liakata-etal-2010-corpora,0,0.0184916,"on section may include information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is interesting since it"
C12-3023,W09-3603,0,0.0197104,"ning. This process can be repeated many times that is called active learning. The idea of active learning is to create a high-performance classifier but to minimize the cost of annotation. On the server side we tested the most popular classifiers including Naive Bayes classifier, Support Vector Machines (SVM), Maximum Entropy Model, Conditional Random Fields, among many others, and SVM is so far the best classifier for this task. The features listed below were used for classification. Most of them have been successfully used in recent related work (Teufel and Moens, 2002; Mullen et al., 2005; Merity et al., 2009; Guo et al., 2011b). The C&C POS tagger and parser (Curran et al., 2007) was used for extracting syntactic features such as grammatical relations (GR) from each sentence. Section. Normalized section names (Introduction, Methods, Results, Discussion). Location in article/section/paragraph. Each article/section/paragraph was divided into ten equal parts. Location was defined by the parts where the sentence begins and ends. Citation. The number of citations in a sentence (0, 1 or more). Table and Figure. The number of referred tables and figures in a sentence (0, 1 or more). N-gram. Any unigrams"
C12-3023,D09-1067,1,0.794821,"article/section/paragraph. Each article/section/paragraph was divided into ten equal parts. Location was defined by the parts where the sentence begins and ends. Citation. The number of citations in a sentence (0, 1 or more). Table and Figure. The number of referred tables and figures in a sentence (0, 1 or more). N-gram. Any unigrams and bigrams in the corpus (an n-gram feature equals 1 if it is observed in the sentence and 0 if not; the rest of the features are defined in a similar way). Verb. Any verbs in the corpus. Verb Class. Verbs are grouped into 60 categories by spectral clustering (Sun and Korhonen, 2009). Each category corresponds to a feature. Tense and Voice. Tense and voice indicated by the POS tag of main verbs and auxiliary verbs. e.g. have|VBZ be|VBN __|VBN indicates present perfect tense, passive voice. Grammatical Relation. Subject (ncsubj), direct object (dobj), indirect object (iobj) and second object (obj2) relations for verbs. e.g. (ncsubj observed difference obj). 186 Subj/Obj. The subjects/objects appearing with any verbs in the corpus. We implemented a number of query strategies for SVM-based active learning, including least confident sampling (Lewis and Gale, 1994), margin sam"
C12-3023,J02-4002,0,0.593882,"nterest, many sections tend to include different types of information (e.g. the Discussion section may include information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining"
C12-3023,D09-1155,0,0.125929,"on (e.g. the Discussion section may include information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is"
C12-3023,W04-1202,0,\N,Missing
C14-1153,P13-1023,1,0.827372,"Missing"
C14-1153,J10-4006,0,0.112148,"Missing"
C14-1153,P99-1008,0,0.273486,"hy (MEG) (Sudre et al., 2012). See (Martin, 2007) for a detailed survey. This parallel evidence to the prominence of our categories provides substance for intriguing future research. 3 Symmetric Patterns Patterns. In this work, patterns are combinations of words and wildcards, which provide a structural phrase representation. Examples of patterns include “X and Y”, “X such as Y”, “X is a country”, etc. Patterns can be used to extract various relations between words. For example, patterns such as “X of a Y” (“basement of a building”) can be useful for detecting the meronymy (part-of) relation (Berland and Charniak, 1999). Symmetric patterns (e.g., “X and Y”, “France and Holland”), which we use in this paper, can be used to detect semantic similarity between words (Widdows and Dorow, 2002). Symmetric Patterns. Symmetric patterns are patterns that contain exactly two wildcards, and where these wildcards are interchangeable. Examples of symmetric patterns include “X and Y”, “X or Y” and “X as well as Y”. Previous works have shown that word pairs that participate in symmetric patterns bare strong semantic resemblance, and consequently, that these patterns can be used to cluster words into semantic categories, whe"
C14-1153,N12-2002,0,0.0199936,"egories expressed by languages, e.g., objects, actions, and properties. We follow human development, acquiring coarse-grained categories and distinctions before detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete “things” (Langacker, 2008, Ch. 4), roughly corresponding to nouns – the main participants in linguistic clauses – that are universally present in the semantics of virtually all languages (Dixon, 2005). Most works on noun classification to semantic categories require large amounts of human annotation to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or rely on language-specific resources such as WordNet (Evans and Orˇasan, 2000; Orˇasan and Evans, 2007). Such heavy supervision is labor intensive and makes these models domain and language dependent. Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can compensate for the lack of input from the senses in text corpora. Our model therefore performs semantic category classification using only a small number of labeled seed words per category. The experiments we conduct show that such weak supervision is sufficient to construct a"
C14-1153,J92-4003,0,0.553962,"milarity measures is described in Section 5.2. SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path lengt"
C14-1153,W00-0717,0,0.0358593,"008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different l"
C14-1153,P06-1038,1,0.918323,"ns Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with known classes to the rest of the concepts. For this aim we apply an iterative variant of the k-Nearest Neighbors algorithm (denoted wit"
C14-1153,P08-1079,1,0.838024,"ossible is the concept of “flexible patterns”, which are composed of high frequency words (HFW) and content words (CW). Every word in the language is defined as either HFW or CW, based on the number of times this word appears in a large corpus. This clustering procedure is applied by traversing a large corpus, and marking words that appear with corpus frequency higher than a predefined threshold t1 as HFWs, and words with corpus frequency lower than t2 as CWs.1 1 We follow (Davidov and Rappoport, 2006) and set t1 = 10−5 , t2 = 10−3 . Note that some words are marked both as HFW and as CW. See (Davidov and Rappoport, 2008) for discussion. 1614 The resulting clusters have a desired property: HFWs are comprised mostly of function words (prepositions, determiners, etc.) while CWs are comprised mostly of content words (nouns, verbs, adjectives and adverbs). This coarse grained clustering is useful for pattern extraction from plain text, since language patterns tend to use fixed function words, while content words change from one instance of the pattern to another (Davidov and Rappoport, 2006). Flexible patterns are extracted by traversing a large corpus and, based on the clustering of words to CWs and HFWs, extract"
C14-1153,P97-1023,0,0.667401,"Missing"
C14-1153,C92-2082,0,0.325937,"Missing"
C14-1153,Y09-1024,0,0.0307906,"Categories. Several works tackled the task of semantic classification, mostly focusing on animacy, concreteness and countability. The vast majority of these works are either supervised (Hatzivassiloglou and McKeown, 1997; Baldwin and Bond, 2003; Peng and Araki, 2005; Øvrelid, 2005; Nagata et al., 2006; Xing et al., 2010; Kwong, 2011; Bowman and Chopra, 2012) or make use of external, language-specific resources such as WordNet (Orˇasan and Evans, 2001; Orˇasan and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a small set of labeled seed words. Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences in instances of hand-crafted patterns such as “X who Y” and “X and his Y”. While their model uses patterns that are tailored to the animacy and gender categories, our model uses automatically induced patterns and is thus applicable to a range of semantic categories. Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais, 1997) based classification features. They used their model to classify nouns into the concrete/abstract category using 40 labeled seed words . Unl"
C14-1153,P08-1068,0,0.264369,"ord embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different label propagation algorithm, while still using the symmetric pattern f"
C14-1153,P08-1119,0,0.805739,"s include “X and Y”, “X as well as Y” and “neither X nor Y”. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with known classes to the res"
C14-1153,Y11-1007,0,0.0612062,"Missing"
C14-1153,N04-1043,0,0.0368862,"2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different label propagation algorithm,"
C14-1153,D13-1006,0,0.0883615,"uages, e.g., objects, actions, and properties. We follow human development, acquiring coarse-grained categories and distinctions before detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete “things” (Langacker, 2008, Ch. 4), roughly corresponding to nouns – the main participants in linguistic clauses – that are universally present in the semantics of virtually all languages (Dixon, 2005). Most works on noun classification to semantic categories require large amounts of human annotation to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or rely on language-specific resources such as WordNet (Evans and Orˇasan, 2000; Orˇasan and Evans, 2007). Such heavy supervision is labor intensive and makes these models domain and language dependent. Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can compensate for the lack of input from the senses in text corpora. Our model therefore performs semantic category classification using only a small number of labeled seed words per category. The experiments we conduct show that such weak supervision is sufficient to construct a high quality classifi"
C14-1153,P06-2077,0,0.0664839,"Missing"
C14-1153,W01-0716,0,0.133972,"Missing"
C14-1153,I05-2018,0,0.0860965,"Missing"
C14-1153,W97-0313,0,0.188417,"on. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label 1620 seeds to achieve good performance; in contrast, our work performs very well with a randomly selected set of labeled seed words. 8 Conclusion We presented a minimally supervised model for noun classification into coarse grained semantic categories. Our model"
C14-1153,D13-1193,1,0.840639,". Previous works have shown that word pairs that participate in symmetric patterns bare strong semantic resemblance, and consequently, that these patterns can be used to cluster words into semantic categories, where a high precision, but low coverage (recall) solution is good enough (Dorow et al., 2005; Davidov and Rappoport, 2006). A key observation of this paper is that symmetric patterns can be also used for semantic classification, where recall is as important as precision. Flexible Patterns. It has been shown in previous work (Davidov and Rappoport, 2006; Turney, 2008; Tsur et al., 2010; Schwartz et al., 2013) that patterns can be extracted from plain text in a fully unsupervised manner. The key idea that makes this procedure possible is the concept of “flexible patterns”, which are composed of high frequency words (HFW) and content words (CW). Every word in the language is defined as either HFW or CW, based on the number of times this word appears in a large corpus. This clustering procedure is applied by traversing a large corpus, and marking words that appear with corpus frequency higher than a predefined threshold t1 as HFWs, and words with corpus frequency lower than t2 as CWs.1 1 We follow (D"
C14-1153,P06-1101,0,0.046761,"abeled words that are used for propagation. Our model, on the other hand, does not require any seed selection procedure, and utilizes a randomly selected set of labeled seed words. Lexical Acquisition. Another line of work focused on the acquisition of semantic categories. In this setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for precision. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the"
C14-1153,D13-1170,0,0.00602652,"th a more sophisticated label propagation algorithm. 5.1.1 Classification Features Baselines In this set of baselines, we use different methods for building our graph. Concretely, instead of adding edges for pairs of words that appear in the same symmetric pattern, we use word similarity measures based on different feature sets as described below. The process of building the graph using the baseline word similarity measures is described in Section 5.2. SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller"
C14-1153,D11-1063,0,0.0331411,"ternal, language-specific resources such as WordNet (Orˇasan and Evans, 2001; Orˇasan and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a small set of labeled seed words. Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences in instances of hand-crafted patterns such as “X who Y” and “X and his Y”. While their model uses patterns that are tailored to the animacy and gender categories, our model uses automatically induced patterns and is thus applicable to a range of semantic categories. Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais, 1997) based classification features. They used their model to classify nouns into the concrete/abstract category using 40 labeled seed words . Unlike our model, which requires only a small set of labeled seeds, their algorithm is actually heavily supervised, requiring thousands of labeled examples for selecting the seed set of labeled words that are used for propagation. Our model, on the other hand, does not require any seed selection procedure, and utilizes a randomly selected set of labeled seed words. Lexical Acquisit"
C14-1153,P09-1050,0,0.0137145,"categories. In this setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for precision. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label 1620 seeds to achieve good performance; in contrast, our work performs very well with a randomly selected set of labeled seed wo"
C14-1153,C02-1114,0,0.757091,"ples of symmetric patterns include “X and Y”, “X as well as Y” and “neither X nor Y”. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with"
C14-1153,W03-1010,0,\N,Missing
C16-1123,Q16-1031,0,0.0570178,"ons of atomic features, this work presents a hierarchical architecture that enables discarding chosen feature combinations. This allows the model to integrate prior typological knowledge, while ignoring uninformative combinations of typological and dependency features. At the same time, it capitalises on the automatisation of feature construction inherent to tensor models to generate combinations of informative typology-based features, further enhancing the added value of typological priors. Another successful integration of externally-defined typological information in parsing is the work of Ammar et al. (2016). They present a multilingual parser trained on a concatenation of syntactic treebanks of multiple languages. To reduce the adverse impact of contradicting syntactic information in treebanks of typologically distinct languages, while still maintaining the benefits of additional training data for cross-linguistically consistent syntactic patterns, the parser encodes a language-specific bias for each given input language. This bias is based on the identity of the language and its WBO features as used in (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015). Differently from p"
C16-1123,D14-1034,1,0.783502,"ly carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed"
C16-1123,W13-2710,0,0.0153812,"g to word order. The predicted typological features, when evaluated against 1301 WALS, achieve high accuracy. This method not only extends WALS word order documentation to hundreds of new languages, but also quantifies the frequency of different word orders across languages – information that is not available in manually crafted typological repositories. Typological information can also be extracted from Interlinear Glossed Text (IGT). Such resources contain morphological segmentation, glosses and English translations of example sentences collected by field linguists. Lewis and Xia (2008) and Bender et al. (2013) demonstrate that IGT can be used to extract typological information relating to word order, case systems and determiners for a variety of languages. Another line of work seeks to increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subseque"
C16-1123,P10-1131,0,0.177593,"nder, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In"
C16-1123,W14-1603,1,0.869447,"alues for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subsequently predict new feature values using nearest-neighbour projection. A classifier-based approach for predicting new feature values from documented WALS information is presented in (Takamura et al., 2016). Coke et al. (2016) predict word order typological features by combining documented typological and genealogical features with the multilingual alignment approach discussed above. An alternative approach for learning typological information uses English as a Second Language (ESL) texts (Berzak et al., 2014). This work demonstrates that morphosyntactic typological similarities between languages are largely preserved in second language structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise these similarities for nearest neighbor prediction of typological features. The method evaluates competitively compared to baselines in the spirit of (Georgi et al., 2010) which rely on existing typological documentation of the target language for determining its nearest neighbors. In addition, a number of stu"
C16-1123,K15-1010,1,0.845013,"nguage model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. 1303 Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of “Contrastive Analysis” which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages. 5 Typological Information and NLP: What’s Next? § 4.2 surveyed the current uses of typological information in NLP. Here we discuss several future research avenues that might benefit from tighter integration"
C16-1123,P07-1036,0,0.0446587,"respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of"
C16-1123,D14-1082,0,0.0308785,"e type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evalua"
C16-1123,N09-1009,0,0.190277,"jority of other languages they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: languag"
C16-1123,D11-1005,0,0.0221498,"ifferent languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological information in recent work, such a survey would be highly valuable in guiding further development. This pa"
C16-1123,W02-1001,0,0.0226129,"r open challenges for typologically-driven NLP is the construction of principled mechanisms for the integration of typological knowledge in machine learning-based algorithms. Here, we briefly discuss a few traditional machine learning frameworks which support encoding of expert information, and as such hold promise for integrating typological information in NLP. Encoding typological knowledge into machine learning requires mechanisms that can bias learning (parameter estimation) and inference (prediction) of the model towards predefined knowledge. Algorithms such as the structured perceptron (Collins, 2002) and structured SVM (Taskar et al., 2004) iterate between an inference step and a parameter update step with respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Inde"
C16-1123,J81-4005,0,0.764144,"Missing"
C16-1123,P11-1061,0,0.0239916,"ardless of language-specific variation; ii) comprehensive systematisation of all possible variation in different languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological info"
C16-1123,P07-1009,0,0.67981,"Missing"
C16-1123,de-marneffe-etal-2014-universal,0,0.088799,"Missing"
C16-1123,P16-1038,0,0.0439992,"al POS tagger. Another application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. 1303 Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of “Contrastive Analysis” which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages. 5 Typologica"
C16-1123,N15-1184,0,0.0326505,"Missing"
C16-1123,C10-1044,0,0.311097,"see Table 1 for feature coverage of other typological databases). The integration of information from different databases is challenging due to differences in feature taxonomies as well as information overlap across repositories. Furthermore, available typological classifications contain different feature types, including nominal, ordinal and interval variables, and features that mix several types of values. This property hinders systematic and efficient encoding of such features in NLP models – a problem which thus far has only received a partial solution in the form of feature binarisation (Georgi et al., 2010). Further, typological databases are constructed manually using limited resources, and do not contain information on the distribution of feature values within a given language. This results in incomplete feature characterisations, as well as inaccurate generalisations. For example, WALS encodes only the dominant noun-adjective ordering for French, although in some cases this language also permits the adjective-noun ordering. Other aspects of typological databases may require feature pruning and preprocessing prior to use. For example, some features in WALS, such as feature 81B “Languages with"
C16-1123,P11-1149,1,0.703578,"and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss t"
C16-1123,N13-1113,1,0.819373,"d, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such a"
C16-1123,W04-3229,0,0.0329235,"uage structure), despite their great diversity. Captured in typological classifications at the level of generalisation useful 1299 for NLP, such information can be used to support multilingual NLP in a variety of ways (Bender, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common"
C16-1123,P14-1006,0,0.0284841,"ual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment betwee"
C16-1123,P11-1057,0,0.0519073,"natural language, regardless of language-specific variation; ii) comprehensive systematisation of all possible variation in different languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased u"
C16-1123,C12-1089,0,0.0412053,"Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually im"
C16-1123,I08-2093,0,0.266592,"gical information relating to word order. The predicted typological features, when evaluated against 1301 WALS, achieve high accuracy. This method not only extends WALS word order documentation to hundreds of new languages, but also quantifies the frequency of different word orders across languages – information that is not available in manually crafted typological repositories. Typological information can also be extracted from Interlinear Glossed Text (IGT). Such resources contain morphological segmentation, glosses and English translations of example sentences collected by field linguists. Lewis and Xia (2008) and Bender et al. (2013) demonstrate that IGT can be used to extract typological information relating to word order, case systems and determiners for a variety of languages. Another line of work seeks to increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluste"
C16-1123,P08-1099,0,0.0349195,"hrough constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingua"
C16-1123,W12-0209,0,0.0637077,"Missing"
C16-1123,D11-1006,0,0.132738,"l., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a trans"
C16-1123,N16-1018,0,0.0371221,"Missing"
C16-1123,D10-1120,0,0.0614546,"pervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald"
C16-1123,P12-1066,0,0.127638,"e used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In some areas of NLP, e"
C16-1123,D12-1128,0,0.183062,"The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological information in recent work, such a survey would be highly valuable in guiding further development. This paper provides such a survey f"
C16-1123,Q16-1030,0,0.052552,"uages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015; Rothe and Sch¨utze, 2015; Osborne et al., 2016; Mrkˇsi´c et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations. 1304 Can NLP Support Typology Construction? As discussed in §4, typological resources are commonly constructed manually by linguists. Despite the progress made in recent years in the digitisatio"
C16-1123,P15-2034,0,0.163715,"Missing"
C16-1123,H05-1108,0,0.170353,"Missing"
C16-1123,petrov-etal-2012-universal,0,0.245026,"2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperfo"
C16-1123,P16-1140,0,0.0255051,"rieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015; Rothe and Sch¨utze, 2015; Osborne et al., 2016; Mrkˇsi´c et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations. 1304 Can NLP Support Typology Construction? As discussed in §4, typological resources are co"
C16-1123,N12-1008,1,0.912594,"t constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal feature"
C16-1123,D10-1067,1,0.759201,"es they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual"
C16-1123,D11-1001,0,0.0132249,"mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs)"
C16-1123,P15-2040,0,0.0407258,"linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although parallel data can be used to giv"
C16-1123,P15-1173,0,0.0417279,"Missing"
C16-1123,D10-1001,0,0.014986,"t rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learnin"
C16-1123,D12-1131,1,0.832511,"raints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued"
C16-1123,P11-2120,0,0.0651153,"fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although para"
C16-1123,song-xia-2014-modern,0,0.0275956,"structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise these similarities for nearest neighbor prediction of typological features. The method evaluates competitively compared to baselines in the spirit of (Georgi et al., 2010) which rely on existing typological documentation of the target language for determining its nearest neighbors. In addition, a number of studies learned typological information tailored to the particular task and data at hand (i.e. task-based development). For example, Song and Xia (2014) process Ancient Chinese using Modern Chinese parsing resources. They manually identify and address statistical patterns in variation between monolingual corpora in each language, and ultimately optimise the model performance by selectively using only the Modern Chinese features which correspond to Ancient Chinese features. Although automatically-learned typological classifications have not been used frequently to date, they hold great promise for extending the use of typological information in NLP. Furthermore, such work offers an additional axis of interaction between linguistic typology and"
C16-1123,D11-1117,0,0.0144064,"ce creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of unive"
C16-1123,N12-1052,0,0.0971877,"Missing"
C16-1123,N13-1126,0,0.219991,"Missing"
C16-1123,L16-1011,0,0.044567,"increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subsequently predict new feature values using nearest-neighbour projection. A classifier-based approach for predicting new feature values from documented WALS information is presented in (Takamura et al., 2016). Coke et al. (2016) predict word order typological features by combining documented typological and genealogical features with the multilingual alignment approach discussed above. An alternative approach for learning typological information uses English as a Second Language (ESL) texts (Berzak et al., 2014). This work demonstrates that morphosyntactic typological similarities between languages are largely preserved in second language structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise th"
C16-1123,N16-1161,0,0.058165,"nological Modeling and Language Learning Besides dependency parsing, several other areas have started integrating typological information in various forms. A number of such works revolve around the task of POS tagging. For example, in Zhang et al. (2012), the previously discussed WBO features were used to inform mappings from language-specific to a universal POS tagset. In (Zhang et al., 2016), WBO feature values are used to evaluate the quality of a multilingual POS tagger. Another application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource"
C16-1123,P10-1040,0,0.0148561,"he modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word"
C16-1123,D14-1187,0,0.0449846,"Missing"
C16-1123,I08-3008,0,0.365977,"ransfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially develope"
C16-1123,D15-1213,0,0.349778,"al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning,"
C16-1123,D12-1125,1,0.921529,"been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petro"
C16-1123,N16-1156,0,0.153027,"databases reviewed in §2; and ii) automatic learning. The two 1300 methods have been used independently and in combination, and both are based on the assumption (be it explicit or implicit) that typological relations may be fruitfully used in NLP. Manual Extraction from Linguistic Resources Manually crafted linguistic resources – in particular the WALS database – have been the most commonly used sources of typological information in NLP. To date, syntactic parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016) and POS tagging (Zhang et al., 2012; Zhang et al., 2016) were the predominant areas for integration of structural information from such databases. In the context of these tasks, the most frequently used features related to word ordering according to coarse syntactic categories. Additional areas with emerging research which leverages externally-extracted typological features are phonological modeling (Tsvetkov et al., 2016; Deri and Knight, 2016) and language learning (Berzak et al., 2015). While information obtained from typological databases has been successfully integrated in several NLP tasks, a number of challenges remain. Perhaps the most cruc"
C16-1123,L16-1262,0,\N,Missing
D10-1032,P06-1013,0,0.0232101,"to TSD is WSD (Section 1 and Section 3). Many algorithmic approaches and techniques have been applied to supervised WSD (for reviews see (Agirre and Edmonds, 2006; Mihalcea and Pedersen, 2005; Navigli, 2009)). Among these are various classifiers, ensemble methods combining several supervised classifiers, bootstrapping and semi-supervised learning methods, using the Web as a corpus and knowledge-based methods relying mainly on machine readable dictionaries. Specifically related to this paper are works that exploit syntax (Martinez et al., 2002; Tanaka et al., 2007) and ensemble methods (e.g. (Brody et al., 2006)) to WSD. The references above also describe some unsupervised word sense induction algorithms. Our TSD algorithm uses the SNOW algorithm, which is a sparse network of classifiers (Section 6). Thus, it most resembles the ensemble approach to WSD. That approach has achieved very good results in several WSD shared tasks (Pedersen, 2000; Florian and Yarowsky, 2002). Since temporal reasoning is a direct application of TSD, research on this direction is relevant. Such research goes back to (Passonneau, 1988), which introduced the PUNDIT temporal reasoning system. For each tensed clause, PUNDIT firs"
D10-1032,D08-1073,0,0.0123498,"WA to Boston’) or not (as in ‘Tourists flew TWA to Boston’, or ‘John always flew his own plane to Boston’). The temporal structure of actual time clauses is then further analyzed. PUNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non-actual time. PUNDIT’s algorithmic approach is that of a Prolog rule based system, compared to our statistical learning corpusbased approach. We are not aware of further research that followed their sense disambiguation direction. Current temporal reasoning research focuses on temporal ordering of events (e.g., (Lapata, 2006; Chambers and Jurafsky, 2008)), for which an accepted atomic task is the identification of the temporal relation between two expressions (see e.g., the TempEval task in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and as"
D10-1032,W01-0502,0,0.0608567,"Missing"
D10-1032,W02-1004,0,0.0135657,"s, using the Web as a corpus and knowledge-based methods relying mainly on machine readable dictionaries. Specifically related to this paper are works that exploit syntax (Martinez et al., 2002; Tanaka et al., 2007) and ensemble methods (e.g. (Brody et al., 2006)) to WSD. The references above also describe some unsupervised word sense induction algorithms. Our TSD algorithm uses the SNOW algorithm, which is a sparse network of classifiers (Section 6). Thus, it most resembles the ensemble approach to WSD. That approach has achieved very good results in several WSD shared tasks (Pedersen, 2000; Florian and Yarowsky, 2002). Since temporal reasoning is a direct application of TSD, research on this direction is relevant. Such research goes back to (Passonneau, 1988), which introduced the PUNDIT temporal reasoning system. For each tensed clause, PUNDIT first decides whether it refers to an actual time (as in ‘We flew TWA to Boston’) or not (as in ‘Tourists flew TWA to Boston’, or ‘John always flew his own plane to Boston’). The temporal structure of actual time clauses is then further analyzed. PUNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non-actual time. PUNDIT’s algori"
D10-1032,P06-1095,0,0.0664147,"Missing"
D10-1032,J93-2004,0,0.0354692,"containing 4702 CSFs) to three datasets: training data (2100 sentences, 3183 forms), development data (300 sentences, 498 forms) and test data (600 sentences, 1021 forms). We used the development data to design the features for our learning model and to tune the parameters of the SNOW sequential model. In addition we used this data to design the rules of the ASF type classifier (which is not statistical and does not have a training phase). For the POS features, we induced POS tags using the MXPOST POS tagger (Ratnaparkhi, 1996). The tagger was trained on sections 2-21 of the WSJ PennTreebank (Marcus et al., 1993) annotated with gold standard POS tags. We used a publicly available implementation of the sequential SNOW model8 . We experimented in three conditions. In the first (TypeUnknown), the ASF type is not known at test time. In the last two, it is known at test time. These two conditions differ in whether the type is taken from the gold standard annotation of the test sentences (TypeKnown), or from the output of the simple rule-based classifier (TypeClassifier, see Section 6). For both conditions, the results reported below are when both ASF type features and possible labels sets are provided duri"
D10-1032,W96-0208,0,0.117812,"Missing"
D10-1032,A00-2009,0,0.00983694,"learning methods, using the Web as a corpus and knowledge-based methods relying mainly on machine readable dictionaries. Specifically related to this paper are works that exploit syntax (Martinez et al., 2002; Tanaka et al., 2007) and ensemble methods (e.g. (Brody et al., 2006)) to WSD. The references above also describe some unsupervised word sense induction algorithms. Our TSD algorithm uses the SNOW algorithm, which is a sparse network of classifiers (Section 6). Thus, it most resembles the ensemble approach to WSD. That approach has achieved very good results in several WSD shared tasks (Pedersen, 2000; Florian and Yarowsky, 2002). Since temporal reasoning is a direct application of TSD, research on this direction is relevant. Such research goes back to (Passonneau, 1988), which introduced the PUNDIT temporal reasoning system. For each tensed clause, PUNDIT first decides whether it refers to an actual time (as in ‘We flew TWA to Boston’) or not (as in ‘Tourists flew TWA to Boston’, or ‘John always flew his own plane to Boston’). The temporal structure of actual time clauses is then further analyzed. PUNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non"
D10-1032,C00-2103,0,0.0399099,"temporal relation between two expressions (see e.g., the TempEval task in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and aspect information with lexical semantics for machine translation. Schiehlen (2000) analyzed the effect tense understanding has on MT. Ye and Zhang (2005) explored tense tagging in a cross-lingual context. Ye et al., (2006) extracted features for tense translation between Chinese and English. Murata et al., (2007) compared the performance of several MT systems in translating tense and aspect and found that various ML techniques perform better on the task. Another related field is ‘deep’ parsing, where a sentence is annotated with a structure containing information that might be relevant for semantic interpretation (e.g. (Hajic, 1998; Baldwin et al., 2007)). TSD senses, howev"
D10-1032,S07-1014,0,0.0244413,"UNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non-actual time. PUNDIT’s algorithmic approach is that of a Prolog rule based system, compared to our statistical learning corpusbased approach. We are not aware of further research that followed their sense disambiguation direction. Current temporal reasoning research focuses on temporal ordering of events (e.g., (Lapata, 2006; Chambers and Jurafsky, 2008)), for which an accepted atomic task is the identification of the temporal relation between two expressions (see e.g., the TempEval task in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and aspect information with lexical semantics for machine translation. Schiehlen (2000) analyzed the effect tense understanding has on MT. Ye and Zhang (2005) explored tense tagging"
D10-1032,I05-1077,0,0.0820161,"ask in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and aspect information with lexical semantics for machine translation. Schiehlen (2000) analyzed the effect tense understanding has on MT. Ye and Zhang (2005) explored tense tagging in a cross-lingual context. Ye et al., (2006) extracted features for tense translation between Chinese and English. Murata et al., (2007) compared the performance of several MT systems in translating tense and aspect and found that various ML techniques perform better on the task. Another related field is ‘deep’ parsing, where a sentence is annotated with a structure containing information that might be relevant for semantic interpretation (e.g. (Hajic, 1998; Baldwin et al., 2007)). TSD senses, however, are not explicitly represented in these grammatical structures, and"
D10-1032,W91-0222,0,\N,Missing
D10-1032,W96-0213,0,\N,Missing
D10-1032,D07-1050,0,\N,Missing
D10-1032,C02-1112,0,\N,Missing
D10-1032,W07-1401,0,\N,Missing
D10-1032,J88-2005,0,\N,Missing
D10-1032,W07-2205,0,\N,Missing
D10-1032,W06-0107,0,\N,Missing
D10-1067,P06-1109,0,0.040791,"We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL with them as well. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model wi"
D10-1067,W06-2912,0,0.120731,"We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL with them as well. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model wi"
D10-1067,P07-1051,0,0.0483607,"Missing"
D10-1067,E03-1009,0,0.400157,"ting of the test set sentences contained in L. In the third stage, each of the test subsets is parsed by a model trained only on its corresponding training subset. This stage is motivated by our assumption that the high and low quality subsets manifest dissimilar syntactic patterns, and consequently the statistics of the parser’s parameters suitable for one subset differ from those suitable for another. We compute the confidence score in the second stage using the unsupervised PUPA algorithm (Reichart and Rappoport, 2009b). POS tags for it are induced using the fully unsupervised algorithm of Clark (2003). The parser we experiment with is the incremental parser of Seginer (2007), whose input consists of raw sentences and does not include any kind of supervised POS tags (created either manually or by a supervised algorithm). Consequently, our algorithm is fully unsupervised. The only parameter it has is NH but ZL improves parser performance for most NH values. BZL is related to boosting. In boosting after training one member of the ensemble, examples are reweighted such that examples that are classified correctly are down-weighted. BZL does something sim687 ilar: it uses PUPA to estimate which"
D10-1067,N09-1009,0,0.20552,"Missing"
D10-1067,N09-1012,0,0.314737,"Missing"
D10-1067,A00-2005,0,0.0372042,"r parser since its training method cannot be trivially bootstrapped with parses created in former steps (Seginer, 2007). Related machine learning methods. ZL is related to ensemble methods. Both ZL and such methods produce multiple learners, each of them trained on a different subset of the training data, and decide which learner to use for a particular test instance. Bagging (Breiman, 1996) and boosting (Freund and Schapire, 1996), where the experts utilize the same learning algorithm and differ in the sample of the training data they use for its training, were applied to supervised parsing (Henderson and Brill, 2000; Becker and Osborne, 2005). In Section 3 we discuss the connection of ZL to boosting. Owing to the fact that ZL produces different learners, it is natural to use it in conjunction with an ensemble method, which is what we do in this paper with our EZL model (Section 3). ZL is also related to active learning (AL) (Cohn and Ladner, 1994). AL also uses training subset selection, with the goal of obtaining a faster learning curve for an algorithm. AL is done in supervised settings, usually in order to minimize human annotation costs. AL algorithms providing faster learning than random subset sele"
D10-1067,J04-3001,0,0.0193001,"f ZL to boosting. Owing to the fact that ZL produces different learners, it is natural to use it in conjunction with an ensemble method, which is what we do in this paper with our EZL model (Section 3). ZL is also related to active learning (AL) (Cohn and Ladner, 1994). AL also uses training subset selection, with the goal of obtaining a faster learning curve for an algorithm. AL is done in supervised settings, usually in order to minimize human annotation costs. AL algorithms providing faster learning than random subset selection for parsing have been proposed (Reichart and Rappoport, 2009a; Hwa, 2004). However, we are not aware of AL applications in which the overall performance on the test set has been improved. In addition, our application here is to an unsupervised problem. Algorithms that utilize unsupervised clustering for class decomposition in order to improve classifiers’ performance (e.g. (Vilalta and Rish, 2003)) are related to ZL. In such methods, examples that belong to the same class are clustered, and the induced clusters are considered as separate classes. These methods, however, have been applied only to supervised classification in contrast to our work that addresses unsup"
D10-1067,I08-2097,0,0.0606717,"osed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL w"
D10-1067,P02-1017,0,0.150661,"d nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL with them as well. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these appr"
D10-1067,P04-1061,0,0.588921,"d for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model with sentences whose length is up to 15 words. As noted above, such a training protocol fails to improve the performance of the Seginer parser. The other approaches in that paper, bootstrapping via iterated learning of increasingly longer sentences and a combination of the bootstrapping and the short sentences approaches, are not directly applicable to the Seginer parser since its training method cannot be trivially bootstrapped with parses created in former steps (Seginer, 2007). Related machine learning methods. ZL"
D10-1067,J93-2004,0,0.0348999,"uently. Therefore, constituents that are more frequent in the set I receive higher scores after proper regularization is applied to prevent potential biases. The tree score is a combination of the scores of its constituents. Full details of the PUPA algorithm are given in (Reichart and Rappoport, 2009b). The resulting score was shown to be strongly correlated with the extrinsic quality of the parse tree, defined to be its Fscore similarity to the manually created (gold standard) parse tree of the sentence. 4 Experimental Setup We experimented with three English corpora: the WSJ Penn Treebank (Marcus et al., 1993) consisting of economic newspaper texts, the BROWN corpus (Francis and Kucera, 1979) consisting of texts of various English genres (e.g. fiction, humor, romance, mystery and adventure) and the GENIA corpus (Kim et al., 2003) consisting of abstracts of scientific articles from the biological domain. All corpora were stripped of all annotation (bracketing and 688 POS tags). For all corpora we report the parser performance on the entire corpus (WSJ: 49206 sentences, BROWN : 24243 sentences, GENIA : 4661 sentences). For WSJ we also provide an analysis of the performance of the parser when applied"
D10-1067,P06-1043,0,0.0463879,"subset coming from the same source. However, even when the training and test data are from the same source, a ZL algorithm may capture 686 fine differences between subsets. The ZL idea is therefore related to the notions of in-domain and out-of-domain (domain adaptation). In the former, the training and test data are assumed to originate from the same domain. In the latter, the test data comes from a different domain, and therefore has different statistics from the training data. Indeed, the performance of NLP algorithms in domain adaptation scenarios is markedly lower than in in-domain ones (McClosky et al., 2006). ZL takes this observation to the extreme, assuming that a similar situation might exist even in indomain scenarios. After all, a ‘domain’ is only a coarse qualification of the nature of a data set. In NLP, a domain is usually specified as the genre of the text involved (e.g., ‘newspapers’). However, there are additional axes that might influence the statistics obtained from training data, e.g., the syntactic nature of sentences. This section presents our ZL algorithms. We start with the simplest possible ZL algorithm where the subsets are randomly selected. We then describe ZL algorithms bas"
D10-1067,D08-1093,0,0.0340055,"that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be mad"
D10-1067,P07-1052,1,0.893911,"lgorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency"
D10-1067,W09-1103,1,0.366907,"nnotated corpora such as the WSJ PennTreebank. Recent works on unlabeled bracketing or dependencies induction include (Klein and Manning, 2002; Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Bod, 2006b; Bod, 2007; Smith and Eisner, 2006; Seginer, 2007; Cohen et al., 2008; Cohen and Smith, 2009; Headden et al., 2009). Most of the works above use POS tag sequences, created either manually or by a supervised algorithm, as input. The only exception is Seginer’s parser, which induces bracketing from plain text. Our confidence-based ZL algorithms use the PUPA unsupervised parsing quality score (Reichart and Rappoport, 2009b). As far as we know, PUPA is the only unsupervised quality assessment algorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey"
D10-1067,W09-1120,1,0.594352,"nnotated corpora such as the WSJ PennTreebank. Recent works on unlabeled bracketing or dependencies induction include (Klein and Manning, 2002; Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Bod, 2006b; Bod, 2007; Smith and Eisner, 2006; Seginer, 2007; Cohen et al., 2008; Cohen and Smith, 2009; Headden et al., 2009). Most of the works above use POS tag sequences, created either manually or by a supervised algorithm, as input. The only exception is Seginer’s parser, which induces bracketing from plain text. Our confidence-based ZL algorithms use the PUPA unsupervised parsing quality score (Reichart and Rappoport, 2009b). As far as we know, PUPA is the only unsupervised quality assessment algorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey"
D10-1067,P07-1049,0,0.464828,"upervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model with sentences whose length is up to 15 words. As noted above, such a training protocol fails to improve the performance of the Seginer parser. The other approaches in that paper, bootstrapping via iterated learning of increasingly longer sentences and a combination of the bootstrapping and the short sentences approaches, are not directly applicable to the Seginer parser since its training method cannot be trivially bootstrapped with parses created in former steps (Seginer, 2007). Related machine learning methods. ZL is related to ensemble methods. Both ZL and such methods produce multiple learners, each of them trained on a different subset of the training data, and decide which learner to use for a particular test instance. Bagging (Breiman, 1996) and boosting (Freund and Schapire, 1996), where the experts utilize the same learning algorithm and differ in the sample of the training data they use for its training, were applied to supervised parsing (Henderson and Brill, 2000; Becker and Osborne, 2005). In Section 3 we discuss the connection of ZL to boosting. Owing t"
D10-1067,P06-1072,0,0.0503191,"Missing"
D10-1067,N10-1116,0,0.235637,"h direction is how to utilize training data in the best possible way. Klein and Manning (2004) report results for their dependency model with valence (DMV) for unsupervised dependency parsing when it is trained and tested on the same corpus (both when sentence length restriction is imposed, such as for WSJ 10, and when it is not, such as for the entire WSJ). Today’s best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ 10 even when the test set includes longer sentences. Recently, Spitkovsky et al., (2010) demonstrated that training the DMV model on sentences of up to 15 words length yields better results on the entire section 23 of WSJ (with no sentence length restriction) than training with the entire WSJ corpus. In contrast to these dependency models, the Seginer constituency parser achieves its best performance when trained on the entire WSJ corpus either if sentence length restriction is imposed on the test corpus or not. The sentence length restriction training protocol of (Spitkovsky et al., 2010), harms this parser. When the parser is trained with the entire WSJ corpus its F-score perfo"
D10-1067,W06-1604,0,0.069454,"quality assessment algorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only."
D12-1125,W06-2920,0,0.414456,"al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007), they are not identical and yield different parsing performance (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). The goal of our work is to automate this process and construct mappings that are optimized for performance on downstream tasks (here we focus on parsing). As our results show, we achieve this goal 0.35 Unigram Frequency -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases] and sales [by corporate insiders] English German 0.3 0.25 0.2 0.15 -Einer der sich [für de"
D12-1125,D08-1092,0,0.028264,"comes from distributional features that capture global statistics. Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In p"
D12-1125,D11-1005,0,0.215234,"further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Niv"
D12-1125,P07-1035,0,0.040376,"Missing"
D12-1125,N06-1041,0,0.0691889,"Missing"
D12-1125,P03-1054,0,0.0143301,"on of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other l"
D12-1125,P08-1068,0,0.0511973,"Missing"
D12-1125,D10-1083,1,0.849245,"Noun Order of Demonstrative and Noun Values SVO, SOV, VSO, VOS, OVS, OSV Postpositions, Prepositions, Inpositions Genitive-Noun, Noun-Genitive Adjective-Noun, Noun-Adjective Demonstrative-Noun, Noun-Demonstrative, before and after Table 1: The set of typological features that we use for source language selection. The first column gives the ID of the feature as listed in WALS. The second column describes the feature and the last column enumerates the allowable values for each feature; besides these values each feature can also have a value of ‘No dominant order’. vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags. For initialization, we perform multiple random restarts and select the one with the lowest final objective score. 7 Results We first present the results of our model using the gold POS tags for the target language. Table 2 summarizes the performance of our model and the baselines. Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines. These results are consistent for both parsers (McDonald et al., 2011; Cohen et al.,"
D12-1125,D07-1072,0,0.0165044,"s on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our task consists of a target corpus written in"
D12-1125,P05-1010,0,0.0159703,"tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formula"
D12-1125,H05-1066,0,0.0438179,"rest (using the method from Section 5.3). For the selected source language, we assume access to the mapping of Petrov et al. (2011). Evaluation Measures We evaluate the quality of the derived mapping in the context of the target language parsing accuracy. In both the training and test data, the language-specific tags are replaced with universal tags: Petrov’s tags for the source languages and learned tags for the target language. We train two non-lexicalized parsers using source annotations and apply them to the target language. The first parser is a non-lexicalized version of the MST parser (McDonald et al., 2005) successfully used in the multilingual context (McDonald et al., 2011). In the second parser, parameters of the target language are estimated as a weighted mixture of parameters learned from supervised source languages (Cohen et al., 2011). For the parser of Cohen et al. (2011), we trained the model on the four languages used in the original paper — English, German, Czech and Italian. When measuring the performance on each of these four languages, we selected another set of four languages with a similar level of diversity.10 Following the standard evaluation practice in parsing, we use directe"
D12-1125,D11-1006,0,0.507153,"ansfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz"
D12-1125,D10-1120,1,0.94089,"ts Institute of Technology The Hebrew University {yuanzh, roiri, regina}@csail.mit.edu gamir@cs.huji.ac.il Abstract While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew. We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We e"
D12-1125,N07-1051,0,0.0150483,"gram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our task consists of a targ"
D12-1125,P06-1055,0,0.0468946,"anguages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our"
D12-1125,H05-1107,0,0.0195795,"our largest gain comes from distributional features that capture global statistics. Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with la"
D12-1125,I08-3008,0,0.24379,"t, Regina Barzilay Amir Globerson Massachusetts Institute of Technology The Hebrew University {yuanzh, roiri, regina}@csail.mit.edu gamir@cs.huji.ac.il Abstract While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew. We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consiste"
D12-1125,D07-1096,0,\N,Missing
D12-1131,W06-2920,0,0.155602,"e WSJ PennTreebank (Marcus et al., 1993) and the QuestionBank (QTB) (Judge et al., 2006). In the WSJ → QTB scenario, we train on sections 2-21 of the WSJ and test on the entire QTB (4000 questions). In the QTB → WSJ scenario, we train on the entire QTB and test on section 23 of the WSJ. Data for Lightly Supervised Training For all English experiments, our data was taken from the WSJ PennTreebank: training sentences from Section 0, development sentences from Section 22, and test sentences from Section 23. For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set (Buchholz and Marsi, 2006) with training data taken from the official training files. We trained the sentence-level models with 50-500 sentences. To verify the robustness of our results, our test sets consist of the official test sets augmented with additional sentences from the official training files such that each test file consists of 25,000 words. Our results on the official test sets are very similar to the results we report and are omitted for brevity. Parameters The model parameters, δ1 , δ2 , and δ3 of the scoring function (Section 4) and α of the Lagrange multipliers update rule (Section 6), were tuned on the"
D12-1131,P04-1056,0,0.0717296,"formation with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann a"
D12-1131,D10-1003,0,0.0232755,"ere used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We complement these works, which focus on consistency between consecutive sentences, and explore corpus level consistency. 3 where u is a vector in R|I(x) |. In practice, u will be a vector of Lagrange multipliers associated with the dependencies of y in our dual decomposition algorithm given in Section 6. We can construct a very similar setting for POS tagging where the goal is to find the best tagging y for a sentence x = (w1 , . . . , wn ). We skip the formal details here. We next introduce notation for Markov random fields (MRFs) (Koller and Friedman, 2009). An MRF consists of an undire"
D12-1131,W03-0407,0,0.0205689,"Missing"
D12-1131,N09-1068,0,0.00939211,"in adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic st"
D12-1131,P05-1045,0,0.0245567,"el algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann and McCallum, ), and Ba"
D12-1131,P10-2036,0,0.0269225,"Missing"
D12-1131,J04-3001,0,0.022584,"ised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, how"
D12-1131,P06-1063,0,0.110247,"Missing"
D12-1131,D10-1125,1,0.938178,"S tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Bel"
D12-1131,I05-1006,0,0.05248,"(Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistic"
D12-1131,J93-2004,0,0.053186,"Missing"
D12-1131,P08-2026,0,0.0106814,"manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper."
D12-1131,P06-1043,0,0.055918,"and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a majo"
D12-1131,N10-1004,0,0.0836437,"s (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic pr"
D12-1131,H05-1066,0,0.802359,"ncy parse is a vector y = {y(m, h) : (m, h) ∈ I(x)} where y(m, h) = 1 if m is a modifier of the head word h. We define the set Y(x) ⊂ {0, 1}|I(x)| to be the set of all valid dependency parses for a sentence x. In this work, we use projective dependency parses, but the method also applies to the set of nonprojective parse trees. Additionally, we have a scoring function f : Y(x) → R. The optimal parse y ∗ for a sentence x is given by, y ∗ = arg maxy∈Y(x) f (y). This sentencelevel decoding problem can often be solved efficiently. For example in commonly used projective dependency parsing models (McDonald et al., 2005), we can compute y ∗ efficiently using variants of the Viterbi algorithm. For this work, we make the assumption that we have an efficient algorithm to find the argmax of X f (y) + u(m, h)y(m, h) = f (y) + u · y (m,h)∈I(x) 1436 {((i, j), li , lj ) : (i, j) ∈ E, li ∈ Li , lj ∈ Lj } A label assignment in the MRF is a binary vector z with z(i, l) = 1 if the label l is selected at node i and z((i, j), li , lj ) = 1 if the labels li , lj are selected for the nodes i, j. In applications such as parsing and POS tagging, some of the label assignments are not allowed. For example, in dependency parsing"
D12-1131,P07-1078,1,0.381082,"ce-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used b"
D12-1131,D08-1050,0,0.0198208,"06) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phe"
D12-1131,D10-1001,1,0.955163,"and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifo"
D12-1131,D07-1111,0,0.0147533,"aper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in huma"
D12-1131,D08-1016,0,0.0219949,"ake a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the"
D12-1131,E03-1008,0,0.364995,"on. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS ta"
D12-1131,D10-1017,0,0.0145414,"co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We compleme"
D12-1131,N03-1033,0,0.0205525,"Missing"
D14-1034,P05-1038,0,0.0670223,"ter Laboratory Technion, IIT Computer Laboratory University of Cambridge Haifa, Israel University of Cambridge sb895@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves an"
D14-1034,N10-1083,0,0.0166289,"Missing"
D14-1034,W05-0621,0,0.0203224,", Israel University of Cambridge sb895@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicate"
D14-1034,A97-1052,0,0.0293914,"appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and the"
D14-1034,chesley-salmon-alt-2006-automatic,0,0.0290533,"riation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field"
D14-1034,D10-1088,0,0.0322158,"Missing"
D14-1034,N09-1009,0,0.0432093,"Missing"
D14-1034,W02-0907,1,0.450511,"ial argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for N"
D14-1034,de-marneffe-etal-2006-generating,0,0.0159356,"iss et al., 2007; Lippincott et al., 2012; Van de Cruys et al., 2012; Reichart and Korhonen, 2013) and other languages, including French (Messiant, 2008), Italian (Lenci et al., 2008), Turkish (Uzun et al., 2008), Japanese (Kawahara and Kurohashi, 2010) and Chinese (Han et al., 2008). The prominent input to these systems are grammatical relations (GRs) which express binary dependencies between words (e.g. direct and indirect objects, various types of complements and conjunctions). These are generated by some parsers (e.g. (Briscoe et al., 2006)) and can be extracted from the output of others (De-Marneffe et al., 2006). Two representative systems for English are the Cambridge system (Preiss et al., 2007) and the BioLexicon system which was used to acquire a substantial lexicon for biomedicine (Venturi et al., 2009). These systems extract GRs at the verb instance level from the output of a parser: the RASP general-language unlexicalized parser3 (Briscoe et al., 2006) and the lexicalized Enju parser tuned to the biomedical domain (Miyao and Tsujii, 2005), respectively. They generate potential SCFs by mapping GRs to a predefined SCF inventory using a set of manually developed rules (the Cambridge system) or by"
D14-1034,P11-1112,0,0.0154919,"f manually developed rules (the Cambridge system) or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations). Finally, a number of recent works addressed related tasks such as argument role clustering for SRL (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titvo and Klementiev, 2012) in an unsupervised manner. While these works differ from ours in the task (clustering arguments rather than verbs) and the level of supervision (applying a supervised parser), like us they analyze the verb argument structure at the instance level. is built through noisy frame filtering (based on frequencies or on external resources and annotations), which aims to remove errors from parsing and argument-adjunct distinction. Clearly, these systems require extensive manual work: a-priori definition of an SCF inventory and rules, manually anno"
D14-1034,D11-1122,0,0.00964019,"f manually developed rules (the Cambridge system) or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations). Finally, a number of recent works addressed related tasks such as argument role clustering for SRL (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titvo and Klementiev, 2012) in an unsupervised manner. While these works differ from ours in the task (clustering arguments rather than verbs) and the level of supervision (applying a supervised parser), like us they analyze the verb argument structure at the instance level. is built through noisy frame filtering (based on frequencies or on external resources and annotations), which aims to remove errors from parsing and argument-adjunct distinction. Clearly, these systems require extensive manual work: a-priori definition of an SCF inventory and rules, manually anno"
D14-1034,I05-1006,0,0.0219923,"Missing"
D14-1034,lenci-etal-2008-unsupervised,0,0.0223961,"epending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address t"
D14-1034,S13-1035,0,0.0202436,"Missing"
D14-1034,P12-1044,1,0.932598,"three different frames, the difference between which is not evident when considering the phrase structure categorization: (1) Direct Transitive: [They]NP [distinguished]VP [the mast]NP [of [ships on the horizon ]NP ]PP . 1 The verb similarity dataset used for the evaluation of our model is publicly available at ie.technion.ac.il/∼roiri/. 278 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278–289, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics that use modern machine learning techniques (Debowski, 2009; Lippincott et al., 2012; Van de Cruys et al., 2012; Reichart and Korhonen, 2013) use supervised parsers to pre-process the data2 . Supervised parsers are notoriously sensitive to domain variation (Lease and Charniak, 2005). As annotation of data for each new domain is unrealistic, current SCF systems suffer from poor portability. This problem is compounded for the many systems that employ manually developed SCF rules because rules are inherently ignorant to domain-specific preferences. The few SCF studies that focused on specific domains (e.g. biomedicine) have reported poor performance due to these reasons (Rimell"
D14-1034,P05-1012,0,0.0116371,"ed to be the concatenation of that verb’s basic feature representation with the basic representations of the words in a size 2 window around the represented verb. The final feature representation for the i-th verb instance in our dataset is therefore defined to be vi = [w−2 , w−1 , vbi , w+1 , w+2 ], where w−k and w+k are the basic feature representations of the words in distance −k or +k from the i-th verb instance in its sentence, and vbi is the basic feature representation of that verb instance. Our basic feature representation is inspired from the feature representation of the MST parser (McDonald et al., 2005) except that in the parser the features represent a directed edge in the complete directed graph defined over the words in a sentence that is to be parsed, while our features are generated for word n-grams. Particularly, our feature set is a concatenation of two sets derived from the MST set described in Table 1 of (McDonald et al., 2005) in the following way: (1) In both sets the parent word in the parser’s set is replaced with the represented word; (2) In one set every child word in the parser’s set is replaced by the word to the left of the represented word and in the other set it is replac"
D14-1034,han-etal-2000-handling,0,0.0214665,"]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong"
D14-1034,messiant-etal-2008-lexschem,1,0.834325,"ir probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is d"
D14-1034,ienco-etal-2008-automatic,0,0.0242489,"s themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF"
D14-1034,W07-2416,0,0.0160652,". For our model we estimated the verb pair similarity using the Tanimato similarity score for binary vectors: P Xi ∧ Yi T (X, Y ) = Pi i xi ∨ Yi For the baseline model, where the features are collocation counts, we used the standard cosine similarity. Our second baseline is identical to our model, except that: (a) the data is parsed with the Stanford parser (version 3.3.0, (Klein and Manning, 2003)) which was trained with sections 2-21 of the WSJ corpus; (b) the phrase structure output of the parser is transformed to the CoNLL dependency format using the official CoNLL 2007 conversion script (Johansson and Nugues, 2007); and then (c) the SCF of each verb instance is inferred using the rule-based system used by (Reichart and Korhonen, 2013). The vector space representation for each verb is then created using the process we described for our model and the same holds for vector comparison. This baseline allows direct comparison of frames induced by our SCF model with those derived from a supervised parser’s output. We computed the Pearson correlation between the scores of each of the models and the human scores. The results demonstrate the superiority of our model in predicting verb similarity: the correlation"
D14-1034,P05-1011,0,0.0119094,"s types of complements and conjunctions). These are generated by some parsers (e.g. (Briscoe et al., 2006)) and can be extracted from the output of others (De-Marneffe et al., 2006). Two representative systems for English are the Cambridge system (Preiss et al., 2007) and the BioLexicon system which was used to acquire a substantial lexicon for biomedicine (Venturi et al., 2009). These systems extract GRs at the verb instance level from the output of a parser: the RASP general-language unlexicalized parser3 (Briscoe et al., 2006) and the lexicalized Enju parser tuned to the biomedical domain (Miyao and Tsujii, 2005), respectively. They generate potential SCFs by mapping GRs to a predefined SCF inventory using a set of manually developed rules (the Cambridge system) or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations"
D14-1034,kawahara-kurohashi-2010-acquiring,0,0.0111722,"omain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems. The system relies on supervised POS tagging rather than"
D14-1034,W05-1002,0,0.0343409,"Cambridge sb895@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question ("
D14-1034,P03-1054,0,0.0724575,"tation, to the best of our knowledge). Second, in order to compare the output of our system to a rule-based SCF system that utilizes a supervised syntactic parser, we turn to a task-based evaluation. We aim to predict the degree of similarity between verb pairs and, following (Pado and Lapata, 2007) , we do so using a syntactic-based vector space model (VSM). We construct three VSMs - (a) one that derives features from our clusters; (b) one whose features come from the output of a state-of-the-art verb type level, rule based, SCF system (Reichart and Korhonen, 2013) that uses a modern parser (Klein and Manning, 2003); and (c) a standard lexical VSM. Below we show that our system compares favorably in both evaluations. Data. We experimented with two datasets taken from different domains: labor legislation and environment (Quochi et al., 2012). These datasets were created through web crawling followed by domain filtering. Each sentence in both datasets may contain multiple verbs but only one target verb has been manually annotated with a SCF. The labour legislation domain dataset contains 4415 annotated verb instances (and hence also ˆ (i, j) W i∈A,j∈B Using this definition, the normalized link ratio of A a"
D14-1034,J05-3003,0,0.0822047,"Missing"
D14-1034,W00-1325,1,0.687278,"orization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooc"
D14-1034,J07-2002,0,0.0118427,"comparison to previous work, is therefore challenging. We therefore evaluate our system in two ways. First, we compare its output, as well as the output of a number of clustering baselines, to the gold standard annotation of corpora from two different domains (the only publicly available ones with instance level SCF annotation, to the best of our knowledge). Second, in order to compare the output of our system to a rule-based SCF system that utilizes a supervised syntactic parser, we turn to a task-based evaluation. We aim to predict the degree of similarity between verb pairs and, following (Pado and Lapata, 2007) , we do so using a syntactic-based vector space model (VSM). We construct three VSMs - (a) one that derives features from our clusters; (b) one whose features come from the output of a state-of-the-art verb type level, rule based, SCF system (Reichart and Korhonen, 2013) that uses a modern parser (Klein and Manning, 2003); and (c) a standard lexical VSM. Below we show that our system compares favorably in both evaluations. Data. We experimented with two datasets taken from different domains: labor legislation and environment (Quochi et al., 2012). These datasets were created through web crawl"
D14-1034,E12-1003,0,0.011159,") or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations). Finally, a number of recent works addressed related tasks such as argument role clustering for SRL (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titvo and Klementiev, 2012) in an unsupervised manner. While these works differ from ours in the task (clustering arguments rather than verbs) and the level of supervision (applying a supervised parser), like us they analyze the verb argument structure at the instance level. is built through noisy frame filtering (based on frequencies or on external resources and annotations), which aims to remove errors from parsing and argument-adjunct distinction. Clearly, these systems require extensive manual work: a-priori definition of an SCF inventory and rules, manually annotated sentences for training a supervised parser, SCF"
D14-1034,P07-1115,1,0.78629,"d positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve"
D14-1034,N03-1033,0,0.0421755,"Missing"
D14-1034,P13-1085,1,0.752952,"h is not evident when considering the phrase structure categorization: (1) Direct Transitive: [They]NP [distinguished]VP [the mast]NP [of [ships on the horizon ]NP ]PP . 1 The verb similarity dataset used for the evaluation of our model is publicly available at ie.technion.ac.il/∼roiri/. 278 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278–289, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics that use modern machine learning techniques (Debowski, 2009; Lippincott et al., 2012; Van de Cruys et al., 2012; Reichart and Korhonen, 2013) use supervised parsers to pre-process the data2 . Supervised parsers are notoriously sensitive to domain variation (Lease and Charniak, 2005). As annotation of data for each new domain is unrealistic, current SCF systems suffer from poor portability. This problem is compounded for the many systems that employ manually developed SCF rules because rules are inherently ignorant to domain-specific preferences. The few SCF studies that focused on specific domains (e.g. biomedicine) have reported poor performance due to these reasons (Rimell et al., 2013). Another limitation of most current SCF sys"
D14-1034,C12-1165,1,0.836868,"Missing"
D14-1034,W09-1121,1,0.889241,"Missing"
D14-1034,C12-1141,1,0.830797,"e to the MRF’s potential functions and then compute the approximate MAP solution of the resulted model using the MPLP algorithm. Noising was done by adding an  term to the lambda values described in section 3.2 7 . This protocol results in a set of cluster (label) assignments for the involved verb instances, which we treat as an ensemble of experts from which a final, high quality, solution is to be induced. The basic idea in ensemble learning is that if several experts independently cluster together two verb instances, our belief that these verbs belong in the same cluster should increase. (Reichart et al., 2012) implemented this idea through the kway normalized cut clustering algorithm (Yu and ˆ = Shi, 2003). Its input is an undirected graph G ˆ W ˆ ) where Vˆ is the set of vertexes, E ˆ is (Vˆ , E, ˆ the set of edges and W is a non-negative and symmetric edge weight matrix. To apply this model ˆ from to our task, we construct the input graph G the labelings (frame assignments) contained in the ensemble. The graph vertexes Vˆ correspond to the verb instances and the (i, j)-th entry of the matrix ˆ is the number of ensemble members that assign W the same label to the i-th and j-th verb instances. For"
D14-1034,P13-1058,0,0.0130749,"Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF informatio"
D14-1034,W07-1516,0,0.0236007,"Missing"
D14-1034,P98-2184,0,0.0333799,"clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the gramm"
D14-1034,D07-1043,0,0.0244332,"Missing"
D14-1034,D12-1131,1,0.394151,"st part of the algorithm generates an excessive number of clusters, and M was then tuned so that these clusters are merged to the desired number of clusters. The λ function, used to measure the similarity between two verbs, is designed to bias the instances of the same verb type to have a higher similarity score. Algorithm 1 therefore tends to assign such instances to the same cluster. In our experiments that was always the case for this algorithm. High Cardinality Verb Sets Potentials. This set of potentials aims to bias larger sets of verb instances to share the same SCF. It is inspired by (Rush et al., 2012) who demonstrated, that syntactic structures that appear at the same syntactic context, in terms of the surrounding POS tags, tend to manifest similar syntactic behavior. While they demonstrated the usefulness of their method for dependency parsing and POS tagging, we implement it for higher level SCFs. We identified syntactic contexts that imply similar SCFs for verb instances appearing inside them. 4 All hyperparameters that require gold-standard annotation for tuning, were tuned using held-out data (Section 4). 5 The values in practice are S = 0.43 for labour legislation and S = 0.38 for en"
D14-1034,J06-2001,0,0.0270007,"am.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincot"
D14-1034,D11-1095,1,0.850079,"ract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et"
D14-1034,W10-1612,0,\N,Missing
D14-1034,P06-4020,0,\N,Missing
D16-1045,P04-1015,0,0.0852056,"ith variable size vectors as well. The CSP algorithm (Algorithm 1) aims to learn a parameter (or weight) vector w ∈ Rd , that separates the training data, i.e. for each training example (x, y) it holds that: y = arg maxy0 ∈Y(x) w · φ(x, y 0 ). To find such a vector the algorithm iterates over the training set examples and solves the above inference (argmax) problem. If the inferred label y ∗ differs from the gold label y the update w = w + ∆φ(x, y, y ∗ ) is performed. For linearly separable training data (see definition 4), CSP is proved to converge to a vector w separating the training data. Collins and Roark (2004) and Huang et al. (2012) expanded the CSP algorithm by proposing various alternatives to the argmax inference problem which is often intractable in structured prediction problems (e.g. in high-order graph-based dependency parsing (McDonald and Pereira, 2006)). The basic idea is replacing the argmax problem with the search for a violation: an output label that the model scores higher 1 In the general case Lx is a set of output sizes, which may be finite or infinite (as in constituency parsing (Collins, 1997)). 470 Algorithm 1 The Structured Perceptron (CSP) 1: 2: 3: 4: Input: data D = {xi , y i"
D16-1045,P97-1003,0,0.496268,"ion 4), CSP is proved to converge to a vector w separating the training data. Collins and Roark (2004) and Huang et al. (2012) expanded the CSP algorithm by proposing various alternatives to the argmax inference problem which is often intractable in structured prediction problems (e.g. in high-order graph-based dependency parsing (McDonald and Pereira, 2006)). The basic idea is replacing the argmax problem with the search for a violation: an output label that the model scores higher 1 In the general case Lx is a set of output sizes, which may be finite or infinite (as in constituency parsing (Collins, 1997)). 470 Algorithm 1 The Structured Perceptron (CSP) 1: 2: 3: 4: Input: data D = {xi , y i }n i=1 , feature mapping φ Output: parameter vector w ∈ Rd Define: ∆φ(x, y, z) , φ(x, y) − φ(x, z) Initialize w = 0. repeat for each (xi , y i ) ∈ D do y ∗ = arg max w · φ(xi , y 0 ) y 0 ∈Y(xi ) 5: if y ∗ 6= y i then 6: w = w + ∆φ(xi , y i , y ∗ ) 7: end if 8: end for 9: until Convergence than the gold standard label. The update rule in these CSP variants is, however, identical to the CSP’s. We, in contrast, propose a novel update rule that exploits the internal structure of the model’s prediction regardle"
D16-1045,W02-1001,0,0.855788,"Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. str"
D16-1045,N10-1115,0,0.0146813,"pecial case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDonald et al., 2005b; McDonald et al., 2005a) and dual-loss based methods ("
D16-1045,P11-1055,0,0.026358,"P special case. SWVP also provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDonald et al., 2005b; McDonald et al., 2005a) and dual-loss based methods (Meshi et al., 2010)) it does not exploit the structure of the predicted label. This may result in valuable information bein"
D16-1045,N12-1015,0,0.386358,"is work we present a new perceptron algorithm with an update rule that exploits the structure of a predicted label when it differs from the gold label (Section 3). Our algorithm is called The Structured Weighted Violations Perceptron (SWVP) as its update rule is based on a weighted sum of updates w.r.t violating assignments and non-violating assignments: assignments to the input example, derived from the predicted label, that score higher (for violations) and lower (for non-violations) than the gold standard label according to the current model. Our concept of violating assignment is based on Huang et al. (2012) that presented a variant of the CSP algorithm where the argmax inference problem is replaced with a violation finding function. Their update rule, however, is identical to that of the CSP algorithm. Importantly, although CSP and the above variant do not exploit the internal structure of the predicted label, they are special cases of SWVP. In Section 4 we prove that for a linearly separable training set, SWVP converges to a linear separator of 469 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 469–478, c Austin, Texas, November 1-5, 2016. 2016 Ass"
D16-1045,P10-1001,0,0.0164849,"than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDonald et al., 2005b; McDonald et al., 2005a) a"
D16-1045,P13-2109,0,0.100605,"ta experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDonald et al., 2005b; McDonald et al., 2005a) and dual-loss based methods (Meshi et al., 2010)) it"
D16-1045,E06-1011,0,0.0487991,"h a vector the algorithm iterates over the training set examples and solves the above inference (argmax) problem. If the inferred label y ∗ differs from the gold label y the update w = w + ∆φ(x, y, y ∗ ) is performed. For linearly separable training data (see definition 4), CSP is proved to converge to a vector w separating the training data. Collins and Roark (2004) and Huang et al. (2012) expanded the CSP algorithm by proposing various alternatives to the argmax inference problem which is often intractable in structured prediction problems (e.g. in high-order graph-based dependency parsing (McDonald and Pereira, 2006)). The basic idea is replacing the argmax problem with the search for a violation: an output label that the model scores higher 1 In the general case Lx is a set of output sizes, which may be finite or infinite (as in constituency parsing (Collins, 1997)). 470 Algorithm 1 The Structured Perceptron (CSP) 1: 2: 3: 4: Input: data D = {xi , y i }n i=1 , feature mapping φ Output: parameter vector w ∈ Rd Define: ∆φ(x, y, z) , φ(x, y) − φ(x, z) Initialize w = 0. repeat for each (xi , y i ) ∈ D do y ∗ = arg max w · φ(xi , y 0 ) y 0 ∈Y(xi ) 5: if y ∗ 6= y i then 6: w = w + ∆φ(xi , y i , y ∗ ) 7: end if"
D16-1045,P05-1012,0,0.0640671,"2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDonald et al., 2005b; McDonald et al., 2005a) and dual-loss based methods (Meshi et al., 2010)) it does not exploit the structure of the predicted label. This may result in valuable information being lost. Consider, for example, the gold and predicted dependency trees of Figure 1. The substantial difference between the trees may be mostly due to the difference in roots (are and worse, respectively). Parameter update w.r.t this mistake may thus be more useful than an update w.r.t the complete trees. In this work we present a new perceptron algorithm with an update rule that exploits the structure of a predicted l"
D16-1045,H05-1066,0,0.225012,"Missing"
D16-1045,D07-1096,0,0.141106,"Missing"
D16-1045,N12-1008,1,0.896102,"so provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDonald et al., 2005b; McDonald et al., 2005a) and dual-loss based methods (Meshi et al., 2010)) it does not exploit the structure of the predicted label. This may result in valuable information being lost. Consider, for example,"
D16-1045,D07-1071,0,0.0526432,"M, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDonald et al., 2005b; McDonald et al., 2005a) and dual-loss based methods (Meshi et al., 2010)) it does not exploit the structure of the predicted l"
D16-1045,P08-1101,0,0.0161953,"n the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results. 1 Introduction The structured perceptron ((Collins, 2002), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging (Zhang and Clark, 2008), dependency parsing (Koo and Collins, 2010; Goldberg and Elhadad, 2010; Martins et al., 2013), semantic parsing (Zettlemoyer and Collins, 2007) and information extraction (Hoffmann et al., 2011; Reichart and Barzilay, 2012), if to name just a few. Like some training algorithms in structured prediction (e.g. structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005), MIRA (Crammer and Singer, 2003) and LaSo (Daum´e III and Marcu, 2005)), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA (McDona"
D16-1045,D10-1001,0,\N,Missing
D16-1045,N12-1023,0,\N,Missing
D16-1068,D12-1133,0,0.018177,"ive function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing. Our algorithm is a simple iterative graph-based method that does not rely on advanced optimization techniques. Moreover, we factorize the grap"
D16-1068,W06-2920,0,0.0235225,"implemented our algorithms within the TurboParser (Martins et al., 2013)2 . That is, every other aspect of the parser – feature set, pruning algorithm, cost-augmented MIRA training (Crammer et al., 2006) etc., is kept fixed but our algorithms replace the inference algorithms: Chu-Liu-Edmonds ((Edmonds, 1967), first order) and dual-decomposition (higher order). We implemented two variants, for algorithm 1 and 2 respectively, and compare their results to those of the original TurboParser. We experiment with the datasets of the CoNLL 2006 and 2007 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006; Nilsson et al., 2007), for a total of 17 languages. When a language is represented in both sets, we used the 2006 version. We followed the standard train/test split of these datasets and, for the 8 languages with a training set of at least 10000 sentences, we randomly sampled 1000 sentences from the training set to serve as a development set. For these languages, we first trained the parser on the training set and then used the development set for hyperparameter tuning (|B|, s, α, β, and γ1 , . . . , γk for k order parsing).34 We employ four evaluation measures, where every measure is comput"
D16-1068,D07-1101,0,0.0333999,"Missing"
D16-1068,P13-1104,0,0.0630672,"algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing. Our algorithm is a simple iterative graph-based method that does not rely on advanced optimization techniques. Moreover, we factorize the graph-based objective into a sum of terms and show t"
D16-1068,P96-1011,0,0.308071,"Missing"
D16-1068,Q13-1033,0,0.0325856,"Missing"
D16-1068,Q15-1035,0,0.0115453,"ph based (McDonald et al., 2005) and transition based (Nivre et al., 2007). The graph based approach aims to optimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing. Our algo"
D16-1068,W13-3518,0,0.0151812,"t polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing. Our algorithm is a simple iterative graph-based method that does not rely on advanced optimization techniques. Moreover, we factorize the graph-based objective into"
D16-1068,D09-1127,0,0.0325875,"Missing"
D16-1068,P10-1001,0,0.0691484,"graph G = (V, E) is defined. The set of vertices is V = {0, ..., n}, with the {1, . . . , n} vertices representing the words of the sentence, in their order of appearance, and the 0 vertex is a specialized root vertex. The set of arcs is E = {(u, v) : u ∈ {0, ..., n}, v ∈ {1, ..., n}, u 6= v}, that is, the root vertex has no incoming arcs. We further define a part of order k to be a subset of E of size k, and denote the set of all parts with parts. For the special case of k = 1 a part is an arc. Different works employed different parts sets (e.g. (Martins et al., 2013; McDonald et al., 2005; Koo and Collins, 2010)). Generally, most parts sets consist of arcs connecting vertices either vertically (e.g. {(u, v), (v, z)} for k = 2) or horizontally (e.g. {(u, v), (u, z)}, for k = 2). In this paper we focus on the parts employed by (Martins et al., 2013), a state-of-the-art parser, but our algorithms are generally applicable for any parts set consistent with this general definition.1 In graph-based dependency parsing, each part p is given a score Wp ∈ R. A Dependency Tree (DT) T is a subset of arcs for which the following conditions hold: (1) Every vertex, except for the root, has an incoming arc: ∀v ∈ V "
D16-1068,P16-1198,1,0.866893,"Missing"
D16-1068,D08-1017,0,0.0643625,"Missing"
D16-1068,P13-2109,0,0.431769,"Goldberg, 2014), and opinion mining (Almeida et al., 2015). The two main approaches for this task are graph based (McDonald et al., 2005) and transition based (Nivre et al., 2007). The graph based approach aims to optimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We"
D16-1068,E06-1011,0,0.0328138,"parsing is instrumental in NLP applications, with recent examples in information extraction (Wu and Weld, 2010), word embeddings (Levy and Goldberg, 2014), and opinion mining (Almeida et al., 2015). The two main approaches for this task are graph based (McDonald et al., 2005) and transition based (Nivre et al., 2007). The graph based approach aims to optimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides o"
D16-1068,H05-1066,0,0.269403,"nput sentence, an input graph G = (V, E) is defined. The set of vertices is V = {0, ..., n}, with the {1, . . . , n} vertices representing the words of the sentence, in their order of appearance, and the 0 vertex is a specialized root vertex. The set of arcs is E = {(u, v) : u ∈ {0, ..., n}, v ∈ {1, ..., n}, u 6= v}, that is, the root vertex has no incoming arcs. We further define a part of order k to be a subset of E of size k, and denote the set of all parts with parts. For the special case of k = 1 a part is an arc. Different works employed different parts sets (e.g. (Martins et al., 2013; McDonald et al., 2005; Koo and Collins, 2010)). Generally, most parts sets consist of arcs connecting vertices either vertically (e.g. {(u, v), (v, z)} for k = 2) or horizontally (e.g. {(u, v), (u, z)}, for k = 2). In this paper we focus on the parts employed by (Martins et al., 2013), a state-of-the-art parser, but our algorithms are generally applicable for any parts set consistent with this general definition.1 In graph-based dependency parsing, each part p is given a score Wp ∈ R. A Dependency Tree (DT) T is a subset of arcs for which the following conditions hold: (1) Every vertex, except for the root, has an"
D16-1068,D07-1096,0,0.159159,"Missing"
D16-1068,P08-1108,0,0.0589202,"Missing"
D16-1068,D12-1067,0,0.0142394,"meida et al., 2015). The two main approaches for this task are graph based (McDonald et al., 2005) and transition based (Nivre et al., 2007). The graph based approach aims to optimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for"
D16-1068,P06-2089,0,0.0688394,"Missing"
D16-1068,D08-1016,0,0.0286874,"es for this task are graph based (McDonald et al., 2005) and transition based (Nivre et al., 2007). The graph based approach aims to optimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based depen"
D16-1068,D07-1099,0,0.0301404,"Missing"
D16-1068,D11-1116,0,0.0504137,"Missing"
D16-1068,P10-1013,0,0.0691314,"Missing"
D16-1068,D08-1059,0,0.0444519,"Missing"
D16-1068,P11-2033,0,0.056285,"ptimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing. Our algorithm is a simple iterative graph-based method that does not rely on advanced optimization techniques. Moreove"
D16-1068,D14-1109,0,0.0185292,"transition based (Nivre et al., 2007). The graph based approach aims to optimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing. Our algorithm is a simple iterative graph"
D16-1068,P14-1019,0,0.0963316,"transition based (Nivre et al., 2007). The graph based approach aims to optimize a global objective function. While exact polynomial inference algorithms exist for projective parsing (Eisner, 1996; McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010, inter alia), high order non-projective parsing is NP-hard (McDonald and Pereira, 2006). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition (Martins et al., 2013), LP relaxations (Riedel et al., 2012), belief propagation (Smith and Eisner, 2008; Gormley et al., 2015) and sampling (Zhang et al., 2014b; Zhang et al., 2014a). The transition based approach (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Honnibal et al., 2013; Choi and McCallum, 2013a, inter alia), and the easy first approach (Goldberg and Elhadad, 2010) which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function. Yet, their sequential greedy solvers are fast and accurate in practice. We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing. Our algorithm is a simple iterative graph"
D16-1235,N09-1003,0,0.316379,"rb similarity due to their small size or narrow coverage of verbs. In particular, a number of word pair evaluation sets are prominent in the distributional semantics 1 In some existing evaluation sets pairs are scored for relatedness which has some overlap with similarity. SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators. For a broader discussion see (Hill et al., 2015). 2174 literature. Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 a"
D16-1235,P98-1013,0,0.378783,"s similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45).6 The basic overview of the VerbNet structure already suggests that measuring verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties. The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenomena. Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007).7 all. For each such class we sampled additional verb types until the class was represented by 3 or 4 member verbs (chosen randomly).8 Following that, we sampled at least 2 verb pairs for each previously ’under-represented’ VN class by pairing 2 member verbs from each such class. This procedure resulted in 81 additional pairs, now 3,153 in total. (Step 4) Finally, to complement this set with a sample of entirely unassociated pairs, we followed"
D16-1235,D14-1034,1,0.389034,"resentative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena. In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pair evaluation resource. 3 The SimVerb-3500 Data Set Design Motivation Hill et al. (2015) argue that comprehensive highquality evaluation resources have to satisfy the following three criteria: (C1) Representative (the resource covers the full range of concepts occurring in n"
D16-1235,P14-1023,0,0.0211925,"rage of all the other raters. SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2). Vector Space Models We compare the performance of prominent representation models on SimVerb-3500. We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use sparse binary vectors built from linguistic resources (NonEval set IAA-1 IAA-2 A LL T EXT WS IM (203) S IM L EX (999) 0.67 0.65 0.67 0.78 0.79 SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge"
D16-1235,P15-2076,0,0.0948827,"SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge bases and hand-crafted lexical resources, see supplementary material). T EXT denotes the best reported score for a model that learns solely on the basis of distributional information. All scores are Spearman’s ρ correlations. Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkši´c et al., 2016)). Descriptions of these models are in the supplementary material. Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500. The correlation between the two data sets calculated on the shared pairs is ρ = 0.91. This proves, as expected, that the ratings are consistent across the two data sets. Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222. Since the number of evaluation"
D16-1235,N15-1184,0,0.117935,"Missing"
D16-1235,W16-2506,0,0.0632045,"oying additional databases or linguistic resources. The performance of the best scoring Paragram+CF model is even on par with the IAA-1 of 0.72. The same model obtains the highest score on SV-3500 (ρ = 0.628), with a clear gap to IAA-1 of 0.84. We attribute these differences in 2178 performance largely to SimVerb-3500 being a more extensive and diverse resource in terms of verb pairs. Development Set A common problem in scored word pair datasets is the lack of a standard split to development and test sets. Previous works often optimise models on the entire dataset, which leads to overfitting (Faruqui et al., 2016) or use custom splits, e.g., 10-fold cross-validation (Schwartz et al., 2015), which make results incomparable with others. The lack of standard splits stems mostly from small size and poor coverage – issues which we have solved with SimVerb-3500. Our development set contains 500 pairs, selected to ensure a broad coverage in terms of similarity ranges (i.e., non-similar and highly similar pairs, as well as pairs of medium similarity are represented) and top-level VN classes (each class is represented by at least 1 member verb). The test set includes the remaining 3,000 verb pairs. The performa"
D16-1235,J15-4004,1,0.907142,"emantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types. Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling. Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different 2173 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics representation lear"
D16-1235,kipper-etal-2004-extending,0,0.102139,"Missing"
D16-1235,P14-2050,0,0.13367,"idual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2). Vector Space Models We compare the performance of prominent representation models on SimVerb-3500. We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use sparse binary vectors built from linguistic resources (NonEval set IAA-1 IAA-2 A LL T EXT WS IM (203) S IM L EX (999) 0.67 0.65 0.67 0.78 0.79 SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL"
D16-1235,W13-3512,0,0.701559,"play a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types. Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling. Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different 2173 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computatio"
D16-1235,N16-1018,0,0.138051,"Missing"
D16-1235,D07-1042,0,0.100538,"Missing"
D16-1235,J05-1004,0,0.62211,"to verb semantics research, we introduce SimVerb-3500 – an extensive intrinsic evaluation resource that is unprecedented in both size and coverage. SimVerb-3500 includes 827 verb types from the University of South Florida Free Association Norms (USF) (Nelson et al., 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al., 2008). This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al., 2005). Moreover, the large standardised development and test sets in SimVerb-3500 allow for principled tuning of hyperparameters, a critical aspect of achieving strong performance with the latest representation learning architectures. In § 2, we discuss previous evaluation resources targeting verb similarity. We present the new SimVerb-3500 data set along with our design choices and the pair selection process in § 3, while the annotation process is detailed in § 4. In § 5 we report the performance of a diverse range of popular representation learning architectures, together with benchmark performan"
D16-1235,D14-1162,0,0.0789657,"Missing"
D16-1235,D10-1114,0,0.0113012,"g models. One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs. This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns. More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015). In future work we aim to apply such methods to the task of verb acquisition. Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes. Better understanding of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in lang"
D16-1235,N10-1013,0,0.0390423,"g models. One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs. This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns. More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015). In future work we aim to apply such methods to the task of verb acquisition. Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes. Better understanding of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in lang"
D16-1235,K15-1026,1,0.904651,"v et al., 2013; Pennington et al., 2014; Faruqui et al., 2015). These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010). Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary. This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015). Introduction Verbs are famously both complex and variable. They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word represent"
D16-1235,P14-1068,0,0.0149921,"are scored for relatedness which has some overlap with similarity. SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators. For a broader discussion see (Hill et al., 2015). 2174 literature. Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena. In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pa"
D16-1235,P10-1040,0,0.0211468,"methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning. 1 Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015). These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010). Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary. This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015). Introduction Verbs are famously both complex and variable. They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and"
D16-1235,Q15-1025,0,0.0741812,"gram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge bases and hand-crafted lexical resources, see supplementary material). T EXT denotes the best reported score for a model that learns solely on the basis of distributional information. All scores are Spearman’s ρ correlations. Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkši´c et al., 2016)). Descriptions of these models are in the supplementary material. Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500. The correlation between the two data sets calculated on the shared pairs is ρ = 0.91. This proves, as expected, that the ratings are consistent across the two data sets. Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222. Since the number of evaluation pairs may influence the results, we ideally want to compare sets of equal size for"
D16-1235,C10-1011,0,\N,Missing
D16-1235,N03-1033,0,\N,Missing
D16-1235,W08-1301,0,\N,Missing
D16-1235,C98-1013,0,\N,Missing
D16-1235,P06-1038,0,\N,Missing
D16-1235,petrov-etal-2012-universal,0,\N,Missing
D16-1235,N13-1092,0,\N,Missing
D16-1235,P15-2070,0,\N,Missing
D16-1235,N16-1060,1,\N,Missing
D16-1235,P16-2084,1,\N,Missing
D16-1235,C12-1059,0,\N,Missing
D16-1235,P13-2109,0,\N,Missing
D18-1022,P15-1119,0,0.0604773,"domain) pairs. In the second, lazy CLCD setup, models have access only to source language reviews - annotated reviews from the source domain, and unannotated reviews from both the source and the target domains. 2 Previous Work We briefly survey work on CL and CD learning and on multilingual word embeddings. We focus on aspects that are relevant to our work rather than on a comprehensive survey of the extensive previous work on these problems. Cross-language transfer CL has been explored extensively in NLP. Example applications include POS tagging (T¨ackstr¨om et al., 2013), syntactic parsing (Guo et al., 2015; Ammar et al., 2016), text classification (Shi et al., 2010; Prettenhofer and Stein, 2010) and sentiment analysis (Wan, 2009; Zhou et al., 2016) among others. We consider the lazy setup to be the desired standard setup of CLCD learning for two reasons. First, in true resource-poor languages we expect it to be hard to find a sufficient number of reviews from many domains, even if they are unannotated 239 Our work is mostly related to two works: (a) Cross-lingual SCL (CL-SCL, (Prettenhofer and Stein, 2010, 2011)); and (b) Distributional Correspondence Indexing (DCI, (Fern´andez et al., 2016)) –"
D18-1022,Q16-1031,0,0.0465047,"the second, lazy CLCD setup, models have access only to source language reviews - annotated reviews from the source domain, and unannotated reviews from both the source and the target domains. 2 Previous Work We briefly survey work on CL and CD learning and on multilingual word embeddings. We focus on aspects that are relevant to our work rather than on a comprehensive survey of the extensive previous work on these problems. Cross-language transfer CL has been explored extensively in NLP. Example applications include POS tagging (T¨ackstr¨om et al., 2013), syntactic parsing (Guo et al., 2015; Ammar et al., 2016), text classification (Shi et al., 2010; Prettenhofer and Stein, 2010) and sentiment analysis (Wan, 2009; Zhou et al., 2016) among others. We consider the lazy setup to be the desired standard setup of CLCD learning for two reasons. First, in true resource-poor languages we expect it to be hard to find a sufficient number of reviews from many domains, even if they are unannotated 239 Our work is mostly related to two works: (a) Cross-lingual SCL (CL-SCL, (Prettenhofer and Stein, 2010, 2011)); and (b) Distributional Correspondence Indexing (DCI, (Fern´andez et al., 2016)) – in both cases pivot"
D18-1022,P07-1034,0,0.2548,"employ belong to the class of the most minimal supervision. In addition, as noted in § 1, in order to allow the lazy CLCD setup, we would like BEs where the source language embeddings are induced with no knowledge of the target language, and we indeed choose such BEs (§ 5). Cross-domain transfer In NLP, CD transfer (a.k.a domain adaptation) has been addressed for many tasks, including sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch¨utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name a handful of examples. Several approaches to CD transfer have been proposed in the ML literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations. Representation learning, the latter, has become prominent in the DNN era, and is the approach we take here. As noted in § 1 we adopt CD models that integrate pivot-based learning with DNNs to perform CLCD. The task we address is cross-language crossdomain (CLCD) learning. Fo"
D18-1022,P07-1056,0,0.840332,"noted in § 1 we adopt CD models that integrate pivot-based learning with DNNs to perform CLCD. The task we address is cross-language crossdomain (CLCD) learning. Formally, we are given a set of labeled examples from language Ls and domain Ds (denoted as the pair (Ls , Ds )). Our goal is to train an algorithm that will be able to correctly label examples from language Lt and domain Dt (Lt , Dt ). The same label set, T , is used across the participating source and target domains and languages. The setup we consider is similar in spirit to the setup known as unsupervised domain adaptation (e.g. (Blitzer et al., 2007; Ziser and Reichart, 2017, 2018)). When taking the representation learning approach to CLCD learning, the training pipeline usually consists of two steps. In the first step, the representation learning model is trained on unlabeled data from the source and target languages and domains, with the goal of generating a joint representation for the source and the target. Below we describe the unlabeled data in the full and the lazy CLCD setups. In the second step, a classifier for the supervised task is trained on the (Ls , Ds ) labeled data. To facilitate language and domain transfer, every examp"
D18-1022,W06-1615,0,0.808364,"bottleneck seriously challenges the world-wide accessibility of NLP technology. To address this problem substantial efforts have been put into the development of cross-domain (CD, (Daum´e III, 2007; Ben-David et al., 2010)) and cross-language (CL) transfer methods. For both areas, while a variety of methods have been developed for many tasks throughout the years (§ 2), with the prominence of deep neural networks (DNNs) the focus of modern methods is shifting towards learning data representations that can serve as a bridge across domains and languages. For CD, this includes: (a) pre-DNN work ((Blitzer et al., 2006, 2007), known as structural correspondence learning (SCL)), that models the connections between pivot features – features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain – and the other, non-pivot, features; (b) DNN work (Glorot et al., 2011; Chen et al., 2012) which employs compress-based noise reduction to learn cross-domain features; and recently also (c) works that combine the two approaches (Ziser and Reichart, 2017, 2018) (henceforth ZR17 and ZR18). For CL, the picture is similar: multilingual representations (usu"
D18-1022,Q17-1010,0,0.105947,"Missing"
D18-1022,N10-1004,0,0.0508352,"hrough comparability of their features (e.g. POS tags) – the BEs we employ belong to the class of the most minimal supervision. In addition, as noted in § 1, in order to allow the lazy CLCD setup, we would like BEs where the source language embeddings are induced with no knowledge of the target language, and we indeed choose such BEs (§ 5). Cross-domain transfer In NLP, CD transfer (a.k.a domain adaptation) has been addressed for many tasks, including sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch¨utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name a handful of examples. Several approaches to CD transfer have been proposed in the ML literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations. Representation learning, the latter, has become prominent in the DNN era, and is the approach we take here. As noted in § 1 we adopt CD models that integrate pivot-based learning with DNNs to perform CLCD. The"
D18-1022,P15-1071,0,0.0337575,"rom both the source and the target domains, to predict whether its associated pivot feature appears in the example or not. Note that no human annotation is required for the training of these classifiers, the supervision signal is in the unlabeled data. The matrix whose columns are the weight vectors of the classifiers is post-processed with singular value decomposition (SVD) and the derived matrix maps feature vectors from the original space to the new. Since the presentation of SCL, pivot-based cross-domain learning has been researched extensively (e.g. (Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Yang et al., 2017)). 4.1 4.2 LSTM Based Methods ZR18 observed that AE-based representation learning models do not exploit the structure of their input examples. Obviously, this can negatively impact text classification tasks, such as sentiment analysis. They hence proposed a structureaware representation learning model, named Pivot Based Language Modeling (PBLM, Figure 2a). PBLM is an LSTM fed with the embeddings of the input example words. As is standard in the LSTM literature, it is possible to feed the model with 1-hot word vectors and multiply them by a (randomly init"
D18-1022,P11-1014,0,0.0266368,"we combine this idea with modern DNNs and BEs to substantially improve CLCD learning. nals such as related images or through comparability of their features (e.g. POS tags) – the BEs we employ belong to the class of the most minimal supervision. In addition, as noted in § 1, in order to allow the lazy CLCD setup, we would like BEs where the source language embeddings are induced with no knowledge of the target language, and we indeed choose such BEs (§ 5). Cross-domain transfer In NLP, CD transfer (a.k.a domain adaptation) has been addressed for many tasks, including sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch¨utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name a handful of examples. Several approaches to CD transfer have been proposed in the ML literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations. Representation learning, the latter, has become prominent in the DNN era, and is the approac"
D18-1022,P10-1114,0,0.233722,"of only one previous work that aims to perform CLCD learning (Fern´andez et al., 2016). However, this work does not employ modern DNN techniques and is substantially outperformed by our methods. Our approach to CLCD learning is rooted in the family of methods that combine the power of both DNNs and pivot-based ideas, and is based on two principles. First, we build on the recent progress in learning multilingual word embeddings (Ruder et al., 2017). Such embeddings help close the lexical gap between languages as they map their different vocabularies to a shared vector space. Second, we follow (Prettenhofer and Stein, 2010, 2011; Fern´andez et al., 2016) and redefine the concept of pivot features for CLCD setups (§ 5). While these authors already employed this idea in order to design pivot-based methods in CL (Prettenhofer and Stein, 2010, 2011) and CLCD (Fern´andez et al., 2016) for text classification and sentiment analysis, their algorithms do not employ DNNs and multilingual embeddings. In this paper we show that it is the combination of bilingual word embeddings (BEs) and structure aware DNNs with the re-defined pivots that leads to high quality CLCD models. Aiming to facilitate transfer to resource poor l"
D18-1022,P07-1078,1,0.766226,"ls such as related images or through comparability of their features (e.g. POS tags) – the BEs we employ belong to the class of the most minimal supervision. In addition, as noted in § 1, in order to allow the lazy CLCD setup, we would like BEs where the source language embeddings are induced with no knowledge of the target language, and we indeed choose such BEs (§ 5). Cross-domain transfer In NLP, CD transfer (a.k.a domain adaptation) has been addressed for many tasks, including sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch¨utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name a handful of examples. Several approaches to CD transfer have been proposed in the ML literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations. Representation learning, the latter, has become prominent in the DNN era, and is the approach we take here. As noted in § 1 we adopt CD models that integrate pivot-based learning with DN"
D18-1022,P07-1033,0,0.25483,"Missing"
D18-1022,D12-1131,1,0.894883,"their features (e.g. POS tags) – the BEs we employ belong to the class of the most minimal supervision. In addition, as noted in § 1, in order to allow the lazy CLCD setup, we would like BEs where the source language embeddings are induced with no knowledge of the target language, and we indeed choose such BEs (§ 5). Cross-domain transfer In NLP, CD transfer (a.k.a domain adaptation) has been addressed for many tasks, including sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch¨utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name a handful of examples. Several approaches to CD transfer have been proposed in the ML literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations. Representation learning, the latter, has become prominent in the DNN era, and is the approach we take here. As noted in § 1 we adopt CD models that integrate pivot-based learning with DNNs to perform CLCD. The task we address is"
D18-1022,I13-1023,0,0.0586866,"Missing"
D18-1022,D10-1103,0,0.0321497,"ccess only to source language reviews - annotated reviews from the source domain, and unannotated reviews from both the source and the target domains. 2 Previous Work We briefly survey work on CL and CD learning and on multilingual word embeddings. We focus on aspects that are relevant to our work rather than on a comprehensive survey of the extensive previous work on these problems. Cross-language transfer CL has been explored extensively in NLP. Example applications include POS tagging (T¨ackstr¨om et al., 2013), syntactic parsing (Guo et al., 2015; Ammar et al., 2016), text classification (Shi et al., 2010; Prettenhofer and Stein, 2010) and sentiment analysis (Wan, 2009; Zhou et al., 2016) among others. We consider the lazy setup to be the desired standard setup of CLCD learning for two reasons. First, in true resource-poor languages we expect it to be hard to find a sufficient number of reviews from many domains, even if they are unannotated 239 Our work is mostly related to two works: (a) Cross-lingual SCL (CL-SCL, (Prettenhofer and Stein, 2010, 2011)); and (b) Distributional Correspondence Indexing (DCI, (Fern´andez et al., 2016)) – in both cases pivot features were redefined to support CL ("
D18-1022,Q13-1001,0,0.0834189,"Missing"
D18-1022,P16-1157,0,0.0416525,"tures – features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain – and the other, non-pivot, features; (b) DNN work (Glorot et al., 2011; Chen et al., 2012) which employs compress-based noise reduction to learn cross-domain features; and recently also (c) works that combine the two approaches (Ziser and Reichart, 2017, 2018) (henceforth ZR17 and ZR18). For CL, the picture is similar: multilingual representations (usually word embeddings) are prominent in the transfer of NLP algorithms from one language to another (e.g. (Upadhyay et al., 2016)). In this paper we aim to take CL and CD transfer a significant step forward and design methods that can adapt NLP algorithms simultaneously across languages and domains. We consider this research problem fundamental to our field as manually annotated resources are often scarce in many domains, even for languages that are considWhile cross-domain and cross-language transfer have long been prominent topics in NLP research, their combination has hardly been explored. In this work we consider this problem, and propose a framework that builds on pivotbased learning, structure-aware Deep Neural Ne"
D18-1022,P09-1027,0,0.16491,"e domain, and unannotated reviews from both the source and the target domains. 2 Previous Work We briefly survey work on CL and CD learning and on multilingual word embeddings. We focus on aspects that are relevant to our work rather than on a comprehensive survey of the extensive previous work on these problems. Cross-language transfer CL has been explored extensively in NLP. Example applications include POS tagging (T¨ackstr¨om et al., 2013), syntactic parsing (Guo et al., 2015; Ammar et al., 2016), text classification (Shi et al., 2010; Prettenhofer and Stein, 2010) and sentiment analysis (Wan, 2009; Zhou et al., 2016) among others. We consider the lazy setup to be the desired standard setup of CLCD learning for two reasons. First, in true resource-poor languages we expect it to be hard to find a sufficient number of reviews from many domains, even if they are unannotated 239 Our work is mostly related to two works: (a) Cross-lingual SCL (CL-SCL, (Prettenhofer and Stein, 2010, 2011)); and (b) Distributional Correspondence Indexing (DCI, (Fern´andez et al., 2016)) – in both cases pivot features were redefined to support CL (in (a)) and CLCD (in (b)) with non-DNN models, in order to perfor"
D18-1022,K17-1040,1,0.889045,"tions that can serve as a bridge across domains and languages. For CD, this includes: (a) pre-DNN work ((Blitzer et al., 2006, 2007), known as structural correspondence learning (SCL)), that models the connections between pivot features – features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain – and the other, non-pivot, features; (b) DNN work (Glorot et al., 2011; Chen et al., 2012) which employs compress-based noise reduction to learn cross-domain features; and recently also (c) works that combine the two approaches (Ziser and Reichart, 2017, 2018) (henceforth ZR17 and ZR18). For CL, the picture is similar: multilingual representations (usually word embeddings) are prominent in the transfer of NLP algorithms from one language to another (e.g. (Upadhyay et al., 2016)). In this paper we aim to take CL and CD transfer a significant step forward and design methods that can adapt NLP algorithms simultaneously across languages and domains. We consider this research problem fundamental to our field as manually annotated resources are often scarce in many domains, even for languages that are considWhile cross-domain and cross-language tr"
D18-1029,E17-1088,0,0.119612,"e standard fixed-vocabulary assumption. MIN=5: only words with corpus frequency above 5 are retained in the final fixed vocabulary V ; 10K: V comprises the 10k most frequent words. improves the perplexity measure, it actually makes the models less useful, especially in morphologically rich languages, as exemplified in Table 1. Our goal is to get a clear picture on how different typological features and the corresponding corpus frequency distributions affect LM performance, without the influence of the unrealistic fixed-vocabulary assumption. Therefore, we work in the full-vocabulary LM setup (Adams et al., 2017; Grave et al., 2017). This means that we explicitly decide to retain also infrequent words in the modeled data: V contains all words occurring at least once in the training set, only unseen words from test data are treated as OOVs. We believe that this setup leads to an evaluation that pinpoints the crucial limitations of standard LM architectures.2 Why Not Open Vocabulary Setup? Recent neural LM architectures have also focused on handling large vocabularies and unseen words using character-aware modeling (Luong and Manning, 2016; Jozefowicz et al., 2016; Kawakami et al., 2017, inter alia). T"
D18-1029,W13-3520,0,0.0177648,"nly for a fraction of the world’s languages. Second, these data are biased because their features may not stem from the underlying distribution, i.e., from what is naturally possible/frequent, but rather 319 can be inherited by genealogical relatedness or borrowed by areal proximity (Bakker, 2010). To mitigate these biases, theoretical works resorted to stratification approaches, where each subgroup of related languages is sampled independently. maximizing their diversity (Dryer, 1989, inter alia). We perform our selection in the same spirit. We start from the Polyglot Wikipedia (PW) project (Al-Rfou et al., 2013) which provides cleaned and tokenised Wikipedia data in 40 languages. However, the majority of the PW languages are similar from the perspective of genealogy (26/40 are Indo-European), geography (28/40 are Western European), and typology (26/40 are fusional). Consequently, the PW set is not a representative sample of the world’s languages. To amend this limitation, we source additional languages with the data coming from the same domain, Wikipedia, considering candidates in descending order of corpus size cleaned and preprocessed by the Polyglot tokeniser (Al-Rfou et al., 2013). Since fusional"
D18-1029,K17-2002,0,0.0277995,"sis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent success and improved performance with LM-based pre-training methodology (Peters et al., 2018; Howard and Ruder, 2018) across a wide variety of syntactic and"
D18-1029,D08-1078,0,0.02949,"rts preliminary results from prior work (Botha and Blunsom, 2014; Adams et al., 2017; Cotterell et al., 2018), and is also backed by insights from linguistic theory on variance of language complexity in general and variance of morphological complexity in specific (McWhorter, 2001; Evans and Levinson, 2009). More broadly and along the same line, earlier research in statistical machine translation (SMT) has also shown that typological factors such as the amount of reordering, the morphological complexity, as well as genealogical relatedness of languages are crucial in predicting success in SMT (Birch et al., 2008; Paul et al., 2009; Daiber, 2018). Our results indicate that the artificial fixed323 5 Unfortunately no values are available in WALS for the feature of flexivity besides a limited domain. vocabulary assumption from prior work produces overly optimistic perplexity scores, and its limitation is even more pronounced in morphologically rich languages, which inherently contain a large number of infrequent words due to their productive morphological systems. The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stil"
D18-1029,D15-1042,0,0.0182386,"ically diverse languages is still missing. The novel dataset we discuss in this paper aims at bridging this gap (see §4). Multilingual Language Modeling A language model computes a probability distribution over sequences of word tokens, and is typically trained to maximise the likelihood of word input sequences. The LM objective is expressed as: P (w1 , ...wn ) = Y P (wi |w1 , ...wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marc"
D18-1029,Q18-1032,1,0.863129,"Missing"
D18-1029,P17-1109,0,0.0426337,", as shown in §4. 4 Data Selection of Languages. Our selection of test languages is guided by the following goals: a) we have to ensure the coverage of typological properties from §3, and b) we want to analyse a large set of languages which extends and surpasses other work in the LM literature (see §2). Since cross-lingual NLP aims at modeling extant languages rather than possible languages (including, e.g., extinct ones), creating a balanced sample is challenging. In fact, attested languages, intended as a random variable, are extremely sparse and not independent-and-identically-distributed (Cotterell and Eisner, 2017). First, available and reliable data exist only for a fraction of the world’s languages. Second, these data are biased because their features may not stem from the underlying distribution, i.e., from what is naturally possible/frequent, but rather 319 can be inherited by genealogical relatedness or borrowed by areal proximity (Bakker, 2010). To mitigate these biases, theoretical works resorted to stratification approaches, where each subgroup of related languages is sampled independently. maximizing their diversity (Dryer, 1989, inter alia). We perform our selection in the same spirit. We star"
D18-1029,P13-2121,0,0.0214948,"Missing"
D18-1029,N18-2085,0,0.180154,"et statistical models in terms of variation in language structures (Ponti et al., 2017). ∗ Both authors equally contributed to this work. In order to evaluate how cross-lingual structural variation hinders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light"
D18-1029,D17-1030,0,0.0229719,"n to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent s"
D18-1029,K15-1017,0,0.0360047,", but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent success and improved performance with LM-based pre-training methodology (Peters et al., 2018; Howard and Ruder"
D18-1029,P18-1031,0,0.0568651,"word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German"
D18-1029,P17-1137,0,0.184332,"ders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light on the current limitations of standard LM models and offer support to further developments in multilingual NLP. In particular, we demonstrate that the previous fixedvocabulary assumption in fact ign"
D18-1029,2005.mtsummit-papers.11,0,0.0412215,"gium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing a short overview of multilingual LM and its possible setups (§2), we describe the cross-lingual variation in morphological systems and propose a novel typologically diverse dataset for LM in §3. We outline the data in §4 and benchmarked language models in §5. Finally, we discuss the results in light of linguistic typology in §6. 2 To the best of our knowledge, the largest datasets used in previous work are from (Müller et al., 2012; Cotterell et al., 2018) and amount to 21 languages from the Europarl data (Koehn, 2005). Despite the large coverage of languages, these sets are still restricted only to the languages of the European Union. On the other hand, the most typologically diverse dataset thus far was released by Vania and Lopez (2017). It includes 10 languages representing some morphological systems. This short survey of related work demonstrates a clear tendency towards extending LM evaluation to other languages, abandoning English-centric assumptions, and focusing on language-agnostic LM architectures. However, a comprehensive evaluation set that systematically covers a wide and balanced spectrum of"
D18-1029,P16-1100,0,0.0493417,"Missing"
D18-1029,P11-1015,0,0.00869074,"representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs"
D18-1029,J93-2004,0,0.0652462,"2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese,"
D18-1029,D16-1209,0,0.0331062,"Missing"
D18-1029,N12-1043,0,0.0266202,"18 Conference on Empirical Methods in Natural Language Processing, pages 316–327 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing a short overview of multilingual LM and its possible setups (§2), we describe the cross-lingual variation in morphological systems and propose a novel typologically diverse dataset for LM in §3. We outline the data in §4 and benchmarked language models in §5. Finally, we discuss the results in light of linguistic typology in §6. 2 To the best of our knowledge, the largest datasets used in previous work are from (Müller et al., 2012; Cotterell et al., 2018) and amount to 21 languages from the Europarl data (Koehn, 2005). Despite the large coverage of languages, these sets are still restricted only to the languages of the European Union. On the other hand, the most typologically diverse dataset thus far was released by Vania and Lopez (2017). It includes 10 languages representing some morphological systems. This short survey of related work demonstrates a clear tendency towards extending LM evaluation to other languages, abandoning English-centric assumptions, and focusing on language-agnostic LM architectures. However, a"
D18-1029,C16-1123,1,0.845009,"Missing"
D18-1029,N09-2056,0,0.100996,"lts from prior work (Botha and Blunsom, 2014; Adams et al., 2017; Cotterell et al., 2018), and is also backed by insights from linguistic theory on variance of language complexity in general and variance of morphological complexity in specific (McWhorter, 2001; Evans and Levinson, 2009). More broadly and along the same line, earlier research in statistical machine translation (SMT) has also shown that typological factors such as the amount of reordering, the morphological complexity, as well as genealogical relatedness of languages are crucial in predicting success in SMT (Birch et al., 2008; Paul et al., 2009; Daiber, 2018). Our results indicate that the artificial fixed323 5 Unfortunately no values are available in WALS for the feature of flexivity besides a limited domain. vocabulary assumption from prior work produces overly optimistic perplexity scores, and its limitation is even more pronounced in morphologically rich languages, which inherently contain a large number of infrequent words due to their productive morphological systems. The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the"
D18-1029,N18-1202,0,0.182197,".wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech"
D18-1029,D17-1010,0,0.0372516,". The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al"
D18-1029,S17-1003,1,0.846376,"Missing"
D18-1029,E17-2025,0,0.0127292,"ternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs. Full Vocabulary Setup. A majority of language models rely on the fixed-vocabulary assumption: they use a spe"
D18-1029,D15-1044,0,0.0129894,"is still missing. The novel dataset we discuss in this paper aims at bridging this gap (see §4). Multilingual Language Modeling A language model computes a probability distribution over sequences of word tokens, and is typically trained to maximise the likelihood of word input sequences. The LM objective is expressed as: P (w1 , ...wn ) = Y P (wi |w1 , ...wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and"
D18-1029,P17-1184,0,0.256638,"tructural variation hinders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light on the current limitations of standard LM models and offer support to further developments in multilingual NLP. In particular, we demonstrate that the previous fixedvocabulary"
D18-1029,P17-1006,1,0.88688,"Missing"
D18-1029,P16-1125,0,0.0132887,"form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs. Full Vocabulary Setup. A majority of language models rely on"
D18-1290,D08-1107,0,0.0437896,"dependency structure. Hence, prior to PRS16 several works have addressed the syntactic structure of web queries. However, all these works were restricted to tasks that are much simpler than full 3 In some setups they used the segmentation signal from the queries and experimented with a five fold cross-validation over the 4000 queries dependency parsing, including POS tagging (Bendersky et al., 2010; Ganchev et al., 2012), phrase chunking (Bendersky et al., 2011), semantic tagging (Manshadi and Li, 2009; Li, 2010) and classification of queries into syntactic classes (Allan and Raghavan, 2002; Barr et al., 2008). NER has been recognized as a fundamental problem in query processing by Guo et al. (2009), and many works since (e.g. (Alasiry et al., 2012; Eiselt and Figueroa, 2013; Zhai et al., 2016)) explored various models and features for the task. Differently from those works, our goal is to design a BiLSTM model that can be easily integrated with modern BiLSTM parsers. We hence use simple input features and a simple NE scheme (e.g. see (Guo et al., 2009) for more fine-grained distinctions). More sophisticated features, entity schemes and deep learning architectures (e.g. (Lample et al., 2016)) are l"
D18-1290,P11-1011,0,0.0183821,"on the web. Query structure and entity analysis As noted in PRS16, web queries differ from standard sentences, as they tend to be shorter and have a unique dependency structure. Hence, prior to PRS16 several works have addressed the syntactic structure of web queries. However, all these works were restricted to tasks that are much simpler than full 3 In some setups they used the segmentation signal from the queries and experimented with a five fold cross-validation over the 4000 queries dependency parsing, including POS tagging (Bendersky et al., 2010; Ganchev et al., 2012), phrase chunking (Bendersky et al., 2011), semantic tagging (Manshadi and Li, 2009; Li, 2010) and classification of queries into syntactic classes (Allan and Raghavan, 2002; Barr et al., 2008). NER has been recognized as a fundamental problem in query processing by Guo et al. (2009), and many works since (e.g. (Alasiry et al., 2012; Eiselt and Figueroa, 2013; Zhai et al., 2016)) explored various models and features for the task. Differently from those works, our goal is to design a BiLSTM model that can be easily integrated with modern BiLSTM parsers. We hence use simple input features and a simple NE scheme (e.g. see (Guo et al., 20"
D18-1290,D07-1086,0,0.12099,"Missing"
D18-1290,I13-1101,0,0.0312253,"ks that are much simpler than full 3 In some setups they used the segmentation signal from the queries and experimented with a five fold cross-validation over the 4000 queries dependency parsing, including POS tagging (Bendersky et al., 2010; Ganchev et al., 2012), phrase chunking (Bendersky et al., 2011), semantic tagging (Manshadi and Li, 2009; Li, 2010) and classification of queries into syntactic classes (Allan and Raghavan, 2002; Barr et al., 2008). NER has been recognized as a fundamental problem in query processing by Guo et al. (2009), and many works since (e.g. (Alasiry et al., 2012; Eiselt and Figueroa, 2013; Zhai et al., 2016)) explored various models and features for the task. Differently from those works, our goal is to design a BiLSTM model that can be easily integrated with modern BiLSTM parsers. We hence use simple input features and a simple NE scheme (e.g. see (Guo et al., 2009) for more fine-grained distinctions). More sophisticated features, entity schemes and deep learning architectures (e.g. (Lample et al., 2016)) are left for the future. Syntactic parsing of Web data Only a handful of papers aimed to parse web data. One important example is the shared task of Petrov and McDonald (201"
D18-1290,N09-1037,0,0.124826,"rely on millions of (query, title) pairs, which deems their algorithm impractical for most users. In practice, they started with a query log of 60M Yahoo Answers pages and ended up using 7.5M queries as distant supervision. In this paper we aim to overcome this limitation by introducing a high quality query parser that can train on several thousands annotated queries to provide higher UAS and segmentation F1 figures compared to those reported in PRS16 (see footnote 3 for their training protocol and data). We finally note that joint parsing and NER was explored in past (Reichart et al., 2008; Finkel and Manning, 2009, 2010), but for edited text, standard grammar and different modeling techniques. Our work re-emphasizes the strong ties between 2701 NER and parsing, in the context of query analysis. 3 Segmentation-Aware Parsing In this section we present a parser that explicitly accounts for the query dependency grammar of PRS16. We start (§ 3.1) with a brief description of the BiLSTM parser of Kiperwasser and Goldberg (2016) (henceforth KG16), that forms the basis for our parser, and then describe our query parser. 3.1 The KG16 BiLSTM Parser KG16 presented a BiLSTM model for transition based dependency par"
D18-1290,P10-1074,0,0.0178988,"Missing"
D18-1290,P12-2047,0,0.0160115,"ork on parsing of user generated content on the web. Query structure and entity analysis As noted in PRS16, web queries differ from standard sentences, as they tend to be shorter and have a unique dependency structure. Hence, prior to PRS16 several works have addressed the syntactic structure of web queries. However, all these works were restricted to tasks that are much simpler than full 3 In some setups they used the segmentation signal from the queries and experimented with a five fold cross-validation over the 4000 queries dependency parsing, including POS tagging (Bendersky et al., 2010; Ganchev et al., 2012), phrase chunking (Bendersky et al., 2011), semantic tagging (Manshadi and Li, 2009; Li, 2010) and classification of queries into syntactic classes (Allan and Raghavan, 2002; Barr et al., 2008). NER has been recognized as a fundamental problem in query processing by Guo et al. (2009), and many works since (e.g. (Alasiry et al., 2012; Eiselt and Figueroa, 2013; Zhai et al., 2016)) explored various models and features for the task. Differently from those works, our goal is to design a BiLSTM model that can be easily integrated with modern BiLSTM parsers. We hence use simple input features and a"
D18-1290,Q13-1033,0,0.0211939,"d using the Xavier initialization (Glorot and Bengio, 2010) and trained as part of the BiLSTM. Figure 1: A sketch of the KG16 arc-hybrid parser. where θ = W 1 , W 2 , b1 , b2 are the MLP parameters and c = vs0 ◦ vs1 ◦ vs2 ◦ vb0 . The parser employs a margin-based loss (MBL) function at each step: M BL = max(0, 1 − max M LPθ (c)[to ] to ∈G + max M LPθ (c)[tp ]) (4) tp ∈T G where T is the set of possible transitions and G is the set of correct transitions. The losses are summed throughout the parsing of a sentence and the parameters are updated accordingly. The parser employs a dynamic oracle (Goldberg and Nivre, 2013), which enables exploration in training. We next describe our modification of the KG16 parser. Specifically, we change the transition logic so that it can directly account for the query grammar defined in PRS16. 3.2 A Segmentation-Aware BiLSTM Parser In order for our parser to directly account for the forest-based query grammar of PRS16, we follow previous work (e.g. Nivre (2009)) and modify its set of actions and transition logic. Before we do that, we start with a more standard modification. An arc-eager KG16 parser The first step in the design of our parser is changing the arc-hybrid system"
D18-1290,Q16-1023,0,0.403787,"NEs (§ 4). We use this data to establish our observation about the importance of NEs for query parsing, and in order to train and test our entity-aware parser. We split the PRS16 corpus to train (2500 queries), development (750) and test (750) sections (§ 6). In this training setup our segmentation and entity-aware parser achieves a segmentation F1-score of 84.5 (100 on single-segment queries, 60.7 on multi-segment queries) and a dependency parsing UAS of 83.5. Our model outperforms its simpler variants that do not utilize segmentation and/or NE information. For example, the BiLSTM parser of Kiperwasser and Goldberg (2016), which forms the basis for our model, scores 67.7 in segmentation F1 and 77.0 in UAS. We note that our training setup is very different than that of PRS16. They trained their parser on edited text from the OntoNotes 5 corpus (Weischedel et al., 2013) augmented with millions of (query, title) pairs, and their test set consists of the 4000 queries of the query treebank, as they do not train on queries.3 While our work is not directly comparable to theirs, it is worth mentioning that their best model scores 70.4 in segmentation F1 and 76.4 in UAS, much lower than the numbers we report here for o"
D18-1290,D14-1108,0,0.0310849,"hence use simple input features and a simple NE scheme (e.g. see (Guo et al., 2009) for more fine-grained distinctions). More sophisticated features, entity schemes and deep learning architectures (e.g. (Lample et al., 2016)) are left for the future. Syntactic parsing of Web data Only a handful of papers aimed to parse web data. One important example is the shared task of Petrov and McDonald (2012) on parsing web data from the Google Web Treebank, consisting of texts from the email, weblog, CQA, newsgroup, and review domains. Other relevant works are the tweet parsers of Foster et al. (2011), Kong et al. (2014) and Liu et al. (2018). However, all these works did not address the unique properties of web queries with question intent that express information needs in a concise manner (e.g. with one or more phrases or sentence fragments) and follow a forest-based grammar. PRS16 were the first, and to the best of our knowledge the only work to address the parsing of web queries with question intent. However, as noted in § 1 their algorithms rely on millions of (query, title) pairs, which deems their algorithm impractical for most users. In practice, they started with a query log of 60M Yahoo Answers page"
D18-1290,P11-1068,0,0.0562694,"Missing"
D18-1290,N16-1030,0,0.0414529,"n, 2002; Barr et al., 2008). NER has been recognized as a fundamental problem in query processing by Guo et al. (2009), and many works since (e.g. (Alasiry et al., 2012; Eiselt and Figueroa, 2013; Zhai et al., 2016)) explored various models and features for the task. Differently from those works, our goal is to design a BiLSTM model that can be easily integrated with modern BiLSTM parsers. We hence use simple input features and a simple NE scheme (e.g. see (Guo et al., 2009) for more fine-grained distinctions). More sophisticated features, entity schemes and deep learning architectures (e.g. (Lample et al., 2016)) are left for the future. Syntactic parsing of Web data Only a handful of papers aimed to parse web data. One important example is the shared task of Petrov and McDonald (2012) on parsing web data from the Google Web Treebank, consisting of texts from the email, weblog, CQA, newsgroup, and review domains. Other relevant works are the tweet parsers of Foster et al. (2011), Kong et al. (2014) and Liu et al. (2018). However, all these works did not address the unique properties of web queries with question intent that express information needs in a concise manner (e.g. with one or more phrases o"
D18-1290,P10-1136,0,0.0257388,", web queries differ from standard sentences, as they tend to be shorter and have a unique dependency structure. Hence, prior to PRS16 several works have addressed the syntactic structure of web queries. However, all these works were restricted to tasks that are much simpler than full 3 In some setups they used the segmentation signal from the queries and experimented with a five fold cross-validation over the 4000 queries dependency parsing, including POS tagging (Bendersky et al., 2010; Ganchev et al., 2012), phrase chunking (Bendersky et al., 2011), semantic tagging (Manshadi and Li, 2009; Li, 2010) and classification of queries into syntactic classes (Allan and Raghavan, 2002; Barr et al., 2008). NER has been recognized as a fundamental problem in query processing by Guo et al. (2009), and many works since (e.g. (Alasiry et al., 2012; Eiselt and Figueroa, 2013; Zhai et al., 2016)) explored various models and features for the task. Differently from those works, our goal is to design a BiLSTM model that can be easily integrated with modern BiLSTM parsers. We hence use simple input features and a simple NE scheme (e.g. see (Guo et al., 2009) for more fine-grained distinctions). More sophis"
D18-1290,N18-1088,0,0.0476209,"features and a simple NE scheme (e.g. see (Guo et al., 2009) for more fine-grained distinctions). More sophisticated features, entity schemes and deep learning architectures (e.g. (Lample et al., 2016)) are left for the future. Syntactic parsing of Web data Only a handful of papers aimed to parse web data. One important example is the shared task of Petrov and McDonald (2012) on parsing web data from the Google Web Treebank, consisting of texts from the email, weblog, CQA, newsgroup, and review domains. Other relevant works are the tweet parsers of Foster et al. (2011), Kong et al. (2014) and Liu et al. (2018). However, all these works did not address the unique properties of web queries with question intent that express information needs in a concise manner (e.g. with one or more phrases or sentence fragments) and follow a forest-based grammar. PRS16 were the first, and to the best of our knowledge the only work to address the parsing of web queries with question intent. However, as noted in § 1 their algorithms rely on millions of (query, title) pairs, which deems their algorithm impractical for most users. In practice, they started with a query log of 60M Yahoo Answers pages and ended up using 7"
D18-1290,P09-1097,0,0.0338688,"lysis As noted in PRS16, web queries differ from standard sentences, as they tend to be shorter and have a unique dependency structure. Hence, prior to PRS16 several works have addressed the syntactic structure of web queries. However, all these works were restricted to tasks that are much simpler than full 3 In some setups they used the segmentation signal from the queries and experimented with a five fold cross-validation over the 4000 queries dependency parsing, including POS tagging (Bendersky et al., 2010; Ganchev et al., 2012), phrase chunking (Bendersky et al., 2011), semantic tagging (Manshadi and Li, 2009; Li, 2010) and classification of queries into syntactic classes (Allan and Raghavan, 2002; Barr et al., 2008). NER has been recognized as a fundamental problem in query processing by Guo et al. (2009), and many works since (e.g. (Alasiry et al., 2012; Eiselt and Figueroa, 2013; Zhai et al., 2016)) explored various models and features for the task. Differently from those works, our goal is to design a BiLSTM model that can be easily integrated with modern BiLSTM parsers. We hence use simple input features and a simple NE scheme (e.g. see (Guo et al., 2009) for more fine-grained distinctions)."
D18-1290,J08-4003,0,0.0267491,"next describe our modification of the KG16 parser. Specifically, we change the transition logic so that it can directly account for the query grammar defined in PRS16. 3.2 A Segmentation-Aware BiLSTM Parser In order for our parser to directly account for the forest-based query grammar of PRS16, we follow previous work (e.g. Nivre (2009)) and modify its set of actions and transition logic. Before we do that, we start with a more standard modification. An arc-eager KG16 parser The first step in the design of our parser is changing the arc-hybrid system of the KG16 parser to an arc-eager system (Nivre, 2008). To do that we change the definitions of the RIGHTarc and LEF Tarc transitions and add a REDUCE transition. The (original) archybrid and the (modified) arc-eager KG16 parsers are denoted with P H and P E , respectively. The motivation for this change is the addition of the REDU CE transition that explicitly facili2702 tates segmentation. After the words in the stack σ are reduced, they cannot be connected to the unprocessed words in the buffer β. This state constitutes a segmentation point. We use this connection between the REDU CE transition and the segmentation operation as an integral par"
D18-1290,P09-1040,0,0.0158421,"of possible transitions and G is the set of correct transitions. The losses are summed throughout the parsing of a sentence and the parameters are updated accordingly. The parser employs a dynamic oracle (Goldberg and Nivre, 2013), which enables exploration in training. We next describe our modification of the KG16 parser. Specifically, we change the transition logic so that it can directly account for the query grammar defined in PRS16. 3.2 A Segmentation-Aware BiLSTM Parser In order for our parser to directly account for the forest-based query grammar of PRS16, we follow previous work (e.g. Nivre (2009)) and modify its set of actions and transition logic. Before we do that, we start with a more standard modification. An arc-eager KG16 parser The first step in the design of our parser is changing the arc-hybrid system of the KG16 parser to an arc-eager system (Nivre, 2008). To do that we change the definitions of the RIGHTarc and LEF Tarc transitions and add a REDUCE transition. The (original) archybrid and the (modified) arc-eager KG16 parsers are denoted with P H and P E , respectively. The motivation for this change is the addition of the REDU CE transition that explicitly facili2702 tates"
D18-1290,N16-1081,1,0.867616,"Missing"
D18-1290,P08-1098,1,0.577133,"in § 1 their algorithms rely on millions of (query, title) pairs, which deems their algorithm impractical for most users. In practice, they started with a query log of 60M Yahoo Answers pages and ended up using 7.5M queries as distant supervision. In this paper we aim to overcome this limitation by introducing a high quality query parser that can train on several thousands annotated queries to provide higher UAS and segmentation F1 figures compared to those reported in PRS16 (see footnote 3 for their training protocol and data). We finally note that joint parsing and NER was explored in past (Reichart et al., 2008; Finkel and Manning, 2009, 2010), but for edited text, standard grammar and different modeling techniques. Our work re-emphasizes the strong ties between 2701 NER and parsing, in the context of query analysis. 3 Segmentation-Aware Parsing In this section we present a parser that explicitly accounts for the query dependency grammar of PRS16. We start (§ 3.1) with a brief description of the BiLSTM parser of Kiperwasser and Goldberg (2016) (henceforth KG16), that forms the basis for our parser, and then describe our query parser. 3.1 The KG16 BiLSTM Parser KG16 presented a BiLSTM model for trans"
D19-1226,Q17-1010,0,0.0982944,"revious state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog"
D19-1226,S17-2001,0,0.0533888,"Missing"
D19-1226,P19-1070,1,0.721523,"Missing"
D19-1226,P15-2011,1,0.869561,"Missing"
D19-1226,W16-2501,1,0.82432,", word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing sp"
D19-1226,J15-4004,1,0.915895,"Missing"
D19-1226,P14-2075,0,0.0774785,"pecialization on LS, we employ ˇ Light-LS (Glavaˇs and Stajner, 2015), a languageagnostic LS tool that makes simplifications based on word similarities in a given vector space. The quality of similarity-based information encoded in the vector space encode is thus expected to directly correlate with the performance of Light-LS. We use LS datasets for Italian (IT) (Tonelli et al., 2016), Spanish (ES) (Saggion et al., 2015; Saggion, 2017), and Portuguese (PT) (Hartmann et al., 2018) to evaluate the specialized spaces in those languages. We rely on the standard LS evaluation metric of Accuˇ racy (Horn et al., 2014; Glavaˇs and Stajner, 2015): it quantifies both the quality and frequency of replacements as a number of correct simplifications divided by the total number of complex words. Results and Analysis. The results are reported in Table 3. As shown in previous work (Vuli´c et al., 2018; Ponti et al., 2018), retrofitting (CLSRI-AR) and the cross-lingual post-specialization transfer (X-PS) are substantially better in the LS task than the original distributional space. However, our full CLSRI-PS model results in substantial boosts in the 2213 LS any additional input for the lexical prediction step (i."
D19-1226,N15-1184,0,0.302339,"Missing"
D19-1226,D18-1330,0,0.0952862,"Missing"
D19-1226,W19-4310,1,0.811783,"Missing"
D19-1226,D15-1242,0,0.242713,"Missing"
D19-1226,W16-1607,0,0.262024,"Missing"
D19-1226,N18-2029,1,0.870858,"Missing"
D19-1226,P18-1004,1,0.752226,"Missing"
D19-1226,N16-1018,0,0.139801,"Missing"
D19-1226,C18-1205,0,0.107783,"and translation of incorrect senses of Ls words. We thus subsequently refine the noisy set of target constraints by having a state-of-the-art neural model for lexico-semantic relation prediction (Glavaˇs and Vuli´c, 2018a), trained on the Ls constraints, discern valid from invalid Lt constraints. Following that, we perform monolingual retrofitting and post-specialization in the target language Lt , as outlined in § 3.2. The Lt distributional vectors can be specialized with the cleaned Lt constraints using any off-the-shelf retrofitting model (Faruqui et al., 2015; Mrkˇsi´c et al., 2016; 2208 Lengerich et al., 2018, inter alia). In this work we opt for the best-performing retrofitting model ATTRACT- REPEL ( AR ) (Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b). AR specializes only the words seen in the cleaned Lt constraints. As the final step, we generalize AR’s specialization to the entire target vocabulary with a post-specialization model (Ponti et al., 2018) that learns the global specialization function from pairs of distributional and ARspecialized vectors of words from Lt constraints. A visual summary of our transfer model is presented in Figure 1. Our proposed CLSRI specialization conceptually diff"
D19-1226,P14-2050,0,0.0346927,"og state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., betwe"
D19-1226,C18-1172,0,0.0157486,"ext simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better downstream performance (Mrkˇsi´c et al., 2016). Moreover, while the joint models are tightly coupled to a concrete word embedding objective, retrofitting models can be applied on top"
D19-1226,P15-1145,0,0.0602024,"ntic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical"
D19-1226,N16-1118,0,0.020871,". The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can br"
D19-1226,P18-2018,1,0.880048,"Missing"
D19-1226,Q17-1022,1,0.892103,"Missing"
D19-1226,L18-1381,0,0.208203,"Missing"
D19-1226,C16-1123,1,0.872206,"Missing"
D19-1226,N15-1100,0,0.0535031,"., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the l"
D19-1226,Q16-1030,0,0.0603068,"rity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better dow"
D19-1226,D14-1162,0,0.0845732,"mantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensi"
D19-1226,N18-1202,0,0.0147967,"ialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation s"
D19-1226,J19-3005,1,0.873216,"Missing"
D19-1226,P17-1163,0,0.0403913,"Missing"
D19-1226,D18-1026,1,0.750703,"Missing"
D19-1226,D18-1299,0,0.171899,"Missing"
D19-1226,Q15-1025,0,0.0752505,"(Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing specialization models (cf. Mrkˇsi´c et al. 2017; Ponti et al. 2018) are executed as vector space post-processors. In short, these techniques force the distributional vectors to conform to external linguistic constraints (e.g., synonymy, meronymy, lexical entailment) extracted from structured external resources (e.g., WordNet, BabelNet) to emphasize the particular relation. As post-processors th"
D19-1226,K15-1026,1,0.801861,"ally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015;"
D19-1226,S17-2016,0,0.0528375,"Missing"
D19-1226,P18-1072,1,0.903026,"Missing"
D19-1226,P14-2089,0,0.035718,"d vectors; semantic specialization of such spaces for a particular lexicosemantic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of"
D19-1226,D14-1161,0,0.211456,"Missing"
D19-1226,P18-1135,0,0.0126936,"ble 2. Several findings emerge from the results. First, as already confirmed in prior work (Vuli´c et al., 2018; Ponti et al., 2018), vectors specialized for semantic similarity are indeed important for DST: we observe improvements with all specialized vectors. The highest gains are observed with the full CSLRIPS model. This confirms two main intuitions: 1) our proposed specialization transfer via lexical induction in the target language is more robust than 15 Note that the original NBT framework in the English DST task has been recently surpassed by more intricate taskspecific architectures (Zhong et al., 2018; Ren et al., 2018), but its lightweight design coupled with its strong dependence on input word vectors still makes it a convenient means to evaluate the effects of different specialization methods. Lexical Simplification Lexical simplification (LS) aims to automatically replace complex words (i.e., specialized terms, words used less frequently and known to fewer speakers) with their simpler in-context synonyms: the simplified text must be grammatical and retain the meaning of the original text. Lexical simplification critically depends on discerning semantic similarity from other types of se"
D19-1226,W17-0228,0,0.123249,"Missing"
D19-1226,N18-1048,1,0.885485,"Missing"
D19-1226,N18-1103,1,0.913929,"Missing"
D19-1226,D17-1270,1,0.902499,"Missing"
D19-1226,P17-1006,1,0.89768,"Missing"
D19-1226,E17-1042,0,0.0520418,"Missing"
D19-1288,W18-5412,0,0.214596,"d stored it in the form of attribute–value features in publicly accessible databases (Croft, 2002; Dryer and Haspelmath, 2013). The usage of such features to inform neural NLP models is still scarce, partly because the evidence in favor of their effectiveness is mixed (Ponti et al., 2018, 2019). In this work, we propose a way to distantly supervise the model with this side information effectively. We extend our non-conditional language models outlined in §3 (BARE) to a series of variants conditioned on language-specific properties, inspired by Östling and Tiedemann (2017) and Platanios et al. (2018). A fundamental difference from these previous works, however, is that they learn such properties in an end-to-end fashion from the data in a joint multilingual learning setting. Obviously, this is not feasible for the zeroshot setting and unreliable for the few-shot setting. Rather, we represent languages with their typological feature vector, which we assume to be readily available both for both training and held-out languages. Let t` ∈ [0, 1]f be a vector of f typological features for language ` ∈ T t E. We reinterpret the conditional language models within the Bayesian framework by estimat"
D19-1288,N16-1161,0,0.0861779,"shing the problem to its most complex formulation, zero-shot inference, and in taking into account the largest sample of languages for language modeling to date. In addition to those considered in our work, there are also alternative methods to condition language models on features. Kalchbrenner and Blunsom (2013) used encoded features as additional biases in recurrent layers. Kiros et al. (2014) put forth a log-bilinear model that allows for a ‘multiplicative interaction’ between hidden representations and input features (such as images). With a similar device, but a different gating method, Tsvetkov et al. (2016) trained a phoneme-level joint multilingual model of words conditioned on typological features from Moran et al. (2014). The use of the Laplace method for neural transfer learning has been proposed by Kirkpatrick et al. (2017), inspired by synaptic consolidation in neuroscience, with the aim to avoid catastrophic forgetting. Kochurov et al. (2018) tackled the problem of continuous learning by approximating the posterior probabilities through stochastic variational inference. Ritter et al. (2018) substitute diagonal Laplace approximation with a Kronecker factored method, leading to better uncer"
D19-1288,Q19-1040,0,0.125021,"rld’s languages, we hope that these findings will help broaden the scope of applications for language technology. 1 Introduction With the success of recurrent neural networks and other black-box models on core NLP tasks, such as language modeling, researchers have turned their attention to the study of the inductive bias such neural models exhibit (Linzen et al., 2016; Marvin and Linzen, 2018; Ravfogel et al., 2018). A number of natural questions have been asked. For example, do recurrent neural language models learn syntax (Marvin and Linzen, 2018)? Do they map onto grammaticality judgments (Warstadt et al., 2019)? However, as Ravfogel et al. (2019) note, “[m]ost of the work so far has focused on English.” Moreover, these studies have almost always focused on training scenarios where a large number of in-language sentences are available. In this work, we aim to find a prior distribution over network parameters that generalize well to new human languages. The recent vein of research on the inductive biases of neural nets implicitly assumes a uniform (unnormalizable) prior over the space of neural network parameters (Ravfogel et al., 2019, inter alia). In contrast, we take a Bayesian-updating approach: F"
D19-1449,E17-1088,0,0.0317067,"ble 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al., 2019), we then reserve the 5K pairs created from the more frequent L1 words for training, while the next 2K pairs are used for test. Smaller training dictionaries (1K and 500 pairs) are created by again"
D19-1449,W13-3520,0,0.0499261,"LWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical strings), in contrast, yield good solutions for all setups. 5 Fur"
D19-1449,D18-1062,0,0.0195462,"seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their m"
D19-1449,2020.lrec-1.495,0,0.468497,"Missing"
D19-1449,P18-1073,0,0.0976736,"e induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dict"
D19-1449,J82-2005,0,0.673463,"Missing"
D19-1449,D16-1136,0,0.0311352,"tive agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al.,"
D19-1449,E14-1049,0,0.0557935,"l., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019"
D19-1449,C10-3010,0,0.0486842,"c Uralic Austronesian Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces"
D19-1449,D18-1029,1,0.891637,"Missing"
D19-1449,W16-1614,0,0.118504,"et al. (2018) and further verified by Glavaš et al. (2019) and Doval et al. (2019), the language pair at hand can have a huge impact on CLWE induction: the adversarial method of Conneau et al. (2018a) often gets stuck in poor local optima and yields degenerate solutions for distant language pairs such as English-Finnish. More recent CLWE methods (Artetxe et al., 2018b; Mohiuddin and Joty, 2019) focus on mitigating this robustness issue. However, they still rely on one critical assumption which leads them to degraded performance for distant language pairs: they assume approximate isomorphism (Barone, 2016; Søgaard et al., 2018) between monolingual embedding spaces to learn the initial seed dictionary. In other words, they assume very similar geometric constellations between two monolingual spaces: due to the Zipfian phenomena in language (Zipf, 1949) such near-isomorphism can be satisfied only for similar languages and for similar domains used for training monolingual vectors. This property is reflected in the results reported in Table 3, the number of unsuccessful setups in Table 4, as well as later in Figure 4. For instance, the largest number of unsuccessful BLI setups with the UNSUPERVISED"
D19-1449,P19-1070,1,0.828323,"Missing"
D19-1449,Q17-1010,0,0.186535,"rature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CLWE induction (e.g., when BLI performance is similar to a random baseline): detecting such CLWEs is espe"
D19-1449,D14-1082,0,0.0593674,"uage pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Lit"
D19-1449,N15-1157,0,0.0457001,"lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries t"
D19-1449,D18-1024,0,0.0746406,"typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can rough"
D19-1449,L18-1550,0,0.0283189,"nguage pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CL"
D19-1449,P15-1119,0,0.0322898,"transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui"
D19-1449,D18-1269,0,0.0727556,"eak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-tra"
D19-1449,N19-1188,1,0.675442,"Missing"
D19-1449,N18-2085,0,0.0180662,"nslations for a test set of source language words. Its lightweight nature allows us to conduct a comprehensive evaluation across a large number of language pairs.5 Since BLI is cast as a ranking task, following Glavaš et al. (2019) we use mean average precision (MAP) as the main evaluation metric: in our BLI setup with only one correct translation for each “query” word, MAP is equal to mean reciprocal rank (MRR).6 (Selection of) Language Pairs. Our selection of test languages is guided by the following goals: a) following recent initiatives in other NLP research (e.g., for language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure the coverage of different genealogical and typological language properties, and b) we aim to analyze a large set of language pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families."
D19-1449,E17-1102,1,0.919319,"Missing"
D19-1449,D18-1043,0,0.222539,"languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language families and morphological types, as we argue that fully unsupervised CLWEs have been designed to support exactly these setups. However, we show that even the most robust unsupervised CLWE method (Artetxe et al., 2018b) still fails for a large number of langu"
D19-1449,N19-1386,0,0.228386,"Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupervised extraction of a seed dictionary; C2) a"
D19-1449,kamholz-etal-2014-panlex,0,0.516415,"Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a Pa"
D19-1449,D18-1047,0,0.0484142,"solute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demon"
D19-1449,K18-1021,0,0.0602112,"an inherently interesting research topic per se. Nonetheless, the main practical motivation for developing such approaches in the first place is to facilitate the construction of multilingual NLP tools and widen the access to language technology for resource-poor languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language fam"
D19-1449,P19-1492,0,0.0933756,"Missing"
D19-1449,D18-1101,0,0.0167767,"t al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupe"
D19-1449,C12-1089,0,0.180369,"n a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and com"
D19-1449,2005.mtsummit-papers.11,0,0.0217419,"more, one could argue that we do not need unsupervised CLWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical"
D19-1449,P15-1165,0,0.159533,"Missing"
D19-1449,P18-1072,1,0.810814,"Missing"
D19-1449,D18-1549,0,0.0740195,"Missing"
D19-1449,P10-1040,0,0.0777116,"e languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vu"
D19-1449,P16-1024,1,0.918618,"Missing"
D19-1449,N19-1045,0,0.0138207,"the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demonstrated that these components can be equally applied to weakly sup"
D19-1449,D17-1270,1,0.904042,"Missing"
D19-1449,N15-1104,0,0.525331,"Missing"
D19-1449,P19-1307,0,0.0605721,"tance, the underlying assumption of all projection-based methods (both supervised and unsupervised) is the topological similarity between monolingual spaces, which is why standard simple linear projections result in lower absolute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. F"
D19-1449,N16-1156,0,0.0208315,"tilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned seve"
D19-1449,D18-1022,1,0.928371,"ardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-ling"
J15-4004,P14-2131,0,0.0210427,"Missing"
J15-4004,P14-1023,0,0.903009,"Missing"
J15-4004,J10-4006,0,0.00593156,"hine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. In particular, in both WS353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating = 6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high rating regardless of whether or not their constituents are similar. Thus, the utility of such resources to the development and application of similarity models is limited, a problem exacerbated by the fact that many researchers appear unaware of what"
J15-4004,W14-2402,0,0.0140987,"Missing"
J15-4004,P13-2010,0,0.0350814,"Missing"
J15-4004,P06-4018,0,0.0582656,"Missing"
J15-4004,P12-1015,0,0.639717,"rhonen}@ cl.cam.ac.uk. ∗∗ Technion, Israel Institute of Technology, Haifa, Israel. E-mail: roiri@ie.technion.ac.il. Submission received: 25 July 2014; revised submission received: 10 June 2015; accepted for publication: 31 August 2015. doi:10.1162/COLI a 00237 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 4 cup are rated as more “similar” than pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977)."
J15-4004,J06-1003,0,0.0176138,"n pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et a"
J15-4004,P14-1024,0,0.0230115,"Missing"
J15-4004,P08-1088,0,0.00473907,"ese are precisely the sort of pairs that are not contained in existing evaluation gold standards. Table 1 lists the USF noun pairs with the lowest similarity scores overall, and also those with the largest additive discrepancy between association strength and similarity. 2.1.1 Association and Similarity in NLP. As noted in the Introduction, the similarity/association distinction is not only of interest to researchers in psychology or linguistics. Models of similarity are particularly applicable to various NLP tasks, such as lexical resource building, semantic parsing, and machine translation (Haghighi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012"
J15-4004,D08-1011,0,0.00927383,"its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. In particular, in both WS353 and M"
J15-4004,Q14-1023,1,0.576245,"Missing"
J15-4004,P12-1092,0,0.688971,"erformance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, rec"
J15-4004,W14-1503,0,0.0286789,"so, we evaluate the models on the SimLex999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets and subsets of more and less strongly associated pairs (Sections 5.2.2–5.2.4). As part of these analyses, we confirm the hypothesis (Agirre et al. 2009; Levy and Goldberg 2014) that models learning from input informed by dependency parsing, rather than simple running-text input, yield improved similarity estimation and, specifically, clearer distinction between similarity and association. In contrast, we find no evidence for a related hypothesis (Agirre et al. 2009; Kiela and Clark 2014) that smaller context windows improve the ability of models to capture similarity. We do, however, observe clear differences in model performance on the distinct concept types included in SimLex-999. Taken together, these experiments demonstrate the benefit of the diversity of concepts 2 www.mturk.com/. 667 Computational Linguistics Volume 41, Number 4 included in SimLex-999; it would not have been possible to derive similar insights by evaluating based on existing gold standards. We conclude by discussing how observations such as these can guide future research into distributional semantic mo"
J15-4004,P14-2135,1,0.451794,"Missing"
J15-4004,C94-1103,0,0.0904133,"Missing"
J15-4004,P14-2050,0,0.7627,"ty of lexical concepts in general, current models achieve notably lower scores on SimLex-999 than on existing gold standard evaluations, and well below the SimLex-999 inter-human agreement ceiling. Finally, we explore ways in which distributional models might improve on this performance in similarity modeling. To do so, we evaluate the models on the SimLex999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets and subsets of more and less strongly associated pairs (Sections 5.2.2–5.2.4). As part of these analyses, we confirm the hypothesis (Agirre et al. 2009; Levy and Goldberg 2014) that models learning from input informed by dependency parsing, rather than simple running-text input, yield improved similarity estimation and, specifically, clearer distinction between similarity and association. In contrast, we find no evidence for a related hypothesis (Agirre et al. 2009; Kiela and Clark 2014) that smaller context windows improve the ability of models to capture similarity. We do, however, observe clear differences in model performance on the distinct concept types included in SimLex-999. Taken together, these experiments demonstrate the benefit of the diversity of concep"
J15-4004,N15-1098,0,0.0323029,"Missing"
J15-4004,P06-1129,0,0.00844882,"st 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2"
J15-4004,W13-3512,0,0.933535,"Missing"
J15-4004,D09-1040,0,0.00699259,"Missing"
J15-4004,D07-1042,0,0.0859608,"Missing"
J15-4004,D10-1114,0,0.00897274,"(Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were ask"
J15-4004,N10-1013,0,0.285524,"(Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were ask"
J15-4004,rose-etal-2002-reuters,0,0.0253268,"Missing"
J15-4004,P14-1068,0,0.566786,"Missing"
J15-4004,P10-1040,0,0.144364,"Missing"
J15-4004,W99-0502,0,0.073997,"of what their evaluation resources actually measure.1 Although certain smaller gold standards—those of Rubenstein and Goodenough (1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these resources suffer from other important limitations. For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations. It is common practice in NLP to define the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Moon"
J15-4004,P94-1019,0,\N,Missing
J15-4004,N04-3012,0,\N,Missing
J15-4004,N09-1003,0,\N,Missing
J15-4004,W13-2609,1,\N,Missing
J19-3005,P13-2037,0,0.0744461,"Missing"
J19-3005,W17-0401,0,0.0483167,"Missing"
J19-3005,P15-2044,0,0.0358947,"Missing"
J19-3005,W14-4203,0,0.0512933,"Missing"
J19-3005,P15-1040,0,0.0140754,"k is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al."
J19-3005,Q16-1031,0,0.0249656,"Missing"
J19-3005,D18-1549,0,0.0165078,"da et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and consequently their rate of infrequent words. Moreover, the ambiguity of mapping between form and meaning of morphemes determines the usefulness of injecting character-level information (Gerz et al. 2018a, 2018b). This variation has to be taken into account in both language transfer and multilingual joint learning. As a final note, we stress that the addition of new features does not concern just future work, but also the existing typology-savvy methods, which c"
J19-3005,D17-1011,0,0.193671,"ases. In this article, we provide an extensive survey of typologically informed NLP methods to date, including the more recent neural approaches not previously surveyed in this area. We consider the impact of typological (including both structural and semantic) information on system performance and discuss the optimal sources for such information. Traditionally, typological information has been obtained from hand-crafted databases and, therefore, it tends to be coarse-grained and incomplete. Recent research has focused on inferring typological information automatically from multilingual data (Asgari and Schütze 2017, inter alia), with the specific purpose of obtaining a more complete and finer-grained set of feature values. We survey these techniques and discuss ways to integrate their predictions into the current NLP algorithms. To the best of our knowledge, this has not yet been covered in the existing literature. In short, the key questions our paper addresses can be summarized as follows: (i) Which NLP tasks and applications can benefit from typology? (ii) What are the advantages and limitations of currently available typological databases? Can data-driven inference of typological features offer an a"
J19-3005,D08-1014,0,0.0572004,"Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be used to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016). Some of these methods are hampered by their resource requirements. In fact, annotation projection and translation need parallel texts to align words and train translation systems, respectively (Agi´c, Hovy, and Søgaard 2015). Moreover, comparisons of stateof-the-art algorithms revealed that model"
J19-3005,W09-0106,0,0.0830635,"ction The world’s languages may share universal features at a deep, abstract level, but the structures found in real-world, surface-level texts can vary significantly. This crosslingual variation has challenged the development of robust, multilingually applicable Natural Language Processing (NLP) technology, and as a consequence, existing NLP is still largely limited to a handful of resource-rich languages. The architecture design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das"
J19-3005,P07-1036,0,0.126415,"Missing"
J19-3005,Q18-1039,0,0.0162199,"al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representa"
J19-3005,C16-1298,0,0.0281595,"Missing"
J19-3005,P11-1061,0,0.244408,"009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promising source of informati"
J19-3005,P07-1009,0,0.17624,"Missing"
J19-3005,P16-1038,0,0.174612,"16) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whether a language allows two sounds to differ only in voicing, such as /t/ and /d/. Finally, a small number of experiments adopted the entire feature inventory of typological databases, without any sort of pre-selection. In particular, Agi´c (2017) and Ammar et al. (2016) extracted all the features in WALS, whereas Deri and Knight (2016) extracted all the features in URIEL. Schone and Jurafsky (2001) did not resort to basic typological features, but rather to “several hundred [implicational universals] applicable to syntax” drawn from the Universal Archive (Plank and Filiminova 1996). Typological attributes that are extracted from typological databases are typically represented as feature vectors in which each dimension encodes a feature value. This feature representation is often binarized (Georgi, Xia, and Lewis 2010): For each possible value v of each database attribute a, a new feature is created with value 1 if it corres"
J19-3005,P15-2139,0,0.0321535,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D15-1040,0,0.041461,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D16-1136,0,0.0403964,"Missing"
J19-3005,D12-1001,0,0.018952,"Missing"
J19-3005,P17-2093,0,0.047937,"Missing"
J19-3005,N15-1184,0,0.0387115,"et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mrkši´c et al. 2016; Osborne, Narayan, and Cohen 2016) have proposed means through which external knowledge sourced from linguistic resources (such as WordNet, BabelNet, or lists of morphemes) can be encoded in word embeddings. Among the state-of-the-art specialization methods ATTRACT- REPEL (Mrkši´c et al. 2017; Vuli´c et al. 2017) pushes together or pulls apart 589 Computational Linguistics Volume 45, Number 3 vector pairs according to relational constraints, while preserving the relationship between words in the original space and possibly propagating the specializ"
J19-3005,C10-1044,0,0.0863316,"Missing"
J19-3005,Q18-1032,1,0.878475,"Missing"
J19-3005,D18-1029,1,0.881238,"Missing"
J19-3005,N15-1157,0,0.0210384,"Missing"
J19-3005,C16-1002,0,0.0416532,"ching scenarios (Adel, Vu, and Schultz 2013). In fact, multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Univ"
J19-3005,P15-1119,0,0.0603047,"Missing"
J19-3005,P16-1228,0,0.0498272,"Missing"
J19-3005,D16-1173,0,0.0523125,"Missing"
J19-3005,P11-1057,0,0.205918,"gure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer"
J19-3005,C12-1089,0,0.0473073,"Missing"
J19-3005,P13-1117,0,0.0959291,"Missing"
J19-3005,P11-2055,0,0.04082,"Missing"
J19-3005,I08-2093,0,0.0406793,"ions. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Glossed Texts (IGT). Such collections of example sentences are collated by linguists and contain grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train context free grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features. 574 Ponti et al. Modeling Language Variation and Universals Unsupervised Propagation Morphosyntactic Annotation Table 2 An overview of the strategies for prediction of typological features. Author Details Requirements Liu (2010) Lewis and Xia (2008) Treebank count IGT projection Treebank IGT, source chunker 20 97 Bender et al. (2013) IGT projection IGT, source chunker 31 Östling ("
J19-3005,P13-3022,0,0.408676,"k-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word order and case alignment 986 word order 6 word order 325 whole word order and passive whole 14"
J19-3005,W15-1521,0,0.0349482,"Missing"
J19-3005,D17-1268,0,0.142382,"Missing"
J19-3005,P08-1099,0,0.0527456,"hallenge: How can the output of the model be biased to agree with the constraints while the efficiency of the search procedure is kept? In this article we do not answer this question directly but rather survey a number of approaches that succeed in dealing with it. Because linear models have been prominent in NLP research for a much longer time, it is not surprising that frameworks for the integration of soft constraints into these models are much more developed. The approaches proposed for this purpose include posterior regularization (PR) (Ganchev et al. 2010), generalized expectation (GE) (Mann and McCallum 2008), constraint-driven learning (CODL) (Chang, Ratinov, and Roth 2007), dual decomposition (DD) (Globerson and Jaakkola 2007; Komodakis, Paragios, and Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distributio"
J19-3005,P05-1012,0,0.0516887,"Missing"
J19-3005,Q17-1022,1,0.921061,"Missing"
J19-3005,N16-1018,0,0.029262,"Missing"
J19-3005,I17-1046,0,0.464172,"ea, and typology-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme"
J19-3005,P12-1066,0,0.286336,"Missing"
J19-3005,D10-1120,0,0.0346019,"Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distribution is defined on the model parameters. PR has already been used for incorporating universal linguistic knowledge into an unsupervised parsing model (Naseem et al. 2010). In the future, it could be extended to typological knowledge, which is a good fit for soft constraints. As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been"
J19-3005,W11-2124,0,0.0331622,"rameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared across languages. Tied parameters are shown as neurons with identical color. Image adapted from Fang and Cohn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vector"
J19-3005,C16-1123,1,0.918794,"Missing"
J19-3005,Q16-1030,0,0.164108,"Missing"
J19-3005,P15-2034,0,0.172911,"syntactically annotated texts. For example, word order features can be calculated by counting the average direction of dependency relations or constituency hierarchies (Liu 2010). Consider the tree of a sentence in Welsh from Bender et al. (2013) in Figure 6. The relative order of verb– subject, and verb–object can be deduced from the position of the relevant nodes VBD, NNS , and NNO (highlighted). Morphosyntactic annotation is often unavailable for resource-lean languages. In such cases, it can be projected from a source language to a target language through language transfer. For instance, Östling (2015) projects source morphosyntactic annotation directly to several languages through a multilingual word alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Gloss"
J19-3005,E17-2102,0,0.0715416,"g multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig, and Littell 2017, see § 4.3"
J19-3005,H05-1108,0,0.0802046,"cific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promisin"
J19-3005,I17-1102,0,0.0522275,"Missing"
J19-3005,P18-1142,1,0.820943,"Missing"
J19-3005,D18-1026,1,0.847658,"Missing"
J19-3005,S17-1003,1,0.801184,"Missing"
J19-3005,N12-1008,1,0.844552,". As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been applied to minimally supervised learning and domain adaptation with soft (non-typological) constraints (Reichart and Barzilay 2012; Rush et al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, simila"
J19-3005,P15-2040,0,0.0202231,"ribution of each language and example. The selection is typically carried out through general language similarity metrics. For instance, Deri and Knight (2016) base their selection on the URIEL language typology database, considering information about genealogical, geographic, syntactic, and phonetic properties. This facilitates language transfer of grapheme-to-phoneme models, by guiding the choice of source languages and aligning phoneme inventories. Metrics for source selection can also be extracted in a data-driven fashion, without explicit reference to structured taxonomies. For instance, Rosa and Zabokrtsky (2015) estimate the Kullback–Leibler divergence between PoS trigram distributions for delexicalized parser transfer. In order to approximate the divergence in syntactic structures between languages, Ponti et al. (2018a) utilize the Jaccard distance between morphological feature sets and the tree edit distance of delexicalized dependency parses of translationally equivalent sentences. A priori and bottom–up approaches can also be combined. For delexicalized parser transfer, Agi´c (2017) relies on a weighted sum of distances based on (1) the PoS divergence defined by Rosa and Zabokrtsky (2015); (2) th"
J19-3005,P15-1173,0,0.0316864,"Missing"
J19-3005,P18-1084,1,0.891535,"Missing"
J19-3005,C14-1098,0,0.0637796,"Missing"
J19-3005,D12-1131,1,0.808979,"Missing"
J19-3005,S10-1027,0,0.0116844,"s-lingual information about frame semantics can be extracted, for example, from the Valency Patterns Leipzig database (ValPaL). Typological information regarding lexical semantics patterns can further assist various NLP tasks by providing information about translationally equivalent words across languages. Such information is provided in databases such as the World Loanword Database (WOLD), the Intercontinental Dictionary Series (IDS), and the Automated Similarity Judgment Program (ASJP). One example task is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, a"
J19-3005,P08-1084,0,0.26666,"ure design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can"
J19-3005,P11-2120,0,0.0488046,"hand, the accuracy of PoS-based metrics deteriorates easily in scenarios with scarce amounts of data. Source language selection is a special case of source language weighting where weights are one-hot vectors. However, weights can also be gradient and consist of real numbers. Søgaard and Wulff (2012) adapt delexicalized parsers by weighting every 584 Ponti et al. Modeling Language Variation and Universals training instance based on the inverse of the Hamming distance between typological (or genealogical) features in source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing"
J19-3005,C12-2115,0,0.139636,"from the practice of discarding features that are not discriminative, when they are identical for all the languages in the sample. Another group of studies used more comprehensive feature sets. The feature set of Daiber, Stanojevi´c, and Sima’an (2016) included not only WALS word order features but also nominal categories (e.g., “Conjunctions and Universal Quantifiers”) and nominal syntax (e.g., “Possessive Classification”). Berzak, Reichart, and Katz (2015) considered all features from WALS associated with morphosyntax and pruned out the redundant ones, resulting in a total of 119 features. Søgaard and Wulff (2012) utilized all the 571 Computational Linguistics Volume 45, Number 3 Figure 4 Feature sets used in a sample of typologically informed experiments for dependency parsing. The numbers refer to WALS ordering (Dryer and Haspelmath 2013). features in WALS with the exception of phonological features. Tsvetkov et al. (2016) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whet"
J19-3005,N13-1126,0,0.051696,"Missing"
J19-3005,N12-1052,0,0.116447,"Missing"
J19-3005,P15-1150,0,0.0238395,"Missing"
J19-3005,L16-1011,0,0.120366,"Missing"
J19-3005,W15-2137,0,0.099488,"Missing"
J19-3005,P12-1068,0,0.0507951,"d labor. Furthermore, the immense range of possible tasks and languages makes the aim of a complete coverage unrealistic. One solution to this problem explored by the research community abandons the use of annotated resources altogether and instead focuses on unsupervised learning. This class of methods infers probabilistic models of the observations given some latent variables. In other words, it unravels the hidden structures within unlabeled text data. Although these methods have been used extensively for multilingual applications (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011; Titov and Klementiev 2012, inter alia), their performance tends to lag behind the more linguistically informed supervised learning approaches (Täckström, McDonald, and Nivre 2013). Moreover, they have been rarely combined with typological knowledge. For these reasons, we do not review them in this section. Other promising ways to overcome data scarcity include transferring models or data from resource-rich to resource-poor languages (§ 3.1) or learning joint models from annotated examples in multiple languages (§ 3.2) in order to leverage language interdependencies. Early approaches of this kind have relied on univers"
J19-3005,N16-1161,0,0.0936611,"hn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig"
J19-3005,P16-1157,0,0.0178939,"§ 4.3). 3.3 Multilingual Representation Learning The multilingual algorithms reviewed in § 3.1 and § 3.2 are facilitated by dense realvalued vector representations of words, known as multilingual word embeddings. These can be learned from corpora and provide pivotal lexical features to several downstream NLP applications. In multilingual word embeddings, similar words (regardless of the actual language) obtain similar representations. Various methods to generate multilingual word embeddings have been developed. We follow the classification proposed by Ruder (2018), and we refer the reader to Upadhyay et al. (2016) for an empirical comparison. 567 Computational Linguistics Volume 45, Number 3 Monolingual mapping generates independent monolingual representations and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual appro"
J19-3005,P11-2084,1,0.843653,"Missing"
J19-3005,P15-2118,1,0.902813,"Missing"
J19-3005,P17-1006,1,0.822412,"Missing"
J19-3005,Q16-1035,0,0.0476954,"source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing when the source is chosen in a supervised way (performance on target development data) and in an unsupervised way (coverage of target PoS sequences). Rather than generating new synthetic data, Ponti et al. (2018a) leverage typological features to pre-process treebanks in order to reduce their variation in language transfer tasks. In particular, they adapt source trees to the typology of a target language with respect to several constructions in a rule-based fashion. For instance, relative clauses in Arabic (Afro–Asiatic) w"
J19-3005,Q17-1011,0,0.141548,"-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word"
J19-3005,D18-1215,0,0.0199401,"ltilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mr"
J19-3005,Q14-1005,0,0.0236961,"et al. (2005). In its original formulation, as illustrated in Figure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompati"
J19-3005,D14-1187,0,0.0145363,"text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-indepe"
J19-3005,W14-1613,0,0.0141525,"ns and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual approaches merge words with contexts of other languages and generate representations based on this mixed corpus. Substitutions are based on Wiktionary (Xiao and Guo 2014) or machine translation (Gouws and Søgaard 2015; Duong et al. 2016). Moreover, the mixed corpus can be produced by randomly shuffling words between aligned documents in two languages (Vuli´c and Moens 2015). Cross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014). Joint"
J19-3005,H01-1035,0,0.123357,"Missing"
J19-3005,I08-3008,0,0.0176835,"). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translat"
J19-3005,C16-1044,0,0.0230991,"Missing"
J19-3005,D15-1213,0,0.0647972,"Missing"
J19-3005,N16-1156,0,0.0228005,"Missing"
J19-3005,D12-1125,1,0.809705,"orical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translat"
J19-3005,P16-1133,0,0.0606511,"Missing"
J19-3005,D18-1022,1,0.844622,"om a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and co"
J19-3005,Q17-1024,0,\N,Missing
J19-3005,D18-1269,0,\N,Missing
K15-1010,W14-1603,1,0.812323,"s to bypass this annotation bottleneck by predicting language specific priors from typological information. Related Work Cross linguistic-transfer was extensively studied in SLA, Linguistics and Psychology (Odlin, 1989; Gass and Selinker, 1992; Jarvis and Pavlenko, 2007). Within this area of research, our work is most closely related to the Contrastive Analysis (CA) framework. Rooted in the comparative linThe current investigation is most closely related to studies that demonstrate that ESL signal can be used to infer pairwise similarities between native languages (Nagata and Whittaker, 2013; Berzak et al., 2014) and in particular, tie 95 the similarities to the typological characteristics of these languages (Berzak et al., 2014). Our work inverts the direction of this analysis by starting with typological features, and utilizing them to predict error patterns in ESL. We also show that the two approaches can be combined in a bootstrapping strategy by first inferring typological properties from automatically extracted morphosyntactic ESL features, and in turn, using these properties for prediction of language specific error distributions in ESL. 3 exemplified in table 1. In addition to concentrating on"
K15-1010,P11-1093,0,0.113033,"ts (Tetreault et al., 2013), including features derived from the CA framework (Wong and Dras, 2009). A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages (Swanson and Charniak, 2013; Swanson and Charniak, 2014). Our approach differs from this research direction by focusing on grammatical errors, and emphasizing prediction of language specific patterns rather than their identification. Previous work on grammatical error correction that examined determiner and preposition errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2014) incorporated native language specific priors in models that are otherwise trained on standard English text. Our work extends the native language tailored treatment of grammatical errors to a much larger set of error types. More importantly, this approach is limited by the availability of manual error annotations for the target language in order to obtain the required error counts. Our framework enables to bypass this annotation bottleneck by predicting language specific priors from typological information. Related Work Cross linguistic-transfer was extensively stud"
K15-1010,N09-1067,0,0.0671086,"Missing"
K15-1010,Q14-1033,0,0.0346907,", including features derived from the CA framework (Wong and Dras, 2009). A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages (Swanson and Charniak, 2013; Swanson and Charniak, 2014). Our approach differs from this research direction by focusing on grammatical errors, and emphasizing prediction of language specific patterns rather than their identification. Previous work on grammatical error correction that examined determiner and preposition errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2014) incorporated native language specific priors in models that are otherwise trained on standard English text. Our work extends the native language tailored treatment of grammatical errors to a much larger set of error types. More importantly, this approach is limited by the availability of manual error annotations for the target language in order to obtain the required error counts. Our framework enables to bypass this annotation bottleneck by predicting language specific priors from typological information. Related Work Cross linguistic-transfer was extensively studied in SLA, Linguistics and"
K15-1010,de-marneffe-etal-2006-generating,0,0.00803991,"Missing"
K15-1010,N13-1009,0,0.0480037,"ns for future work in section 7. 2 Computational work touching on crosslinguistic transfer was mainly conducted in relation to the Native Language Identification (NLI) task, in which the goal is to determine the native language of the author of an ESL text. Much of this work focuses on experimentation with different feature sets (Tetreault et al., 2013), including features derived from the CA framework (Wong and Dras, 2009). A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages (Swanson and Charniak, 2013; Swanson and Charniak, 2014). Our approach differs from this research direction by focusing on grammatical errors, and emphasizing prediction of language specific patterns rather than their identification. Previous work on grammatical error correction that examined determiner and preposition errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2014) incorporated native language specific priors in models that are otherwise trained on standard English text. Our work extends the native language tailored treatment of grammatical errors to a much larger set of error types. More importantly, thi"
K15-1010,E14-4033,0,0.0418762,"n 7. 2 Computational work touching on crosslinguistic transfer was mainly conducted in relation to the Native Language Identification (NLI) task, in which the goal is to determine the native language of the author of an ESL text. Much of this work focuses on experimentation with different feature sets (Tetreault et al., 2013), including features derived from the CA framework (Wong and Dras, 2009). A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages (Swanson and Charniak, 2013; Swanson and Charniak, 2014). Our approach differs from this research direction by focusing on grammatical errors, and emphasizing prediction of language specific patterns rather than their identification. Previous work on grammatical error correction that examined determiner and preposition errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2014) incorporated native language specific priors in models that are otherwise trained on standard English text. Our work extends the native language tailored treatment of grammatical errors to a much larger set of error types. More importantly, this approach is limited by the"
K15-1010,C10-1044,0,0.148113,"Missing"
K15-1010,W13-1706,0,0.0916345,"cross native languages. Section 5 presents the regression model for prediction of ESL error distributions. The bootstrapping framework which utilizes automatically inferred typological features is described in section 6. Finally, we present the conclusion and directions for future work in section 7. 2 Computational work touching on crosslinguistic transfer was mainly conducted in relation to the Native Language Identification (NLI) task, in which the goal is to determine the native language of the author of an ESL text. Much of this work focuses on experimentation with different feature sets (Tetreault et al., 2013), including features derived from the CA framework (Wong and Dras, 2009). A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages (Swanson and Charniak, 2013; Swanson and Charniak, 2014). Our approach differs from this research direction by focusing on grammatical errors, and emphasizing prediction of language specific patterns rather than their identification. Previous work on grammatical error correction that examined determiner and preposition errors (Rozovskaya and Roth, 2011;"
K15-1010,U09-1008,0,0.0468311,"ion of ESL error distributions. The bootstrapping framework which utilizes automatically inferred typological features is described in section 6. Finally, we present the conclusion and directions for future work in section 7. 2 Computational work touching on crosslinguistic transfer was mainly conducted in relation to the Native Language Identification (NLI) task, in which the goal is to determine the native language of the author of an ESL text. Much of this work focuses on experimentation with different feature sets (Tetreault et al., 2013), including features derived from the CA framework (Wong and Dras, 2009). A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages (Swanson and Charniak, 2013; Swanson and Charniak, 2014). Our approach differs from this research direction by focusing on grammatical errors, and emphasizing prediction of language specific patterns rather than their identification. Previous work on grammatical error correction that examined determiner and preposition errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2014) incorporated native language specific priors"
K15-1010,P11-1019,0,0.101115,"udy. First, as our focus is on structural features that can be expressed in written form, we discard all the features associated with the categories Phonology, Lexicon4 , Sign Languages and Other. We further discard 24 features which either have a documented value for only one language, or have the same value in all the languages. The resulting feature-set contains 119 features, with an average of 2.9 values per feature, and 92.6 documented features per language. Data 3.1 Typological Database ESL Corpus We obtain ESL essays from the Cambridge First Certificate in English (FCE) learner corpus (Yannakoudakis et al., 2011), a publicly available subset of the Cambridge Learner Corpus (CLC)1 . The corpus contains upper-intermediate level essays by native speakers of 16 languages2 . Discarding Swedish and Dutch, which have only 16 documents combined, we take into consideration the remaining following 14 languages, with the corresponding number of documents in parenthesis: Catalan (64), Chinese (66), French (146), German (69), Greek (74), Italian (76), Japanese (82), Korean (86), Polish (76), Portuguese (68), Russian (83), Spanish (200), Thai (63) and Turkish (75). The resulting dataset contains 1228 documents with"
K15-1026,D13-1167,0,0.0182713,"tation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by"
K15-1026,N09-1003,0,0.646382,"f-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models that go beyond the bag-ofwords assumption. Similarity vs. Association Most recent VSM research does not distinguish between association and similarity in a principled way, although notable exceptions exist. Turney (2012) constructed two VSMs with the explicit goal of capturing either similarity or association. A classifier that uses the output of these models was able to predict whether two concepts are associated, similar or both. Agirre et al. (2009) partitioned the wordsim353 dataset into two subsets, one focused on similarity and the other on association. They demonstrated the importance of the association/similarity distinction by showing that some VSMs perform relatively well on one subset while others perform comparatively better on the other. Recently, Hill et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset"
K15-1026,P13-1174,0,0.0338768,"Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Table 1: The six most frequent pattern candidates that co"
K15-1026,P14-1023,0,0.21428,"Missing"
K15-1026,S13-1005,0,0.0664835,"Missing"
K15-1026,N15-1100,0,0.342972,"d similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by defining an SP templa"
K15-1026,D14-1162,0,0.108133,"Abstract has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little infor"
K15-1026,C92-2082,0,0.395839,"l et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used the"
K15-1026,J15-4004,1,0.60606,"Missing"
K15-1026,P08-1119,0,0.0131529,"onymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language"
K15-1026,E14-1051,0,0.0131748,"two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T X log p(wt |wt−c , . . . , wt−1 , wt+1 , . . . , wt+c ) max t=1 where"
K15-1026,P14-2050,0,0.364695,"−c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN"
K15-1026,P14-2086,0,0.150344,"Missing"
K15-1026,D13-1193,1,0.834039,"approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand cra"
K15-1026,C14-1153,1,0.851528,"l tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Table 1: The six most frequent pattern candidates that contain exactly two wildcards and 1-3 words in our corpus. terns inclu"
K15-1026,N03-1033,0,0.0542377,"re 7 and 10. 6 www.cl.cam.ac.uk/˜fh295/simlex.html 7 code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (ρ) between the ranking derived from m’s scores and the one derived from the human scores. Model GloVe BOW CBOW Dep NNSE skip-gram Spearman’s ρ 0.35 0.423 0.43 0.436 0.455 0.462 SP(−) SP(+) 0.434 0.517 Joint (SP"
K15-1026,N13-1090,0,0.67305,"oiri@ie.technion.ac.il Abstract has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Parti"
K15-1026,C08-1114,0,0.0274924,"that cooccur in SPs are semantically similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In thi"
K15-1026,J13-3004,0,0.0208148,"lly similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2"
K15-1026,C12-1118,0,0.122551,"t=1 where T is the number of words in the corpus, and c is a pre-determined window size. Another word2vec architecture, skip-gram, aims to predict the past and future context given a word. Its objective is: T X X log p(wt+j |wt ) max t=1 −c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is"
K15-1026,nivre-etal-2006-maltparser,0,0.00942273,"tml 7 code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (ρ) between the ranking derived from m’s scores and the one derived from the human scores. Model GloVe BOW CBOW Dep NNSE skip-gram Spearman’s ρ 0.35 0.423 0.43 0.436 0.455 0.462 SP(−) SP(+) 0.434 0.517 Joint (SP(+) , skip-gram) Average Human Score 0.563"
K15-1026,C02-1114,0,0.129662,"ty, we propose an alternative, pattern-based, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space word representation. Our representation is constructed in the following way (Section 3). For each word w, we construct a vector v of size V , where V is the size of the lexicon. Each element in v represents the cooccurrence in SPs of w with another word in the lexicon, which results in a sparse word representation. Unlike most previous works that applied SPs to NLP tasks, we do not use a har"
K15-1026,D12-1111,0,0.166728,"that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. Thi"
K15-1026,C10-2028,1,\N,Missing
K15-1026,P06-1038,1,\N,Missing
K15-1026,P99-1008,0,\N,Missing
K17-1013,W13-3520,0,0.0499698,"eled conjll. If both are used, the label is simply conj=conjlr+conjll.7 Consequently, the individual context bags we use in all experiments are: subj, obj, comp, nummod, appos, nmod, acl, amod, prep, adv, compound, conjlr, conjll. 4.2 Training and Evaluation We run the algorithm for context configuration selection only once, with the SGNS training setup described below. Our main evaluation setup is presented below, but the learned configurations are tested in additional setups, detailed in Sect. 5. Training Data Our training corpus is the cleaned and tokenised English Polyglot Wikipedia data (Al-Rfou et al., 2013),8 consisting of approxi7 Given the coordination structure boys and girls, conjlr training pairs are (boys, girls_conj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of t"
K17-1013,P14-2131,0,0.0737811,"v et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still th"
K17-1013,P14-1023,0,0.0288632,"alian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and ve"
K17-1013,C10-1011,0,0.0100926,"nj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of the UD treebank annotated with UPOS tags. The data were then parsed with UD using the graph-based Mate parser v3.61 (Bohnet, 2010)10 with standard settings on TRAIN + DEV of the UD treebank. Evaluation We experiment with the verb pair (222 pairs), adjective pair (111 pairs), and noun pair (666 pairs) portions of SimLex-999. We report Spearman’s ρ correlation between the ranks derived from the scores of the evaluated models and the human scores. Our evaluation setup is borrowed from Levy et al. (2015): we perform 2-fold cross-validation, where the context configurations are optimised on a development set, separate from the unseen test data. Unless stated otherwise, the reported scores are always the averages of the 2 runs"
K17-1013,D14-1082,0,0.0395504,"at the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, rea"
K17-1013,W14-1618,0,0.0925589,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,P06-1038,1,0.647241,"sequential position of each context word. Given the example from Fig. 1, POSIT with the window size 2 extracts the following contexts for discovers: Australian_-2, scientist_-1, stars_+2, with_+1. - DEPS-All: All dependency links without any context selection, extracted from dependency-parsed data with prepositional arc collapsing. - COORD: Coordination-based contexts are used as fast lightweight contexts for improved representations of adjectives and verbs (Schwartz et al., 2016). This is in fact the conjlr context bag, a subset of DEPS-All. - SP: Contexts based on symmetric patterns (SPs, (Davidov and Rappoport, 2006; Schwartz et al., 2015)). For example, if the word X and the word 9 10 Context Group Adj Verb Noun conjlr (A+N+V) obj (N+V) prep (N+V) amod (A+N) compound (N) adv (V) nummod (-) 0.415 -0.028 0.188 0.479 -0.124 0.197 -0.142 0.281 0.309 0.344 0.058 -0.019 0.342 -0.065 0.401 0.390 0.387 0.398 0.416 0.104 0.029 Table 1: 2-fold cross-validation results for an illustrative selection of individual context bags. Results are presented for the noun, verb and adjective subsets of SimLex-999. Values in parentheses denote the class-specific initial pools to which each context is selected based on its ρ sc"
K17-1013,de-marneffe-etal-2014-universal,0,0.0293418,"Missing"
K17-1013,W08-1301,0,0.125848,"Missing"
K17-1013,N15-1184,0,0.0343743,"searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scopre stelle con telescopio nmod amod Australian nsubj scientist dobj discovers case stars with telescope prep:with Figure 1: Ext"
K17-1013,C12-1059,0,0.0242772,"ed in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS configuration is outscored by SP, but the margin is negligible. We als"
K17-1013,D15-1242,0,0.0909931,"bs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case do"
K17-1013,Q15-1016,0,0.665137,"trate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, S"
K17-1013,N15-1142,0,0.0798687,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,D15-1161,0,0.115421,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,P15-1145,0,0.038685,"ore useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Sci"
K17-1013,P13-2109,0,0.0606688,"Missing"
K17-1013,P13-2017,0,0.0177128,"rch algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based configuration space can be developed for multiple languages based on the same design principles. Indeed, in this work we show that the optimal configurations for English translate to improved representations in two additional languages, German and Italian. And so, given a (UD-)parsed training corpus, for each target word w with modifiers m1 , . . . , mk and a head h, the word w is paired with context elements m1 _r1 , . . . , mk"
K17-1013,N15-1050,0,0.0247132,"its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as contexts for V and A si"
K17-1013,N16-1118,0,0.353877,"ill considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when l"
K17-1013,P14-2050,0,0.0802645,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,Q17-1022,1,0.874088,"Missing"
K17-1013,J07-2002,0,0.313892,"Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended"
K17-1013,D14-1162,0,0.0756258,"figurations across languages: these configurations improve the SGNS performance when trained with German or Italian corpora and evaluated on class-specific subsets of the multilingual SimLex-999 (Leviant and Reichart, 2015), without any language-specific tuning. 2 Related Work Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic"
K17-1013,petrov-etal-2012-universal,0,0.227836,"path of the best-scoring lower-level configuration even if its score is lower than that of its origin. As we do not observe any significant improvement with this variant, we opt for the faster and simpler one. 5 https://bitbucket.org/yoavgo/word2vecf 6 SGNS for all models was trained using stochastic gradient descent and standard settings: 15 negative samples, global learning rate: 0.025, subsampling rate: 1e − 4, 15 epochs. Universal Dependencies as Labels The adopted UD scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the universal POS tagset (Petrov et al., 2012). It is straightforward to “translate” previous annotation schemes to UD (de Marneffe et al., 2014). Providing a consistently annotated inventory of categories for similar syntactic constructions across languages, the UD scheme facilitates representation learning in languages other than English, as shown in (Vuli´c and Korhonen, 2016; Vuli´c, 2017). Individual Context Bags Standard post-parsing steps are performed in order to obtain an initial list of individual context bags for our algorithm: (1) Prepositional arcs are collapsed ((Levy and Goldberg, 2014a; Vuli´c and Korhonen, 2016), see Fig."
K17-1013,P93-1034,0,0.74216,"Missing"
K17-1013,K15-1026,1,0.948135,"for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In thi"
K17-1013,N16-1060,1,0.312769,"). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In this work, we propose a simple yet effective framework for selecting context configurations, which yields improved representations for verbs, adjectives, and nouns. We s"
K17-1013,N03-1033,0,0.0161394,"ining Setup We first test whether the context configurations learned in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS co"
K17-1013,P10-1040,0,0.0566914,"ning time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not"
K17-1013,Q14-1020,0,0.0199341,"ified by similar adjectives”. In another example, “two verbs are similar if they are used as predicates of similar nominal subjects” (the nsubj and nsubjpass dependency relations). First, we have to define an expressive context configuration space that contains potential training configurations and is effectively decomposed so that useful configurations may be sought algorithmically. We can then continue by designing a search algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based conf"
K17-1013,E17-2065,1,0.823839,"Missing"
K17-1013,P16-2084,1,0.761929,"Missing"
K17-1013,Q15-1025,0,0.0546541,"dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope n"
K17-1013,D12-1086,0,0.0288956,"epresent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as"
K17-1013,P14-2089,0,0.0430435,"ctures, a particular dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers"
K17-1040,P07-1033,0,0.0835581,"Missing"
K17-1040,P07-1056,0,0.105432,"utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works. Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012). These models are believed to extract features that are robust to crossdomain variations. However, while excelling on benchmark domain adaptation tasks such as crossdomain product sentiment classification (Blitzer et al., 2007), the reasons to this success are not entirely understood. In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (Blitzer et al., 2006, 2007). Following the auxiliary problems approach to semi-supervised learning (Ando and Zhang, 2005), this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task. Non-pivot features from different domains wh"
K17-1040,W06-1615,0,0.320022,"pproaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012). These models are believed to extract features that are robust to crossdomain variations. However, while excelling on benchmark domain adaptation tasks such as crossdomain product sentiment classification (Blitzer et al., 2007), the reasons to this success are not entirely understood. In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (Blitzer et al., 2006, 2007). Following the auxiliary problems approach to semi-supervised learning (Ando and Zhang, 2005), this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between the domains. Elegant and well motivated as it may be, SCL does not form the state-of-the-art since the neural approaches took over. In"
K17-1040,P15-1071,0,0.345261,"e final compact representations. The SVD derived matrix serves as a transformation matrix which maps feature vectors in the original space into a low-dimensional real-valued feature space. Numerous works have employed the SCL method in particular and the concept of pivot features for domain adaptation in general. A prominent method is spectral feature alignment (SFA, (Pan et al., 2010)). This method aims to align domain-specific (non-pivot) features from different domains into unified clusters, with the help of domain-independent (pivot) features as a bridge. Recently, Gouws et al. (2012) and Bollegala et al. (2015) implemented ideas related to those described here within an NN for cross-domain sentiment classification. For example, the latter work trained a word embedding model so that for every document, regardless of its domain, pivots are good predictors of non-pivots, and the pivots’ embeddings are similar across domains. Yu and Jiang (2016) presented a convolutional NN that learns sentence embeddings using two auxiliary tasks (whether the sentence contains a positive or a negative domain independent sentiment word), purposely avoiding prediction with respect to a large set of pivot features. In con"
K17-1040,P11-1014,0,0.0631801,"Adaptation Yftah Ziser and Roi Reichart Faculty of Industrial Engineering and Management, Technion, IIT syftah@campus.technion.ac.il, roiri@ie.technion.ac.il Abstract can perform properly on data from other domains, is therefore recognized as a fundamental challenge in NLP. Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch¨utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works. Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012). These models are believed to extract features that are robust to crossdomain variations. However, while excelling on benchmark domain adaptation tasks such as crossdomain product sentiment classification (Blitzer et al., 2007), the reasons to this success are not entirely understood. In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sent"
K17-1040,W04-3237,0,0.143878,"of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features. In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets. In Section 4 we discuss this issue in the context of sentiment classification. For representation learning, SCL employs the pivot features in order to learn mappings from the original feature space of both domains to a shared, Background and Contribution Domain adaptation is a fundamental, long standing problem in NLP (e.g. (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Daume III and Marcu, 2006)). The challenge stems from the fact that data in the source and the target domains are often distributed differently, making it hard for a model trained in the source domain to make valuable predictions in the target domain. Domain adaptation has various setups, differing with respect to the amounts of labeled and unlabeled data available in the source and target do401 are typically trained to minimize a reconstruction error loss(x, r(x)). Example loss functions are the squared error, the Kullback-Leibler (KL) divergence and the cross entropy of elements of x and e"
K17-1040,P07-1034,0,0.0894155,"e Learning for Domain Adaptation Yftah Ziser and Roi Reichart Faculty of Industrial Engineering and Management, Technion, IIT syftah@campus.technion.ac.il, roiri@ie.technion.ac.il Abstract can perform properly on data from other domains, is therefore recognized as a fundamental challenge in NLP. Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch¨utze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works. Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012). These models are believed to extract features that are robust to crossdomain variations. However, while excelling on benchmark domain adaptation tasks such as crossdomain product sentiment classification (Blitzer et al., 2007), the reasons to this success are not entirely understood. In the pre-NN era, a prominent approach to domain adaptation in NLP,"
K17-1040,P16-2005,0,0.227031,"model is trained to minimize a denoising reconstruction error loss(x, r(˜ x)). SDA for crossdomain sentiment classification was implemented by Glorot et al. (2011). Later, Chen et al. (2012) proposed the marginalized SDA (MSDA) model that is more computationally efficient and scalable to high-dimensional feature spaces than SDA. Marginalization of denoising autoencoders has gained interest since MSDA was presented. Yang and Eisenstein (2014) showed how to improve efficiency further by exploiting noising functions designed for structured feature spaces, which are common in NLP. More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al. (2016). There is a recent interest in models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014), for example the variational fair autoencoder model (Louizos et al., 2016), for domain adaptation. However, these models are still not competitive with MSDA on the tasks we consider here. low-dimensional, real-valued feature space. This is done by training classifiers whose input consists of the non-pivot features of an input example and their bina"
K17-1040,D16-1023,0,0.207701,"ature alignment (SFA, (Pan et al., 2010)). This method aims to align domain-specific (non-pivot) features from different domains into unified clusters, with the help of domain-independent (pivot) features as a bridge. Recently, Gouws et al. (2012) and Bollegala et al. (2015) implemented ideas related to those described here within an NN for cross-domain sentiment classification. For example, the latter work trained a word embedding model so that for every document, regardless of its domain, pivots are good predictors of non-pivots, and the pivots’ embeddings are similar across domains. Yu and Jiang (2016) presented a convolutional NN that learns sentence embeddings using two auxiliary tasks (whether the sentence contains a positive or a negative domain independent sentiment word), purposely avoiding prediction with respect to a large set of pivot features. In contrast to these works our model can learn useful cross-domain representations for any type of input example and in our cross-domain sentiment classification experiments it learns document level embeddings. That is, unlike Bollegala et al. (2015) we do not learn word embeddings and unlike Yu and Jiang (2016) we are not restricted to inpu"
K17-1040,N10-1004,0,0.0571129,"Missing"
K17-1040,P07-1078,1,0.373762,"Missing"
K17-1040,N03-1027,0,0.420544,"target domains to the set of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features. In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets. In Section 4 we discuss this issue in the context of sentiment classification. For representation learning, SCL employs the pivot features in order to learn mappings from the original feature space of both domains to a shared, Background and Contribution Domain adaptation is a fundamental, long standing problem in NLP (e.g. (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Daume III and Marcu, 2006)). The challenge stems from the fact that data in the source and the target domains are often distributed differently, making it hard for a model trained in the source domain to make valuable predictions in the target domain. Domain adaptation has various setups, differing with respect to the amounts of labeled and unlabeled data available in the source and target do401 are typically trained to minimize a reconstruction error loss(x, r(x)). Example loss functions are the squared error, the Kullback-Leibler (KL) divergence and the cross entrop"
K17-1040,D12-1131,1,0.65988,"Missing"
K17-1040,I13-1023,0,0.116956,"Missing"
K17-1040,P14-2088,0,0.383797,"oder called Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008)). In a denoising autoencoder (DEA) the input vector x is stochastically corrupted into a vector x ˜, and the model is trained to minimize a denoising reconstruction error loss(x, r(˜ x)). SDA for crossdomain sentiment classification was implemented by Glorot et al. (2011). Later, Chen et al. (2012) proposed the marginalized SDA (MSDA) model that is more computationally efficient and scalable to high-dimensional feature spaces than SDA. Marginalization of denoising autoencoders has gained interest since MSDA was presented. Yang and Eisenstein (2014) showed how to improve efficiency further by exploiting noising functions designed for structured feature spaces, which are common in NLP. More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al. (2016). There is a recent interest in models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014), for example the variational fair autoencoder model (Louizos et al., 2016), for domain adaptation. However, these models are still not competitive with MSDA on the tasks we"
K19-1021,E17-1088,0,0.0541429,"data). Following that, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of e"
K19-1021,Q16-1022,0,0.0419405,"Missing"
K19-1021,P17-2093,0,0.0157947,"at, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of evaluation. Note that"
K19-1021,E17-2040,0,0.0287154,"Missing"
K19-1021,C18-1139,0,0.0675297,"Missing"
K19-1021,E17-2067,0,0.0188003,"word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representa"
K19-1021,Q17-1010,0,0.661817,"tity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of"
K19-1021,D18-1366,0,0.0341798,"translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more"
K19-1021,D17-1078,0,0.0241869,"ecific Data: Task Data. The maximum number of training instances for all languages is again provided in Table 1. As before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keepi"
K19-1021,N18-2085,0,0.0176097,"ithout any available data (Kornai, 2013; Ponti et al., 2018) is a challenge left for future work. (Selection of) Languages. Both sources of data scarcity potentially manifest in degraded task performance for low-resource languages: our goal is to analyze the extent to which these factors affect downstream tasks across morphologically diverse language types that naturally come with varying data sizes to train their respective embeddings and task-based models. Our selection of test languages is therefore guided by the following goals: a) following recent initiatives (e.g. in language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure coverage of different genealogical and typological properties; b) we aim to cover low-resource languages with varying amounts of available WE data and task-specific data. We select 16 languages in total spanning 4 broad In what follows, we further motivate our work by analyzing two different sources of data scarcity: 217 2 For instance, as of April 2019, Wikipedia is available only in 304 out of the estimated 7,000 existing languages. Agglutinative EMB FGET NER MTAG BERT Fusional Introflexive Isolating BM BXR MYV TE TR ZU EN FO GA GOT MT RUE AM HE YO ZH 4"
K19-1021,N15-1140,0,0.04425,"Missing"
K19-1021,Q18-1003,0,0.0380649,"Missing"
K19-1021,D18-1029,1,0.899767,"Missing"
K19-1021,L18-1550,0,0.0288291,"ubword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. In"
K19-1021,L18-1473,1,0.706967,". Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e."
K19-1021,P82-1020,0,0.744642,"Missing"
K19-1021,C18-1216,0,0.0212074,"its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource set"
K19-1021,P18-1007,0,0.0258865,"t al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in"
K19-1021,N16-1030,0,0.0378679,"d-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data spars"
K19-1021,N19-1423,0,0.646923,"at the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the le"
K19-1021,D18-1549,0,0.0214746,"ning the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on variou"
K19-1021,N19-1154,0,0.0223364,"nd taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-in"
K19-1021,P13-1149,0,0.0307702,"leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by p"
K19-1021,E14-2006,0,0.124496,"r (iii) both, we analyse how different data regimes affect the final task performance. 2) We experiment with 16 languages representing 4 diverse morphological types, with a focus on truly low-resource languages such as Zulu, Rusyn, Buryat, or Bambara. 3) We experiment with a variety of subword-informed representation architectures, where the focus is on unsupervised, widely portable language-agnostic methods such as the ones based on character n-grams (Luong and Manning, 2016; Bojanowski et al., 2017), Byte Pair Encodings (BPE) (Sennrich et al., 2016; Heinzerling and Strube, 2018), Morfessor (Smit et al., 2014), or BERT-style pretraining and fine-tuning (Devlin et al., 2019) which relies on WordPieces (Wu et al., 2016). We demonstrate that by tuning subword-informed models in low-resource settings we can obtain substantial gains over subwordagnostic models such as skip-gram with negative sampling (Mikolov et al., 2013) across the board. The main goal of this study is to identify viable and effective subword-informed approaches for truly low-resource languages and offer modeling guidance in relation to the target task, the language at hand, and the (un)availability of general and/or task-specific tra"
K19-1021,W18-1205,0,0.0212108,"e word itself can be appended to the subword sequence and embedded into the subword space in order to incorporate word-level information (Bojanowski et al., 2017). To encode subword order, s can be further enriched by a trainable position embedding p. We use addition to combine subword and position embeddings, namely s := s + p, which has become the de-facto standard method to encode positional information (Gehring et al., 2017; Vaswani et al., 2017; Devlin et al., 2019). Finally, the subword embedding sequence is passed to a composition function, which computes the final word representation. Li et al. (2018) and Zhu et al. (2019) have empirically verified that composition by simple addition, among other more complex composition functions, is a robust choice. Therefore, we use addition in all our experiments. Similar to Bojanowski et al. (2017); Zhu et al. (2019), we adopt skip-gram with negative sampling 218 Component Segmentation Option Morfessor BPE char n-gram Label morf bpeX charn Word token exclusion inclusion ww+ Position embedding exclusion additive pp+ addition add Composition function resentations, and can benefit from the information. Table 2: Components for constructing subwordinformed"
K19-1021,P16-1100,0,0.0620352,"Missing"
K19-1021,P17-1178,0,0.10163,"before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keeping hyper-parameters fixed across different data points will possibly result in underfitting for larger d"
K19-1021,N18-1202,0,0.0352715,"representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the me"
K19-1021,D18-1169,0,0.130939,"a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words"
K19-1021,D17-1010,0,0.0259855,"2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowle"
K19-1021,P16-2067,0,0.096051,"Missing"
K19-1021,P16-1162,0,0.314176,"well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018)"
K19-1021,P17-1184,0,0.0193316,"ed from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused o"
K19-1021,D15-1083,0,0.0434509,"Missing"
K19-1021,C18-1153,0,0.0422342,"Missing"
K19-1021,D18-1059,0,0.0518042,"Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource setups. Our study centers on the following"
K19-1021,N19-1097,1,0.876896,"Missing"
N12-1008,D09-1014,0,0.0243549,"rds in a document with event boundaries based on the local surroundings of a candidate boundary. The resulting maximum aposteriori problem is: 72 θf (rf ) f ∈F where θf are the potential functions and {rf |f ⊆ {1, . . . , n}, f ∈ F } is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these components are given by the likelih"
N12-1008,P11-1098,0,0.0486312,"h decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task. Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck an"
N12-1008,P07-1036,0,0.273391,"Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task. Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck and McCallum, 2010). Constraints have been explored both at sentence and document level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. In our work we focus on document-level constraints. We utilize both discourse and record-coherence constraints to encourage consistency between local sequence models. There has also be"
N12-1008,P05-1022,0,0.0861833,"Missing"
N12-1008,P11-1054,1,0.840695,"cument level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. In our work we focus on document-level constraints. We utilize both discourse and record-coherence constraints to encourage consistency between local sequence models. There has also been unsupervised work that demonstrates the benefit of domain-specific constraints (Chen et al., 2011). In our work we show that domain-specific constraints based on the common structure of newspaper articles are also useful to guide a supervised model. 3 Model M AP (θ) = Problem Formulation Given a document, our goal is to extract field values and aggregate them into event records. The training data consists of event annotations where each word in the document is tagged with a field and with an event id. If a word is not a filler for a field, it is annotated with a default NULL field value. At test time, the number of events is not given and has to be inferred from the data. Model Structure O"
N12-1008,P03-1028,0,0.0218479,"ext of template extraction and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. T"
N12-1008,J93-3001,0,0.184149,"Missing"
N12-1008,M92-1002,0,0.660563,"Missing"
N12-1008,W02-1001,0,0.0174696,"Missing"
N12-1008,N09-1037,0,0.0103682,"ng Potentials associate words in a document with event boundaries based on the local surroundings of a candidate boundary. The resulting maximum aposteriori problem is: 72 θf (rf ) f ∈F where θf are the potential functions and {rf |f ⊆ {1, . . . , n}, f ∈ F } is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these compone"
N12-1008,P05-1045,0,0.0576092,"Missing"
N12-1008,H92-1045,0,0.0173255,"l, thereby enabling multiple variable values for multi-event documents. The second record coherence potential — Record Density Potential — aims to reduce empty fields in the event record. This potential turns on when a local extractor fails to identify a filler for a field when processing a given event segment. If this segment contains words that are labeled as potential fillers in the context of other events in the training data, we prefer assignments that associate them with the field that otherwise would have been empty. This potential is inspired by the one sense per discourse constraint (Gale et al., 1992) that associates all the occurrences of the word in a document with the same semantic meaning. 1 The potential is defined for the following fields: Terrorist Organization, Weapon, City, and Country. 4 Inference Dual Decomposition The global potentials encode important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi , fj ∈ F , had been empty, we could have found the MAP assignment by solvin"
N12-1008,I11-1081,0,0.0422086,"Missing"
N12-1008,D10-1125,0,0.026922,"e important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi , fj ∈ F , had been empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as required by the infe"
N12-1008,P07-1075,0,0.0108687,"their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a line"
N12-1008,D07-1075,0,0.0737565,"Missing"
N12-1008,D09-1016,0,0.134713,"IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task"
N12-1008,M92-1008,0,0.0727864,"Missing"
N12-1008,D11-1001,0,0.0324077,"empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as required by the inference algorithm: M AP (θ) = max y,z X θj (rj ) + j∈J X X X min L(δ), L(δ) = δf j (rj )]+ max[θj (rj ) + r δ j j∈J f :j∈f X X δf j (rj )] max[θf (rf ) − rf rj f :j∈f end ← T RU E for f ∈ F do X δf j (rj )] rpkf = arg max[θf (rf ) − rf j∈f for j ∈ f do if rljk 6= rpkf j then gfkj (rljk ) + = 1 gfk"
N12-1008,W04-2401,0,0.203758,"Missing"
N12-1008,D10-1001,0,0.0460839,"al potentials encode important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi , fj ∈ F , had been empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as r"
N12-1008,C04-1078,0,0.0183377,"action and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the jo"
N12-1008,D10-1099,0,0.0208222,"boundaries based on the local surroundings of a candidate boundary. The resulting maximum aposteriori problem is: 72 θf (rf ) f ∈F where θf are the potential functions and {rf |f ⊆ {1, . . . , n}, f ∈ F } is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these components are given by the likelihoods of the corresp"
N12-1008,M92-1023,0,\N,Missing
N13-1113,J96-1002,0,0.0249814,"Missing"
N13-1113,P07-1036,0,0.490984,"lude the presumed DNA adduct of BA and might thus be slightly overestimated. The verb “calculated” usually indicates the “Method” class, but, when accompanied by the modal verb “might”, it is more likely to imply that authors are interpreting their own results (i.e. the “Conclusion” class in AZ). This can be explicitly encoded in the model through a target distribution for sentences containing certain modal verbs. Recent work has shown that explicit declaration of domain and expert knowledge can be highly useful for structured NLP tasks such as parsing, POS tagging and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Ganchev et al., 2010). These works have encoded expert knowledge through constraints, with different frameworks differing in the type of constraints and the inference and learning algorithms used. We build on the Generalized Expectation (GE) framework (Mann and McCallum, 2007) which encodes expert knowledge through a preference (i.e. soft) constraints for parameter settings for which the predicted label distribution matches a target distribution. In order to integrate domain knowledge with a features-based model, we develop a simple taxonomy of constraints (i.e. desi"
N13-1113,C12-1041,1,0.945708,"ul even when no labeled data is available. 1 Introduction Techniques that enable automatic analysis of the information structure of scientific articles can help scientists identify information of interest in the growing volume of scientific literature. For example, classification of sentences according to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18"
N13-1113,P07-2009,0,0.220165,"Missing"
N13-1113,W10-1913,1,0.883021,"me that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76. We propose a novel approach to this task in which traditional feature-based models are augmented with explicit declarative expert and domain knowledge, and apply it to sentence-based AZ. We explore two sources of declarative knowledge for our task - discourse and lexical. One way to utilize discourse knowledge is to guide the model predictions"
N13-1113,D11-1025,1,0.868095,"Missing"
N13-1113,I08-1050,0,0.150893,"ve zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76. We propose a novel approach to this task in which traditional feature-based models are augmented with explicit declarative expert and domain knowledge, and apply it to sentence-based AZ. We explore two sources of declarative knowledge for our task - discourse and lexical. One way to utilize discourse know"
N13-1113,P06-1027,0,0.0287614,"DIFF FUT 16.9 74.8 0.5 4.0 16.9 17.9 0.6 1.4 68.9 1.5 22.3 5.9 0.2 12.1 63.5 0.8 0.1 0.1 2.4 2.8 13.2 0.2 2.1 1.1 34.8 5.4 97.5 11.7 0.7 4.3 0.1 0.2 1.1 13.3 0.2 0.7 Table 5: Performance of baselines on the Discussion section. BKG PROB METH RES CON CN Full supervision SVM .56 0 0 0 .84 MaxEnt .55 .08 0 0 .84 Light supervision with 150 labeled sentence SVM .26 0 0 0 .80 TSVM .25 .04 .04 .03 .33 MaxEnt .25 0 0 0 .80 MaxEnt+ER .23 0 0 0 .80 .35 .38 DIFF FUT 0 0 0 0 .05 0 14 .06 .10 0 .07 0 0 .02 0 0 ductive SVM (TSVM) and semi-supervised MaxEnt based on Entropy Regularization (ER) (Vapnik, 1998; Jiao et al., 2006). SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011)) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles. Parameter tuning The boundaries of the reference probabilities (ak and bk in Equation (8)) were defined and optimized on the development data which consists of one third of the corpus. We considered six types of boundaries: Fairly High for 1, High for [0.9,1), Medium High for [0.5,0.9), Medium Low for [0.1,0.5), Low for [0,0.1), and Fairly Low for 0. Evaluation We ev"
N13-1113,liakata-etal-2010-corpora,0,0.0380636,"Previous work Information structure analysis The information structure of scientific documents (e.g. journal articles, abstracts, essays) can be analyzed in terms of patterns of topics, functions or relations observed in multi-sentence scientific text. Computational approaches have mainly focused on analysis based on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009), discourse structure (Burstein et al., 2003; Webber et al., 2011), qualitative dimensions (Shatkay et al., 2008), scientific claims (Blake, 2009), scientific concepts (Liakata et al., 2010) and information status (Markert et al., 2012). Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga"
N13-1113,W06-3309,0,0.0827281,"ing to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76. We propose a novel approach to this task in which traditional feature-based models are augmented with explicit declarative expert and domain knowledge, and apply it to sentence-based AZ. We explore two sources of declarative knowledge for our task - discourse and lexical. One way to"
N13-1113,P08-1099,0,0.645249,"A adduct of BA and might thus be slightly overestimated. The verb “calculated” usually indicates the “Method” class, but, when accompanied by the modal verb “might”, it is more likely to imply that authors are interpreting their own results (i.e. the “Conclusion” class in AZ). This can be explicitly encoded in the model through a target distribution for sentences containing certain modal verbs. Recent work has shown that explicit declaration of domain and expert knowledge can be highly useful for structured NLP tasks such as parsing, POS tagging and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Ganchev et al., 2010). These works have encoded expert knowledge through constraints, with different frameworks differing in the type of constraints and the inference and learning algorithms used. We build on the Generalized Expectation (GE) framework (Mann and McCallum, 2007) which encodes expert knowledge through a preference (i.e. soft) constraints for parameter settings for which the predicted label distribution matches a target distribution. In order to integrate domain knowledge with a features-based model, we develop a simple taxonomy of constraints (i.e. desired class distributions)"
N13-1113,P12-1084,0,0.0883378,"Missing"
N13-1113,W09-3603,0,0.0236176,".2 0.2 2.1 1.1 34.8 5.4 97.5 11.7 0.7 4.3 0.1 0.2 1.1 13.3 0.2 0.7 Table 5: Performance of baselines on the Discussion section. BKG PROB METH RES CON CN Full supervision SVM .56 0 0 0 .84 MaxEnt .55 .08 0 0 .84 Light supervision with 150 labeled sentence SVM .26 0 0 0 .80 TSVM .25 .04 .04 .03 .33 MaxEnt .25 0 0 0 .80 MaxEnt+ER .23 0 0 0 .80 .35 .38 DIFF FUT 0 0 0 0 .05 0 14 .06 .10 0 .07 0 0 .02 0 0 ductive SVM (TSVM) and semi-supervised MaxEnt based on Entropy Regularization (ER) (Vapnik, 1998; Jiao et al., 2006). SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011)) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles. Parameter tuning The boundaries of the reference probabilities (ak and bk in Equation (8)) were defined and optimized on the development data which consists of one third of the corpus. We considered six types of boundaries: Fairly High for 1, High for [0.9,1), Medium High for [0.5,0.9), Medium Low for [0.1,0.5), Low for [0,0.1), and Fairly Low for 0. Evaluation We evaluated the precision, recall and F-score for each category, using a standard ten-fold cross-validat"
N13-1113,C12-2097,1,0.831417,"on status (Markert et al., 2012). Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga et al., 2012; Reichart and Korhonen, 2012). Unfortunately, these approaches do not reach the performance level of fully supervised models, let alone exceed it. Our novel method addresses this problem. Declarative knowledge and constraints Previous work has shown that incorporating declarative constraints into feature-based machine learning models works well in many NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2008; Bellare et al., 2009; Ganchev et al., 2010). Such constraints can be used in a semi-supervised or unsupervised fashion. For example, (Mann and McCallum, 2008) shows that using CRF in conjunction wit"
N13-1113,D09-1067,1,0.887348,"Missing"
N13-1113,J02-4002,0,0.828448,"tperforms lightly supervised feature-based models, showing that our approach can be useful even when no labeled data is available. 1 Introduction Techniques that enable automatic analysis of the information structure of scientific articles can help scientists identify information of interest in the growing volume of scientific literature. For example, classification of sentences according to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Li"
N13-1113,D09-1155,0,0.347904,"of existing fully and lightly supervised models. Even a fully unsupervised version of this model outperforms lightly supervised feature-based models, showing that our approach can be useful even when no labeled data is available. 1 Introduction Techniques that enable automatic analysis of the information structure of scientific articles can help scientists identify information of interest in the growing volume of scientific literature. For example, classification of sentences according to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performa"
N13-1113,varga-etal-2012-unsupervised,0,0.0937764,"2010) and information status (Markert et al., 2012). Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga et al., 2012; Reichart and Korhonen, 2012). Unfortunately, these approaches do not reach the performance level of fully supervised models, let alone exceed it. Our novel method addresses this problem. Declarative knowledge and constraints Previous work has shown that incorporating declarative constraints into feature-based machine learning models works well in many NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2008; Bellare et al., 2009; Ganchev et al., 2010). Such constraints can be used in a semi-supervised or unsupervised fashion. For example, (Mann and McCallum, 2008) shows tha"
N16-1060,D14-1034,1,0.809186,"djectives. A number of evaluation sets consisting of word pairs scored by humans for semantic relations (mostly association and similarity) are in use for VSM evaluation. These include: RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), WordSim353 (Finkelstein et al., 2001), MEN (Bruni 1 Coor ∪ CoorC = DepAll, Coor ∩ CoorC = ∅ et al., 2014) and SimLex999 (Hill et al., 2014).2 Nouns are dominant in almost all of these datasets. For example, RG-65, MC-30 and WordSim353 consist of noun pairs almost exclusively. A few datasets contain pairs of verbs (Yang and Powers, 2006; Baker et al., 2014). The MEN dataset, although dominated by nouns, also contains verbs and adjectives. Nonetheless, the human judgment scores in these datasets reflect relatedness between words. In contrast, the recent SimLex999 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the"
N16-1060,P14-1023,0,0.0791358,"word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extracted from plain text. 4 Experiments Model. We keep the VSM fixed throughout our experiments, changing only the context type. This methodology allows us to evaluate the impact of different contexts on the VSM performance, as context choice is the only modeling decision that changes across experimental conditions. Our VSM is the word2vec skip-gram model (w2vSG, Mikolov et al. (2013a)), which obtains state-ofthe-art results on a variety of NLP tasks (Baroni et al., 2014). We employ the word2vec toolkit.4 For all context types other than BOW we use the word2vec package of (Levy and Goldberg, 2014),5 which augments the standard word2vec toolkit with code that allows arbitrary context definition. Experimental Setup. We experiment with the verb pair (222 pairs) and adjective pair (111 pairs) portions of SimLex999 (Hill et al., 2014). We report the Spearman ρ correlation between the ranks derived from the scores of the evaluated models and the human scores provided in SimLex999.6 We train the w2v-SG model with five different context types: (a) BOW contexts (SG-BOW"
N16-1060,P06-1038,1,0.911956,"Missing"
N16-1060,P07-1030,1,0.829956,"Missing"
N16-1060,C10-2028,1,0.689434,"Missing"
N16-1060,P13-1174,0,0.0126494,"Missing"
N16-1060,C92-2082,0,0.0548764,"Missing"
N16-1060,J15-4004,1,0.897757,"Missing"
N16-1060,P08-1119,0,0.0717551,"Missing"
N16-1060,P14-2050,0,0.60595,"2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Me"
N16-1060,N15-1098,0,0.0104165,"n, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyponymy and entailment. Rubinstein et al. (2015) showed that while state-of-the-art embeddings are successful at capturing taxonomic information (e.g., cow is an animal), they are much less successful in capturing attributive properties (bananas are yellow). In (Schwartz et al., 2015), we showed that word embeddings are unable to distinguish between pairs of words with opposite meanings (antonyms, e.g., good/bad). In this paper we study the difficulties of bag-of-words based word embeddings in representing verb similarity."
N16-1060,N16-1118,0,0.0192828,"4), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyp"
N16-1060,N13-1090,0,0.150625,"pairs of verbs (Yang and Powers, 2006; Baker et al., 2014). The MEN dataset, although dominated by nouns, also contains verbs and adjectives. Nonetheless, the human judgment scores in these datasets reflect relatedness between words. In contrast, the recent SimLex999 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extract"
N16-1060,H05-1105,0,0.0834407,"Missing"
N16-1060,P06-1033,0,0.0393585,"n our corpus. Indeed, due to the significant overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP cont"
N16-1060,W09-3811,0,0.0137806,"based model of Schwartz et al. (2015), with (SRR15) and without (SRR15− ) its antonym detection method. The two rightmost columns present the run time of the w2v-SG models in minutes (Time) and the number of context instances used by the model (#Cont.).10 For each SimLex999 portion, the score of the best w2v-SG model across context types is highlighted in bold font. ated by the word2vec script.7 Models (b)-(d) require the dependency parse trees of the corpus as input. To generate these trees, we employ the Stanford POS Tagger (Toutanova et al., 2003)8 and the stack version of the MALT parser (Nivre et al., 2009).9 The SP contexts are generated using the SPs extracted by the DR 06 algorithm from our training corpus (see Section 3). For BOW contexts, we experiment with three window sizes (2, 5 and 10) and report the best results (window size of 2 across conditions). For dependency based contexts we follow the standard convention in the literature: we consider the immediate heads and modifiers of the represented word. All models are trained with 500 dimensions, the default value of the word2vec script. Other hyperparameters were also set to the default values of the code packages. Results. Table 1 prese"
N16-1060,J07-2002,0,0.121164,"9 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recen"
N16-1060,D14-1162,0,0.116226,"Missing"
N16-1060,P15-2119,1,0.320289,"ect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyponymy and entailment. Rubinstein et al. (2015) showed that while state-of-the-art embeddings are successful at capturing taxonomic information (e.g., cow is an animal), they are much less successful in capturing attributive properties (bananas are yellow). In (Schwartz et al., 2015), we showed that word embeddings are unable to distinguish between pairs of words with opposite meanings (antonyms, e.g., good/bad). In this paper we study the difficulties of bag-of-words based word embeddings in representing verb similarity. 3 Symmetric Patterns (SPs) Lexico-syntactic patterns are templates of text that contain both words and wildcards (Hears"
N16-1060,P11-1067,1,0.763494,"due to the significant overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extract"
N16-1060,C12-1147,1,0.173989,"overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extracted from plain text. 4 Ex"
N16-1060,D13-1193,1,0.55537,"Missing"
N16-1060,C14-1153,1,0.901785,"Missing"
N16-1060,K15-1026,1,0.754717,"ts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2"
N16-1060,N03-1033,0,0.0501082,"s (see text). The bottom lines present the results of the count SP-based model of Schwartz et al. (2015), with (SRR15) and without (SRR15− ) its antonym detection method. The two rightmost columns present the run time of the w2v-SG models in minutes (Time) and the number of context instances used by the model (#Cont.).10 For each SimLex999 portion, the score of the best w2v-SG model across context types is highlighted in bold font. ated by the word2vec script.7 Models (b)-(d) require the dependency parse trees of the corpus as input. To generate these trees, we employ the Stanford POS Tagger (Toutanova et al., 2003)8 and the stack version of the MALT parser (Nivre et al., 2009).9 The SP contexts are generated using the SPs extracted by the DR 06 algorithm from our training corpus (see Section 3). For BOW contexts, we experiment with three window sizes (2, 5 and 10) and report the best results (window size of 2 across conditions). For dependency based contexts we follow the standard convention in the literature: we consider the immediate heads and modifiers of the represented word. All models are trained with 500 dimensions, the default value of the word2vec script. Other hyperparameters were also set to"
N16-1060,J06-3003,0,0.0359954,"Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination"
N16-1060,C08-1114,0,0.0254482,"gs. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which"
N16-1060,C02-1114,0,0.196931,"Missing"
N16-1081,D08-1107,0,0.302327,"ocus on grammar and parsing of Web data in general and queries in particular. Syntactic Query Analysis Web queries differ from standard sentences in a number of aspects: they tend to be shorter, not to follow standard grammatical conventions, and to convey more information than can be directly inferred from their words. Consequently, a number of works addressed their syntactic analysis. Allan and Raghavan (2002) use part-of-speech (POS) tag patterns in order to manually map very short queries into clarification questions, which are then presented to the user to help them clarify their intent. Barr et al. (2008) trained POS taggers for Web queries and used a set of rules to map the resulting tagged queries into one of seven syntactic categories, whose merit is tested in the context of information retrieval tasks. Manshadi and Li (2009) and Li (2010) addressed the task of semantic tagging and structural analysis of Web queries, focusing on noun phrase queries. Bendersky et al. (2010) used the POS tags of the top-retrieved documents to enhance the initial POS tagging of query terms. Bendersky et al. (2011) proposed a joint framework for annotating queries with POS tags and phrase chunks. Ganchev et al."
N16-1081,P11-1011,0,0.648211,"into clarification questions, which are then presented to the user to help them clarify their intent. Barr et al. (2008) trained POS taggers for Web queries and used a set of rules to map the resulting tagged queries into one of seven syntactic categories, whose merit is tested in the context of information retrieval tasks. Manshadi and Li (2009) and Li (2010) addressed the task of semantic tagging and structural analysis of Web queries, focusing on noun phrase queries. Bendersky et al. (2010) used the POS tags of the top-retrieved documents to enhance the initial POS tagging of query terms. Bendersky et al. (2011) proposed a joint framework for annotating queries with POS tags and phrase chunks. Ganchev et al. (2012) trained a POS tagger on automatically tagged queries. The POS tags of the training queries are projected from sentences containing the query terms within Web pages retrieved for them. The retrieved sentences were POS tagged using an offthe-shelf tagger. These works, as opposed to ours, do not aim to produce a complete syntactic analysis of queries. Syntactic Parsing of Web Data To the best of our knowledge, only a handful of works have aimed at building syntactic parsers for Web data. Petr"
N16-1081,D07-1086,0,0.290799,"ogative sentence rooted in the word done and the second is a noun phrase which specifies the pronoun it from the first segment. Finally, query (c) consists of two segments, each a noun phrase, presumably connected by an is-made-of semantic relation. As in query (b), the segments of this query are syntactically independent, but unlike query (b), their semantic connection is more loose. The existence of such loose connections motivates us to exclude semantic subcategorization from the syntactic layer. We note that the notion of segment has another meaning, within the task of query segmentation (Bergsma and Wang, 2007; Guo et al., 2008; Tan and Peng, 2008; Mishra et al., 2011; Hagen et al., 2012). This task’s goal is to identify words in the query that together form compound concepts or phrases, like “Chicago Bulls”. As such, this task differs from ours as it defines segmentation in semantic rather than syntactic terms. We also note that query segments are distinct from the concept of fragments in constituency parsing. Marcus et al. (1993) introduce fragments in order to overcome problems involving the attachment point of various modifying phrases, e.g. in the sentence ‘In Asia, as [FRAG in Europe], a new"
N16-1081,D12-1133,0,0.0399309,"Missing"
N16-1081,D14-1082,0,0.0545477,"Missing"
N16-1081,P13-1104,0,0.12463,"a syntactic segment: an independent syntactic unit within a potentially larger syntactic structure. A query may include several segments, potentially related to each other semantically but lacking an explicit syntactic connection. Hence, query analysis consists of the query’s segments and their internal dependency structure, and may be complemented by the inter-segment semantic relationships. Therefore, we constructed a new query treebank consisting of 5,000 CQA queries, manually annotated according to our extended grammar. A comparison of direct application of an off-the-shelf parser (Clear (Choi and McCallum, 2013)) trained on edited text (OntoNotes 5 (Weischedel et al., 2013)) to a raw query with the application of the same parser to the gold-standard segments of that query is given in Fig. 1. Second, we develop two CQA query parsing algorithms that can adapt any given off-the-shelf dependency parser trained on standard edited text to produce syntactic structures that conform to the extended grammar. Both our algorithms employ distant supervision in the form of a training set consisting of millions of (query, title) pairs. The title is the title of the Yahoo Answers question page that was clicked by th"
N16-1081,P15-1038,0,0.0222465,"Missing"
N16-1081,W08-1301,0,0.0350268,"Missing"
N16-1081,N13-1037,0,0.0371951,"Missing"
N16-1081,P08-2056,0,0.083004,"Missing"
N16-1081,P12-2047,0,0.243481,"et al. (2008) trained POS taggers for Web queries and used a set of rules to map the resulting tagged queries into one of seven syntactic categories, whose merit is tested in the context of information retrieval tasks. Manshadi and Li (2009) and Li (2010) addressed the task of semantic tagging and structural analysis of Web queries, focusing on noun phrase queries. Bendersky et al. (2010) used the POS tags of the top-retrieved documents to enhance the initial POS tagging of query terms. Bendersky et al. (2011) proposed a joint framework for annotating queries with POS tags and phrase chunks. Ganchev et al. (2012) trained a POS tagger on automatically tagged queries. The POS tags of the training queries are projected from sentences containing the query terms within Web pages retrieved for them. The retrieved sentences were POS tagged using an offthe-shelf tagger. These works, as opposed to ours, do not aim to produce a complete syntactic analysis of queries. Syntactic Parsing of Web Data To the best of our knowledge, only a handful of works have aimed at building syntactic parsers for Web data. Petrov and McDonald (2012) conducted a shared task on parsing Web data from the Google Web Treebank, consisti"
N16-1081,Q13-1033,0,0.0645688,"Missing"
N16-1081,W13-3518,0,0.0349846,"Missing"
N16-1081,P03-1054,0,0.0759006,"Missing"
N16-1081,D14-1108,0,0.0444139,"mains. The participating systems relied mostly on existing domain adaptation techniques to adapt parsers trained on existing treebanks of edited text to the Web. Foster et al. (2011) took a similar approach for tweet parsing. Contrary to our approach, these works rely on existing grammatical frameworks, particularly phrase-structure and dependency grammars, and do not aim at adapting them to domains such as Web queries, where standard grammar does not properly describe the language. This may be the reason Web queries were not included in the shared task. A work that is more related to ours is Kong et al. (2014), who addressed the task of tweet parsing. Like us, they adapt the grammatical annotation scheme to the target linguistic domain and produce a multi-rooted syntactic structure. However, CQA queries and tweets exhibit different syntactic properties: (1) tweets often consist of multiple sentences, while CQA queries are concise in nature and usually correspond to a phrase, a fragment of a sentence, or several of these concatenated; and (2) queries are generated in order to retrieve information from the Web. Tweets, on the other hand, usually aim to convey a short message. These differences lead u"
N16-1081,P14-1130,0,0.0608884,"Missing"
N16-1081,P10-1136,0,0.167942,"onvey more information than can be directly inferred from their words. Consequently, a number of works addressed their syntactic analysis. Allan and Raghavan (2002) use part-of-speech (POS) tag patterns in order to manually map very short queries into clarification questions, which are then presented to the user to help them clarify their intent. Barr et al. (2008) trained POS taggers for Web queries and used a set of rules to map the resulting tagged queries into one of seven syntactic categories, whose merit is tested in the context of information retrieval tasks. Manshadi and Li (2009) and Li (2010) addressed the task of semantic tagging and structural analysis of Web queries, focusing on noun phrase queries. Bendersky et al. (2010) used the POS tags of the top-retrieved documents to enhance the initial POS tagging of query terms. Bendersky et al. (2011) proposed a joint framework for annotating queries with POS tags and phrase chunks. Ganchev et al. (2012) trained a POS tagger on automatically tagged queries. The POS tags of the training queries are projected from sentences containing the query terms within Web pages retrieved for them. The retrieved sentences were POS tagged using an o"
N16-1081,P09-1097,0,0.380413,"tical conventions, and to convey more information than can be directly inferred from their words. Consequently, a number of works addressed their syntactic analysis. Allan and Raghavan (2002) use part-of-speech (POS) tag patterns in order to manually map very short queries into clarification questions, which are then presented to the user to help them clarify their intent. Barr et al. (2008) trained POS taggers for Web queries and used a set of rules to map the resulting tagged queries into one of seven syntactic categories, whose merit is tested in the context of information retrieval tasks. Manshadi and Li (2009) and Li (2010) addressed the task of semantic tagging and structural analysis of Web queries, focusing on noun phrase queries. Bendersky et al. (2010) used the POS tags of the top-retrieved documents to enhance the initial POS tagging of query terms. Bendersky et al. (2011) proposed a joint framework for annotating queries with POS tags and phrase chunks. Ganchev et al. (2012) trained a POS tagger on automatically tagged queries. The POS tags of the training queries are projected from sentences containing the query terms within Web pages retrieved for them. The retrieved sentences were POS tag"
N16-1081,J93-2004,0,0.0545684,"vates us to exclude semantic subcategorization from the syntactic layer. We note that the notion of segment has another meaning, within the task of query segmentation (Bergsma and Wang, 2007; Guo et al., 2008; Tan and Peng, 2008; Mishra et al., 2011; Hagen et al., 2012). This task’s goal is to identify words in the query that together form compound concepts or phrases, like “Chicago Bulls”. As such, this task differs from ours as it defines segmentation in semantic rather than syntactic terms. We also note that query segments are distinct from the concept of fragments in constituency parsing. Marcus et al. (1993) introduce fragments in order to overcome problems involving the attachment point of various modifying phrases, e.g. in the sentence ‘In Asia, as [FRAG in Europe], a new order is taking shape’. While proper treatment of such phrases often requires extra syntactic information, their syntactic connection to other parts of the sentence is present, unlike between query segments. 673 3.3 Query Treebank Following our proposed grammar, we constructed a treebank by manually annotating 5,000 queries that landed on Yahoo Answers (see §3.1). These queries were randomly split into a 4,000-query test set a"
N16-1081,P13-2109,0,0.048726,"Missing"
N16-1081,N03-1033,0,0.0367478,"Missing"
N16-1081,N10-1004,0,\N,Missing
N18-1112,P07-1056,0,0.339159,"ence, PBLM not only exploits the sequential nature of its input text, but its output states can naturally feed LSTM and CNN task classifiers. Notice that PBLM is very flexible: instead of pivot based unigram prediction it can be defined to predict pivots of arbitrary length (e.g. the next bigram or trigram), or, alternatively, it can be defined over sentences or other textual units instead of words. Following a large body of DA work, we experiment (Section 5) with the task of binary sentiment classification. We consider adaptation between each domain pair in the four product review domains of Blitzer et al. (2007) (12 domain pairs) as well as between these domains and an airline review domain (Nguyen, 2015) and vice versa (8 domain pairs). The latter 8 setups are particularly 2 Pivots are defined with respect to a (source, target) domain pair. The pivots highlighted in the figure are the pivots for this review in all the setups we explored. I was at first very excited with my new Zyliss ::::::::::: salad spinner - it is easy to spin and looks ::::: great ... . However, ... it doesn’t get your greens very dry. I’ve been surprised and disappointed by the amount of water left on lettuce after spinning, an"
N18-1112,W06-1615,0,0.973925,"s and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 1 Our code is publicly available at: https://github. com/yftah89/PBLM-Domain-Adaptation. 2014)), the unprecedented growth of heterogeneous online content calls for more progress. DA through Representation Learning (DReL), where the DA method induces shared representations for the examples in the source and the target domains, has become prominent in the Neural Network (NN) era. A seminal (non-NN) DReL work is structural correspondence learning (SCL) (Blitzer et al., 2006, 2007) which models the connections between pivot features – features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain – and the other, non-pivot, features. While this approach explicitly models the correspondence between the source and the target domains, it has been outperformed by NN-based models, particularly those based on autoencoders (AEs, (Glorot et al., 2011; Chen et al., 2012)) which employ compress-based noise reduction to extract features that empirically support domain adaptation. Recently, Ziser and Reichar"
N18-1112,P15-1071,0,0.252691,"he unlabeled data from both the source and the target domains, to predict whether its associated pivot feature appears in the example or not. Note that no human annotation is required for the training of these classifiers, the supervision signal is in the unlabeled data. The matrix whose columns are the weight vectors of the classifiers is post-processed with singular value decomposition (SVD) and the derived matrix maps feature vectors from the original space to the new. Since the presentation of SCL, pivot-based DA has been researched extensively (e.g. (Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Ziser and Reichart, 2017)). PBLM is a pivot-based method but, in contrast to previous models, it relies on sequential NNs to exploit the structure of the input text. Even models such as (Bollegala et al., 2015), that embed pivots and non-pivots so that the former can predict if the latter appear in their neighborhood, learn a single representation for all the occurrences of a word in the input corpus. That is, Bollegala et al. (2015), as well as other methods that learn cross-domain word embeddings (Yang et al., 2017), learn wordtype representations, rather than context s"
N18-1112,W04-3237,0,0.170365,"Missing"
N18-1112,P16-2005,0,0.110778,"nction d, and its output is a reconstruction of its input x: r(x) = d(e(x)). The parameters of the model are trained to minimize a loss between x and r(x), such as their KullbackLeibler (KL) divergence or their cross entropy. Variants of AEs are prominent in recent DA literature. Examples include Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008; Glorot et al., 2011) and marginalized SDA (MSDA, (Chen et al., 2012)) that is more computationally efficient and scalable to high-dimensional feature spaces than SDA, and has been extended in various manners (e.g. (Yang and Eisenstein, 2014; Clinchant et al., 2016)). Finally, models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) have recently been applied in DA (e.g. variational fair autoencoder (Louizos et al., 2016)), but in our experiments they were still not competitive with MSDA. While AE based models have set a new stateof-the-art for DA in NLP, they are mostly based on noise reduction in the representation and do not exploit task specific and linguistic information. This paved the way for ZR17 that integrated pivotbased ideas into domain adaptation with AEs. Combining Pivots and AEs in Domain Adaptation ZR17 co"
N18-1112,P07-1033,0,0.205495,"Missing"
N18-1112,P07-1034,0,0.328632,"text classifiers such as LSTM and CNN. We experiment with the task of cross-domain sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, due to the reliance of many algorithms on costly labeled data which is scarce in many domains. To save annotation efforts, DA aims to import algorithms trained with labeled data from one or several domains to new ones. While DA algorithms have long been developed for many tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 1 Our code is publicly available at: https://github. com/yftah89/PBLM-Domain-Adaptation. 2014)), the unprecedented growth of heterogeneous online content calls for more progress. DA through Representation Learning (DReL), where the DA method induces shared representations for the examples in the source and the target domains, has become prominent in the Neural Network (NN) era. A seminal (non-NN) DReL work is structural correspondence learning (SCL) (Blitzer et al., 2006, 2007) which models"
N18-1112,D14-1181,0,0.00329961,"Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics input example, these methods can feed only task classifiers such as SVM and feed-forward NNs that take a single vector as input, but cannot feed sequential (e.g. RNNs and LSTMs (Hochreiter and Schmidhuber, 1997)) or convolution (CNNs (LeCun et al., 1998)) networks that require an input vector per word or sentence in their input. This may be a serious limitation given the excellent performance of structure aware models in a large variety of NLP tasks, including sentiment analysis and text classification (e.g.(Kim, 2014; Yogatama et al., 2017)) - prominent DA evaluation tasks. Fig. 1 demonstrates the limitation of structureindifferent modeling in DA for sentiment analysis. While the example review contains more positive pivot features (see definition in Sec. 2), the sentiment expressed in the review is negative. A representation learning method should encode the review structure (e.g. the role of the terms at first and However) in order to uncover the sentiment.2 In this paper we overcome these limitations. We present (Section 3) the Pivot Based Language Model (PBLM) - a domain adaptation model that (a) is a"
N18-1112,Q16-1023,0,0.00789023,"Missing"
N18-1112,N10-1004,0,0.0160802,"as LSTM and CNN. We experiment with the task of cross-domain sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, due to the reliance of many algorithms on costly labeled data which is scarce in many domains. To save annotation efforts, DA aims to import algorithms trained with labeled data from one or several domains to new ones. While DA algorithms have long been developed for many tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 1 Our code is publicly available at: https://github. com/yftah89/PBLM-Domain-Adaptation. 2014)), the unprecedented growth of heterogeneous online content calls for more progress. DA through Representation Learning (DReL), where the DA method induces shared representations for the examples in the source and the target domains, has become prominent in the Neural Network (NN) era. A seminal (non-NN) DReL work is structural correspondence learning (SCL) (Blitzer et al., 2006, 2007) which models the connections between"
N18-1112,P11-1007,0,0.0638303,"periment with the task of cross-domain sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, due to the reliance of many algorithms on costly labeled data which is scarce in many domains. To save annotation efforts, DA aims to import algorithms trained with labeled data from one or several domains to new ones. While DA algorithms have long been developed for many tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 1 Our code is publicly available at: https://github. com/yftah89/PBLM-Domain-Adaptation. 2014)), the unprecedented growth of heterogeneous online content calls for more progress. DA through Representation Learning (DReL), where the DA method induces shared representations for the examples in the source and the target domains, has become prominent in the Neural Network (NN) era. A seminal (non-NN) DReL work is structural correspondence learning (SCL) (Blitzer et al., 2006, 2007) which models the connections between pivot featur"
N18-1112,D17-1312,0,0.0177504,"ively (e.g. (Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Ziser and Reichart, 2017)). PBLM is a pivot-based method but, in contrast to previous models, it relies on sequential NNs to exploit the structure of the input text. Even models such as (Bollegala et al., 2015), that embed pivots and non-pivots so that the former can predict if the latter appear in their neighborhood, learn a single representation for all the occurrences of a word in the input corpus. That is, Bollegala et al. (2015), as well as other methods that learn cross-domain word embeddings (Yang et al., 2017), learn wordtype representations, rather than context specific representations. In Sec. 3 we show how PBLM’s context specific outputs naturally feed structure aware task classifiers such as LSTM and CNN. AE Based Domain Adaptation The basic elements of an autoencoder are an encoder function e and a decoder function d, and its output is a reconstruction of its input x: r(x) = d(e(x)). The parameters of the model are trained to minimize a loss between x and r(x), such as their KullbackLeibler (KL) divergence or their cross entropy. Variants of AEs are prominent in recent DA literature. Examples"
N18-1112,P14-2088,0,0.455192,"function e and a decoder function d, and its output is a reconstruction of its input x: r(x) = d(e(x)). The parameters of the model are trained to minimize a loss between x and r(x), such as their KullbackLeibler (KL) divergence or their cross entropy. Variants of AEs are prominent in recent DA literature. Examples include Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008; Glorot et al., 2011) and marginalized SDA (MSDA, (Chen et al., 2012)) that is more computationally efficient and scalable to high-dimensional feature spaces than SDA, and has been extended in various manners (e.g. (Yang and Eisenstein, 2014; Clinchant et al., 2016)). Finally, models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) have recently been applied in DA (e.g. variational fair autoencoder (Louizos et al., 2016)), but in our experiments they were still not competitive with MSDA. While AE based models have set a new stateof-the-art for DA in NLP, they are mostly based on noise reduction in the representation and do not exploit task specific and linguistic information. This paved the way for ZR17 that integrated pivotbased ideas into domain adaptation with AEs. Combining Pivots and AEs in"
N18-1112,D16-1023,0,0.45407,"oth the source and the target domains, to predict whether its associated pivot feature appears in the example or not. Note that no human annotation is required for the training of these classifiers, the supervision signal is in the unlabeled data. The matrix whose columns are the weight vectors of the classifiers is post-processed with singular value decomposition (SVD) and the derived matrix maps feature vectors from the original space to the new. Since the presentation of SCL, pivot-based DA has been researched extensively (e.g. (Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Ziser and Reichart, 2017)). PBLM is a pivot-based method but, in contrast to previous models, it relies on sequential NNs to exploit the structure of the input text. Even models such as (Bollegala et al., 2015), that embed pivots and non-pivots so that the former can predict if the latter appear in their neighborhood, learn a single representation for all the occurrences of a word in the input corpus. That is, Bollegala et al. (2015), as well as other methods that learn cross-domain word embeddings (Yang et al., 2017), learn wordtype representations, rather than context specific representati"
N18-1112,K17-1040,1,0.555187,"tzer et al., 2006, 2007) which models the connections between pivot features – features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain – and the other, non-pivot, features. While this approach explicitly models the correspondence between the source and the target domains, it has been outperformed by NN-based models, particularly those based on autoencoders (AEs, (Glorot et al., 2011; Chen et al., 2012)) which employ compress-based noise reduction to extract features that empirically support domain adaptation. Recently, Ziser and Reichart (2017) (ZR17) proposed to marry these approaches. They have presented the autoencoder-SCL models and demonstrated their superiority over a large number of previous approaches, particularly those that employ pivot-based ideas only or NNs only. Current DReL methods, however, suffer from a fundamental limitation: they ignore the structure of their input text (usually sentence or document). This is reflected both in the way they represent their input text, typically with a single vector whose coordinates correspond to word counts or indicators across the text, and in their output which typically consist"
N18-1112,N03-1027,0,0.488008,"Missing"
N18-1112,D12-1131,1,0.883549,"n sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, due to the reliance of many algorithms on costly labeled data which is scarce in many domains. To save annotation efforts, DA aims to import algorithms trained with labeled data from one or several domains to new ones. While DA algorithms have long been developed for many tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 1 Our code is publicly available at: https://github. com/yftah89/PBLM-Domain-Adaptation. 2014)), the unprecedented growth of heterogeneous online content calls for more progress. DA through Representation Learning (DReL), where the DA method induces shared representations for the examples in the source and the target domains, has become prominent in the Neural Network (NN) era. A seminal (non-NN) DReL work is structural correspondence learning (SCL) (Blitzer et al., 2006, 2007) which models the connections between pivot features – features that are frequent in the sour"
N18-1112,Q14-1002,0,0.168426,"Missing"
N19-1354,P15-1034,0,0.137901,"Missing"
N19-1354,K18-2005,0,0.262058,"n dependency parsing required careful feature engineering (McDonald et al., 2005b; Koo et al., 2008), this has become less of a concern in recent years with the emergence of deep neural networks (Kiperwasser and Goldberg, 2016; Dozat et al., 2017). Nonetheless, an accurate parser still requires a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ign"
N19-1354,P17-1110,0,0.0225826,"connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic progra"
N19-1354,Q16-1022,0,0.0247513,"theless, an accurate parser still requires a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ignores the uncertainty around the estimated parameters. When training data is small these challenges are more pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the"
N19-1354,K17-3002,0,0.139222,"assumption between the arcs, the parsing is done via a dynamic programming solution that finds the dependency parse ? ∗ such that, ( ) ∑ ∗ ? = argmax SCORE(? ) = ARC -SCORE (?, ?) . ? (?,?)∈? Next, given ? ∗ , for each arc (?, ?) ∈ ? ∗ the LABEL -SCORE (?, ?, ?) is computed as: ( ) ′ ′(???−ℎ???) ′(???−???) ′ ? ×tanh(? ?? +? ?? +? ) [?], ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ head specialized ?? modifier specialized ?? where ? is a dependency relation type, ? ∈ {?? }??=1 , ? ′ is (? × ℎ), ?′ is (? × 1), and ? ′(.) is (ℎ × ?). The label for each dependency arc (?, ?) is then chosen by a max operation. Dozat et al. (2017) proposed an extension of the BiLSTM parser by replacing the non-linear transformation of ARC-SCORE and LABEL -SCORE with a linear transformation (BiAFFINE). This was further extended by Che et al. (2018) who utilized contextualized word embeddings (Peters et al., 2018). Both extensions showed success in dependency parsing shared tasks (Zeman et al., 2017, 2018). Our Neural Parser Architecture Our network architecture extends the BiLSTM model with an additional BiLSTM layer and input signals. While our 3510 architecture is not the core contribution of this paper, we aim to implement our BNNP o"
N19-1354,P18-1128,1,0.834955,"shared task on parsing to Universal Dependencies (UD) (Zeman et al., 2017).5 5.1 Experimental Setup We use the UDPipe baseline outputs for segmentation and POS tagging of the raw test data (released along with the raw test data). While segmentation and POS errors substantially impact the quality of the final predicted parse, their exploration is beyond our scope. Our evaluation metric is Labeled Attachment Score (LAS), computed by the shared task evaluation script. Statistical significance, when mentioned, is computed over 20 runs, via the Kolmogorov-Smirnov test (Reimers and Gurevych, 2017; Dror et al., 2018) with ? = 0.01. Mono-Lingual Experiments We experiment with Persian (fa), Korean (ko), Russian (ru), Turkish (tr), Vietnamese (vi) and Irish (ga), all with less than 5? training sentences (Table 1). For comparison we report the scores published by the top system of the CoNLL 2017 shared task, B iAFFINE (Dozat et al., 2017), noting the following differences between their input and output and ours. The BiAFFINE parser: (i) uses the UDPipe outputs for segmentation but corrects POS errors before parsing, (ii) includes both language specific and universal POS tags in the input layer while we only i"
N19-1354,C96-1058,0,0.122623,"oodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can be done concurrently for all samples, resulting in a running time identical to the non-Bayesian approach. For each labelled edge, we replace its score in the ARC-SCORE matrix with its occurrence count in the collection of sampled trees and infer the final tree using counts as scores. The predicted structure is then passed to the label predictor, which assigns labels to the edges (§2). This decoding approach, while selecting the global structure with the highest probability under the approximate posterior, could potentially allow for additional corrections of the highest scoring tree"
N19-1354,N09-1037,0,0.0193894,"1997) lends itself as a natural choice for low-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning"
N19-1354,P17-1030,0,0.146707,"rameters which offers the desired degree of uncertainty by exploring the posterior space during inference. However, this solution comes with a high computational cost, specifically in DNNs, and is often replaced by regularization techniques such as dropout (Srivastava et al., 2014) as well as ensemble learning and prediction averaging (Liu et al., 2018; Che et al., 2018). Bayesian neural networks (BNNs) have attracted some attention (Welling and Teh, 2011; HernándezLobato and Adams, 2015; Li et al., 2016; Gong et al., 2018). Yet, its current application to NLP is limited to language modeling (Gan et al., 2017), and BNNs have not been developed for structured 3509 Proceedings of NAACL-HLT 2019, pages 3509–3519 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics prediction tasks such as dependency parsing. In this paper we aim to close this gap and propose the first BNN for dependency parsing (BNNP). To address the costs of inference step, we apply an efficient sampling procedure via stochastic gradient Langevin dynamics (SGLD) (Welling and Teh, 2011). At training, samples from the posterior distribution of the parser parameters are generated via controlled"
N19-1354,P96-1024,0,0.194908,"and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can be done concurrently for all samples, resulting in a running time identical to the non-Bayesian approach. For each labelled edge, we replace its score in the ARC-SCORE matrix with its occurrence count in the collection of sampled trees and infer the final tree using counts as scores. The predicted structure is then passed to the label predictor, which assigns labels to the edges (§2). This decoding approach, wh"
N19-1354,D17-1206,0,0.0277421,"respectively. To train the discriminator, a sum of the cross-entropy losses for ?1∶? is used (denoted by ???? ). As the parameters of the discriminators are being updated, the gradient signals to minimize the discriminator’s error are backpropagated with an opposite sign to the shared BiLSTM layer, which adversarially encourages the shared BiLSTM to fool the discriminator. Our training schedule alternates between the two modes, in one mode optimizing the shared and taskspecific parameters based on ????? and ??? (in 4 We also tried layer-wise placements of tasks (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and the results were slightly worse. Details are omitted for space reason. 3513 random order), while in the other mode optimizing ???? which includes the shared and discriminatorspecific parameters. 5 Experiments and Results We experiment with mono-lingual and cross-lingual dependency parsing using the treebanks of the CoNLL 2017 shared task on parsing to Universal Dependencies (UD) (Zeman et al., 2017).5 5.1 Experimental Setup We use the UDPipe baseline outputs for segmentation and POS tagging of the raw test data (released along with the raw test data). While segmentation and POS errors su"
N19-1354,N18-1084,0,0.0478884,"Missing"
N19-1354,D17-1302,0,0.0199562,"due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a)"
N19-1354,Q16-1023,0,0.0215492,"anji (kmr), Buriat (bxr), and Northern Sami (sme). We also report the results for each language, where the combination of training datasets for the rest of the languages (marked as +) was used for training. The cross-lingual experiments are done on delexicalized 5 For train and dev sets (1-1983), test set (1-2184), and pretrained embeddings (1-1989) see: https: //lindat.mff.cuni.cz/repository/xmlui/ handle/11234/{1-1983,1-2184,1-1989} parses after replacing the words with their Universal POS tags. Models and Baselines - Single-Task We consider the following models: BASE is the BiLSTM model of Kiperwasser and Goldberg (2016); BASE ++ extends BASE by having 2 layers of BiLSTM s and using 1 layer of character level BiLSTM (§2); + SHARED includes an additional BiLSTM (dashed box in Figure 1). We included this to provide a fair comparison (in terms of the number of parameters) with the multi-task experiments but we apply a higher dropout rate to resolve overfitting; ENSEMBLE denotes a collection of 9 + SHARED models each randomly initialized (Reimers and Gurevych, 2017) and trained for MLE with MBR (§3.3) applied for prediction; MAP denotes the + SHARED model optimized for MAP instead of MLE; + SGLD denotes Bayesian"
N19-1354,D18-1290,1,0.853481,"ow-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Chara"
N19-1354,K17-1041,0,0.107917,"Missing"
N19-1354,P05-1012,0,0.175811,"2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can be done concurrently for all samples, resulting in a running time identical to the non-Bayesian approach. For each labelled edge, we replace its score in the ARC-SCORE matrix with its occurrence count in the collection of sampled trees and infer the final tree using counts as scores. The predicted structure is then passed to the label predictor, which assigns labels to the edges (§2). This decoding approach, while selecting the global structure with the highest probability under the approximate posterior, could potentially allow for additiona"
N19-1354,P08-1068,0,0.11998,"Missing"
N19-1354,D16-1180,0,0.101046,"SCORE(? )− SCORE(? ) For label prediction, hinge loss with ? = 1 is used (denoted by ??? ). We refer to the parser loss as: ????? = ??? + ??? . Beyond MLE Training The point-estimate of DNN parameters is computationally efficient, but ignores the uncertainty around model parameters during learning. This results in an overconfidence over model predictions during the inference phase. A common generic practice to incorporate a degree of uncertainty is to consider an ensemble of models. Indeed, for dependency parsing ensemble learning has shown to improve accuracy (Surdeanu and Manning, 2010; Kuncoro et al., 2016; Che et al., 2018). However, ensembles are computationally demanding due to the large number of participating models. The de-facto approach to overcome this 2 Stacking BiLSTM s is believed to be helpful. In our case, the addition of a third layer led to overfitting. has been to randomly perturb the structure of the network for each training instance by switching off connections between the nodes, a practice known as dropConnect (Wan et al., 2013), or eliminating the nodes entirely, which is known as dropout (Srivastava et al., 2014). Hinton et al. (2012) demonstrated that dropping out a node"
N19-1354,N18-1202,0,0.0183951,"puted as: ( ) ′ ′(???−ℎ???) ′(???−???) ′ ? ×tanh(? ?? +? ?? +? ) [?], ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ head specialized ?? modifier specialized ?? where ? is a dependency relation type, ? ∈ {?? }??=1 , ? ′ is (? × ℎ), ?′ is (? × 1), and ? ′(.) is (ℎ × ?). The label for each dependency arc (?, ?) is then chosen by a max operation. Dozat et al. (2017) proposed an extension of the BiLSTM parser by replacing the non-linear transformation of ARC-SCORE and LABEL -SCORE with a linear transformation (BiAFFINE). This was further extended by Che et al. (2018) who utilized contextualized word embeddings (Peters et al., 2018). Both extensions showed success in dependency parsing shared tasks (Zeman et al., 2017, 2018). Our Neural Parser Architecture Our network architecture extends the BiLSTM model with an additional BiLSTM layer and input signals. While our 3510 architecture is not the core contribution of this paper, we aim to implement our BNNP on a strong architecture. In §5.3 we demonstrate the contribution of these additions to our final results. In our BNNP, each word is represented as, ?? = ?(?? )◦?(?? )◦?(?  ? )◦?(?? ), where, similar to the BiAFFINE parser, ?(?? ) is a character-level representation of"
N19-1354,N16-1121,0,0.0202361,"e parser still requires a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ignores the uncertainty around the estimated parameters. When training data is small these challenges are more pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the learning in the absenc"
N19-1354,E17-1005,0,0.0152841,"s Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter"
N19-1354,P14-2050,0,0.0695435,"Missing"
N19-1354,Q17-1020,0,0.0135469,"s a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ignores the uncertainty around the estimated parameters. When training data is small these challenges are more pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the learning in the absence of sufficient data, and (i"
N19-1354,N18-1088,1,0.842192,"ore pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the learning in the absence of sufficient data, and (ii) predicting under the full posterior distribution of model parameters which offers the desired degree of uncertainty by exploring the posterior space during inference. However, this solution comes with a high computational cost, specifically in DNNs, and is often replaced by regularization techniques such as dropout (Srivastava et al., 2014) as well as ensemble learning and prediction averaging (Liu et al., 2018; Che et al., 2018). Bayesian neural networks (BNNs) have attracted some attention (Welling and Teh, 2011; HernándezLobato and Adams, 2015; Li et al., 2016; Gong et al., 2018). Yet, its current application to NLP is limited to language modeling (Gan et al., 2017), and BNNs have not been developed for structured 3509 Proceedings of NAACL-HLT 2019, pages 3509–3519 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics prediction tasks such as dependency parsing. In this paper we aim to close this gap and propose the first BNN for dependency parsing (BNNP)"
N19-1354,P08-1098,1,0.655231,"ask learning (Caruana, 1997) lends itself as a natural choice for low-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learni"
N19-1354,D17-1035,0,0.0397089,"treebanks of the CoNLL 2017 shared task on parsing to Universal Dependencies (UD) (Zeman et al., 2017).5 5.1 Experimental Setup We use the UDPipe baseline outputs for segmentation and POS tagging of the raw test data (released along with the raw test data). While segmentation and POS errors substantially impact the quality of the final predicted parse, their exploration is beyond our scope. Our evaluation metric is Labeled Attachment Score (LAS), computed by the shared task evaluation script. Statistical significance, when mentioned, is computed over 20 runs, via the Kolmogorov-Smirnov test (Reimers and Gurevych, 2017; Dror et al., 2018) with ? = 0.01. Mono-Lingual Experiments We experiment with Persian (fa), Korean (ko), Russian (ru), Turkish (tr), Vietnamese (vi) and Irish (ga), all with less than 5? training sentences (Table 1). For comparison we report the scores published by the top system of the CoNLL 2017 shared task, B iAFFINE (Dozat et al., 2017), noting the following differences between their input and output and ours. The BiAFFINE parser: (i) uses the UDPipe outputs for segmentation but corrects POS errors before parsing, (ii) includes both language specific and universal POS tags in the input l"
N19-1354,D10-1001,0,0.0434372,"computed as tanh(? ′′ ?? +?′′ )[?], ? where ? ∈ {?? }?=1 , ? ′′ is (? × ?), and ?′′ is (? × 1). To train the POS tagger, the cross-entropy loss is 1st BiLSTM is Multi-task learning (Caruana, 1997) lends itself as a natural choice for low-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is t"
N19-1354,P16-2038,0,0.20852,"rk hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Giv"
N19-1354,N10-1091,0,0.0172665,"??? = max 0, COST(? , ? )+ SCORE(? )− SCORE(? ) For label prediction, hinge loss with ? = 1 is used (denoted by ??? ). We refer to the parser loss as: ????? = ??? + ??? . Beyond MLE Training The point-estimate of DNN parameters is computationally efficient, but ignores the uncertainty around model parameters during learning. This results in an overconfidence over model predictions during the inference phase. A common generic practice to incorporate a degree of uncertainty is to consider an ensemble of models. Indeed, for dependency parsing ensemble learning has shown to improve accuracy (Surdeanu and Manning, 2010; Kuncoro et al., 2016; Che et al., 2018). However, ensembles are computationally demanding due to the large number of participating models. The de-facto approach to overcome this 2 Stacking BiLSTM s is believed to be helpful. In our case, the addition of a third layer led to overfitting. has been to randomly perturb the structure of the network for each training instance by switching off connections between the nodes, a practice known as dropConnect (Wan et al., 2013), or eliminating the nodes entirely, which is known as dropout (Srivastava et al., 2014). Hinton et al. (2012) demonstrated tha"
N19-1354,P16-1136,0,0.155752,"Missing"
N19-1354,N18-1123,0,0.0195712,"the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can"
N19-1354,K18-2001,0,0.0267615,"Missing"
N19-1354,K17-3001,0,0.356075,"⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ head specialized ?? modifier specialized ?? where ? is a dependency relation type, ? ∈ {?? }??=1 , ? ′ is (? × ℎ), ?′ is (? × 1), and ? ′(.) is (ℎ × ?). The label for each dependency arc (?, ?) is then chosen by a max operation. Dozat et al. (2017) proposed an extension of the BiLSTM parser by replacing the non-linear transformation of ARC-SCORE and LABEL -SCORE with a linear transformation (BiAFFINE). This was further extended by Che et al. (2018) who utilized contextualized word embeddings (Peters et al., 2018). Both extensions showed success in dependency parsing shared tasks (Zeman et al., 2017, 2018). Our Neural Parser Architecture Our network architecture extends the BiLSTM model with an additional BiLSTM layer and input signals. While our 3510 architecture is not the core contribution of this paper, we aim to implement our BNNP on a strong architecture. In §5.3 we demonstrate the contribution of these additions to our final results. In our BNNP, each word is represented as, ?? = ?(?? )◦?(?? )◦?(?  ? )◦?(?? ), where, similar to the BiAFFINE parser, ?(?? ) is a character-level representation of the word ?? , generated by a BiLSTM :  ? (?1∶|? |)◦LSTM  ? (?|? |∶1 ). ?(?? ) = LSTM"
P07-1052,P05-1022,0,0.721384,"ion Many algorithms for major NLP applications such as information extraction (IE) and question answering (QA) utilize the output of statistical parsers (see (Yates et al., 2006)). While the average performance of statistical parsers gradually improves, the quality of many of the parses they produce is too low for applications. When the training and test 408 data are taken from different domains (the parser adaptation scenario) the ratio of such low quality parses becomes even higher. Figure 1 demonstrates these phenomena for two leading models, Collins (1999) model 2, a generative model, and Charniak and Johnson (2005), a reranking model. The parser adaptation scenario is the rule rather than the exception for QA and IE systems, because these usually operate over the highly variable Web, making it very difficult to create a representative corpus for manual annotation. Medium quality parses may seriously harm the performance of such systems. In this paper we address the problem of assessing parse quality, using a Sample Ensemble Parse Assessment (SEPA) algorithm. We use the level of agreement among several copies of a parser, each of which trained on a different sample from the training data, to predict the"
P07-1052,W01-0521,0,0.0786942,"Missing"
P07-1052,A00-2005,0,0.244236,"Missing"
P07-1052,H05-1064,0,0.038262,"Missing"
P07-1052,N03-1022,0,0.0850279,"ted by sampling, with replacement, L examples from the training pool, where L is the size of the training pool. Conversely, each of our samples is smaller than the training set, and is created by sampling without replacement. See Section 3 (‘regarding S’) for a discussion of this issue. predictors in classifiers’ output to posterior probabilities is given in (Caruana and Niculescu-Mizil, 2006). As far as we know, the application of a sample based parser ensemble for assessing parse quality is novel. Many IE and QA systems rely on the output of parsers (Kwok et al., 2001; Attardi et al., 2001; Moldovan et al., 2003). The latter tries to address incorrect parses using complex relaxation methods. Knowing the quality of a parse could greatly improve the performance of such systems. 3 The Sample Ensemble Parse Assessment (SEPA) Algorithm In this section we detail our parse assessment algorithm. Its input consists of a parsing algorithm A, an annotated training set T R, and an unannotated test set T E. The output provides, for each test sentence, the parse generated for it by A when trained on the full training set, and a grade assessing the parse’s quality, on a continuous scale between 0 to 100. Application"
P07-1052,P00-1016,0,0.496173,"Missing"
P07-1052,W06-1604,0,0.262901,"e to web-based applications such as QA and IE. Generative statistical parsers compute a probability p(a, s) for each sentence annotation, so the immediate technique that comes to mind for assessing parse quality is to simply use p(a, s). Another seemingly trivial method is to assume that shorter sentences would be parsed better than longer ones. However, these techniques produce results that are far from optimal. In Section 5 we show the superiority of our method over these and other baselines. Surprisingly, as far as we know there is only one previous work explicitly addressing this problem (Yates et al., 2006). Their WOODWARD algorithm filters out high quality parses by performing semanProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 408–415, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Fraction of parses 1 Collins, ID Collins, Adap. Charniak, ID Charniak,Adap. 0.8 0.6 0.4 0.2 80 85 90 95 100 F score Figure 1: F-score vs. the fraction of parses whose f-score is at least that f-score. For the in-domain scenario, the parsers are tested on sec 23 of the WSJ Penn Treebank. For the parser adaptation scenario, they are te"
P07-1052,J03-4003,0,\N,Missing
P07-1078,A00-2018,0,0.278039,"Missing"
P07-1078,P05-1022,0,0.0729994,"the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They train the PCFG parser and the reranker with the manually annotated WSJ data, and parse the NANC data with the 50-best PCFG parser. Then they proceed in two directi"
P07-1078,W03-0407,0,0.204778,"eds in adapting a generative parser between domains using a small manually annotated dataset. • The first formulation (related to the number of unknown words in a sentence) of when selftraining is valuable. Section 2 discusses previous work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second la"
P07-1078,J04-3001,0,0.0522258,"Missing"
P07-1078,H05-1064,0,0.0186989,"f selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They train the PCFG parser and the reranker with the manually annotated WSJ data, and parse the NANC data with the 50-best PCFG parser. Then they proceed in two directions. In the first, they reorder the 50-best parse list"
P07-1078,N06-1020,0,0.839769,"work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They"
P07-1078,P06-1043,0,0.610862,"work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They"
P07-1078,E03-1008,0,0.819559,"l seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only. However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only one of our scenarios (OI, see below). A work closely related to ours is (Steedman et al., 2003a), which applied co-training (Blum and Mitchell, 1998) and self-training to Collins’ parsing model using a small seed dataset (500 sentences for both methods and 1,000 sentences for co-training only). The seed, self-training and test datasets they used are similar to those we use in our II experiment (see below), but the self-training protocols are different. They first train the parser with the seed sentences sampled from WSJ sections 2-21. Then, iteratively, 30 sentences are sampled from these sections, parsed by the parser, and the 20 best sentences (in terms of parser confidence defined a"
P07-1078,N03-1031,0,0.507694,"l seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only. However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only one of our scenarios (OI, see below). A work closely related to ours is (Steedman et al., 2003a), which applied co-training (Blum and Mitchell, 1998) and self-training to Collins’ parsing model using a small seed dataset (500 sentences for both methods and 1,000 sentences for co-training only). The seed, self-training and test datasets they used are similar to those we use in our II experiment (see below), but the self-training protocols are different. They first train the parser with the seed sentences sampled from WSJ sections 2-21. Then, iteratively, 30 sentences are sampled from these sections, parsed by the parser, and the 20 best sentences (in terms of parser confidence defined a"
P07-1078,J03-4003,0,\N,Missing
P08-1098,W04-3202,0,0.02149,"ure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in 867 or"
P08-1098,P96-1042,0,0.173428,"verage of a wide variety of domains in human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a"
P08-1098,W05-0619,0,0.00963254,") and Brown (right) 40000 5000 10000 15000 constituents 20000 25000 30000 35000 constituents Figure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994)"
P08-1098,J04-3001,0,0.70764,"an expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse t"
P08-1098,J93-2004,0,0.0340148,"ding classifier. As a consequence, training such a classifier which takes into account several annotation tasks will best be performed on a rich corpus annotated with respect to all inputrelevant tasks. Both kinds of annotation tasks, similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as"
P08-1098,P00-1016,0,0.0259827,"n human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might us"
P08-1098,J05-1004,0,0.0421395,"similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as possible for a set of classifiers instead of a single classifier only. This enables the creation of a single combined corpus annotated with respect to various annotation tasks, while preserving the advantages of AL with 862 respect to"
P08-1098,P07-1052,1,0.855102,"etric. It is calculated on the token-level as V Etok (t) = − c V (li , t) V (li , t) 1 X log (1) log k i=0 k k where V (lki ,t) is the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each sentence in the unlabeled pool, the agreement between the committee members was calculated using the function reported by Reichart and Rappoport (2007): AF (s) = 1 N X f score(mi , ml ) (2) i,l∈[1...N ],i6=l Where mi and ml are the committee members and N = k2 ·(k22 −1) is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF (s). 4.2 Experimental settings For the NE task we employed the classifier described by Tomanek et al. (2007): The NE tagger is based on Conditional Random Fields (Lafferty et al., 2001) 5 We randomly sampled L = e"
P08-1098,W04-1221,0,0.0193363,"Missing"
P08-1098,P04-1075,0,0.0205662,"E task on WSJ (left) and Brown (right) 40000 5000 10000 15000 constituents 20000 25000 30000 35000 constituents Figure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combinati"
P08-1098,tomanek-hahn-2008-approximating,1,0.775115,"selection can be a better choice than one-sided selection in multiple annotation scenarios. Thus, considering all annotation tasks in the selection process (even if the selection protocol is as simple as the alternating selection protocol) is better than selecting only with respect to one task. Further, it should be noted that overall the more sophisticated rank combination protocol does not perform much better than the simpler alternating selection protocol in all scenarios. Finally, Figure 4 shows the disagreement curves for the two tasks on the WSJ corpus. As has already been discussed by Tomanek and Hahn (2008), disagreement curves can be used as a stopping criterion and to monitor the progress of AL-driven annotation. This is especially valuable when no annotated validation set is available (which is needed for plotting learning curves). We can see that the disagreement curves significantly flatten approximately at the same time as the learning curves do. In the context of MTAL, disagreement curves might not only be interesting as a stopping criterion but rather as a switching criterion, i.e., to identify when MTAL could be turned into one-sided selection. This would be the case if in an MTAL scena"
P08-1098,D07-1051,1,0.935507,"tions to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse trees and named entity annotations. For such an applic"
P08-1098,D07-1082,0,0.0153277,"protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in 867 order to improve the classification error rate for one specific classification task. In contrast, the focus of multi-task AL is on strategies to select training material for multi classifier systems where all classifiers cover different classification tasks. 7 Discussion Our treatment of MTAL within the context of the or"
P08-1098,J03-4003,0,\N,Missing
P08-1098,W03-0419,0,\N,Missing
P08-1098,P02-1016,0,\N,Missing
P08-1117,P05-1022,0,0.0890764,"Missing"
P08-1117,P04-1054,0,0.0535599,"n-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems"
P08-1117,P08-1079,1,0.824531,"to identify specialized phrase types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations be"
P08-1117,P07-1030,1,0.762465,"e types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities;"
P08-1117,P07-2040,0,0.0615941,"unberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to"
P08-1117,J93-2004,0,0.0322102,"represented by commas, there are two main strands of research with similar goals: 1) systems that directly analyze commas, whether labeling them with syntactic information or correcting inappropriate use in text; and 2) systems that extract relations from text, typically by trying to identify paraphrases. The significance of interpreting the role of commas in sentences has already been identified by (van Delden and Gomez, 2002; Bayraktar et al., 1998) and others. A review of the first line of research is given in (Say and Akman, 1997). In (Bayraktar et al., 1998) the WSJ PennTreebank corpus (Marcus et al., 1993) is analyzed and a very detailed list of syntactic patterns that correspond to different roles of commas is created. However, they do not study the extraction of entailed relations as a function of the comma’s interpretation. Furthermore, the syntactic patterns they identify are unlexicalized and would not support the level of semantic relations that we show in this paper. Finally, theirs is a manual process completely dependent on syntactic patterns. While our comma resolution system uses syntactic parse information as its main source of features, the approach we have developed focuses on the"
P08-1117,P06-1015,0,0.0143877,"o relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of senten"
P08-1117,P06-1102,0,0.0129114,"tem has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general ap"
P08-1117,W04-2401,1,0.775126,"eloping comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited ac"
P08-1117,P06-2094,0,0.0190797,", it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and li"
P08-1117,W04-3206,0,0.0131183,", so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of sentences from WSJ (not among"
P09-1004,P98-1013,0,0.349106,"Missing"
P09-1004,A97-1052,0,0.109374,"n carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using shallow vs. full constituency syntactic information in an English SRL system. Their results clearly demonstrate the advantage of using full annotation. The identification of arguments has also been carried out in the context of automatic subcategorization frame acquisition. Notable examples include (Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002) who all used statistical hypothesis testing to filter a parser’s output for arguments, with the goal of compiling verb subcategorization lexicons. However, these works differ from ours as they attempt to characterize the behavior of a verb type, by collecting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, no"
P09-1004,burchardt-etal-2006-salsa,0,0.0836252,"Missing"
P09-1004,W04-2412,0,0.0805222,"Missing"
P09-1004,W05-0620,0,0.274152,"Missing"
P09-1004,E03-1009,0,0.0425319,"Missing"
P09-1004,P07-1071,0,0.0258833,"Missing"
P09-1004,palmer-etal-2008-pilot,0,0.00647986,"tation of the corpus to be annotated. The three works above are relevant but incomparable to our work, due to the extensive amount of supervision (namely, VerbNet and a rule-based or supervised syntactic system) they used, both in detecting the syntactic structure and in detecting the arguments. Work has been carried out in a few other languages besides English. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of"
P09-1004,N06-2026,0,0.085771,"Missing"
P09-1004,J05-1004,0,0.746745,"Missing"
P09-1004,J02-3001,0,0.977533,"classifier to determine for each word whether it is inside, outside or in the beginning of an argument (Hacioglu and Ward, 2003). Other works have integrated argument classification and identification into one step (Collobert and Weston, 2007), while others went further and combined the former two along with parsing into a single model (Musillo Related Work The advance of machine learning based approaches in this field owes to the usage of large scale annotated corpora. English is the most stud29 proach used similarity measures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using"
P09-1004,P06-2038,0,0.0231847,"gorization lexicons. However, these works differ from ours as they attempt to characterize the behavior of a verb type, by collecting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, notably in the CoNLL 2001 shared task (Tjong Kim Sang and D`ejean, 2001). Clause information has been applied to accelerating a syntactic parser (Glaysher and Moldovan, 2006). and Merlo, 2006). Work on less supervised methods has been scarce. Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. However, they utilize the extensive verb lexicon VerbNet, which lists the possible argument structures allowable for each verb, and supervised syntactic tools. Using VerbNet along with the output of a rule-based chunker (in 2004) and a supervised syntactic parser (in 2005), they spot instances in the corpus that are very similar to the syntactic patterns listed in VerbNet. They then use these as seed"
P09-1004,J08-2006,0,0.717808,"Missing"
P09-1004,P07-1025,0,0.143726,"equential tagging of words, training an SVM classifier to determine for each word whether it is inside, outside or in the beginning of an argument (Hacioglu and Ward, 2003). Other works have integrated argument classification and identification into one step (Collobert and Weston, 2007), while others went further and combined the former two along with parsing into a single model (Musillo Related Work The advance of machine learning based approaches in this field owes to the usage of large scale annotated corpora. English is the most stud29 proach used similarity measures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a de"
P09-1004,W06-1601,0,0.77507,"hm requires thousands to dozens of thousands sentences annotated with POS tags, syntactic annotation and SRL annotation. Current algorithms show impressive results but only for languages and domains where plenty of annotated data is available, e.g., English newspaper texts (see Section 2). Results are markedly lower when testing is on a domain wider than the training one, even in English (see the WSJ-Brown results in (Pradhan et al., 2008)). Only a small number of works that do not require manually labeled SRL training data have been done (Swier and Stevenson, 2004; Swier and Stevenson, 2005; Grenager and Manning, 2006). These papers have replaced this data with the VerbNet (Kipper et al., 2000) lexical resource or a set of manually written rules and supervised parsers. A potential answer to the SRL training data bottleneck are unsupervised SRL models that require little to no manual effort for their training. Their output can be used either by itself, or as training material for modern supervised SRL algorithms. In this paper we present an algorithm for unsupervised argument identification. The only type of annotation required by our algorithm is POS tagThe task of Semantic Role Labeling (SRL) is often divi"
P09-1004,P07-1049,0,0.170148,"model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of argument candidates, which a-priori contains all consecutive sequences of words that do not contain the predicate in question. 3.1 Algorithm overview As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sen30 L tence (Seginer, 2007). This parser is unique in that it is able to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) achieving state-of-the-art results. Since our algorithm uses millions to tens of millions sentences, we must use very fast tools. The parser’s high speed (thousands of words per second) enables us to process these large amounts of data. L L DT NNS The materials L IN L L L VBP in DT NN reach each The only type of supervised annotation we use is POS tagging. We use the taggers MXPOST (Ratnaparkhi, 1996) for English and TreeTagger (Schmid, 1994) for Spanish, to obtain"
P09-1004,C04-1186,0,0.0108749,"asures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using shallow vs. full constituency syntactic information in an English SRL system. Their results clearly demonstrate the advantage of using full annotation. The identification of arguments has also been carried out in the context of automatic subcategorization frame acquisition. Notable examples include (Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002) who all used statistical hypothesis testing to filter a parser’s output for arguments, with the goal of compiling verb subcategorization lexicons. However,"
P09-1004,P06-1072,0,0.0195592,"Missing"
P09-1004,N03-2009,0,0.0560056,"Missing"
P09-1004,N07-2021,0,0.0249603,"sh. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of argument candidates, which a-priori contains all consecutive sequences of words that do not contain the predicate in question. 3.1 Algorithm overview As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sen30 L tence (Seginer, 2007). This parser is unique in that it is able to induce a bracketing (unlabeled parsing) fro"
P09-1004,H05-1111,0,0.159997,"cting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, notably in the CoNLL 2001 shared task (Tjong Kim Sang and D`ejean, 2001). Clause information has been applied to accelerating a syntactic parser (Glaysher and Moldovan, 2006). and Merlo, 2006). Work on less supervised methods has been scarce. Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. However, they utilize the extensive verb lexicon VerbNet, which lists the possible argument structures allowable for each verb, and supervised syntactic tools. Using VerbNet along with the output of a rule-based chunker (in 2004) and a supervised syntactic parser (in 2005), they spot instances in the corpus that are very similar to the syntactic patterns listed in VerbNet. They then use these as seed for a bootstrapping algorithm, which consequently identifies the verb arguments in the corpus and assigns their semantic roles"
P09-1004,W01-0708,0,0.0857682,"Missing"
P09-1004,W04-3212,0,0.0783785,"e form. WP. The constituent is preceded by a Wh-pronoun. That. The constituent is preceded by a “that” marked by an “IN” POS tag indicating that it is a subordinating conjunction. Argument identification. For each predicate in the corpus, its argument candidates are now defined to be the constituents contained in the minimal clause containing the predicate. However, these constituents may be (and are) nested within each other, violating a major restriction on SRL arguments. Hence we now prune our set, by keeping only the siblings of all of the verb’s ancestors, as is common in supervised SRL (Xue and Palmer, 2004). Spanish CQUE. The constituent is preceded by a word with the POS “CQUE” which denotes the word “que” as a conjunction. INT. The constituent is preceded by a word with the POS “INT” which denotes an interrogative pronoun. CSUB. The constituent is preceded by a word with one of the POSs “CSUBF”, “CSUBI” or “CSUBX”, which denote a subordinating conjunction. Figure 2: The set of lexico-syntactic patterns that mark clauses which were used by our model. 3.3 Using collocations We use the following observation to filter out some superfluous argument candidates: since the arguments of a predicate man"
P09-1004,J08-2004,0,0.0210271,"g a test corpus. However, ARGID was not the task of that work, which dealt solely with argument classification. ARGID was performed by manually-created rules, requiring a supervised or manual syntactic annotation of the corpus to be annotated. The three works above are relevant but incomparable to our work, due to the extensive amount of supervision (namely, VerbNet and a rule-based or supervised syntactic system) they used, both in detecting the syntactic structure and in detecting the arguments. Work has been carried out in a few other languages besides English. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannot"
P09-1004,W04-3213,0,\N,Missing
P09-1004,W96-0213,0,\N,Missing
P09-1004,J08-2001,0,\N,Missing
P09-1004,W05-0628,0,\N,Missing
P09-1004,J04-4004,0,\N,Missing
P09-1004,S07-1008,0,\N,Missing
P09-1004,N07-1070,0,\N,Missing
P09-1004,C98-1013,0,\N,Missing
P09-1004,P93-1032,0,\N,Missing
P09-1004,W06-2303,0,\N,Missing
P10-1132,C04-1080,0,0.200831,"esults (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small numb"
P10-1132,P06-3002,0,0.135348,"omputational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tag"
P10-1132,J92-4003,0,0.310333,"Missing"
P10-1132,E03-1009,0,0.444192,"on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005"
P10-1132,D07-1023,0,0.0562564,"Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation eff"
P10-1132,C04-1052,0,0.0150501,"0 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary prov"
P10-1132,D08-1036,0,0.806358,"formance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model"
P10-1132,J01-2001,0,0.345512,"gment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to their association with the prototypes. Second, we use a distributional representation which has been shown to be effective for unsupervised parsing (Seginer, 2007). Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001). 3 Distributional Algorithm Our algorithm is given a plain text corpus and optionally a desired number of clusters k. Its output is a partitioning of words into clusters. The algorithm utilizes two representations, distributional and morphological. Although eventually the latter is used before the former, for clarity of presentation we begin by detailing the base distributional algorithm. In the next section we describe the morphological representation and its integration into the base algorithm. Overview. The algorithm consists of two main stages: landmark clusters discovery, and word mappin"
P10-1132,D07-1043,0,0.0843834,"accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high"
P10-1132,P07-1094,0,0.788838,"lgorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use t"
P10-1132,E95-1020,0,0.340359,"Missing"
P10-1132,P09-1059,0,0.0187048,"words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task. 1 Introduction Part-of-speech (POS) tagging is a fundamental NLP task, used by a wide variety of applications. However, there is no single standard POS tagging scheme, even for English. Schemes vary significantly across corpora and even more so across languages, creating difficulties in using POS tags across domains and for multi-lingual systems (Jiang et al., 2009). Automatic induction of POS tags from plain text can greatly alleviate this problem, as well as eliminate the efforts incurred by manual annotations. It is also a problem of great theoretical interest. Consequently, POS induction is a vibrant research area (see Section 2). In this paper we present an algorithm based on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the ∗ Omri Abend is grateful to the Azr"
P10-1132,D07-1031,0,0.388703,"lark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstra"
P10-1132,P07-1049,0,0.547456,"vised POS Induction through Prototype Discovery Omri Abend1∗ Roi Reichart2 1 Ari Rappoport1 Institute of Computer Science, 2 ICNC Hebrew University of Jerusalem {omria01|roiri|arir}@cs.huji.ac.il Abstract central members. Our algorithm first clusters words based on a fine morphological representation. It then clusters the most frequent words, defining landmark clusters which constitute the cores of the categories. Finally, it maps the rest of the words to these categories. The last two stages utilize a distributional representation that has been shown to be effective for unsupervised parsing (Seginer, 2007). We evaluated the algorithm in both English and German, using four different mapping-based and information theoretic clustering evaluation measures. The results obtained are generally better than all existing POS induction algorithms. Section 2 reviews related work. Sections 3 and 4 detail the algorithm. Sections 5, 6 and 7 describe the evaluation, experimental setup and results. We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of t"
P10-1132,P05-1044,0,0.156947,"given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding t"
P10-1132,D09-1072,0,0.35004,"to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology"
P10-1132,J94-2001,0,0.0948809,"best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model"
P10-1132,P09-1057,0,0.290719,"and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the ent"
P10-1132,C08-1091,1,0.86954,"a derived accuracy. The Many-to-1 measure finds the mapping between the gold standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster. The 1-to-1 measure finds the mapping between the induced and gold standard clusters which maximizes accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three d"
P10-1132,W09-1121,1,0.696459,"rs are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high frequency threshold), n = 50 (the para"
P10-1132,W10-2911,1,0.823065,"nts is based on their association with the ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computat"
P10-1132,W10-2909,1,0.563823,"Missing"
P10-1132,D09-1071,0,\N,Missing
P10-1132,N06-1041,0,\N,Missing
P10-1132,P08-1085,0,\N,Missing
P11-1067,N10-1083,0,0.0100955,"Missing"
P11-1067,P10-1131,0,0.00560222,"ing logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-super"
P11-1067,W04-1501,0,0.0673155,"in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex"
P11-1067,D10-1117,0,0.186649,"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 2004; Cohen et al., 2008; Headden et al., 2009; Spitkovsky et al., 2010a; Gillenwater et al., 2010; Berg-Kirkpatrick et al., 2010; Blunsom and Cohn, 2010, inter alia). Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated. However, there are cases in which there is no linguistic consensus as to what the correct annotation is (K¨ubler et al., 2009). Examples include which verb is the head in a verb group structure (e.g., “can” or “eat” in “can eat”), and which ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguis"
P11-1067,N09-1009,0,0.0289598,"blematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of researc"
P11-1067,P09-1041,0,0.00977741,"re. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent sta"
P11-1067,P10-2036,0,0.0356408,"Missing"
P11-1067,C08-1042,0,0.0740417,"Missing"
P11-1067,N09-1012,0,0.26345,"Missing"
P11-1067,W07-2416,0,0.0665149,"note that these controversies are reflected in the evaluation of this task, resulting in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s sc"
P11-1067,P04-1061,0,0.712703,"lizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substa"
P11-1067,J93-2004,0,0.0435294,"Missing"
P11-1067,D10-1120,0,0.0191741,"Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and ve"
P11-1067,nivre-etal-2006-maltparser,0,0.00840752,"applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (2005) compared two different conversion schemes in German supervised constituency parsing and found one to have positive influence on parsing quality. Dependen"
P11-1067,P06-1033,0,0.363717,"iner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (200"
P11-1067,rambow-etal-2002-dependency,0,0.0689242,", if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possib"
P11-1067,P06-1072,0,0.0773069,"Missing"
P11-1067,N10-1116,0,0.195398,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,P10-1130,0,0.232129,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W10-2902,0,0.312419,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W05-1516,0,0.038097,"Missing"
P11-1067,W06-2904,0,0.0476608,"Missing"
P11-1067,W03-3023,0,0.776919,"”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possible, since different papers use different gold standard annotations even when they are all deriv"
P11-1067,J03-4003,0,\N,Missing
P11-1067,D07-1112,0,\N,Missing
P11-1067,D07-1096,0,\N,Missing
P11-1149,P09-1010,0,0.0651001,"interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different appro"
P11-1149,P07-1036,1,0.198445,"omingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as"
P11-1149,N09-1034,1,0.825093,"and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Re"
P11-1149,N03-1004,0,0.0122379,"iew, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to"
P11-1149,W10-2903,1,0.859398,"on top, preferably by a large margin to allow generalization.The structured learning algorithm can directly use the top ranking predictions of the model (line 8 in Alg. 1) as training data. In this case the underlying algorithm is a structural SVM with squared-hinge loss, using hamming distance as the distance function. We use the cuttingplane method to efficiently optimize the learning process’ objective function. 4 Model Semantic parsing as formulated in Eq. 1 is an inference procedure selecting the top ranked output logical formula. We follow the inference approach in (Roth and Yih, 2007; Clarke et al., 2010) and formalize this process as an Integer Linear Program (ILP). Due to space consideration we provide a brief description, and refer the reader to that paper for more details. Combined The two approaches defined above capture different views of the data, a natural question is then - can these two measures be combined to provide a more powerful estimation? We suggest a third approach which combines the first two approaches. It first uses the score produced by the latter approach 2 Without normalization longer sentences would have more to filter out unlikely candidates, and then ranks the influe"
P11-1149,W99-0613,0,0.0778258,"on: training the model using examples selected according to the model’s parameters (i.e., the top ranking structures) may not generalize much further beyond the existing model, as the training examples will simply reinforce the existing model. The statistics used for confidence estimation are different than those used by the model to create the output structures, and can therefore capture additional information unobserved by the prediction model. This assumption is based on the well established idea of multi-view learning, applied successfully to many NL applications (Blum and Mitchell, 1998; Collins and Singer, 1999). According to this idea if two models use different views of the data, each of them can enhance the learning process of the other. The success of our learning procedure hinges on finding good confidence measures, whose confidence prediction correlates well with the true quality of the prediction. The ability of unsupervised confidence estimation to provide high quality confidence predictions can be explained by the observation that prominent prediction patterns are more likely to be correct. If a non-random model produces a prediction pattern multiple times it is likely to be an indication of"
P11-1149,N04-4028,0,0.0109676,"riterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic pars"
P11-1149,W05-0602,0,0.271332,"result obtained by approximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et"
P11-1149,P06-1115,0,0.721507,"ged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learnin"
P11-1149,P09-1011,0,0.277238,"tracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011)"
P11-1149,P08-2055,0,0.0122683,"tion model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to the best of our knowledge. To compensat"
P11-1149,N06-1020,0,0.0106108,"direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works"
P11-1149,P06-2080,0,0.125091,"pproximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et a"
P11-1149,D09-1001,0,0.44694,"ervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al.,"
P11-1149,P07-1052,1,0.831409,"n unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mi"
P11-1149,P07-1078,1,0.786226,"n unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mi"
P11-1149,W10-2909,1,0.808796,"Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to the best of our knowledge. To compensate for the lack of training data we use a self-training protocol, driven by unsupervised confidence estimation. We demonstrate empirically that our approach results in a high preforming semantic parser and show that confidence"
P11-1149,P07-1076,0,0.0342205,"les for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised lea"
P11-1149,W00-1317,0,0.0141848,"result obtained in any of the learning algorithm iterations (Best), the result obtained by approximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefor"
P11-1149,P11-1145,0,0.0349398,"y, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A"
P11-1149,P10-1098,0,0.159121,"rs. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation"
P11-1149,J07-1003,0,0.00798533,"timate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to ret"
P11-1149,N06-1056,0,0.367679,"result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using"
P11-1149,P07-1121,0,0.586562,"vert NL into a formal MR has countless applications. The term semantic parsing has been used ambiguously to refer to several semantic tasks (e.g., semantic role labeling). We follow the most common definition of this task: finding a mapping between NL input and its interpretation expressed in a welldefined formal MR language. Unlike shallow semantic analysis tasks, the output of a semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system. 1486 Current approaches for this task take a data driven approach (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser — a set of parameterized rules mapping lexical items and syntactic patterns to their MR. Given a sentence, these rules are applied recursively to derive the most probable interpretation. Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data. Unfortunately annotating sentences with their MR is a time consuming task which requi"
P11-1149,P95-1026,0,0.3364,"al step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored"
P11-1149,W06-1604,0,0.0219323,"es its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process"
P11-1149,D07-1071,0,0.903992,"reasons, as the ability to convert NL into a formal MR has countless applications. The term semantic parsing has been used ambiguously to refer to several semantic tasks (e.g., semantic role labeling). We follow the most common definition of this task: finding a mapping between NL input and its interpretation expressed in a welldefined formal MR language. Unlike shallow semantic analysis tasks, the output of a semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system. 1486 Current approaches for this task take a data driven approach (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser — a set of parameterized rules mapping lexical items and syntactic patterns to their MR. Given a sentence, these rules are applied recursively to derive the most probable interpretation. Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data. Unfortunately annotating sentences with their MR is a time co"
P11-1149,P09-1110,0,0.0810929,"nce criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional ste"
P13-1085,D10-1088,0,0.100138,"Missing"
P13-1085,C12-1165,1,0.76043,"Missing"
P13-1085,D12-1065,0,0.0296961,"l Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1 , . . . , yN }, a DPP P defines a probability measure on the set of all subsets of Y, 2Y . Kulesza and Taskar (2012c) restricted their discussion of DDPs to L-ensembles, where the probability of a subset Y ∈ Y is defined through a positive semi-definite matrix L indexed by the elements of Y: alternative models such as Markov Random Fields (MRFs), is efficient, theoretically and practically, for DPPs. 3.2 DPPs are p"
P13-1085,C94-1042,0,0.079275,"nd Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The lin"
P13-1085,P98-1013,0,0.0257233,"larity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and"
P13-1085,N06-2015,0,0.0145547,"; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded"
P13-1085,ienco-etal-2008-automatic,0,0.257449,"e clusters induced by our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler tha"
P13-1085,D07-1017,0,0.0310741,"ic information in SCF acquisition (Korhonen, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable mod´ S´eaghdha, 2010; Ritter and Etzioni, 2010; els (O Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al.,"
P13-1085,W05-0621,0,0.485004,"Missing"
P13-1085,kawahara-kurohashi-2010-acquiring,0,0.296994,"inst a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to SCF acquisition suffer from lack of semantic information which is neede"
P13-1085,A97-1052,0,0.393055,"hen defined over the joint SCF and SP kernel, this new algorithm can be used to induce VCs that are valuable for both tasks. We also contribute by evaluating the value of the clusters induced by our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame indu"
P13-1085,P10-1045,0,0.0450577,"Missing"
P13-1085,C08-1057,1,0.831645,"These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parse"
P13-1085,J05-1004,0,0.114074,"at et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were"
P13-1085,W02-0907,1,0.837735,"SCF and SP kernel, this new algorithm can be used to induce VCs that are valuable for both tasks. We also contribute by evaluating the value of the clusters induced by our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that ident"
P13-1085,P07-1115,1,0.940249,"ent structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unif"
P13-1085,D11-1130,0,0.13274,"Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these"
P13-1085,P10-1044,0,0.025356,"ly syntax-driven acquisition process. Previous works have showed the benefit of hand-coded semantic information in SCF acquisition (Korhonen, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable mod´ S´eaghdha, 2010; Ritter and Etzioni, 2010; els (O Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemant"
P13-1085,P99-1014,0,0.197007,"Missing"
P13-1085,lenci-etal-2008-unsupervised,0,0.143571,"uisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to S"
P13-1085,P08-1057,0,0.353164,"onstrating the benefit our approach. [show]VERB [no evidence to the usefulness of joint learning leaning for these tasks]DOBJ. Finally, VC induction involves clustering together verbs with similar meaning, reflected in similar SCFs and SPs. For example, ”show” in the above examples could get clustered together with ”demonstrate” and ”indicate”. Because these challenging tasks capture complementary information about predicate argument structure, they should be able to inform and support each other. Recently, researchers have begun to investigate the benefits of their joint learning. Schulte im Walde et al. (2008) integrated SCF and VC acquisition and used it for WordNet-based ´ S´eaghdha (2010) presented a SP classification. O “dual-topic” model for SPs that induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervise"
P13-1085,P08-1050,0,0.100784,"in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of"
P13-1085,J06-2001,0,0.430591,"ion about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument stru"
P13-1085,P12-1044,1,0.794435,"Missing"
P13-1085,messiant-etal-2008-lexschem,1,0.890917,"y our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Cur"
P13-1085,D09-1067,1,0.962363,"c, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence pr"
P13-1085,D11-1095,1,0.94025,"d van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of informat"
P13-1085,W05-1002,0,0.387461,"Missing"
P13-1085,J05-3003,0,0.243331,"Missing"
P13-1085,P10-1097,0,0.0501069,"Missing"
P13-1085,W09-0211,0,0.0578131,"Missing"
P13-1085,D11-1097,1,0.892213,"Missing"
P13-1085,W09-0210,1,0.724227,"ntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence provides a framework for"
P13-1085,P11-1156,0,0.0652817,"Missing"
P13-1085,W98-1505,0,\N,Missing
P13-1085,W10-1612,0,\N,Missing
P13-1085,P07-1028,0,\N,Missing
P13-1085,C98-1013,0,\N,Missing
P13-1085,P09-2019,0,\N,Missing
P13-1085,chesley-salmon-alt-2006-automatic,0,\N,Missing
P16-1198,W06-2920,0,0.182577,"Missing"
P16-1198,D07-1101,0,0.383801,"Missing"
P16-1198,W05-1504,0,0.109175,"Missing"
P16-1198,E12-1008,0,0.0406703,"Missing"
P16-1198,Q15-1035,0,0.542351,"Missing"
P16-1198,P06-1063,0,0.0317044,"Missing"
P16-1198,P10-1001,0,0.223319,"Missing"
P16-1198,D10-1125,0,0.0466361,"Missing"
P16-1198,J93-2004,0,0.0536623,"Missing"
P16-1198,P09-1039,0,0.0560953,"Missing"
P16-1198,D11-1022,0,0.0387869,"Missing"
P16-1198,P13-2109,0,0.157969,"earlier transition based work. 2 Undirected MST with the Boruvka Algorithm In this section we define the MST problem in undirected graphs. We then discuss the Burovka algorithm (Boruvka, 1926; Nesetril et al., 2001) which forms the basis for the randomized algorithm of (Karger et al., 1995) we employ in this paper. In the next section we will describe the Karger et al. (1995) algorithm in more details. Problem Definition. For a connected undirected graph G(V, E), where V is the set of n vertices fer here to the classical implementation employed by modern parsers (e.g. (McDonald et al., 2005b; Martins et al., 2013)). 2105 and E the set of m weighted edges, the MST problem is defined as finding the sub-graph of G which is the tree (a connected acyclic graph) with the lowest sum of edge weights. The opposite problem – finding the maximum spanning tree – can be solved by the same algorithms used for the minimum variant by simply negating the graph’s edge weights. Graph Contraction. In order to understand the Boruvka algorithm, let us first define the Graph Contraction operation. For a given undirected ˜ ⊆ E, this opergraph G(V, E) and a subset E ation creates a new graph, GC (VC , EC ). In this new graph,"
P16-1198,E06-1011,0,0.387379,"Missing"
P16-1198,D07-1096,0,0.38472,"Missing"
P16-1198,D12-1067,0,0.738231,"Missing"
P16-1198,N12-1054,0,0.0357112,"Missing"
P16-1198,D10-1001,0,0.0530841,"Missing"
P16-1198,D08-1016,0,0.231051,"Missing"
P16-1198,D12-1030,0,0.0356175,"Missing"
P16-1198,W07-2216,0,0.062195,"Missing"
P16-1198,P05-1012,0,0.16488,"is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n2 ) directed MST inference. 1 Introduction Dependency parsers are major components of a large number of NLP applications. As application models are applied to constantly growing amounts of data, efficiency becomes a major consideration. 1 We refer to parsing approaches that produce only projective dependency trees as projective parsing and to approaches that produce all types of dependency trees as non-projective parsing. 2 Some pruning algorithms require initial construction of the full graph, which r"
P16-1198,H05-1066,0,0.142187,"is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n2 ) directed MST inference. 1 Introduction Dependency parsers are major components of a large number of NLP applications. As application models are applied to constantly growing amounts of data, efficiency becomes a major consideration. 1 We refer to parsing approaches that produce only projective dependency trees as projective parsing and to approaches that produce all types of dependency trees as non-projective parsing. 2 Some pruning algorithms require initial construction of the full graph, which r"
P16-1198,C96-1058,0,\N,Missing
P17-1006,W16-1603,0,0.0323402,"ll and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). RR is supported by the IntelICRI grant: Hybrid Models for Minimally Supervised Information Extraction from Conversations. The authors are grateful to the anonymous reviewers for their helpful suggestions. 64 References Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12:2493–2537. http://dl.acm.org/citation"
P17-1006,ehrmann-etal-2014-representing,0,0.0133747,"r verb conjugation (aspettare / aspettiamo); (3) regular formation of past participle (aspettare / aspettato); and (4) rules regarding grammatical gender (bianco / bianca). Besides these, another set of rules is used for German and Russian: (5) regular declension (e.g., asiatisch / asiatischem). Table 3: Vocabulary sizes and counts of ATTRACT (A) and R EPEL (R) constraints. constraints. These can be extracted from a variety of semantic databases such as WordNet (Fellbaum, 1998), the Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015), or BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) as done in prior work (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016, i.a.). In this work, we investigate another option: extracting constraints without curated knowledge bases in a spectrum of languages by exploiting inherent language-specific properties related to linguistic morphology. This relaxation ensures a wider portability of ATTRACTR EPEL to languages and domains without readily available or adequate resources. Extracting R EPEL Pairs As another source of implicit semantic signals, W also contains words which represent derivational antonyms: e.g., two words that d"
P17-1006,D14-1082,0,0.0120453,"roposed method does not require curated knowledge bases or gold lexicons. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remu"
P17-1006,W14-4340,1,0.939514,"impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c"
P17-1006,N15-1184,0,0.635015,"aduale lenti lente lenta veloce rapido en_book books memoir novel storybooks blurb booked rebook booking rebooked books de_buch sachbuch buches romandebüt büchlein pamphlet bücher büch büche büches büchen it_libro romanzo racconto volumetto saggio ecclesiaste libri libra librare libre librano Table 1: The nearest neighbours of three example words (expensive, slow and book) in English, German and Italian before (top) and after (bottom) morph-fitting. proliferation of word forms in morphologically rich languages. Formalised as an instance of the post-processing semantic specialisation paradigm (Faruqui et al., 2015; Mrkši´c et al., 2016), morphfitting is steered by a set of linguistic constraints derived from simple language-specific rules which describe (a subset of) morphological processes in a language. The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see Fig. 1 and Tab. 2. The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in the transformed vector space, while at the same time pushing antonymous examples away from each other"
P17-1006,E14-1049,0,0.198571,"contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity ratings for 3,500 verb pairs.7 SimLex-"
P17-1006,N16-1077,0,0.0279301,"nally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a"
P17-1006,N15-1070,0,0.0942068,"approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words. 7 Conclusion and Future Work We have presented a novel morph-fitting method which injects morpholog"
P17-1006,D15-1245,0,0.0180451,"require curated knowledge bases or gold lexicons. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio risch"
P17-1006,E17-1049,0,0.024409,"rect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model whi"
P17-1006,D15-1242,0,0.404513,"Missing"
P17-1006,N13-1092,0,0.182887,"Missing"
P17-1006,D16-1235,1,0.903455,"Missing"
P17-1006,P13-1149,0,0.162129,"ig. 1). The final A and R constraint counts are given in Tab. 3. The full sets of rules are available as supplemental material. Extracting ATTRACT Pairs The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word. On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017). For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)). This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations. We define two rules for English, widely recognised as morphologically simple (Avramidis and Koehn, 2008; Cotterell et al"
P17-1006,W13-4066,0,0.0234804,", DE and IT are trained using four variants of the SGNS - LARGE vectors: 1) the initial distributional vectors; 2) morph-fixed vectors; 3) and 4) the two variants of morph-fitted vectors (see Sect. 3). As shown by Mrkši´c et al. (2017b), semantic specialisation of the employed word vectors benThe Dialogue State Tracking Challenge (DSTC) shared task series formalised the evaluation and provided labelled DST datasets (Henderson et al., 2014a,b; Williams et al., 2016). While a plethora of DST models are available based on, e.g., handcrafted rules (Wang et al., 2014) or conditional random fields (Lee and Eskenazi, 2013), the recent DST methodology has seen a shift towards neural62 0.45 0.45 0.85 0.85 SimLex 0.75 0.30 0.70 0.25 0.65 0.20 0.15 Distrib MFix MFit-A MFit-AR SimLex (Spearman’s ρ) SimLex (Spearman’s ρ) 0.35 0.40 0.60 0.35 0.75 0.30 0.70 0.25 0.65 0.20 0.15 (a) English 0.80 Distrib MFix MFit-A MFit-AR DST Performance (Joint) 0.80 DST Performance (Joint) 0.40 DST 0.60 (b) German 0.45 0.85 0.45 0.35 0.75 0.30 0.70 0.25 0.65 0.20 0.15 Distrib MFix MFit-A MFit-AR SimLex SimLex (Spearman’s ρ) SimLex (Spearman’s ρ) 0.80 DST Performance (Joint) 0.40 0.40 0.35 0.30 0.25 0.20 0.15 0.60 (c) Italian Distrib MF"
P17-1006,W14-4337,0,0.147086,"Missing"
P17-1006,W13-4065,0,0.0658382,"Missing"
P17-1006,Q17-1022,1,0.883478,"Missing"
P17-1006,E17-1001,0,0.0359627,"We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediate utterance and conte"
P17-1006,P15-1145,0,0.280709,"Missing"
P17-1006,W15-1521,0,0.150173,"= bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity"
P17-1006,P16-2074,0,0.0671559,"ion into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words. 7 Conclusion and Future Work We have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic co"
P17-1006,W13-3512,0,0.0917221,"ts of the post-processing specialisation algorithm and the constraint selection. Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters)"
P17-1006,Q16-1030,0,0.239528,"ty and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. Related Work Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them"
P17-1006,K16-1006,0,0.171359,"al models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, w"
P17-1006,P15-2070,0,0.0998422,"Missing"
P17-1006,D14-1162,0,0.0854243,"each language are provided in Tab. 3.6 We label these collections of vectors SGNS - LARGE. all of our intrinsic and extrinsic experiments. Morph-fitting Variants We analyse two variants of morph-fitting: (1) using ATTRACT constraints only (MF IT-A), and (2) using both ATTRACT and R EPEL constraints (MF IT-AR). Other Starting Distributional Vectors We also analyse the impact of morph-fitting on other collections of well-known EN word vectors. These vectors have varying vocabulary coverage and are trained with different architectures. We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN"
P17-1006,E17-1029,0,0.030739,"sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediat"
P17-1006,P15-2130,1,0.859159,"Missing"
P17-1006,C14-1015,0,0.0339755,"rithm and the constraint selection. Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al"
P17-1006,N01-1024,0,0.0956175,"constraints such as (rispettosa, irrispettosi) (see Fig. 1). The final A and R constraint counts are given in Tab. 3. The full sets of rules are available as supplemental material. Extracting ATTRACT Pairs The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word. On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017). For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)). This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations. We define two rules for English, widely recognised as morphologically"
P17-1006,P17-1163,1,0.882418,"Missing"
P17-1006,K15-1026,1,0.916649,"Simple Language-Specific Rules Ivan Vuli´c1 , Nikola Mrkši´c1 , Roi Reichart2 Diarmuid Ó Séaghdha3 , Steve Young1 , Anna Korhonen1 1 2 3 University of Cambridge Technion, Israel Institute of Technology Apple Inc. {iv250,nm480,sjy11,alk23}@cam.ac.uk doseaghdha@apple.com roiri@ie.technion.ac.il Abstract 2011). Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.). Morphologically rich languages, in which “substantial grammatical information. . . is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP. This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. In the case of distributional vector space models, morphological complexity brings two challenges to the fore: Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word f"
P17-1006,N16-1060,1,0.847607,"analyse the impact of morph-fitting on other collections of well-known EN word vectors. These vectors have varying vocabulary coverage and are trained with different architectures. We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity"
P17-1006,N15-1186,0,0.023404,"ge-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological v"
P17-1006,E17-1042,1,0.846566,"Missing"
P17-1006,Q15-1025,0,0.601637,"redite) (dressed, undressed) (similar, dissimilar) (formality, informality) (stabil, unstabil) (geformtes, ungeformt) (relevant, irrelevant) (abitata, inabitato) (realtà, irrealtà) (attuato, inattuato) words from the in-batch ATTRACT constraints to be closer to one another than to any other word in the current mini-batch. The second term pushes antonyms away from each other. If (xl , xr ) ∈ BR is the current minibatch of R EPEL constraints, this term can be expressed as follows: The ATTRACT-R EPEL model, proposed by Mrkši´c et al. (2017b), is an extension of the PARAGRAM procedure proposed by Wieting et al. (2015). It provides a generic framework for incorporating similarity (e.g. successful and accomplished) and antonymy constraints (e.g. nimble and clumsy) into pre-trained word vectors. Given the initial vector space and collections of ATTRACT and R EPEL constraints A and R, the model gradually modifies the space to bring the designated word vectors closer together or further apart. The method’s cost function consists of three terms. The first term pulls the ATTRACT examples (xl , xr ) ∈ A closer together. If BA denotes the current mini-batch of ATTRACT examples, this term can be expressed as: X Germ"
P17-1006,P16-1230,1,0.801978,"Missing"
P17-1006,D16-1157,0,0.0330579,"→ 66.3 (MF IT-AR), setting a new state-of-the-art score for both datasets. The morph-fixed vectors do not enhance DST performance, probably because fixing word vectors to their highest frequency inflectional form eliminates useful semantic content encoded in the original vectors. On the other hand, morph-fitting makes use of this information, supplementing it with semantic relations between different morphological forms. These conclusions are in line with the SimLex gains, where morph-fitting outperforms both distributional and morph-fixed vectors. 63 spaces for extrinsic tasks such as DST. 6 Wieting et al., 2016; Verwimp et al., 2017, i.a.). In contrast to prior work, our model decouples the use of morphological information, now provided in the form of inflectional and derivational rules transformed into constraints, from the actual training. This pipelined approach results in a simpler, more portable model. In spirit, our work is similar to Cotterell et al. (2016b), who formulate the idea of post-training specialisation in a generative Bayesian framework. Their work uses gold morphological lexicons; we show that competitive performance can be achieved using a non-exhaustive set of simple rules. Our"
P17-1006,P15-2111,0,0.0142836,"models from the literature in lieu of ATTRACT-R EPEL using the same set of “morphological” synonymy and antonymy constraints. We compare ATTRACT-R EPEL to the retrofitting model Further Discussion The simplicity of the used language-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existi"
P17-1006,W10-1401,0,0.0148159,"rsity of Cambridge Technion, Israel Institute of Technology Apple Inc. {iv250,nm480,sjy11,alk23}@cam.ac.uk doseaghdha@apple.com roiri@ie.technion.ac.il Abstract 2011). Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.). Morphologically rich languages, in which “substantial grammatical information. . . is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP. This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. In the case of distributional vector space models, morphological complexity brings two challenges to the fore: Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for languag"
P17-1006,P10-1040,0,0.0585254,"ervation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio rischioso costosa costosa costose costosi dispendioso dispendiose en_slow fast slow"
P17-1006,P14-2089,0,0.265019,"nd naturally extends to constraints from other sources (e.g., WordNet) in future work. Another practical difference is that we focus on similarity and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. Related Work Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological"
P17-1006,P13-1118,0,0.0513226,"Missing"
P17-1006,E17-1040,0,0.0904069,"Missing"
P17-1006,E17-2033,0,0.0793456,"uct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediate utterance and context representations. Th"
P17-1006,D13-1141,0,0.035066,"ns. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio rischioso costosa costosa costose costosi dis"
P17-1006,P16-1024,1,0.914352,"Missing"
P17-1006,W13-3520,0,\N,Missing
P17-1006,J15-4004,1,\N,Missing
P17-1006,P08-1087,0,\N,Missing
P17-1006,P14-2050,0,\N,Missing
P17-1006,P14-2131,0,\N,Missing
P17-1006,D16-1047,0,\N,Missing
P17-1006,P16-1156,0,\N,Missing
P17-1006,P16-2084,1,\N,Missing
P17-1006,N15-1140,0,\N,Missing
P17-1155,P05-1074,0,0.0664215,"ses system (Koehn et al., 2007) and an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task. Paraphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they are close in nature to our task. Quirk et al. (2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005). Xu et al. (2015) presented the task of paraphrase generation while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT. Work on paraphrasing and summarization is often evaluated using MT evaluation measures such as BLEU. As BLEU is precision-oriented, complementary recall-oriented measures are often used as well. A prominent example is ROUGE (Lin, 2004), a family of measures used mostly for evaluation in automatic summarization: candidate summaries are scored according to the fraction of n-grams fro"
P17-1155,W14-2600,0,0.559168,"rcasm has dramatically increased over the past few years. This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014). Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; Gonz´alez-Ib´anez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted. Therefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT, paraphrasing and text summarization. Sarcasm Detection Recent computational work on sarcasm revolves mainly around detection. Due to the large volume of detection work, we survey only several representative examples. Tsur et al. (2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-revie"
P17-1155,P11-1020,0,0.0235088,"ion while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT. Work on paraphrasing and summarization is often evaluated using MT evaluation measures such as BLEU. As BLEU is precision-oriented, complementary recall-oriented measures are often used as well. A prominent example is ROUGE (Lin, 2004), a family of measures used mostly for evaluation in automatic summarization: candidate summaries are scored according to the fraction of n-grams from the human references they contain. We also utilize PINC (Chen and Dolan, 2011), a measure which rewards paraphrases for being different from their source, by introducing new n-grams. PINC is often combined with BLEU due to their complementary nature: while PINC rewards n-gram novelty, BLEU rewards similarity to the reference. The highest correlation with human judgments is achieved by the product of PINC with a sigmoid function of BLEU (Chen and Dolan, 2011). 3 A Parallel Sarcastic Tweets Corpus To properly investigate our task, we collected a dataset, first of its kind, of sarcastic tweets and their non-sarcastic (honest) interpretations. This data, as well as the inst"
P17-1155,W10-2914,0,0.552707,"eterson et al., 2012) literature. In computational work, the interest in sarcasm has dramatically increased over the past few years. This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014). Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; Gonz´alez-Ib´anez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted. Therefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT, paraphrasing and text summarization. Sarcasm Detection Recent computational work on sarcasm revolves mainly around detection. Due to the large volume of detection work, we survey only several representative examples. Tsur et al. (2010) and Davidov et al. (2010) presente"
P17-1155,W11-2107,0,0.0215049,"ation. Machine Translation We approach our task as one of monolingual MT, where we translate sarcastic English into non-sarcastic English. Therefore, our starting point is the application of MT techniques and evaluation measures. The three major approaches to MT are phrase based (Koehn et al., 2007), syntax based (Koehn et al., 2003) and the recent neural approach. For automatic MT evaluation, often an n-gram co-occurrence based scoring is performed in order to measure the lexical closeness between a candidate and a reference translations. Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al., 2002), which represents precision: the fraction of n-grams from the machine generated translation that also appear in the human reference. Here we employ the phrase based Moses system (Koehn et al., 2007) and an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task. Paraphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they ar"
P17-1155,esuli-sebastiani-2006-sentiwordnet,0,0.0264305,"adness, sorrow, fear, disappointment, regret, danger... Table 4: Examples of two positive and two negative clusters created by the SIGN algorithm. timent word is replaced with its cluster 7 and the transformed data is fed into an MT system (Moses in this work), at both its training and test phases. Consequently, at test time the MT system outputs non-sarcastic utterances with clusters replacing sentiment words. Finally, SIGN performs a declustering process on these MT outputs, replacing sentiment clusters with suitable words. In order to detect the sentiment of words, we turn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al., 1990). Using SentiWordNet’s positivity and negativity scores, we collect from our training data a set of distinctly positive words (∼ 70) and a set of distinctly negative words (∼ 160).8 We then utilize the pre-trained dependency-based word embeddings of Levy and Goldberg (2014)9 and cluster each set using the k-means algorithm with L2 distance. We aim to have ten words on average in each cluster, and so the positive set is clustered into 7 clusters, and the negative set into 16 clusters. Table 4 presents examples from our clusters. Upon re"
P17-1155,P11-2102,0,0.0758914,"Missing"
P17-1155,W11-2123,0,0.0137847,"then analyze the performance of these two systems, and based on our conclusions we design our SIGN model. 4 For example, we consider ”Best day ever #sarcasm” and its interpretation ”Worst day ever” to agree on the sentiment, despite the use of opposite sentiment words. Phrase Based MT We employ Moses5 , using word alignments extracted by GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diagfinal strategy. We use phrases of up to 8 words to build our phrase table, and do not filter sentences according to length since tweets contain at most 140 characters. We employ the KenLM algorithm (Heafield, 2011) for language modeling, and train it on the non-sarcastic tweet interpretations (the target side of the parallel corpus). Neural Machine Translation We use GroundHog, a publicly available implementation of an RNN encoder-decoder, with LSTM hidden states.6 Our encoder and decoder contain 250 hidden units each. We use the minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model, where each SGD update is computed using a minibatch of 16 utterances. Following Sutskever et al. (2014), we use beam search for test time decoding. Henceforth we ref"
P17-1155,P07-2045,0,0.0180826,"ture design. Moreover, it presents fundamental notions, such as the sentiment polarity of the sarcastic utterance and of its interpretation, that we adopt. Finally, when utterances are not marked for sarcasm as in the Twitter domain, or when these labels are not reliable, detection is a necessary step before interpretation. Machine Translation We approach our task as one of monolingual MT, where we translate sarcastic English into non-sarcastic English. Therefore, our starting point is the application of MT techniques and evaluation measures. The three major approaches to MT are phrase based (Koehn et al., 2007), syntax based (Koehn et al., 2003) and the recent neural approach. For automatic MT evaluation, often an n-gram co-occurrence based scoring is performed in order to measure the lexical closeness between a candidate and a reference translations. Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al., 2002), which represents precision: the fraction of n-grams from the machine generated translation that also appear in the human reference. Here we employ the phrase based Moses system (Koehn et al., 2007) and an RNN-encoder-decod"
P17-1155,P14-2050,0,0.0123033,"stem outputs non-sarcastic utterances with clusters replacing sentiment words. Finally, SIGN performs a declustering process on these MT outputs, replacing sentiment clusters with suitable words. In order to detect the sentiment of words, we turn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al., 1990). Using SentiWordNet’s positivity and negativity scores, we collect from our training data a set of distinctly positive words (∼ 70) and a set of distinctly negative words (∼ 160).8 We then utilize the pre-trained dependency-based word embeddings of Levy and Goldberg (2014)9 and cluster each set using the k-means algorithm with L2 distance. We aim to have ten words on average in each cluster, and so the positive set is clustered into 7 clusters, and the negative set into 16 clusters. Table 4 presents examples from our clusters. Upon receiving a sarcastic tweet, at both training and test, SIGN searches it for sentiment words according to the positive and negative sets. If such 7 This means that we replace a word with cluster-j where j is the number of the cluster to which the word belongs. 8 The scores are in the [0,1] range. We set the threshold of 0.6 for both"
P17-1155,maynard-greenwood-2014-cares,0,0.025952,"well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature. In computational work, the interest in sarcasm has dramatically increased over the past few years. This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014). Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; Gonz´alez-Ib´anez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted. Therefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT, paraphrasing and text summarization. Sarcasm Detection Recent computational work on sarcasm revolves mainly around detection. Due to the"
P17-1155,J03-1002,0,0.0175157,"s about the generation of one English sentence given another, a natural starting point is treating it as monolingual MT. We hence begin with utilizing two widely used MT systems, representing two different approaches: Phrase Based MT vs. Neural MT. We then analyze the performance of these two systems, and based on our conclusions we design our SIGN model. 4 For example, we consider ”Best day ever #sarcasm” and its interpretation ”Worst day ever” to agree on the sentiment, despite the use of opposite sentiment words. Phrase Based MT We employ Moses5 , using word alignments extracted by GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diagfinal strategy. We use phrases of up to 8 words to build our phrase table, and do not filter sentences according to length since tweets contain at most 140 characters. We employ the KenLM algorithm (Heafield, 2011) for language modeling, and train it on the non-sarcastic tweet interpretations (the target side of the parallel corpus). Neural Machine Translation We use GroundHog, a publicly available implementation of an RNN encoder-decoder, with LSTM hidden states.6 Our encoder and decoder contain 250 hidden units each. We use the minibatch stochastic gradient"
P17-1155,P02-1040,0,0.118689,"of monolingual MT, where we translate sarcastic English into non-sarcastic English. Therefore, our starting point is the application of MT techniques and evaluation measures. The three major approaches to MT are phrase based (Koehn et al., 2007), syntax based (Koehn et al., 2003) and the recent neural approach. For automatic MT evaluation, often an n-gram co-occurrence based scoring is performed in order to measure the lexical closeness between a candidate and a reference translations. Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al., 2002), which represents precision: the fraction of n-grams from the machine generated translation that also appear in the human reference. Here we employ the phrase based Moses system (Koehn et al., 2007) and an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task. Paraphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they are close in nature to our task. Quirk et al. (2004)"
P17-1155,J04-3002,0,0.0174575,"nd Donald Trump are. Since sarcasm is a refined and indirect form of speech, its interpretation may be challenging for certain populations. For example, studies show that children with deafness, autism or Asperger’s Syndrome struggle with non literal communication such as sarcastic language (Peterson et al., 2012; Kimhi, 2014). Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization (Popescu et al., 2005; Pang and Lee, 2008; Wiebe et al., 2004). Extracting the honest meaning behind the sarcasm may alleviate such issues. In order to design an automatic sarcasm interpretation system, we first rely on previous work in established similar tasks (section 2), particularly machine translation (MT), borrowing algorithms as well as evaluation measures. In section 4 we discuss the automatic evaluation measures we apply in our work and present human based measures for: (a) the fluency of a generated nonsarcastic utterance, (b) its adequacy as interpretation of the original sarcastic tweet’s meaning, and (c) whether or not it captures the senti"
P17-1155,W10-4223,0,0.0594371,"Missing"
P17-1155,S15-2001,0,0.0163859,"an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task. Paraphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they are close in nature to our task. Quirk et al. (2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005). Xu et al. (2015) presented the task of paraphrase generation while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT. Work on paraphrasing and summarization is often evaluated using MT evaluation measures such as BLEU. As BLEU is precision-oriented, complementary recall-oriented measures are often used as well. A prominent example is ROUGE (Lin, 2004), a family of measures used mostly for evaluation in automatic summarization: candidate summaries are scored according to the fraction of n-grams from the human refere"
P17-1155,D13-1066,0,0.351504,"Missing"
P17-1155,Y13-1035,0,0.0299156,"directions for our task, regarding both algorithms and evaluation. 2 Related Work The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature. In computational work, the interest in sarcasm has dramatically increased over the past few years. This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014). Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; Gonz´alez-Ib´anez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted. Therefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT,"
P18-1084,D17-1105,0,0.0222852,"guages by optimizing a contrastive loss function. Furthermore, Rajendran et al. (2016) extend the work of Chandar et al. (2016) and propose to use a pivot representation in multimodal multilingual setups, with English representations serving as the pivot. While these works learn shared multimodal multilingual vector spaces, we demonstrate improved performance with our models (see §7). Finally, although not directly comparable, recent work in neural machine translation has constructed models that can translate image descriptions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which suppor"
P18-1084,J75-4040,0,0.644684,"Missing"
P18-1084,W16-3210,0,0.0396034,"Missing"
P18-1084,E14-1049,0,0.0419552,"tions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al., 2015). In this work, we propose to use (D)PCCA, which organically supports our setup: it conditions the two (textual) views on a shared (visual) view. CCA-based methods (including PCCA) require the estimation of covariance matrices over all training data (Kessy et al., 2017). This hinders the use of DNNs with these models, as DNNs are typically trained via stochastic optimization over mini911 batches on very large training sets. To address this limitation, va"
P18-1084,N10-1011,0,0.0379506,"ing test time inference.1 1 Introduction Research in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005). In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vi"
P18-1084,D16-1044,0,0.0425216,"ultimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017;"
P18-1084,D15-1070,0,0.428522,"Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al., 2015). In this work, we propose to use (D)PCCA, which organically supports our setup: it conditions the two (textual) views on a shared (visual) view. CCA-based methods (including PCCA) require the estimation of covariance matrices over all training data (Kessy et al., 2017). This hinders the use of DNNs with these models, as DNNs are typically trained via stochastic optimization over mini911 batches on very large training sets. To address this limitation, various optimization methods for Deep CCA were proposed. Andrew et al. (2013) use L-BFGS (Byrd et al., 1995) over all tra"
P18-1084,N16-1022,0,0.0272562,"ot surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017; Rajendran et al., 2016). In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information. This additional perceptual modality bridges the gap between languages and reveals latent connections between concept"
P18-1084,D17-1303,0,0.506495,"; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017; Rajendran et al., 2016). In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information. This additional perceptual modality bridges the gap between languages and reveals latent connections between concepts in the multilingual setup. The shared visual information in our work takes the form of images with word-level tags or sentence-level descriptions assigned in more than one language. We propose a deep neural architecture termed Deep Partial Canonical Correlation Analysis (DPCCA) based on the Partial CCA (PCCA) method (Rao"
P18-1084,W17-6809,1,0.882302,"Missing"
P18-1084,Q14-1023,1,0.856574,"EMB 0.582 0.160 0.306 0.407 0.164 0.285 Table 3: Results on EN and DE SimLex-999 (POS-based evaluation). All scores are Spearman’s rank correlations. INIT EMB refers to initial pre-trained monolingual word embeddings (see §6). EN-DE WIW EN-IT WIW EN-RU WIW EN DE EN IT EN RU DPCCA (A) DPCCA (B) PCCA 0.398 0.405 0.374 0.400 0.400 0.301 0.412 0.413 0.370 0.429 0.427 0.386 0.404 0.413 0.374 0.407 0.402 0.374 DCCA NOI GCCA 0.390 0.395 0.398 0.386 0.413 0.414 0.422 0.407 0.407 0.412 0.398 0.396 INIT EMB 0.321 0.278 0.321 0.361 0.321 0.385 Model more abstract than nouns (Hartmann and Søgaard, 2017; Hill et al., 2014). Considering the fact that SimLex-999 consists of 666 noun pairs, 222 verb pairs and 111 adjective pairs, this is the reason that the gains of DPCCA over the strongest baselines across the entire evaluation set are more modest (Table 4). We note again that the same patterns presented in Table 3 for EN-DE – more prominent verb and adjective gains and a smaller gain on nouns – also hold for EN-IT and EN-RU (see the supplementary material). Table 4: Results (Spearman rank correlation) of our models and the strongest baselines on Multilingual SimLex-999 (all data). a selection of strongest baseli"
P18-1084,P15-2019,0,0.0433432,"Missing"
P18-1084,J15-4004,1,0.89777,"e, we propose an effective optimization algorithm for DPCCA, inspired by the work of Wang et al. (2015b) on Deep CCA (DCCA) optimization. We evaluate our DPCCA architecture on two semantic tasks: 1) multilingual word similarity and 2) cross-lingual image description retrieval. For the former, we construct and provide to the community a new Word-Image-Word (WIW) dataset containing bilingual lexicons for three languages with shared images for 5K+ concepts. WIW is used as training data for word similarity experiments, while evaluation is conducted on the standard multilingual SimLex-999 dataset (Hill et al., 2015; Leviant and Reichart, 2015). The results reveal stable improvements over a large space of non-deep and deep CCA-style baselines in both tasks. Most importantly, 1) PCCA is overall better than other methods which do not use the additional perceptual view; 2) DPCCA outperforms PCCA, indicating the importance of nonlinear transformations modeled through DNNs; 3) DPCCA outscores DCCA, again verifying the importance of conditioning multilingual text embedding induction on the shared visual view; and 4) DPCCA outperforms two recent multi-modal bilingual models which also leverage visual informatio"
P18-1084,P16-1227,0,0.0214066,"ion. Furthermore, Rajendran et al. (2016) extend the work of Chandar et al. (2016) and propose to use a pivot representation in multimodal multilingual setups, with English representations serving as the pivot. While these works learn shared multimodal multilingual vector spaces, we demonstrate improved performance with our models (see §7). Finally, although not directly comparable, recent work in neural machine translation has constructed models that can translate image descriptions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2"
P18-1084,Q15-1016,0,0.0416597,"owing values: {2,3,4,5} for number of layers, {tanh, sigmoid, ReLU} as the activation functions (we use the same activation function in all the layers of the same network), {64,128,256} for minibatch size, {0.001,0.0001} for learning rate, and {128,256} for L (the size of the output vectors). The dimensions of all mid-layers are set to the input size. We use the Adam optimizer (Kingma and Ba, 2015), with the number of epochs set to 300. For all participating models, we report test performance of the best hyperparameter on the validation set. For word similarity, following a standard practice (Levy et al., 2015; Vuli´c et al., 2017) we tune all models on one half of the SimLex data and evaluate on the other half, and vice versa. The reported score is the average of the two halves. Similarity scores for all tasks were computed using the cosine similarity measure. 7 Results and Discussion Cross-lingual Image Description Retrieval We report two standard evaluation metrics: 1) Recall at 1 (R@1) scores, and 2) the sentence-level BLEU+1 metric (Lin and Och, 2004), a variant of BLEU which smooths terms for higher-order n-grams, making it more suitable for evaluating short sentences. The scores for the retr"
P18-1084,C04-1072,0,0.016355,"cipating models, we report test performance of the best hyperparameter on the validation set. For word similarity, following a standard practice (Levy et al., 2015; Vuli´c et al., 2017) we tune all models on one half of the SimLex data and evaluate on the other half, and vice versa. The reported score is the average of the two halves. Similarity scores for all tasks were computed using the cosine similarity measure. 7 Results and Discussion Cross-lingual Image Description Retrieval We report two standard evaluation metrics: 1) Recall at 1 (R@1) scores, and 2) the sentence-level BLEU+1 metric (Lin and Och, 2004), a variant of BLEU which smooths terms for higher-order n-grams, making it more suitable for evaluating short sentences. The scores for the retrieval task with all models are summarized in Table 2. R@1 BLEU+1 Model EN→DE DE→EN EN→DE DE→EN DPCCA (Variant A) DPCCA (Variant B) DPCCA(B)+DCCA NOI (concat) DCCA NOI (Wang et al., 2015b) DCCA SDL (Chang et al., 2017) DCCA (Wang et al., 2015a) DCCAE (Wang et al., 2015a) IMG PIVOT (Gella et al., 2017) BCN (Rajendran et al., 2016) PCCA (Rao, 1969) CCA (Hotelling, 1936) GCCA (Funaki and Nakayama, 2015) NCCA (Michaeli et al., 2016) PPCCA (Mukuta and Harad"
P18-1084,W16-2360,0,0.0227027,"ran et al. (2016) extend the work of Chandar et al. (2016) and propose to use a pivot representation in multimodal multilingual setups, with English representations serving as the pivot. While these works learn shared multimodal multilingual vector spaces, we demonstrate improved performance with our models (see §7). Finally, although not directly comparable, recent work in neural machine translation has constructed models that can translate image descriptions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al.,"
P18-1084,N15-1028,0,0.0190758,"tures of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al., 2015). In this work, we propose to use (D)PCCA, which organically supports our setup: it conditions the two (textual) views on a shared (visual) view. CCA-based methods (including PCCA) require the estimation of covariance matrices over all training data (Kessy et al., 2017). This hinders the use of DNNs with these models, as DNNs are typically trained via stochastic optimization over mini911 batches on very large training sets. To address this limitation, various optimization methods for Deep"
P18-1084,P16-4010,0,0.0190778,"ord similarity task. WIW contains three bilingual lexicons (EN-DE, EN-IT, EN-RU) with images shared between words in a lexicon entry. Each WIW entry is a triplet: an English word, its translation in DE/IT/RU, and a set of images relevant to the pair. English words were taken from the January 2017 Wikipedia dump. After removing stop words and punctuation, we extract the 6,000 most frequent words from the cleaned corpus not present in SimLex. DE/IT/RU words were obtained semiautomatically from the EN words using Google Translate. The images are crawled from the Bing search engine using MMFeat9 (Kiela, 2016) by querying the EN words only. Following the suggestions from the study of Kiela et al. (2016), we save the top 20 images as relevant images.10 Table 1 provides a summary of the WIW dataset. The dataset contains both concrete and abstract words, and words of different POS tags.11 This property has an influence on the image collection: similar to Kiela et al. (2014), we have noticed that images of more concrete concepts are less dispersed (see also examples from Figure 2). 6 Experimental Setup Data Preprocessing and Embeddings For the sentence-level task, all descriptions were lower9 https://g"
P18-1084,D14-1005,0,0.408971,".1 1 Introduction Research in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005). In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu e"
P18-1084,P14-2135,0,0.0603706,"xtract the 6,000 most frequent words from the cleaned corpus not present in SimLex. DE/IT/RU words were obtained semiautomatically from the EN words using Google Translate. The images are crawled from the Bing search engine using MMFeat9 (Kiela, 2016) by querying the EN words only. Following the suggestions from the study of Kiela et al. (2016), we save the top 20 images as relevant images.10 Table 1 provides a summary of the WIW dataset. The dataset contains both concrete and abstract words, and words of different POS tags.11 This property has an influence on the image collection: similar to Kiela et al. (2014), we have noticed that images of more concrete concepts are less dispersed (see also examples from Figure 2). 6 Experimental Setup Data Preprocessing and Embeddings For the sentence-level task, all descriptions were lower9 https://github.com/douwekiela/mmfeat. Offensive words and images are manually cleaned. 11 POS tag information is taken from the NLTK toolkit for the English words. 10 Figure 2: WIW examples from each of the three bilingual lexicons. Note that the designated words can be either abstract (true), express an action (dance) or be more concrete (plant). cased and tokenized. Each s"
P18-1084,D16-1043,0,0.0323394,"Missing"
P18-1084,D15-1015,1,0.933823,"Missing"
P18-1084,J99-4009,0,0.0580764,"CA outperforms two recent multi-modal bilingual models which also leverage visual information (Gella et al., 2017; Rajendran et al., 2016). 2 Related Work This work is related to two research threads: 1) multi-modal models that combine vision and language, with a focus on multilingual settings; 2) correlational multi-view models based on CCA which learn a shared vector space for multiple views. Multi-Modal Modeling in Multilingual Settings Research in cognitive science suggests that human meaning representations are grounded in our perceptual system and sensori-motor experience (Harnad, 1990; Lakoff and Johnson, 1999; Louwerse, 2011). Visual context serves as a useful crosslingual grounding signal (Bruni et al., 2014; Glavaˇs et al., 2017) due to its language invariance, even enabling the induction of word-level bilingual semantic spaces solely through tagged images obtained from the Web (Bergsma and Van Durme, 2011; Kiela et al., 2015). Vuli´c et al. (2016) combine text embeddings with visual features via simple techniques of concatenation and averaging to obtain bilingual multi-modal representations, with noted improvements over text-only embeddings on word similarity and bilingual lexicon extraction. H"
P18-1084,N15-1016,0,0.110189,"ch in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005). In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual"
P18-1084,Q17-1022,1,0.905072,"Missing"
P18-1084,N16-1021,0,0.351509,"; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017; Rajendran et al., 2016). In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information. This additional perceptual modality bridges the gap between languages and reveals latent connections between concepts in the multilingual setup. The shared visual information in our work takes the form of images with word-level tags or sentence-level descriptions assigned in more than one language. We propose a deep neural architecture termed Deep Partial Canonical Correlation Analysis (DPCCA) based on the Partial CCA (PCCA) method (Rao, 1969). To the best of o"
P18-1084,N15-1058,0,0.0604349,"Missing"
P18-1084,P16-2031,1,0.909024,"Missing"
P18-1084,K17-1013,1,0.893802,"Missing"
P18-1084,Q14-1006,0,0.042945,"h query. In addition, in our setup, images are not available during inference: retrieval is performed based solely on text queries. This enables a fair comparison between our model and many baseline models that cannot represent images and text in a shared space. Moreover, it allows us to test our model in the realistic setup where images are not available at test time. To avoid the use of images at retrieval time with DPCCA, we perform the retrieval on F (X) and G(Y ), rather than on F (X|Z) and G(Y |Z) (see §3.2). We use the Multi30K dataset (Elliott et al., 2016), originated from Flickr30K (Young et al., 2014) that is comprised of Flicker images described with 1-5 English descriptions per image. Multi30K adds Update parameters: WF ← WF − η∇WF , UH ← UH − η∇UH 1 ] ˆ − 2 F |H, and compute ∇VG , ∇UH Fix F |H = Σ F F |H with respect to: ] min |b1t |kG|H − F |Hk2F VG ,UH Update parameters: VG ← VG − η∇VG , UH ← UH − η∇UH end for Output: (WF , VG , UH ) German descriptions to a total of 30,014 images: most were written independently of the English descriptions, while some are direct translations. Each image is associated with one English and one German description. We rely on the original Multi30K splits"
P18-1128,2005.mtsummit-papers.11,0,0.0362175,"ests described in § 3, that are commonly used in NLP setups, is that the data samples are independent and identically distributed. This assumption, however, is rarely true in NLP setups. For example, the popular WSJ Penn Treebank corpus (Marcus et al., 1993) consists of 2,499 articles from a three year Wall Street Journal (WSJ) collection of 98,732 stories. Obviously, some of the sentences included in the corpus come from the same article, were written by the same author or were reviewed before publication by the same editor. As another example, many sentences in the Europarl parallel corpus (Koehn, 2005) that is very popular in the machine translation literature are taken from the same parliament discussion. An independence assumption between the sentences in these corpora is not likely to hold. This dependence between test examples violates the conditions under which the theoretical guarantees of the various tests were developed. The impact of this phenomenon on our results is hard to quantify, partly because it is hard to quantify the nature of the dependence between test set examples in NLP datasets. Some papers are even talking about abandoning the null hypothesis statistical significance"
P18-1128,W05-0909,0,0.0693465,"r considerations, on the distribution of the test statistics, δ(X). From equation 1 it is clear that δ(X) depends on the evaluation measure M. We hence turn to discuss the evaluation measures employed in NLP. In § 4 we analyze the (long) ACL and TACL 2017 papers, and observe that the most commonly used evaluation measures are the 12 measures that appear in Table 1. Notice that seven of these measures: Accuracy, Precision, Recall, F-score, Pearson and Spearman correlations and Perplexity, are not specific to NLP. The other five measures: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), UAS and LAS (K¨ubler et al., 2009), are unique measures that were developed for NLP applications. BLEU and METEOR are standard evaluation measures for machine translation, ROUGE for extractive summarization, and UAS and LAS for dependency parsing. While UAS and LAS are in fact accuracy measures, BLEU, ROUGE and METEOR are designed for tasks where there are several possible outputs - a characteristic property of several NLP tasks. In machine translation, for example, a sentence in one language can be translated in multiple ways to another language. Consequently, BLEU takes an n-gram based app"
P18-1128,P17-1000,0,0.3072,"dards also affect the way significance testing should be performed. An NLP-specific discussion of significance testing is hence in need. In § 3 we discuss the considerations to be made in order to select the proper statistical significance test in NLP setups. We propose a simple decision tree algorithm for this purpose, and survey the prominent significance tests – parametric and nonparametric – for NLP tasks and data. In § 4 we survey the current evaluation and significance testing practices of the community. We provide statistics collected from the long papers of the latest ACL proceedings (Barzilay and Kan, 2017) as well as from the papers published in the TACL journal during 2017. Our analysis reveals that there is still a room for improvement in the way statistical significance is used in papers published in our top-tier publication venues. Particularly, a large portion of the surveyed papers do not test the significance of their results, or use incorrect tests for this purpose. Finally, in § 5 we discuss open issues. A particularly challenging problem is that while most significance tests assume the test set consists of independent observations, most NLP datasets consist of dependent data points. F"
P18-1128,D12-1091,0,0.27431,"re it is theoretically justified to employ the t-test is described in (Sethuraman, 1963). The authors prove that for large enough data, the sampling distribution of a certain function of the Pearson’s correlation coefficient follows the Student’s t-distribution with n − 2 degrees of freedom. With the recent surge in word similarity research with word embedding models, this result is of importance to our community. For other evaluation measures, such as F-score, BLEU, METEOR and ROUGE that do not compute means, the common practice is to assume that they are not normally distributed (Yeh, 2000; Berg-Kirkpatrick et al., 2012). We believe this issue requires a further investigation and suggest that it may be best to rely on the normality tests discussed in § 3.1 when deciding whether or not to employ the t-test. 1386 3.2.2 Non-parametric Tests When the test statistic distribution is unknown, non-parametric significance testing should be used. The non-parametric tests that are commonly used in NLP setups can be divided into two families that differ with respect to their statistical power and computational complexity. The first family consists of tests that do not consider the actual values of the evaluation measures"
P18-1128,W06-1615,0,0.0557809,"n examples. The null hypothesis for this test states that the marginal probability for each outcome (label one or label two) is the same for both algorithms. That is, when applying both algorithms on the same data we would expect them to be correct/incorrect on the same proportion of items. Under the null hypothesis, with a sufficiently large number of disagreements between the algorithms, the test statistic has a distribution of χ2 with one degree of freedom. This test is appropriate for binary classification tasks, and has been indeed used in such NLP works (e.g. sentiment classificaiton, (Blitzer et al., 2006; Ziser and Reichart, 2017)). The Cochran’s Q test (Cochran, 1950) generalizes the McNemar’s test for multi-class classification setups. The sign test and its variants consider only pairwise ranks: which algorithm performs better on each test example. In NLP setups, however, we also have access to the evaluation measure values, and this allows us to rank the differences between the algorithms. The Wilcoxon signed-rank test makes use of such a rank and hence, while it does not consider the evaluation measure values, it is more powerful than the sign test and its variants. Wilcoxon signed-rank t"
P18-1128,P17-1064,0,0.0138848,"otstrap test This test is very similar to approximate randomization of the permutation test, with the difference that the sampling is done with replacements (i.e., an example from the original test data can appear more than once in a sample). The idea of bootstrap is to use the samples as surrogate populations, for the purpose of approximating the sampling distribution of the statistic. The p-value is calculated in a similar manner to the permutation test. Bootstrap was used with a variety of NLP tasks, including machine translation, text summarization and semantic parsing (e.g. (Koehn, 2004; Li et al., 2017; Wu et al., 2017; Ouchi et al., 2017)). The test is less effective for small test sets, as it assumes that the test set distribution does not deviate too much from the population distribution. Clearly, Sampling-based methods are computationally intensive and can be intractable for large datasets, even with modern computing power. In such cases, sampling-free methods form an available alternative. 3.3 Significance Test Selection With the discussion of significance test families - parametric vs. non-parametric (§ 3.1), and the properties of the actual significance tests (§ 3.2) we are now ready"
P18-1128,W04-1013,0,0.106119,"ciation for Computational Linguistics observed in an individual comparison, is not coincidental. Statistical significance testing of each individual comparison is the basic building block of replicability analysis – its accurate performance is a pre-condition for any multiple dataset analysis. Statistical significance testing (§ 2) is a well researched problem in the statistical literature. However, the unique structured nature of natural language data is reflected in specialized evaluation measures such as BLEU (machine translation, (Papineni et al., 2002)), ROUGE (extractive summarization, (Lin, 2004)), UAS and LAS (dependency parsing, (K¨ubler et al., 2009)). The distribution of these measures is of great importance to statistical significance testing. Moreover, certain properties of NLP datasets and the community’s evaluation standards also affect the way significance testing should be performed. An NLP-specific discussion of significance testing is hence in need. In § 3 we discuss the considerations to be made in order to select the proper statistical significance test in NLP setups. We propose a simple decision tree algorithm for this purpose, and survey the prominent significance test"
P18-1128,J93-2004,0,0.0630275,"this section we would like to point on two issues that remain open even after our investigation. We hope that bringing these issues to the attention of the research community will encourage our fellow researchers to come up with appropriate solutions. The first open issue is that of dependent observations. An assumption shared by the statistical significance tests described in § 3, that are commonly used in NLP setups, is that the data samples are independent and identically distributed. This assumption, however, is rarely true in NLP setups. For example, the popular WSJ Penn Treebank corpus (Marcus et al., 1993) consists of 2,499 articles from a three year Wall Street Journal (WSJ) collection of 98,732 stories. Obviously, some of the sentences included in the corpus come from the same article, were written by the same author or were reviewed before publication by the same editor. As another example, many sentences in the Europarl parallel corpus (Koehn, 2005) that is very popular in the machine translation literature are taken from the same parliament discussion. An independence assumption between the sentences in these corpora is not likely to hold. This dependence between test examples violates the"
P18-1128,P07-1005,0,0.0250987,"rawn from distributions with equal medians. The test statistic is the number of examples for which algorithm A is better than algorithm B, and the null hypothesis states that given a new pair of measurements (e.g. evaluations (ai , bi ) of the two algorithms on a new test example), then ai and bi are equally likely to be larger than the other (Gibbons and Chakraborti, 2011). The sign test has limited practical implications since it only checks if algorithm A is better than B and ignores the extent of the difference. Yet, it has been used in a variety of NLP papers (e.g. (Collins et al., 2005; Chan et al., 2007; Rush et al., 2012)). The assumptions of this test is that the data samples are i.i.d, the differences come from a continuous distribution (not necessarily normal) and that the values are ordered. The next test is a special case of the sign test for binary classification (or a two-tailed sign test). McNemar’s test (McNemar, 1947) This test is designed for paired nominal observations (binary labels). The test is applied to a 2 × 2 contingency table, which tabulates the outcomes of two algorithms on a sample of n examples. The null hypothesis for this test states that the marginal probability f"
P18-1128,P05-1066,0,0.0283357,"hed pair samples are drawn from distributions with equal medians. The test statistic is the number of examples for which algorithm A is better than algorithm B, and the null hypothesis states that given a new pair of measurements (e.g. evaluations (ai , bi ) of the two algorithms on a new test example), then ai and bi are equally likely to be larger than the other (Gibbons and Chakraborti, 2011). The sign test has limited practical implications since it only checks if algorithm A is better than B and ignores the extent of the difference. Yet, it has been used in a variety of NLP papers (e.g. (Collins et al., 2005; Chan et al., 2007; Rush et al., 2012)). The assumptions of this test is that the data samples are i.i.d, the differences come from a continuous distribution (not necessarily normal) and that the values are ordered. The next test is a special case of the sign test for binary classification (or a two-tailed sign test). McNemar’s test (McNemar, 1947) This test is designed for paired nominal observations (binary labels). The test is applied to a 2 × 2 contingency table, which tabulates the outcomes of two algorithms on a sample of n examples. The null hypothesis for this test states that the mar"
P18-1128,Q17-1033,1,0.936879,"is consistently better than the other, hopefully with a sufficiently large margin, then it should also be better on future, currently unknown, datasets. Yet, the experimental differences might be coincidental. Here comes statistical significance testing into the picture: we have to make sure that the probability of falsely concluding that one algorithm is better than the other is very small. We note that in this paper we do not deal with the problem of drawing valid conclusions from multiple comparisons between algorithms across a large number of datasets , a.k.a. replicability analysis (see (Dror et al., 2017)). Instead, our focus is on a single comparison: how can we make sure that the difference between the two algorithms, as 1383 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1383–1392 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics observed in an individual comparison, is not coincidental. Statistical significance testing of each individual comparison is the basic building block of replicability analysis – its accurate performance is a pre-condition for any multiple dataset analysis. Statist"
P18-1128,P17-1146,0,0.0127489,"ilar to approximate randomization of the permutation test, with the difference that the sampling is done with replacements (i.e., an example from the original test data can appear more than once in a sample). The idea of bootstrap is to use the samples as surrogate populations, for the purpose of approximating the sampling distribution of the statistic. The p-value is calculated in a similar manner to the permutation test. Bootstrap was used with a variety of NLP tasks, including machine translation, text summarization and semantic parsing (e.g. (Koehn, 2004; Li et al., 2017; Wu et al., 2017; Ouchi et al., 2017)). The test is less effective for small test sets, as it assumes that the test set distribution does not deviate too much from the population distribution. Clearly, Sampling-based methods are computationally intensive and can be intractable for large datasets, even with modern computing power. In such cases, sampling-free methods form an available alternative. 3.3 Significance Test Selection With the discussion of significance test families - parametric vs. non-parametric (§ 3.1), and the properties of the actual significance tests (§ 3.2) we are now ready to provide a simple recipe for signif"
P18-1128,P02-1040,0,0.104516,"–1392 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics observed in an individual comparison, is not coincidental. Statistical significance testing of each individual comparison is the basic building block of replicability analysis – its accurate performance is a pre-condition for any multiple dataset analysis. Statistical significance testing (§ 2) is a well researched problem in the statistical literature. However, the unique structured nature of natural language data is reflected in specialized evaluation measures such as BLEU (machine translation, (Papineni et al., 2002)), ROUGE (extractive summarization, (Lin, 2004)), UAS and LAS (dependency parsing, (K¨ubler et al., 2009)). The distribution of these measures is of great importance to statistical significance testing. Moreover, certain properties of NLP datasets and the community’s evaluation standards also affect the way significance testing should be performed. An NLP-specific discussion of significance testing is hence in need. In § 3 we discuss the considerations to be made in order to select the proper statistical significance test in NLP setups. We propose a simple decision tree algorithm for this purp"
P18-1128,W05-0908,0,0.272127,"tic under all possible labellings (permutations) of the test set. The (two-sided) p-value of the test is calculated as the proportion of these permutations where the absolute difference was greater than or equal to the absolute value of the difference in the output of the algorithm. Obviously, permutation tests are computationally intensive due to the exponentially large number of possible permutations. In practice, approximate randomization tests are used where a pre-defined limited number of permutations are drawn from the space of all possible permutations, without replacements (see, e.g. (Riezler and Maxwell, 2005) in the context of machine translation). The bootstrap test (Efron and Tibshirani, 1994) is based on a closely related idea. Paired bootstrap test This test is very similar to approximate randomization of the permutation test, with the difference that the sampling is done with replacements (i.e., an example from the original test data can appear more than once in a sample). The idea of bootstrap is to use the samples as surrogate populations, for the purpose of approximating the sampling distribution of the statistic. The p-value is calculated in a similar manner to the permutation test. Boots"
P18-1128,D12-1131,1,0.758245,"ions with equal medians. The test statistic is the number of examples for which algorithm A is better than algorithm B, and the null hypothesis states that given a new pair of measurements (e.g. evaluations (ai , bi ) of the two algorithms on a new test example), then ai and bi are equally likely to be larger than the other (Gibbons and Chakraborti, 2011). The sign test has limited practical implications since it only checks if algorithm A is better than B and ignores the extent of the difference. Yet, it has been used in a variety of NLP papers (e.g. (Collins et al., 2005; Chan et al., 2007; Rush et al., 2012)). The assumptions of this test is that the data samples are i.i.d, the differences come from a continuous distribution (not necessarily normal) and that the values are ordered. The next test is a special case of the sign test for binary classification (or a two-tailed sign test). McNemar’s test (McNemar, 1947) This test is designed for paired nominal observations (binary labels). The test is applied to a 2 × 2 contingency table, which tabulates the outcomes of two algorithms on a sample of n examples. The null hypothesis for this test states that the marginal probability for each outcome (lab"
P18-1128,N13-1068,0,0.0342652,"e sign test and its variants. Wilcoxon signed-rank test (Wilcoxon, 1945) Like the sign test variants, this test is used when comparing two matched samples (e.g. UAS values of two dependency parsers on a set of sentences). Its null hypothesis is that the differences follow a symmetric distribution around zero. First, the absolute values of the differences are ranked. Then, each rank gets a sign according to the sign of the difference. The Wilcoxon test statistic sums these signed ranks. The test is actually applicable for most NLP setups and it has been used widely (e.g. (Søgaard et al., 2014; Søgaard, 2013; Yang and Mitchell, 2017)) due to its improved power compared to the sign test variants. As noted above, sampling-free tests trade statistical power for efficiency. Sampling-based methods take the opposite approach. This family includes two main methods: permutation/randomization tests (Noreen, 1989) and the 1387 paired bootstrap (Efron and Tibshirani, 1994). Pitman’s permutation test This test estimates the test statistic distribution under the null hypothesis by calculating the values of this statistic under all possible labellings (permutations) of the test set. The (two-sided) p-value of"
P18-1128,W14-1601,0,0.112657,"more powerful than the sign test and its variants. Wilcoxon signed-rank test (Wilcoxon, 1945) Like the sign test variants, this test is used when comparing two matched samples (e.g. UAS values of two dependency parsers on a set of sentences). Its null hypothesis is that the differences follow a symmetric distribution around zero. First, the absolute values of the differences are ranked. Then, each rank gets a sign according to the sign of the difference. The Wilcoxon test statistic sums these signed ranks. The test is actually applicable for most NLP setups and it has been used widely (e.g. (Søgaard et al., 2014; Søgaard, 2013; Yang and Mitchell, 2017)) due to its improved power compared to the sign test variants. As noted above, sampling-free tests trade statistical power for efficiency. Sampling-based methods take the opposite approach. This family includes two main methods: permutation/randomization tests (Noreen, 1989) and the 1387 paired bootstrap (Efron and Tibshirani, 1994). Pitman’s permutation test This test estimates the test statistic distribution under the null hypothesis by calculating the values of this statistic under all possible labellings (permutations) of the test set. The (two-sid"
P18-1128,P17-1065,0,0.0147906,"test is very similar to approximate randomization of the permutation test, with the difference that the sampling is done with replacements (i.e., an example from the original test data can appear more than once in a sample). The idea of bootstrap is to use the samples as surrogate populations, for the purpose of approximating the sampling distribution of the statistic. The p-value is calculated in a similar manner to the permutation test. Bootstrap was used with a variety of NLP tasks, including machine translation, text summarization and semantic parsing (e.g. (Koehn, 2004; Li et al., 2017; Wu et al., 2017; Ouchi et al., 2017)). The test is less effective for small test sets, as it assumes that the test set distribution does not deviate too much from the population distribution. Clearly, Sampling-based methods are computationally intensive and can be intractable for large datasets, even with modern computing power. In such cases, sampling-free methods form an available alternative. 3.3 Significance Test Selection With the discussion of significance test families - parametric vs. non-parametric (§ 3.1), and the properties of the actual significance tests (§ 3.2) we are now ready to provide a sim"
P18-1128,P17-1132,0,0.0161275,"its variants. Wilcoxon signed-rank test (Wilcoxon, 1945) Like the sign test variants, this test is used when comparing two matched samples (e.g. UAS values of two dependency parsers on a set of sentences). Its null hypothesis is that the differences follow a symmetric distribution around zero. First, the absolute values of the differences are ranked. Then, each rank gets a sign according to the sign of the difference. The Wilcoxon test statistic sums these signed ranks. The test is actually applicable for most NLP setups and it has been used widely (e.g. (Søgaard et al., 2014; Søgaard, 2013; Yang and Mitchell, 2017)) due to its improved power compared to the sign test variants. As noted above, sampling-free tests trade statistical power for efficiency. Sampling-based methods take the opposite approach. This family includes two main methods: permutation/randomization tests (Noreen, 1989) and the 1387 paired bootstrap (Efron and Tibshirani, 1994). Pitman’s permutation test This test estimates the test statistic distribution under the null hypothesis by calculating the values of this statistic under all possible labellings (permutations) of the test set. The (two-sided) p-value of the test is calculated as"
P18-1128,C00-2137,0,0.377658,"ne case where it is theoretically justified to employ the t-test is described in (Sethuraman, 1963). The authors prove that for large enough data, the sampling distribution of a certain function of the Pearson’s correlation coefficient follows the Student’s t-distribution with n − 2 degrees of freedom. With the recent surge in word similarity research with word embedding models, this result is of importance to our community. For other evaluation measures, such as F-score, BLEU, METEOR and ROUGE that do not compute means, the common practice is to assume that they are not normally distributed (Yeh, 2000; Berg-Kirkpatrick et al., 2012). We believe this issue requires a further investigation and suggest that it may be best to rely on the normality tests discussed in § 3.1 when deciding whether or not to employ the t-test. 1386 3.2.2 Non-parametric Tests When the test statistic distribution is unknown, non-parametric significance testing should be used. The non-parametric tests that are commonly used in NLP setups can be divided into two families that differ with respect to their statistical power and computational complexity. The first family consists of tests that do not consider the actual v"
P18-1128,K17-1040,1,0.813606,"hypothesis for this test states that the marginal probability for each outcome (label one or label two) is the same for both algorithms. That is, when applying both algorithms on the same data we would expect them to be correct/incorrect on the same proportion of items. Under the null hypothesis, with a sufficiently large number of disagreements between the algorithms, the test statistic has a distribution of χ2 with one degree of freedom. This test is appropriate for binary classification tasks, and has been indeed used in such NLP works (e.g. sentiment classificaiton, (Blitzer et al., 2006; Ziser and Reichart, 2017)). The Cochran’s Q test (Cochran, 1950) generalizes the McNemar’s test for multi-class classification setups. The sign test and its variants consider only pairwise ranks: which algorithm performs better on each test example. In NLP setups, however, we also have access to the evaluation measure values, and this allows us to rank the differences between the algorithms. The Wilcoxon signed-rank test makes use of such a rank and hence, while it does not consider the evaluation measure values, it is more powerful than the sign test and its variants. Wilcoxon signed-rank test (Wilcoxon, 1945) Like t"
P18-1142,W17-0401,0,0.422071,"Missing"
P18-1142,W14-4203,0,0.17361,"Missing"
P18-1142,P17-2021,0,0.0122502,"resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1 . Importantly, although UD is"
P18-1142,P16-1231,0,0.0152094,"port LAS scores using three different source languages: (1) the highestranked source according to the Jaccard index; (2) a source sampled from the middle of the list ranked by the Jaccard indices; (3) a very dissimilar language sampled from the bottom of the ranked list. The total number of sentences used for training corresponds to the smallest of the three source language treebanks in order to isolate the effect of treebank size on the final transfer results. We conduct experiments with two well-known transition-based parsers (Nivre, 2006): (1) DeSR (Attardi et al., 2007) and (2) SyntaxNet (Andor et al., 2016; Alberti et al., 2017). The two were selected as they represent two different architectures: the former is an SVM-based model with a polynomial kernel, whereas the latter is a feed-forward neural network with beam search based on conditional random fields. The results are evaluated in terms of LAS and UAS scores. Neural Machine Translation. For NMT, we examine whether the tree processing procedure from §2.3 can reduce anisomorphism between source and target language syntactic structures. We thus run NMT models in two settings: with and without the anisomorphism reduction procedure. For this e"
P18-1142,P17-1042,0,0.015014,"ovide a different forget gate ftk for each child. Hidden layers Hidden size Input size Batch size Epochs qt = σ (Wq xt + Uq ht−1 + bq ) (5) ct = ft ct−1 + it tanh (Wc xt + Uc ht−1 + bc ) (6) ht = ot tanh(ct ) (7) In our resource-lean cross-lingual scenario the language of the training data (English) differs from that of the target (Arabic). Since TreeLSTM is a lexicalised model, we employ multilingual word embeddings, such that the words of both languages lie in the shared cross-lingual semantic space. In particular, we map English into Arabic through the iterative Procustes method devised by Artetxe et al. (2017). The results are evaluated through the Pearson correlation and the Mean Squared Error (MSE) between predicted and golden labels. Learning rate Optimiser Dropout Results and Discussion Source Selection. The results for cross-lingual parser transfer with the DeSR parser are provided in Figure 3, while the results with SyntaxNet are provided as supplemental material as they follow the same trends. The selection of the source for Nematus (NMT) TreeLSTM (STS) 2 512 160 256 12 (greed); 10 (beam) 0.8 Adam 0.2 / 0.3 2 1000 280 80 Early stopping 1−4 AdaDelta 0.1 / 0.2 1 300 512 25 5 1−2 SGD 0 Table 1:"
P18-1142,D07-1119,0,0.0979575,"Missing"
P18-1142,S17-2001,0,0.0189177,"y annotated by SyntaxNet. The data for cross-lingual STS are chosen to resemble a real-world scenario with a resource-poor target language. The training data (9,709 sentence 8 http://universaldependencies.org/ Language names are substituted in this work by their corresponding ISO 639-1 codes. A table of names and codes is provided in the supplemental material. 10 http://opus.nlpl.eu/OpenSubtitles.php 1535 9 pairs) are in English, taken from the STS benchmark, the ensemble of all the datasets from SemEval 2012-2017 STS tasks. The test data (250 sentence pairs) come from Task 1 of SemEval 2017 (Cer et al., 2017); target language is Arabic.11 All the sentence pairs are associated with a label ranging from 0 (dissimilarity) to 5 (equivalence). 4 Methodology Cross-lingual Dependency Parsing. To assess if the anisomorphism metrics devised in §2.2 are reliable in finding compatible languages for knowledge transfer, we use the Jaccard index of the morphological feature sets as a criterion to choose source languages for cross-lingual parser transfer. We adopt the variant of delexicalised model transfer (Zeman and Resnik, 2008) for this task. This technique ignores lexicalised features and leverages only lan"
P18-1142,P16-1038,0,0.032079,"turned out to be useful for correcting programming scripts (Tai, 1979), evolution studies, and most notably accounting for transformations in constituency trees (Selkow, 1977). Although previous works were aware of the problem of anisomorphism in the context of syntax-based NLP applications (Ambati, 2008), to our knowledge we are the first to quantify it formally and to leverage it in cross-lingual NLP. For source selection, similarity metrics from prior work mostly relied on information stored in typological databases (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Deri and Knight, 2016). Otherwise, the metrics were derived empirically: they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although th"
P18-1142,N16-1024,0,0.0196463,"limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morp"
P18-1142,P16-1078,0,0.0953751,"can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1"
P18-1142,P03-1011,0,0.115293,"they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims"
P18-1142,W15-2114,0,0.0138647,"ntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics In this paper we address these two challenges. We propose the concept of isomorphism (i.e., identity of shapes: syntactic structures) and its opposite, anisomorphism, as a probe to measuring quantitatively the extent to which syntactic tree pairs are cross-lingually compatible. We assess the varia"
P18-1142,2007.mtsummit-papers.29,0,0.07936,"esian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau i"
P18-1142,de-marneffe-etal-2014-universal,0,0.071785,"Missing"
P18-1142,P12-1066,0,0.74434,"upport to cross-lingual transfer, it also supports monolingual applications with a quality comparable to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers"
P18-1142,P15-2040,0,0.51718,"e to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguist"
P18-1142,D17-1038,0,0.034432,"our knowledge we are the first to quantify it formally and to leverage it in cross-lingual NLP. For source selection, similarity metrics from prior work mostly relied on information stored in typological databases (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Deri and Knight, 2016). Otherwise, the metrics were derived empirically: they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner ("
P18-1142,C16-1123,1,0.925498,"Missing"
P18-1142,E17-3017,0,0.0213831,"Missing"
P18-1142,P15-2034,0,0.123715,"Missing"
P18-1142,W16-2209,0,0.0151383,"15) implemented in the Nematus suite12 (Sennrich et al., 2017). The encoder is a bidirectional gated recurrent network. For each step i, the decoder predicts the next word in output by taking as input the current hidden state hi , the previous word wi−1 and a context vector, Pn i.e., a weighted sum of all the hidden states j=1 wj · h1 . The weights are learned by a multilayer perceptron that estimates the likelihood of the alignment between the predicted word and each of the input words: wi,j = P (a|yi , xj ). This model is enriched with additional linguistic features on input, as proposed by Sennrich and Haddow (2016). In particular, we select the following which are proven as useful in prior work, and also relevant to our experiment: word form, POS tag, and dependency relations. These features are concatenated and fed to the encoder. Tree processing from §2.3 affects these features (and consequently the sentence representation) by changing the initial tree structure. For instance, the original tree in Figure 2a and the processed one in Figure 2c would correspond to these feature sets: Original Preprocessed ladayhim¯a ⊕ N ⊕ ROOT him¯a ⊕ N ⊕ N SUBJ D UMMY ⊕ V ⊕ ROOT ‘aˇsy¯a‘u ⊕ N ⊕ D OBJ muˇstarakatun ⊕ A ⊕"
P18-1142,P02-1040,0,0.102066,"M-based model with a polynomial kernel, whereas the latter is a feed-forward neural network with beam search based on conditional random fields. The results are evaluated in terms of LAS and UAS scores. Neural Machine Translation. For NMT, we examine whether the tree processing procedure from §2.3 can reduce anisomorphism between source and target language syntactic structures. We thus run NMT models in two settings: with and without the anisomorphism reduction procedure. For this experiment we rely on a state-of-the-art syntax-aware NMT architecture. We report its performance by BLEU scores (Papineni et al., 2002). 11 http://alt.qcri.org/semeval2017/ task1/ In particular, we use an attentional encoder-decoder network that jointly learns to translate and align words (Bahdanau et al., 2015) implemented in the Nematus suite12 (Sennrich et al., 2017). The encoder is a bidirectional gated recurrent network. For each step i, the decoder predicts the next word in output by taking as input the current hidden state hi , the previous word wi−1 and a context vector, Pn i.e., a weighted sum of all the hidden states j=1 wj · h1 . The weights are learned by a multilayer perceptron that estimates the likelihood of th"
P18-1142,D09-1086,0,0.21974,"bstantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics In this paper we address these two challenges. We propose the concept of isomorphism (i.e., identity of shapes: syntactic str"
P18-1142,P11-1157,0,0.211875,"Missing"
P18-1142,W17-0903,1,0.791524,"Missing"
P18-1142,S17-1003,1,0.825881,"Missing"
P18-1142,Q17-1020,0,0.160289,"translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP. 1 Introduction Linguistic information can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variatio"
P18-1142,D09-1085,0,0.0306106,"ashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morphological and syntactic differences across languages. This phenomenon, which we have labeled as anismorphism, can challenge the transfer of knowledge from"
P18-1142,J16-4004,0,0.0130895,"he order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morphological and syntactic differences across languages. This phenomenon, which we have labeled as anismorphism, can challenge the transfer of knowledge from one language to another. We have pro"
P18-1142,N13-1126,0,0.507114,"Missing"
P18-1142,P15-1150,0,0.0924878,"Missing"
P18-1142,W15-2137,0,0.089893,"for both machine translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP. 1 Introduction Linguistic information can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content word"
P18-1142,P16-2069,0,0.0787408,"Missing"
P18-1142,E17-1034,0,0.0624443,"Missing"
P18-1142,E17-2065,1,0.884029,"Missing"
P18-1142,P16-2084,1,0.906217,"Missing"
P18-1142,Q16-1035,0,0.116694,", and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1 . Importantly, although UD is tailored to offer support to cross-lingual transfer, it also supports monolingual applications wit"
P18-1142,I08-3008,0,0.202796,"12-2017 STS tasks. The test data (250 sentence pairs) come from Task 1 of SemEval 2017 (Cer et al., 2017); target language is Arabic.11 All the sentence pairs are associated with a label ranging from 0 (dissimilarity) to 5 (equivalence). 4 Methodology Cross-lingual Dependency Parsing. To assess if the anisomorphism metrics devised in §2.2 are reliable in finding compatible languages for knowledge transfer, we use the Jaccard index of the morphological feature sets as a criterion to choose source languages for cross-lingual parser transfer. We adopt the variant of delexicalised model transfer (Zeman and Resnik, 2008) for this task. This technique ignores lexicalised features and leverages only language-independent features instead. For each language from a sample of 7 (typologically diverse) targets, we report LAS scores using three different source languages: (1) the highestranked source according to the Jaccard index; (2) a source sampled from the middle of the list ranked by the Jaccard indices; (3) a very dissimilar language sampled from the bottom of the ranked list. The total number of sentences used for training corresponds to the smallest of the three source language treebanks in order to isolate"
P18-1142,D15-1213,0,0.471468,"rts monolingual applications with a quality comparable to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 -"
P18-1142,W07-0401,0,0.107951,"Missing"
P19-1266,Q17-1033,1,0.853784,"le at: https://github.com/ rtmdrr/deepComparison was deterministic and the number of configurations a model could have was rather small – decisions about model design were usually limited to feature selection and the selection of one of a few loss functions. Consequently, when one model performed better than another on unseen data it was safe to argue that the winning model was generally better, especially when the results were statistically significant (Dror et al., 2018), and when the effect of multiple hypothesis testing was taken into account in cases of evaluation with multiple datasets (Dror et al., 2017). With the recent emergence of Deep Neural Networks (DNNs), data-driven performance comparison has become much more complicated. While models such as LSTM (Hochreiter and Schmidhuber, 1997), Bi-LSTM (Schuster and Paliwal, 1997) and the transformer (Vaswani et al., 2017) improved the state-of-the-art in many NLP tasks (e.g. (Dozat and Manning, 2017; Hershcovich et al., 2017; Yadav and Bethard, 2018)), it is much more difficult to compare the performance of algorithms that are based on these models. This is because the loss functions of these models are non-convex (Dauphin et al., 2014), making"
P19-1266,P18-1128,1,0.846286,"and relatively simple (e.g. (Toutanova et al., 2003; Finkel et al., 2008; Ritter et al., 2011)). Hence, their training 1 Our code is available at: https://github.com/ rtmdrr/deepComparison was deterministic and the number of configurations a model could have was rather small – decisions about model design were usually limited to feature selection and the selection of one of a few loss functions. Consequently, when one model performed better than another on unseen data it was safe to argue that the winning model was generally better, especially when the results were statistically significant (Dror et al., 2018), and when the effect of multiple hypothesis testing was taken into account in cases of evaluation with multiple datasets (Dror et al., 2017). With the recent emergence of Deep Neural Networks (DNNs), data-driven performance comparison has become much more complicated. While models such as LSTM (Hochreiter and Schmidhuber, 1997), Bi-LSTM (Schuster and Paliwal, 1997) and the transformer (Vaswani et al., 2017) improved the state-of-the-art in many NLP tasks (e.g. (Dozat and Manning, 2017; Hershcovich et al., 2017; Yadav and Bethard, 2018)), it is much more difficult to compare the performance of"
P19-1266,P08-1109,0,0.0399531,"e here will set a new working practice in the NLP community.1 1 Introduction A large portion of the research activity in Natural Language Processing (NLP) is devoted to the development of new algorithms for existing or new tasks. To evaluate the quality of a new method, its performance on unseen datasets is compared to the performance of existing methods. The progress of the field hence crucially depends on our ability to draw conclusions from such comparisons. In the past, most supervised NLP models have been linear (or log-linear), convex and relatively simple (e.g. (Toutanova et al., 2003; Finkel et al., 2008; Ritter et al., 2011)). Hence, their training 1 Our code is available at: https://github.com/ rtmdrr/deepComparison was deterministic and the number of configurations a model could have was rather small – decisions about model design were usually limited to feature selection and the selection of one of a few loss functions. Consequently, when one model performed better than another on unseen data it was safe to argue that the winning model was generally better, especially when the results were statistically significant (Dror et al., 2018), and when the effect of multiple hypothesis testing wa"
P19-1266,P17-1104,0,0.0169326,"g model was generally better, especially when the results were statistically significant (Dror et al., 2018), and when the effect of multiple hypothesis testing was taken into account in cases of evaluation with multiple datasets (Dror et al., 2017). With the recent emergence of Deep Neural Networks (DNNs), data-driven performance comparison has become much more complicated. While models such as LSTM (Hochreiter and Schmidhuber, 1997), Bi-LSTM (Schuster and Paliwal, 1997) and the transformer (Vaswani et al., 2017) improved the state-of-the-art in many NLP tasks (e.g. (Dozat and Manning, 2017; Hershcovich et al., 2017; Yadav and Bethard, 2018)), it is much more difficult to compare the performance of algorithms that are based on these models. This is because the loss functions of these models are non-convex (Dauphin et al., 2014), making the solution to which they converge (a local minimum or a saddle point) sensitive to random weight initialization and the order of training examples. Moreover, as these complex models are not fully understood, their training is often enhanced by heuristics such as random dropouts (Srivastava et al., 2014) that introduces another level of non-determinism to the training pro"
P19-1266,J93-2004,0,0.0641582,"hm is not large (criterion (b)). To demonstrate the appropriateness of this method for the comparison between two DNNs we next revisit the extensive experimental setup of Reimers and Gurevych (2017a). 5 Analysis Tasks and Models In this section we demonstrate the potential impact of testing for almost stochastic dominance on the way empirical results of NLP models are analyzed. We use the data of Reimers and Gurevych (2017a)5 and Reimers and Gurevych (2017b).6 This data contains 510 comparison setups for five common NLP sequence tagging tasks: Part Of Speech (POS) tagging with the WSJ corpus (Marcus et al., 1993), syntactic chucking with the CoNLL 2000 data (Sang and Buchholz, 2000), Named Entity Recognition with the CoNLL 2003 data (Sang and De Meulder, 2003), Entity Recognition with the ACE2005 data (Walker et al., 2006), and event detection with the TempEval3 data (UzZaman et al., 2013). In each setup two leading DNNs, either different architectures or variants of the same model but with different hyper-parameter configurations, are compared across various choices of random seeds and hyperparameter configurations. The exact details of the comparisons are beyond the scope of this paper; they are doc"
P19-1266,D17-1035,0,0.096723,"runs of the inferior model; (b) The decision mechanism should be powerful, being able to make decisions in most possible decision tasks; and, finally, (c) Since both models depend on random decisions, it is likely that none of them is promised to be superior over the other in all cases (e.g. with all possible random seeds). A powerful comparison tool should hence augment its decision with a confidence score, reflecting the probability that the superior model will indeed produce a better output. Analysis of existing solutions (§ 3, 5): The comparison problem we address has been highlighted by Reimers and Gurevych (2017b, 2018), who established its importance in an extensive experimentation with neural sequence models (Reimers and Gurevych, 2017a), and proposed two main solutions (§3). One solution, which we refer to as the collection of statistics (COS) solution, is based on the analysis of statistics of the empirical score distribution of the two algorithms – such as their mean, median and standard deviation (std), as well as their minimum and maximum values. Unfortunately, this solution does not respect criterion (a) as it does not deal with significance, and as we demonstrate in §5 its power (criterion ("
P19-1266,D11-1141,0,0.0781601,"Missing"
P19-1266,W00-0726,0,0.0388084,"of this method for the comparison between two DNNs we next revisit the extensive experimental setup of Reimers and Gurevych (2017a). 5 Analysis Tasks and Models In this section we demonstrate the potential impact of testing for almost stochastic dominance on the way empirical results of NLP models are analyzed. We use the data of Reimers and Gurevych (2017a)5 and Reimers and Gurevych (2017b).6 This data contains 510 comparison setups for five common NLP sequence tagging tasks: Part Of Speech (POS) tagging with the WSJ corpus (Marcus et al., 1993), syntactic chucking with the CoNLL 2000 data (Sang and Buchholz, 2000), Named Entity Recognition with the CoNLL 2003 data (Sang and De Meulder, 2003), Entity Recognition with the ACE2005 data (Walker et al., 2006), and event detection with the TempEval3 data (UzZaman et al., 2013). In each setup two leading DNNs, either different architectures or variants of the same model but with different hyper-parameter configurations, are compared across various choices of random seeds and hyperparameter configurations. The exact details of the comparisons are beyond the scope of this paper; they are documented in the above papers. For each experimental setup, we report the"
P19-1266,W03-0419,0,0.146663,"Missing"
P19-1266,N03-1033,0,0.0172304,"hope the test we propose here will set a new working practice in the NLP community.1 1 Introduction A large portion of the research activity in Natural Language Processing (NLP) is devoted to the development of new algorithms for existing or new tasks. To evaluate the quality of a new method, its performance on unseen datasets is compared to the performance of existing methods. The progress of the field hence crucially depends on our ability to draw conclusions from such comparisons. In the past, most supervised NLP models have been linear (or log-linear), convex and relatively simple (e.g. (Toutanova et al., 2003; Finkel et al., 2008; Ritter et al., 2011)). Hence, their training 1 Our code is available at: https://github.com/ rtmdrr/deepComparison was deterministic and the number of configurations a model could have was rather small – decisions about model design were usually limited to feature selection and the selection of one of a few loss functions. Consequently, when one model performed better than another on unseen data it was safe to argue that the winning model was generally better, especially when the results were statistically significant (Dror et al., 2018), and when the effect of multiple"
P19-1266,N03-1000,0,0.147674,"Missing"
P19-1266,S13-2001,0,0.028034,"of testing for almost stochastic dominance on the way empirical results of NLP models are analyzed. We use the data of Reimers and Gurevych (2017a)5 and Reimers and Gurevych (2017b).6 This data contains 510 comparison setups for five common NLP sequence tagging tasks: Part Of Speech (POS) tagging with the WSJ corpus (Marcus et al., 1993), syntactic chucking with the CoNLL 2000 data (Sang and Buchholz, 2000), Named Entity Recognition with the CoNLL 2003 data (Sang and De Meulder, 2003), Entity Recognition with the ACE2005 data (Walker et al., 2006), and event detection with the TempEval3 data (UzZaman et al., 2013). In each setup two leading DNNs, either different architectures or variants of the same model but with different hyper-parameter configurations, are compared across various choices of random seeds and hyperparameter configurations. The exact details of the comparisons are beyond the scope of this paper; they are documented in the above papers. For each experimental setup, we report the outcome of three alternative comparison methods: collection of statistics (COS), stochastic order (SO), and almost stochastic order (ASO). For COS, we report the mean, std, and median of the scores for each alg"
P19-1266,C18-1182,0,0.0254831,"er, especially when the results were statistically significant (Dror et al., 2018), and when the effect of multiple hypothesis testing was taken into account in cases of evaluation with multiple datasets (Dror et al., 2017). With the recent emergence of Deep Neural Networks (DNNs), data-driven performance comparison has become much more complicated. While models such as LSTM (Hochreiter and Schmidhuber, 1997), Bi-LSTM (Schuster and Paliwal, 1997) and the transformer (Vaswani et al., 2017) improved the state-of-the-art in many NLP tasks (e.g. (Dozat and Manning, 2017; Hershcovich et al., 2017; Yadav and Bethard, 2018)), it is much more difficult to compare the performance of algorithms that are based on these models. This is because the loss functions of these models are non-convex (Dauphin et al., 2014), making the solution to which they converge (a local minimum or a saddle point) sensitive to random weight initialization and the order of training examples. Moreover, as these complex models are not fully understood, their training is often enhanced by heuristics such as random dropouts (Srivastava et al., 2014) that introduces another level of non-determinism to the training process. Finally, the increas"
P19-1438,Q13-1005,0,0.0815669,"elves in a world where even more functionality could be accessed via a natural language user interface (NLUI). If so, we better seek answers to the following questions: Will every developing team need to hire NLP experts to develop a NLUI for 1 Our code and data are https://github.com/givoli/TechnionNLI. available at: their specific application? Can we hope for a general framework that once trained on annotated data from a set of domains, does not require annotated data from a newly presented domain? Previous work on tasks related to NLUI for applications mostly relied on in-domain data (e.g. Artzi and Zettlemoyer (2013); Long et al. (2016)), and papers that did not rely on in-domain data did not attempt to parse instructions into compositional logical forms (Kim et al., 2016). To fill this gap, we address the task of zeroshot semantic parsing for instructions: training a parser so that it can parse instructions into compositional logical forms, where the instructions are from domains that were not seen during training. Formally, our task assumes a set D = {d1 , ..., dn } of source domains, each corresponding to a simple application (e.g. a calendar or a file manager) and an application program interface (API"
P19-1438,D13-1160,0,0.026457,"resource can be either a very large corpus (Gerber and Ngomo, 2011; Krishnamurthy and Mitchell, 2012), search results from the Web (Cai and Yates, 2013) or pairs of a sentence and an associated logical form (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). In our task none of the above resources is assumed to be available but instead we use the description phrases of the interface methods. Cross-domain and Zero-shot Semantic Parsing Previous semantic parsers use supervised training, either with (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010) or without (Clarke et al., 2010; Berant et al., 2013) logical forms annota4455 tion, in addition to unsupervised training (Goldwasser et al., 2011). We take the supervised approach with no logical form annotations. While most semantic parsing work trains on indomain data, there are some exceptions. Cai and Yates (2013) and Kwiatkowski et al. (2013) introduced semantic parsers for question answering that can parse utterances from Free917 (Cai and Yates, 2013) such that no Freebase entities or relations appear in both training and test examples. We also note the relevance of the dataset presented in Pasupat and Liang (2015) which contains question"
P19-1438,P13-1042,0,0.132434,"a meaning representation over a space of compositional logical forms. Other tasks related to ours include program synthesis (Raza et al., 2015; Desai et al., 2016) and mapping natural language to bash code (Lin et al., 2018), but these also did not consider zero-shot setups and did not synthesize code in the context of an application state. Semantic Parsing with In-domain Data Among the semantic parsing work that relied on in-domain data, many relied on a domain-specific lexicon (Kwiatkowski et al., 2010; Gerber and Ngomo, 2011; Krishnamurthy and Mitchell, 2012; Zettlemoyer and Collins, 2005; Cai and Yates, 2013) which maps natural language phrases to primitive logical forms. Many of these works automatically constructed a domain-specific lexicon using some additional domain-specific resources that are associated with the entities and relations of the given domain. Such a resource can be either a very large corpus (Gerber and Ngomo, 2011; Krishnamurthy and Mitchell, 2012), search results from the Web (Cai and Yates, 2013) or pairs of a sentence and an associated logical form (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). In our task none of the above resources is assumed to be available bu"
P19-1438,W10-2903,0,0.0434305,"given domain. Such a resource can be either a very large corpus (Gerber and Ngomo, 2011; Krishnamurthy and Mitchell, 2012), search results from the Web (Cai and Yates, 2013) or pairs of a sentence and an associated logical form (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). In our task none of the above resources is assumed to be available but instead we use the description phrases of the interface methods. Cross-domain and Zero-shot Semantic Parsing Previous semantic parsers use supervised training, either with (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010) or without (Clarke et al., 2010; Berant et al., 2013) logical forms annota4455 tion, in addition to unsupervised training (Goldwasser et al., 2011). We take the supervised approach with no logical form annotations. While most semantic parsing work trains on indomain data, there are some exceptions. Cai and Yates (2013) and Kwiatkowski et al. (2013) introduced semantic parsers for question answering that can parse utterances from Free917 (Cai and Yates, 2013) such that no Freebase entities or relations appear in both training and test examples. We also note the relevance of the dataset presented in Pasupat and Liang (2015) w"
P19-1438,P18-1128,1,0.869174,"Missing"
P19-1438,P11-1149,1,0.791405,"hell, 2012), search results from the Web (Cai and Yates, 2013) or pairs of a sentence and an associated logical form (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). In our task none of the above resources is assumed to be available but instead we use the description phrases of the interface methods. Cross-domain and Zero-shot Semantic Parsing Previous semantic parsers use supervised training, either with (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010) or without (Clarke et al., 2010; Berant et al., 2013) logical forms annota4455 tion, in addition to unsupervised training (Goldwasser et al., 2011). We take the supervised approach with no logical form annotations. While most semantic parsing work trains on indomain data, there are some exceptions. Cai and Yates (2013) and Kwiatkowski et al. (2013) introduced semantic parsers for question answering that can parse utterances from Free917 (Cai and Yates, 2013) such that no Freebase entities or relations appear in both training and test examples. We also note the relevance of the dataset presented in Pasupat and Liang (2015) which contains questions about Wikipedia tables, such that the context of each question is a single table. They evalu"
P19-1438,P17-2098,0,0.0128835,"ted in Pasupat and Liang (2015) which contains questions about Wikipedia tables, such that the context of each question is a single table. They evaluate a parser on questions about tables that have not been observed during training. Their work does not fully constitute zero-shot semantic parsing due to table columns across the train/test split that share column headers (which correspond to primitive logical forms that represent relations). Our parser is based on the floating parser introduced in that paper, and the space of logical forms we use is very similar to theirs (see § 4.1). Recently, Herzig and Berant (2017) and Su and Yan (2017) experimented with the Overnight dataset (Wang et al., 2015) in cross-domain settings. These papers did not experiment with zeroshot setups (i.e. training without any data from the target domain), and they both observed that the less in-domain training data was used, the more training data from other domains was valuable. Recently Herzig and Berant (2018) explored zeroshot semantic parsing with the Overnight dataset. Their framework, unlike ours, requires logical form annotation, and is designed for question answering rather than instruction parsing. Another zero-shot sem"
P19-1438,D18-1190,0,0.0608792,"correspond to primitive logical forms that represent relations). Our parser is based on the floating parser introduced in that paper, and the space of logical forms we use is very similar to theirs (see § 4.1). Recently, Herzig and Berant (2017) and Su and Yan (2017) experimented with the Overnight dataset (Wang et al., 2015) in cross-domain settings. These papers did not experiment with zeroshot setups (i.e. training without any data from the target domain), and they both observed that the less in-domain training data was used, the more training data from other domains was valuable. Recently Herzig and Berant (2018) explored zeroshot semantic parsing with the Overnight dataset. Their framework, unlike ours, requires logical form annotation, and is designed for question answering rather than instruction parsing. Another zero-shot semantic parsing task was introduced in Yu et al. (2018a). The task requires mapping natural language questions to SQL queries, and includes a setting in which no databases appear in both the training and test sets (as attempted in Yu et al. (2018b)). 3 Task and Data We now describe our task and dataset. 3.1 sponds to a lighting control system application that allows the user to"
P19-1438,D16-1222,0,0.113488,"ill every developing team need to hire NLP experts to develop a NLUI for 1 Our code and data are https://github.com/givoli/TechnionNLI. available at: their specific application? Can we hope for a general framework that once trained on annotated data from a set of domains, does not require annotated data from a newly presented domain? Previous work on tasks related to NLUI for applications mostly relied on in-domain data (e.g. Artzi and Zettlemoyer (2013); Long et al. (2016)), and papers that did not rely on in-domain data did not attempt to parse instructions into compositional logical forms (Kim et al., 2016). To fill this gap, we address the task of zeroshot semantic parsing for instructions: training a parser so that it can parse instructions into compositional logical forms, where the instructions are from domains that were not seen during training. Formally, our task assumes a set D = {d1 , ..., dn } of source domains, each corresponding to a simple application (e.g. a calendar or a file manager) and an application program interface (API) consisting of a set of interface methods. Each interface method is augmented with a list of description phrases that are expected to be used by the users of"
P19-1438,D17-1160,0,0.0254909,"Missing"
P19-1438,D12-1069,0,0.0670292,"Missing"
P19-1438,D13-1161,0,0.0237527,"ove resources is assumed to be available but instead we use the description phrases of the interface methods. Cross-domain and Zero-shot Semantic Parsing Previous semantic parsers use supervised training, either with (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010) or without (Clarke et al., 2010; Berant et al., 2013) logical forms annota4455 tion, in addition to unsupervised training (Goldwasser et al., 2011). We take the supervised approach with no logical form annotations. While most semantic parsing work trains on indomain data, there are some exceptions. Cai and Yates (2013) and Kwiatkowski et al. (2013) introduced semantic parsers for question answering that can parse utterances from Free917 (Cai and Yates, 2013) such that no Freebase entities or relations appear in both training and test examples. We also note the relevance of the dataset presented in Pasupat and Liang (2015) which contains questions about Wikipedia tables, such that the context of each question is a single table. They evaluate a parser on questions about tables that have not been observed during training. Their work does not fully constitute zero-shot semantic parsing due to table columns across the train/test split that s"
P19-1438,D10-1119,0,0.0418402,"ike in our task, the tasks that are investigated in those papers do not require the mapping of natural language to a meaning representation over a space of compositional logical forms. Other tasks related to ours include program synthesis (Raza et al., 2015; Desai et al., 2016) and mapping natural language to bash code (Lin et al., 2018), but these also did not consider zero-shot setups and did not synthesize code in the context of an application state. Semantic Parsing with In-domain Data Among the semantic parsing work that relied on in-domain data, many relied on a domain-specific lexicon (Kwiatkowski et al., 2010; Gerber and Ngomo, 2011; Krishnamurthy and Mitchell, 2012; Zettlemoyer and Collins, 2005; Cai and Yates, 2013) which maps natural language phrases to primitive logical forms. Many of these works automatically constructed a domain-specific lexicon using some additional domain-specific resources that are associated with the entities and relations of the given domain. Such a resource can be either a very large corpus (Gerber and Ngomo, 2011; Krishnamurthy and Mitchell, 2012), search results from the Web (Cai and Yates, 2013) or pairs of a sentence and an associated logical form (Zettlemoyer and"
P19-1438,J13-2005,0,0.0206052,"t) pair. The method call c can be invoked in the context of s with the provided application logic, producing the denotation y = JzKs , the resulting state. For each logical form z ∈ Zx the parser extracts a feature vector φ(x, s, z). The probability assigned to a logical form candidate z ∈ Zx is defined by a log-linear model: exp(θT φ(x, s, z)) T 0 z 0 ∈Zx exp(θ φ(x, s, z )) pθ (z|x, s) = P where θ is the weight vector. The logical form with maximal probability is chosen as the predicted logical form, and its denotation is the predicted desired state. Our logical form space is based on λ-DCS (Liang, 2013), as in the original FParser, but we use an additional derivation rule that derives the logical form f (z1 , ..., zn ), denoting a method call, given the primitive logical form f (denoting an interface method) and the logical forms z1 , ..., zn (each denoting a set of entities that correspond to an argument of f ). The objective function is the L1 regularized loglikelihood of the correct denotations across the training data: N 1 X J(θ) = log pθ (yi |xi , si ) − λkθk1 N (1) i=1 where pθ (y|x, s) is the sum of the probabilities assigned to all the candidate logical forms with the denotation y. 4"
P19-1438,L18-1491,0,0.0200608,"sing of instruction sequences for future research. A lot of work has been done on slot tagging and goal-oriented dialog (Kim et al., 2016; Gaˇsi´c et al., 2017; Zhao and Eskenazi, 2018) which, similarly to our work, involves automatically enabling an NLUI to a given system. Unlike in our task, the tasks that are investigated in those papers do not require the mapping of natural language to a meaning representation over a space of compositional logical forms. Other tasks related to ours include program synthesis (Raza et al., 2015; Desai et al., 2016) and mapping natural language to bash code (Lin et al., 2018), but these also did not consider zero-shot setups and did not synthesize code in the context of an application state. Semantic Parsing with In-domain Data Among the semantic parsing work that relied on in-domain data, many relied on a domain-specific lexicon (Kwiatkowski et al., 2010; Gerber and Ngomo, 2011; Krishnamurthy and Mitchell, 2012; Zettlemoyer and Collins, 2005; Cai and Yates, 2013) which maps natural language phrases to primitive logical forms. Many of these works automatically constructed a domain-specific lexicon using some additional domain-specific resources that are associated"
P19-1438,P16-1138,0,0.793732,"re functionality could be accessed via a natural language user interface (NLUI). If so, we better seek answers to the following questions: Will every developing team need to hire NLP experts to develop a NLUI for 1 Our code and data are https://github.com/givoli/TechnionNLI. available at: their specific application? Can we hope for a general framework that once trained on annotated data from a set of domains, does not require annotated data from a newly presented domain? Previous work on tasks related to NLUI for applications mostly relied on in-domain data (e.g. Artzi and Zettlemoyer (2013); Long et al. (2016)), and papers that did not rely on in-domain data did not attempt to parse instructions into compositional logical forms (Kim et al., 2016). To fill this gap, we address the task of zeroshot semantic parsing for instructions: training a parser so that it can parse instructions into compositional logical forms, where the instructions are from domains that were not seen during training. Formally, our task assumes a set D = {d1 , ..., dn } of source domains, each corresponding to a simple application (e.g. a calendar or a file manager) and an application program interface (API) consisting of a se"
P19-1438,P15-1142,0,0.148764,"not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application’s initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser.1 1 Introduction The idea of interacting with machines via natural language instructions and queries has fascinated researchers for decades (Winograd, 1971). Recent years have seen an increasing number of applications that have a natural language interface, either in the form of chatbots or via “intelligent personal assistants” such as Alexa (Amaz"
P19-1438,D17-1127,0,0.0140223,"5) which contains questions about Wikipedia tables, such that the context of each question is a single table. They evaluate a parser on questions about tables that have not been observed during training. Their work does not fully constitute zero-shot semantic parsing due to table columns across the train/test split that share column headers (which correspond to primitive logical forms that represent relations). Our parser is based on the floating parser introduced in that paper, and the space of logical forms we use is very similar to theirs (see § 4.1). Recently, Herzig and Berant (2017) and Su and Yan (2017) experimented with the Overnight dataset (Wang et al., 2015) in cross-domain settings. These papers did not experiment with zeroshot setups (i.e. training without any data from the target domain), and they both observed that the less in-domain training data was used, the more training data from other domains was valuable. Recently Herzig and Berant (2018) explored zeroshot semantic parsing with the Overnight dataset. Their framework, unlike ours, requires logical form annotation, and is designed for question answering rather than instruction parsing. Another zero-shot semantic parsing task was"
P19-1438,P15-1129,0,0.130491,"Missing"
P19-1438,D18-1193,0,0.0132695,"ith the Overnight dataset (Wang et al., 2015) in cross-domain settings. These papers did not experiment with zeroshot setups (i.e. training without any data from the target domain), and they both observed that the less in-domain training data was used, the more training data from other domains was valuable. Recently Herzig and Berant (2018) explored zeroshot semantic parsing with the Overnight dataset. Their framework, unlike ours, requires logical form annotation, and is designed for question answering rather than instruction parsing. Another zero-shot semantic parsing task was introduced in Yu et al. (2018a). The task requires mapping natural language questions to SQL queries, and includes a setting in which no databases appear in both the training and test sets (as attempted in Yu et al. (2018b)). 3 Task and Data We now describe our task and dataset. 3.1 sponds to a lighting control system application that allows the user to turn the lights on and off in each room in their house. Formally, a domain has a set of interface methods (e.g. turnLightOn and turnLightOff) that can be invoked with some arguments. Each argument is a set of entities (e.g. a set of Room entities). There are two kinds of e"
P19-1438,W18-5001,0,0.0300131,"answer or a change in some state, respectively. Our work is the first to address the novel semantic parsing task of mapping natural language instructions into compositional logical forms in zero-shot settings. While in our task each example contains a single sentence instruction, there are works on semantic parsing for instruction sequences (MacMahon et al., 2006; Long et al., 2016), but not in a zero-shot setup. We keep zero-shot parsing of instruction sequences for future research. A lot of work has been done on slot tagging and goal-oriented dialog (Kim et al., 2016; Gaˇsi´c et al., 2017; Zhao and Eskenazi, 2018) which, similarly to our work, involves automatically enabling an NLUI to a given system. Unlike in our task, the tasks that are investigated in those papers do not require the mapping of natural language to a meaning representation over a space of compositional logical forms. Other tasks related to ours include program synthesis (Raza et al., 2015; Desai et al., 2016) and mapping natural language to bash code (Lin et al., 2018), but these also did not consider zero-shot setups and did not synthesize code in the context of an application state. Semantic Parsing with In-domain Data Among the se"
P19-1591,P07-1056,0,0.975509,"score of each pivot p min(f ,fp−target ) to be: f reqScore(p) = max(fp−source , p−source ,fp−target ) and rank the pivots in a descending order of f reqScore scores. This way, pivots with more similar frequencies in the unlabeled data of both domains are ranked higher and will be exposed earlier to the PBLM algorithm. 4 Experiments We implemented the setup of ZR18, including datasets, baselines, and hyperparameter details. Task and Domains Following ZR18, and a large body of DA work, we experiment with the task of binary cross-domain sentiment classification with the product review domains of Blitzer et al. (2007) – Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K). We also consider the airline review domain that was presented by ZR18, who demonstrated that adaptation from the Blitzer product domains to this domain, and vice versa, is more challenging than adaptation between the Blitzer product domains. For each of the domains we consider 2000 labeled reviews, 1000 positive and 1000 negative, and unlabeled reviews: 6000 (B), 34741 (D), 13153 (E), 16785 (K) and 39396 (A). Since PBLM is computationally demanding, and employing TRL to PBLM requires multiple PBLM training processes, we p"
P19-1591,W06-1615,0,0.962258,"ctively applied in a variety of target domains. Indeed, DA algorithms have been developed for many NLP tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 2014)). A number of approaches for DA have been proposed (§ 2). With the raise of Neural Networks (NNs), DA through Representation Learning (DReL) where a shared feature space for the source and the target domains is learned, has 1 Our code is publicly available at: https://github. com/yftah89/TRL-PBLM. become prominent. Earlier DReL approaches (Blitzer et al., 2006, 2007) were based on a linear mapping of the original feature space to a new one, modeling the connections between pivot features – features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain – and the complementary set of non-pivot features. This approach was later outperformed by autoencoder (AE) based methods (Glorot et al., 2011; Chen et al., 2012), which employ compress-based noise reduction to extract the shared feature space, but do not explicitly model the correspondence between the source and the target domains. R"
P19-1591,P15-1071,0,0.0399335,"lly these pivot features are defined to be: (a) frequent in the unlabeled data from both domains; and (b) highly correlated with the task label in the source domain labeled data. The remaining features are referred to as non-pivot features. In SCL, the division of the original feature set into the pivot and non-pivot subsets is utilized in order to learn a linear mapping from the original feature space of both domains into a shared, low-dimensional, real-valued feature space. Since SCL was presented, pivot-based DReL has been researched extensively (e.g. (Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Ziser and Reichart, 2017, 2018a)). In contrast to SCL that learns a linear transforamtion between pivot and non-pivot features, the next line of work aimed to learn representations with non-linear models, without making the distinction between pivot and non-pivot features. The basic idea of these models is training an autoencoder (AE) on the unlabeled data from both the source and the target domains, reasoning that the hidden representation of such a model should be less noisy and hence robust to domain changes. Examples of AE variants in recent DReL literature include St"
P19-1591,W04-3237,0,0.213384,"Missing"
P19-1591,P07-1034,0,0.53902,"than plain PBLM across model configurations, making the model much better fitted for practical use.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, as many language processing algorithms require costly labeled data that can be found in only a handful of domains. To solve this annotation bottleneck, DA aims to train algorithms with labeled data from one or more source domains so that they can be effectively applied in a variety of target domains. Indeed, DA algorithms have been developed for many NLP tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 2014)). A number of approaches for DA have been proposed (§ 2). With the raise of Neural Networks (NNs), DA through Representation Learning (DReL) where a shared feature space for the source and the target domains is learned, has 1 Our code is publicly available at: https://github. com/yftah89/TRL-PBLM. become prominent. Earlier DReL approaches (Blitzer et al., 2006, 2007) were based on a linear mapping of the original feature space to a new one, modeling the connections between pivot featur"
P19-1591,P16-2005,0,0.0960551,"on-linear models, without making the distinction between pivot and non-pivot features. The basic idea of these models is training an autoencoder (AE) on the unlabeled data from both the source and the target domains, reasoning that the hidden representation of such a model should be less noisy and hence robust to domain changes. Examples of AE variants in recent DReL literature include Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008; Glorot et al., 2011), the more efficient and salable marginalized SDA (MSDA, (Chen et al., 2012)), and MSDA variants (e.g. (Yang and Eisenstein, 2014; Clinchant et al., 2016)). Models based on variational AEs (Kingma and Welling, 2014; Rezende et al., 2014) have also been applied in DA (e.g. variational fair autoencoder (Louizos et al., 2016)), but they were outperformed by MSDA in Ziser and Reichart (2018a). Ziser and Reichart (2017) combined AEs with pivot-based DA. Their models (AE-SCL and AESCL-SR) are based on a three layer feed-forward network where the non-pivot features are fed to the input layer, encoded into a hidden representation and this hidden representation is then decoded into the pivot features of the input example. AESCL-SR utilizes word embeddin"
P19-1591,P07-1033,0,0.409206,"Missing"
P19-1591,N10-1004,0,0.0319658,"s model configurations, making the model much better fitted for practical use.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, as many language processing algorithms require costly labeled data that can be found in only a handful of domains. To solve this annotation bottleneck, DA aims to train algorithms with labeled data from one or more source domains so that they can be effectively applied in a variety of target domains. Indeed, DA algorithms have been developed for many NLP tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 2014)). A number of approaches for DA have been proposed (§ 2). With the raise of Neural Networks (NNs), DA through Representation Learning (DReL) where a shared feature space for the source and the target domains is learned, has 1 Our code is publicly available at: https://github. com/yftah89/TRL-PBLM. become prominent. Earlier DReL approaches (Blitzer et al., 2006, 2007) were based on a linear mapping of the original feature space to a new one, modeling the connections between pivot features – features that are"
P19-1591,D17-1035,0,0.0139733,"labeled data from the source and target domains (AEs in ZR17, LSTMs (Hochreiter and Schmidhuber, 1997) in ZR18). The first limitation is due to the large number of pivot features (several hundreds in each source/target domain pair in their experiments), which makes the classification task challenging and may harm the quality of the resulting crossdomain representations. As another limitation, NNs, and especially those that perform sequence tagging like PBLM (Pivot Based Language Modeling, ZR18), are highly sensitive to model design and hyper-parameter selection decisions (Hutter et al., 2014; Reimers and Gurevych, 2017). Intuitively, if a DA approach is not robust across hyper-parameter configurations, it is more chal5895 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5895–5906 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lenging to apply this approach to a variety of domain pairs. This is particularly worrisome in unsupervised domain adaptation (our focus setup, § 2), where no target domain labeled data is available, and hyper-parameter and configuration tuning is performed on source domain labeled data only. In t"
P19-1591,N03-1027,0,0.276878,"r, TRL-PBLM-CNN is more robust than plain PBLM-CNN, consistently achieving a higher maximum, minimum and average results as well as a lower standard deviation across the 30 configurations we considered for each model. We consider this a major result since, as noted above, stability is crucial for the real-world applicability of an unsupervised domain adaptation algorithm, since the selection of model configuration in this setup does not involve target domain labeled data and is hence inherently noisy and risky. 2 Background and Previous Work Domain adaptation is a long standing NLP challenge (Roark and Bacchiani, 2003; Chelba and 2 Since TRL-PBLM requires multiple PBLM training stages, it was computationally demanding to experiment with all the 20 domain pairs of ZR18. See § 4 for more details. 5896 Acero, 2004; Daum´e III and Marcu, 2006). Major approaches to DA include: instance re-weighting (Huang et al., 2007; Mansour et al., 2009), subsampling from both domains (Chen et al., 2011) and DA through Representation Learning (DReL) where a joint source and target feature representation is learned. DReL has shown to be the state-ofthe-art for unsupervised DA (Ziser and Reichart, 2017, 2018a,b), and is the ap"
P19-1591,D12-1131,1,0.820587,"for practical use.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, as many language processing algorithms require costly labeled data that can be found in only a handful of domains. To solve this annotation bottleneck, DA aims to train algorithms with labeled data from one or more source domains so that they can be effectively applied in a variety of target domains. Indeed, DA algorithms have been developed for many NLP tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 2014)). A number of approaches for DA have been proposed (§ 2). With the raise of Neural Networks (NNs), DA through Representation Learning (DReL) where a shared feature space for the source and the target domains is learned, has 1 Our code is publicly available at: https://github. com/yftah89/TRL-PBLM. become prominent. Earlier DReL approaches (Blitzer et al., 2006, 2007) were based on a linear mapping of the original feature space to a new one, modeling the connections between pivot features – features that are frequent in the source and the target domains and are hi"
P19-1591,P16-1043,0,0.064237,"Missing"
P19-1591,Q14-1002,0,0.150899,"Missing"
P19-1591,D13-1141,0,0.0403663,"Missing"
P19-1591,N10-1116,0,0.100303,"Missing"
P19-1591,P11-1007,0,0.0228989,"making the model much better fitted for practical use.1 1 Introduction Domain adaptation (DA, (Daum´e III, 2007; BenDavid et al., 2010)) is a fundamental challenge in NLP, as many language processing algorithms require costly labeled data that can be found in only a handful of domains. To solve this annotation bottleneck, DA aims to train algorithms with labeled data from one or more source domains so that they can be effectively applied in a variety of target domains. Indeed, DA algorithms have been developed for many NLP tasks and domains (e.g. (Jiang and Zhai, 2007; McClosky et al., 2010; Titov, 2011; Bollegala et al., 2011; Rush et al., 2012; Schnabel and Sch¨utze, 2014)). A number of approaches for DA have been proposed (§ 2). With the raise of Neural Networks (NNs), DA through Representation Learning (DReL) where a shared feature space for the source and the target domains is learned, has 1 Our code is publicly available at: https://github. com/yftah89/TRL-PBLM. become prominent. Earlier DReL approaches (Blitzer et al., 2006, 2007) were based on a linear mapping of the original feature space to a new one, modeling the connections between pivot features – features that are frequent in t"
P19-1591,P10-1040,0,0.068063,"Missing"
P19-1591,D16-1157,0,0.0438407,"Missing"
P19-1591,P14-2088,0,0.298291,"earn representations with non-linear models, without making the distinction between pivot and non-pivot features. The basic idea of these models is training an autoencoder (AE) on the unlabeled data from both the source and the target domains, reasoning that the hidden representation of such a model should be less noisy and hence robust to domain changes. Examples of AE variants in recent DReL literature include Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008; Glorot et al., 2011), the more efficient and salable marginalized SDA (MSDA, (Chen et al., 2012)), and MSDA variants (e.g. (Yang and Eisenstein, 2014; Clinchant et al., 2016)). Models based on variational AEs (Kingma and Welling, 2014; Rezende et al., 2014) have also been applied in DA (e.g. variational fair autoencoder (Louizos et al., 2016)), but they were outperformed by MSDA in Ziser and Reichart (2018a). Ziser and Reichart (2017) combined AEs with pivot-based DA. Their models (AE-SCL and AESCL-SR) are based on a three layer feed-forward network where the non-pivot features are fed to the input layer, encoded into a hidden representation and this hidden representation is then decoded into the pivot features of the input example. AESCL-"
P19-1591,D16-1023,0,0.0873186,"are defined to be: (a) frequent in the unlabeled data from both domains; and (b) highly correlated with the task label in the source domain labeled data. The remaining features are referred to as non-pivot features. In SCL, the division of the original feature set into the pivot and non-pivot subsets is utilized in order to learn a linear mapping from the original feature space of both domains into a shared, low-dimensional, real-valued feature space. Since SCL was presented, pivot-based DReL has been researched extensively (e.g. (Pan et al., 2010; Gouws et al., 2012; Bollegala et al., 2015; Yu and Jiang, 2016; Ziser and Reichart, 2017, 2018a)). In contrast to SCL that learns a linear transforamtion between pivot and non-pivot features, the next line of work aimed to learn representations with non-linear models, without making the distinction between pivot and non-pivot features. The basic idea of these models is training an autoencoder (AE) on the unlabeled data from both the source and the target domains, reasoning that the hidden representation of such a model should be less noisy and hence robust to domain changes. Examples of AE variants in recent DReL literature include Stacked Denoising Auto"
P19-1591,K17-1040,1,0.889588,"eling the connections between pivot features – features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain – and the complementary set of non-pivot features. This approach was later outperformed by autoencoder (AE) based methods (Glorot et al., 2011; Chen et al., 2012), which employ compress-based noise reduction to extract the shared feature space, but do not explicitly model the correspondence between the source and the target domains. Recently, methods that marry the complementary strengths of NNs and pivot-based ideas (Ziser and Reichart (2017, 2018a), denoted here with ZR17 and ZR18, respectively) established a new state-of-the-art. Despite their strong empirical results, relying on NNs and on the distinction between pivot and non-pivot features, the models in ZR17 and ZR18 suffer from two limitations. These limitations stem from the fact that in order to create the shared feature space these models train NNs to predict the existence of pivot features in unlabeled data from the source and target domains (AEs in ZR17, LSTMs (Hochreiter and Schmidhuber, 1997) in ZR18). The first limitation is due to the large number of pivot feature"
P19-1591,N18-1112,1,0.4764,"e hidden representation of such a model should be less noisy and hence robust to domain changes. Examples of AE variants in recent DReL literature include Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008; Glorot et al., 2011), the more efficient and salable marginalized SDA (MSDA, (Chen et al., 2012)), and MSDA variants (e.g. (Yang and Eisenstein, 2014; Clinchant et al., 2016)). Models based on variational AEs (Kingma and Welling, 2014; Rezende et al., 2014) have also been applied in DA (e.g. variational fair autoencoder (Louizos et al., 2016)), but they were outperformed by MSDA in Ziser and Reichart (2018a). Ziser and Reichart (2017) combined AEs with pivot-based DA. Their models (AE-SCL and AESCL-SR) are based on a three layer feed-forward network where the non-pivot features are fed to the input layer, encoded into a hidden representation and this hidden representation is then decoded into the pivot features of the input example. AESCL-SR utilizes word embeddings to exploit the similarities between pivot-based features, outperforming AE-SCL, and many other DReL models. A major limitation of the ZR17 models is that they do not exploit the structure of their input examples, which can harm docu"
P19-1591,D18-1022,1,0.863273,"e hidden representation of such a model should be less noisy and hence robust to domain changes. Examples of AE variants in recent DReL literature include Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008; Glorot et al., 2011), the more efficient and salable marginalized SDA (MSDA, (Chen et al., 2012)), and MSDA variants (e.g. (Yang and Eisenstein, 2014; Clinchant et al., 2016)). Models based on variational AEs (Kingma and Welling, 2014; Rezende et al., 2014) have also been applied in DA (e.g. variational fair autoencoder (Louizos et al., 2016)), but they were outperformed by MSDA in Ziser and Reichart (2018a). Ziser and Reichart (2017) combined AEs with pivot-based DA. Their models (AE-SCL and AESCL-SR) are based on a three layer feed-forward network where the non-pivot features are fed to the input layer, encoded into a hidden representation and this hidden representation is then decoded into the pivot features of the input example. AESCL-SR utilizes word embeddings to exploit the similarities between pivot-based features, outperforming AE-SCL, and many other DReL models. A major limitation of the ZR17 models is that they do not exploit the structure of their input examples, which can harm docu"
Q14-1023,D10-1115,0,0.083666,"Missing"
Q14-1023,P12-1015,0,0.1434,"also what the child perceives about the world around it when the word is heard. Learning the meaning of words requires not only a sensitivity to both linguistic and perceptual input, but also the ability to process and combine information from these modalities in a productive way. However, the majority of perceptual input for the models in these studies corresponds directly to concrete noun concepts, such as chocolate or cheeseburger, and the superiority of the multi-modal over the corpus-only approach has only been established when evaluations include such concepts (Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012). It is thus unclear if the multi-modal approach is effective for more abstract words, such as guilt or obesity. Indeed, since empirical evidence indicates differences in the representational frameworks of both concrete and abstract concepts (Paivio, 1991; Hill et al., 2013), and verb and noun concepts (Markman and Wisniewski, 1997), perceptual information may not fulfill the same role in the representation of the various concept types. This potential challenge to the multi-modal approach is of particular practical importance since"
Q14-1023,D08-1094,0,0.0586223,"Missing"
Q14-1023,N10-1011,0,0.314881,"a corresponding set of pairs {(w1 , w2 ) ∈ U SF : w1 , w2 ∈ L} is defined for evaluation. These details are summarized in Table 2. Evaluation lists, sets of pairs and USF scores are downloadable from our website. 3.3 Evaluation Methodology All models are evaluated by measuring correlations with the free-association scores in the USF dataset (Nelson et al., 2004). This dataset contains the freeassociation strength of over 150,000 word pairs.3 These data reflect the cognitive proximity of concepts and have been widely used in NLP as a goldstandard for computational models (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). For evaluation pairs (c1 , c2 ) we calculate the cosine similarity between our learned feature representations for c1 and c2 , a standard measure of the proximity of two vectors (Turney et al., 2010), and follow previous studies (Leong and Mihalcea, 2011; Huang et al., 2012) in using Spearman’s ρ as a measure of correlation between these values and our goldstandard.4 All representations in this section are combined by concatenation, since the present focus is not on combination methods.5 3 Free-association strength is measured by"
Q14-1023,J02-3001,0,0.00451227,"295 1716 66 127 221 Examples yacht, cup fear, respect cup, respect kiss, launch differ, obey kiss, differ Table 2: Evaluation sets used throughout. All nouns and all verbs are the union of abstract and concrete subsets and mixed abstract-concrete or concrete-abstract pairs. Table 1: Grammatical features for noun/verb concepts nouns, such as shiver or walk, often refer to processes rather than entities. To capture such effects, we count the frequency of occurrence with the POS categories ajdective, adverb, noun and verb. Grammatical Features Grammatical role is a strong predictor of semantics (Gildea and Jurafsky, 2002). For instance, the subject of transitive verbs is more likely to refer to an animate entity than a noun chosen at random. Syntactic context also predicts verb semantics (Kipper et al., 2008). We thus count the frequency of nouns in a range of (nonlexicalized) syntactic contexts, and of verbs in one of the six most common subcategorization-frame classes as defined in Van de Cruys et al. (2012). These contexts are detailed in Table 1. 3.2 Evaluation Sets We create evaluation sets of abstract and concrete concepts, and introduce a complementary dichotomy between nouns and verbs, the two POS cate"
Q14-1023,S13-1035,0,0.0149563,"ction 2). In light of these considerations, this paper addresses three questions: (1) Which information sources (modalities) are important for acquiring concepts of different types? (2) Can perceptual input be propagated effectively from concrete to more abstract words? (3) What is the best way to combine information from the different sources? We construct models that acquire semantic representations for four sets of concepts: concrete nouns, abstract nouns, concrete verbs and abstract verbs. The linguistic input to the models comes from the recently released Google Syntactic N-Grams Corpus (Goldberg and Orwant, 2013), from which a selection of linguistic features are extracted. Perceptual input is approximated by data from the McRae et al. (2005) norms, which encode perceptual properties of concrete nouns, and the ESPGame dataset (Von Ahn and Dabbish, 2004), which contains manually generated descriptions of 100,000 images. To address (1) we extract representations for each concept type from combinations of information sources. We first focus on different classes of linguistic features, before extending our models to the multi-modal context. While linguistic information overall effectively reflects the mea"
Q14-1023,P12-1092,0,0.0267663,"(Nelson et al., 2004). This dataset contains the freeassociation strength of over 150,000 word pairs.3 These data reflect the cognitive proximity of concepts and have been widely used in NLP as a goldstandard for computational models (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). For evaluation pairs (c1 , c2 ) we calculate the cosine similarity between our learned feature representations for c1 and c2 , a standard measure of the proximity of two vectors (Turney et al., 2010), and follow previous studies (Leong and Mihalcea, 2011; Huang et al., 2012) in using Spearman’s ρ as a measure of correlation between these values and our goldstandard.4 All representations in this section are combined by concatenation, since the present focus is not on combination methods.5 3 Free-association strength is measured by presenting subjects with a cue word and asking them to produce the first word they can think of that is associated with that cue word. 4 We consider Spearman’s ρ, a non-parametric ranking correlation, to be more appropriate than Pearson’s r for free association data, which is naturally skewed and non-continuous. 5 When combining multiple"
Q14-1023,J06-2001,0,0.0185032,"Missing"
Q14-1023,W10-0608,1,0.904359,"Missing"
Q14-1023,P14-2135,1,0.614518,"hese comparisons, the optimal combination method is selected in each case. 294 deed, conceptual concreteness appears to determine the degree to which perceptual input is beneficial, since representations of abstract verbs, the most abstract concepts in our experiments, were actually degraded by this additional information. One important contribution of this work is therefore an insight into when multi-modal models should or should not aim to combine and/or propagate perceptual input to ensure that optimal representations are learned. In this respect, our conclusions align with the findings of Kiela and Hill (2014), who take an explicitly visual approach to resolving the same question. Various methods for propagating and combining perceptual information with linguistic input were presented. We proposed ridge regression for inferring perceptual representations for abstract concepts, which proved more robust than alternatives across the range of concept types. This approach is particularly simple to implement, since it is based on an established statistical prodedure. In addition, we introduced weighted gram matrix combination for combining representations from distinct modalities of differing sparsity an"
Q14-1023,C94-1103,0,0.0597624,"Missing"
Q14-1023,I11-1162,0,0.064937,"or speaker intention, but also what the child perceives about the world around it when the word is heard. Learning the meaning of words requires not only a sensitivity to both linguistic and perceptual input, but also the ability to process and combine information from these modalities in a productive way. However, the majority of perceptual input for the models in these studies corresponds directly to concrete noun concepts, such as chocolate or cheeseburger, and the superiority of the multi-modal over the corpus-only approach has only been established when evaluations include such concepts (Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012). It is thus unclear if the multi-modal approach is effective for more abstract words, such as guilt or obesity. Indeed, since empirical evidence indicates differences in the representational frameworks of both concrete and abstract concepts (Paivio, 1991; Hill et al., 2013), and verb and noun concepts (Markman and Wisniewski, 1997), perceptual information may not fulfill the same role in the representation of the various concept types. This potential challenge to the multi-modal approach is of particular practi"
Q14-1023,P13-1085,1,0.80337,"our evaluations, perceptual input actually degrades representation quality. This highlights the 286 need to consider the concreteness of the target domain when constructing multi-modal models. To address (3), we present various means of combining information from different modalities. We propose weighted gram matrix combination, a technique in which representations of distinct modalities are mapped to a space of common dimension where coordinates reflect proximity to other concepts. This transformation, which has been shown to enhance semantic representations in the context of verbclustering (Reichart and Korhonen, 2013), reduces representation sparsity and facilitates a productbased combination that results in greater inter-modal dependency. Weighted gram matrix combination outperforms alternatives such as concatenation and Canonical Correlation Analysis (CCA) (Hardoon et al., 2004) when combining representations from two similarly rich information sources. In Section 3, we present experiments with linguistic features designed to address question (1). These analyses are extended to multi-modal models in Section 4, where we also address (2) and (3). We first discuss the relevance of concreteness and part-ofsp"
Q14-1023,D13-1115,0,0.270695,"Missing"
Q14-1023,D12-1130,0,0.0802291,"ersity of Cambridge fh295@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract Many computational semantic models represent words as real-valued vectors, encoding their relative frequency of occurrence in particular forms and contexts in linguistic corpora (Sahlgren, 2006; Turney et al., 2010). Motivated both by parallels with human language acquisition and by evidence that many word meanings are grounded in the perceptual system (Barsalou et al., 2003), recent research has explored the integration into text-based models of input that approximates the visual or other sensory modalities (Silberer and Lapata, 2012; Bruni et al., 2014). Such models can learn higher-quality semantic representations than conventional corpusonly models, as evidenced by a range of evaluations. Multi-modal models that learn semantic representations from both linguistic and perceptual input outperform language-only models on a range of evaluations, and better reflect human concept acquisition. Most perceptual input to such models corresponds to concrete noun concepts and the superiority of the multimodal approach has only been established when evaluating on such concepts. We therefore investigate which concepts can be effecti"
Q14-1023,P13-1056,0,0.0328046,"pairs. The path similarity between words w1 and w2 is the shortest distance between synsets of w1 and w2 in the WordNet taxonomy (Fellbaum, 1999), which correlates significantly with human judgements of concept similarity (Pedersen et al., 2004).10 The correlations with the USF data (left hand column, Table 4) of our linguistic-only models (ρ = 0.094 − 0.233) and best performing multi-modal models (on both concrete nouns, ρ = 0.397, and more abstract concepts, ρ = 0.095 − 0.301) were higher than the best comparable models described elsewhere (Feng and Lapata, 2010; Silberer and Lapata, 2012; Silberer et al., 2013).11 This confirms 10 Other widely-used evaluation gold-standards, such as WordSim 353 and the MEN dataset, do not contain a sufficient number of abstract concepts for the current purpose. 11 Feng and Lapata (2010) report ρ = .08 for language-only 293 both that the underlying linguistic space is of high quality and that the ESP and McRae perceptual input is similarly or more informative than the input applied in previous work. Consistent with previous studies, adding perceptual input improved the quality of concrete noun representations as measured against both USF and path similarity gold-stan"
Q14-1023,C12-1165,1,\N,Missing
Q15-1010,P14-1056,0,0.025493,"relations and their temporal spans and applied distant learning to find approximate instances for classifier training. A set of constraint templates specific to temporal learning were also specified. In contrast, we do not use manually specified guidance in constraint learning. Particularly, we construct constraints from latent variables (topics in topic modeling) estimated from raw text rather than applying maximum likelihood estimation over observed variables (fluents and temporal expressions) in labeled data. Our method is therefore less dependent on human supervision. Even more recently, (Anzaroot et al., 2014) presented a supervised dual-decomposition based method, in the context of citation field extraction, which automatically generates large families of constraints and learn their costs with a convex optimization objective during training. Our work is unsupervised, as opposed to their model which requires a manually annotated training corpus for constraint learning. Information Structure Analysis Various schemes have been proposed for analysing the information structure of scientific documents, in particular the patterns of topics, functions and relations at sentence level. Existing schemes incl"
Q15-1010,P07-1036,0,0.0874532,"f (Guo et al., 2013a) and those in the gold standard. Our work demonstrates the great potential of automatically induced declarative knowledge in both improving the performance of information structure analysis and reducing reliance of human supervision. 2 Previous Work Automatic Declarative Knowledge Induction Learning with declarative knowledge offers effective means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it h"
Q15-1010,C12-1041,1,0.925037,"round or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challen"
Q15-1010,P07-2009,0,0.0246166,"Missing"
Q15-1010,P07-1094,0,0.0417074,"g 150 sentences from that section, as in the lightly supervised case in (Guo et al., 2013a) (MaxEnt); and (d) a baseline that assigns all the sentences in a given section to the most frequent gold-standard category of that section (Table 3). This baseline emulates the use of section names for information structure classification. Our constraints, which we use in the TopicGE and TopicGC models, are based on topics that are learned on the test corpus. While having access to the raw test text at training time is a standard assumption in many unsupervised NLP works (e.g. (Klein and Manning, 2004; Goldwater and Griffiths, 2007; Lang and Lapata, 2014)), it is important to quantify the extent to which our method depends on its access to the test set. We therefore constructed the TopicGE* model which is identical to TopicGE except that the topics are learned from another collection of 47 biomedical articles containing 9352 sentences. Like our test set, these articles are from the cancer risk assessment domain - all of them were published in the Toxicol. Sci. journal in the years 2009-2012 and were retrieved using the PubMed search engine with the key words “cancer risk assessment”. There is no overlap between this new"
Q15-1010,W10-1913,1,0.89922,"l., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analysis using expert knowledge in the form of discourse and lexical"
Q15-1010,D11-1025,1,0.811028,"a sentence) into a category that represents the information type it conveys. By information structure we refer to a particular type of discourse structure that focuses on the functional role of a unit in the discourse (Webber et al., 2011). For instance, in the scientific literature, the functional role of a sentence could be the background or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et a"
Q15-1010,N13-1113,1,0.0921206,"2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analysis using expert knowledge in the form of discourse and lexical constraints. Their model produces promising results, especially for sections and categories where 131 Transactions of the Association for Computational Linguistics, vol. 3, pp. 131–143, 2015. Action Editor: Masaaki Nagata. c Submission batch: 10/2014; Revision batch 1/2015; Publish"
Q15-1010,I08-1050,0,0.0267098,"faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analysis using expert knowled"
Q15-1010,P04-1061,0,0.0511901,"a particular section using 150 sentences from that section, as in the lightly supervised case in (Guo et al., 2013a) (MaxEnt); and (d) a baseline that assigns all the sentences in a given section to the most frequent gold-standard category of that section (Table 3). This baseline emulates the use of section names for information structure classification. Our constraints, which we use in the TopicGE and TopicGC models, are based on topics that are learned on the test corpus. While having access to the raw test text at training time is a standard assumption in many unsupervised NLP works (e.g. (Klein and Manning, 2004; Goldwater and Griffiths, 2007; Lang and Lapata, 2014)), it is important to quantify the extent to which our method depends on its access to the test set. We therefore constructed the TopicGE* model which is identical to TopicGE except that the topics are learned from another collection of 47 biomedical articles containing 9352 sentences. Like our test set, these articles are from the cancer risk assessment domain - all of them were published in the Toxicol. Sci. journal in the years 2009-2012 and were retrieved using the PubMed search engine with the key words “cancer risk assessment”. There"
Q15-1010,J14-3006,0,0.0211183,"on, as in the lightly supervised case in (Guo et al., 2013a) (MaxEnt); and (d) a baseline that assigns all the sentences in a given section to the most frequent gold-standard category of that section (Table 3). This baseline emulates the use of section names for information structure classification. Our constraints, which we use in the TopicGE and TopicGC models, are based on topics that are learned on the test corpus. While having access to the raw test text at training time is a standard assumption in many unsupervised NLP works (e.g. (Klein and Manning, 2004; Goldwater and Griffiths, 2007; Lang and Lapata, 2014)), it is important to quantify the extent to which our method depends on its access to the test set. We therefore constructed the TopicGE* model which is identical to TopicGE except that the topics are learned from another collection of 47 biomedical articles containing 9352 sentences. Like our test set, these articles are from the cancer risk assessment domain - all of them were published in the Toxicol. Sci. journal in the years 2009-2012 and were retrieved using the PubMed search engine with the key words “cancer risk assessment”. There is no overlap between this new dataset and our test se"
Q15-1010,liakata-etal-2010-corpora,0,0.0234495,"sed, as opposed to their model which requires a manually annotated training corpus for constraint learning. Information Structure Analysis Various schemes have been proposed for analysing the information structure of scientific documents, in particular the patterns of topics, functions and relations at sentence level. Existing schemes include argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), discourse structure (Burstein et al., 2003; Webber et al., 2011), qualitative dimensions (Shatkay et al., 2008), scientific claims (Blake, 2009), scientific concepts (Liakata et al., 2010), and information status (Markert et al., 2012), among others. Most previous work on automatic analysis of information structure relies on supervised learning (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Given the prohibitive cost 133 of manual annotation, unsupervised and minimally supervised techniques such as clustering (Kiela et al., 2014) and topic modeling (Varga et al., 2012; ´ S´eaghdha and Teufel, 2014) are highly important. O However, the performance of such approaches shows a"
Q15-1010,W06-3309,0,0.0402377,"ted articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analys"
Q15-1010,W04-1013,0,0.00604792,"Missing"
Q15-1010,P08-1099,0,0.565872,"tandard. Our work demonstrates the great potential of automatically induced declarative knowledge in both improving the performance of information structure analysis and reducing reliance of human supervision. 2 Previous Work Automatic Declarative Knowledge Induction Learning with declarative knowledge offers effective means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it here to information structure analysis of scientifi"
Q15-1010,P12-1084,0,0.0243885,"manually annotated training corpus for constraint learning. Information Structure Analysis Various schemes have been proposed for analysing the information structure of scientific documents, in particular the patterns of topics, functions and relations at sentence level. Existing schemes include argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), discourse structure (Burstein et al., 2003; Webber et al., 2011), qualitative dimensions (Shatkay et al., 2008), scientific claims (Blake, 2009), scientific concepts (Liakata et al., 2010), and information status (Markert et al., 2012), among others. Most previous work on automatic analysis of information structure relies on supervised learning (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Given the prohibitive cost 133 of manual annotation, unsupervised and minimally supervised techniques such as clustering (Kiela et al., 2014) and topic modeling (Varga et al., 2012; ´ S´eaghdha and Teufel, 2014) are highly important. O However, the performance of such approaches shows a large room for improvement. Our work is specif"
Q15-1010,D12-1080,0,0.146718,"g a detailed word list for each information structure category. For example, words such as “assay” were carefully selected and used as a strong indicator of the “Method” category: p(Method|assay) was constrained to be high (above 0.9). Such a constraint (developed for the biomedical domain) may not be applicable to a new domain (e.g. computer science) with a different vocabulary and writing style. In fact, most existing works on learning with declarative knowledge rely on manually constructed constraints. Little work exists on automatic declarative knowledge induction. A notable exception is (McClosky and Manning, 2012) that proposed a constraint learning model for timeline extraction. This approach, however, requires human supervision in several forms including task specific constraint templates (see Section 2). We present a novel framework for learning declarative knowledge which requires very limited human involvement. We apply it to information structure analysis, based on two key observations: 1) Each information structure category defines a distribution over a section-specific and an article-level set of linguistic features. 2) Each sentence in a scientific document, while having a dominant category, m"
Q15-1010,C14-1002,0,0.197314,"Missing"
Q15-1010,N12-1008,1,0.851522,"ive means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it here to information structure analysis of scientific documents. Little prior work exists on automatic constraint learning. Recently, (McClosky and Manning, 2012) investigated the approach for timeline extraction. They used a set of gold relations and their temporal spans and applied distant learning to find approximate instances for classifier training. A set of constrai"
Q15-1010,C12-2097,1,0.854093,"2). We present a novel framework for learning declarative knowledge which requires very limited human involvement. We apply it to information structure analysis, based on two key observations: 1) Each information structure category defines a distribution over a section-specific and an article-level set of linguistic features. 2) Each sentence in a scientific document, while having a dominant category, may consist of features mostly related to other categories. This flexible view enables us to make use of topic models which have not proved useful in previous related works (Varga et al., 2012; Reichart and Korhonen, 2012). We construct topic models at both the individual section and article level and apply these models to data, identifying latent topics and their key linguistic features. This information is used to constrain or bias unsupervised models for the task in a straightforward way: we automatically generate constraints for a GE model and a bias term for a graph clustering objective, such that the resulting models assign each of the input sentences to one information 132 Zone Background (BKG) Problem (PROB) Method (METH) Result (RES) Conclusion (CON) Connection (CN) Difference (DIFF) Future work (FUT)"
Q15-1010,W09-1121,1,0.778261,"Missing"
Q15-1010,D12-1131,1,0.831464,"supervision. 2 Previous Work Automatic Declarative Knowledge Induction Learning with declarative knowledge offers effective means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it here to information structure analysis of scientific documents. Little prior work exists on automatic constraint learning. Recently, (McClosky and Manning, 2012) investigated the approach for timeline extraction. They used a set of gold relat"
Q15-1010,J02-4002,0,0.50499,"ance, in the scientific literature, the functional role of a sentence could be the background or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and che"
Q15-1010,D09-1155,0,0.0886792,"e structure that focuses on the functional role of a unit in the discourse (Webber et al., 2011). For instance, in the scientific literature, the functional role of a sentence could be the background or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather l"
Q15-1010,varga-etal-2012-unsupervised,0,0.0791648,"mplates (see Section 2). We present a novel framework for learning declarative knowledge which requires very limited human involvement. We apply it to information structure analysis, based on two key observations: 1) Each information structure category defines a distribution over a section-specific and an article-level set of linguistic features. 2) Each sentence in a scientific document, while having a dominant category, may consist of features mostly related to other categories. This flexible view enables us to make use of topic models which have not proved useful in previous related works (Varga et al., 2012; Reichart and Korhonen, 2012). We construct topic models at both the individual section and article level and apply these models to data, identifying latent topics and their key linguistic features. This information is used to constrain or bias unsupervised models for the task in a straightforward way: we automatically generate constraints for a GE model and a bias term for a graph clustering objective, such that the resulting models assign each of the input sentences to one information 132 Zone Background (BKG) Problem (PROB) Method (METH) Result (RES) Conclusion (CON) Connection (CN) Differ"
Q17-1022,S15-1003,0,0.0166028,"scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and"
Q17-1022,Q16-1031,0,0.0184747,"gual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by co"
Q17-1022,P98-1013,0,0.0636087,"gue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating"
Q17-1022,P14-2131,0,0.0313084,"ract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories:"
Q17-1022,D14-1082,0,0.0239881,"lude: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train dis"
Q17-1022,P06-1038,0,0.011388,"enson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vecto"
Q17-1022,P14-1129,0,0.0264494,"Missing"
Q17-1022,D16-1136,0,0.0135803,"differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for reso"
Q17-1022,ehrmann-etal-2014-representing,0,0.0525517,"ural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones. Table 1 illustrates the effects of cross-lingual ATTRACT-R EPEL specialization by showing the nearest neighbors for three English words across three cross-lingual spaces. 309 Transactions of the Association for Computational Linguistics, vol. 5, pp. 309–324, 2017. Action Ed"
Q17-1022,E14-1049,0,0.359731,"al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vu"
Q17-1022,P15-2076,0,0.0360459,"borne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tuning Pre-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. F"
Q17-1022,N15-1184,0,0.532158,"s using Monolingual and Cross-Lingual Constraints Nikola Mrkši´c1,2 , Ivan Vuli´c1 , Diarmuid Ó Séaghdha2 , Ira Leviant3 Roi Reichart3 , Milica Gaši´c1 , Anna Korhonen1 , Steve Young1,2 1 University of Cambridge 2 Apple Inc. 3 Technion, IIT Abstract These models typically build on distributional ones by using human- or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections. Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016). We term this approach semantic specialization. We present ATTRACT-R EPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. ATTRACT-R EPEL facilitates the use of constraints from mono- and crosslingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct highquality vector spaces for a plethora of different languages, facilitating semantic transfer from high-"
Q17-1022,ganitkevitch-callison-burch-2014-multilingual,0,0.0509444,"Missing"
Q17-1022,N13-1092,0,0.0261119,"Missing"
Q17-1022,D16-1235,1,0.905072,"Missing"
Q17-1022,D14-1012,0,0.0360428,"tion datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into"
Q17-1022,P15-1119,0,0.0296068,"Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (Søgaard et al., 2015; Guo et al., 2015). However, prior work on cross-lingual word embedding has tended not to exploit pre-existing linguistic resources such as BabelNet. In this work, we make use of cross-lingual constraints derived from such repositories to induce high-quality cross-lingual vector spaces by facilitating semantic transfer from highto lower-resource languages. In our experiments, we show that cross-lingual vector spaces produced by ATTRACT-R EPEL consistently outperform a representative selection of five strong cross-lingual word embedding models in both intrinsic and extrinsic evaluation across several languages."
Q17-1022,W14-4337,0,0.0396757,"re expressed by slot-value pairs such as [price: cheap] or [food: Thai]. For modular task-based systems, the Dialogue State Tracking (DST) component is in charge of maintaining the belief state, which is the system’s internal distribution over the possible states of the dialogue. Figure 1 shows the correct dialogue state for each turn of an example dialogue. Unseen Data/Labels As dialogue ontologies can be very large, many of the possible class labels (i.e., the various food types or street names) will not occur in the training set. To overcome this problem, delexicalization-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrkši´c et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values. This is done through exact matching supplemented with semantic lexicons which encode rephrasings, morphology and other linguistic variation. For instance, such lexicons would be required to deal with the underlined non-exact matches in Figure 1. Exact Matching as a Bottleneck Semantic lexicons can be hand-crafted for small dialogue domains. Mrkši´c et al. (2016) showed that semantically specialized vect"
Q17-1022,W14-4340,1,0.938481,"re expressed by slot-value pairs such as [price: cheap] or [food: Thai]. For modular task-based systems, the Dialogue State Tracking (DST) component is in charge of maintaining the belief state, which is the system’s internal distribution over the possible states of the dialogue. Figure 1 shows the correct dialogue state for each turn of an example dialogue. Unseen Data/Labels As dialogue ontologies can be very large, many of the possible class labels (i.e., the various food types or street names) will not occur in the training set. To overcome this problem, delexicalization-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrkši´c et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values. This is done through exact matching supplemented with semantic lexicons which encode rephrasings, morphology and other linguistic variation. For instance, such lexicons would be required to deal with the underlined non-exact matches in Figure 1. Exact Matching as a Bottleneck Semantic lexicons can be hand-crafted for small dialogue domains. Mrkši´c et al. (2016) showed that semantically specialized vect"
Q17-1022,P14-1006,0,0.0667397,"ts models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual"
Q17-1022,J15-4004,1,0.925417,"multilingual DST models, which brings further performance improvements. 1 In this paper we advance the semantic specialization paradigm in a number of ways. We introduce a new algorithm, ATTRACT-R EPEL, that uses synonymy and antonymy constraints drawn from lexical resources to tune word vector spaces using linguistic information that is difficult to capture with conventional distributional training. Our evaluation shows that ATTRACT-R EPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We the"
Q17-1022,D15-1127,0,0.0127607,"our method to use existing cross-lingual resources to tie distributional vector spaces of different languages into a unified vector space which benefits from positive semantic transfer between its constituent languages. 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; V"
Q17-1022,D14-1070,0,0.0192932,"chart, 2015). To show that our approach yields semantically informative vectors for lower-resource languages, we collect intrinsic evaluation datasets for Hebrew and Croatian and show that cross-lingual specialization significantly improves word vector quality in these two (comparatively) low-resource languages. In the second part of the paper, we explore the use of ATTRACT-R EPEL-specialized vectors in a downstream application. One important motivation for training word vectors is to improve the lexical coverage of supervised models for language understanding tasks, e.g., question answering (Iyyer et al., 2014) or textual entailment (Rocktäschel et al., 2016). In 1 Some (negative) effects of the distributional hypothesis do persist. For example, nl_krieken (Dutch for cherries), is identified as a synonym for en_morning, presumably because the idiom ‘het krieken van de dag’ translates to ‘the crack of dawn’. 2 Our approach is not suited for languages for which no lexical resources exist. However, many languages have some coverage in cross-lingual lexicons. For instance, BabelNet 3.7 automatically aligns WordNet to Wikipedia, providing accurate cross-lingual mappings between 271 languages. In our eval"
Q17-1022,N15-1070,0,0.0340476,"ons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tuning Pre-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles o"
Q17-1022,D15-1245,0,0.0248451,"EL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations"
Q17-1022,D15-1242,0,0.0418903,"epresentations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use sy"
Q17-1022,W16-1607,0,0.169112,"Missing"
Q17-1022,C12-1089,0,0.0230184,"eover, we show that starting from distributional vectors allows our method to use existing cross-lingual resources to tie distributional vector spaces of different languages into a unified vector space which benefits from positive semantic transfer between its constituent languages. 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al."
Q17-1022,2005.mtsummit-papers.11,0,0.0253673,"into a shared cross-lingual space. Ideally, sharing information across languages should lead to improved semantic content for each language, especially for those with limited monolingual resources. Antonymy BabelNet is also used to extract both monolingual and cross-lingual antonymy constraints. Following Faruqui et al. (2015), who found PPDB constraints more beneficial than the WordNet ones, we do not use BabelNet for monolingual synonymy. Availability of Resources Both PPDB and BabelNet are created automatically. However, PPDB relies on large, high-quality parallel corpora such as Europarl (Koehn, 2005). In total, Multilingual PPDB provides collections of paraphrases for 22 languages. On the other hand, BabelNet uses Wikipedia’s interlanguage links and statistical machine translation (Google Translate) to provide cross-lingual mappings for 271 languages. In our evaluation, we show that PPDB and BabelNet can be used jointly to improve word representations for lower-resource languages by tying them into bilingual spaces with high-resource ones. We validate this claim on Hebrew and Croatian, which act as ‘lower-resource’ languages because of their lack of any PPDB resource and their relatively"
Q17-1022,P15-1027,0,0.0203234,"2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mi"
Q17-1022,P14-2050,0,0.10357,"use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones. T"
Q17-1022,P15-1145,0,0.0286255,"Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and d"
Q17-1022,W15-1521,0,0.0244293,". 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shar"
Q17-1022,P15-2130,1,0.34097,"Missing"
Q17-1022,N16-1018,1,0.276948,"Missing"
Q17-1022,P17-1163,1,0.07397,"Missing"
Q17-1022,P16-2074,0,0.108128,"-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection of semantically specialized word vectors. ATTRACT-R EPEL is an instance of the second family of models, providing a portable, light-weight approach for incorporating external knowledge into arbitrary vector spaces. In our experiments, we show that ATTRACT-R EPEL outperforms previously proposed post-processors, setting the new state-of-art performance on the widely"
Q17-1022,N15-1100,0,0.0338437,"ducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tunin"
Q17-1022,Q16-1030,0,0.48294,"de WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2"
Q17-1022,D14-1162,0,0.11428,"t ATTRACT-R EPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to i"
Q17-1022,N15-1058,0,0.0309108,"Missing"
Q17-1022,W16-1622,0,0.0658587,"al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection of semantically specialized word vectors. ATTRACT-R EPEL is an instance of the second family of models, providing a portable, light-weight approach for incorporating external knowledge into arbitrary vector spaces. In our experiments, we show that ATTRACT-R EPEL outperforms previously proposed post-processors, setting the new state-of-art performance on the widely used SimLex-999 word similarity dataset. Moreover, we show that starting from distributional vectors allows our method to use existing cross-lingual"
Q17-1022,P15-1173,0,0.0714818,"Missing"
Q17-1022,K15-1026,1,0.312629,"al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than represe"
Q17-1022,P13-1045,0,0.0159713,"ithub.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly)"
Q17-1022,D13-1170,0,0.0047346,"ithub.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly)"
Q17-1022,P15-1165,0,0.0197866,"Missing"
Q17-1022,P10-1040,0,0.0185912,"tian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexica"
Q17-1022,P16-1157,0,0.028014,"ual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (Søgaard"
Q17-1022,P16-2084,1,0.84249,"Missing"
Q17-1022,P16-1024,1,0.758571,"Missing"
Q17-1022,E17-1042,1,0.780077,"Missing"
Q17-1022,Q15-1025,0,0.116195,"d Cross-Lingual Constraints Nikola Mrkši´c1,2 , Ivan Vuli´c1 , Diarmuid Ó Séaghdha2 , Ira Leviant3 Roi Reichart3 , Milica Gaši´c1 , Anna Korhonen1 , Steve Young1,2 1 University of Cambridge 2 Apple Inc. 3 Technion, IIT Abstract These models typically build on distributional ones by using human- or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections. Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016). We term this approach semantic specialization. We present ATTRACT-R EPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. ATTRACT-R EPEL facilitates the use of constraints from mono- and crosslingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct highquality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource one"
Q17-1022,D16-1157,0,0.0166515,"nsfer (+0.19 / +0.11 over monolingual specialization), with Italian vectors’ performance coming close to the top-performing English ones. 316 Comparison to Baselines Table 3 gives an exhaustive comparison of ATTRACT-R EPEL to counterfitting: ATTRACT-R EPEL achieved substantially stronger performance in all experiments. We believe these results conclusively show that the contextsensitive updates and L2 regularization employed by ATTRACT-R EPEL present a better alternative to the context-insensitive attract/repel terms and pair-wise regularization employed by counter-fitting.11 State-of-the-Art Wieting et al. (2016) note that the hyperparameters of the widely used Paragram-SL999 vectors (Wieting et al., 2015) are tuned on SimLex999, and as such are not comparable to methods which hold out the dataset. This implies that further work which uses these vectors (e.g., (Mrkši´c et al., 11 To understand the relative importance of the contextsensitive updates and the change in regularization, we can compare the two methods to the retrofitting procedure (Faruqui et al., 2015). Retrofitting uses L2 regularization (like ATTRACTR EPEL) and a ‘global’ attract term (like counter-fitting). The performance of retrofitti"
Q17-1022,D12-1111,0,0.0614025,"Missing"
Q17-1022,P14-2089,0,0.0246193,") into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches s"
Q17-1022,D13-1141,0,0.0291883,"rce-intensive. All resources related to this paper are available at www.github.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical enta"
Q17-1022,J14-3005,1,\N,Missing
Q17-1022,C98-1013,0,\N,Missing
Q17-1033,N09-1003,0,0.141454,"Missing"
Q17-1033,W13-3520,0,0.0305386,"web text (WB). Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)). We copy the test set UAS results of Choi et al. (2015) and compute p−values using the data downloaded from http://amandastent.com/dependable/. POS Tagging We consider a multilingual setup, analyzing the results reported in (Pinter et al., 2017). The authors compare their M IMICK model with the model of Ling et al. (2015), denoted with CHAR → TAG . Evaluation is performed on 23 of the 44 languages shared by the Polyglot word embedding dataset (Al-Rfou et al., 2013) and the universal dependencies (UD) dataset (De Marneffe et al., 2014). Pinter et al. (2017) choose their languages so that they reflect a variety of typological, and particularly morphological, properties. The training/test split is the standard UD split. We copy the word level accuracy figures of Pinter et al. (2017) for the low resource training set setup, the focus setup of that paper. The authors kindly sent us their p-values. Sentiment Classification In this task, an algorithm is trained on reviews from one domain and should classify the sentiment of reviews from another domain to the p"
Q17-1033,D14-1034,1,0.810313,"s generated by a model trained on a 42B token common web crawl.12 We employed the demo of Faruqui and Dyer (2014) to perform a Spearman correlation evaluation of these vector collections on 12 English word pair datasets: WS-353 (Finkelstein et al., 2001b), WS-353-SIM (Agirre et al., 2009), WS-353-REL (Agirre et al., 2009), MC-30 (Miller and Charles, 1991), RG-65 (Rubenstein and Goodenough, 1965), Rare-Word (Luong et al., 2013), MEN (Bruni et al., 2012), MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), YP-130 (Yang and Powers, ), SimLex-999 (Hill et al., 2016), and Verb-143 (Baker et al., 2014). 6.2 Statistical Significance Tests We first calculate the p−values for each task and dataset according to the principals of p−values computation for NLP as discussed in Yeh (2000), BergKirkpatrick et al. (2012) and Søgaard et al. (2014). For dependency parsing, we employ the aparametric paired bootstrap test (Efron and Tibshirani, 1994) that does not assume any distribution on the test statistics. We chose this test because the distribution of the values for the measures commonly applied in this task is unknown. We implemented the test as in (Berg-Kirkpatrick et al., 2012) with a bootstrap s"
Q17-1033,P14-1023,0,0.319746,"lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015). In some works (e.g., Baroni et al. (2014)) these embedding models are compared across a large number of simple tasks. As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical significance figures and sometimes also with cross-comparison statistics such as average performance figures. Here, we analyze the conclusions that can be drawn from this information and suggest that with the growing number of comparisons, a more intricate analysis is required. Existing Analysis Frameworks Machine learning work on multiple dataset co"
Q17-1033,D12-1091,0,0.373023,"ce between the algorithms is statistically significant. For this goal they propose methods such as a paired t-test, a nonparametric sign-rank test and a wins/losses/ties count, all computed across the results collected from all participating datasets. In contrast, our goal is to count and identify the datasets for which one algorithm significantly outperforms the other, which provides more intricate information, especially when the datasets come from different sources. In NLP, several studies addressed the problem of measuring the statistical significance of results on a single dataset (e.g., Berg-Kirkpatrick et al. (2012); Søgaard (2013); Søgaard et al. (2014)). Søgaard (2013) is, to the best of our knowledge, the only work that addressed the statistical properties of evaluation with multiple datasets. For this aim he modified the statistical tests proposed in Demˇsar (2006) to use a Gumbel distribution assumption on the test statistics, which he considered to suit NLP better than the original Gaussian assumption. However, while this procedure aims to estimate the effect size across datasets, it answers neither the counting nor the identification question of Section 1. In the next section we provide the prelim"
Q17-1033,W06-1615,0,0.0376631,"mple, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017). Multiple dataset comparisons are also abundant in domain adaptation work. Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daum´e III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degr"
Q17-1033,P07-1056,0,0.25158,"l corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017). Multiple dataset comparisons are also abundant in domain adaptation work. Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daum´e III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different s"
Q17-1033,P12-1015,0,0.0180576,"OW (Mikolov et al., 2013) vectors, generated by the model titled the best “predict” model in Baroni et al. (2014);11 and (b) GloVe (Pennington et al., 2014) vectors generated by a model trained on a 42B token common web crawl.12 We employed the demo of Faruqui and Dyer (2014) to perform a Spearman correlation evaluation of these vector collections on 12 English word pair datasets: WS-353 (Finkelstein et al., 2001b), WS-353-SIM (Agirre et al., 2009), WS-353-REL (Agirre et al., 2009), MC-30 (Miller and Charles, 1991), RG-65 (Rubenstein and Goodenough, 1965), Rare-Word (Luong et al., 2013), MEN (Bruni et al., 2012), MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), YP-130 (Yang and Powers, ), SimLex-999 (Hill et al., 2016), and Verb-143 (Baker et al., 2014). 6.2 Statistical Significance Tests We first calculate the p−values for each task and dataset according to the principals of p−values computation for NLP as discussed in Yeh (2000), BergKirkpatrick et al. (2012) and Søgaard et al. (2014). For dependency parsing, we employ the aparametric paired bootstrap test (Efron and Tibshirani, 1994) that does not assume any distribution on the test statistics. We chose this test because the dis"
Q17-1033,W06-2920,0,0.0821304,"ificance with Multiple Datasets Rotem Dror Gili Baumer Marina Bogomolov Roi Reichart Faculty of Industrial Engineering and Management, Technion, IIT {rtmdrr@campus|sgbaumer@campus|marinabo|roiri}.technion.ac.il Abstract For example, the phrase structure parsers of Charniak (2000) and Collins (2003) were mostly evaluated on the Wall Street Journal Penn Treebank (Marcus et al., 1993), consisting of written, edited English text of economic news. In contrast, modern dependency parsers are expected to excel on the 19 languages of the CoNLL 2006-2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nilsson et al., 2007), and additional challenges, such as the shared task on parsing multiple English Web domains (Petrov and McDonald, 2012), are continuously proposed. With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Rep"
Q17-1033,P07-1007,0,0.0151561,"ourcetarget language pairs. This is done, for example, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017). Multiple dataset comparisons are also abundant in domain adaptation work. Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daum´e III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple dataset"
Q17-1033,A00-2018,0,0.0438609,"Missing"
Q17-1033,P15-1038,0,0.0225886,"t, even if some of the datasets are independent. 6 NLP Applications In this section we demonstrate the potential impact of replicability analysis on the way experimental results are analyzed in NLP setups. We explore four NLP applications: (a) two where the datasets are independent: multi-domain dependency parsing and multilingual POS tagging; and (b) two where dependency between the datasets does exist: cross-domain sentiment classification and word similarity prediction with word embedding models. 6.1 Data Dependency Parsing We consider a multidomain setup, analyzing the results reported in Choi et al. (2015). The authors compared ten state-of-the-art parsers from which we pick three: (a) Mate (Bohnet, 2010)8 that performed best on the majority of datasets; (b) Redshift (Honnibal et al., 2013)9 which demonstrated comparable, still somewhat lower, performance compared to Mate; 8 9 and (c) SpaCy (Honnibal and Johnson, 2015) that was substantially outperformed by Mate. All parsers were trained and tested on the English portion of the OntoNotes 5 corpus (Weischedel et al., 2011; Pradhan et al., 2013), a large multigenre corpus consisting of the following 7 genres: broadcasting conversations (BC), broa"
Q17-1033,J03-4003,0,0.0361102,"Missing"
Q17-1033,P07-1033,0,0.280989,"Missing"
Q17-1033,de-marneffe-etal-2014-universal,0,0.0268934,"Missing"
Q17-1033,P14-5004,0,0.0185883,"y of their AE-SCL-SR model to MSDA (Chen et al., 2011), a well known domain adaptation 10 code.google.com/p/mate-tools. github.com/syllog1sm/Redshift. http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment 479 method, and kindly sent us the required p-values. Word Similarity We compare two state-of-the-art word embedding collections: (a) word2vec CBOW (Mikolov et al., 2013) vectors, generated by the model titled the best “predict” model in Baroni et al. (2014);11 and (b) GloVe (Pennington et al., 2014) vectors generated by a model trained on a 42B token common web crawl.12 We employed the demo of Faruqui and Dyer (2014) to perform a Spearman correlation evaluation of these vector collections on 12 English word pair datasets: WS-353 (Finkelstein et al., 2001b), WS-353-SIM (Agirre et al., 2009), WS-353-REL (Agirre et al., 2009), MC-30 (Miller and Charles, 1991), RG-65 (Rubenstein and Goodenough, 1965), Rare-Word (Luong et al., 2013), MEN (Bruni et al., 2012), MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), YP-130 (Yang and Powers, ), SimLex-999 (Hill et al., 2016), and Verb-143 (Baker et al., 2014). 6.2 Statistical Significance Tests We first calculate the p−values for each task and dataset"
Q17-1033,N09-1032,0,0.0186095,"ultilingual example is, naturally, machine translation, where it is customary to compare algorithms across a large number of sourcetarget language pairs. This is done, for example, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017). Multiple dataset comparisons are also abundant in domain adaptation work. Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daum´e III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013;"
Q17-1033,J15-4004,1,0.760649,". This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015). In some works (e.g., Baroni et al. (2014)) these embedding models are compared across a large number of simple tasks. As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical significance figures and sometimes also with cross-comparison statistics such as average performance figures. Here, we analyze the conclusions that can be drawn from this information and suggest that with the growing number of comparisons, a more intricate analysis is required. Existing Analysis Frameworks M"
Q17-1033,D15-1162,0,0.0130529,"rsing and multilingual POS tagging; and (b) two where dependency between the datasets does exist: cross-domain sentiment classification and word similarity prediction with word embedding models. 6.1 Data Dependency Parsing We consider a multidomain setup, analyzing the results reported in Choi et al. (2015). The authors compared ten state-of-the-art parsers from which we pick three: (a) Mate (Bohnet, 2010)8 that performed best on the majority of datasets; (b) Redshift (Honnibal et al., 2013)9 which demonstrated comparable, still somewhat lower, performance compared to Mate; 8 9 and (c) SpaCy (Honnibal and Johnson, 2015) that was substantially outperformed by Mate. All parsers were trained and tested on the English portion of the OntoNotes 5 corpus (Weischedel et al., 2011; Pradhan et al., 2013), a large multigenre corpus consisting of the following 7 genres: broadcasting conversations (BC), broadcasting news (BN), news magazine (MZ), newswire (NW), pivot text (PT), telephone conversations (TC) and web text (WB). Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)). We copy the test set UAS results of Choi et al. (2015) and com"
Q17-1033,W13-3518,0,0.0158634,"alyzed in NLP setups. We explore four NLP applications: (a) two where the datasets are independent: multi-domain dependency parsing and multilingual POS tagging; and (b) two where dependency between the datasets does exist: cross-domain sentiment classification and word similarity prediction with word embedding models. 6.1 Data Dependency Parsing We consider a multidomain setup, analyzing the results reported in Choi et al. (2015). The authors compared ten state-of-the-art parsers from which we pick three: (a) Mate (Bohnet, 2010)8 that performed best on the majority of datasets; (b) Redshift (Honnibal et al., 2013)9 which demonstrated comparable, still somewhat lower, performance compared to Mate; 8 9 and (c) SpaCy (Honnibal and Johnson, 2015) that was substantially outperformed by Mate. All parsers were trained and tested on the English portion of the OntoNotes 5 corpus (Weischedel et al., 2011; Pradhan et al., 2013), a large multigenre corpus consisting of the following 7 genres: broadcasting conversations (BC), broadcasting news (BN), news magazine (MZ), newswire (NW), pivot text (PT), telephone conversations (TC) and web text (WB). Train and test set size (in sentences) range from 6672 to 34,492 and"
Q17-1033,W07-0733,0,0.0330708,"establishing new standards for our community. Multiple Comparisons in NLP Multiple comparisons of algorithms over datasets from different languages, domains and genres have become a de-facto standard in many areas of NLP. Here we survey a number of representative examples. A full list of NLP tasks is beyond the scope of this paper. A common multilingual example is, naturally, machine translation, where it is customary to compare algorithms across a large number of sourcetarget language pairs. This is done, for example, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017). Multiple dataset comparisons are also abundant in domain adaptation work. Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daum´e III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes"
Q17-1033,2005.mtsummit-papers.11,0,0.0107954,"the need for establishing new standards for our community. Multiple Comparisons in NLP Multiple comparisons of algorithms over datasets from different languages, domains and genres have become a de-facto standard in many areas of NLP. Here we survey a number of representative examples. A full list of NLP tasks is beyond the scope of this paper. A common multilingual example is, naturally, machine translation, where it is customary to compare algorithms across a large number of sourcetarget language pairs. This is done, for example, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017). Multiple dataset comparisons are also abundant in domain adaptation work. Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daum´e III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence"
Q17-1033,P14-2050,0,0.0311139,"and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015). In some works (e.g., Baroni et al. (2014)) these embedding models are compared across a large number of simple tasks. As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical signif"
Q17-1033,D15-1176,0,0.0208915,": broadcasting conversations (BC), broadcasting news (BN), news magazine (MZ), newswire (NW), pivot text (PT), telephone conversations (TC) and web text (WB). Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)). We copy the test set UAS results of Choi et al. (2015) and compute p−values using the data downloaded from http://amandastent.com/dependable/. POS Tagging We consider a multilingual setup, analyzing the results reported in (Pinter et al., 2017). The authors compare their M IMICK model with the model of Ling et al. (2015), denoted with CHAR → TAG . Evaluation is performed on 23 of the 44 languages shared by the Polyglot word embedding dataset (Al-Rfou et al., 2013) and the universal dependencies (UD) dataset (De Marneffe et al., 2014). Pinter et al. (2017) choose their languages so that they reflect a variety of typological, and particularly morphological, properties. The training/test split is the standard UD split. We copy the word level accuracy figures of Pinter et al. (2017) for the low resource training set setup, the focus setup of that paper. The authors kindly sent us their p-values. Sentiment Classif"
Q17-1033,W13-3512,0,0.0562076,"Missing"
Q17-1033,J93-2004,0,0.0602071,"Missing"
Q17-1033,E17-4003,0,0.0569501,"tter than previous (probably more simple) ones. In this work, we demonstrate this problem and show that it becomes more severe as the number of evaluation sets grows, which seems to be the current trend in NLP. We adopt a known general statistical methodology for addressing the counting (question (1)) and identification (question (2)) problems, by choosing the tests and procedures which are valid for 2 “Replicability” is sometimes referred to as “reproducibility”. In recent NLP work the term reproducibility was used when trying to get identical results on the same data (N´ev´eol et al., 2016; Marrese-Taylor and Matsuo, 2017). In this paper, we adopt the meaning of “replicability” and its distinction from “reproducibility” from Peng (2011) and Leek and Peng (2015) and refer to replicability analysis as the effort to show that a finding is consistent over different datasets from different domains or languages, and is not idiosyncratic to a specific scenario. 472 situations encountered in NLP problems, and giving specific recommendations for such situations. Particularly, we first demonstrate (Section 3) that the current prominent approach in the NLP literature, identifying the datasets for which the difference betw"
Q17-1033,W16-6110,0,0.126643,"Missing"
Q17-1033,D14-1162,0,0.0828276,"III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015). In some works (e.g., Baroni et al. (2014)) these embedding models are compared across a large number of simple tasks. As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usu"
Q17-1033,D17-1010,0,0.0135086,"an et al., 2013), a large multigenre corpus consisting of the following 7 genres: broadcasting conversations (BC), broadcasting news (BN), news magazine (MZ), newswire (NW), pivot text (PT), telephone conversations (TC) and web text (WB). Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)). We copy the test set UAS results of Choi et al. (2015) and compute p−values using the data downloaded from http://amandastent.com/dependable/. POS Tagging We consider a multilingual setup, analyzing the results reported in (Pinter et al., 2017). The authors compare their M IMICK model with the model of Ling et al. (2015), denoted with CHAR → TAG . Evaluation is performed on 23 of the 44 languages shared by the Polyglot word embedding dataset (Al-Rfou et al., 2013) and the universal dependencies (UD) dataset (De Marneffe et al., 2014). Pinter et al. (2017) choose their languages so that they reflect a variety of typological, and particularly morphological, properties. The training/test split is the standard UD split. We copy the word level accuracy figures of Pinter et al. (2017) for the low resource training set setup, the focus set"
Q17-1033,K15-1026,1,0.853389,"d sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007). More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015). In some works (e.g., Baroni et al. (2014)) these embedding models are compared across a large number of simple tasks. As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical significance figures and somet"
Q17-1033,P14-1068,0,0.0299687,"f datasets is being created. This is particularly notice473 able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks. For example, it is customary to compare word embedding models (Mikolov et al., 2013; ´ S´eaghdha and KorhoPennington et al., 2014; O nen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015). In some works (e.g., Baroni et al. (2014)) these embedding models are compared across a large number of simple tasks. As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical significance figures and sometimes also with cross-comparison statistics such as average performance figures. Here, we analyze the conclusions that can be drawn from this information and suggest that with the growing number of comparisons, a more intricate analysis is required. Existing A"
Q17-1033,D08-1027,0,0.134663,"Missing"
Q17-1033,W14-1601,0,0.511679,"ficant. For this goal they propose methods such as a paired t-test, a nonparametric sign-rank test and a wins/losses/ties count, all computed across the results collected from all participating datasets. In contrast, our goal is to count and identify the datasets for which one algorithm significantly outperforms the other, which provides more intricate information, especially when the datasets come from different sources. In NLP, several studies addressed the problem of measuring the statistical significance of results on a single dataset (e.g., Berg-Kirkpatrick et al. (2012); Søgaard (2013); Søgaard et al. (2014)). Søgaard (2013) is, to the best of our knowledge, the only work that addressed the statistical properties of evaluation with multiple datasets. For this aim he modified the statistical tests proposed in Demˇsar (2006) to use a Gumbel distribution assumption on the test statistics, which he considered to suit NLP better than the original Gaussian assumption. However, while this procedure aims to estimate the effect size across datasets, it answers neither the counting nor the identification question of Section 1. In the next section we provide the preliminary knowledge from the field of stati"
Q17-1033,N13-1068,0,0.268595,"tistically significant. For this goal they propose methods such as a paired t-test, a nonparametric sign-rank test and a wins/losses/ties count, all computed across the results collected from all participating datasets. In contrast, our goal is to count and identify the datasets for which one algorithm significantly outperforms the other, which provides more intricate information, especially when the datasets come from different sources. In NLP, several studies addressed the problem of measuring the statistical significance of results on a single dataset (e.g., Berg-Kirkpatrick et al. (2012); Søgaard (2013); Søgaard et al. (2014)). Søgaard (2013) is, to the best of our knowledge, the only work that addressed the statistical properties of evaluation with multiple datasets. For this aim he modified the statistical tests proposed in Demˇsar (2006) to use a Gumbel distribution assumption on the test statistics, which he considered to suit NLP better than the original Gaussian assumption. However, while this procedure aims to estimate the effect size across datasets, it answers neither the counting nor the identification question of Section 1. In the next section we provide the preliminary knowledge"
Q17-1033,C00-2137,0,0.333273,"12 English word pair datasets: WS-353 (Finkelstein et al., 2001b), WS-353-SIM (Agirre et al., 2009), WS-353-REL (Agirre et al., 2009), MC-30 (Miller and Charles, 1991), RG-65 (Rubenstein and Goodenough, 1965), Rare-Word (Luong et al., 2013), MEN (Bruni et al., 2012), MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), YP-130 (Yang and Powers, ), SimLex-999 (Hill et al., 2016), and Verb-143 (Baker et al., 2014). 6.2 Statistical Significance Tests We first calculate the p−values for each task and dataset according to the principals of p−values computation for NLP as discussed in Yeh (2000), BergKirkpatrick et al. (2012) and Søgaard et al. (2014). For dependency parsing, we employ the aparametric paired bootstrap test (Efron and Tibshirani, 1994) that does not assume any distribution on the test statistics. We chose this test because the distribution of the values for the measures commonly applied in this task is unknown. We implemented the test as in (Berg-Kirkpatrick et al., 2012) with a bootstrap size of 500 and with 105 repetitions. For multilingual POS tagging, we employ the Wilcoxon signed-rank test (Wilcoxon, 1945) on the differences of the sentence level accuracy scores"
Q17-1033,K17-1040,1,0.922946,"al. (2017) choose their languages so that they reflect a variety of typological, and particularly morphological, properties. The training/test split is the standard UD split. We copy the word level accuracy figures of Pinter et al. (2017) for the low resource training set setup, the focus setup of that paper. The authors kindly sent us their p-values. Sentiment Classification In this task, an algorithm is trained on reviews from one domain and should classify the sentiment of reviews from another domain to the positive and negative classes. For replicability analysis we explore the results of Ziser and Reichart (2017) for the cross-domain sentiment classification task of Blitzer et al. (2007). The data in this task consists of Amazon product reviews from 4 domains: books (B), DVDs (D), electronic items (E), and kitchen appliances (K), for the total of 12 domain pairs, each domain having a 2000 review test set.10 Ziser and Reichart (2017) compared the accuracy of their AE-SCL-SR model to MSDA (Chen et al., 2011), a well known domain adaptation 10 code.google.com/p/mate-tools. github.com/syllog1sm/Redshift. http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment 479 method, and kindly sent us the required p-value"
Q17-1033,D07-1096,0,\N,Missing
Q18-1032,E17-1088,0,0.221535,"guarantee a reliable word estimate. Language Modeling (LM) is a key NLP task, serving Since gradual parameter estimation based on conas an important component for applications that retextual information is not feasible for rare phenomena quire some form of text generation, such as machine 451 Transactions of the Association for Computational Linguistics, vol. 6, pp. 451–465, 2018. Action Editor: Brian Roark. Submission batch: 12/2017; Revision batch: 5/2018; Published 7/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. in the full vocabulary setup (Adams et al., 2017), it is of crucial importance to construct and enable techniques that can obtain these parameters in alternative ways. One solution is to draw information from additional sources, such as characters and character sequences. As a consequence, such character-aware models should facilitate LM word-level prediction in a real-life LM setup which deals with a large amount of low-frequency or unseen words. Efforts into this direction have yielded exciting results, primarily on the input side of neural LMs. A standard RNN LM architecture relies on two word representation matrices learned during traini"
Q18-1032,W13-3520,0,0.0421794,"ubsequent fine-tuning. The preserve cost acts as a regularisation pulling the “fine-tuned” vector back to its initial value: X pres(Pw , Nw ) = λreg ||ˆ vw − vw ||2 . (6) xw ∈V w λreg = 10−9 is the L2 -regularisation constant (Mrkši´c et al., 2017); vˆw is the original word vector before the procedure. This term tries to preserve the semantic content present in the original vector space, as long as this information does not contradict the knowledge injected by the constraints. The final cost function adds the two costs: cost = attr + pres. 6 Experiments Datasets We use the Polyglot Wikipedia (Al-Rfou et al., 2013) for all available languages except for Japanese, Chinese, and Thai, and add these and further languages using Wikipedia dumps. The Wiki dumps were cleaned and preprocessed by the Polyglot tokeniser. We construct similarly-sized datasets by extracting 46K sentences for each language from the beginning of each dump, filtered to contain only full sentences, and split into train (40K), validation (3K), and test (3K). The final list of languages along with standard language codes (ISO 639-1 standard, used throughout the paper) and statistics on vocabulary and token counts are provided in Table 4."
Q18-1032,P16-1156,0,0.0377669,"Missing"
Q18-1032,D09-1003,0,0.0188023,"pped to ±2.4 A full summary of all hyper-parameters and their values is provided in Table 3. (Baseline) Language Models The availability of LM evaluation sets in a large number of diverse languages, described in Section 2, now provides an opportunity to perform a full-fledged multilingual analysis of representative LM architectures. At the same time, these different architectures serve as the baselines for our novel model which fine-tunes the output matrix M w . As mentioned, the traditional LM setup is to use words both on the input and on the output side (Goodman, 2001; Bengio et al., 2003; Deschacht and Moens, 2009) relying on n-gram word sequences. We evaluate a strong model from the n-gram family of models from the KenLM package (https://github.com/kpu/kenlm): it is based on 5grams with extended Kneser-Ney smoothing (KN5) (Kneser and Ney, 1995; Heafield et al., 2013) 5 . The rationale behind including this non-neural model is to also probe the limitations of such n-gram-based LM architectures on a diverse set of languages. Recurrent neural networks (RNNs), especially Long-Short-Term Memory networks (LSTMs), have taken over the LM universe recently (Mikolov et al., 2010; Sundermeyer et al., 2015; Chen e"
Q18-1032,N15-1184,0,0.116334,"Missing"
Q18-1032,D15-1042,0,0.057779,"rphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction Daniela Gerz1 , Ivan Vuli´c1 , Edoardo Ponti1 Jason Naradowsky3 , Roi Reichart2 Anna Korhonen1 1 2 Language Technology Lab, DTAL, University of Cambridge Faculty of Industrial Engineering and Management, Technion, IIT 3 Johns Hopkins University 1 {dsg40,iv250,ep490,alk23}@cam.ac.uk 2 roiri@ie.technion.ac.il 3 narad@jhu.edu Abstract translation (Vaswani et al., 2013), speech recognition (Mikolov et al., 2010), dialogue generation (Serban et Neural architectures are prominent in the conal., 2016), or summarisation (Filippova et al., 2015). struction of language models (LMs). HowA traditional recurrent neural network (RNN) LM ever, word-level prediction is typically agnossetup operates on a limited closed vocabulary of tic of subword-level information (characters words (Bengio et al., 2003; Mikolov et al., 2010). and character sequences) and operates over The limitation arises due to the model learning paa closed vocabulary, consisting of a limited rameters exclusive to single words. A standard trainword set. Indeed, while subword-aware models boost performance across a variety of NLP ing procedure for neural LMs gradually modi"
Q18-1032,N13-1092,0,0.0746533,"Missing"
Q18-1032,P13-2121,0,0.0327097,"a full-fledged multilingual analysis of representative LM architectures. At the same time, these different architectures serve as the baselines for our novel model which fine-tunes the output matrix M w . As mentioned, the traditional LM setup is to use words both on the input and on the output side (Goodman, 2001; Bengio et al., 2003; Deschacht and Moens, 2009) relying on n-gram word sequences. We evaluate a strong model from the n-gram family of models from the KenLM package (https://github.com/kpu/kenlm): it is based on 5grams with extended Kneser-Ney smoothing (KN5) (Kneser and Ney, 1995; Heafield et al., 2013) 5 . The rationale behind including this non-neural model is to also probe the limitations of such n-gram-based LM architectures on a diverse set of languages. Recurrent neural networks (RNNs), especially Long-Short-Term Memory networks (LSTMs), have taken over the LM universe recently (Mikolov et al., 2010; Sundermeyer et al., 2015; Chen et al., 2016, i.a.). These LMs map a sequence of input words to embedding vectors using a look-up matrix. The embeddings are passed to the LSTM as input, and 3 This choice has been motivated by the observation that rare words tend to have other rare words as"
Q18-1032,P17-1137,0,0.083385,"Missing"
Q18-1032,2005.mtsummit-papers.11,0,0.0462052,"ised in Table 6. As 461 one important finding, we observe that the gains in perplexity using our fine-tuning AP method extend also to these larger evaluation datasets. In particular, we find improvements of the same magnitude as in the PTB-sized data sets over the strongest baseline model (Char-CNN-LSTM) for all MWC languages. For instance, perplexity is reduced from 1781 to 1578 for Russian, and from 365 to 352 for English. We also observe a gain for French and Spanish with perplexity reduced from 282 to 272 and 255 to 243 respectively. In addition, we test on samples of the Europarl corpus (Koehn, 2005; Tiedemann, 2012) which contains approximately 10 times more tokens than our PTB-sized evaluation data: we use 400K sentences from Europarl for training and testing. However, this data comes from a much narrower domain of parliamentary proceedings: this property yields a very low type/token ratio as visible from Table 6. In fact, we find the type/token ratio in this corpus to be on the same level or even smaller than isolating languages (compare with the scores in Table 4): 0.02 for Dutch and 0.03 for Czech. This leads to similar perplexities with and without +AP for these two selected test l"
Q18-1032,P16-1100,0,0.111324,"Missing"
Q18-1032,P11-1015,0,0.0605287,"sed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of"
Q18-1032,J93-2004,0,0.064122,"nd Typological Diversity A language model defines a probability distribution over sequences of tokens, and is typically trained to maximise the likelihood of token input sequences. Formally, the LM objective is expressed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent wor"
Q18-1032,D16-1209,0,0.261842,"fforts into this direction have yielded exciting results, primarily on the input side of neural LMs. A standard RNN LM architecture relies on two word representation matrices learned during training for its input and next-word prediction. This effectively means that there are two sets of per-word specific parameters that need to be trained. Recent work shows that it is possible to generate a word representation on-the-fly based on its constituent characters, thereby effectively solving the problem for the parameter set on the input side of the model (Kim et al., 2016; Luong and Manning, 2016; Miyamoto and Cho, 2016; Ling et al., 2015). However, it is not straightforward how to advance these ideas to the output side of the model, as this second set of word-specific parameters is directly responsible for the next-word prediction: it has to encode a much wider range of information, such as topical and semantic knowledge about words, which cannot be easily obtained from its characters alone (Jozefowicz et al., 2016). While one solution is to directly output characters instead of words (Graves, 2013; Miyamoto and Cho, 2016), a recent work from Jozefowicz et al. (2016) suggests that such purely character-base"
Q18-1032,Q17-1022,1,0.901431,"Missing"
Q18-1032,oostdijk-2000-spoken,0,0.0770072,"tion (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutch LM. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Perhaps the largest and most diverse set of languages used for multilingual LM evaluation so far is the one of Vania and Lopez (2017). Their study includes 10 languages in total representing several morphological types (fusional, e.g., Russian, and agglutinative, e.g., Finnish), as well as languages with particular morphological phenomena (root-and-pattern in Hebrew and reduplication in Malay). In this work, we provide LM evaluation datasets for 50 typologically diverse languages, wi"
Q18-1032,E17-2025,0,0.0199535,"in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutch LM. Kawakami et al. (2017) evaluate on 7 Euro"
Q18-1032,tiedemann-2012-parallel,0,0.0172916,"6. As 461 one important finding, we observe that the gains in perplexity using our fine-tuning AP method extend also to these larger evaluation datasets. In particular, we find improvements of the same magnitude as in the PTB-sized data sets over the strongest baseline model (Char-CNN-LSTM) for all MWC languages. For instance, perplexity is reduced from 1781 to 1578 for Russian, and from 365 to 352 for English. We also observe a gain for French and Spanish with perplexity reduced from 282 to 272 and 255 to 243 respectively. In addition, we test on samples of the Europarl corpus (Koehn, 2005; Tiedemann, 2012) which contains approximately 10 times more tokens than our PTB-sized evaluation data: we use 400K sentences from Europarl for training and testing. However, this data comes from a much narrower domain of parliamentary proceedings: this property yields a very low type/token ratio as visible from Table 6. In fact, we find the type/token ratio in this corpus to be on the same level or even smaller than isolating languages (compare with the scores in Table 4): 0.02 for Dutch and 0.03 for Czech. This leads to similar perplexities with and without +AP for these two selected test languages. The thir"
Q18-1032,P17-1184,0,0.284804,"gical systems. We discuss the implications of typological diversity on the LM task, both theoretically in Section 2, and empirically in Section 7; we find a clear correspondence between performance of state-of-the art LMs and structural linguistic properties. Further, the consistent perplexity gains across the large sample of languages suggest wide applicability of our novel method. Finally, this article can also be read as a comprehensive multilingual analysis of current LM architectures on a set of languages which is much larger than the ones used in recent LM work (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017). We hope that this article with its new datasets, methodology and models, all available online at http://people.ds.cam. ac.uk/dsg40/lmmrl.html, will pave the way for true multilingual research in language modeling. 2 LM Data and Typological Diversity A language model defines a probability distribution over sequences of tokens, and is typically trained to maximise the likelihood of token input sequences. Formally, the LM objective is expressed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level pred"
Q18-1032,D13-1140,0,0.067895,"Missing"
Q18-1032,E17-1040,0,0.0805667,"Missing"
Q18-1032,P17-1006,1,0.915822,"Missing"
Q18-1032,P16-1125,0,0.0319225,".ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutc"
Q18-1032,Q15-1025,0,0.161463,"ction cannot fully capture word-level semantics and even hurts LM performance (Jozefowicz et al., 2016). However, shared subword units still provide useful evidence of shared semantics (Cotterell et al., 2016; Vuli´c et al., 2017): injecting this into the space M w to additionally reflect shared subword-level information should lead to improved word vector estimates, especially for MRLs. 5.1 Fine-Tuning and Constraints We inject this information into M w by adapting recent fine-tuning (often termed retrofitting or specialisation) methods for vector space post-processing (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017; Vuli´c et al., 2017, i.a.). These models enrich initial vector spaces by encoding external knowledge provided in the form of simple linguistic constraints (i.e., word pairs) into the initial vector space. There are two fundamental differences between our work and previous work on specialisation. First, previous models typically use rich hand-crafted lexical resources such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013), or manually defined rules (Vuli´c et al., 2017) to extract the constraints, while we generate them directly using the"
Q19-1041,P05-1022,0,0.0652497,"aries are those that consist of a high quality and diverse list of sentences extracted from the text. In other cases the members of the solution list are exploited when solving an end goal application. For example, dependency forests were used in order to improve machine translation (Tu et al., 2010; Ma et al., 2018) and sentiment analysis (Tu et al., 2012). In yet other cases it is a first step towards learning a high quality structure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Other examples include bagging (Breiman, 1996; Sun and Wan, 2013) and boosting (Bawden and Crabb´e, 2016) as well as other ensemble methods (Surdeanu and Manning, 2010; T¨ackstr¨om et al., 2013; Kuncoro et al., 2016) that are often applied when the data available for model training is limited, in cases where exact argmax inferen"
Q19-1041,P02-1062,0,0.201038,"(Nenkova and McKeown, 2011) good summaries are those that consist of a high quality and diverse list of sentences extracted from the text. In other cases the members of the solution list are exploited when solving an end goal application. For example, dependency forests were used in order to improve machine translation (Tu et al., 2010; Ma et al., 2018) and sentiment analysis (Tu et al., 2012). In yet other cases it is a first step towards learning a high quality structure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Other examples include bagging (Breiman, 1996; Sun and Wan, 2013) and boosting (Bawden and Crabb´e, 2016) as well as other ensemble methods (Surdeanu and Manning, 2010; T¨ackstr¨om et al., 2013; Kuncoro et al., 2016) that are often applied when the data available for model train"
Q19-1041,J05-1003,0,0.109107,"cKeown, 2011) good summaries are those that consist of a high quality and diverse list of sentences extracted from the text. In other cases the members of the solution list are exploited when solving an end goal application. For example, dependency forests were used in order to improve machine translation (Tu et al., 2010; Ma et al., 2018) and sentiment analysis (Tu et al., 2012). In yet other cases it is a first step towards learning a high quality structure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Other examples include bagging (Breiman, 1996; Sun and Wan, 2013) and boosting (Bawden and Crabb´e, 2016) as well as other ensemble methods (Surdeanu and Manning, 2010; T¨ackstr¨om et al., 2013; Kuncoro et al., 2016) that are often applied when the data available for model training is limited, in case"
Q19-1041,P15-1033,0,0.0913928,"inals using a maximum-likelihood approach on this sample. Particularly, in our first-order dependency parsing example we set μe to be the number of trees in the K-list that contain the edge e. As noted above, the idea of computing an MST over single-edge marginals has been proposed in Kuncoro et al. (2016) where the marginals were computed in a manner similar to ours, using the K parse trees of their K ensemble members. Our novelty is with respect to the way the dependency trees in the K-list are extracted: while they built on the non-convexity of neural networks and ran an LSTM-based parser (Dyer et al., 2015) from different random initializations, we develop a perturbation-based framework. Our method for K-list generation is often more efficient than that of Kuncoro et al. (2016). Whereas we train a parser and a noise function and can then generate the K-list by solving K argmax problems, their method requires the training of K LSTM parsers. Task2: Lightly Supervised Monolingual Dependency Parsing. For this setup we chose 12 low-resource languages (13 corpora) that have between 300 and 5k training sentences: Danish, Estonian, Greek, Hungarian, Indonesian, Korean, Latvian, Old Church Slavonic, Pers"
Q19-1041,Q16-1022,0,0.0360384,"Missing"
Q19-1041,C16-1001,0,0.0616624,"Missing"
Q19-1041,Q17-1010,0,0.0283251,".1 for 1-best. However, the quality of the perturbated list is much higher than that of the K-best list, as is indicated, for example, in the gap between their best oracle solutions (46 vs. 37.6). These results emphasize the importance of high Table 3: Corpus UAS, cross-lingual parsing, K = 100. FC layer to avoid second-order effects where perturbation parameters are multiplied by each other. While we consider here a deep learning model, the noise injection scheme is very simple.13 To close the lexical gap between languages we train the English model with the English fastText word embeddings (Bojanowski et al., 2017; Grave 13 BiLSTM layer sizes are: word embedding: 300, output representations: 256, first FC: 512, second FC: 216. 654 Method 1-best K-best AFN MFN KG Av. UAS (M) 69.2 58.4 69.6 70.7 65.8 Md. UAS (M) 71.1 55.8 71.7 72.7 66.9 Av. UAS (O) 69.2 77.8 77.7 84 65.8 Md. UAS (O) 71.1 78 79.8 83.4 66.9 # Cor. (M) 0 0 0 12 1 # Cor. (O) 0 0 0 13 0 Table 4: Results summary, mono-lingual parsing, K = 100. Table format is identical to Table 1. We run this experiment with 31 corpora of 14 UD languages: Arabic, German, English, Spanish, French, Hebrew, Japanese, Korean, Dutch, Portuguese, Slovenian, Swedish,"
Q19-1041,P09-1042,0,0.0356389,"meter (σ ) but rather use fixed noise parameters for the perturbated models (see below). As opposed to the cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012"
Q19-1041,D13-1111,0,0.0280668,"even though we integrate our method into a linear parser (Huang and Sagae, 2010), our modified parser outperforms a state-of-the-art (non-perturbated) BiLSTM parser (Kiperwasser and Goldberg, 2016) on our tasks. 2 K-lists in NLP Structured models in NLP Many NLP tasks, particularly tagging and parsing, involve the inference of a high-dimensional discrete structure y = (y1 , . . . , ym ). For example, in part-of-speech (POS) tagging of an n-word input sentence, each yi 2 Many machine translation (MT) works aimed to generate diverse K-lists of translated sentences (e.g., Macherey et al., 2008; Gimpel et al., 2013; Li and Jurafsky, 2016). However, these methods are specific to MT, whereas we focus on a general framework for structured prediction in NLP. 644 variable corresponds to an input word (and hence m = n), and is assigned a value in {1, . . . , P } where P is the number of POS tags. In dependency parsing, a graph G = (V, E ) is defined over an n-word input sentence such that each vertex corresponds to a word in the input sentence (|V |= n) and each arc corresponds to an ordered word pair (|E |= m = n2 ). In the structured model, each ordered pair of words in the input sentence is assigned a vari"
Q19-1041,L18-1550,0,0.0502185,"Missing"
Q19-1041,N16-1121,0,0.015066,"cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for structured prediction"
Q19-1041,P07-1050,0,0.0238485,"a document), in many cases a diverse ∗ Both authors contributed equally to this work. Our code is at: https://github.com/ramyazdi/ perturbations. 1 643 Transactions of the Association for Computational Linguistics, vol. 7, pp. 643–659, 2019. https://doi.org/10.1162/tacl a 00291. Action Editor: Francois Yvon. Submission batch: 3/2019; Revision batch: 7/2019; Published 9/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  (Lafferty et al., 2001), K-best Maximum Spanning Tree (MST) algorithms for graph-based dependency parsing (Camerini et al., 1980; Hall, 2007), and so forth. However, the members of K-best lists are typically quite similar to each other and do not substantially deviate from the argmax solution of the model.2 Ensemble techniques, in contrast, are often designed to encourage diversity of the K-list members, but they require the training of multiple models (often one model per solution in the K-list) which is prohibitive for large K values. In this work we propose a new method for learning K-lists from machine learning models, focusing on structured prediction models in NLP. Our method is based on the MAP-perturbations model (Hazan et"
Q19-1041,P10-1110,0,0.525814,"etup are similar, except that we consider 13 UD corpora (written in 12 languages) that have limited training data. This setup is monolingual, we train and test on data from the same corpus. Our results demonstrate the quality of the Klists generated by our algorithm and of the tree returned by the MOM procedure. We compare our lists and final solution to those of a variety of alternative algorithms for K-list generation, including the K-best variant of the parser’s argmax inference algorithm, and demonstrate substantial gains. Finally, even though we integrate our method into a linear parser (Huang and Sagae, 2010), our modified parser outperforms a state-of-the-art (non-perturbated) BiLSTM parser (Kiperwasser and Goldberg, 2016) on our tasks. 2 K-lists in NLP Structured models in NLP Many NLP tasks, particularly tagging and parsing, involve the inference of a high-dimensional discrete structure y = (y1 , . . . , ym ). For example, in part-of-speech (POS) tagging of an n-word input sentence, each yi 2 Many machine translation (MT) works aimed to generate diverse K-lists of translated sentences (e.g., Macherey et al., 2008; Gimpel et al., 2013; Li and Jurafsky, 2016). However, these methods are specific"
Q19-1041,P14-1126,0,0.0142906,"rbated models (see below). As opposed to the cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any"
Q19-1041,D13-1176,0,0.0144885,"and diverse list of sentences extracted from the text. In other cases the members of the solution list are exploited when solving an end goal application. For example, dependency forests were used in order to improve machine translation (Tu et al., 2010; Ma et al., 2018) and sentiment analysis (Tu et al., 2012). In yet other cases it is a first step towards learning a high quality structure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Other examples include bagging (Breiman, 1996; Sun and Wan, 2013) and boosting (Bawden and Crabb´e, 2016) as well as other ensemble methods (Surdeanu and Manning, 2010; T¨ackstr¨om et al., 2013; Kuncoro et al., 2016) that are often applied when the data available for model training is limited, in cases where exact argmax inference in the model is indefeasible, or when training i"
Q19-1041,D08-1076,0,0.0589789,"tantial gains. Finally, even though we integrate our method into a linear parser (Huang and Sagae, 2010), our modified parser outperforms a state-of-the-art (non-perturbated) BiLSTM parser (Kiperwasser and Goldberg, 2016) on our tasks. 2 K-lists in NLP Structured models in NLP Many NLP tasks, particularly tagging and parsing, involve the inference of a high-dimensional discrete structure y = (y1 , . . . , ym ). For example, in part-of-speech (POS) tagging of an n-word input sentence, each yi 2 Many machine translation (MT) works aimed to generate diverse K-lists of translated sentences (e.g., Macherey et al., 2008; Gimpel et al., 2013; Li and Jurafsky, 2016). However, these methods are specific to MT, whereas we focus on a general framework for structured prediction in NLP. 644 variable corresponds to an input word (and hence m = n), and is assigned a value in {1, . . . , P } where P is the number of POS tags. In dependency parsing, a graph G = (V, E ) is defined over an n-word input sentence such that each vertex corresponds to a word in the input sentence (|V |= n) and each arc corresponds to an ordered word pair (|E |= m = n2 ). In the structured model, each ordered pair of words in the input senten"
Q19-1041,Q16-1023,0,0.220307,"data. This setup is monolingual, we train and test on data from the same corpus. Our results demonstrate the quality of the Klists generated by our algorithm and of the tree returned by the MOM procedure. We compare our lists and final solution to those of a variety of alternative algorithms for K-list generation, including the K-best variant of the parser’s argmax inference algorithm, and demonstrate substantial gains. Finally, even though we integrate our method into a linear parser (Huang and Sagae, 2010), our modified parser outperforms a state-of-the-art (non-perturbated) BiLSTM parser (Kiperwasser and Goldberg, 2016) on our tasks. 2 K-lists in NLP Structured models in NLP Many NLP tasks, particularly tagging and parsing, involve the inference of a high-dimensional discrete structure y = (y1 , . . . , ym ). For example, in part-of-speech (POS) tagging of an n-word input sentence, each yi 2 Many machine translation (MT) works aimed to generate diverse K-lists of translated sentences (e.g., Macherey et al., 2008; Gimpel et al., 2013; Li and Jurafsky, 2016). However, these methods are specific to MT, whereas we focus on a general framework for structured prediction in NLP. 644 variable corresponds to an input"
Q19-1041,P13-2109,0,0.048778,"Missing"
Q19-1041,P10-1001,0,0.0165239,"is intractable, we first learn θ and then σ . We assume two training sets: S = {(xi , yi )}N i=1 and S = {(x i , yi )}N i=1 . Our training recipe is as follows: Notice that for first order parsing, our running example in this paper, this approach is essentially identical to the inference algorithm of Kuncoro et al. (2016), which was aimed at distilling a final solution from an ensemble of parsers. However, this MOM approach can naturally be extended beyond single variable potentials. For example, we can consider variable pair potentials or potentials over variable triplets and perform exact (Koo and Collins, 2010) or approximated (Martins et al., 2013; Tchernowitz et al., 2016) inference for second and third order problems. Here, for simplicity, we focus on single variable potentials and solve the resulting MOM problem directly with an exact MST algorithm. In what follows we first show that the MOM approach—recovering the best spanning tree according to the log-marginals of one Gibbsperturbation model—can be interpreted as a MAP approach over marginal probabilities of a continuous-discrete Gibbs model. We then discuss how we estimate the marginal probabilities μe (Equation 13). y ∈T 1. Learn the parame"
Q19-1041,D16-1180,0,0.0451843,"Missing"
Q19-1041,H05-1066,0,0.26546,"Missing"
Q19-1041,D11-1006,0,0.0229112,"use fixed noise parameters for the perturbated models (see below). As opposed to the cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provi"
Q19-1041,E17-1021,0,0.0750413,"a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for structured prediction in NLP in cases where high quality parameter estimation is challenging and"
Q19-1041,P12-1066,0,0.0281511,"Data. We consider two dependency parsing tasks: cross-lingual and monolingual but lightly supervised. For both tasks we consider Version 8 649 In this setup, to keep with the low resource language spirit, we do not learn the noise parameter (σ ) but rather use fixed noise parameters for the perturbated models (see below). As opposed to the cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Lik"
Q19-1041,N19-1393,0,0.0128973,"ervised. For both tasks we consider Version 8 649 In this setup, to keep with the low resource language spirit, we do not learn the noise parameter (σ ) but rather use fixed noise parameters for the perturbated models (see below). As opposed to the cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-t"
Q19-1041,P18-1142,1,0.82023,"Missing"
Q19-1041,N12-1005,0,0.0244589,"of a high quality and diverse list of sentences extracted from the text. In other cases the members of the solution list are exploited when solving an end goal application. For example, dependency forests were used in order to improve machine translation (Tu et al., 2010; Ma et al., 2018) and sentiment analysis (Tu et al., 2012). In yet other cases it is a first step towards learning a high quality structure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Other examples include bagging (Breiman, 1996; Sun and Wan, 2013) and boosting (Bawden and Crabb´e, 2016) as well as other ensemble methods (Surdeanu and Manning, 2010; T¨ackstr¨om et al., 2013; Kuncoro et al., 2016) that are often applied when the data available for model training is limited, in cases where exact argmax inference in the model is"
Q19-1041,D15-1039,0,0.018796,"below). As opposed to the cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for"
Q19-1041,P07-1078,1,0.603746,"parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for structured prediction in NLP in cases where high quality parameter estimation is challenging and the argmax solution is likely not to be the highest quality solution. We choose the tasks of crosslingual and lightly supervised dependency parsing since they form prominent NLP examples for our problem. We hence focus our experiments on an in-depth exploration of the impact of our framework on a dependency parser, rather than on a thorough comparison t"
Q19-1041,P15-2040,0,0.0140368,"rts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for structured prediction in NLP in cases where high quality parameter estimation is challenging and the argmax solution is likely not to be the highest quality solution."
Q19-1041,E03-1008,0,0.0509226,"ion projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for structured prediction in NLP in cases where high quality parameter estimation is challenging and the argmax solution is likely not to be the highest quality solution. We choose the tasks of crosslingual and lightly supervised dependency parsing since they form prominent NLP examples for our problem. We hence focus our experiments on an in-depth exploration of the impact of our framework on a dependency"
Q19-1041,Q13-1025,0,0.0154469,"Tu et al., 2012). In yet other cases it is a first step towards learning a high quality structure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Other examples include bagging (Breiman, 1996; Sun and Wan, 2013) and boosting (Bawden and Crabb´e, 2016) as well as other ensemble methods (Surdeanu and Manning, 2010; T¨ackstr¨om et al., 2013; Kuncoro et al., 2016) that are often applied when the data available for model training is limited, in cases where exact argmax inference in the model is indefeasible, or when training is not deterministic. In such cases, an ensemble of approximated solutions is fed into another model that extracts a final high quality solution. Unfortunately, both alternatives suffer from inherent limitations. K-best lists can be extracted by extensions of the argmax inference algo"
Q19-1041,N10-1091,0,0.0309859,"ure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Other examples include bagging (Breiman, 1996; Sun and Wan, 2013) and boosting (Bawden and Crabb´e, 2016) as well as other ensemble methods (Surdeanu and Manning, 2010; T¨ackstr¨om et al., 2013; Kuncoro et al., 2016) that are often applied when the data available for model training is limited, in cases where exact argmax inference in the model is indefeasible, or when training is not deterministic. In such cases, an ensemble of approximated solutions is fed into another model that extracts a final high quality solution. Unfortunately, both alternatives suffer from inherent limitations. K-best lists can be extracted by extensions of the argmax inference algorithm for many models: the K-best Viterbi algorithm (Golod, 2009) for Hidden Markov Models (Rabiner, 1"
Q19-1041,D12-1131,1,0.767046,"chev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for structured prediction in NLP in cases where high quality parameter estimation is challenging and the argmax solution is likely not to be the highest quality solution. We choose the tasks of crosslingual and lightly supervised dependency parsing since they form prominent NLP examples for our problem. We hence focus our experiments on an in-depth exploration of the impact of our framework on a dependency parser, rather than on a thorough comparison to previously proposed approaches. 5.2 Models and Experiments Pa"
Q19-1041,Q16-1035,0,0.020309,"ogical features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique that can enhance any machine learning model for structured prediction in NLP in cases where high quality parameter estimation is challenging and the argmax solution is likely not to be the highest quality solution. We choose the tasks of crosslingual and lightly supervised dependency parsing since they form pr"
Q19-1041,D16-1068,1,0.858967,"raining sets: S = {(xi , yi )}N i=1 and S = {(x i , yi )}N i=1 . Our training recipe is as follows: Notice that for first order parsing, our running example in this paper, this approach is essentially identical to the inference algorithm of Kuncoro et al. (2016), which was aimed at distilling a final solution from an ensemble of parsers. However, this MOM approach can naturally be extended beyond single variable potentials. For example, we can consider variable pair potentials or potentials over variable triplets and perform exact (Koo and Collins, 2010) or approximated (Martins et al., 2013; Tchernowitz et al., 2016) inference for second and third order problems. Here, for simplicity, we focus on single variable potentials and solve the resulting MOM problem directly with an exact MST algorithm. In what follows we first show that the MOM approach—recovering the best spanning tree according to the log-marginals of one Gibbsperturbation model—can be interpreted as a MAP approach over marginal probabilities of a continuous-discrete Gibbs model. We then discuss how we estimate the marginal probabilities μe (Equation 13). y ∈T 1. Learn the parameters θ of the Gibbs (base) model with the training set S . 2. Lea"
Q19-1041,Q18-1046,0,0.0224619,"Missing"
Q19-1041,C14-1175,0,0.0189223,"ers for the perturbated models (see below). As opposed to the cross-lingual setup, all the parsers are lexicalized, as this is a mono-lingual setup. Previous Work Recent years have seen substantial efforts devoted to our setups. For crosslingual parsing, the proposed approaches include the use of typological features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ponti et al., 2018; Scholivet et al., 2019), annotation projection and other means of using parallel text from the source and target languages (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Tiedemann, 2014; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi´c et al., 2016; Vilares et al., 2016; Schlichtkrull and Søgaard, 2017), similarity modeling for parser selection (Rosa and Zabokrtsky, 2015), late decoding (Søgaard and Schlichtkrull, 2017) and synthetic languages (Wang and Eisner, 2016, 2018b,a). Likewise, lightly supervised parsing has been addressed with a variety of approaches, including co-training (Steedman et al., 2003), self-training (Reichart and Rappoport, 2007) and inter-sentence consistency constraints (Rush et al., 2012). Our goal is to provide a technique th"
Q19-1041,D18-1163,0,0.0313455,"Missing"
Q19-1041,C10-1123,0,0.0348429,"agement, Technion, IIT {amichay.d|ramyazdi1012|tamir.hazan|roireichart}@gmail.com Abstract list of meaningful structures is of fundamental importance. This can stem from several reasons. First, it can be a defining property of the task. For example, in extractive summarization (Nenkova and McKeown, 2011) good summaries are those that consist of a high quality and diverse list of sentences extracted from the text. In other cases the members of the solution list are exploited when solving an end goal application. For example, dependency forests were used in order to improve machine translation (Tu et al., 2010; Ma et al., 2018) and sentiment analysis (Tu et al., 2012). In yet other cases it is a first step towards learning a high quality structure that cannot be learned by the model through standard argmax inference. For example, in the well-studied reranking setup (Collins, 2002; Collins and Koo, 2005; Charniak and Johnson, 2005; Son et al., 2012; Kalchbrenner and Blunsom, 2013), a K-best list of solutions is first extracted from a baseline learner, which typically has a limited feature space, and is then transferred to another feature-rich model that chooses the best solution from this list. Othe"
Q19-1041,D15-1213,0,0.0446963,"Missing"
Q19-1041,P16-2069,0,0.0456477,"Missing"
Q19-1041,P14-1019,0,0.0223272,"not fit well the test data. A final tree distilled from such a candidate list is more likely to be of higher quality than the list distilled from the list of the top scoring K structures, due to the better representation of the solution space. Unfortunately, this approach comes with a caveat: sampling a structure from the Gibbs distribution is often slower than finding the MAP assignment (Goldberg and Jerrum, 2007; Sontag et al., 2008). In our running example, the sampling of first order graph-based dependency parsing depends on the mean hitting time of a random walk in a graph (Wilson, 1996; Zhang et al., 2014), which is slower than finding the maximum spanning tree of the same graph. qθ (γ ) = e 1 (γe −θe )2 √ e 2 . 2π For now we assume that the variance of the posterior qθ (γ ) is 1 and defer its learning to § 3. Perturbation models measure the probability a structure is of maximal score, when considering all perturbations: pγ (y1 , . . . , ym ) = Pγ ∼qθ [y γ = y ] Perturbation-based K-lists Perturbation models define probability distributions over highdimensional discrete structures for which sampling is as fast as solving the MAP problem of a base, non-perturbated, model (Papandreou and Yuille,"
Q19-1044,J04-3004,0,0.0603548,"Missing"
Q19-1044,P15-1034,0,0.0178592,"success for deep learning models has been quite limited (§ 2). Our goal is to develop a self-training algorithm that can substantially enhance DNN models in cases where labeled training data are scarce. Particularly, we are focusing (§ 5) on the lightly supervised setup where only a small in-domain labeled dataset is available, and on the domain adaptation setup where the labeled dataset may be large but it comes from a different domain than the one to which the model is meant to be applied. Our focus task is dependency parsing, which is essential for many NLP tasks (Levy and Goldberg, 2014; Angeli et al., 2015; Toutanova et al., 2016; Hadiwinoto and Ng, 2017; Marcheggiani et al., 2017), but where self-training has typically failed (§ 2). Moreover, neural dependency parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) substantially outperform their linear predecessors, which makes the development of self-training methods that can enhance these parsers in low-resource setups a crucial challenge. We present a novel self-training method, suitable for neural dependency parsing. Our algorithm (§ 4) follows recent work that has demonstrated the power of pre-training for improving DNN models i"
Q19-1044,P18-1073,0,0.0360232,"es. For the domain adaptation case we consider 16 setups: 6 in different English domains and 10 in 5 other languages. Our Deep Contextualized Self-training (DCST) algorithm demonstrates substantial performance gains over a variety of baselines, including traditional self-training and the recent cross-view training approach (CVT) (Clark et al., 2018) that was designed for semi-supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multi"
Q19-1044,K18-2005,0,0.158284,"r, task-specific, layers of the final model are trained on the labeled data, while the parameters of the pre-trained embedding network are kept fixed. The most common pre-training task is language modeling or a closely related variant (McCann et al., 2017; Peters et al., 2018; Ziser and Reichart, 2018; Devlin et al., 2019). The outputs of the pre-trained DNN are often referred to as contextualized word embeddings, as these DNNs typically generate a vector embedding for each input word, which takes its context into account. Pre-training has led to performance gains in many NLP tasks. Recently, Che et al. (2018) incorporated ELMo embeddings (Peters et al., 2018) into a neural dependency parser and reported improvements over a range of Universal Dependency (UD) (McDonald et al., 2013; Niver et al., 2016, 2018) languages in the fully supervised setup. In this paper we focus on the lightly supervised and domain adaptation setups, trying to compensate for the lack of labeled data by exploiting automatically labeled trees generated by the base parser for unlabeled sentences. Our main experiments (§7) are with models that utilize non-contextualized word embeddings. We believe this is a more practical setup"
Q19-1044,C08-1015,0,0.0303823,"impact of deep contextualized self-training on top of contextualized word embeddings. Figure 1: The BiAFFINE parser. 3 Background: The BiAFFINE Parser Lightly Supervised Learning and Domain Adaptation for Dependency Parsing Finally, we briefly survey earlier attempts to learn parsers in setups where labeled data from the domain to which the parser is meant to be applied is scarce. We exclude from this brief survey literature that has already been mentioned above. Some notable attempts are: exploiting short dependencies in the parser’s output when applied to large target domain unlabeled data (Chen et al., 2008), adding inter-sentence consistency constraints at test time (Rush et al., 2012), selecting effective training domains (Plank and Van Noord, 2011), exploiting parsers trained on different domains through a mixture of experts (McClosky et al., 2010), embedding features in a vector space (Chen et al., 2014), and Bayesian averaging of a range of parser parameters (Shareghi et al., 2019). Recently, Sato et al. (2017) presented an adversarial model for cross-domain dependency parsing in which the encoders of the source and the target domains are integrated through a gating mechanism. Their approach"
Q19-1044,C14-1078,0,0.029262,"ed data from the domain to which the parser is meant to be applied is scarce. We exclude from this brief survey literature that has already been mentioned above. Some notable attempts are: exploiting short dependencies in the parser’s output when applied to large target domain unlabeled data (Chen et al., 2008), adding inter-sentence consistency constraints at test time (Rush et al., 2012), selecting effective training domains (Plank and Van Noord, 2011), exploiting parsers trained on different domains through a mixture of experts (McClosky et al., 2010), embedding features in a vector space (Chen et al., 2014), and Bayesian averaging of a range of parser parameters (Shareghi et al., 2019). Recently, Sato et al. (2017) presented an adversarial model for cross-domain dependency parsing in which the encoders of the source and the target domains are integrated through a gating mechanism. Their approach requires target domain labeled data for parser training and hence it cannot be applied in the unsupervised domain adaptation setup we explore (§ 5). We adopt their gating mechanism to our model and extend it to integrate more than two encoders into a final model. 697 The parser we utilize in our experime"
Q19-1044,D18-1217,0,0.302563,"s then trained on the manually labeled data. We experiment (§ 6,7) with a large variety of lightly supervised and domain adaptation dependency parsing setups. For the lightly supervised case we consider 17 setups: 7 in different English domains and 10 in other languages. For the domain adaptation case we consider 16 setups: 6 in different English domains and 10 in 5 other languages. Our Deep Contextualized Self-training (DCST) algorithm demonstrates substantial performance gains over a variety of baselines, including traditional self-training and the recent cross-view training approach (CVT) (Clark et al., 2018) that was designed for semi-supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised a"
Q19-1044,N19-1423,0,0.214007,"diwinoto and Ng, 2017; Marcheggiani et al., 2017), but where self-training has typically failed (§ 2). Moreover, neural dependency parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) substantially outperform their linear predecessors, which makes the development of self-training methods that can enhance these parsers in low-resource setups a crucial challenge. We present a novel self-training method, suitable for neural dependency parsing. Our algorithm (§ 4) follows recent work that has demonstrated the power of pre-training for improving DNN models in NLP (Peters et al., 2018; Devlin et al., 2019) Neural dependency parsing has proven very effective, achieving state-of-the-art results on numerous domains and languages. Unfortunately, it requires large amounts of labeled data, which is costly and laborious to create. In this paper we propose a selftraining algorithm that alleviates this annotation bottleneck by training a parser on its own output. Our Deep Contextualized Self-training (DCST) algorithm utilizes representation models trained on sequence labeling tasks that are derived from the parser’s output when applied to unlabeled data, and integrates these models with the base parser"
Q19-1044,P81-1022,0,0.475919,"Missing"
Q19-1044,P18-1128,1,0.840918,"is considered the target. For all domain adaptation experiments, when training the final hybrid parser (Figure 3) we sometimes found it useful to keep the parameters of the BiLSTM tagger(s) fixed in order to avoid an overfitting of the final parser to the source domain. We treat the decision of whether or not to keep the parameters of the tagger(s) fixed as a hyper-parameter of the DCST models and tune it on the development data. We measure parsing accuracy with the standard Unlabeled and Labeled Attachment Scores (UAS and LAS), and measure statistical significance with the t-test (following Dror et al., 2018). Models and Baselines We consider four variants of our DCST algorithm, differing on the word tagging scheme on which the BiLSTM of step 4 is trained (§ 4.1): DCST-NC: with the Number of Children scheme, DCST-DR: with the Distance from the Root scheme, DCST-RPE: with the Relative POS-based Encoding scheme, and DCST-ENS where the parser is integrated with three BiLSTMs, one for each scheme (where ENS stands for ensemble) (§ 4.2). To put the results of our DCST algorithm in context, we compare its performance to the following baselines. Base: the BiAFFINE parser (§ 3), trained on the labeled tra"
Q19-1044,P17-1104,0,0.0276094,"n sequence labeling tasks that are derived from the parser’s output when applied to unlabeled data, and integrates these models with the base parser through a gating mechanism. We conduct experiments across multiple languages, both in low resource in-domain and in cross-domain setups, and demonstrate that DCST substantially outperforms traditional self-training as well as recent semi-supervised training methods.1 1 Introduction Deep neural networks (DNNs) have improved the state-of-the-art in a variety of NLP tasks. These include dependency parsing (Dozat and Manning, 2017), semantic parsing (Hershcovich et al., 2017), named entity recognition (Yadav and Bethard, 2018), part of speech (POS) tagging (Plank and Agi´c, 2018), and machine translation (Vaswani et al., 2017), among others. Unfortunately, DNNs rely on in-domain labeled training data, which is costly and laborious to achieve. This annotation bottleneck limits the applicability of NLP technology to a small number of languages and domains. It is hence not a surprise that substantial recent research efforts have been 1 Our code is publicly available at https://github. com/rotmanguy/DCST. 695 Transactions of the Association for Computational Linguisti"
Q19-1044,N06-2015,0,0.0765344,"to parse sentences from the domain of L and U. The Unsupervised Domain Adaptation Setup In this setup we are given a labeled source domain |L| dataset L = {(xli , yil )}i=1 of sentences and their gold dependency trees, and an unlabeled dataset |U| U = {(xui )}i=1 of sentences from a different target domain. Unlike the lightly-supervised setup, here L may be large enough to train a high-quality parser as long as the training and test sets come from the same domain. However, our goal here is to parse sentences from the target domain. Data We consider two datasets: (a) The English OntoNotes 5.0 (Hovy et al., 2006) corpus. This corpus consists of text from 7 domains: broadcast conversation (bc: 11877 training, 2115 development, and 2209 test sentences), broadcast news (bn: 10681, 1293, 1355), magazine (mz: 6771, 640, 778), news (nw: 34967, 5894, 2325), bible (pt: 21518, 1778, 1867), telephone conversation (tc: 12889, 1632, 1364), and Web (wb: 15639, 2264, 1683).2 The corpus is annotated with constituency parse trees and POS tags, as well as other labels that we do not use in our experiments. The constituency trees were converted to dependency trees using the Elitcloud conversion tool.3 In the lightly su"
Q19-1044,W18-2713,0,0.0620193,"Missing"
Q19-1044,Q18-1017,0,0.0254667,"6 0.327 0.322 0.316 0.312 0.600 0.551 0.538 0.534 0.524 Table 5: Tagging scheme error analysis. Model UAS LAS Base DCST-LM Self-Training CVT 54.86 55.26 54.22 50.61 52.65 52.63 52.16 46.13 DCST-ENS 58.85 56.64 Table 6: Sentence length adaptation results. distance of a word from the root in the gold tree and the corresponding distance in the predicted tree; Absolute Difference of Positional Distance from the Head (AD-PDH): The absolute difference between the positional distance of a word from its head word according to the gold tree and the corresponding number according to the predicted tree (Kiperwasser and Ballesteros, 2018) (we count the words that separate the head from the modifier in the sentence, considering the distance negative if the word is to the right of its head); and POS Head Error: an indicator function which returns 0 if the POS tag of the head word of a given word according to the gold tree is identical to the corresponding POS tag in the predicted tree, and 1 otherwise. For all the metrics we report the mean value across all words in our test sets. The values of AD-NC, AD-DR, and AD-PDH are hence in the [0, M ] range, where M is the length of the longest sentence in the corpus. The values of the"
Q19-1044,Q16-1023,0,0.0409722,"data are scarce. Particularly, we are focusing (§ 5) on the lightly supervised setup where only a small in-domain labeled dataset is available, and on the domain adaptation setup where the labeled dataset may be large but it comes from a different domain than the one to which the model is meant to be applied. Our focus task is dependency parsing, which is essential for many NLP tasks (Levy and Goldberg, 2014; Angeli et al., 2015; Toutanova et al., 2016; Hadiwinoto and Ng, 2017; Marcheggiani et al., 2017), but where self-training has typically failed (§ 2). Moreover, neural dependency parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) substantially outperform their linear predecessors, which makes the development of self-training methods that can enhance these parsers in low-resource setups a crucial challenge. We present a novel self-training method, suitable for neural dependency parsing. Our algorithm (§ 4) follows recent work that has demonstrated the power of pre-training for improving DNN models in NLP (Peters et al., 2018; Devlin et al., 2019) Neural dependency parsing has proven very effective, achieving state-of-the-art results on numerous domains and languages. Unfortunately, it requires"
Q19-1044,P11-1149,1,0.769462,"in 5 other languages. Our Deep Contextualized Self-training (DCST) algorithm demonstrates substantial performance gains over a variety of baselines, including traditional self-training and the recent cross-view training approach (CVT) (Clark et al., 2018) that was designed for semi-supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researc"
Q19-1044,P14-2050,0,0.0519939,"variety of NLP tasks, its success for deep learning models has been quite limited (§ 2). Our goal is to develop a self-training algorithm that can substantially enhance DNN models in cases where labeled training data are scarce. Particularly, we are focusing (§ 5) on the lightly supervised setup where only a small in-domain labeled dataset is available, and on the domain adaptation setup where the labeled dataset may be large but it comes from a different domain than the one to which the model is meant to be applied. Our focus task is dependency parsing, which is essential for many NLP tasks (Levy and Goldberg, 2014; Angeli et al., 2015; Toutanova et al., 2016; Hadiwinoto and Ng, 2017; Marcheggiani et al., 2017), but where self-training has typically failed (§ 2). Moreover, neural dependency parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) substantially outperform their linear predecessors, which makes the development of self-training methods that can enhance these parsers in low-resource setups a crucial challenge. We present a novel self-training method, suitable for neural dependency parsing. Our algorithm (§ 4) follows recent work that has demonstrated the power of pre-training for i"
Q19-1044,L18-1550,0,0.155016,"wn to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researched compared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li, 2005; Søgaard, 2010) on DNNs for POS tagging and sentiment analysis. They found 696 self-training to be effective for the sentiment classification task, but it failed to improve their BiLSTM POS tagging architecture. Tri-training has shown effective for both the classification and the sequence tagging task, and in Vinyals et al. (2015) it has shown useful for neural constituency parsing. This is in-line with Steedman et al. (2003), who demonstrated the effectiveness of the closely related co-traini"
Q19-1044,P18-1130,0,0.0297897,"nally re-train the Base parser on both the manual and automatic trees. We would also like to test the value of training a representation model to predict the dependency labeling schemes of § 4.1, in comparison to the now standard pre-training with a language modeling objective. Hence, we experiment with a variant of DCST where the BiLSTM of step 4 is trained as a language model (DCST-LM). Finally, we compare to the cross-view training algorithm (CVT) (Clark et al., 2018), which was developed for semi-supervised learning with DNNs.6 Hyper-parameters We use the BiAFFINE parser implementation of Ma et al. (2018).7 We consider the following hyper-parameters for the parser and the sequence tagger: 100 epochs with an early stopping criterion according to the development set, the ADAM optimizer (Kingma and Ba, 2015), a batch size of 16, a learning rate of 0.002, and dropout probabilities of 0.33. The 3-layer stacked BiLSTMs of the parser and the sequence tagger generate hidden representations of size 1024. The fully connected layers of the tagger are of size 128 (first layer) 6 https://github.com/tensorflow/models/ tree/master/research/cvt text. 7 https://github.com/XuezheMax/NeuroNLP2. 5 In languages wh"
Q19-1044,K17-1041,0,0.0242063,"al is to develop a self-training algorithm that can substantially enhance DNN models in cases where labeled training data are scarce. Particularly, we are focusing (§ 5) on the lightly supervised setup where only a small in-domain labeled dataset is available, and on the domain adaptation setup where the labeled dataset may be large but it comes from a different domain than the one to which the model is meant to be applied. Our focus task is dependency parsing, which is essential for many NLP tasks (Levy and Goldberg, 2014; Angeli et al., 2015; Toutanova et al., 2016; Hadiwinoto and Ng, 2017; Marcheggiani et al., 2017), but where self-training has typically failed (§ 2). Moreover, neural dependency parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) substantially outperform their linear predecessors, which makes the development of self-training methods that can enhance these parsers in low-resource setups a crucial challenge. We present a novel self-training method, suitable for neural dependency parsing. Our algorithm (§ 4) follows recent work that has demonstrated the power of pre-training for improving DNN models in NLP (Peters et al., 2018; Devlin et al., 2019) Neural dependency parsing ha"
Q19-1044,N06-1020,0,0.107934,"nt cross-view training approach (CVT) (Clark et al., 2018) that was designed for semi-supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researched compared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li,"
Q19-1044,P06-1043,0,0.280161,"nt cross-view training approach (CVT) (Clark et al., 2018) that was designed for semi-supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researched compared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li,"
Q19-1044,W04-2405,0,0.112384,"ferent English domains and 10 in other languages. For the domain adaptation case we consider 16 setups: 6 in different English domains and 10 in 5 other languages. Our Deep Contextualized Self-training (DCST) algorithm demonstrates substantial performance gains over a variety of baselines, including traditional self-training and the recent cross-view training approach (CVT) (Clark et al., 2018) that was designed for semi-supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for"
Q19-1044,D14-1162,0,0.0837169,"4.13 74.32 81.05 81.40 81.95 82.40 75.09 75.41 75.98 76.61 58.17 58.30 59.49 59.60 39.95 40.25 41.45 41.72 86.17 86.19 86.86 86.96 79.91 79.68 80.92 80.85 69.93 69.46 70.23 70.37 65.91 65.65 66.26 66.88 Base-FS 86.13 81.46 85.55 82.93 91.06 88.12 77.42 62.31 85.02 81.59 86.04 82.22 85.18 81.36 62.21 46.23 89.84 85.12 73.26 69.69 Table 2: Lightly supervised UD results with 500 training sentences. Base-FS is an upper bound. and 64 (second layer). All other parser hyperparameters are identical to those of the original implementation. We utilize 300-dimensional pre-trained word embeddings: GloVe (Pennington et al., 2014)8 for English and FastText (Grave et al., 2018)9 for the UD languages. Character and POS embeddings are 100-dimensional and are initialized to random normal vectors. CVT is run for 15K gradient update steps. Note, that with few exceptions, DCST-NC is the least effective method among the syntactically self-trained DCST alternatives. This indicates that encoding the number of children each word has in the dependency tree is not a sufficiently informative view of the tree. 7 Results Table 1 presents the lightly supervised OntoNotes results when training with 500 labeled sentences, and Table 2 pre"
Q19-1044,N18-1202,0,0.172577,"Missing"
Q19-1044,D18-1061,0,0.0612155,"Missing"
Q19-1044,P11-1157,0,0.0487733,"Missing"
Q19-1044,P07-1078,1,0.903779,"supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researched compared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li, 2005; Søgaard, 2010) on DNNs for POS tagging and sentiment analysis. They found 696 self-trai"
Q19-1044,P18-1096,0,0.0596283,"training has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researched compared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li, 2005; Søgaard, 2010) on DNNs for POS tagging and sentiment analysis. They found 696 self-training to be effective for the sentiment classification task, but it failed to improve their BiLSTM POS tagging architecture. Tri-training has shown effective for both the classification and the sequence tagging task, and in Vinyals et al. (2015) it has shown useful for neural constituency parsing. This is in-line with Steedman et al. (2003), who demonstrated the effectiveness of the closely related co-traini"
Q19-1044,D12-1131,1,0.950638,"ense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researched compared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li, 2005; Søgaard, 2010) on DNNs for POS tagging and sentiment analysis. They found 696 self-training to be effective for the sentiment classification task, but it failed to improve their BiLSTM POS tagging architecture. Tri"
Q19-1044,K18-2004,0,0.0662719,"Missing"
Q19-1044,K17-3007,0,0.132925,"y literature that has already been mentioned above. Some notable attempts are: exploiting short dependencies in the parser’s output when applied to large target domain unlabeled data (Chen et al., 2008), adding inter-sentence consistency constraints at test time (Rush et al., 2012), selecting effective training domains (Plank and Van Noord, 2011), exploiting parsers trained on different domains through a mixture of experts (McClosky et al., 2010), embedding features in a vector space (Chen et al., 2014), and Bayesian averaging of a range of parser parameters (Shareghi et al., 2019). Recently, Sato et al. (2017) presented an adversarial model for cross-domain dependency parsing in which the encoders of the source and the target domains are integrated through a gating mechanism. Their approach requires target domain labeled data for parser training and hence it cannot be applied in the unsupervised domain adaptation setup we explore (§ 5). We adopt their gating mechanism to our model and extend it to integrate more than two encoders into a final model. 697 The parser we utilize in our experiments is the BiAFFINE parser (Dozat and Manning, 2017). Because the structure of the parser affects our DCST alg"
Q19-1044,K17-3001,0,0.0634071,"Missing"
Q19-1044,P16-1136,0,0.0184087,"rning models has been quite limited (§ 2). Our goal is to develop a self-training algorithm that can substantially enhance DNN models in cases where labeled training data are scarce. Particularly, we are focusing (§ 5) on the lightly supervised setup where only a small in-domain labeled dataset is available, and on the domain adaptation setup where the labeled dataset may be large but it comes from a different domain than the one to which the model is meant to be applied. Our focus task is dependency parsing, which is essential for many NLP tasks (Levy and Goldberg, 2014; Angeli et al., 2015; Toutanova et al., 2016; Hadiwinoto and Ng, 2017; Marcheggiani et al., 2017), but where self-training has typically failed (§ 2). Moreover, neural dependency parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) substantially outperform their linear predecessors, which makes the development of self-training methods that can enhance these parsers in low-resource setups a crucial challenge. We present a novel self-training method, suitable for neural dependency parsing. Our algorithm (§ 4) follows recent work that has demonstrated the power of pre-training for improving DNN models in NLP (Peters et al., 20"
Q19-1044,N19-1354,1,0.842086,". We exclude from this brief survey literature that has already been mentioned above. Some notable attempts are: exploiting short dependencies in the parser’s output when applied to large target domain unlabeled data (Chen et al., 2008), adding inter-sentence consistency constraints at test time (Rush et al., 2012), selecting effective training domains (Plank and Van Noord, 2011), exploiting parsers trained on different domains through a mixture of experts (McClosky et al., 2010), embedding features in a vector space (Chen et al., 2014), and Bayesian averaging of a range of parser parameters (Shareghi et al., 2019). Recently, Sato et al. (2017) presented an adversarial model for cross-domain dependency parsing in which the encoders of the source and the target domains are integrated through a gating mechanism. Their approach requires target domain labeled data for parser training and hence it cannot be applied in the unsupervised domain adaptation setup we explore (§ 5). We adopt their gating mechanism to our model and extend it to integrate more than two encoders into a final model. 697 The parser we utilize in our experiments is the BiAFFINE parser (Dozat and Manning, 2017). Because the structure of t"
Q19-1044,P10-2038,0,0.169281,"nd in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was found useful for neural dependency parsing in fully supervised multilingual settings (Rybak and Wr´oblewska, 2018). The impact of self-training on DNNs is less researched compared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li, 2005; Søgaard, 2010) on DNNs for POS tagging and sentiment analysis. They found 696 self-training to be effective for the sentiment classification task, but it failed to improve their BiLSTM POS tagging architecture. Tri-training has shown effective for both the classification and the sequence tagging task, and in Vinyals et al. (2015) it has shown useful for neural constituency parsing. This is in-line with Steedman et al. (2003), who demonstrated the effectiveness of the closely related co-training method (Blum and Mitchell, 1998) for linear constituency parsers. Lastly, Clark et al. (2018) presented the CVT al"
Q19-1044,E03-1008,0,0.0621325,"pared with the extensive investigation with linear models. Recently, Ruder and Plank (2018) evaluated the impact of self-training and the closely related tri-training method (Zhou and Li, 2005; Søgaard, 2010) on DNNs for POS tagging and sentiment analysis. They found 696 self-training to be effective for the sentiment classification task, but it failed to improve their BiLSTM POS tagging architecture. Tri-training has shown effective for both the classification and the sequence tagging task, and in Vinyals et al. (2015) it has shown useful for neural constituency parsing. This is in-line with Steedman et al. (2003), who demonstrated the effectiveness of the closely related co-training method (Blum and Mitchell, 1998) for linear constituency parsers. Lastly, Clark et al. (2018) presented the CVT algorithm, a variant of self-training that uses unsupervised representation learning. CVT differs from classical self-training in the way it exploits the unlabeled data: It trains auxiliary models on restricted views of the input to match the predictions of the full model that observes the whole input. We propose a self-training algorithm based on deep contextualized embeddings, where the embedding model is train"
Q19-1044,P19-1439,0,0.0519469,"Missing"
Q19-1044,C18-1182,0,0.0292772,"arser’s output when applied to unlabeled data, and integrates these models with the base parser through a gating mechanism. We conduct experiments across multiple languages, both in low resource in-domain and in cross-domain setups, and demonstrate that DCST substantially outperforms traditional self-training as well as recent semi-supervised training methods.1 1 Introduction Deep neural networks (DNNs) have improved the state-of-the-art in a variety of NLP tasks. These include dependency parsing (Dozat and Manning, 2017), semantic parsing (Hershcovich et al., 2017), named entity recognition (Yadav and Bethard, 2018), part of speech (POS) tagging (Plank and Agi´c, 2018), and machine translation (Vaswani et al., 2017), among others. Unfortunately, DNNs rely on in-domain labeled training data, which is costly and laborious to achieve. This annotation bottleneck limits the applicability of NLP technology to a small number of languages and domains. It is hence not a surprise that substantial recent research efforts have been 1 Our code is publicly available at https://github. com/rotmanguy/DCST. 695 Transactions of the Association for Computational Linguistics, vol. 7, pp. 695–713, 2019. https://doi.org/10.11"
Q19-1044,P95-1026,0,0.823852,"setups: 7 in different English domains and 10 in other languages. For the domain adaptation case we consider 16 setups: 6 in different English domains and 10 in 5 other languages. Our Deep Contextualized Self-training (DCST) algorithm demonstrates substantial performance gains over a variety of baselines, including traditional self-training and the recent cross-view training approach (CVT) (Clark et al., 2018) that was designed for semi-supervised learning with DNNs. 2 Previous Work Self-training in NLP Self-training has shown useful for various NLP tasks, including word sense disambiguation (Yarowsky, 1995; Mihalcea, 2004), bilingual lexicon induction (Artetxe et al., 2018), neural machine translation (Imamura and Sumita, 2018), semantic parsing (Goldwasser et al., 2011), and sentiment analysis (He and Zhou, 2011). For constituency parsing, selftraining has shown to improve linear parsers both when considerable training data are available (McClosky et al., 2006a,b), and in the lightly supervised and the cross-domain setups (Reichart and Rappoport, 2007). Although several authors failed to demonstrate the efficacy of self-training for dependency parsing (e.g., Rush et al., 2012), recently it was"
Q19-1044,W18-5448,0,0.0263792,"AS Base Base+RG DCST-LM Self-Training CVT 69.92 73.12 73.59 69.50 59.77 81.83 80.86 83.33 81.53 81.53 59.05 58.97 59.41 59.67 51.12 60.31 60.52 60.54 61.41 50.31 67.82 67.54 67.52 68.02 58.60 80.72 80.36 80.95 82.01 70.07 65.03 65.93 65.19 66.47 50.82 62.75 61.50 62.46 63.84 45.15 77.08 77.58 77.40 77.60 45.25 77.93 78.04 77.62 77.64 62.87 DCST-ENS 75.28 86.50 59.75 60.98 69.13 83.06 67.65 63.46 77.86 78.97 Base-FS 84.46 83.70 84.44 78.09 90.02 81.22 81.71 84.99 82.43 86.67 Table 4: Unsupervised Domain adaptation UD results. Base-FS is an upper bound. substantially enhanced model performance (Zhang and Bowman, 2018; Tenney et al., 2019; Wang et al., 2019; Wieting and Kiela, 2019). Our results lead to two conclusions. Firstly, Base+RG, the model that is identical to the syntactically trained DCST except that the BiAFFINE parser is integrated with a randomly initialized BiLSTM through our gating mechanism, is consistently outperformed by all our syntactically self-trained DCST models, with very few exceptions. Secondly, in line with the conclusions of the aforementioned papers, Base+RG is one of the strongest baselines in our experiments. Perhaps most importantly, in most experiments this model outperform"
Q19-1044,N18-1112,1,0.835808,"eneck limits the applicability of NLP technology to a small number of languages and domains. It is hence not a surprise that substantial recent research efforts have been 1 Our code is publicly available at https://github. com/rotmanguy/DCST. 695 Transactions of the Association for Computational Linguistics, vol. 7, pp. 695–713, 2019. https://doi.org/10.1162/tacl a 00294 Action Editor: Yue Zhang. Submission batch: 7/2019; Revision batch: 9/2019; Published 12/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  and particularly for domain adaptation (Ziser and Reichart, 2018). However, whereas in previous work a representation model, also known as a contextualized embedding model, is trained on a language modeling related task, our algorithm utilizes a representation model that is trained on sequence prediction tasks derived from the parser’s output. Our representation model and the base parser are integrated into a new model through a gating mechanism, and the resulting parser is then trained on the manually labeled data. We experiment (§ 6,7) with a large variety of lightly supervised and domain adaptation dependency parsing setups. For the lightly supervised ca"
Q19-1044,N19-1077,0,\N,Missing
Q19-1044,P13-2017,0,\N,Missing
Q19-1044,N10-1004,0,\N,Missing
W09-1103,W04-3202,0,0.205117,"Missing"
W09-1103,J04-3001,0,0.699938,"tance. In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated sentences S, select a subset S1 ⊂ S for human annotation such that the human efforts in annotating S1 are minimized while the parser performance when trained with this sample is maximized. Previous works addressing training sample size vs. parser performance for constituency parsers (Section 2) evaluated training sample size using the total number of constituents (TC). Sentences differ in length and therefore in annotation efforts, and it has been argued (see, e.g, (Hwa, 2004)) that TC reflects the number of decisions the human annotator makes when syntactically annotating the sample, assuming uniform cost for each decision. In this paper we posit that important aspects of the efforts involved in annotating a sample are not reflected by the TC measure. Since annotators analyze sentences rather than a bag of constituents, sentence structure has a major impact on their cognitive efforts. Sizeable psycholinguistic literature points to the connection between nested structures in the syntactic structure of a sentence and its annotation efforts. This has motivated us to"
W09-1103,I05-1006,0,0.0136421,"uction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated sentences S, select a subset S1 ⊂ S for human annotation such that the human efforts in annotating S1 are minimized while the parser performance when trained with this sample is maximized. Previous works addressing training sample size vs. parser p"
W09-1103,W06-1606,0,0.0141812,"a novel optimisation problem, the constrained multiset multicover problem, and for cluster-based sampling according to syntactic parameters. Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance. 1 Introduction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: given a parsing algorithm"
W09-1103,P00-1016,0,0.04922,"Missing"
W09-1103,P08-1098,1,0.78239,"diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Cognitively Driven Evaluation Measures While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance. We focus on structures that are widely agreed to impose a high cognitive load on the human annotator and on theories considering the cognitive resources required in parsing a complete sentence. Based on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentences. Nested structures. A nested structure is a par"
W09-1103,W07-1001,0,0.0600623,"e present work, and TE gave the best results. They used an uncertainty sampling protocol, where in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters. The most uncertain sentences are selected from the clusters, the training taking into account the densities of the clusters. They reduced the number of training sentences required for their parser to achieve its best performance from 1300 to 400. The importance of cognitively driven measures of sentences’ syntactic complexity has been recognized by Roark et al. (2007) who demonstrated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity"
W09-1103,P02-1016,0,0.264408,"Missing"
W09-1103,D07-1051,0,0.0146346,"ated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Cognitively Driven Evaluation Measures While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance. We focus on structures that are widely agreed to impose a high cognitive load on the human annotator and on theories considering the cognitive resources required in parsing a complete sentence. Based on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentenc"
W09-1103,P05-1073,0,0.0255509,"Missing"
W09-1103,C08-1143,0,0.0193374,"ere in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters. The most uncertain sentences are selected from the clusters, the training taking into account the densities of the clusters. They reduced the number of training sentences required for their parser to achieve its best performance from 1300 to 400. The importance of cognitively driven measures of sentences’ syntactic complexity has been recognized by Roark et al. (2007) who demonstrated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Co"
W09-1103,J93-2004,0,\N,Missing
W09-1103,J04-4004,0,\N,Missing
W09-1103,J03-4003,0,\N,Missing
W09-1108,P06-1109,0,0.0183595,"ndidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al.,"
W09-1108,W06-2912,0,0.0126787,"ndidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al.,"
W09-1108,P07-1051,0,0.0125582,"lldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durm"
W09-1108,P99-1016,0,0.0286053,"orpus annotation and other human input used, and in their basic algorithmic approach. Some methods directly aim at concept acquisition, while the direct goal in some is the construction of hyponym (‘is-a’) hierarchies. A subtree in such a hierarchy can be viewed as defining a concept. A major algorithmic approach is to represent word contexts as vectors in some space and use distributional measures and clustering in that space. Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Several studies avoid requiring any syntactic annotation. Some methods are based on decompo49 sition of a lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comparison to pattern approaches (see below), and most of them do not scale well for very large datasets. The second main approach is to use lex"
W09-1108,P05-1022,0,0.0409138,"ws results of this test. P P+ Reg. Window 4 4 ×4 Window 18 5 No windowing 33 21 Table 4: Percentage of noisy concepts as a function of windowing. We can see that while windowing is still essential even with available parser data, using this data we can significantly reduce windowing requirements, allowing us to discover more concepts from the same data. Timing requirements are modest, considering we parsed such large amounts of data. BNC parsing took 45 minutes, and the total single-machine processing time for the 68Gb DMOZ corpus was 4 days6 . In comparison, a state-of-art supervised parser (Charniak and Johnson, 2005) would process the same amount of data in 1.3 years7 . 7 Discussion We have presented a framework which utilizes an efficient fully unsupervised parser for unsupervised pattern-based discovery of concepts. We showed that utilization of unsupervised parser in pattern acquisition not only allows successful extraction of MWEs but also improves the quality of obtained concepts, avoiding noise and adding new terms missed by the parse-less approach. At the same time, the framework remains fully unsupervised, allowing its straightforward application to different languages as supported by our bilingua"
W09-1108,P06-1038,1,0.276144,"named entity recognition tools (Dorow et al., 2005). Pantel et al. (2004) reduce the depth of linguistic data used, but their method requires POS tagging. TextRunner (Banko et al., 2007) utilizes a set of pattern-based seed-less strategies in order to extract relational tuples from text. However, this system contains many language-specific modules, including the utilization of a parser in one of the processing stages. Thus the majority of the existing pattern-based concept acquisition systems rely on pattern/word seeds or supervised language-specific tools, some of which are very inefficient. Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. This framework allows a fully unsupervised seed-less discovery of concepts without relying on language-specific tools. However, it completely ignores potentially useful syntactic or morphological information. For example, the pattern ‘X and his Y’ is useful for acquiring the concept of family member types, as in “his siblings and his parents’. Without syntactic information, it can capture noise, as in “... in ireland) and his wife)” (parentheses denote syntactic constituen"
W09-1108,P07-1030,1,0.856066,"dels (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing with the most recent and comprehensive data, web-based evaluation has some methodological drawbacks such as limited repeatability (Kilgarriff, 2007). In this study we apply our framework on offline corpora in settings similar to that of previous work, in order to be able to make proper comparisons. 3 Efficient Unsupervised Parsing Our method utilizes the information induced by unsupervised parsers. Specifically, we make use of the bracketings induced by Seginer’s parser"
W09-1108,E09-1021,1,0.73873,"plar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing with the most recent and comprehensive data, web-based evaluation has some methodological drawbacks such as limited repeatability (Kilgarriff, 2007). In this study we apply our framework on offline corpora in settings similar to that of previous work, in order to be able to make proper comparisons. 3 Efficient Unsupervised Parsing Our method utilizes the information induced by unsupervised parsers. Specifically, we make use of the bracketings induced by Seginer’s parser1 (Seginer, 2007). This parser has advantages in three ma"
W09-1108,P06-1046,0,0.0196722,"measures and clustering in that space. Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Several studies avoid requiring any syntactic annotation. Some methods are based on decompo49 sition of a lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comparison to pattern approaches (see below), and most of them do not scale well for very large datasets. The second main approach is to use lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns, numerous pattern-based methods have been proposed for the discovery of concepts from seeds. Other studies develop concept acquisition for on-de"
W09-1108,C08-1042,0,0.0310889,"Missing"
W09-1108,C92-2082,0,0.0225641,"lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comparison to pattern approaches (see below), and most of them do not scale well for very large datasets. The second main approach is to use lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns, numerous pattern-based methods have been proposed for the discovery of concepts from seeds. Other studies develop concept acquisition for on-demand tasks where concepts are defined by user-provided seeds. Many of these studies utilize information obtained by language-specific parsing and named entity recognition tools (Dorow et al., 2005). Pantel et al. (2004) reduce the depth of linguistic data used, but their method requires POS tagging. TextRunner (Banko et al., 2007) utilizes a set of pattern-based seed-less strategies in orde"
W09-1108,P02-1017,0,0.0189153,"nsidered as a candidate for the ‘dog type’ concept), and misses the discovery of a valid multiword candidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed framework"
W09-1108,P04-1061,0,0.0710161,"nd misses the discovery of a valid multiword candidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzio"
W09-1108,P98-2127,0,0.227447,"on lexical acquisition of all sorts and the acquisition of concepts in particular. Concept acquisition methods differ in the type of corpus annotation and other human input used, and in their basic algorithmic approach. Some methods directly aim at concept acquisition, while the direct goal in some is the construction of hyponym (‘is-a’) hierarchies. A subtree in such a hierarchy can be viewed as defining a concept. A major algorithmic approach is to represent word contexts as vectors in some space and use distributional measures and clustering in that space. Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Several studies avoid requiring any syntactic annotation. Some methods are based on decompo49 sition of a lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comp"
W09-1108,J93-2004,0,0.0336989,"Missing"
W09-1108,P93-1024,0,0.709275,"Missing"
W09-1108,P07-1049,0,0.224633,"tion in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing with the most recent and comprehensive data, web-"
W09-1108,P06-1072,0,0.0191763,"s a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing wit"
W09-1108,C02-1114,0,0.423695,"mples of such patterns include “((C1 )in C2 ))”, “(C1 )(such(as(((C2 )”, and “(C1 )and(C2 )”3 . We dismiss rare patterns that appear less than TP times per million words. Symmetric patterns. Many of the pattern candidates discovered in the previous stage are not usable. In order to find a usable subset, we focus on the symmetric patterns. We define a symmetric pattern as a pattern in which the same pair of terms (C words) is likely to appear in both left-to-right and right-toleft orders. In order to identify symmetric patterns, for each pattern we define a pattern graph G(P ), as proposed by (Widdows and Dorow, 2002). If term pair (C1 , C2 ) appears in pattern P in some context, 3 This paper does not use any punctuation since the parser is provided with sentences having all non-alphabetic characters removed. We assume word separation. C1,2 can be a word or a multiword expression. we add nodes c1 , c2 to the graph and a directed edge EP (c1 , c2 ) between them. In order to select symmetric patterns, we create such a pattern graph for every discovered pattern, and create a symmetric subgraph SymG(P) in which we take only bidirectional edges from G(P ). Then we compute three measures for each pattern candida"
W09-1108,C96-1003,0,\N,Missing
W09-1108,W02-0908,0,\N,Missing
W09-1108,J98-1004,0,\N,Missing
W09-1108,P08-1003,0,\N,Missing
W09-1108,C98-2122,0,\N,Missing
W09-1120,P08-1087,0,0.0373849,"Missing"
W09-1120,P06-1109,0,0.161926,"quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is ver"
W09-1120,W06-2912,0,0.276366,"quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is ver"
W09-1120,E03-1009,0,0.635014,"ides a quality score for every sentence in a parsed sentences set. An NLP application can then decide if to use a parse or not, according to its own definition of a high quality parse. For example, it can select every sentence whose score is above some threshold, or the k top scored sentences. The selection strategy is application dependent and is beyond the scope of this paper. The unsupervised parser we use is the Seginer (2007) incremental parser2 , which achieves state-ofthe-art results without using manually created POS tags. The POS tags we use are induced by the unsupervised tagger of (Clark, 2003)3 . Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward. Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality (Reichart and Rappoport, 2007). While the algorithms of Yates et al. (2006), Kawahara and Uchimoto (2008) and Ravi et al. (2008) are supervised (Se"
W09-1120,N04-4028,0,0.0137071,"highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ 20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA 20, 15610 sentences, 108540 constiteunts), b"
W09-1120,P06-1111,0,0.0345575,"Missing"
W09-1120,I08-2097,0,0.171963,"s of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighbor"
W09-1120,P02-1017,0,0.653127,"tagger and an unsupervised parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many"
W09-1120,P04-1061,0,0.485316,"d parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used"
W09-1120,P08-2055,0,0.0273299,"arsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ 20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA 20, 15610 sentences, 108540 constiteunts), both containing newspaper texts. The unsupervised parsers of the kind add"
W09-1120,N03-1022,0,0.0302606,"Missing"
W09-1120,D08-1093,0,0.432067,"e on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighboring constituents. Co"
W09-1120,P07-1052,1,0.766884,"ality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the POS sequences of their yield"
W09-1120,C08-1091,1,0.869411,"Missing"
W09-1120,P07-1076,0,0.0153557,"e). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ 20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA 20, 15610 s"
W09-1120,P07-1049,0,0.460773,"German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for man"
W09-1120,P08-1117,1,0.858039,"Missing"
W09-1120,P06-1072,0,0.219551,"from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representativ"
W09-1120,J07-1003,0,0.0139084,"rain the parser. In the first work, high quality parses were selected using an ensemble method, while in the second a binary classifier was used (see above). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences o"
W09-1120,P01-1067,0,0.153538,"Missing"
W09-1120,W06-1604,0,0.219236,"ssessment of high quality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the"
W09-1120,C08-1015,0,\N,Missing
W09-1120,N03-1004,0,\N,Missing
W09-1120,J08-2005,0,\N,Missing
W09-1120,J03-4003,0,\N,Missing
W09-1120,D07-1111,0,\N,Missing
W09-1120,P08-2026,0,\N,Missing
W09-1120,C96-2102,0,\N,Missing
W09-1121,P98-1012,0,0.113655,"e range is (0, 1] and gives higher scores to clusterings that are preferable. As noted by (Rosenberg and Hirschberg, 2007), the Q measure does not explicitly address the completeness of the suggested clustering. Due to the cost term, if two clusterings have the same H(C|K) value, the model prefers the one with the lower number of clusters, but the trade-off between homogeneity and completeness is not explicitly addressed. In the next section we describe the V and VI measures, which are IT measures that explicitly assess both the homogeneity and completeness of the clustering solution. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. It does not explicitly use IT concepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Rin"
W09-1121,P08-2012,0,0.0258213,"completeness is not explicitly addressed. In the next section we describe the V and VI measures, which are IT measures that explicitly assess both the homogeneity and completeness of the clustering solution. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. It does not explicitly use IT concepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Ringger, 2008), using clustering F-score, the adjusted Rand index, V, VI and Q2 for document clustering; and (Reichart and Rappoport, 2008), using greedy 1-to1 and many-to-1 mappings for evaluating labeled parse tree induction. Schulte im Walde (2003) used clustering to induce semantic verb classes and extensively discussed non-IT based clustering evaluation measures. Pfitzner et al."
W09-1121,P07-1094,0,0.0810787,"emonstrate the superiority of NVI in a large experiment involving an important NLP application, grammar induction, using real corpus data in English, German and Chinese. 1 Introduction Clustering is a major technique in machine learning and its application areas. It lies at the heart of unsupervised learning, which has great potential advantages over supervised learning. This is especially true for NLP, due to the high efforts and costs incurred by the human annotations required for training supervised algorithms. Recent NLP problems addressed by clustering include POS induction (Clark, 2003; Goldwater and Griffiths, 2007), word sense disambiguation (Shin and Choi, 2004), semantic role labeling (Baldewein et al., 2004), pitch accent type disambiguation (Levow, 2006) and grammar induction (Klein, 2005). Evaluation of clustering results is a challenging task. In this paper we address the external measures setting, where a correct assignment of elements to classes is available and is used for evaluating the quality of another assignment of the elements into clusters. Many NLP works have used external clustering evaluation measures (see Section 2). Recently, two measures have been proposed that avoid many of the we"
W09-1121,C08-1042,0,0.0722077,"measures that explicitly assess both the homogeneity and completeness of the clustering solution. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. It does not explicitly use IT concepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Ringger, 2008), using clustering F-score, the adjusted Rand index, V, VI and Q2 for document clustering; and (Reichart and Rappoport, 2008), using greedy 1-to1 and many-to-1 mappings for evaluating labeled parse tree induction. Schulte im Walde (2003) used clustering to induce semantic verb classes and extensively discussed non-IT based clustering evaluation measures. Pfitzner et al. (2008) presented a comparison of clustering evaluation measures (IT based and others). While their analysis"
W09-1121,N06-1029,0,0.0220773,"nese. 1 Introduction Clustering is a major technique in machine learning and its application areas. It lies at the heart of unsupervised learning, which has great potential advantages over supervised learning. This is especially true for NLP, due to the high efforts and costs incurred by the human annotations required for training supervised algorithms. Recent NLP problems addressed by clustering include POS induction (Clark, 2003; Goldwater and Griffiths, 2007), word sense disambiguation (Shin and Choi, 2004), semantic role labeling (Baldewein et al., 2004), pitch accent type disambiguation (Levow, 2006) and grammar induction (Klein, 2005). Evaluation of clustering results is a challenging task. In this paper we address the external measures setting, where a correct assignment of elements to classes is available and is used for evaluating the quality of another assignment of the elements into clusters. Many NLP works have used external clustering evaluation measures (see Section 2). Recently, two measures have been proposed that avoid many of the weaknesses of previous measures and exhibit several attractive properties (see Sections 2 and 3): the VI measure (Meila, 2007) and the V measure (Ro"
W09-1121,C08-1091,1,0.889755,"oncepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Ringger, 2008), using clustering F-score, the adjusted Rand index, V, VI and Q2 for document clustering; and (Reichart and Rappoport, 2008), using greedy 1-to1 and many-to-1 mappings for evaluating labeled parse tree induction. Schulte im Walde (2003) used clustering to induce semantic verb classes and extensively discussed non-IT based clustering evaluation measures. Pfitzner et al. (2008) presented a comparison of clustering evaluation measures (IT based and others). While their analysis is extensive, their experiments were confined to artificial data. In this work, we experiment with a complex NLP application using large real datasets. 3 dom variables. When assuming the uniform distribution, the probability of an event (a clas"
W09-1121,D07-1043,0,0.234476,"6) and grammar induction (Klein, 2005). Evaluation of clustering results is a challenging task. In this paper we address the external measures setting, where a correct assignment of elements to classes is available and is used for evaluating the quality of another assignment of the elements into clusters. Many NLP works have used external clustering evaluation measures (see Section 2). Recently, two measures have been proposed that avoid many of the weaknesses of previous measures and exhibit several attractive properties (see Sections 2 and 3): the VI measure (Meila, 2007) and the V measure (Rosenberg and Hirschberg, 2007). However, each of these has a serious drawback. The possible values of VI lie in [0, 2log N ], where N is the size of the clustered dataset. Hence it has limited use when comparing performance on different datasets. V measure values lie in [0, 1] regardless of the dataset, but the measure strongly favors a clustering having many small clusters. In addition, V does not have many of the attractive properties of VI. This paper has two contributions. First, we propose the NVI measure, a normalization of VI which guarantees that the score of clusterings that VI considers good lies in [0,1], regard"
W09-1121,P07-1049,0,0.0845474,"Missing"
W09-1121,C02-1145,0,0.038109,"Missing"
W09-1121,W04-0817,0,\N,Missing
W09-1121,E03-1009,0,\N,Missing
W09-1121,J06-2001,0,\N,Missing
W09-1121,C98-1012,0,\N,Missing
W10-2909,P06-3002,0,0.0656106,"ddressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrast"
W10-2909,W95-0101,0,0.0949316,", 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3 The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. 61 In addition to comparing the different algorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm a"
W10-2909,J92-4003,0,0.440785,"Missing"
W10-2909,E03-1009,0,0.761571,"o local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results. 1 arir@cs.huji.ac.il Introduction Unsupervised part-of-speech (POS) induction is of major theoretical and practical importance. It counters the arbitrary nature of manually designed tag sets,"
W10-2909,D07-1023,0,0.155129,"Missing"
W10-2909,P08-1085,0,0.142883,"d algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words i"
W10-2909,P07-1094,0,0.646832,"drastically, exhibit better correlation between perplexity and external clustering quality3 . Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types cluster"
W10-2909,P05-1044,0,0.494732,"2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) with a detailed demonstration in (Johnson, 2007). All these authors (except Smith and Eisner (2005), see below), however, reported average results over several runs and did not try to identify the runs that produce high quality tagging. Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. In this p"
W10-2909,D07-1031,0,0.78092,"for which the entropy-based filter improves perplexity more drastically, exhibit better correlation between perplexity and external clustering quality3 . Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used dis"
W10-2909,N09-1010,0,0.0478669,"Missing"
W10-2909,D08-1109,0,0.0436534,"2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3 The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. 61 In addition to comparing the different algorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm and our ZCC algorithm. Clustering Quality Evaluation. The induced POS tags have arbitrary names. To evaluate them against a manually annotated corpus, a proper correspondence"
W10-2909,J06-4002,0,0.0639965,"Missing"
W10-2909,D09-1072,0,0.496511,"of 0.9) remains relatively constant. Figure 2 (right) shows that models for which the entropy-based filter improves perplexity more drastically, exhibit better correlation between perplexity and external clustering quality3 . Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and"
W10-2909,J94-2001,0,0.490967,"ood or data probability for this tagger) is evaluated on the test set. Moreover, we show that our algorithm outperforms existing POS taggers for most evaluation measures (Table 3). Identifying good solutions among many runs of a randomly-initialized algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et a"
W10-2909,P09-1057,0,0.287856,"supervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyde"
W10-2909,W09-1121,1,0.93754,"gorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm and our ZCC algorithm. Clustering Quality Evaluation. The induced POS tags have arbitrary names. To evaluate them against a manually annotated corpus, a proper correspondence with the gold standard POS tags should be established. Many evaluation measures for unsupervised clustering against gold standard exist. Here we use measures from two well accepted families: mapping based and information theoretic (IT) based. For a recent discussion on this subject see (Reichart and Rappoport, 2009). The mapping based measures are accuracy with greedy many-to-1 (M-1) and with greedy 1-to-1 (1-1) mappings of the induced to the gold labels. In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster. After each induced label is mapped to a gold label, tagging accuracy is computed. Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus. The IT based measures we use are V (Rosenberg and Hirschber"
W10-2909,D07-1043,0,0.077745,"rt and Rappoport, 2009). The mapping based measures are accuracy with greedy many-to-1 (M-1) and with greedy 1-to-1 (1-1) mappings of the induced to the gold labels. In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster. After each induced label is mapped to a gold label, tagging accuracy is computed. Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus. The IT based measures we use are V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009). The latter is a normalization of the VI measure (Meila, 2007). VI and NVI induce the same order over clusterings but NVI values for good clusterings lie in [0, 1]. For V, the higher the score, the better the clustering. For NVI lower scores imply improved clustering quality. We use e as the base of the logarithm. Evaluation of the Quality Test. To measure the correlation between the score produced by the tagging quality test and the external quality of a tagging, we use two well accepted measures: Spearman’s rank correlation coefficient and Kendall Tau"
W10-2909,E95-1020,0,0.466767,"is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and"
W10-2909,P04-1062,0,0.0284705,"ed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. Moreover, we show that our algorithm outperforms existing POS taggers for most evaluation measures (Table 3). Identifying good solutions among many runs of a randomly-initialized algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag,"
W10-2909,C04-1052,0,\N,Missing
W10-2909,D09-1071,0,\N,Missing
W10-2909,N06-1041,0,\N,Missing
W10-2909,C04-1080,0,\N,Missing
W10-2909,D08-1036,0,\N,Missing
W10-2911,E03-1009,0,0.841377,"great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level e"
W10-2911,D07-1023,0,0.0965229,"e two average variants, micro and macro. Macro average computes the total number of matches over all words and normalizes in the end. Recall (R), Precision (P) and their harmonic average (Fscore) are accordingly defined: R= IM Pl i=1 |Ai | P = M acroI = In the example in Section 3 showing an unreasonable behavior of IT-based measures, the score depends on r for both MacroI and MicroI. With our new measures, recall is always 1, but precision is nr . This is true both for 1-1 and M-1 mappings. Hence, the new measures show reasonable behavior in this example for all r values. MicroI was used in (Dasgupta and Ng, 2007) with a manually compiled mapping. Their mapping was not based on a well-defined scheme but on a heuristic. Moreover, providing a manual mapping might be impractical when the number of clusters is large, and can be inaccurate, especially when the clustering is not of very high quality. In the following we discuss how to compute the 1-1 and M-1 greedy mappings for each measure. IM Pl i=1 |h(Bi )| 2RP = R+P l 1-1 Mapping. We compute h by finding the maximal weighted matching in a bipartite graph. In this graph one side represents the induced clusters, the other represents the gold classes and th"
W10-2911,P06-1038,1,0.769359,"tudy, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algorithm that will be invoked on a diverse set of cor"
W10-2911,P08-1079,1,0.802772,"iment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algorithm that will be invoked on a diverse set of corpora having different token st"
W10-2911,N09-1019,0,0.0203112,"against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute merits of different a"
W10-2911,D08-1036,0,0.454036,"tion, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute"
W10-2911,P06-1144,0,0.0349584,"Missing"
W10-2911,P07-1094,0,0.337074,"s on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on t"
W10-2911,W06-1633,0,0.0304537,"ance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuabl"
W10-2911,C08-1042,0,0.109725,"Missing"
W10-2911,C08-1091,1,0.872091,"polysemous, the common case in NLP. We demonstrate the benefits of our measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different"
W10-2911,W09-1121,1,0.936781,"n Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among H(C) = − P|C| H(C|K) = − c=1 P|K| I k=1 ck N P|K |P|C| c=1 k=1 log Ick N P|K| I k=1 ck N I log P|C|ck c=1 Ick H(K) and H(K|C) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: h= ( c= ( 1 1− H(C|K) H(C) H(C) = 0 H(C) 6= 0 1 1− H(K|C) H(K) H(K) = 0 H(K) 6= 0 V = 79 2hc h+c N V I(C, K) = ( H(C|K)+H(K|C) H(C) H(K) show below, these measures do not suffer from the problems discussed for IT measures in Section 3. All measures are mapping-based: first, a mapping between the induced and gold clusters is performed, and then a measure E is computed. As is common in the"
W10-2911,D07-1043,0,0.0829272,"sion and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among H(C) = − P|C| H(C|K) = − c=1 P|K| I k=1 ck N P|K |P|C| c=1 k=1 log Ick N P|K| I k=1 ck N I log P|C|ck c=1 Ick H(K) and H(K|C) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: h= ( c= ( 1 1− H(C|K) H(C) H(C) = 0 H(C) 6= 0 1 1− H(K|C) H(K) H(K) = 0 H(K) 6= 0 V = 79 2hc h+c N V I(C, K) = ( H(C|K)+H(K|C) H(C) H(K) show below, these measures do not suffer from the problems discussed for IT measures in Section 3. All measures are mapping-based: first, a mapping between the induced and gold clusters is performed, and then a meas"
W10-2911,J06-2001,0,0.335372,"ur measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algo"
W10-2911,P04-1062,0,0.0714122,"Missing"
W10-2911,J93-2004,0,\N,Missing
W10-2911,D09-1071,0,\N,Missing
W10-2911,P10-1132,1,\N,Missing
W14-1603,N09-1067,0,0.307579,"Missing"
W14-1603,W13-1706,0,0.207157,"Missing"
W14-1603,de-marneffe-etal-2006-generating,0,0.0254328,"Missing"
W14-1603,N03-1033,0,0.0042805,"Language Similarities from ESL Our feature set, summarized in table 2, contains features which are strongly related to many of the structural features in WALS. In particular, we use features derived from labeled dependency parses. These features encode properties such as the types of dependency relations, ordering and distance between the head and the dependent. Additional syntactic information is obtained using POS ngrams. Finally, we consider derivational and inflectional morphological affixation. The annotations required for our syntactic features are obtained from the Stanford POS tagger (Toutanova et al., 2003) and the Stanford parser (de Marneffe et al., 2006). The morphological features are extracted heuristically. Our first goal is to derive a notion of similarity between languages with respect to their native speakers’ distinctive structural usage patterns of ESL. A simple way to obtain such similarities is to train a probabilistic NLI model on ESL texts, and interpret the uncertainty of this classifier in distinguishing between a pair of native languages as a measure of their similarity. 4.1 NLI Model The log-linear NLI model is defined as follows: exp(θ · f (x, y)) 0 y 0 ∈Y exp(θ · f (x, y ))"
W14-1603,C10-1044,0,0.32744,"ering of languages and typological feature prediction. Most notably, Teh et al. (2007) and subsequently Daum´e III (2009) Related Work Our work integrates two areas of research, crosslinguistic transfer and linguistic typology. 2.1 Language Typology Cross-linguistic Transfer The study of cross-linguistic transfer has thus far evolved in two complementary strands, the linguistic comparative approach, and the computational detection based approach. While the com1 22 http://wals.info/ predicted typological features from language trees constructed with a Bayesian hierarchical clustering model. In Georgi et al. (2010) additional clustering approaches were compared using the same features and evaluation method. In addition to the feature prediction task, these studies also evaluated their clustering results by comparing them to genetic language clusters. Our approach differs from this line of work in several aspects. First, similarly to our WALS based baselines, the clustering methods presented in these studies are affected by the sparsity of available typological data. Furthermore, these methods rely on existing typological documentation for the target languages. Both issues are obviated in our English bas"
W14-1603,P11-1019,0,0.0641894,"features, where k is the number of values the original feature can take. Note that beyond the well known issues with feature binarization, this strategy is not optimal for some of the features. For example, the feature 111A Non-periphrastic Causative Constructions whose possible values are presented in table 1 would have been better encoded with two binary features rather than four. The question of optimal encoding for the WALS feature set requires expert analysis and will be addressed in future research. Datasets Cambridge FCE We use the Cambridge First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) as our source of ESL data. This corpus is a subset of the Cambridge Learner Corpus (CLC)2 . It contains English essays written by upper-intermediate level learners of English for the FCE examination. The essay authors represent 16 native languages. We discarded Dutch and Swedish speakers due to the small number of documents available for these languages (16 documents in total). The remaining documents are associated with the following 14 native languages: Catalan, Chinese, French, German, Greek, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Thai and Turkish. Overall, our co"
W14-1603,P12-1066,0,0.275566,"vice versa. In particular, it paves the way for a novel and powerful framework for comparing native languages through second language performance. This framework overcomes many of the inherent difficulties of direct comparison between languages, and the lack of sufficient typological documentation for the vast majority of the world’s languages. Further on, we utilize this transfer enabled framework for the task of reconstructing typological features. Automated prediction of language typology is extremely valuable for both linguistic studies and NLP applications which rely on such information (Naseem et al., 2012; T¨ackstr¨om et al., 2013). Furthermore, this task provides an objective external testbed for the quality of our native Linguists and psychologists have long been studying cross-linguistic transfer, the influence of native language properties on linguistic performance in a foreign language. In this work we provide empirical evidence for this process in the form of a strong correlation between language similarities derived from structural features in English as Second Language (ESL) texts and equivalent similarities obtained from the typological features of the native languages. We leverage th"
W14-1603,N13-1126,0,0.090093,"Missing"
