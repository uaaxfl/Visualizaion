1994.amta-1.10,H93-1038,1,0.792898,"Missing"
1994.amta-1.10,1993.tmi-1.4,1,0.733597,"Missing"
1995.tmi-1.17,A94-1016,1,0.686167,"Missing"
1996.amta-1.35,C96-1030,1,0.87856,"Missing"
1996.amta-1.35,1995.tmi-1.17,1,0.904059,"Missing"
1996.amta-1.35,A94-1016,1,0.893947,"Missing"
1996.amta-1.35,H94-1005,0,0.213367,"Missing"
1997.mtsummit-systems.9,C96-1030,1,0.80687,"2. MT Technology used in DIPLOMAT The MT component of DIPLOMAT is the Pangloss-Lite (PanLite) system [3]. PanLite is a standalone C++ re-implementation of several major components from the Pangloss machine translation system [7]. Pangloss was a joint project between three sites: the Computing Research Laboratory of New Mexico State University, the Information Sciences Institute of the University of Southern California, and the Center for Machine Translation of Carnegie Mellon University. It was funded by the U.S. Department of Defense. PanLite incorporates the Pangloss Example-Based MT (EBMT) [1] and Transfer-Based MT engines, and its statistical language modeller [2], as well as the newly-implemented iCelos morphological analyzer, within the Multi-Engine MT architecture [4] developed during the course of the Pangloss project (described further below). Due to improved design and the C++ implementation, PanLite runs very quickly. For example, the EBMT engine formerly required several minutes to translate a typical newswire sentence; it now requires under 10 seconds (and this with a much larger corpus). 261 To allow its use in the widest variety of applications, PanLite has been designe"
1997.mtsummit-systems.9,1995.tmi-1.17,1,0.835188,"gloss-Lite (PanLite) system [3]. PanLite is a standalone C++ re-implementation of several major components from the Pangloss machine translation system [7]. Pangloss was a joint project between three sites: the Computing Research Laboratory of New Mexico State University, the Information Sciences Institute of the University of Southern California, and the Center for Machine Translation of Carnegie Mellon University. It was funded by the U.S. Department of Defense. PanLite incorporates the Pangloss Example-Based MT (EBMT) [1] and Transfer-Based MT engines, and its statistical language modeller [2], as well as the newly-implemented iCelos morphological analyzer, within the Multi-Engine MT architecture [4] developed during the course of the Pangloss project (described further below). Due to improved design and the C++ implementation, PanLite runs very quickly. For example, the EBMT engine formerly required several minutes to translate a typical newswire sentence; it now requires under 10 seconds (and this with a much larger corpus). 261 To allow its use in the widest variety of applications, PanLite has been designed to translate strings provided either on the standard input or via netwo"
1997.mtsummit-systems.9,1996.amta-1.35,1,0.281065,"tian-Creole/English and Korean/English systems. The full DIPLOMAT system also involves research in speech understanding and synthesis, as well as wearable computer systems. The speech understanding system used is the well-known SPHINX II HMM-based continuous speech recognition system [5,8]; the speech synthesis system is a newly-developed subword concatenative system [6]. The user interface uses the UNICODE multilingual character encoding standard to facilitate rapid addition of new languages. 2. MT Technology used in DIPLOMAT The MT component of DIPLOMAT is the Pangloss-Lite (PanLite) system [3]. PanLite is a standalone C++ re-implementation of several major components from the Pangloss machine translation system [7]. Pangloss was a joint project between three sites: the Computing Research Laboratory of New Mexico State University, the Information Sciences Institute of the University of Southern California, and the Center for Machine Translation of Carnegie Mellon University. It was funded by the U.S. Department of Defense. PanLite incorporates the Pangloss Example-Based MT (EBMT) [1] and Transfer-Based MT engines, and its statistical language modeller [2], as well as the newly-imple"
1997.mtsummit-systems.9,A94-1016,1,0.807649,"om the Pangloss machine translation system [7]. Pangloss was a joint project between three sites: the Computing Research Laboratory of New Mexico State University, the Information Sciences Institute of the University of Southern California, and the Center for Machine Translation of Carnegie Mellon University. It was funded by the U.S. Department of Defense. PanLite incorporates the Pangloss Example-Based MT (EBMT) [1] and Transfer-Based MT engines, and its statistical language modeller [2], as well as the newly-implemented iCelos morphological analyzer, within the Multi-Engine MT architecture [4] developed during the course of the Pangloss project (described further below). Due to improved design and the C++ implementation, PanLite runs very quickly. For example, the EBMT engine formerly required several minutes to translate a typical newswire sentence; it now requires under 10 seconds (and this with a much larger corpus). 261 To allow its use in the widest variety of applications, PanLite has been designed to translate strings provided either on the standard input or via network sockets, and to produce as output either the best composite string or the full chart of scored alternative"
1999.mtsummit-1.82,C96-1030,0,0.0992647,"f outputs (Brown and Frederking 1995). These selection techniques attempt to produce the best overall result, taking the probability of transitions between segments into account as well as modifying the quality scores of individual segments. A recent evaluation (Hogan and Frederking 1998) has demonstrated that the MEMT architecture can indeed produce better translations than any single component MT engine. MT Summit VII Sept. 1999 Figure 1: Multi-Engine MT Architecture For the languages developed so far, the primary engines that we have produced have been EBMT and lexical-transfer MT: • EBMT (Brown 1996) uses a sentence-aligned corpus to produce translations. When such a corpus is available, fairly good quality MT for a new domain is available essentially immediately. EBMT is basically a more sophisticated version of Translation Memory, in that sub-sentential chunks of words are matched, allowing much greater coverage. Sentences that match in full are translated exactly, but sub-sentential chunks are matched with a variety of heuristics, which are reflected in the quality scores assigned to the corresponding outputs. • Lexical-transfer MT employs a very simple, very old technology: bilingual"
1999.mtsummit-1.82,1995.tmi-1.17,1,0.700542,"ed chart data structure (Kay 1967, Winograd 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modeling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking 1995). These selection techniques attempt to produce the best overall result, taking the probability of transitions between segments into account as well as modifying the quality scores of individual segments. A recent evaluation (Hogan and Frederking 1998) has demonstrated that the MEMT architecture can indeed produce better translations than any single component MT engine. MT Summit VII Sept. 1999 Figure 1: Multi-Engine MT Architecture For the languages developed so far, the primary engines that we have produced have been EBMT and lexical-transfer MT: • EBMT (Brown 1996) uses a sentence-aligned c"
1999.mtsummit-1.82,hogan-frederking-1998-evaluation,1,0.806242,"Missing"
1999.mtsummit-1.82,H93-1038,1,0.888934,"Missing"
1999.mtsummit-1.82,A94-1016,1,0.839096,"ment and broad coverage require humanintervention, but this is readily available, since there is always a trained user present, and we assume that both speakers are trying to cooperate in communicating. This system therefore provides the type of generalpurpose, human-assisted MT that we need for the Translating Telephone. We will briefly describe the most pertinent aspects of the current system here; more details are available elsewhere (Frederking et al. 1997, Frederking et al. 1999). 2.1 Multi-Engine Machine Translation Diplomat uses the Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg 1994). As shown in Figure 1 below, MEMT feeds an input text to several MT engines in parallel, with each engine employing a different MT technology. Morphological analysis, part-of-speech tagging, and possibly other text enhancements can be shared by the engines. Each engine attempts to translate the entire input text, segmenting each sentence in whatever manner is most appropriate for its technology, and putting the resulting translated output segments into a shared chart data structure (Kay 1967, Winograd 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the q"
1999.mtsummit-1.82,C67-1009,0,0.458794,"nslation Diplomat uses the Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg 1994). As shown in Figure 1 below, MEMT feeds an input text to several MT engines in parallel, with each engine employing a different MT technology. Morphological analysis, part-of-speech tagging, and possibly other text enhancements can be shared by the engines. Each engine attempts to translate the entire input text, segmenting each sentence in whatever manner is most appropriate for its technology, and putting the resulting translated output segments into a shared chart data structure (Kay 1967, Winograd 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modeling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking 1995). These"
1999.mtsummit-1.82,W97-0409,1,0.774769,"face. We describe our initial work, which is an extension of the Diplomat wearable speech translator. 1 Introduction The Translating Telephone has been the goal of several major speech-to-speech translation projects, such as those at ATR (Moritomi et al. 1993) in Japan, the JANUS group at Carnegie Mellon University (Waibel et al. 1991, Woszczyna et al. 1994), and others in the C-STAR consortium. While there have been numerous significant accomplishments in these projects over a number of years, the ultimate goal of a useful Translating Telephone still seems remote. We in the Diplomat project (Frederking et al. 1997, Frederking et al. 1999) have therefore decided to attack the problem from a new direction, to determine whether more rapid progress can be made. Instead of gradually extending a limited-domain, fully-automatic system, we will begin with a broadcoverage, human-aided speech-to-speech translation system, and attempt to move towards full automation. The rest of Section 1 further describes our strategic view of machine translation research, and our previous work in this particular strategic direction. Section 2 presents the current state of the Diplomat system, which provides the foundation for t"
1999.mtsummit-1.82,P07-2007,0,0.0480748,"Missing"
2001.mtsummit-papers.69,C96-1030,1,0.886417,"Missing"
2001.mtsummit-papers.69,hogan-frederking-1998-evaluation,1,\N,Missing
2001.mtsummit-papers.69,C00-1019,1,\N,Missing
2001.mtsummit-papers.69,H01-1002,1,\N,Missing
2005.mtsummit-posters.10,W04-0107,1,0.823896,"igure 1: The elicitation tool is used by the informant for translation and alignment. Sentences are presented individually and can be annotated with context information when necessary. for quick design and implementation of elicitation corpora. Furthermore, we will look at a use of these methods to create a specific kind of corpus called a typological-functional corpus. This type of corpus is designed to elicit a range of language features (for example, tense, person, number) and explore the way those features are manifested in a target language. 2 and morphological paradigms is described in (Monson et al. 2004). Rule refinement via interaction with a consultant is described in (Font-Llitjos et al. 2005). At run time, the AVENUE system consists of a transfer engine and a decoder. The transfer engine encompases analysis, transfer, and generation and produces a large lattice of possible translations. The decoder uses statistical techniques to zero in on the best scoring hypothesis. (Lavie et al. 2003) The AVENUE Project 3 The work described in this paper takes place in the context of the AVENUE machine translation project1. AVENUE is focused on the development of machine translation systems for low-res"
2005.mtsummit-posters.10,2001.mtsummit-road.7,1,0.88585,"Missing"
2005.mtsummit-posters.10,2005.eamt-1.13,0,0.0327725,"Missing"
2007.tmi-papers.1,N06-2002,1,0.909934,"of data if the data is highly structured (Lavie et al. 2003). The elicitation corpus is therefore designed to produce highly structured data. Each sentence is designed to elicit a specific morphosyntactic property of the language, and sentences are organized into minimal pairs (e.g., A tree is falling and A tree fell) to compare the effects of changing one grammatical feature at a time. Probst (2005) describes automatic rule learning from elicited data. A small sample of elicitation sentences is included in the list below. A more detailed description of the elicitation corpus can be found in Alvarez et al, (2006). • • • • • • • • • Mary is writing a book for John. Who let him eat the sandwich? Who had the machine crush the car? They did not make the policeman run. Our brothers did not destroy files. He said that there is not a manual. The teacher who wrote a textbook left. The policeman chased the man who was a thief. Mary began to work. Each sentence in the elicitation corpus is associated with a set of featurevalue pairs, which represent the meaning elements that may be reflected in the 2 morphosyntax of the language. Figure 1 shows an example of an elicitation sentence and its feature structure. As"
2007.tmi-papers.1,I05-2035,0,0.0264337,"variety of reasons, it was not practical to train our statistical transfer system on this data. We therefore assessed the impact of these elicitation errors by training two EBMT systems on our Thai data. One trained on our original unsupervised corpus and the other trained on a corpus corrected of elicitation errors. This evaluation is described in section 6. 3 Related Work Two other projects that we know of formulate grammars based on elicited data. In addition to the Boas system mentioned above, which attempts to train naïve informants to provide linguistic information, the Grammar Matrix (Bender and Flickinger, 2005) collects facts like the existence of subject-verb agreement from a field worker and then automatically produces an HPSG grammar for the language. Both of these use knowledge that a trained human has put into technical linguistic form. In contrast, our approach analyzes translations of elicitation corpus sentences, and the underlying feature structures they represent, to derive the linguistic facts about the language automatically. 3 The Corpus and Support Materials Our elicitation corpus is a monolingual corpus of 3124 English sentences. We designed it to be translated into any human language"
2007.tmi-papers.1,C96-1030,0,0.0338486,"ill conclude by discussing the implications of using linguistically naïve consultants as a resource for building MT systems. 2 Background The AVENUE project has two related foci: building MT systems in lowresource scenarios, and making robust, hybrid MT systems using combinations of deep linguistic knowledge and statistical techniques. The hybrid system is a statistical transfer system (Lavie et al. 2004), which makes use of transfer rules as well as a statistical decoder. The rules can be written by hand, or learned automatically (Probst 2005). The AVENUE system also includes an EBMT system (Brown 1996), in order to use any pre-existing parallel texts that do happen to be available. One hypothesis of the AVENUE work for low-resource scenarios is that MT systems can be learned from small amounts of data if the data is highly structured (Lavie et al. 2003). The elicitation corpus is therefore designed to produce highly structured data. Each sentence is designed to elicit a specific morphosyntactic property of the language, and sentences are organized into minimal pairs (e.g., A tree is falling and A tree fell) to compare the effects of changing one grammatical feature at a time. Probst (2005)"
A94-1016,J90-2002,0,0.0615918,"Missing"
A94-1016,H93-1038,1,0.74872,"has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual M T engines will be reported separately and are not, therefore, described in detail here. 2 INTEGRATING MULTI-ENGINE OUTPUT In our experiment we used three MT engines: * a knowledge-based MT (KBMT) system, the mainline Pangloss engine (Frederking et al., 1993b); • an example-based MT (EBMT) system (see (Nirenburg et al., 1993; Nirenburg et al., 1994b); the original idea is due to Nagao (Nagao, 1984)); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - - a 95 machine-readable dictionary (the Collins Spanish/English), the lexicons used by the K B M T modules, a large set of user-generated bilingual glossaries as well as a gazetteer and a list of proper and organization names. on average thirteen years o[ flight (time). This is a sentence from one of the 1993 ARPA MT eva"
A94-1016,E93-1062,1,0.743153,"has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual M T engines will be reported separately and are not, therefore, described in detail here. 2 INTEGRATING MULTI-ENGINE OUTPUT In our experiment we used three MT engines: * a knowledge-based MT (KBMT) system, the mainline Pangloss engine (Frederking et al., 1993b); • an example-based MT (EBMT) system (see (Nirenburg et al., 1993; Nirenburg et al., 1994b); the original idea is due to Nagao (Nagao, 1984)); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - - a 95 machine-readable dictionary (the Collins Spanish/English), the lexicons used by the K B M T modules, a large set of user-generated bilingual glossaries as well as a gazetteer and a list of proper and organization names. on average thirteen years o[ flight (time). This is a sentence from one of the 1993 ARPA MT eva"
A94-1016,1993.tmi-1.4,1,0.751309,"Missing"
C00-2154,1995.tmi-1.17,1,0.74166,"hich have no locality requirements, and may therefore be distributed across machines and networks as necessary. The User and Editor Clients were described in xx2.1 and 2.2. We will now examine the most important processing mechanisms, including machine translation and speech recognition/synthesis. 3.2 Machine Translation For Machine Translation, we rely on the Panlite Multi-Engine Machine Translation (MEMT) Server (Frederking and Brown, 1996). This system, which is outlined in Figure 4, makes use of several translation engines at once, combining their output with a statistical language model (Brown and Frederking, 1995). Each translation engine makes use of a di erent translation technology, and produces multiple, possibly overlapping, translations for every part of the input that it can translate. All of the translations produced by the various engines are placed in a chart data structure (Kay, 1967; Winograd, 1983), indexed by their position in the input utterance. A statistical language model is used, together with scores provided by the translation engines, to determine the optimal path through the set of translated segments, which information is also stored in the chart. Upon completion of translation,"
C00-2154,C96-1030,0,0.0556763,"tions on JavaTM Applets. nizer, and therefore consumes a greater amount of resources than the other servers. Most processing is intended to be performed by clients, which have no locality requirements, and may therefore be distributed across machines and networks as necessary. The User and Editor Clients were described in xx2.1 and 2.2. We will now examine the most important processing mechanisms, including machine translation and speech recognition/synthesis. 3.2 Machine Translation For Machine Translation, we rely on the Panlite Multi-Engine Machine Translation (MEMT) Server (Frederking and Brown, 1996). This system, which is outlined in Figure 4, makes use of several translation engines at once, combining their output with a statistical language model (Brown and Frederking, 1995). Each translation engine makes use of a di erent translation technology, and produces multiple, possibly overlapping, translations for every part of the input that it can translate. All of the translations produced by the various engines are placed in a chart data structure (Kay, 1967; Winograd, 1983), indexed by their position in the input utterance. A statistical language model is used, together with scores provi"
C00-2154,1996.amta-1.35,1,0.746389,"ecurity restrictions on JavaTM Applets. nizer, and therefore consumes a greater amount of resources than the other servers. Most processing is intended to be performed by clients, which have no locality requirements, and may therefore be distributed across machines and networks as necessary. The User and Editor Clients were described in xx2.1 and 2.2. We will now examine the most important processing mechanisms, including machine translation and speech recognition/synthesis. 3.2 Machine Translation For Machine Translation, we rely on the Panlite Multi-Engine Machine Translation (MEMT) Server (Frederking and Brown, 1996). This system, which is outlined in Figure 4, makes use of several translation engines at once, combining their output with a statistical language model (Brown and Frederking, 1995). Each translation engine makes use of a di erent translation technology, and produces multiple, possibly overlapping, translations for every part of the input that it can translate. All of the translations produced by the various engines are placed in a chart data structure (Kay, 1967; Winograd, 1983), indexed by their position in the input utterance. A statistical language model is used, together with scores provi"
C00-2154,1999.mtsummit-1.82,1,0.840084,"Missing"
C00-2154,C67-1009,0,0.492813,"achine Translation For Machine Translation, we rely on the Panlite Multi-Engine Machine Translation (MEMT) Server (Frederking and Brown, 1996). This system, which is outlined in Figure 4, makes use of several translation engines at once, combining their output with a statistical language model (Brown and Frederking, 1995). Each translation engine makes use of a di erent translation technology, and produces multiple, possibly overlapping, translations for every part of the input that it can translate. All of the translations produced by the various engines are placed in a chart data structure (Kay, 1967; Winograd, 1983), indexed by their position in the input utterance. A statistical language model is used, together with scores provided by the translation engines, to determine the optimal path through the set of translated segments, which information is also stored in the chart. Upon completion of translation, the chart data structure is made available for use by the rest of the WebDIPLOMAT system. Currently, we employ Lexical Transfer and ExSource Target Language Language Morphological Analyzer User Interface Transfer-Based MT Example-Based MT Statistical Modeller Knowledge-Based MT Expansi"
C00-2154,W97-0412,0,0.0293296,"that it can transport, and client software is tightly focussed on the text domain. Such limitations have not, however, prevented researchers from experimenting with the possibilities of incorporating machine translation or speech into the chat experience (Lenzo, 1998; Seligman et al., 1998). The outcome of these experiments has been to show that commercial machine translation systems may be reasonably integrated into the chat room, and that commercial speech software can be connected to existing chat software to provide the desired experience. We have taken a di erent road. It has been noted (Seligman, 1997; Frederking et al., 2000) that broadcoverage machine translation and speech recognition cannot now be useful unless users can interact with the system to improve results. While Seligman et al. (1998) were able to e ect user editing of speech recognition by editing text before submitting it for translation, they were unable to do the same for the translation system, primarily due to limitations of commercial software. Additional limitations are encountered in the communication medium: chat is not amenable to non-text interaction with translation agents, and commercial chat software does not, i"
C94-1019,H94-1019,0,0.0600941,"Missing"
C94-1019,1992.tmi-1.12,0,0.10997,"Missing"
C94-1019,J85-1002,0,0.0514431,"Missing"
C94-1019,1992.tmi-1.14,0,0.0240526,".,ricnted and do not aim ill advltnch]g our knowledge itl&apos;Joul either basic tnechaI l i S l l l S o f text coralschess;on and production O f C O I l l ] ) t l l e l + models shnHhlling stich i11e.challiSlllS. Tile lwo lllOSl recently popular techncd o~,{ical parndit, ms ill machine t r a n s l a t i o n - - - e×ample-I&apos;~ased Iranslalion (EBMT) and stalisliCSdlased transhlthm (SIIM&apos;f) - - - r e quire linguistic knowledge only :is an aflerlhollghl. While the represenlatives of the above paradigms are still al lhc stage, of e.ilher building toy systems (e.g., Furuse and litht, 1992; McLean, 1992,Jones, 1992, Maruyama and Wltlan,aim, 1992) or struggling with tile natural constraints olal&gt; proaches that eschew the Sttldy ol"" langual.,e ;is such (e.g., Brown et at., 1990), .it number of llropi`&apos;sals have come up lor some hybridization oF M&apos;I: [n some such .aplnO&apos;,tches, tJ I( Corl)llS analysis is tised (&apos;Ill"" ltlllhl]:{ analysis ;lid Ii.{uIsfcr grammars (e.t;., Su and (&apos;hang, 1992). Ill olhcrs, a standlu&apos;d tr:msfcx-I&apos;~L~ed aPl/rtmch (TBMT) is followed usiny, hadilh/nal analysis and generalhm technhlueS bul havin!,, a IranslEr component Imscd on aligned I,ilingual corp(ira ((lrishnmn and Kosnka, I"")"
C94-1019,1992.tmi-1.21,0,0.0239964,"Missing"
C94-1019,1992.tmi-1.15,0,0.0217843,"Missing"
C94-1019,1992.tmi-1.4,0,0.0189808,"Missing"
C94-1019,1992.tmi-1.22,0,0.0307214,"Missing"
C94-1019,C88-1016,0,\N,Missing
C94-1019,J90-2002,0,\N,Missing
C94-1019,1992.tmi-1.23,0,\N,Missing
clark-etal-2008-toward,carbonell-etal-2002-automatic,1,\N,Missing
clark-etal-2008-toward,probst-lavie-2004-structurally,0,\N,Missing
clark-etal-2008-toward,N06-2002,1,\N,Missing
clark-etal-2008-toward,P07-1009,0,\N,Missing
clark-etal-2008-toward,W04-2706,0,\N,Missing
E93-1062,A92-1045,1,0.792096,"Missing"
feely-etal-2014-cmu,W12-5205,0,\N,Missing
feely-etal-2014-cmu,N13-1031,0,\N,Missing
feely-etal-2014-cmu,levin-etal-2014-resources,1,\N,Missing
feely-etal-2014-cmu,seraji-etal-2012-basic,0,\N,Missing
frederking-etal-2002-field,A94-1016,1,\N,Missing
frederking-etal-2002-field,C96-1030,1,\N,Missing
H01-1002,C96-1030,1,0.936318,"Missing"
H01-1002,1997.tmi-1.13,1,0.865412,"Missing"
H01-1002,1999.tmi-1.3,1,0.855174,"Missing"
H01-1002,C00-1019,1,0.91282,"Missing"
H01-1002,H94-1005,0,0.346482,"Missing"
H01-1002,hogan-frederking-1998-evaluation,1,0.902648,"Missing"
H93-1038,1991.mtsummit-papers.13,0,0.1541,"r o d u c t i o n Fully automated machine translation of unconstrained texts is beyond the state of the art today. The need for mechanizing the translation process is, however, very urgent. It is desirable, therefore, to seek ways o f both speeding up the process of translating texts and making it less expensive. In this paper we describe an environment that facilitates the integration of automatic machine translation (MT) and machine-aided translation (MAT). station provides the user interface and the integration platform. It is similar in spirit to systems such as the Translator's Workbench[3]. The processing in PANGLOSSgoes as follows: 1. an input passage is broken into sentences; 2. a fully-automated translation o f each full sentence is attempted; if it fails, then 3. a fully-automated translation o f smaller chunks of text is attempted (currently, these are noun phrases); . the material that does not get covered by noun phrases is treated in a &quot;word-for-word&quot; mode, whereby translation suggestions for each word (or phrase) are sought in the system's MT lexicons, an online bilingual dictionary, and a set o f user-Supplied glossaries; . The resulting list o f translated noun phras"
H93-1038,A92-1045,1,0.826242,"ranslation o f smaller chunks of text is attempted (currently, these are noun phrases); . the material that does not get covered by noun phrases is treated in a &quot;word-for-word&quot; mode, whereby translation suggestions for each word (or phrase) are sought in the system's MT lexicons, an online bilingual dictionary, and a set o f user-Supplied glossaries; . The resulting list o f translated noun phrases and translation suggestions for words and phrases is displayed in a special editor window, where the human user finalizes the translation. This environment, called the Translator's Workstation (TWS)[5], has been developed in the framework of the PANGLOSS machine translation project. 1 The main goal of this project is to develop a system that will, from the very beginning, produce high-quality output. This can only be attained currently by keeping the human being in the translation loop. The main measure of progress in the development of the Pangloss system is the gradual increase in the level of automation. This entire process can be viewed as helping a human translator, by doing parts of the job automatically and making the rest less time-consuming. We have designed and implemented an inte"
H94-1026,E93-1062,1,0.856241,"e 7: Chart-walk algorithm who that whom which ""were have"" have ""have got"" ""were possess"" possess ""were hold"" hold ""hold on to"" ""hold up"" ""were grasp"" in on onto at by average middle mid-point NIL years of from about for by flight ""to dash off"" ""to clear off"" ""to leave the parental nest"" ""spread one&apos;s wings"" ""to overhear sth in passing"" ""to catch on immediately"" ""get it at once"" ""to be pretty smart"" &apos;flight feathers"" calculated as a weighted average of the scores in the row to its left, in the column below it and the previous contents of the array cell for its position. So to calculate element (1,4), we compare the combined scores of the best walks over (1,1) and (2,4), (1,2) and (3,4), and (1,3) and (4,4) with the scores of any chart edges going from 1 to 4, and take the maximum. When the score in the top-right comer is produced, the algorithm is finished, and the associated set of edges is the final chart-walk result. It may seem that the scores should increase towards the top-right comer. In our experiment, howevel~ this has not generally been the case. Indeed, the system suggested a number of high-scoring short edges, but many low-scoring edges had to be included to span the entire i"
H94-1026,1993.tmi-1.4,1,0.813047,"g simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart-like data structure and select the overall best translation using a set of simple heuristics. 2. INTEGRATING MULTI-ENGINE OUTPUT The MT configuration in our experiment used three MT engines: 147 • a knowledge-based MT (K.BMT) system, the mainline Pangloss engine[l]; • an example-based MT (EBMT) system (see [2, 3]; the original idea is due to Nagao[4]); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - a machine-readable dictionary (the Collins Spanish/English), the lexicons used by the KBMT modules, a large set of usergenerated bilingual glossaries as well as a gazetteer and a List of proper and organization names. The results (target language words and phrases) were recorded in a chart whose initial edges corresponded to words in the source language input. As a result of the operation of each of the MT engines, new edge"
H94-1026,2002.tmi-tutorials.1,0,0.0310161,"g simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart-like data structure and select the overall best translation using a set of simple heuristics. 2. INTEGRATING MULTI-ENGINE OUTPUT The MT configuration in our experiment used three MT engines: 147 • a knowledge-based MT (K.BMT) system, the mainline Pangloss engine[l]; • an example-based MT (EBMT) system (see [2, 3]; the original idea is due to Nagao[4]); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - a machine-readable dictionary (the Collins Spanish/English), the lexicons used by the KBMT modules, a large set of usergenerated bilingual glossaries as well as a gazetteer and a List of proper and organization names. The results (target language words and phrases) were recorded in a chart whose initial edges corresponded to words in the source language input. As a result of the operation of each of the MT engines, new edge"
H94-1026,H93-1038,1,0.886372,"ntal substring order differences. Following a venerable tradition in MT, we used a target language-dependent set of postprocessing rules to alleviate this problem (e.g., by switching the order of adjectives and nouns in a noun phrase if it was produced by the word-for-word engine). 3. TRANSLATION DELIVERY SYSTEM Results of multi-engine MT were fed in our experiment into a translator&apos;s workstation (TWS)[5], through which a translator either approved the system&apos;s output or modified it. The main option for human interaction in TWS currently is the Component Machine-Aided Translation (CMAT) editor[6]. A view of this editor is presented in Figure 9. (The user can see the original source language text in another editor window.) The user can use menus, function keys and mouse clicks to change the system&apos;s initially chosen candidate trans0 0 i 2 3 4 5 6 7 8 9 tO 11 12 13 14 15 16 17 18 19 20 21 22 5 1 I0 2.5 2 3 4 5 7.3 6.75 6.4 5.6 2.25 3.16 3.62 3.3 2 3.5 4.0 3.5 5 5.0 4.0 5 3.5 2 6 7 8 9 5.57 3.58 3.8 4.25 4.0 3.5 5 5.5 3.78 4.0 4.4 4.25 4.0 5.0 5 5.1 3.56 3.71 4.0 3.8 3.5 4.0 3.5 2 5.1 3.72 3.87 4.14 4.0 3.8 4.25 4.0 3.5 5 I0 ii 6.0 5.66 4.85 4.59 5 . 1 1 4.8 5.5 5.11 5.57 5.13 5.66 5.14"
hogan-frederking-1998-evaluation,W97-0411,0,\N,Missing
hogan-frederking-1998-evaluation,A83-1029,0,\N,Missing
hogan-frederking-1998-evaluation,A94-1016,1,\N,Missing
hogan-frederking-1998-evaluation,C96-1030,0,\N,Missing
hogan-frederking-1998-evaluation,C90-3044,0,\N,Missing
hogan-frederking-1998-evaluation,1991.mtsummit-papers.9,0,\N,Missing
hogan-frederking-1998-evaluation,H93-1038,1,\N,Missing
hogan-frederking-1998-evaluation,C67-1009,0,\N,Missing
lavie-etal-2002-nespole,W00-0203,1,\N,Missing
lavie-etal-2002-nespole,costantini-etal-2002-nespole,1,\N,Missing
lavie-etal-2002-nespole,H01-1007,1,\N,Missing
levin-etal-2014-resources,khokhlova-zakharov-2010-studying,0,\N,Missing
levin-etal-2014-resources,D10-1004,0,\N,Missing
levin-etal-2014-resources,ivanova-etal-2008-evaluating,0,\N,Missing
levin-etal-2014-resources,P14-1024,1,\N,Missing
levin-etal-2014-resources,macwhinney-fromm-2014-two,1,\N,Missing
levin-etal-2014-resources,W13-0906,1,\N,Missing
levin-etal-2014-resources,feely-etal-2014-cmu,1,\N,Missing
marujo-etal-2012-supervised,N03-1033,0,\N,Missing
marujo-etal-2012-supervised,guthrie-etal-2010-efficient,0,\N,Missing
monson-etal-2008-linguistic,2001.mtsummit-road.7,1,\N,Missing
monson-etal-2008-linguistic,2004.tmi-1.1,1,\N,Missing
monson-etal-2008-linguistic,N06-2002,1,\N,Missing
monson-etal-2008-linguistic,J01-2001,0,\N,Missing
monson-etal-2008-linguistic,W07-1315,1,\N,Missing
N03-4010,N01-1005,0,0.0248439,"be combined dynamically to determine the optimal answer. For more complex questions, a more flexible and powerful control mechanism is required. For example, LCC (D. Moldovan and Surdeanu, 2002) has implemented feedback loops which ensure that processing constraints are met by retrieving more documents or expanding question terms. The LCC system includes a passage retrieval loop, a lexico-semantic loop and a logic proving loop. The IBM PIQUANT system (Carroll et al., 2002) combines knowledge-based agents using predictive annotation with a statistical approach based on a maximum entropy model (Ittycheriah et al., 2001). Both the LCC and IBM systems represent a departure from the standard pipelined approach to QA architecture, and both work well for straightforward factoid questions. Nevertheless, both approaches incorporate a pre-determined set of processing steps or strategies, and have limited ability to reason about new types of questions not previously encountered. Practically useful question answering in non-factoid domains (e.g., intelligence analysis) requires more sophisticated question decomposition, reasoning, and answer synthesis. For these hard questions, QA architectures must define relationshi"
N03-4015,W02-0717,1,\N,Missing
N03-4015,lavie-etal-2002-nespole,1,\N,Missing
N06-2002,2005.mtsummit-posters.10,1,0.792873,"mitive subject matter that may be seen as insulting. Moreover, we did our best to avoid lexical gaps; for example, many languages do not have a single word that means winner. 7 chine learning we will use information detected from translated sentences in order to decide what parts of the feature space are redundant and what parts must be explored and translated next. A further description of this process can be read in Levin et al. (2006). Additionally, we will change from using humans to write sentences and context fields to having them generated by using a natural language generation system (Alvarez et al. 2005). We also ran small scale experiments to measure translator accuracy and consistency and encountered positive results. Hebrew and Japanese translators provided consistent, accurate translations. Large scale experiments will be conducted in the near future to see if the success of the smaller experiments will carry over to a larger scale. Translator accuracy was also an important objective and we took pains to construct natural sounding, unambiguous sentences. The context field is used to clarify the sentence meaning and spell out features that may not manifest themselves in English. 5 Tools In"
nallasamy-etal-2008-nineoneone,W03-2118,0,\N,Missing
nallasamy-etal-2008-nineoneone,W06-3711,0,\N,Missing
nallasamy-etal-2008-nineoneone,H01-1007,0,\N,Missing
W02-0711,A94-1016,1,\N,Missing
W02-0711,C96-1030,1,\N,Missing
W02-0711,frederking-etal-2002-field,1,\N,Missing
W08-0410,N06-2002,1,0.800422,"ture value was given to the bilingual person to translate). This is the case for the first four features and their values in Figure 1. The last two function features and their values tell us what possible roles participants and clauses can take in sentences. 5.2 Elicitation Corpus As outlined in Section 3, feature detection uses an Elicitation Corpus (see Figure 2), a corpus that has been carefully constructed to provide a large number of minimal pairs of sentences such as He sings and She sings so that only a single feature (e.g. gender) differs between the two sentences (Levin et al., 2006; Alvarez et al., 2006). If two features had varied at once (e.g. It sang) or lexical choice varied (e.g. She reads), then making assertions about which features the language does and does not express becomes much more difficult. Notice that each input sentence has been tagged with an identifier for a lexical cluster as a preprocessing step. Specifying lexical clusters ensures that we don’t compare sentences with different content just because their feature structures match. For example, we would not want to compare Dog bites man and Man bites dog nor The student snored and The professor snored. Note that bag-of-wor"
W08-0410,W07-0409,0,0.0338923,"Missing"
W08-0410,carbonell-etal-2002-automatic,1,0.888087,"showing subject-verb agreement being separated by 12 words. ment if it produced a target-side dependency tree as in Ding and Palmer (2005). However, we are not aware of any systems that attempt this. Therefore, the correct hypotheses, which have correct agreement, will likely be produces as hypotheses of traditional beam-search MT systems, but their features might not be able to discern the correct hypothesis, allowing it to fall below the 1-best or out of the beam entirely. By constructing a feature-rich grammar in a framework that allows unification-based feature constraints such as AVENUE (Carbonell et al., 2002), we can prune these bad hypotheses lacking agreement from the search space. Returning to the example of subject-verb agreement, consider the following Urdu sentences taken from the Urdu-English Elicitation Corpus in LDC’s LCTL language pack: Danish ne Amna Danish ERG Amna “Danish punished Amna.” Danish Amna ko Danish Amna DAT “Danish punishes Amna.” ko DAT sza punish sza punish di give.PERF dita give.HAB hai be.PRES These examples show the split-ergativity of Urdu in which the ergative marker ne is used only for the subject of transitive, perfect aspect verbs. In particular, since these sente"
W08-0410,P05-1033,0,0.0108628,"can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved. We conclude by outlining how the resulting output can then be used in inducing a morphosyntactically feature-rich grammar for AVENUE, a modern syntax-based MT system. 1 2 Introduction Recent trends in Machine Translation have begun moving toward the incorporation of syntax and structure in translation models in hopes of gaining better translation quality. In fact, some structurebased systems have already shown that they can outperform phrase-based SMT systems (Chiang, 2005). Still, even the best-performing data-driven systems have not fully explored the depth of such linguistic features as morphosyntax. Certainly, many have brought linguistically motivated features into their models in the past. Huang and Knight (2006) explored relabeling of nonterminal symbols to embed more information diTask Overview Feature Detection is the process of determining from a corpus annotated with feature structures (Figure 2) which feature values (Figure 1) have a distinct representation in a target language in terms of morphemes (Figure 3). By leveraging knowledge from the field"
W08-0410,P05-1067,0,0.0270765,"roblem without using a huge number of non-terminals, each marked for all possible agreements. A syntax-based system might be able to check this sort of agreeek talb alm arshad jo mchhlyoN ke liye pani maiN aata phink raha tha . . . a.SG student named Irshad who fish for water in flour throw PROG.SG.M be.PAST.SG.M “A student named Irshad who was throwing flour in the water for the fish . . . ” Figure 5: A glossed example from parallel text in LDC’s Urdu-English LCTL language pack showing subject-verb agreement being separated by 12 words. ment if it produced a target-side dependency tree as in Ding and Palmer (2005). However, we are not aware of any systems that attempt this. Therefore, the correct hypotheses, which have correct agreement, will likely be produces as hypotheses of traditional beam-search MT systems, but their features might not be able to discern the correct hypothesis, allowing it to fall below the 1-best or out of the beam entirely. By constructing a feature-rich grammar in a framework that allows unification-based feature constraints such as AVENUE (Carbonell et al., 2002), we can prune these bad hypotheses lacking agreement from the search space. Returning to the example of subject-ve"
W08-0410,N06-1031,0,0.0188419,"ure-rich grammar for AVENUE, a modern syntax-based MT system. 1 2 Introduction Recent trends in Machine Translation have begun moving toward the incorporation of syntax and structure in translation models in hopes of gaining better translation quality. In fact, some structurebased systems have already shown that they can outperform phrase-based SMT systems (Chiang, 2005). Still, even the best-performing data-driven systems have not fully explored the depth of such linguistic features as morphosyntax. Certainly, many have brought linguistically motivated features into their models in the past. Huang and Knight (2006) explored relabeling of nonterminal symbols to embed more information diTask Overview Feature Detection is the process of determining from a corpus annotated with feature structures (Figure 2) which feature values (Figure 1) have a distinct representation in a target language in terms of morphemes (Figure 3). By leveraging knowledge from the field of language typology, we know what types of phenomena are possible across languages and, thus, which features to include in our feature specification. But not every language will display each of these phenomena. Our goal is to determine which feature"
W08-0410,D07-1091,0,0.0246453,"t aspect. If, during translation, a hypothesis is proposed that does not meet either of these conditions, unification will fail and the hypothesis will be pruned 1 . Certainly, unification-based grammars are not the 1 If the reader is not familiar with Unification Grammars, we recommend Kaplan (1995) 81 only way in which this rich source of linguistic information could be used to augment a structure-based translation system. One could also imagine a system in which the feature annotations are simply used to improve the discriminative power of a model. For example, factored translation models (Koehn and Hoang, 2007) retain the simplicity of phrase-based SMT while adding the ability to incorporate additional features. Similarly, there exists a continuum of degrees to which this linguistic information can be used in current syntax-based MT systems. As modern systems move toward integrating many features (Liang et al., 2006), resources such as this will become increasingly important in improving translation quality. 5 System Description In the following sections, we will describe the process of inductive feature detection by way of a running example. 5.1 Feature Specification The first input to our system i"
W08-0410,P06-1096,0,0.0171477,"n which this rich source of linguistic information could be used to augment a structure-based translation system. One could also imagine a system in which the feature annotations are simply used to improve the discriminative power of a model. For example, factored translation models (Koehn and Hoang, 2007) retain the simplicity of phrase-based SMT while adding the ability to incorporate additional features. Similarly, there exists a continuum of degrees to which this linguistic information can be used in current syntax-based MT systems. As modern systems move toward integrating many features (Liang et al., 2006), resources such as this will become increasingly important in improving translation quality. 5 System Description In the following sections, we will describe the process of inductive feature detection by way of a running example. 5.1 Feature Specification The first input to our system is a feature specification (Figure 1). The feature specification used for this experiment was written by an expert in language typology and is stored in a human-readable XML format. It is intended to cover a large number of phenomena that are possible in the languages of the world. Note that features beginning w"
W08-0410,W07-1315,1,0.803627,"manually written rules. We define inductive feature detection as a recall-oriented task since its output is intended to be analyzed by a Morphosyntactic Lexicon Generator, which will address the issue of precision. This, in turn, allows us to inform a rule learner about which language features can be clustered and handled by a single set of rules and which must be given special attention. However, due to the complexity of this component, describing it is beyond the scope of this paper. We also note that future work will include the integration of a morphology analysis system such as ParaMor (Monson et al., 2007) to extract and annotate the valuable morphosyntactic information of inflected languages. An example of this processing pipeline is given in Figure 4. Feature np-gen np-gen np-gen np-def np-def np-num np-num c-ten c-ten Value m f n + sg dl-pl past-pres fut Candidate Morphemes el, ni˜no ella, ni˜na *unobserved* el, la, las una, unas el, ella, la, una, come, ni˜no, ni˜na las, unas, comen, ni˜nas – *unobserved* Figure 3: An example of the output of our system for the above corpus: a list of feature-morpheme pairings. Elicitation Corpus Decoder Inductive Feature Detection Morphosyntactic Lexicon G"
W08-0410,clark-etal-2008-toward,1,\N,Missing
W08-1509,H01-1007,0,0.0773097,"Missing"
W08-1509,W03-2118,0,\N,Missing
W08-1509,W06-3711,0,\N,Missing
W08-1509,nallasamy-etal-2008-nineoneone,1,\N,Missing
W10-2420,D08-1031,0,0.0544838,"Missing"
W10-2420,P05-1045,0,0.00584914,"EAF = CEAF(GNM, ONM) Following our previous notation, we denote CONE B3 and CONE CEAF collectively as Score(GNM, ONM). We observe that Score(GNM, ONM) measures a NE-CRR system’s performance for the NE-CRR subtask of named mentions extraction and grouping (NMEG). We find that Score(GNM, ONM) is highly correlated with Score(G, O) for all the freely available NECRR systems over various datasets. This provides the neccessary justification for the use of Score(GNM, ONM). We use SYNERGY (Shah et al., 2010), an ensemble NER system that combines the UIUC NER (Ritanov and Roth, 2009) and Stanford NER (Finkel et al., 2005) systems, to produce GNM and ONM from G and O by selecting named mentions. However, any other good NER system would serve the same purpose. We see that while standard evaluation metrics require the use of G, i.e. the full set of NE-CRR gold standard annotations including named, nominal and pronimal mentions, CONE metrics require only GNM, i.e. gold standard annotations consisting of named mentions only. The key advantage of using CONE metrics is that GNM can be automatically approximated using an NER system with a good degree of accuracy. This is because state-of-the-art NER systems achieve ne"
W10-2420,P09-1016,1,0.793761,"t al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine et al., 2009) for evaluating transliteration alignment. It reduces the need for alignment gold standard and highly correlates with transliteration system performance. Thus it is able to 137 serve as a good metric for transliteration alignment. We contrast our work with (Stoyanov et al., 2009), who show that the co-reference resolution problem can be separated into different parts according to the type of the mention. Some parts are relatively easy to solve. The resolver performs equally well in each part across datasets. They use the statistics of mentions in different parts with test results on other data"
W10-2420,W09-1119,0,0.0241493,"d Entity (NE) annotations are easy to produce; indeed, there are many NE annotated corpora of different sizes and genres. Similarly, there are few CRR systems and even the best scores obtained by them are only in the region of F1 = 0.5 - 0.6. There are only four such CRR systems freely available, to the best of our knowledge (Bengston and Roth, 2007; Versley et al., 2008; Baldridge and Torton, 2004; Baldwin and Carpenter, 2003). In comparison, there are numerous Named Entity recognition (NER) systems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95 (Ratinov and Roth, 2009; Finkel et al., 136 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136–144, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics 2005). Although these facts can be partly attributed to the ‘hardness’ of CRR compared to NER, they also reflect the substantial gap between NER and CRR research. In this paper, we present a set of metrics, collectively called CONE, that leverage widely available NER systems and resources and tools for the task of evaluating co-reference resolution systems. The basic idea behind CONE is to predict a CRR system’s performan"
W10-2420,P08-4003,0,0.132307,"an in-depth knowledge of linguistics and a high level of understanding of the particular text. Consequently, very few corpora with gold standard CRR annotations are available (NIST, 2003; MUC-6, 1995; Agirre, 2007). By contrast, gold standard Named Entity (NE) annotations are easy to produce; indeed, there are many NE annotated corpora of different sizes and genres. Similarly, there are few CRR systems and even the best scores obtained by them are only in the region of F1 = 0.5 - 0.6. There are only four such CRR systems freely available, to the best of our knowledge (Bengston and Roth, 2007; Versley et al., 2008; Baldridge and Torton, 2004; Baldwin and Carpenter, 2003). In comparison, there are numerous Named Entity recognition (NER) systems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95 (Ratinov and Roth, 2009; Finkel et al., 136 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136–144, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics 2005). Although these facts can be partly attributed to the ‘hardness’ of CRR compared to NER, they also reflect the substantial gap between NER and CRR research. In this paper"
W10-2420,M95-1005,0,0.26742,"Missing"
W10-2420,N03-1020,0,0.0673346,"evaluating NE-CRR systems on an unlabeled dataset, and discuss possible drawbacks and extensions of these metrics. Finally, in section 7 we present our conclusions and ideas for future work. 2 Related Work There has been a substantial amount of research devoted to automatic evaluation for natural language processing, especially tasks involving language generation. The BLEU score (Papineni et al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine et al., 2009) for evaluating transliteration alignment. It reduces the need for alignment gold standard and highly correlates with transliteration system performance. Thus it is able to 137 serve as a good metric"
W10-2420,P04-1077,0,0.086991,"Missing"
W10-2420,D09-1032,0,0.0239929,"led dataset, and discuss possible drawbacks and extensions of these metrics. Finally, in section 7 we present our conclusions and ideas for future work. 2 Related Work There has been a substantial amount of research devoted to automatic evaluation for natural language processing, especially tasks involving language generation. The BLEU score (Papineni et al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine et al., 2009) for evaluating transliteration alignment. It reduces the need for alignment gold standard and highly correlates with transliteration system performance. Thus it is able to 137 serve as a good metric for transliteration alignment. We contrast o"
W10-2420,H05-1004,0,0.50293,"ntity. In this paper, we consider the task of Named Entity CRR (NE-CRR) only. Most, if not all, recent efforts in the field of CRR have concentrated on machine-learning based approaches. Many of them formulate the problem as a pair-wise binary classification task, in which possible co-reference between every pair of mentions is considered, and produce chains of coreferring mentions for each entity as their output. One of the most important problems in CRR is the evaluation of CRR results. Different evaluation metrics have been proposed for this task. Bcubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) are the two most popular metrics; they compute Precision, Recall and F1 measure between matched equivalent classes and use weighted sums of Precision, Recall and F1 to produce a global score. Like all metrics, B3 and CEAF require gold standard annotations; however, gold standard CRR annotations are scarce, because producing such annotations involves a substantial amount of human effort since it requires an in-depth knowledge of linguistics and a high level of understanding of the particular text. Consequently, very few corpora with gold standard CRR annotations are available (NIST, 2003; MUC-"
W10-2420,P02-1040,0,0.0943525,"n 4. In section 5, we provide experimental results that illustrate the performance of CONE B3 and CONE CEAF compared to B3 and CEAF respectively. In Section 6, we give an example of the application of CONE metrics by evaluating NE-CRR systems on an unlabeled dataset, and discuss possible drawbacks and extensions of these metrics. Finally, in section 7 we present our conclusions and ideas for future work. 2 Related Work There has been a substantial amount of research devoted to automatic evaluation for natural language processing, especially tasks involving language generation. The BLEU score (Papineni et al., 2002) proposed for evaluating machine translation results is the best known example of this. It uses n-gram statistics between machine generated results and references. It inspired the ROUGE metric (Lin and Hovy, 2003) and other methods (Louis and Nenkova, 2009) to perform automatic evaluation of text summarization. Both these metrics have show strong correlation between automatic evaluation results and human judgments. The two metrics successfully reduce the need for human judgment and help speed up research by allowing large-scale evaluation. Another example is the alignment entropy (Pervouchine"
W10-2420,P09-1074,0,\N,Missing
W10-2420,versley-etal-2008-bart-modular,0,\N,Missing
W97-0409,C96-1030,0,0.132693,"OMAT (showing how it is well-suited for interactive user correction), describe our approach to rapid-deployment speech recognition and then discuss our approach to interactive user correction of errors in the overall system. 2 Multi-Engine Translation Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-"
W97-0409,1995.tmi-1.17,1,0.768974,"chart data structure (Kay, 1967; Winograd, 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modelling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking, 1995; Frederking, 1994). These selection techniques attempt to produce the best overall result, taking the probability of transitions between segments into account as well as modifying the quality scores of individual segments. Differences in the development times and costs of different .technologies can be exploited to enable MT systems to be rapidly deployed for new languages (Frederking and Brown, 1996). If parallel corpora are available for a new language pair, the E B M T engine can provide translations for a new language in a m a t t e r of hours. Knowledgebases for lexical-transfer M T can"
W97-0409,1991.mtsummit-papers.3,0,0.129582,"Missing"
W97-0409,A94-1016,1,0.942357,"T system that performs initial translations at a useful level of quality between a new language and English within a matter of days or weeks, with continual, graceful improvement to a good level of quality over a period of months. The speech understanding component used is the SPHINX II HMM-based speaker-independent continuous speech recognition system (Huang el al., 1992; Ravishankar, 1996), with techniques for rapidly developing acoustic and language models for new languages (Rudnicky, 1995). The machine translation (MT) technology is the MultiEngine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994), described further below. The speech synthes!s component is a newly-developed concatenative system (Lenzo, 1997) based on variable-sized compositional units. This use of subword concatenation is especially important, since it is the only currently available method for rapidly bringing up synthesis for a new language. DIPLOMAT thus involves research in MT, speech understanding and synthesis, interface design, as well as wearable computer systems. While beginning our investigations into new semi-automatic techniques for both speech and MT knowledge-base development, we have already produced an"
W97-0409,C67-1009,0,0.681711,"such as electronic dictionaries or online bilingual parallel texts. The Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994) makes it possible to exploit the differences between MT technologies. As shown in Figure 1, M E M T feeds an input text to several MT engines in parallel, with each engine employing a different MT technology 1. Each engine attempts to translate the entire input text, segmenting each sentence in whatever manner is most appropriate for its technology, and putting the resulting translated output segments into a shared chart data structure (Kay, 1967; Winograd, 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modelling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking, 1995; Fre"
W97-0409,A83-1029,0,0.0247733,"ion Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-line corpora, such as electronic dictionaries or online bilingual parallel texts. The Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994) makes it possible to exploit the differences between MT technologies. As show"
W97-0409,1991.mtsummit-papers.9,0,0.0818523,"Missing"
W97-0409,H93-1038,1,0.938628,". 2 Multi-Engine Translation Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-line corpora, such as electronic dictionaries or online bilingual parallel texts. The Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994) makes it possible to exploit the differences between MT technol"
W97-0409,1996.amta-1.35,1,0.879027,"re used in DIPLOMAT (showing how it is well-suited for interactive user correction), describe our approach to rapid-deployment speech recognition and then discuss our approach to interactive user correction of errors in the overall system. 2 Multi-Engine Translation Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-"
