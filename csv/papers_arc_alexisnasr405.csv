2021.iwpt-1.3,The Reading Machine: A Versatile Framework for Studying Incremental Parsing Strategies,2021,-1,-1,2,1,5811,franck dary,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"The Reading Machine, is a parsing framework that takes as input raw text and performs six standard nlp tasks: tokenization, pos tagging, morphological analysis, lemmatization, dependency parsing and sentence segmentation. It is built upon Transition Based Parsing, and allows to implement a large number of parsing configurations, among which a fully incremental one. Three case studies are presented to highlight the versatility of the framework. The first one explores whether an incremental parser is able to take into account top-down dependencies (i.e. the influence of high level decisions on low level ones), the second compares the performances of an incremental and a pipe-line architecture and the third quantifies the impact of the right context on the predictions made by an incremental parser."
2021.cmcl-1.13,{TALEP} at {CMCL} 2021 Shared Task: Non Linear Combination of Low and High-Level Features for Predicting Eye-Tracking Data,2021,-1,-1,2,1,5811,franck dary,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"In this paper we describe our contribution to the CMCL 2021 Shared Task, which consists in predicting 5 different eye tracking variables from English tokenized text. Our approach is based on a neural network that combines both raw textual features we extracted from the text and parser-based features that include linguistic predictions (e.g. part of speech) and complexity metrics (e.g., entropy of parsing). We found that both the features we considered as well as the architecture of the neural model that combined these features played a role in the overall performance. Our system achieved relatively high accuracy on the test data of the challenge and was ranked 2nd out of 13 competing teams and a total of 30 submissions."
2020.coling-main.298,{SLICE}: Supersense-based Lightweight Interpretable Contextual Embeddings,2020,-1,-1,3,0,21397,cindy aloui,Proceedings of the 28th International Conference on Computational Linguistics,0,"Contextualised embeddings such as BERT have become de facto state-of-the-art references in many NLP applications, thanks to their impressive performances. However, their opaqueness makes it hard to interpret their behaviour. SLICE is a hybrid model that combines supersense labels with contextual embeddings. We introduce a weakly supervised method to learn interpretable embeddings from raw corpora and small lists of seed words. Our model is able to represent both a word and its context as embeddings into the same compact space, whose dimensions correspond to interpretable supersenses. We assess the model in a task of supersense tagging for French nouns. The little amount of supervision required makes it particularly well suited for low-resourced scenarios. Thanks to its interpretability, we perform linguistic analyses about the predicted supersenses in terms of input word and context representations."
N19-1393,Typological Features for Multilingual Delexicalised Dependency Parsing,2019,0,1,3,0,26277,manon scholivet,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,The existence of universal models to describe the syntax of languages has been debated for decades. The availability of resources such as the Universal Dependencies treebanks and the World Atlas of Language Structures make it possible to study the plausibility of universal grammar from the perspective of dependency parsing. Our work investigates the use of high-level language descriptions in the form of typological features for multilingual dependency parsing. Our experiments on multilingual parsing for 40 languages show that typological information can indeed guide parsers to share information between similar languages beyond simple language identification.
D19-5803,{CALOR}-{QUEST} : generating a training corpus for Machine Reading Comprehension models from shallow semantic annotations,2019,0,0,6,0,17997,frederic bechet,Proceedings of the 2nd Workshop on Machine Reading for Question Answering,0,"Machine reading comprehension is a task related to Question-Answering where questions are not generic in scope but are related to a particular document. Recently very large corpora (SQuAD, MS MARCO) containing triplets (document, question, answer) were made available to the scientific community to develop supervised methods based on deep neural networks with promising results. These methods need very large training corpus to be efficient, however such kind of data only exists for English and Chinese at the moment. The aim of this study is the development of such resources for other languages by proposing to generate in a semi-automatic way questions from the semantic Frame analysis of large corpora. The collect of natural questions is reduced to a validation/test set. We applied this method on the CALOR-Frame French corpus to develop the CALOR-QUEST resource presented in this paper."
2019.jeptalnrecital-court.4,{CALOR}-{QUEST} : un corpus d{'}entra{\\^\\i}nement et d{'}{\\'e}valuation pour la compr{\\'e}hension automatique de textes (Machine reading comprehension is a task related to Question-Answering where questions are not generic in scope but are related to a particular document),2019,-1,-1,6,0,17997,frederic bechet,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"La compr{\'e}hension automatique de texte est une t{\^a}che faisant partie de la famille des syst{\`e}mes de Question/R{\'e}ponse o{\`u} les questions ne sont pas {\`a} port{\'e}e g{\'e}n{\'e}rale mais sont li{\'e}es {\`a} un document particulier. R{\'e}cemment de tr{\`e}s grand corpus (SQuAD, MS MARCO) contenant des triplets (document, question, r{\'e}ponse) ont {\'e}t{\'e} mis {\`a} la disposition de la communaut{\'e} scientifique afin de d{\'e}velopper des m{\'e}thodes supervis{\'e}es {\`a} base de r{\'e}seaux de neurones profonds en obtenant des r{\'e}sultats prometteurs. Ces m{\'e}thodes sont cependant tr{\`e}s gourmandes en donn{\'e}es d{'}apprentissage, donn{\'e}es qui n{'}existent pour le moment que pour la langue anglaise. Le but de cette {\'e}tude est de permettre le d{\'e}veloppement de telles ressources pour d{'}autres langues {\`a} moindre co{\^u}t en proposant une m{\'e}thode g{\'e}n{\'e}rant de mani{\`e}re semi-automatique des questions {\`a} partir d{'}une analyse s{\'e}mantique d{'}un grand corpus. La collecte de questions naturelle est r{\'e}duite {\`a} un ensemble de validation/test. L{'}application de cette m{\'e}thode sur le corpus CALOR-Frame a permis de d{\'e}velopper la ressource CALOR-QUEST pr{\'e}sent{\'e}e dans cet article."
L18-1014,Handling Normalization Issues for Part-of-Speech Tagging of Online Conversational Text,2018,0,1,3,0,188,geraldine damnati,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"For the purpose of POS tagging noisy user-generated text, should normalization be handled as a preliminary task or is it possiblen to handle misspelled words directly in the POS tagging model? We propose in this paper a combined approach where some errorsn are normalized before tagging, while a Gated Recurrent Unit deep neural network based tagger handles the remaining errors. Wordn embeddings are trained on a large corpus in order to address both normalization and POS tagging. Experiments are run on Contactn Center chat conversations, a particular type of formal Computer Mediated Communication data."
L18-1159,Semantic Frame Parsing for Information Extraction : the {CALOR} corpus,2018,0,4,5,0,184,gabriel marzinotto,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This paper presents a publicly available corpus of French encyclopedic history texts annotated according to the Berkeley FrameNet formalism. The main difference in our approach compared to previous works on semantic parsing with FrameNet is that we are not interested here in full text parsing but rather on partial parsing. The goal is to select from the FrameNet resources the minimal set of frames that are going to be useful for the applicative framework targeted, in our case Information Extraction from encyclopedic documents. Such an approach leverages the manual annotation of larger corpora than those obtained through full text parsing and therefore opens the door to alternative methods for Frame parsing than those used so far on the FrameNet 1.5 benchmark corpus. The approaches compared in this study rely on an integrated sequence labeling model which jointly optimizes frame identification and semantic role segmentation and identification. The models compared are CRFs and multitasks bi-LSTMs."
L18-1716,Adding Syntactic Annotations to Flickr30k Entities Corpus for Multimodal Ambiguous Prepositional-Phrase Attachment Resolution,2018,0,1,2,1,30320,sebastien delecraz,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,We propose in this paper to add to the captions of the Flickr30k Entities corpus some syntactic annotations in order to study the joint processing of image and language features for the Preposition-Phrase attachment disambiguation task. The annotation has been performed on the English version of the captions and automatically projected on their French and German translations.
2018.jeptalnrecital-long.13,Correction automatique d{'}attachements pr{\\'e}positionnels par utilisation de traits visuels ({PP}-attachement resolution using visual features),2018,-1,-1,4,1,30320,sebastien delecraz,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"La d{\'e}sambigu{\""\i}sation des rattachements pr{\'e}positionnels est une t{\^a}che syntaxique qui demande des connaissances s{\'e}mantiques, pouvant {\^e}tre extraites d{'}une image associ{\'e}e au texte trait{\'e}. Nous pr{\'e}sentons et analysons les difficult{\'e}s de cette t{\^a}che pour laquelle nous construisons un syst{\`e}me complet entra{\^\i}n{\'e} sur une version {\'e}tendue des annotations du corpus Flickr30k Entities. Lorsque la s{\'e}mantique lexicale n{'}est pas disponible, l{'}information visuelle apporte 3 {\%} d{'}am{\'e}lioration."
2018.jeptalnrecital-court.42,Annotation en Actes de Dialogue pour les Conversations d{'}Assistance en Ligne (Dialog Acts Annotations for Online Chats),2018,-1,-1,2,0,31010,robin perrotin,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,Les conversations techniques en ligne sont un type de productions linguistiques qui par de nombreux aspects se d{\'e}marquent des objets plus usuellement {\'e}tudi{\'e}s en traitement automatique des langues : il s{'}agit de dialogues {\'e}crits entre deux locuteurs qui servent de support {\`a} la r{\'e}solution coop{\'e}rative des probl{\`e}mes des usagers. Nous proposons de d{\'e}crire ici ces conversations par un {\'e}tiquetage en actes de dialogue sp{\'e}cifiquement con{\c{c}}u pour les conversations en ligne. Diff{\'e}rents syst{\`e}mes de pr{\'e}dictions ont {\'e}t{\'e} {\'e}valu{\'e}s ainsi qu{'}une m{\'e}thode permettant de s{'}abstraire des sp{\'e}cificit{\'e}s lexicales du corpus d{'}apprentissage.
W17-6311,Correcting prepositional phrase attachments using multimodal corpora,2017,7,1,2,1,30320,sebastien delecraz,Proceedings of the 15th International Conference on Parsing Technologies,0,"PP-attachments are an important source of errors in parsing natural language. We propose in this article to use data coming from a multimodal corpus, combining textual, visual and conceptual information, as well as a correction strategy, to propose alternative attachments in the output of a parser."
D17-1180,{TAG} Parsing with Neural Networks and Vector Representations of Supertags,2017,22,9,5,0,3374,jungo kasai,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (elementary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely, and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is {``}almost parsing.{''} We demonstrate that the embedding vector representations the parser induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in distributional semantics. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences."
W16-3621,Syntactic parsing of chat language in contact center conversation corpus,2016,12,3,1,1,5812,alexis nasr,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Chat language is often referred to as Computer-mediated communication (CMC). Most of the previous studies on chat language has been dedicated to collecting  chat room  data as it is the kind of data which is the most accessible on the WEB. This kind of data falls under the informal register whereas we are interested in this paper in understanding the mechanisms of a more formal kind of CMC: dialog chat in contact centers. The particularities of this type of dialogs and the type of language used by customers and agents is the focus of this paper towards understanding this new kind of CMC data. The challenges for processing chat data comes from the fact that Natural Language Processing tools such as syntactic parsers and part of speech taggers are typically trained on mismatched conditions, we describe in this study the impact of such a mismatch for a syntactic parsing task."
W16-3309,Revisiting Supertagging and Parsing: How to Use Supertags in Transition-Based Parsing,2016,1,0,3,0,33762,wonchang chung,Proceedings of the 12th International Workshop on Tree Adjoining Grammars and Related Formalisms ({TAG}+12),0,We discuss the use of supertags derived from a TAG in transition-based parsing. We show some initial experimental results which suggest that using a representation of a supertag in terms of its structural and linguistic dimensions outperforms the use of atomic supertags.
L16-1363,{D}e{Q}ue: A Lexicon of Complex Prepositions and Conjunctions in {F}rench,2016,9,0,2,0,12002,carlos ramisch,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We introduce DeQue, a lexicon covering French complex prepositions (CPRE) like {``}{\`a} partir de{''} (from) and complex conjunctions (CCONJ) like {``}bien que{''} (although). The lexicon includes fine-grained linguistic description based on empirical evidence. We describe the general characteristics of CPRE and CCONJ in French, with special focus on syntactic ambiguity. Then, we list the selection criteria used to build the lexicon and the corpus-based methodology employed to collect entries. Finally, we quantify the ambiguity of each construction by annotating around 100 sentences randomly taken from the FRWaC. In addition to its theoretical value, the resource has many potential practical applications. We intend to employ DeQue for treebank annotation and to train a dependency parser that can takes complex constructions into account."
J16-1002,Integrating Selectional Constraints and Subcategorization Frames in a Dependency Parser,2016,52,2,2,1,35499,seyed mirroshandel,Computational Linguistics,0,"Statistical parsers are trained on treebanks that are composed of a few thousand sentences. In order to prevent data sparseness and computational complexity, such parsers make strong independence hypotheses on the decisions that are made to build a syntactic tree. These independence hypotheses yield a decomposition of the syntactic structures into small pieces, which in turn prevent the parser from adequately modeling many lexico-syntactic phenomena like selectional constraints and subcategorization frames. Additionally, treebanks are several orders of magnitude too small to observe many lexico-syntactic regularities, such as selectional constraints and subcategorization frames. In this article, we propose a solution to both problems: how to account for patterns that exceed the size of the pieces that are modeled in the parser and how to obtain subcategorization frames and selectional constraints from raw corpora and incorporate them in the parsing process. The method proposed was evaluated on French and on English. The experiments on French showed a decrease of 41.6% of selectional constraint violations and a decrease of 22% of erroneous subcategorization frame assignment. These figures are lower for English: 16.21% in the first case and 8.83% in the second."
C16-1040,Deeper syntax for better semantic parsing,2016,21,4,4,0,35700,olivier michalon,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Syntax plays an important role in the task of predicting the semantic structure of a sentence. But syntactic phenomena such as alternations, control and raising tend to obfuscate the relation between syntax and semantics. In this paper we predict the semantic structure of a sentence using a deeper syntax than what is usually done. This deep syntactic representation abstracts away from purely syntactic phenomena and proposes a structural organization of the sentence that is closer to the semantic representation. Experiments conducted on a French corpus annotated with semantic frames showed that a semantic parser reaches better performances with such a deep syntactic input."
W15-3207,{POS}-tagging of {T}unisian Dialect Using {S}tandard {A}rabic Resources and Tools,2015,40,5,2,1,20984,ahmed hamdi,Proceedings of the Second Workshop on {A}rabic Natural Language Processing,0,"Developing natural language processing tools usually requires a large number of resources (lexica, annotated corpora, etc.), which often do not exist for less-resourced languages. One way to overcome the problem of lack of resources is to devote substantial efforts to build new ones from scratch. Another approach is to exploit existing resources of closely related languages. In this paper, we focus on developing a part-of-speech tagger for the Tunisian Arabic dialect (TUN), a low-resource language, by exploiting its close-ness to Modern Standard Arabic (MSA), which has many state-of-the-art resources and tools. Our system achieved an accuracy of 89% (xe2x88xbc20% absolute improvement over an MSA tagger baseline)."
W15-0212,Rapid {F}rame{N}et annotation of spoken conversation transcripts,2015,8,3,4,0,35943,jeremy trione,Proceedings of the 11th Joint {ACL}-{ISO} Workshop on Interoperable Semantic Annotation ({ISA}-11),0,None
P15-1108,Joint Dependency Parsing and Multiword Expression Tokenization,2015,24,6,1,1,5812,alexis nasr,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Complex conjunctions and determiners are often considered as pretokenized units in parsing. This is not always realistic, since they can be ambiguous. We propose a model for joint dependency parsing and multiword expressions identification, in which complex function words are represented as individual tokens linked with morphological dependencies. Our graph-based parser includes standard second-order features and verbal subcategoriza-tion features derived from a syntactic lexicon .We train it on a modified version of the French Treebank enriched with morphological dependencies. It recognizes 81.79% of ADVque conjunctions with 91.57% precision, and 82.74% of deDET determiners with 86.70% precision."
W14-5311,Automatically building a {T}unisian Lexicon for Deverbal Nouns,2014,9,6,3,1,20984,ahmed hamdi,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"The sociolinguistic situation in Arabic countries is characterized by diglossia (Ferguson, 1959) : whereas one variant Modern Standard Arabic (MSA) is highly codified and mainly used for written communication, other variants coexist in regular everydayxe2x80x99s situations (dialects). Similarly, while a number of resources and tools exist for MSA (lexica, annotated corpora, taggers, parsers . . . ), very few are available for the development of dialectal Natural Language Processing tools. Taking advantage of the closeness of MSA and its dialects, one way to solve the problem of the lack of resources for dialects consists in exploiting available MSA resources and NLP tools in order to adapt them to process dialects. This paper adopts this general framework: we propose a method to build a lexicon of deverbal nouns for Tunisian (TUN) using MSA tools and resources as starting material."
nasr-etal-2014-automatically,Automatically enriching spoken corpora with syntactic information for linguistic studies,2014,7,7,1,1,5812,alexis nasr,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,Syntactic parsing of speech transcriptions faces the problem of the presence of disfluencies that break the syntactic structure of the utterances. We propose in this paper two solutions to this problem. The first one relies on a disfluencies predictor that detects disfluencies and removes them prior to parsing. The second one integrates the disfluencies in the syntactic structure of the utterances and train a disfluencies aware parser.
N13-1024,Enforcing Subcategorization Constraints in a Parser Using Sub-parses Recombining,2013,26,9,2,1,35499,seyed mirroshandel,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Treebanks are not large enough to adequately model subcategorization frames of predicative lexemes, which is an important source of lexico-syntactic constraints for parsing. As a consequence, parsers trained on such treebanks usually make mistakes when selecting the arguments of predicative lexemes. In this paper, we propose an original way to correct subcategorization errors by combining sub-parses of a sentence S that appear in the list of the n-best parses of S. The subcategorization information comes from three different resources, the first one is extracted from a treebank, the second one is computed on a large corpora and the third one is an existing syntactic lexicon. Experiments on the French Treebank showed a 15.24% reduction of erroneous subcategorization frames (SF) selections for verbs as well as a relative decrease of the error rate of 4% Labeled Accuracy Score on the state of the art parser on this treebank."
F13-1029,Translating verbs between {MSA} and arabic dialects through deep morphological analysis (Un syst{\\`e}me de traduction de verbes entre arabe standard et arabe dialectal par analyse morphologique profonde) [in {F}rench],2013,-1,-1,4,1,20984,ahmed hamdi,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
2013.mtsummit-papers.16,The Effects of Factorizing Root and Pattern Mapping in Bidirectional {T}unisian - {S}tandard {A}rabic Machine Translation,2013,19,8,4,1,20984,ahmed hamdi,Proceedings of Machine Translation Summit XIV: Papers,0,"The development of natural language processing tools for dialects faces the severe problem of lack of resources. In cases of diglossia, as in Arabic, one variant, Modern Standard Arabic (MSA), has many resources that can be used to build natural language processing tools. Whereas other variants, Arabic dialects, are resource poor. Taking advantage of the closeness of MSA and its dialects, one way to solve the problem of limited resources, consists in performing a translation of the dialect into MSA in order to use the tools developed for MSA. We describe in this paper an architecture for such a translation and we evaluate it on Tunisian Arabic verbs. Our approach relies on modeling the translation process over the deep morphological representations of roots and patterns, commonly used to model Semitic morphology. We compare different techniques for how to perform the cross-lingual mapping. Our evaluation demonstrates that the use of a decent coverage rootpattern lexicon of Tunisian and MSA with a backoff that assumes independence of mapping roots and patterns is optimal in reducing overall ambiguity and increasing recall."
W12-5107,Dictionary-ontology cross-enrichment,2012,0,0,3,0,42104,emmanuel eckard,Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon,0,None
W12-3412,Generative Constituent Parsing and Discriminative Dependency Reranking: Experiments on {E}nglish and {F}rench,2012,26,3,3,0.89685,5824,joseph roux,Proceedings of the {ACL} 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,0,We present an architecture for parsing in two steps. A phrase-structure parser builds for each sentence ann-best list of analyses which are converted to dependency trees. These dependency structures are then rescored by a discriminative reranker. Our method is language agnostic and enables the incorporation of additional information which are useful for the choice of the best parse candidate. We test our approach on the the Penn Treebank and the French Treebank. Evaluation shows a significative improvement on different parse metrics.
S12-1024,Extracting a Semantic Lexicon of {F}rench Adjectives from a Large Lexicographic Dictionary,2012,14,1,3,0,31064,selja seppala,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We present a rule-based method to automatically create a large-coverage semantic lexicon of French adjectives by extracting paradigmatic relations from lexicographic definitions. Formalized adjectival resources are, indeed, scarce for French and they mostly focus on morphological and syntactic information. Our objective is, therefore, to contribute enriching the available set of resources by taking advantage of reliable lexicographic data and formalizing it with the well-established lexical functions formalism. The resulting semantic lexicon of French adjectives can be used in NLP tasks such as word sense disambiguation or machine translation. After presenting related work, we describe the extraction method and the formalization procedure of the data. Our method is then quantitatively and qualitatively evaluated. We discuss the results of the evaluation and conclude on some perspectives."
P12-1082,Semi-supervised Dependency Parsing using Lexical Affinities,2012,28,7,2,1,35499,seyed mirroshandel,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Treebanks are not large enough to reliably model precise lexical phenomena. This deficiency provokes attachment errors in the parsers trained on such data. We propose in this paper to compute lexical affinities, on large corpora, for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser. Experiments on the French Treebank showed a relative decrease of the error rate of 7.1% Labeled Accuracy Score yielding the best parsing results on this treebank."
bazillon-etal-2012-syntactic,Syntactic annotation of spontaneous speech: application to call-center conversation data,2012,10,8,4,1,39875,thierry bazillon,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the syntactic annotation process of the DECODA corpus. This corpus contains manual transcriptions of spoken conversations recorded in the French call-center of the Paris Public Transport Authority (RATP). Three levels of syntactic annotation have been performed with a semi-supervised approach: POS tags, Syntactic Chunks and Dependency parses. The main idea is to use off-the-shelf NLP tools and models, originaly developped and trained on written text, to perform a first automatic annotation on the manually transcribed corpus. At the same time a fully manual annotation process is performed on a subset of the original corpus, called the GOLD corpus. An iterative process is then applied, consisting in manually correcting errors found in the automatic annotations, retraining the linguistic models of the NLP tools on this corrected corpus, then checking the quality of the adapted models on the fully manual annotations of the GOLD corpus. This process iterates until a certain error rate is reached. This paper describes this process, the main issues raising when adapting NLP tools to process speech transcriptions, and presents the first evaluations performed with these new adapted tools."
W11-2917,Active Learning for Dependency Parsing Using Partially Annotated Sentences,2011,20,18,2,1,35499,seyed mirroshandel,Proceedings of the 12th International Conference on Parsing Technologies,0,"Current successful probabilistic parsers require large treebanks which are difficult, time consuming, and expensive to produce. Some parts of these data do not contain any useful information for training a parser. Active learning strategies allow to select the most informative samples for annotation. Most existing active learning strategies for parsing rely on selecting uncertain sentences for annotation. We show in this paper that selecting full sentences is not an optimal solution and propose a way to select only subparts of sentences."
P11-4015,{MACAON} An {NLP} Tool Suite for Processing Word Lattices,2011,17,30,1,1,5812,alexis nasr,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"MACAON is a tool suite for standard NLP tasks developed for French. MACAON has been designed to process both human-produced text and highly ambiguous word-lattices produced by NLP tools. MACAON is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other through XML files. In addition, exchange protocols with external tools are easily definable. MACAON is a fast, modular and open tool, distributed under GNU Public License."
I11-1007,"Active Learning Strategies for Support Vector Machines, Application to Temporal Relation Classification",2011,21,8,3,1,35499,seyed mirroshandel,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Temporal relations between events is a valuable source of information which can be used in a large number of natural language processing applications such as question answering, summarization, and information extraction. Supervised temporal relation classification requires large corpora which are difficult, time consuming, and expensive to produce. Active learning strategies are well-suited to reduce this effort by efficiently selecting the most informative samples for labeling. This paper presents novel active learning strategies based on support vector machines (SVM) for temporal relation classification. A large number of empirical comparisons of different active learning algorithms and various kernel functions in SVM shows that proposed active learning strategies are effective for the given task."
2011.jeptalnrecital-long.8,Qui {\\^e}tes-vous ? Cat{\\'e}goriser les questions pour d{\\'e}terminer le r{\\^o}le des locuteurs dans des conversations orales (Who are you? Categorize questions to determine the role of speakers in oral conversations),2011,-1,-1,5,1,39875,thierry bazillon,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La fouille de donn{\'e}es orales est un domaine de recherche visant {\`a} caract{\'e}riser un flux audio contenant de la parole d{'}un ou plusieurs locuteurs, {\`a} l{'}aide de descripteurs li{\'e}s {\`a} la forme et au contenu du signal. Outre la transcription automatique en mots des paroles prononc{\'e}es, des informations sur le type de flux audio trait{\'e} ainsi que sur le r{\^o}le et l{'}identit{\'e} des locuteurs sont {\'e}galement cruciales pour permettre des requ{\^e}tes complexes telles que : Â« chercher des d{\'e}bats sur le th{\`e}me X Â», Â« trouver toutes les interviews de Y Â», etc. Dans ce cadre, et en traitant des conversations enregistr{\'e}es lors d{'}{\'e}missions de radio ou de t{\'e}l{\'e}vision, nous {\'e}tudions la mani{\`e}re dont les locuteurs expriment des questions dans les conversations, en partant de l{'}intuition initiale que la forme des questions pos{\'e}es est une signature du r{\^o}le du locuteur dans la conversation (pr{\'e}sentateur, invit{\'e}, auditeur, etc.). En proposant une classification du type des questions et en utilisant ces informations en compl{\'e}ment des descripteurs g{\'e}n{\'e}ralement utilis{\'e}s dans la litt{\'e}rature pour classer les locuteurs par r{\^o}le, nous esp{\'e}rons am{\'e}liorer l{'}{\'e}tape de classification, et valider par la m{\^e}me occasion notre intuition initiale."
2011.jeptalnrecital-long.26,Mod{\\`e}les g{\\'e}n{\\'e}ratif et discriminant en analyse syntaxique : exp{\\'e}riences sur le corpus arbor{\\'e} de {P}aris 7 (Generative and discriminative models in parsing: experiments on the {P}aris 7 Treebank),2011,-1,-1,4,0.89685,5824,joseph roux,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons une architecture pour l{'}analyse syntaxique en deux {\'e}tapes. Dans un premier temps un analyseur syntagmatique construit, pour chaque phrase, une liste d{'}analyses qui sont converties en arbres de d{\'e}pendances. Ces arbres sont ensuite r{\'e}{\'e}valu{\'e}s par un r{\'e}ordonnanceur discriminant. Cette m{\'e}thode permet de prendre en compte des informations auxquelles l{'}analyseur n{'}a pas acc{\`e}s, en particulier des annotations fonctionnelles. Nous validons notre approche par une {\'e}valuation sur le corpus arbor{\'e} de Paris 7. La seconde {\'e}tape permet d{'}am{\'e}liorer significativement la qualit{\'e} des analyses retourn{\'e}es, quelle que soit la m{\'e}trique utilis{\'e}e."
2011.jeptalnrecital-court.5,Cr{\\'e}ation de clusters s{\\'e}mantiques dans des familles morphologiques {\\`a} partir du {TLF}i (Creating semantic clusters in morphological families from the {TLF}i),2011,-1,-1,3,0.582021,15665,nuria gala,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La constitution de ressources linguistiques est une t{\^a}che longue et co{\^u}teuse. C{'}est notamment le cas pour les ressources morphologiques. Ces ressources d{\'e}crivent de fa{\c{c}}on approfondie et explicite l{'}organisation morphologique du lexique compl{\'e}t{\'e}e d{'}informations s{\'e}mantiques exploitables dans le domaine du TAL. Le travail que nous pr{\'e}sentons dans cet article s{'}inscrit dans cette perspective et, plus particuli{\`e}rement, dans l{'}optique d{'}affiner une ressource existante en s{'}appuyant sur des informations s{\'e}mantiques obtenues automatiquement. Notre objectif est de caract{\'e}riser s{\'e}mantiquement des familles morpho-phonologiques (des mots partageant une m{\^e}me racine et une continuit{\'e} de sens). Pour ce faire, nous avons utilis{\'e} des informations extraites du TLFi annot{\'e} morpho-syntaxiquement. Les premiers r{\'e}sultats de ce travail seront analys{\'e}s et discut{\'e}s."
2010.jeptalnrecital-demonstration.16,{MACAON} Une cha{\\^\\i}ne linguistique pour le traitement de graphes de mots,2010,-1,-1,1,1,5812,alexis nasr,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,
W09-3818,Constructing parse forests that include exactly the n-best {PCFG} trees,2009,11,4,2,0,46836,pierre boullier,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"This paper describes and compares two algorithms that take as input a shared PCFG parse forest and produce shared forests that contain exactly the n most likely trees of the initial forest. Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG f-structure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank."
N09-2047,{MICA}: A Probabilistic Dependency Parser Based on Tree Insertion Grammars (Application Note),2009,16,20,3,0.040439,4704,srinivas bangalore,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"MICA is a dependency parser which returns deep dependency representations, is fast, has state-of-the-art performance, and is freely available."
2009.jeptalnrecital-long.3,Analyse syntaxique en d{\\'e}pendances de l{'}oral spontan{\\'e},2009,-1,-1,1,1,5812,alexis nasr,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article d{\'e}crit un mod{\`e}le d{'}analyse syntaxique de l{'}oral spontan{\'e} ax{\'e} sur la reconnaissance de cadres valenciels verbaux. Le mod{\`e}le d{'}analyse se d{\'e}compose en deux {\'e}tapes : une {\'e}tape g{\'e}n{\'e}rique, bas{\'e}e sur des ressources g{\'e}n{\'e}riques du fran{\c{c}}ais et une {\'e}tape de r{\'e}ordonnancement des solutions de l{'}analyseur r{\'e}alis{\'e} par un mod{\`e}le sp{\'e}cifique {\`a} une application. Le mod{\`e}le est {\'e}valu{\'e} sur le corpus MEDIA."
W04-3308,{S}uper{T}agging and Full Parsing,2004,17,19,1,1,5812,alexis nasr,Proceedings of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction, and we show that, assuming hypothetically perfect performance in the first phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser. This is an improvement of about 50% over previously reported results using a simple heuristic parser."
W04-1503,A Simple String-Rewriting Formalism for Dependency Grammar,2004,-1,-1,1,1,5812,alexis nasr,Proceedings of the Workshop on Recent Advances in Dependency Grammar,0,None
C04-1082,Tagging with Hidden {M}arkov Models Using Ambiguous Tags,2004,10,3,1,1,5812,alexis nasr,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Part of speech taggers based on Hidden Markov Models rely on a series of hypotheses which make certain errors inevitable. The idea developed in this paper consists in allowing a limited, controlled ambiguity in the output of the tagger in order to avoid a number of errors. The ambiguity takes the form of ambiguous tags which denote subsets of the tagset. These tags are used when the tagger hesitates between the different components of the ambiguous tags. They are introduced in an existing lexicon and 3-gram database. Their lexical and syntactic counts are computed on the basis of the lexical and syntactic counts of their constituents, using impurity functions. The tagging process itself, based on the Viterbi algorithm, is unchanged. Experiments conducted on the Brown corpus show a recall of 0.982, for an ambiguity rate of 1.233 which is to be compared with a baseline recall of 0.978 for an ambiguity rate of 1.414 using the same ambiguous tags and with a recall of 0.955 corresponding to the one best solution of standard tagging (without ambiguous tags)."
2004.jeptalnrecital-long.10,Couplage d{'}un {\\'e}tiqueteur morpho-syntaxique et d{'}un analyseur partiel repr{\\'e}sent{\\'e}s sous la forme d{'}automates finis pond{\\'e}r{\\'e}s,2004,-1,-1,1,1,5812,alexis nasr,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,Cet article pr{\'e}sente une mani{\`e}re d{'}int{\'e}grer un {\'e}tiqueteur morpho-syntaxique et un analyseur partiel. Cette integration permet de corriger des erreurs effectu{\'e}es par l{'}{\'e}tiqueteur seul. L{'}{\'e}tiqueteur et l{'}analyseur ont {\'e}t{\'e} r{\'e}alis{\'e}s sous la forme d{'}automates pond{\'e}r{\'e}s. Des r{\'e}sultats sur un corpus du fran{\c{c}}ais ont montr{\'e} une dimintion du taux d{'}erreur de l{'}ordre de 12{\%}.
W02-2214,Context-Free Parsing of a {T}ree {A}djoining {G}rammar Using Finite-State Machines,2002,15,3,1,1,5812,alexis nasr,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,None
C02-2026,Creating a Finite-State Parser with Application Semantics,2002,7,10,4,0,1354,owen rambow,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"Parsli is a finite-state (FS) parser which can be tailored to the lexicon, syntax, and semantics of a particular application using a hand-editable declarative lexicon. The lexicon is defined in terms of a lexicalized Tree Adjoining Grammar, which is subsequently mapped to a FS representation. This approach gives the application designer better and easier control over the natural language understanding component than using an off-the-shelf parser. We present results using Parsli on an application that creates 3D-images from typed input."
P00-1011,Tagging Unknown Proper Names Using Decision Trees,2000,10,49,2,0,17997,frederic bechet,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a supervised learning method to automatically select from a set of noun phrases, embedding proper names of different semantic classes, their most distinctive features. The result of the learning process is a decision tree which classifies an unknown proper name on the basis of its context of occurrence. This classifier is used to estimate the probability distribution of an out of vocabulary proper name over a tagset. This probability distribution is itself used to estimate the parameters of a stochastic part of speech tagger."
P98-1106,"Pseudo-Projectivity, A Polynomially Parsable Non-Projective Dependency Grammar",1998,13,74,2,0,14272,sylvain kahane,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,None
C98-1102,Pseudo-Projectivity: A Polynomially Parsable Non-Projective Dependency Grammar,1998,13,74,2,0,14272,sylvain kahane,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,None
palmer-etal-1998-rapid,Rapid prototyping of domain-apecific machine translation systems,1998,9,14,3,0,4859,martha palmer,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper reports on an experiment in assembling a domain-specific machine translation prototype system from off-the-shelf components. The design goals of this experiment were to reuse existing components, to use machine-learning techniques for parser specialization and for transfer lexicon extraction, and to use an expressive, lexicalized formalism for the transfer component."
1997.mtsummit-workshop.12,Enriching lexical transfer with cross-linguistic semantic features or how to do interlingua without interlingua,1997,7,12,1,1,5812,alexis nasr,AMTA/SIG-IL First Workshop on Interlinguas,0,"In this paper, we propose an alternative to interlingua which can capture the analyses and generalizations that interlinguas can express, but which uses cross-linguistic semantic features rather than a separate level of representation. This alternative we call lexico-structural transfer. Lexico-structural transfer relies on the expressive power of a lexicalized syntactic representation (or xe2x80x9clexicalized grammarxe2x80x9d for short). In a lexicalized grammar, lexemes are associated with syntactic structure; in the transfer lexicon, we do not simply relate words (or context-free rewrite rules) from one language to words (or context-free rewrite rules) from another language. Instead, we relate lexemes along with relevant syntactic structure (essentially, their syntactic projection along with syntactic and lexical-semantic features). Several different lexicalized grammar formalisms have been proposed in the past, including notably Tree Adjoining Grammar (Joshi, 1987), Lexical-Functional Grammar (Kaplan and Bresnan, 1982) and various dependency grammars. We will present our work using a transfer formalism based on a dependency grammar, namely Melxe2x80x99cukxe2x80x99s Meaning Text Theory (MTT) (Melxe2x80x99cuk, 1988), specifically the xe2x80x9cDeep Syntactic Levelxe2x80x9d. This level of representation is similar in crucial respects to the derivation structures of TAG (Rambow and Joshi, 1996) and to the f-structure of LFG. There are two main reasons why we may want to investigate an alternative to the use of an interlingua:"
1995.iwpt-1.23,A Formalism and a Parser for Lexicalised Dependency Grammars,1995,-1,-1,1,1,5812,alexis nasr,Proceedings of the Fourth International Workshop on Parsing Technologies,0,
