2012.amta-papers.27,W10-1408,1,0.906003,"Missing"
2012.amta-papers.27,W09-0437,0,0.0184242,"syntax-based modelling. The string-to-tree modelling in this work is based on their approach. Zollmann et al. (2008) observed that the gain achieved by hierarchical and syntax-based models could be largely compensated for by increasing the reordering limit in the phrase-based model. They also found that, for language pairs involving substantial reordering like Chinese-English, tree-based models performed better than phrase-based. However, for relatively monotonic pairs like ArabicEnglish, all models produced similar results. Experimenting with French-English, GermanEnglish and English-German, Auli et al. (2009) compared a phrase-based model to a hierarchical phrase-based model by exploring as much of the search space of both types of models as was computationally feasible. Given that the search spaces were very similar, they concluded that the differences between the two types of models can be explained by the way they score hypotheses rather than by the hypotheses they produce. Using the same framework as in this work, Hoang et al. (2009) compared phrase-based, hierarchical phrase-based and string-to-tree models. While the phrase-based and hierarchical phrase-based models achieved similar results,"
2012.amta-papers.27,P07-1051,0,0.0215658,"e it to be. While relative improvements over phrase-based baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture long-distance reordering — a problem for phrase-based models. A combined framework of such varying techniques can exploit the advantages of all of them while compensating for the weaknesses of each individual method. To accomplish this goal, a more detailed insight into the characteristics of each method may be useful. Towards this objective, we look for possible systematic differences between variants"
2012.amta-papers.27,J07-2003,0,0.659981,"systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and conclude. 2 Related Work DeNeefe et al. (2007) compared a string-to-tree model with a phrase-based model. While the syntaxbased model performed better than the phrase-based mo"
2012.amta-papers.27,D07-1079,0,0.278714,"rsity Dublin 9, Ireland Jennifer Foster† ‡ Symantec Research Labs Ballycoolin Business Park Blanchardstown, Dublin 15, Ireland firstname.lastname@dcu.ie firstname lastname@symantec.com Abstract trees on the source side (tree-to-string), target side (string-to-tree), or both (tree-to-tree). Utilisation of such linguistic generalisation, however, has proven to be a more complicated task than one might first imagine it to be. While relative improvements over phrase-based baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture"
2012.amta-papers.27,W11-2107,0,0.028467,"M 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-domain translation is challenging and that English-German translation is more difficult than English-Fr"
2012.amta-papers.27,W07-0732,0,0.0339778,"to investigate such behaviours, we select 100 sentences from each development set, and compare the outputs of two of the systems, namely HP and ST, for each of these sentences. 50 of the selected sentences are the solo-win cases of HP and the other 50 are those of ST (see section 5.3). The reason why these two systems are selected is that HP is the overall best performing system, and ST is the best performing syntax-based systems according to the various comparisons so far. Each data set was evaluated by a linguist using eight error categories. These categories are adapted from those used by Dugast et al. (2007) to evaluate post-editing changes. The evaluators were asked to count the number of errors in each output sentence under each category. While they were given the reference translation, they were not constrained to it and were allowed to compare against the closest correct translation to the output itself. We believe that this can better reflect the real performance of the systems, as it is not limited to a single reference, though we might lose some correlation with automatic metScore ties Real ties PB Any/Solo wins HP Any/Solo wins TS Any/Solo wins ST Any/Solo wins TT Any/Solo wins In-domain"
2012.amta-papers.27,I11-1100,1,0.84065,"Missing"
2012.amta-papers.27,W02-1039,0,0.0401634,"baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture long-distance reordering — a problem for phrase-based models. A combined framework of such varying techniques can exploit the advantages of all of them while compensating for the weaknesses of each individual method. To accomplish this goal, a more detailed insight into the characteristics of each method may be useful. Towards this objective, we look for possible systematic differences between variants of phrase-based and syntax-based systems via various ana"
2012.amta-papers.27,N04-1035,0,0.0395526,"ng syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and conclude. 2 Related Work DeNeefe et al. (2007) compared a string-to-tree model with a phrase-based model. While the syntaxbased model performed better than the phrase-based model on Chinese-to-English translation, it was shown to be worse on Arabic-to-English translation. They found that non-lexical rules form only a small fraction of the translation rule table in syntax-based modelling. The string-to-tree modelling in this w"
2012.amta-papers.27,2009.iwslt-papers.4,0,0.104625,"wever, for relatively monotonic pairs like ArabicEnglish, all models produced similar results. Experimenting with French-English, GermanEnglish and English-German, Auli et al. (2009) compared a phrase-based model to a hierarchical phrase-based model by exploring as much of the search space of both types of models as was computationally feasible. Given that the search spaces were very similar, they concluded that the differences between the two types of models can be explained by the way they score hypotheses rather than by the hypotheses they produce. Using the same framework as in this work, Hoang et al. (2009) compared phrase-based, hierarchical phrase-based and string-to-tree models. While the phrase-based and hierarchical phrase-based models achieved similar results, they both performed slightly better than the syntax-based model. They argue that, in order to improve syntax-based modelling, word alignment should be amended. There have been several efforts to exploit the difference between such models in MT system combination or multi-engine machine translation (MEMT) (Huang and Papineni, 2007). The task, however, has been shown to be difficult. Zwarts and Dras (2008) tried to identify what type o"
2012.amta-papers.27,D07-1029,0,0.020084,"ay they score hypotheses rather than by the hypotheses they produce. Using the same framework as in this work, Hoang et al. (2009) compared phrase-based, hierarchical phrase-based and string-to-tree models. While the phrase-based and hierarchical phrase-based models achieved similar results, they both performed slightly better than the syntax-based model. They argue that, in order to improve syntax-based modelling, word alignment should be amended. There have been several efforts to exploit the difference between such models in MT system combination or multi-engine machine translation (MEMT) (Huang and Papineni, 2007). The task, however, has been shown to be difficult. Zwarts and Dras (2008) tried to identify what type of sentence could be better translated by a syntax-based model compared to a phrase-based model. Using a classification approach, they separately tested three sets of features. Sentence length and system-internal features including decoder output score did not lead to an accurate classifier. They then hypothesised that noisy parse trees may impede the performance of the syntaxbased system and built another classifier based on source sentence length, parser confidence score, and linked fragme"
2012.amta-papers.27,2006.amta-papers.8,0,0.0223031,"y translated into French using an online translation tool and then post-edited by human translators. 4. German forum data: 1,500 sentences taken from the Symantec English online forums, split into development (600) and test (900). These were automatically translated into German using an online translation tool and then postedited by human translators. Baseline Systems We train the following five statistical machine translation systems: 1. PB: a standard phrase-based system (Och and Ney, 2004) 2. HP: a hierarchical phrase-based system (Chiang, 2007) 3. TS: a tree-to-string syntax-based system (Huang et al., 2006). System Comparison 5.1 Multiple Metrics In order to carry out a reliable comparison, we evaluate the baseline systems at the document level using 1 We used the SAMT-2 parse relaxation method. The two parsers achieve Parseval labelled f-scores in the 89-90 range on Section 23 of the Wall Street Journal section of the Penn Treebank. Due to some character encoding issue, our own parser could not be used to parse the German data and this is why the Berkeley parser is used instead. 2 PB HP TS ST TT Oracle 1-best Oracle 500-best BLEU 0.6140 0.6188 0.5919 0.6013 0.5783 0.6658 0.7770 NIST 10.73 10.78"
2012.amta-papers.27,H94-1020,0,0.0570017,"er (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al., 2003) for training the French model and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1994) for training the English model.2 4 5 1. French translation memory: 5,000 held-out sentences from the Symantec English-French translation memory, split into development (2000) and test (3000). 2. German translation memory: 5,000 held-out sentences from the Symantec English-German translation memory, split into development (2000) and test (3000) 3. French forum data: 1,500 sentences taken from the Symantec English online forums, split into development (600) and test (900). These were automatically translated into French using an online translation tool and then post-edited by human translators."
2012.amta-papers.27,P08-1114,0,0.0468741,"Missing"
2012.amta-papers.27,J04-4002,0,0.487368,"n translation rule coverage means that these models do not necessarily generate output which is more grammatical than the output produced by the phrase-based models. Although the systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and c"
2012.amta-papers.27,P03-1021,0,0.0325614,"luding a maximum phrase length of 7 and a decoder distortion limit of 6 when applied. The HP system was trained using the default settings including a maximum chart span of 20. The same chart span was used for the TS,ST and TT systems. To relax the strict constraint on rule extraction in the TS, ST and TT systems, any pairs of adjacent nodes in the parse tree are combined together to form new nodes (Zollmann et al., 2008)1 . This significantly increases the number of extracted rules and consequently the translation accuracy. All five systems are tuned using minimum error rate training (MERT) (Och, 2003) on the respective developments sets. We use our in-house C++ implementation of a PCFG-LA parser (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al.,"
2012.amta-papers.27,P02-1040,0,0.0858599,".90 7.04 6.71 6.75 6.68 7.40 8.25 En-Fr TER 0.6024 0.5904 0.6118 0.6057 0.6121 0.5408 0.4717 GTM 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-doma"
2012.amta-papers.27,N07-1051,0,0.0158851,"e number of extracted rules and consequently the translation accuracy. All five systems are tuned using minimum error rate training (MERT) (Och, 2003) on the respective developments sets. We use our in-house C++ implementation of a PCFG-LA parser (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al., 2003) for training the French model and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1994) for training the English model.2 4 5 1. French translation memory: 5,000 held-out sentences from the Symantec English-French translation memory, split into development (2000) and test (3000). 2. German translation memory: 5,000 held-out sentences from the Symantec English-German translation memory, split into development (2000) and test (3000) 3. French fo"
2012.amta-papers.27,P06-1055,0,0.0355118,"rm new nodes (Zollmann et al., 2008)1 . This significantly increases the number of extracted rules and consequently the translation accuracy. All five systems are tuned using minimum error rate training (MERT) (Och, 2003) on the respective developments sets. We use our in-house C++ implementation of a PCFG-LA parser (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al., 2003) for training the French model and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1994) for training the English model.2 4 5 1. French translation memory: 5,000 held-out sentences from the Symantec English-French translation memory, split into development (2000) and test (3000). 2. German translation memory: 5,000 held-out sentences from the Symantec English-German tran"
2012.amta-papers.27,W06-1608,0,0.065892,"Missing"
2012.amta-papers.27,W12-3117,1,0.827502,"the gain to be achieved by combining outputs. Manual analysis of the outputs and translation process showed that there was no obvious systematic difference between syntax-based and non-syntaxbased modelling, mostly due to the relaxation of syntactic constraints on translation rule extraction. This makes it difficult to find features to be exploited in combining these models, despite the potential gain which was observed in their oracle combination. In the future, we hope to perform successful system combination by exploring the space of features used in our recent work on quality estimation (Rubino et al., 2012). Another avenue for future work is to focus on improving parser accuracy on our datasets by leveragSource Reference HP output If you choose to continue, you will need to set the options manually from the Altiris eXpress Deployment Server Configuration control panel applet. Si vous d´ecidez de continuer, vous devrez configurer les options manuellement a` partir de l’applet du panneau de configuration Altiris eXpress Deployment Server. Si vous d´ecidez de continuer, vous devrez configurer les options manuellement a` partir de l’applet Altiris eXpress Deployment Server Configuration Control Pane"
2012.amta-papers.27,2006.amta-papers.25,0,0.267766,"n-Fr TER 0.6024 0.5904 0.6118 0.6057 0.6121 0.5408 0.4717 GTM 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-domain translation is challenging and"
2012.amta-papers.27,2003.mtsummit-papers.51,0,0.0604137,"18 0.6057 0.6121 0.5408 0.4717 GTM 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-domain translation is challenging and that English-German transl"
2012.amta-papers.27,P98-2230,0,0.0871534,"he syntax-based methods underperform the phrase-based models and the relaxation of syntactic constraints to broaden translation rule coverage means that these models do not necessarily generate output which is more grammatical than the output produced by the phrase-based models. Although the systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is"
2012.amta-papers.27,P01-1067,0,0.107206,"hods underperform the phrase-based models and the relaxation of syntactic constraints to broaden translation rule coverage means that these models do not necessarily generate output which is more grammatical than the output produced by the phrase-based models. Although the systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the mod"
2012.amta-papers.27,P08-1064,0,0.0504096,"Missing"
2012.amta-papers.27,W06-3119,0,0.0357834,"e in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and conclude. 2 Related Work DeNeefe et al. (2007) compared a string-to-tree model with a phrase-based model. While the syntaxbased model performed better than the phrase-based model on Chinese-to-English translation, it was shown to be worse on Arabic-to-English translation. They found that non-lexical rules form only a small fraction of the translation rule table in syntax-based modelling. The string-to-tree modelling in this work is based on their approach."
2012.amta-papers.27,C08-1144,0,0.244566,"d Jennifer Foster† ‡ Symantec Research Labs Ballycoolin Business Park Blanchardstown, Dublin 15, Ireland firstname.lastname@dcu.ie firstname lastname@symantec.com Abstract trees on the source side (tree-to-string), target side (string-to-tree), or both (tree-to-tree). Utilisation of such linguistic generalisation, however, has proven to be a more complicated task than one might first imagine it to be. While relative improvements over phrase-based baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture long-distance reordering"
2012.amta-papers.27,C98-2225,0,\N,Missing
2012.amta-papers.27,D08-1076,0,\N,Missing
2012.amta-papers.27,P08-1000,0,\N,Missing
2012.eamt-1.55,2011.mtsummit-papers.35,0,0.48897,"Missing"
2012.eamt-1.55,W07-0732,0,0.187808,"Missing"
2012.eamt-1.55,W09-0419,0,0.343315,"Missing"
2012.eamt-1.55,W08-0509,0,0.0282299,"s and the most frequent words from the out-of-domain corpora. Let us note that a high weight of 0.9 is associated with the medical LM despite its small size, which is explained by the great specificity of the medical domain. Three Translation Models (TMs) incorporating a phrase table and a lexicalized reordering model are also built using M OSES: one (T Mg ) from the out-of-domain data, one (T Mm ) from the medical set and a last one (T Mg+m ) from all the parallel corpora. For that purpose, bilingual data are aligned at the word level using the IBM 4 model (Och and Ney, 2003) with MG IZA ++ (Gao and Vogel, 2008) . The score weights of a given TM and a selected LM are finally computed in each tested configuration using the Minimum Error Rate Training (MERT) method (Och, 2003) to optimize BLEU on the EMEA development corpus. To mix the information from the out and in-domain in T Mg+m , we resorted to the multiple translation tables option implemented into M OSES. With this feature, we can provide two translation tables to 224 the decoder; the decoder first retrieves translation pairs from the in-domain phrase table, and resorts to the out-of-domain phrase-table as a fall-back. 3.3 SPE Systems In order"
2012.eamt-1.55,2007.mtsummit-papers.34,0,0.399842,"Missing"
2012.eamt-1.55,N03-1017,0,0.0336377,"machine translation system can be adapted a posteriori to a specific domain. Two SMT systems are studied: a state-of-the-art phrasebased implementation and an online publicly available system. Our experiments also indicate that selecting sentences for post-editing leads to significant improvements of translation quality and that more gains are still possible with respect to an oracle measure. 1 Introduction Phrase-Based Machine Translation (PBMT) is a popular approach to Statistical Machine Translation (SMT) that leads to accurate translation results (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003). The statistical models used in PBMT are based on the probabilities of bidirectional alignment of phrases between two sentences in the translation relation. The linguistic resources used to estimate such probabilities are parallel corpora and the main resulting statistical model is a translation table. Therefore, parallel corpora are the cornerstone for high quality translation. However, such resources are expensive to construct. c 2012 European Association for Machine Translation. 221 This lack of parallel data still remains an issue in PBMT. This phenomenon is accentuated by the diversity o"
2012.eamt-1.55,P07-2045,0,0.0152324,"f a SVM classifier using a translated corpus where each sentence is associated with its ∆BLEU class. composed of a phrase-based SPE system and a sentence-level automatic quality estimator based on Partial Least Squares. 3 Experimental Setup In brief, the general idea of the work presented in this paper is to increase the quality of in-domain translations, generated by an out-of-domain SMT system, through a post-editing step. In order to thoroughly evaluate our approach, two SMT systems are considered to translate from the source language to the target language: the M OSES PBMT implementation (Koehn et al., 2007) and the G OOGLE T RANSLATE online system1 . The post-editing step is then performed using M OSES in both cases. The latter case (the online system) will help to justify our approach showing that a powerful yet fixed MT system can be profitably combined with a system trained on a small set of in-domain data. The approach is evaluated at two levels: first, we evaluate the accuracy of each translation system on a domain specific translation task. Second, we focus on the use of SPE systems to process each translation system output. Section 3.1 introduces the out and in-domain data used in our exp"
2012.eamt-1.55,2005.mtsummit-papers.11,0,0.151206,"at two levels: first, we evaluate the accuracy of each translation system on a domain specific translation task. Second, we focus on the use of SPE systems to process each translation system output. Section 3.1 introduces the out and in-domain data used in our experiments. These data can be combined in different ways inside LMs and TMs; the resulting translation systems are described in Section 3.2. Then, Section 3.3 provides information about our SPE models. 3.1 Resources Out-of-domain data are presented in Table 1. The bilingual parallel corpora are the sixth version of the Europarl corpus (Koehn, 2005) and the United Nations corpus (Rafalovitch and Dale, 2009). The 1 http://translate.google.com/ 223 monolingual corpora are composed of the target language part of the sixth version of the News Commentary corpus taken from the Project Syndicate website2 , and the Shuffled News Crawl corpus. All these corpora were made available for the 2011 Workshop on Machine Translation (WMT11)3 . The bilingual data are used to build translation models, whereas the monolingual data are employed to train language models. Corpus Sentences Words Bilingual Training Data Europarl v6 1.8 M 50 M United Nations 12 M"
2012.eamt-1.55,W02-1018,0,0.0404641,"that an out-of-domain machine translation system can be adapted a posteriori to a specific domain. Two SMT systems are studied: a state-of-the-art phrasebased implementation and an online publicly available system. Our experiments also indicate that selecting sentences for post-editing leads to significant improvements of translation quality and that more gains are still possible with respect to an oracle measure. 1 Introduction Phrase-Based Machine Translation (PBMT) is a popular approach to Statistical Machine Translation (SMT) that leads to accurate translation results (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003). The statistical models used in PBMT are based on the probabilities of bidirectional alignment of phrases between two sentences in the translation relation. The linguistic resources used to estimate such probabilities are parallel corpora and the main resulting statistical model is a translation table. Therefore, parallel corpora are the cornerstone for high quality translation. However, such resources are expensive to construct. c 2012 European Association for Machine Translation. 221 This lack of parallel data still remains an issue in PBMT. This phenomenon is accentuat"
2012.eamt-1.55,J03-1002,0,0.00747818,"ll the words of the in-domain corpus and the most frequent words from the out-of-domain corpora. Let us note that a high weight of 0.9 is associated with the medical LM despite its small size, which is explained by the great specificity of the medical domain. Three Translation Models (TMs) incorporating a phrase table and a lexicalized reordering model are also built using M OSES: one (T Mg ) from the out-of-domain data, one (T Mm ) from the medical set and a last one (T Mg+m ) from all the parallel corpora. For that purpose, bilingual data are aligned at the word level using the IBM 4 model (Och and Ney, 2003) with MG IZA ++ (Gao and Vogel, 2008) . The score weights of a given TM and a selected LM are finally computed in each tested configuration using the Minimum Error Rate Training (MERT) method (Och, 2003) to optimize BLEU on the EMEA development corpus. To mix the information from the out and in-domain in T Mg+m , we resorted to the multiple translation tables option implemented into M OSES. With this feature, we can provide two translation tables to 224 the decoder; the decoder first retrieves translation pairs from the in-domain phrase table, and resorts to the out-of-domain phrase-table as a"
2012.eamt-1.55,P03-1021,0,0.048971,"by the great specificity of the medical domain. Three Translation Models (TMs) incorporating a phrase table and a lexicalized reordering model are also built using M OSES: one (T Mg ) from the out-of-domain data, one (T Mm ) from the medical set and a last one (T Mg+m ) from all the parallel corpora. For that purpose, bilingual data are aligned at the word level using the IBM 4 model (Och and Ney, 2003) with MG IZA ++ (Gao and Vogel, 2008) . The score weights of a given TM and a selected LM are finally computed in each tested configuration using the Minimum Error Rate Training (MERT) method (Och, 2003) to optimize BLEU on the EMEA development corpus. To mix the information from the out and in-domain in T Mg+m , we resorted to the multiple translation tables option implemented into M OSES. With this feature, we can provide two translation tables to 224 the decoder; the decoder first retrieves translation pairs from the in-domain phrase table, and resorts to the out-of-domain phrase-table as a fall-back. 3.3 SPE Systems In order to build the SPE system for domain adaptation, we decide to translate the EMEA training corpus with each tested SMT system. Then, with the output of each system align"
2012.eamt-1.55,W07-0704,0,0.496651,"Missing"
2012.eamt-1.55,P02-1040,0,0.0917527,"nslation outputs to specific domains. 2.2 SPE for Adaptation The research presented in this paper addresses the issue of adapting an out-of-domain machine translation system using a small in-domain bilingual parallel corpus. We study various uses of out and in-domain data to build Language Models (LMs) and Translation Models (TMs) inside the sourceto-target language PBMT. Then, we evaluate the post-editing model using out and in-domain data to build LMs and in-domain data only for the SPE model. We also describe a new method to select sentences using classifiers built with the BLEU criterion (Papineni et al., 2002). Figure 1 illustrates the general architecture of our experimental setup, described in the next section. The source language part of the in-domain parallel corpus is first translated into the target language by an SMT system. Then, the generated translation hypotheses are aligned with their translation references in order to form a monolingual parallel corpus and to build a SPE model. When a test corpus is translated and has to be post-edited, we propose two different approaches. The first one is a naive application of SPE which post-edits all the sentences of the test corpus. The second one"
2012.eamt-1.55,2009.mtsummit-posters.15,0,0.0397605,"cy of each translation system on a domain specific translation task. Second, we focus on the use of SPE systems to process each translation system output. Section 3.1 introduces the out and in-domain data used in our experiments. These data can be combined in different ways inside LMs and TMs; the resulting translation systems are described in Section 3.2. Then, Section 3.3 provides information about our SPE models. 3.1 Resources Out-of-domain data are presented in Table 1. The bilingual parallel corpora are the sixth version of the Europarl corpus (Koehn, 2005) and the United Nations corpus (Rafalovitch and Dale, 2009). The 1 http://translate.google.com/ 223 monolingual corpora are composed of the target language part of the sixth version of the News Commentary corpus taken from the Project Syndicate website2 , and the Shuffled News Crawl corpus. All these corpora were made available for the 2011 Workshop on Machine Translation (WMT11)3 . The bilingual data are used to build translation models, whereas the monolingual data are employed to train language models. Corpus Sentences Words Bilingual Training Data Europarl v6 1.8 M 50 M United Nations 12 M 300 M EMEA (Medical) 160 k 4M Monolingual Training Data Ne"
2012.eamt-1.55,W05-0822,0,0.163048,"Missing"
2012.eamt-1.55,N07-1064,0,0.487571,"Missing"
2012.eamt-1.55,W07-0728,0,0.0501105,"Missing"
2012.eamt-1.55,2006.amta-papers.25,0,0.326878,"Missing"
2012.eamt-1.55,2007.tmi-papers.27,0,0.0346768,"Missing"
2012.eamt-1.55,2011.mtsummit-papers.16,0,0.0286281,"Missing"
2012.eamt-1.55,2002.tmi-tutorials.2,0,0.0391721,"nch to English show that an out-of-domain machine translation system can be adapted a posteriori to a specific domain. Two SMT systems are studied: a state-of-the-art phrasebased implementation and an online publicly available system. Our experiments also indicate that selecting sentences for post-editing leads to significant improvements of translation quality and that more gains are still possible with respect to an oracle measure. 1 Introduction Phrase-Based Machine Translation (PBMT) is a popular approach to Statistical Machine Translation (SMT) that leads to accurate translation results (Zens et al., 2002; Marcu and Wong, 2002; Koehn et al., 2003). The statistical models used in PBMT are based on the probabilities of bidirectional alignment of phrases between two sentences in the translation relation. The linguistic resources used to estimate such probabilities are parallel corpora and the main resulting statistical model is a translation table. Therefore, parallel corpora are the cornerstone for high quality translation. However, such resources are expensive to construct. c 2012 European Association for Machine Translation. 221 This lack of parallel data still remains an issue in PBMT. This p"
2013.mtsummit-papers.13,D11-1033,0,0.549609,"Automatic quality estimation is used to identify such poorly translated sentences in the target domain. Our experiments reveal that this approach provides statistically significant improvements over the unadapted baseline and achieves comparable scores to that of conventional data selection approaches with significantly smaller amounts of selected data. 1 Introduction The quality of translations generated by a statistical machine translation (SMT) system depends heavily on the amount of available parallel training data, as well as on the domain-specificity of the training and target datasets (Axelrod et al., 2011). Real-life translation tasks are usually domain-specific in nature and require large volumes of in-domain parallel training data. However, such domain-specific parallel training data is often sparse or completely unavailable. In such scenarios, domain adaptation techniques are necessary to effectively leverage available out-ofdomain or related-domain parallel data. Supplementary data selection (Hildebrand et al., 2005; Axelrod et al., 2011) is one such popular technique which uses out-of-domain parallel data to supplement sparse in-domain data. However, combining lots of out-of-domain data wi"
2013.mtsummit-papers.13,W05-0909,0,0.023065,"selected from this English forum data and manually translated by professional translators. Table 1 reports the statistics on all the datasets used in all our experiments. The SMT system used in our experiments is based on the standard phrase-based SMT toolkit: Moses (Koehn et al., 2007). The feature weights are tuned using Minimum Error Rate Training (Och, 2003) on the devset. All the LMs in our experiments are created using the IRSTLM (Federico et al., 2008) language modelling toolkit. Finally, translations of the test sets in every phase of our experiments are evaluated using BLEU, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The classification and regression models used in the QE component of our approach are based on Support Vector Machines (SVMs) (Joachims, 1999) using Radial Basis function (RBF) kernels. We use the LibSVM toolkit:5 a free open source implementation of the technology, for all our classification/regression model training and predictions. In order to tune the features of the SVMbased classification and regression models the grid search functionality associated with LibSVM is used. The process of feature extraction is performed using an inhouse tool. http://ww"
2013.mtsummit-papers.13,C12-1010,1,0.840576,"Missing"
2013.mtsummit-papers.13,W12-3102,0,0.185957,"propriate use-case for our approach. The rest of paper is organised as follows: Section 2 presents related work relevant to our approach. Section 3 details the QE and data selection methods. Section 4 presents the experimental setup and results followed by discussions and conclusions in Section 5 and 6, respectively. 2 Related Work QE for SMT was first applied at the wordlevel (Ueffing et al., 2003) and then extended to the sentence-level (Blatz et al., 2003). More recently, several studies have focused on using human scores to evaluate the translation quality in terms of post-editing effort (Callison-Burch et al., 2012) or translation adequacy (Specia et al., 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data selection. In order to select supplementary out-of-domain data relevant to the target domain, a variety of criteria have been explored in the MT literature, ranging from information retrieval techniques (Hildebrand et al., 2005)"
2013.mtsummit-papers.13,W07-0717,0,0.425072,", 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data selection. In order to select supplementary out-of-domain data relevant to the target domain, a variety of criteria have been explored in the MT literature, ranging from information retrieval techniques (Hildebrand et al., 2005) to perplexity on ‘in-domain’ datasets (Foster and Kuhn, 2007). Axelrod et al. (2011) presented a technique using the bilingual difference of cross-entropy on ‘in-domain’ and ‘out-of-domain’ language models for ranking and selection by thresholding, which outperformed the monolingual perplexity based techniques. More recently, Banerjee et al. (2012) presented a novel translation-quality evaluation (rather than prediction) based data selection technique using an incremental translation model merging approach. While all these approaches select data with respect to the entire available target domain data, our approach uses only a sub-part of the same compri"
2013.mtsummit-papers.13,2005.eamt-1.19,0,0.211678,"tical machine translation (SMT) system depends heavily on the amount of available parallel training data, as well as on the domain-specificity of the training and target datasets (Axelrod et al., 2011). Real-life translation tasks are usually domain-specific in nature and require large volumes of in-domain parallel training data. However, such domain-specific parallel training data is often sparse or completely unavailable. In such scenarios, domain adaptation techniques are necessary to effectively leverage available out-ofdomain or related-domain parallel data. Supplementary data selection (Hildebrand et al., 2005; Axelrod et al., 2011) is one such popular technique which uses out-of-domain parallel data to supplement sparse in-domain data. However, combining lots of out-of-domain data with small amounts of in-domain data might negatively affect translation quality by overwhelming the in-domain characteristics. Hence relevant data selection is used, where only a sub-part of the out-of-domain data, relevant to the target domain, supplements the sparse indomain training data. Conventionally, the data selection process is guided by all available monolingual (or bilingual) target-domain data. Sentence pair"
2013.mtsummit-papers.13,P07-1034,0,0.0456762,"rc00 (s)] + [P Pitrg (s) − P Potrg (s)] (3) where P Pisrc0 indicates the perplexity on the indomain LM trained only on the source-side of the poorly translated sentences while P Posrc +isrc00 refers to the LM trained on the remaining targetdomain data and out-of-domain data. Note that the target side of the scoring remains the same, as there is no notion of good or bad translations in the target side of the bitext data. 3.3 Data Combination Multiple techniques exist in the SMT literature to combine out-of-domain data with in-domain data. The combination could be done using instance weighting (Jiang and Zhai, 2007), or by linearly interpolating the phrase tables (Foster and Kuhn, 2007). Considering the success of linear interpolation outperforming the other techniques (Sennrich, 2012), we choose this technique to combine the two datasets. In order to learn the interpolation weights, LMs are constructed on the target side of the in-domain training set and the selected supplementary data. 1 As cross-entropy and perplexity are monotonically related, they produce the same ranking. These LMs are then interpolated using expectation maximisation on the target side of the devset to learn the optimal mixture wei"
2013.mtsummit-papers.13,P07-2045,0,0.00407217,"et. Figure 1 shows the variation of F1 scores for different values of the prediction threshold, and our choice of threshold value of 0.4 corresponding to the best F1 score. Accuracy/F1 Score data selection and in determining the set of potentially poorly translated sentences in the targetdomain. The dev and test sets are randomly selected from this English forum data and manually translated by professional translators. Table 1 reports the statistics on all the datasets used in all our experiments. The SMT system used in our experiments is based on the standard phrase-based SMT toolkit: Moses (Koehn et al., 2007). The feature weights are tuned using Minimum Error Rate Training (Och, 2003) on the devset. All the LMs in our experiments are created using the IRSTLM (Federico et al., 2008) language modelling toolkit. Finally, translations of the test sets in every phase of our experiments are evaluated using BLEU, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The classification and regression models used in the QE component of our approach are based on Support Vector Machines (SVMs) (Joachims, 1999) using Radial Basis function (RBF) kernels. We use the LibSVM toolkit:5 a free ope"
2013.mtsummit-papers.13,W04-3250,0,0.361562,"Missing"
2013.mtsummit-papers.13,2005.mtsummit-papers.11,0,0.071804,"hts are subsequently used to combine the individual feature values for every phrase pair from two phrase-tables using a weighted linear interpolation scheme. For the LMs, individual models trained on the in-domain and selected out-ofdomain datasets are interpolated in a similar fashion with interpolation weights set on the devset. 4 Experimental Setup 4.1 Datasets and Tools The primary in-domain training data for our baseline systems comprises En–Fr bilingual datasets from Symantec TMs. Considering the wider vocabulary of the forum content, we use the freely available Europarl (EP) version 6 (Koehn, 2005) and News Commentary (NC)2 datasets in combination with the Symantec TMs to create a stronger second baseline model. We then use the following two freely available parallel datasets from the web, as the supplementary resources for data selection experiments: • OpenSubtitles2011 (OPS) Corpus3 . • MultiUN (UN) Parallel Corpus4 Data Set Line Cnt. En. Token Fr. Token Symantec TM 3,659,455 72,604,817 82,046,300 Bi- Europarl 1,924,594 52,139,148 57,837,037 text News-Comm. 134,757 3,338,552 3,917,982 1,692 22,661 25,840 Data Dev Test 1,032 13,160 15,164 Supp.MultiUN 9,010,933 227,085,145 263,051,365"
2013.mtsummit-papers.13,P03-1021,0,0.0102979,"hreshold, and our choice of threshold value of 0.4 corresponding to the best F1 score. Accuracy/F1 Score data selection and in determining the set of potentially poorly translated sentences in the targetdomain. The dev and test sets are randomly selected from this English forum data and manually translated by professional translators. Table 1 reports the statistics on all the datasets used in all our experiments. The SMT system used in our experiments is based on the standard phrase-based SMT toolkit: Moses (Koehn et al., 2007). The feature weights are tuned using Minimum Error Rate Training (Och, 2003) on the devset. All the LMs in our experiments are created using the IRSTLM (Federico et al., 2008) language modelling toolkit. Finally, translations of the test sets in every phase of our experiments are evaluated using BLEU, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The classification and regression models used in the QE component of our approach are based on Support Vector Machines (SVMs) (Joachims, 1999) using Radial Basis function (RBF) kernels. We use the LibSVM toolkit:5 a free open source implementation of the technology, for all our classification/regress"
2013.mtsummit-papers.13,W12-5706,1,0.884336,"Missing"
2013.mtsummit-papers.13,2012.eamt-1.55,1,0.89222,"Missing"
2013.mtsummit-papers.13,2012.eamt-1.43,0,0.0179014,"osrc +isrc00 refers to the LM trained on the remaining targetdomain data and out-of-domain data. Note that the target side of the scoring remains the same, as there is no notion of good or bad translations in the target side of the bitext data. 3.3 Data Combination Multiple techniques exist in the SMT literature to combine out-of-domain data with in-domain data. The combination could be done using instance weighting (Jiang and Zhai, 2007), or by linearly interpolating the phrase tables (Foster and Kuhn, 2007). Considering the success of linear interpolation outperforming the other techniques (Sennrich, 2012), we choose this technique to combine the two datasets. In order to learn the interpolation weights, LMs are constructed on the target side of the in-domain training set and the selected supplementary data. 1 As cross-entropy and perplexity are monotonically related, they produce the same ranking. These LMs are then interpolated using expectation maximisation on the target side of the devset to learn the optimal mixture weights. These weights are subsequently used to combine the individual feature values for every phrase pair from two phrase-tables using a weighted linear interpolation scheme."
2013.mtsummit-papers.13,2006.amta-papers.25,0,0.466665,"hree individual components involved in our approach. 3.1 Automatic Quality Estimation To distinguish between the good and the bad translations of the target-domain (English forum data in our context), we experimented with both classification as well as regression-based QE approaches. For both sets of experiments, we extract 17 features similar to the baseline QE setup 102 suggested by the organisers of the WMT12 shared task (Callison-Burch et al., 2012), which were shown to perform well on a post-editing effort prediction task. In our study, we want to predict the Translation Edit Rate (TER) (Snover et al., 2006) to spot bad translations. Given the TER scores for a set of translations, identifying the bad translations requires a threshold value, such that all sentences having TER scores above this threshold would be labelled as bad translation. However, a translation with a low TER score may still be considered bad since TER does not incorporate the notion of semantic equivalence (Snover et al., 2006). To set the value of this threshold, we selected two sets of 50 sentences randomly from our QE En–Fr training data such that there was an overlap of 10 sentences in each. These sentences along with their"
2013.mtsummit-papers.13,2011.mtsummit-papers.58,0,0.0250412,"is organised as follows: Section 2 presents related work relevant to our approach. Section 3 details the QE and data selection methods. Section 4 presents the experimental setup and results followed by discussions and conclusions in Section 5 and 6, respectively. 2 Related Work QE for SMT was first applied at the wordlevel (Ueffing et al., 2003) and then extended to the sentence-level (Blatz et al., 2003). More recently, several studies have focused on using human scores to evaluate the translation quality in terms of post-editing effort (Callison-Burch et al., 2012) or translation adequacy (Specia et al., 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data selection. In order to select supplementary out-of-domain data relevant to the target domain, a variety of criteria have been explored in the MT literature, ranging from information retrieval techniques (Hildebrand et al., 2005) to perplexity on ‘in-domain’ datasets (Foster"
2013.mtsummit-papers.13,2003.mtsummit-papers.52,0,0.0406122,"ata is often noisy, usergenerated and has a wider vocabulary and colloquialisms. This difference between the training and target datasets necessitates the use of supplementary data for adaptation, thus making this an appropriate use-case for our approach. The rest of paper is organised as follows: Section 2 presents related work relevant to our approach. Section 3 details the QE and data selection methods. Section 4 presents the experimental setup and results followed by discussions and conclusions in Section 5 and 6, respectively. 2 Related Work QE for SMT was first applied at the wordlevel (Ueffing et al., 2003) and then extended to the sentence-level (Blatz et al., 2003). More recently, several studies have focused on using human scores to evaluate the translation quality in terms of post-editing effort (Callison-Burch et al., 2012) or translation adequacy (Specia et al., 2011). The promising results obtained in QE lead to interesting applications in MT, such as sentenceselection for statistical post-editing (Rubino et al., 2012) or system combination (Okita et al., 2012). In this paper, we apply QE techniques to identify bad translations from the target domain to drive domain adaptation by data sel"
2013.mtsummit-posters.12,O10-3002,0,0.0338159,"Missing"
2013.mtsummit-posters.13,P11-1022,0,0.0216977,"ddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). For an overview on various feature sets and machine learning algorithms, we refer the reader to the recent shared-task on the topic (Callison-Burch et al., 2012). Most QE work focuses on estimating a score that indicates overall quality having professional translators as intended user, e.g. post-editing effort. Little work has been done for other applications of MT, such as gisting. One notable exception is the work by Specia et al. (2011), where adequacy scores for Arabic-English translations are predicted. The feature set used include standard features that try to capture general aspects o"
2013.mtsummit-posters.13,C04-1046,0,0.238333,"robust than those used in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy. 1 Introduction Quality estimation (QE) for machine translation (MT) is an area concerned with predicting a quality indicator for an automatically translated text without referring to human translations (the so-called reference translations typical of most MT evaluation metrics) (Blatz et al., 2004; Specia et al., 2009). The widespread use of MT in the translation industry has strongly motivated work in this area. As a consequence, the majority of existing work focuses on predicting some form of post-editing effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features use"
2013.mtsummit-posters.13,W10-1703,0,0.0187796,"est) to 5 scale (lowest). The three scores per segment pair are weighted and averaged in order to obtain one continuous score (∈ [1; 5]). From this dataset, 1, 832 segments are used to train the QE models while 422 segments are used as a test set. To build the topic models, we use the parallel corpora used by the M OSES system which generated the Spanish translations. This 2 3 http://www.tausdata.org/ http://www.bing.com/translator/ 298 corpus contains the concatenation of Europarl v5 and the News Commentary corpus from WMT10 translation task in English and Spanish (∼ 1.7M translation pairs) (Callison-Burch et al., 2010). 4.2 Additional Features In addition to the TM features described in Section 3, for all datasets, for comparison we consider a baseline set of 17 features that performed well across languages in previous work and were used as the official baseline in the WMT12 QE task (Callison-Burch et al., 2012): – number of tokens in the source & target sentences; – average source token length; – average number of occurrences of the target word within the target sentence; – number of punctuation marks in source and target sentences; – language model (LM) probability of source and target sentences using 3-g"
2013.mtsummit-posters.13,W12-3102,1,0.866279,"Missing"
2013.mtsummit-posters.13,P10-1064,0,0.0295252,"Missing"
2013.mtsummit-posters.13,P07-2045,0,0.00602471,"n of all Arabic-English newswire parallel data provided by LDC and the Arabic-English UN data,1 totalling ∼ 6.4M translation pairs after removing sentences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3"
2013.mtsummit-posters.13,C10-1070,0,0.0138361,"n 2 2 i=1 (ui ) × i=1 (vi ) Instead of measuring the distance between two n-dimensional sets of points, it is also possible to compute the sum of the absolute differences of their coordinates. These metrics, usually categorised as the Minkowski family of distance metrics, are inspired by a grid, city-like, organisation of the space. Usually referred as rectilinear or Manhattan distance, the city-block distance is directly inspired by the Euclidean distance (Krause, 1975) and has shown interesting results when applied to language processing tasks, such as context-based terminology translation (Laroche and Langlais, 2010). The city-block distance between two n-dimensional vectors u and v is given by (3): n X cityblock(u, v) = |ui − vi | (3) i=1 These three metrics allow us to compute the distance between source and target distributions assuming that they are represented in an Euclidean space. To avoid this constraint, we use two other measures in this study, based on probabilistic uncertainty as introduced by Shannon’s work (Shannon, 1948). With the measure of relative entropy, an asymmetric way of comparing two distributions suggested in (Kullback and Leibler, 1951) is given by (4): n X ui KL(u, v) = ui ln (4"
2013.mtsummit-posters.13,W12-3122,0,0.354907,"professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general aspects of quality are different from features that define adequacy. Previous work on adequacy estimation has focused on linguistic features contrasting the source and translation texts (Specia et al., 2011; Mehdad et al., 2012), e.g. the proportion of overlapping typed dependency relations in the source and target sentences with arguments that align to each other (based on wordalignment information). While these can provide interesting indicators, they are often very sparse and noisy. Sparsity happens because many of these features do not apply to most sentences, such as features comparing named entities in the source and target sentences. A significant amount of noise can come from the fact that linguistic processors, such as syntactic parsers and named entity recognisers, need to be applied to potentially low-qual"
2013.mtsummit-posters.13,D09-1092,0,0.10337,"ose used in previous work in that they focus on important (content) words in the source and target texts, as opposed to more abstract linguistic relationships between these words. We believe these are more robust as they do not depend on further analysis, and that they can be made less sparse through the exploitation of models with different dimensionalities and the use of distance metrics between topic distributions as opposed to topic distributions themselves. A challenge we face is how to model topics in a bilingual setting. We exploit two variants of TMs for that: Polylingual Topic Model (Mimno et al., 2009) and Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 295–302. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. a joint Latent Dirichlet Allocation approach (Blei et al., 2003), and a few variants of features based on these models, including the word distribution themselves and distance metrics between source and target distributions (Section 3). We experiment with three families of datasets: two annotated for ad"
2013.mtsummit-posters.13,P03-1021,0,0.00239403,"ntences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3 accessed through the second version of their API. Finally, the last system is the Systran Enterprise Server version 6, customised with the"
2013.mtsummit-posters.13,P02-1040,0,0.107966,"translation pairs after removing sentences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3 accessed through the second version of their API. Finally, the last system is the Systran Enterprise Server version"
2013.mtsummit-posters.13,2011.mtsummit-papers.27,0,0.0889134,"mised with the use of a domain specific 10K+ dictionary entries. Each translation is evaluated by a professional translator using two possible labels: 0 if the translation does not preserve the meaning of the source sentence and 1 if the meaning is preserved. The final dataset contains, for each of the three translations generated by the three MT systems, 694 source segments, 694 translated segments, and one adequacy score. From this dataset, 500 segments are used to train the QE models and 194 segments are held out for evaluation purposes. More information about this dataset can be found in (Roturier and Bensadoun, 2011). To build the topic models for the adequacy features, we use the in-domain Translation Memories used to train the M OSES system. WMT12 QE Data The WMT12 QE dataset (Callison-Burch et al., 2012) is composed of 2, 254 English sentences translated into Spanish by a M OSES phrase-based system and evaluated by three professional translators in terms of postediting effort on a 1 (highest) to 5 scale (lowest). The three scores per segment pair are weighted and averaged in order to obtain one continuous score (∈ [1; 5]). From this dataset, 1, 832 segments are used to train the QE models while 422 seg"
2013.mtsummit-posters.13,W12-3117,1,0.696965,"11), extracted from both source and target sentences, e.g. common dependency relations in source and target sentences. Both previous approaches for adequacy estimation severely suffer from data sparsity while attempting to model contrastive linguistic information between source and target sentences. As a consequence, the reported results are poor, sometimes even below simple baselines such as the majority class on the training data. None of the previous work uses lexicalised features or topic models built based on those features for adequacy estimation. As we will discuss in the next section, Rubino et al. (2012) used topic models as part of a larger feature set to estimate post-editing effort. However, the contribution of this information source was not tested. 3 Topic Models for Quality Estimation The first study on using topic modelling for QE was conducted in (Rubino et al., 2012) as part of the WMT12 QE shared task to estimate the postediting effort of news texts translated from English to Spanish. A joint LDA approach was used. We expand on that work by exploring two types of bilingual topic models and defining a number of variants of features based on source and target (translation) distributio"
2013.mtsummit-posters.13,P10-1063,0,0.176662,"g more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). For an overview on various feature sets and machine learning algorithms, we refer the reader to the recent shared-task on the topic (Callison-Burch et al., 2012). Most QE work focuses on estimating a score that indicates overall quality having professional translators as intended user, e.g. post-editing effort. Little work has been done fo"
2013.mtsummit-posters.13,2010.jec-1.5,1,0.750247,"elves and distance metrics between source and target distributions (Section 3). We experiment with three families of datasets: two annotated for adequacy, containing newswire and user-generated content (from a product forum), and one news dataset annotated for postediting effort (Section 4). We show that TM features are more effective for both adequacyannotated types of datasets (Section 5). 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system"
2013.mtsummit-posters.13,2009.eamt-1.5,1,0.650408,"ed in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy. 1 Introduction Quality estimation (QE) for machine translation (MT) is an area concerned with predicting a quality indicator for an automatically translated text without referring to human translations (the so-called reference translations typical of most MT evaluation metrics) (Blatz et al., 2004; Specia et al., 2009). The widespread use of MT in the translation industry has strongly motivated work in this area. As a consequence, the majority of existing work focuses on predicting some form of post-editing effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general a"
2013.mtsummit-posters.13,2011.mtsummit-papers.58,1,0.919036,"diting effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general aspects of quality are different from features that define adequacy. Previous work on adequacy estimation has focused on linguistic features contrasting the source and translation texts (Specia et al., 2011; Mehdad et al., 2012), e.g. the proportion of overlapping typed dependency relations in the source and target sentences with arguments that align to each other (based on wordalignment information). While these can provide interesting indicators, they are often very sparse and noisy. Sparsity happens because many of these features do not apply to most sentences, such as features comparing named entities in the source and target sentences. A significant amount of noise can come from the fact that linguistic processors, such as syntactic parsers and named entity recognisers, need to be applied t"
2013.mtsummit-posters.13,2011.eamt-1.12,1,0.598309,"etween source and target distributions (Section 3). We experiment with three families of datasets: two annotated for adequacy, containing newswire and user-generated content (from a product forum), and one news dataset annotated for postediting effort (Section 4). We show that TM features are more effective for both adequacyannotated types of datasets (Section 5). 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translatio"
2014.eamt-1.45,espla-gomis-etal-2014-comparing,1,0.53776,"Missing"
2014.eamt-1.45,P07-2045,0,0.0196031,"36 0.2945 0.2927 0.3583 0.2456 0.3767 TER 0.5601 0.5295 0.4848 0.5016 0.5755 0.5756 0.4726 0.6582 0.4451 OOV 9.5 7.6 7.2 12.6 12.4 6.3 23.1 4.1 Table 2: SMT results. 4 Table 1: Statistics of the parallel datasets. For each dataset the first line corresponds to statistics for Croatian and the second to English. two additional datasets: union and intersection. These are the union and intersection of datasets 10best and reliable. 3 BLEU 0.4092 0.4382 0.5304 0.5176 0.4064 0.4105 0.5448 0.3224 0.5722 Machine Translation Systems Phrase-based statistical MT (PB-SMT) systems are built with Moses 2.1 (Koehn et al., 2007). Tuning is carried out on the development set with minimum error rate training (Och, 2003). All the MT systems use an English language model (LM) from our system for French→English at the WMT-2014 translation shared task (Rubino et al., 2014).9 We built individual LMs on each dataset provided at WMT-2014 and then interpolated them on a development set of the news domain (news2012). Most systems are built on a single dataset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled data"
2014.eamt-1.45,2010.eamt-1.35,1,0.878655,"Missing"
2014.eamt-1.45,P03-1021,0,0.0380974,"4451 OOV 9.5 7.6 7.2 12.6 12.4 6.3 23.1 4.1 Table 2: SMT results. 4 Table 1: Statistics of the parallel datasets. For each dataset the first line corresponds to statistics for Croatian and the second to English. two additional datasets: union and intersection. These are the union and intersection of datasets 10best and reliable. 3 BLEU 0.4092 0.4382 0.5304 0.5176 0.4064 0.4105 0.5448 0.3224 0.5722 Machine Translation Systems Phrase-based statistical MT (PB-SMT) systems are built with Moses 2.1 (Koehn et al., 2007). Tuning is carried out on the development set with minimum error rate training (Och, 2003). All the MT systems use an English language model (LM) from our system for French→English at the WMT-2014 translation shared task (Rubino et al., 2014).9 We built individual LMs on each dataset provided at WMT-2014 and then interpolated them on a development set of the news domain (news2012). Most systems are built on a single dataset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled datasets (1best, 10best, reliable and all) and two systems built on the union and intersection"
2014.eamt-1.45,W13-2506,0,0.182025,"ur legislation and natural environment for English–French and English– Greek (Pecina et al., 2012) and automotive for German to Italian and French (L¨aubli et al., 2013). The rest of the paper is organised as follows. Section 2 presents the crawled datasets used in this study and details the processing undertaken to prepare them for MT. Section 3 details the different MT systems built. Section 4 shows and comments the results obtained. Finally, Section 5 draws conclusions and outlines future lines of work. 2 Crawled Datasets Datasets were crawled using two crawlers: ILSP Focused Crawler (FC) (Papavassiliou et al., 2013) and Bitextor (Espl`a-Gomis et al., 2010). The detection of parallel documents was carried out with two settings for each crawler: 10best and 1best for Bitextor and reliable and all for FC (see (Espl`aGomis et al., 2014) for further details). It is worth mentioning that reliable and 1best are subsets of all and 10best, respectively. These subsets were obtained with a more strict configuration of each crawler and, therefore, are expected to contain higher quality parallel text. In addition, a set of parallel segments was obtained by aligning only those pairs of documents which were checked manu"
2014.eamt-1.45,P02-1040,0,0.0979486,"Missing"
2014.eamt-1.45,2012.eamt-1.38,1,0.898626,"Missing"
2014.eamt-1.45,E12-1055,0,0.0212094,"taset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled datasets (1best, 10best, reliable and all) and two systems built on the union and intersection of the best performing10 dataset of each crawler: 10best and reliable. There is also one system (gen+u) built on two datasets, the general-domain (gen) dataset and a domain-specific dataset (union). Phrase tables from the individual systems gen and union are interpolated so that the perplexity on the development set is minimised (Sennrich, 2012). 9 http://www.statmt.org/wmt14/ translation-task.html 10 According to the BLEU score on the development set. The MT systems are evaluated with a set of stateof-the-art evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). For each system we also report the percentage of out-ofvocabulary (OOV) tokens. Table 2 shows the scores obtained by each MT system. We compare our systems to two baselines: a PB-SMT system built on general-domain data (gen) and an on-line MT system, Google Translate11 (google). Systems built solely on in-domain d"
2014.eamt-1.45,2006.amta-papers.25,0,0.202709,"Missing"
2015.eamt-1.45,E06-1032,0,\N,Missing
2015.eamt-1.45,W10-1751,0,\N,Missing
2015.eamt-1.45,W14-3301,0,\N,Missing
2015.eamt-1.45,P02-1040,0,\N,Missing
2015.eamt-1.45,W14-3319,1,\N,Missing
2015.eamt-1.45,P11-1105,0,\N,Missing
2015.eamt-1.45,P10-2041,0,\N,Missing
2015.eamt-1.45,W05-0909,0,\N,Missing
2015.eamt-1.45,P07-2045,0,\N,Missing
2015.eamt-1.45,W07-0718,0,\N,Missing
2015.eamt-1.45,C14-1111,0,\N,Missing
2015.eamt-1.45,P12-3005,0,\N,Missing
2015.eamt-1.45,2012.eamt-1.67,1,\N,Missing
2015.eamt-1.45,2014.eamt-1.4,1,\N,Missing
2015.eamt-1.45,W14-3320,0,\N,Missing
2015.eamt-1.45,2005.mtsummit-papers.11,0,\N,Missing
2015.eamt-1.45,ljubesic-etal-2014-tweetcat,1,\N,Missing
2015.eamt-1.45,W15-3036,1,\N,Missing
2015.eamt-1.45,rubino-etal-2014-quality,1,\N,Missing
2015.eamt-1.45,W15-3022,1,\N,Missing
2015.eamt-1.45,W15-4903,1,\N,Missing
2015.eamt-1.45,2015.eamt-1.4,1,\N,Missing
2015.eamt-1.45,espla-gomis-etal-2014-comparing,1,\N,Missing
2015.eamt-1.45,W15-3001,0,\N,Missing
2015.eamt-1.45,ljubesic-toral-2014-cawac,1,\N,Missing
2015.eamt-1.45,W14-0405,1,\N,Missing
2015.eamt-1.45,2005.iwslt-1.8,0,\N,Missing
2015.eamt-1.45,W16-3421,1,\N,Missing
2015.eamt-1.45,D07-1078,0,\N,Missing
2015.eamt-1.45,W08-0509,0,\N,Missing
2015.eamt-1.45,W11-2123,0,\N,Missing
2015.eamt-1.45,P14-1129,0,\N,Missing
2015.eamt-1.45,W16-2347,0,\N,Missing
2015.eamt-1.45,W16-2375,1,\N,Missing
2015.eamt-1.45,W16-2367,1,\N,Missing
2015.eamt-1.45,W16-3423,1,\N,Missing
2020.acl-main.532,D18-1045,0,0.0443142,"f monolingual data (Sennrich et al., 2016a) is undoubtedly the most prevalent one, as it remains widely used in state-of-the-art NMT systems (Barrault et al., 2019). NMT systems trained on back-translated data can generate more fluent translations (Sennrich et al., 2016a) thanks to the use of much larger data in the target language to better train the decoder, especially for low-resource conditions where only a small quantity of parallel training data is available. However, the impact of the noisiness of the synthetic source sentences generated by NMT largely remains unclear and understudied. Edunov et al. (2018) even showed that introducing synthetic noise in back-translations actually improves translation quality and enables the Q1. Do NMT systems trained on large backtranslated data capture some of the characteristics of human-produced translations, i.e., translationese? Q2. Does a tag for back-translations really help differentiate translationese from original texts? Q3. Are NMT systems trained on back-translation for low-resource conditions as sensitive to translationese as in high-resource conditions? 2 Motivation During the training with back-translated data (Sennrich et al., 2016a), we can exp"
2020.acl-main.532,P18-4020,0,0.0334377,"Missing"
2020.acl-main.532,P07-2045,0,0.0145408,"rather balanced on their source side between translationese and original texts. For 1 http://www.statmt.org/wmt19/ translation-task.html 2 http://www.statmt.org/wmt15/ translation-task.html 3 After pre-processing and cleaning, we obtained 5.2M and 32.8M sentence pairs for en-de and en-fr, respectively. evaluation, since most of the WMT test sets are made of both original and translationese texts, we used all the newstest sets, from WMT10 to WMT19 for en-de, and from WMT08 to WMT15 for en-fr.4 All our data were pre-processed in the same way: we performed tokenization and truecasing with Moses (Koehn et al., 2007). 3.2 NMT Systems For NMT, we used the Transformer (Vaswani et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018) with standard hyper-parameters for training a Transformer base model.5 To compress the vocabulary, we learned 32k byte-pair encoding (BPE) operations (Sennrich et al., 2016b) for each side of the parallel training data. The back-translations were generated through decoding with Marian the sampled monolingual sentences using beam search with a beam size of 12 and a length normalization of 1.0. The back-translated data were then concatenated to the original parallel data"
2020.acl-main.532,P02-1040,0,0.10906,"ansformer base model.5 To compress the vocabulary, we learned 32k byte-pair encoding (BPE) operations (Sennrich et al., 2016b) for each side of the parallel training data. The back-translations were generated through decoding with Marian the sampled monolingual sentences using beam search with a beam size of 12 and a length normalization of 1.0. The back-translated data were then concatenated to the original parallel data and a new NMT model was trained from scratch using the same hyperparameters used to train the model that generated the back-translations. We evaluated all systems with BLEU (Papineni et al., 2002) computed by sacreBLEU (Post, 2018). To evaluate only on the part of the test set that have original text or translationese on the source side, we used the --origlang option of sacreBLEU with the value “non-L1” for translationese texts and “L1” for original texts, where L1 is the source language, and report on their respective BLEU scores.6 3.3 Results in Resource-Rich Conditions Our results with back-translations (BT) and tagged back-translations (T-BT) are presented in Table 1. When using BT, we consistently observed a drop of BLEU scores for original texts for all the translations tasks, wi"
2020.acl-main.532,P16-1009,0,0.648674,"ginal parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in lowresource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown. 1 Introduction During training, neural machine translation (NMT) can leverage a large amount of monolingual data in the target language. Among existing ways of exploiting monolingual data in NMT, the so-called back-translation of monolingual data (Sennrich et al., 2016a) is undoubtedly the most prevalent one, as it remains widely used in state-of-the-art NMT systems (Barrault et al., 2019). NMT systems trained on back-translated data can generate more fluent translations (Sennrich et al., 2016a) thanks to the use of much larger data in the target language to better train the decoder, especially for low-resource conditions where only a small quantity of parallel training data is available. However, the impact of the noisiness of the synthetic source sentences generated by NMT largely remains unclear and understudied. Edunov et al. (2018) even showed that int"
2020.acl-main.532,P16-1162,0,0.611239,"ginal parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in lowresource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown. 1 Introduction During training, neural machine translation (NMT) can leverage a large amount of monolingual data in the target language. Among existing ways of exploiting monolingual data in NMT, the so-called back-translation of monolingual data (Sennrich et al., 2016a) is undoubtedly the most prevalent one, as it remains widely used in state-of-the-art NMT systems (Barrault et al., 2019). NMT systems trained on back-translated data can generate more fluent translations (Sennrich et al., 2016a) thanks to the use of much larger data in the target language to better train the decoder, especially for low-resource conditions where only a small quantity of parallel training data is available. However, the impact of the noisiness of the synthetic source sentences generated by NMT largely remains unclear and understudied. Edunov et al. (2018) even showed that int"
2020.coling-main.385,N19-1423,0,0.172588,"references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning and direct fine-tuning of the pre-trained language model. However, fine-tuning pre-trained models is highly unstable when the dataset is small (Devlin et al., 2019; Zhang et al., 2020), which is the case in QE for MT as annotated datasets are scarse. To provide a smooth transition between pre-training and fine-tuning, an intermediate training step has been proposed (Phang et al., 2018), using large scale labeled data relevant to the target task. This approach is nonetheless limited by it"
2020.coling-main.385,2020.acl-main.740,0,0.034613,"Missing"
2020.coling-main.385,W19-5406,0,0.203061,"Missing"
2020.coling-main.385,W17-4763,0,0.115819,"nslation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning and direct fine-tuning of the pre-trained language model. However, fine-tuning pre-trained models is highly unstable when the dataset is small (Devlin et al., 2019; Zhang et al., 2020), which is the case in QE for MT as annotated datasets are scarse. To provide a smooth transition between pre-training a"
2020.coling-main.385,W19-5407,0,0.196873,"aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning and direct fine-tuning of the pre-trained language model. However, fine-tuning pre-trained models is highly unstable when the dataset is small (Devlin et al., 2019; Zhang et al., 2020), which is the case in QE for MT as annotated datasets are scarse. To provide a smooth transition between pre-training and fine-tuning, an intermediate training step has been proposed (Phang et al., 2018), using l"
2020.coling-main.385,2020.acl-main.703,0,0.0323107,"ediate training task and will explore this research direction in future work. Our proposed approach can be seen as an extension of the work done by (Gururangan et al., 2020), where the authors evaluated the impact of continued pre-training LMs on domain and task relevant data, but keeping the training objective similar to the pre-training step. Our approach differs from this work by adding a training component relevant to the final QE task, i.e. fake masking. We would like to explore masking variants in future work, for instance by allowing variable masking spans such as the ones proposed in (Lewis et al., 2020). Finally, an extension of our work inspired by (Rubino, 2020) is to explore the pre-training masking hyper-parameters, namely the number of tokens masked and randomly replaced by other tokens sampled from the vocabulary. Because detecting mistranslations in MT output is crucial for QE, increasing the number of replaced tokens would force the model to learn more accurately which tokens are mistranslated or not. This hyper-parameter search could be part of the intermediate training procedure to avoid increasing the computational costs of large LMs pre-training. Acknowledgements A part of this w"
2020.coling-main.385,L18-1004,0,0.0147155,"f 320 sentence pairs. For QE fine-tuning, two learning rates were used, one for the pre-trained LM layers and one for the randomly initialized QE layers. We optimized these learning rates through hyper-parameter search and kept the best values based on the performance reached on the validation set. The batch size was set to 32 sentence pairs with gradient accumulation over 8 batches. 3.4 Datasets The intermediate training data was composed of the supplementary parallel corpus released for the WMT’19 QE shared task for the English–German pair and the Escape corpus for the English–Russian pair (Negri et al., 2018) filtered with the QE training and validation vocabularies. The sentence and word-level annotated datasets use for training, validation and testing our QE models were the official 1 2 More details about the task are available at http://www.statmt.org/wmt19/qe-task.html Model called xlm-mlm-tlm-xnli15-1024 and available at https://github.com/huggingface/transformers 4357 Sentence-level Domain- Fineadaptation tuning r↑ ρ↑ X X 0.321 0.440 0.481 0.510 X X 0.263 0.402 0.495 0.528 X X X X Source F1 ↑ MCC ↑ Word-level MT words MT gaps F1 ↑ MCC ↑ F1 ↑ MCC ↑ F1 ↑ MT all MCC ↑ MAE ↓ RMSE ↓ 0.396 0.478 0"
2020.coling-main.385,P02-1040,0,0.106674,"o-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straigh"
2020.coling-main.385,quirk-2004-training,0,0.0787766,"task. The proposed method does not rely on annotated data and is complementary to QE methods involving pre-trained sentence encoders and domain adaptation. Experiments on English-to-German and English-to-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained"
2020.coling-main.385,2020.wmt-1.121,1,0.817429,"ure work. Our proposed approach can be seen as an extension of the work done by (Gururangan et al., 2020), where the authors evaluated the impact of continued pre-training LMs on domain and task relevant data, but keeping the training objective similar to the pre-training step. Our approach differs from this work by adding a training component relevant to the final QE task, i.e. fake masking. We would like to explore masking variants in future work, for instance by allowing variable masking spans such as the ones proposed in (Lewis et al., 2020). Finally, an extension of our work inspired by (Rubino, 2020) is to explore the pre-training masking hyper-parameters, namely the number of tokens masked and randomly replaced by other tokens sampled from the vocabulary. Because detecting mistranslations in MT output is crucial for QE, increasing the number of replaced tokens would force the model to learn more accurately which tokens are mistranslated or not. This hyper-parameter search could be part of the intermediate training procedure to avoid increasing the computational costs of large LMs pre-training. Acknowledgements A part of this work was conducted under the commissioned research program “Res"
2020.coling-main.385,2006.amta-papers.25,0,0.139252,"s show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning"
2020.coling-main.385,2009.eamt-1.5,0,0.0503891,"oposed method does not rely on annotated data and is complementary to QE methods involving pre-trained sentence encoders and domain adaptation. Experiments on English-to-German and English-to-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings"
2020.coling-main.385,W18-5446,0,0.0453026,"/s> + + + + + + + + + + + + + 0 1 2 3 4 5 0 1 2 3 4 5 6 + + + + + + + + + + + + + en en en en en en de de de de de de de Figure 1: Intermediate self-supervised learning task based on the translation language model training objective of XLM with the addition of NULL tokens associated with randomly inserted MASK tokens. 2 Intermediate Self-supervised Task for QE Pre-trained language model (LM) fine-tuning has shown to improve the results of many natural language processing tasks such as grammatical sentence classification, paraphrases detection or textual entailment to name a few popular tasks (Wang et al., 2018). Some of the prevailing fine-tuned pre-trained models studied in the literature are BERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019), among others. At the core of these approaches are similar LM techniques, using the sequentiality of languages Q to learn probabilities over sequences (X) of words (xi , i ∈ [0; n]) as in p(X) = ni=1 p(xn |x1 , ..., xn ) (causal LM) or randomly masking some input tokens and learning to retrieve them based on both left and right contexts (masked LM). The masked LM approach introduced in BERT was extended in XLM to learn relations between translated s"
2020.ngt-1.3,I17-1001,0,0.0669869,"Missing"
2020.ngt-1.3,2000.eamt-1.5,0,0.626159,"Missing"
2020.ngt-1.3,W18-6301,0,0.0510352,"Missing"
2020.ngt-1.3,P02-1040,0,0.106902,"he fact that vanilla and our tied-multi models have identical shapes when n and m for encoder and decoder layers are specified. the same number of examples during training,6 we trained the models for 300k iterations, with 1 GPU for the vanilla models and 2 GPUs with batch size halved for our tied-multi model. We averaged the last 10 checkpoints saved every after 1k updates, decoded the test sentences, fixing a beam size7 of 4 and length penalty of 0.6, and post-processed the decoded results using the detokenizer.perl and detruecase.perl in Moses. We evaluated our models using the BLEU metric (Papineni et al., 2002) implemented in sacreBLEU (Post, 2018).8 We also present the time consumed to translate the test data, which includes times for the model instantiation, loading the checkpoints, sub-word splitting and indexing, decoding, and subword de-indexing and merging, whereas times for detokenization are not taken into account. Note that we did not use any development data for two reasons. First, we train all models for the same number of iterations. Second, we use checkpoint averaging before decoding, which does not require development data unlike early stopping. the one for the vanilla model, when perf"
2020.ngt-1.3,N19-1423,0,0.0388521,"he independence of target labels (layer combinations) for a given input sentence allows for ties, the model is able to predict multiple layer combinations for the same input sentence. We implemented the model f with a multi-head self-attention neural network inspired by Vaswani et al. (2017). The number of layers and attention heads are optimized during a hyper-parameter search, while the feed-forward layer dimensionality is fixed to 2,048. Input sequences of tokens are mapped to their corresponding embeddings, initialized by the embedding table of the tied-multi NMT model. Similarly to BERT (Devlin et al., 2019), a specific token is prepended to input sequence before being fed to the classifier. This token is finally fed during the forward pass to the output linear layer for sentence classification. The output linear layer has K dimensions, allowing to output as many logits as the number of layer combinations in the tied-multi NMT model. Finally, a sigmoid function outputs probabilities for each layer combination among the K possible combinations. The parameters θ of the model f are learned using mini-batch stochastic gradient descent with Nesterov momentum (Sutskever et al., 2013) and the loss funct"
2020.ngt-1.3,W16-2341,0,0.115507,"Missing"
2020.ngt-1.3,W18-6319,0,0.0137795,"ve identical shapes when n and m for encoder and decoder layers are specified. the same number of examples during training,6 we trained the models for 300k iterations, with 1 GPU for the vanilla models and 2 GPUs with batch size halved for our tied-multi model. We averaged the last 10 checkpoints saved every after 1k updates, decoded the test sentences, fixing a beam size7 of 4 and length penalty of 0.6, and post-processed the decoded results using the detokenizer.perl and detruecase.perl in Moses. We evaluated our models using the BLEU metric (Papineni et al., 2002) implemented in sacreBLEU (Post, 2018).8 We also present the time consumed to translate the test data, which includes times for the model instantiation, loading the checkpoints, sub-word splitting and indexing, decoding, and subword de-indexing and merging, whereas times for detokenization are not taken into account. Note that we did not use any development data for two reasons. First, we train all models for the same number of iterations. Second, we use checkpoint averaging before decoding, which does not require development data unlike early stopping. the one for the vanilla model, when performing decoding with the 6 encoder and"
2020.ngt-1.3,D18-1457,0,0.0231562,"coder-decoder layer combinations. Section 4 describes our efforts towards designing and evaluating a mechanism for dynamically selecting encoderdecoder layer combinations prior to decoding. Section 5 describes two orthogonal extensions to our model aiming at further model compression and speeding-up of decoding. The paper ends with Section 6 containing conclusion and future work. 2 Related Work There are studies that exploit multiple layers simultaneously. Wang et al. (2018) fused hidden representations of multiple layers in order to improve the translation quality. Belinkov et al. (2017) and Dou et al. (2018) attempted to identify which layer can generate useful representations for different natural language processing tasks. Unlike them, we make all layers of the encoder and decoder usable for decoding with any encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that ties parameters between layers (Dabre and Fujita, 2019) and/or between the encoder and decoder within a single model (Xia et al., 2019; Dabre"
2020.ngt-1.3,K16-1029,0,0.0613098,"Missing"
2020.ngt-1.3,D16-1139,0,0.308824,"ny encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that ties parameters between layers (Dabre and Fujita, 2019) and/or between the encoder and decoder within a single model (Xia et al., 2019; Dabre and Fujita, 2019). Parameter tying leads to compact models, but they usually suffer from drops in inference quality. In this paper, we counter such drops with knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016; Freitag et al., 2017). This approach utilizes smoothed data or smoothed training signals instead of the actual training data. A model with a large number of parameters and high per1 Rather than casting the encoder-decoder model into a single column model with (N + M ) layers. 25 formance provides smoothed distributions that are then used as labels for training small models instead of one-hot vectors. As one of the aims in this work is model size reduction, it is related to a growing body of work that addresses the computational requirement reduction. Pruning of pre-trained models (See et al."
2020.ngt-1.3,N04-1022,0,0.101292,"wn in Figure 2, we tackled an advanced problem: dynamic selection of one layer combination prior to decoding.11 4.1 Method We formalize the encoder-decoder layer combination selection with a supervised learning approach 10 We measured the collapsed time for a fair comparison, assuming that all vanilla models were trained on a single GPU one after another, even though one may be able to use multiple GPUs to train the 36 vanilla models in parallel. 11 This is the crucial difference from two post-decoding processes: translation quality estimation (Specia et al., 2010) and n-best-list re-ranking (Kumar and Byrne, 2004). 28 the macro Fβ implemented following (4). where the objective is to minimize the following loss function (2). arg min θ 1 X L(f (si ; θ), tik ), |S |i LiFβ (2) s ∈S  = 1 − (1 + β 2 ) ·  P ·R , (β 2 · P ) + R (4) P P where µ/ k yˆki , R = µ/ k yki , and µ = P iP = yk · yki ). k (ˆ The final loss function is the linear interpolation of LBCE averaged over the K classes and LFβ with parameter λ, both averaged over the batch: λ × LBCE + (1 − λ) × LFβ . We tune α, β, and λ during the classifier hyper-parameter search based on the validation loss. where si is the i-th input sentence (1 ≤ i ≤ |S|"
2020.ngt-1.3,C18-1255,0,0.0209797,"on 2 briefly reviews related work for compressing neural models. Section 3 covers our method that ties multiple models by softmaxing all encoder-decoder layer combinations. Section 4 describes our efforts towards designing and evaluating a mechanism for dynamically selecting encoderdecoder layer combinations prior to decoding. Section 5 describes two orthogonal extensions to our model aiming at further model compression and speeding-up of decoding. The paper ends with Section 6 containing conclusion and future work. 2 Related Work There are studies that exploit multiple layers simultaneously. Wang et al. (2018) fused hidden representations of multiple layers in order to improve the translation quality. Belinkov et al. (2017) and Dou et al. (2018) attempted to identify which layer can generate useful representations for different natural language processing tasks. Unlike them, we make all layers of the encoder and decoder usable for decoding with any encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that tie"
2020.ngt-1.3,P18-1166,0,0.0400003,"Missing"
2020.wmt-1.121,N19-1423,0,0.232653,"ted through intermediate training (Phang et al., 2018) and fine-tuned in a multi-task fashion for the sentence and word-level QE objectives. It was shown during the QE shared task at WMT’19 (Fonseca et al., 2019) that pre-trained language models (LM) fine-tuned for QE reach state-of-the-art results at the levels of sentence and word following the predictor–estimator architecture (Kim et al., 2017) or using a fully endto-end approach (Kepler et al., 2019; Kim et al., 2019; Zhou et al., 2019). However, fine-tuning pretrained LMs is highly unstable when the dataset used for fine-tuning is small (Devlin et al., 2019; Zhang et al., 2020), which is usually the case for QE, as annotated datasets are scarce and expensive to produce, and WMT QE datasets are no exceptions (the shared task datasets are presented in Table 3). This fine-tuning instability might be due to neural network (NN) optimization difficulties or lack of generalization. (Mosbach et al., 2020) To reduce fine-tuning instability of pre-trained LMs, Phang et al. (2018) introduced intermediate training, using large scale labeled data relevant to the target task in order to provide the pre-trained model with a transition step towards the final ta"
2020.wmt-1.121,P18-1031,0,0.022029,"-editing effort on the official WMT’20 QE validation set. The id column refers to the Model ID as presented in Table 2. The id 0 denotes the out-of-the-box XLM checkpoint without domain or task adaptation through intermediate training and without QE fine-tuning. The id 0? denotes the QE fine-tuned XLM checkpoint without domain or task adaptation through intermediate training. the following hyper-parameters: masked LM and output layer learning rates, dropout rate, using or not class weights for the softmax function, and finally the decay rate applied to the discriminative fine-tuning approach (Howard and Ruder, 2018). During hyper-parameter search and training of the final models, the batch size was set to 64 sequence pairs and the learning rate was warmed-up linearly for 200 steps. The remaining hyper-parameters were set to values identical to the ones presented in Section 2.3. 3.3 Evaluation We present in this Section the results obtained during our experiments, first on the official validation set and then on the official test set, based on the masked LMs presented in Table 2. For the sentence-level post-editing effort prediction, the official primary metric was the Pearson correlation coefficient (r)"
2020.wmt-1.121,W19-5406,0,0.180805,"Missing"
2020.wmt-1.121,W18-5446,0,0.0396088,"+ + + + + + + + en en en en en en de de de de de de de Figure 1: Intermediate self-supervised learning task based on the translation language model training objective of XLM with the addition of NULL tokens associated with randomly inserted MASK tokens. to any pre-trained LM, but also when training a masked LM from scratch. 2.1 Approach Description The fine-tuning of pre-trained LM has been applied to and has improved the performances of many natural language processing tasks such as grammatical sentence classification, paraphrases detection or textual entailment to name a few popular tasks. (Wang et al., 2018) Some of the prevailing fine-tuned pretrained models studied in the literature are BERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019), among others. At the core of these approaches are similar LM techniques, using the sequentiality of languages to learn probabilities over sequences Q (X) of words (xi , i ∈ [0; n]) as in p(X) = ni=1 p(xn |x1 , ..., xn ) (causal LM) or randomly masking some input tokens and learning to retrieve them based on both left and right contexts (masked LM). The masked LM approach introduced in BERT was extended in XLM to learn relations between translated sen"
2020.wmt-1.121,W19-5411,0,0.0258699,"in this paper is based on pre-trained cross-lingual language models (XLM) (Conneau and Lample, 2019), domain and task-adapted through intermediate training (Phang et al., 2018) and fine-tuned in a multi-task fashion for the sentence and word-level QE objectives. It was shown during the QE shared task at WMT’19 (Fonseca et al., 2019) that pre-trained language models (LM) fine-tuned for QE reach state-of-the-art results at the levels of sentence and word following the predictor–estimator architecture (Kim et al., 2017) or using a fully endto-end approach (Kepler et al., 2019; Kim et al., 2019; Zhou et al., 2019). However, fine-tuning pretrained LMs is highly unstable when the dataset used for fine-tuning is small (Devlin et al., 2019; Zhang et al., 2020), which is usually the case for QE, as annotated datasets are scarce and expensive to produce, and WMT QE datasets are no exceptions (the shared task datasets are presented in Table 3). This fine-tuning instability might be due to neural network (NN) optimization difficulties or lack of generalization. (Mosbach et al., 2020) To reduce fine-tuning instability of pre-trained LMs, Phang et al. (2018) introduced intermediate training, using large scale la"
2020.wmt-1.121,W17-4763,0,0.189674,"output without using a translation reference. The system developed for the task and described in this paper is based on pre-trained cross-lingual language models (XLM) (Conneau and Lample, 2019), domain and task-adapted through intermediate training (Phang et al., 2018) and fine-tuned in a multi-task fashion for the sentence and word-level QE objectives. It was shown during the QE shared task at WMT’19 (Fonseca et al., 2019) that pre-trained language models (LM) fine-tuned for QE reach state-of-the-art results at the levels of sentence and word following the predictor–estimator architecture (Kim et al., 2017) or using a fully endto-end approach (Kepler et al., 2019; Kim et al., 2019; Zhou et al., 2019). However, fine-tuning pretrained LMs is highly unstable when the dataset used for fine-tuning is small (Devlin et al., 2019; Zhang et al., 2020), which is usually the case for QE, as annotated datasets are scarce and expensive to produce, and WMT QE datasets are no exceptions (the shared task datasets are presented in Table 3). This fine-tuning instability might be due to neural network (NN) optimization difficulties or lack of generalization. (Mosbach et al., 2020) To reduce fine-tuning instability"
2020.wmt-1.121,W19-5407,0,0.275107,"task and described in this paper is based on pre-trained cross-lingual language models (XLM) (Conneau and Lample, 2019), domain and task-adapted through intermediate training (Phang et al., 2018) and fine-tuned in a multi-task fashion for the sentence and word-level QE objectives. It was shown during the QE shared task at WMT’19 (Fonseca et al., 2019) that pre-trained language models (LM) fine-tuned for QE reach state-of-the-art results at the levels of sentence and word following the predictor–estimator architecture (Kim et al., 2017) or using a fully endto-end approach (Kepler et al., 2019; Kim et al., 2019; Zhou et al., 2019). However, fine-tuning pretrained LMs is highly unstable when the dataset used for fine-tuning is small (Devlin et al., 2019; Zhang et al., 2020), which is usually the case for QE, as annotated datasets are scarce and expensive to produce, and WMT QE datasets are no exceptions (the shared task datasets are presented in Table 3). This fine-tuning instability might be due to neural network (NN) optimization difficulties or lack of generalization. (Mosbach et al., 2020) To reduce fine-tuning instability of pre-trained LMs, Phang et al. (2018) introduced intermediate training,"
2020.wmt-1.121,2020.coling-main.385,1,0.817429,"ics while there is no best performing model on all metrics for the EN–DE pair. When comparing the models obtained with configurations #1 and #5, which differ mainly on the introduction of fake masks for the latter, best performances are reached by model #5 as indicated by the three metrics, showing that fake masking is helpful in predicting sentence-level post-editing effort. However, the batch size and the learning rate also differ for these two configurations. A more consistant ablation study allowing for a fair comparison between configurations with and without fake masking is presented in Rubino and Sumita (2020). 0.193 0.193 0.167 0.162 0.178 0.170 0.164 0.158 0.158 0.178 Table 4: Sentence-level predicted post-editing effort on the official WMT’20 QE validation set. The id column refers to the Model ID as presented in Table 2. The id 0 denotes the out-of-the-box XLM checkpoint without domain or task adaptation through intermediate training and without QE fine-tuning. The id 0? denotes the QE fine-tuned XLM checkpoint without domain or task adaptation through intermediate training. the following hyper-parameters: masked LM and output layer learning rates, dropout rate, using or not class weights for t"
2020.wmt-1.23,D18-1338,0,0.0178053,"t BLEU scores on this data allow for checkpoint saving. The parameters for decoding were fixed: a beam size of 4 and a length penalty of 0.6. • Big: 1024 embedding dimensions, 4,096 dimensions for the feed-forward, and 16 heads Highway Transformer Residual connections (RCs) (Srivastava et al., 2015a; He et al., 2016) have been shown to increase forward and backward information flow in deep neural networks (Hardt and Ma, 2017) and thus are a crucial component of the Transformer architecture. Removing them has a negative impact on training and on the overall performances of the resulting model (Bapna et al., 2018). However, incorporating RCs through the addition operation as it is commonly done in the Transformer network does not allow for a distribution of weights between carrying or transforming the input. An alternative, inspired by the Highway Network (Srivastava et al., 2015b) and implemented within the Transformer by Chai et al. (2020), includes a trainable gating mechanism that regulates the information flow. We applied a few modifications to the implementation proposed in Chai et al. (2020): removing all layer normalization operations, adding depth-aware parameter initialization (Junczys-Dowmun"
2020.wmt-1.23,P19-1309,0,0.0132665,"for Pl→En and Ja→En. Our results for En→Pl across all configurations remained similar, which defies the findings of previous work on back-translation and forward translation. We give the provided parallel data using Bivec (Luong et al., 2015). 2. Make all possible bilingual sentence pairs from News Crawl corpora in the source and target languages. 3. For each sentence pair, compute the similarity between the source and target sentences using the bilingual word embeddings trained with Bivec simply by measuring cosine similarity over the averaged word embeddings in each sentence as proposed by Artetxe and Schwenk (2019).6 4. Finally, keep only the sentence pairs with a score higher than a threshold among {1.0, 1.0025, 1.05} and select the value that results in the sentence pairs leading to the highest BLEU score on the validation data when mixing the selected sentence pairs with the original parallel data for training NMT. Table 3 gives an overview of the results obtained with the additional sentence pairs extracted from News Crawl. We did not observe significant improvements as we could only extract a very small amount of useful sentence pairs. Nevertheless, we decided to keep these additional data to train"
2020.wmt-1.23,W09-0432,0,0.0381508,"We did not observe significant improvements as we could only extract a very small amount of useful sentence pairs. Nevertheless, we decided to keep these additional data to train our other NMT systems, since it did not appear harmful according to BLEU. However, as we report in Section 8, it was not the optimal choice to obtain the best results on the test data. 5.2 Backward and Forward Translation of Monolingual Data Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back(ward)-translation, to significantly improve translation quality (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011; Sennrich et al., 2016a). We used the Fairseq Big system, trained on the provided parallel data and 6 We used the “Ratio” version of the scoring function. 233 System #sent. pairs En→Pl Pl→En Ja→En 0 24.4 29.1 18.1 BT 12.5M 25M 50M 26.1 26.2 26.3 32.1 32.1 31.7 21.1 21.2 21.1 TBT 12.5M 25M 50M 26.1 26.1 26.3 30.3 32.3 32.2 21.1 21.2 21.4 FWD 12.5M 25M 50M 26.3 26.4 26.3 29.7 29.5 29.6 18.3 17.4 16.4 Marian Base Table 4: Results of Marian Base on the validation data obtained using synthetic parallel data as back-translations (BT), tagged back-translations (TBT) or forw"
2020.wmt-1.23,Q17-1010,0,0.00778064,"r En–Pl En–Ja 8.7M 15.2M #tokens 239.5M (En) 394.5M (En) 310.0M (Pl) 380.6M (Ja) Table 1: Statistics of our pre-processed parallel data. corpora but also sampled only 200M lines from the “Common Crawl” corpora. To tune/validate and evaluate our systems, we used the official validation and test data provided by the organizers. 3.2 Pre-processing and Cleaning Since some corpora were crawled from the Web and therefore potentially very noisy, we first performed language identification on all the data to keep only lines that have a high probability of being in the right language. We used fastText (Bojanowski et al., 2017) and its large model for language identification.5 We only retained sentences that have a probability higher than 0.75 to be in the right language. For the parallel data, if at least one side of each sentence pair did not match this criteria, we removed the pair from the corpus. We used Moses (Koehn et al., 2007) punctuation normalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b)"
2020.wmt-1.23,P07-2045,0,0.0223191,"Missing"
2020.wmt-1.23,W19-5206,0,0.0208394,"llel data extracted from the News Crawl monolingual corpora, denoted “NC.” The columns “#sent. pairs” indicate how many sentence pairs were extracted from the News Crawl monolingual corpora. the aligned News Crawl sentence pairs, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data, putting the synthetic side on the source side, to train from scratch a new NMT system. We also experimented with forward translation, i.e., with the synthetic part on the target side, and tagged back-translation (Caswell et al., 2019), which simply adds a tag at the beginning of each back-translation, as it has shown to lead to better results, especially when translating texts in their original language (Marie et al., 2020). For English, we translated 50M sentences made of the entire News Crawl 2019 corpus and randomly added sentences from News Crawl 2018 corpus until we have 50M sentences. For Polish and Japanese, we translated the entire News Crawl corpora and added sentences from the Common Crawl corpus until we have 50M sentences. For each configuration, i.e., back-translation, tagged back-translation, and forward tran"
2020.wmt-1.23,2020.acl-main.616,0,0.015196,"to increase forward and backward information flow in deep neural networks (Hardt and Ma, 2017) and thus are a crucial component of the Transformer architecture. Removing them has a negative impact on training and on the overall performances of the resulting model (Bapna et al., 2018). However, incorporating RCs through the addition operation as it is commonly done in the Transformer network does not allow for a distribution of weights between carrying or transforming the input. An alternative, inspired by the Highway Network (Srivastava et al., 2015b) and implemented within the Transformer by Chai et al. (2020), includes a trainable gating mechanism that regulates the information flow. We applied a few modifications to the implementation proposed in Chai et al. (2020): removing all layer normalization operations, adding depth-aware parameter initialization (Junczys-Dowmunt, 2019; Zhang et al., 2019), and initializing biases so that the residual blocks are initially forced to carry information rather than transforming it (Srivastava et al., 2015b). 4.2 5 Training Data Augmentation 5.1 Parallel Data Alignment We extracted additional training parallel data from the News Crawl monolingual corpora with t"
2020.wmt-1.23,D18-2012,0,0.017899,"ormalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b) on the concatenation of English and Polish News Crawl 2019 corpora. We performed sub-word segmentation using this vocabulary on the Polish and English parallel and monolingual data. For the Ja– En language pair, we independently learned 32k BPE operations on the English News Crawl 2019 corpus for English, 32k sentence piece (Kudo and Richardson, 2018) operations on the Japanese News Crawl 2019 corpus for Japanese, and then applied the operations to perform sub-word segmentation on the data in their respective language. For further cleaning of the data, we applied the script “clean-corpus-n.perl” from Moses to remove empty lines and sentences longer than 120 sub-word tokens. Tables 1 and 2 present the statistics of the parallel and monolingual data, respectively, after pre-processing. Data Pre-processing and Cleaning 3.1 #sent. pairs Data As parallel data to train our systems, we used all the provided data for all our targeted translation d"
2020.wmt-1.23,N12-1047,0,0.0207236,"translation probabilities, for both translation directions Scores given by a 4-gram language model trained on all the monolingual corpora in the target language Difference between the length of the source sentence and the length of the translation hypothesis, and its absolute value Table 5: Set of features used by our reranking systems. The “Feature” column refers to the same feature used in Marie and Fujita (2018). The numbers between parentheses indicate the number of scores in each feature set. some plausible explanations for this peculiarity in our analysis in Section 8. 6 chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018). All the following features we used are described in detail by Marie and Fujita (2018). As listed in Table 5, it includes all scores given by all our NMT models. We computed sentence-level translation probabilities using the lexical translation probabilities learned by mgiza on all the parallel training data of our NMT systems. One 4-gram language model trained on the target language model was also used. To account for hypotheses length, we added the difference, and its absolute value, between the n"
2020.wmt-1.23,W15-1521,0,0.0107916,"erences in BLEU when increasing the size of the back-translated data. TBT improves over BT as expected (Caswell et al., 2019), but only for Pl→En and Ja→En. On the other hand, using forward translations significantly decreased BLEU scores, as expected, since it introduces NMT translations to the target side of the training data (Bogoychev and Sennrich, 2019), but again only for Pl→En and Ja→En. Our results for En→Pl across all configurations remained similar, which defies the findings of previous work on back-translation and forward translation. We give the provided parallel data using Bivec (Luong et al., 2015). 2. Make all possible bilingual sentence pairs from News Crawl corpora in the source and target languages. 3. For each sentence pair, compute the similarity between the source and target sentences using the bilingual word embeddings trained with Bivec simply by measuring cosine similarity over the averaged word embeddings in each sentence as proposed by Artetxe and Schwenk (2019).6 4. Finally, keep only the sentence pairs with a score higher than a threshold among {1.0, 1.0025, 1.05} and select the value that results in the sentence pairs leading to the highest BLEU score on the validation da"
2020.wmt-1.23,2020.acl-main.253,0,0.032647,"ven the BLEU score on the validation data in the original language. It means that the translationese texts in the validation data had a negative impact on all our decisions for selecting the best framework, architecture, additional parallel sentences, and so forth, and that we could potentially had better results by taking our decisions by using only the original texts in the validation data. Translationese texts are particularly harmful for training a Reranker, as we can observe for Pl→En. Using them as training data for the • Back-translations should decrease BLEU scores for original texts (Edunov et al., 2020). • Tagged back-translations should improve BLEU scores for original texts (Marie et al., 2020). • Forward translation should lead to lower BLEU scores for translationese texts (Bogoychev and Sennrich, 2019). A possible explanation is that the texts denoted as “original” in the validation data and the test data, 235 # System Arch. NC BT TBT Orig. 1 2 3 4 5 6 Marian Marian Fairseq Fairseq Fairseq Fairseq Base Base Base Highway Big Big 7 8 9 Marian Fairseq Fairseq Big Base Big X X X 10 11 12 Marian Fairseq Fairseq Big Base Big X X X 13 Reranker∗ 14 15 Reranker Non-orig. Reranker Orig. En→Pl Vali"
2020.wmt-1.23,W18-1811,1,0.787407,"ws Translation Task.1 We participated in three translation directions: Japanese→English (Ja→En), English→Polish (En→Pl), and Polish→English (Pl→En). All our systems are constrained, i.e., we used only the parallel and monolingual data provided by the organizers to train and tune them, and validated/selected our best systems exclusively using the official validation data provided by the organizers. We trained NMT systems with several different frameworks and architectures, and combined them, for each translation direction, through n-best list reranking using informative features as proposed by Marie and Fujita (2018). This simple combination method, associated with the exploitation of large tagged back-translated monolingual data, improved BLEU scores on the official 1 2 Description of the Task The task is to translate texts in the news domain. For this purpose, news articles were sampled from online newspapers from September– November 2019. The sources of the test data are original texts whereas the targets are humanproduced translations, i.e., participants are not asked to translate translationese texts unlike past WMT translation tasks. Although organizers also mentioned that the provided validation da"
2020.wmt-1.23,2020.acl-main.532,1,0.903164,"aligned News Crawl sentence pairs, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data, putting the synthetic side on the source side, to train from scratch a new NMT system. We also experimented with forward translation, i.e., with the synthetic part on the target side, and tagged back-translation (Caswell et al., 2019), which simply adds a tag at the beginning of each back-translation, as it has shown to lead to better results, especially when translating texts in their original language (Marie et al., 2020). For English, we translated 50M sentences made of the entire News Crawl 2019 corpus and randomly added sentences from News Crawl 2018 corpus until we have 50M sentences. For Polish and Japanese, we translated the entire News Crawl corpora and added sentences from the Common Crawl corpus until we have 50M sentences. For each configuration, i.e., back-translation, tagged back-translation, and forward translation, we also experimented with sub-samples of 12.5M (only with Marian), and 25M synthetic sentence pairs, in addition to using the entire 50M sentence pairs, for retraining the NMT systems."
2020.wmt-1.23,P16-1009,0,0.0937486,"ojanowski et al., 2017) and its large model for language identification.5 We only retained sentences that have a probability higher than 0.75 to be in the right language. For the parallel data, if at least one side of each sentence pair did not match this criteria, we removed the pair from the corpus. We used Moses (Koehn et al., 2007) punctuation normalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b) on the concatenation of English and Polish News Crawl 2019 corpora. We performed sub-word segmentation using this vocabulary on the Polish and English parallel and monolingual data. For the Ja– En language pair, we independently learned 32k BPE operations on the English News Crawl 2019 corpus for English, 32k sentence piece (Kudo and Richardson, 2018) operations on the Japanese News Crawl 2019 corpus for Japanese, and then applied the operations to perform sub-word segmentation on the data in their respective language. For further cleaning of the data, we applied the script “clean-corpus-n."
2020.wmt-1.23,W19-5321,0,0.0190502,"et al., 2018). However, incorporating RCs through the addition operation as it is commonly done in the Transformer network does not allow for a distribution of weights between carrying or transforming the input. An alternative, inspired by the Highway Network (Srivastava et al., 2015b) and implemented within the Transformer by Chai et al. (2020), includes a trainable gating mechanism that regulates the information flow. We applied a few modifications to the implementation proposed in Chai et al. (2020): removing all layer normalization operations, adding depth-aware parameter initialization (Junczys-Dowmunt, 2019; Zhang et al., 2019), and initializing biases so that the residual blocks are initially forced to carry information rather than transforming it (Srivastava et al., 2015b). 4.2 5 Training Data Augmentation 5.1 Parallel Data Alignment We extracted additional training parallel data from the News Crawl monolingual corpora with the following procedure: Frameworks and Settings Marian Our Models trained with the Marian toolkit (Junczys-Dowmunt et al., 2018) were only 1. Jointly train bilingual word embeddings on 232 Configuration w/o NC w/ NC En→Pl 26.3 26.4 En–Pl Pl→En #sent. pairs 30.4 30.6 0 257."
2020.wmt-1.23,P16-1162,0,0.261499,"ojanowski et al., 2017) and its large model for language identification.5 We only retained sentences that have a probability higher than 0.75 to be in the right language. For the parallel data, if at least one side of each sentence pair did not match this criteria, we removed the pair from the corpus. We used Moses (Koehn et al., 2007) punctuation normalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b) on the concatenation of English and Polish News Crawl 2019 corpora. We performed sub-word segmentation using this vocabulary on the Polish and English parallel and monolingual data. For the Ja– En language pair, we independently learned 32k BPE operations on the English News Crawl 2019 corpus for English, 32k sentence piece (Kudo and Richardson, 2018) operations on the Japanese News Crawl 2019 corpus for Japanese, and then applied the operations to perform sub-word segmentation on the data in their respective language. For further cleaning of the data, we applied the script “clean-corpus-n."
2020.wmt-1.23,P18-4020,0,0.0416571,"Missing"
2020.wmt-1.23,D19-1083,0,0.0437447,"Missing"
2021.acl-long.566,W05-0909,0,0.594351,"Missing"
2021.acl-long.566,E06-1032,0,0.562705,"Missing"
2021.acl-long.566,W17-3203,0,0.0208976,"not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increasing number of publications exclusively rely on BLEU scores to draw their conclusions. The large majority of publications do not perform statistical significance testing, especially since 2016. Moreover, an increasing number of papers copy and compare BLEU scores published by"
2021.acl-long.566,P18-1128,0,0.0442926,"Missing"
2021.acl-long.566,D10-1092,0,0.0786641,"Missing"
2021.acl-long.566,P18-4020,0,0.0457711,"Missing"
2021.acl-long.566,2020.emnlp-main.175,0,0.0241777,"Missing"
2021.acl-long.566,W04-3250,0,0.439686,"tion methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increasing number of publications exclusively rely on BLEU scores to draw their conclusions. The large majority of publications do not perform statistical significance testing, especially since 2016. Moreover, an increasing number o"
2021.acl-long.566,W15-3049,0,0.0476228,"Missing"
2021.acl-long.566,W18-6319,0,0.0925719,"volved over the years. What could be considered, by the research community, as a good evaluation methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increasing number of publications exclusively rely on BLEU scores to draw their conclusions. The large majority of publications do not perf"
2021.acl-long.566,J18-3002,0,0.0175093,"valid. Consequently, we assume that evaluation in MT is conducted with different degrees of thoroughness across papers and that evaluation practices have evolved over the years. What could be considered, by the research community, as a good evaluation methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed"
2021.acl-long.566,W05-0908,0,0.259631,"Missing"
2021.acl-long.566,2006.amta-papers.25,0,0.361931,"Missing"
2021.acl-long.566,2020.acl-main.321,0,0.0216192,"Missing"
2021.acl-long.566,N19-1313,0,0.0279495,"Missing"
2021.acl-long.566,2020.acl-main.448,0,0.129992,"ently, we assume that evaluation in MT is conducted with different degrees of thoroughness across papers and that evaluation practices have evolved over the years. What could be considered, by the research community, as a good evaluation methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increas"
2021.acl-long.566,P02-1040,0,0.128184,", tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility. 1 Introduction New research publications in machine translation (MT) regularly introduce new methods and algorithms to improve the translation quality of MT systems. In the literature, translation quality is usually evaluated with automatic metrics such as BLEU (Papineni et al., 2002) and, more rarely, by humans. To assess whether an MT system performs better than another MT system, their scores given by an automatic metric are directly compared. While such comparisons between MT systems are exhibited in the large majority of MT papers, there are no well-defined guideline nor clear prerequisites under which a comparison between MT systems is considered valid. Consequently, we assume that evaluation in MT is conducted with different degrees of thoroughness across papers and that evaluation practices have evolved over the years. What could be considered, by the research comm"
2021.eval4nlp-1.15,C04-1046,0,0.695139,"s a metric-to-input attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE scores are usually obtained with surface-level MT metrics computed between translation outputs and human produced references or post-edits such as HTER (Snover et al., 2006), we propose to make use of several metrics simultaneously in order to model translation errors at various granularities, i.e. 1 Introduction at the character, token, and phrase levels. Second, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not available. Sentence-level QE is usually con- Third, by employing an attention mechanism beducted using human produced direct assessments tween metric embeddings and bilingual input rep(DA) (Graham et al., 2013) or post-edits. The lat- resentations, the metric-to-input attention weights ter allows to derive token-l"
2021.eval4nlp-1.15,2020.acl-main.747,0,0.0191082,"cluding the pretrained models, the datasets and functions for sentence-level QE are mean-squared the training procedure. All pretrained models and 149 Pretrained Models Test 4.1 Two types of pretrained models were necessary to conduct our experiments: contextual embedding LMs to encode bilingual input sequences and MT models to produce synthetic data required for QE pretraining. Contextual embedding LMs used in our experiments are based on a pretrained XLM-R checkpoint, namely xlm-roberta-large from the HuggingFace Transformers library (Wolf et al., 2020). This model, initially introduced in (Conneau et al., 2020), was pretrained on 2.5TB of filtered CommonCrawl data, covering 100 languages with a vocabulary of 250k BPE tokens (Sennrich et al., 2016), 1, 024 embedding and hidden-state dimensions, 4, 096-dimensional feed-forward layers and 16 attention heads. MT models used in our experiments are transformer-based neural MT (NMT) models. For two language pairs and translation directions of the Eval4NLP 2021 shared task, namely Estonian→English (ET–EN) and Romanian→English (RO–EN), we used pretrained NMT models made available by the WMT’20 QE shared task organizers (Specia et al., 2020).1 For German→Chin"
2021.eval4nlp-1.15,N19-1423,0,0.00495012,"d our submissions reach the first position in all language pairs. The extraction of metricto-input attention weights show that different metrics focus on different parts of the source and target text, providing strong rationales in the decision-making process of the QE model. indicators can be seen as explanations for sentencelevel scores, whether given by humans or automatically produced. However, explainability of QE models decisions is obscured by contemporary approaches relying on large data-driven neural-based models, making use of pretrained contextual language models (LM) such as BERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019), albeit showing steady performance increase as reported in the QE shared tasks (Fonseca et al., 2019; Specia et al., 2020). Yet, the QE layers and architectures are rarely investigated, neither for performance nor for interpretability purposes, and the center of attention is mainly on large pretrained models and generating additional (synthetic) training corpora. In this paper, we present a novel QE architecture which encompasses a metric-to-input attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE"
2021.eval4nlp-1.15,N13-1073,0,0.0241408,"o the target side of the parallel corpora to produce sentence-level scores based on chrF (Popovi´c, 2016), TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) metrics. Additionally, only for the synthetic data, we produced token-level scores following the usual procedure to determine post-editing effort (Specia et al., 2020).3 For this step, word alignments were required to obtain source-side token-level quality indicators. We used the same parallel corpora to produce synthetic data and to train word alignments based on the IBM 2 model (Brown et al., 1993) and trained using fast_align (Dyer et al., 2013). Details about the synthetic data are presented in Table 2.4 The special case of DE–ZH resulting from preliminary experiments, we noticed for this language pair that the translation quality of the synthetic data was low compared to the three other language pairs. We assumed that it was due to two issues: 3 Scripts and procedure available at https://github. com/deep-spin/qe-corpus-builder 4 Parallel corpora were collected from the WMT news translation task (Tiedemann, 2016) and OPUS (Tiedemann, 2016). 150 the quality of the DE–ZH parallel corpora and the performance of the NMT model. To tackle"
2021.eval4nlp-1.15,2020.tacl-1.35,0,0.0205835,"Missing"
2021.eval4nlp-1.15,W13-2305,0,0.0101631,"cond, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not available. Sentence-level QE is usually con- Third, by employing an attention mechanism beducted using human produced direct assessments tween metric embeddings and bilingual input rep(DA) (Graham et al., 2013) or post-edits. The lat- resentations, the metric-to-input attention weights ter allows to derive token-level quality indicators indicate where each metric focuses given an input such as good and bad tags (Fonseca et al., 2019; sequence, increasing the interpretability of the QE Specia et al., 2020). Token-level QE is particu- components. We conduct a set of experiments on larly useful for applications such as source pre- the Eval4NLP 2021 shared task dataset (Fomicheva editing or focused MT post-editing, but requires et al., 2021) using only the training data along high-quality fine-grained a"
2021.eval4nlp-1.15,W19-5406,0,0.0153987,"le the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and data artifacts which correlate with QE scores, instead of encoding translation-related features from source and target inputs (Sun et al., 2020). These findings corroborate with the empirical observation made by Kepler et al. (2019), where the authors obtained the best word-level QE results using BERT and ignoring target language features when predicting source quality labels and vice-versa. Additionally, most recent QE approaches do not allow for the interpretability of sentence-level QE predictions at test time and leads to the current state of QE as a set of black-box components. Furthermore, token-level error annotations is costly to produce. 3 Metric Embedding and Attention Motivated by the limitations to contextual LM based QE, we propose a novel architecture employing metric embeddings and attention, which is comp"
2021.eval4nlp-1.15,W17-4763,0,0.0370106,"according to the accuracy of their translation, while annotations of target tokens also take into account their position in the target sequence. metric is initially represented as a one-hot vector, noted mj ∈ Rg with j ∈ [1, g] ⊂ N. Its corresponding embedding is retrieved with mj · E, forming the query used in the attention mechanism (eqn. 1): Multiple losses are then computed, one for the sentence-level and one for each word-level outputs (source and target tokens), based on gold labels to train (or finetune) the contextual LM and the QE model in an end-to-end fashion using backpropagation (Kim et al., 2017; Lee, 2020; Rubino and Sumita, 2020). Commonly used losses are cross-entropy and mean-squared error for classification and regression respectively. Qi,j = (mj · E) · WiQ However, this approach has limitations While the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and"
2021.eval4nlp-1.15,2021.eval4nlp-1.17,0,0.0373775,"Missing"
2021.eval4nlp-1.15,2020.wmt-1.118,0,0.338845,"ccuracy of their translation, while annotations of target tokens also take into account their position in the target sequence. metric is initially represented as a one-hot vector, noted mj ∈ Rg with j ∈ [1, g] ⊂ N. Its corresponding embedding is retrieved with mj · E, forming the query used in the attention mechanism (eqn. 1): Multiple losses are then computed, one for the sentence-level and one for each word-level outputs (source and target tokens), based on gold labels to train (or finetune) the contextual LM and the QE model in an end-to-end fashion using backpropagation (Kim et al., 2017; Lee, 2020; Rubino and Sumita, 2020). Commonly used losses are cross-entropy and mean-squared error for classification and regression respectively. Qi,j = (mj · E) · WiQ However, this approach has limitations While the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and data artif"
2021.eval4nlp-1.15,2020.wmt-1.116,0,0.0290512,"Missing"
2021.eval4nlp-1.15,2020.wmt-1.119,0,0.0365083,"Missing"
2021.eval4nlp-1.15,N16-3020,0,0.0432879,"s section the training procedures employed for QE pretraining on the synthetic data and finetuning on the officially released training data. 5 Results and Analysis We present in Table 3 the results obtained on the Eval4NLP 2021 shared task as reported by the organizers, including our baselines and final submissions along with the three baselines proposed QE pretraining was conducted per language by the organizers, namely random scores, Tranpair starting from the XLM-R checkpoint presented sQuest (Ranasinghe et al., 2020b) combined with in Section 4.1 using two different random seeds and LIME (Ribeiro et al., 2016) (noted Official baseline learning rates. Additionally, four QE pretraining were conducted on the concatenation of all syn- 1), and XMoverScore (Zhao et al., 2020) combined with SHAP (Lundberg and Lee, 2017) (noted Ofthetic data using four different random seeds and ficial baseline 2). Our baselines are composed of two learning rates. Training was ran for two epochs ensembles of two finetuned language-specific QE for the language specific models and for a single pretrained models while our final submissions are epoch for the remaining ones. We restricted the composed of ensembles of eight fine"
2021.eval4nlp-1.15,N19-4009,0,0.0179507,"sional feed-forward layers and 16 attention heads. MT models used in our experiments are transformer-based neural MT (NMT) models. For two language pairs and translation directions of the Eval4NLP 2021 shared task, namely Estonian→English (ET–EN) and Romanian→English (RO–EN), we used pretrained NMT models made available by the WMT’20 QE shared task organizers (Specia et al., 2020).1 For German→Chinese (DE–ZH) and Russian→German (RU–DE), the two zero-shot pairs of the shared task, we used the mBART50 model (Liu et al., 2020; Tang et al., 2020).2 All NMT models are based on the fairseq library (Ott et al., 2019). 4.2 Valid Train scripts used in our experiments are based on PyTorch (Paszke et al., 2019) and all computations are conducted on NVIDIA V100 GPUs with CUDA v10.2. Datasets Two datasets were used in our experiments: a synthetic dataset for QE pretraining, and the shared task dataset consisting of training, validation and test sets. Details of the latter dataset are presented in Table 1 while we give more information about the synthetic data in this section. 1 Models available at https://github.com/ facebookresearch/mlqe/blob/master/nmt_ models/README-models.md 2 Model available at https://git"
2021.eval4nlp-1.15,2020.wmt-1.121,1,0.724687,"cific component is a regression head formalized by y s = σ(φ(hL 1:n )· s s s s s W + b ), where y ∈ [0, 1] ⊂ R. W and b are trainable parameters of the linear output layer, φ is a pooling function, and σ is the sigmoid function. The output y s of this QE component is a score indicating the sentence-level translation quality. Current state-of-the-art QE approaches are commonly based on sentence encoders taking as input For token-level QE a classification head is imt t t source–translation pairs (Ranasinghe et al., 2020a; plemented as y1:n = softmax (hL 1:n · W + b ), t n×|C| Wang et al., 2020; Rubino, 2020). Encoders are where y1:n ∈ R with C the set of word-level usually contextual LMs pretrained on large amount QE classes, W t and bt are trainable parameters t of this of multilingual data. Existing QE implementations of the linear output layer. The output y1:n commonly rely on additional layers added on top of QE component is a vector of labels indicating the 147 translation quality of corresponding input tokens. Source tokens are annotated according to the accuracy of their translation, while annotations of target tokens also take into account their position in the target sequence. metric is"
2021.eval4nlp-1.15,P02-1040,0,0.109336,"/ 411.0M 600.5M / 601.2M 422.8M / 708.1M 256.9M / 262.7M 4.8M / 2.8M 4.0M / 3.6M 4.5M / 3.3k 4.4M / 4.4M Table 2: Synthetic data produced for QE pretraining. Tokens and types columns contain source / MT counts, M stands for millions and k for thousands, Chinese tokens and types are characters. Synthetic data generation was based on gathered parallel corpora translated by the NMT systems presented in Section 4.1. The translated sentences were compared to the target side of the parallel corpora to produce sentence-level scores based on chrF (Popovi´c, 2016), TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) metrics. Additionally, only for the synthetic data, we produced token-level scores following the usual procedure to determine post-editing effort (Specia et al., 2020).3 For this step, word alignments were required to obtain source-side token-level quality indicators. We used the same parallel corpora to produce synthetic data and to train word alignments based on the IBM 2 model (Brown et al., 1993) and trained using fast_align (Dyer et al., 2013). Details about the synthetic data are presented in Table 2.4 The special case of DE–ZH resulting from preliminary experiments, we noticed for this"
2021.eval4nlp-1.15,W16-2341,0,0.096671,"Missing"
2021.eval4nlp-1.15,quirk-2004-training,0,0.357709,"attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE scores are usually obtained with surface-level MT metrics computed between translation outputs and human produced references or post-edits such as HTER (Snover et al., 2006), we propose to make use of several metrics simultaneously in order to model translation errors at various granularities, i.e. 1 Introduction at the character, token, and phrase levels. Second, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not available. Sentence-level QE is usually con- Third, by employing an attention mechanism beducted using human produced direct assessments tween metric embeddings and bilingual input rep(DA) (Graham et al., 2013) or post-edits. The lat- resentations, the metric-to-input attention weights ter allows to derive token-level quality"
2021.eval4nlp-1.15,2020.wmt-1.122,0,0.639903,"extual token embeddings from the topmost (i.e., L-th) layer of the LM. For sentence-level QE the specific component is a regression head formalized by y s = σ(φ(hL 1:n )· s s s s s W + b ), where y ∈ [0, 1] ⊂ R. W and b are trainable parameters of the linear output layer, φ is a pooling function, and σ is the sigmoid function. The output y s of this QE component is a score indicating the sentence-level translation quality. Current state-of-the-art QE approaches are commonly based on sentence encoders taking as input For token-level QE a classification head is imt t t source–translation pairs (Ranasinghe et al., 2020a; plemented as y1:n = softmax (hL 1:n · W + b ), t n×|C| Wang et al., 2020; Rubino, 2020). Encoders are where y1:n ∈ R with C the set of word-level usually contextual LMs pretrained on large amount QE classes, W t and bt are trainable parameters t of this of multilingual data. Existing QE implementations of the linear output layer. The output y1:n commonly rely on additional layers added on top of QE component is a vector of labels indicating the 147 translation quality of corresponding input tokens. Source tokens are annotated according to the accuracy of their translation, while annotations"
2021.eval4nlp-1.15,2020.coling-main.445,0,0.0858503,"extual token embeddings from the topmost (i.e., L-th) layer of the LM. For sentence-level QE the specific component is a regression head formalized by y s = σ(φ(hL 1:n )· s s s s s W + b ), where y ∈ [0, 1] ⊂ R. W and b are trainable parameters of the linear output layer, φ is a pooling function, and σ is the sigmoid function. The output y s of this QE component is a score indicating the sentence-level translation quality. Current state-of-the-art QE approaches are commonly based on sentence encoders taking as input For token-level QE a classification head is imt t t source–translation pairs (Ranasinghe et al., 2020a; plemented as y1:n = softmax (hL 1:n · W + b ), t n×|C| Wang et al., 2020; Rubino, 2020). Encoders are where y1:n ∈ R with C the set of word-level usually contextual LMs pretrained on large amount QE classes, W t and bt are trainable parameters t of this of multilingual data. Existing QE implementations of the linear output layer. The output y1:n commonly rely on additional layers added on top of QE component is a vector of labels indicating the 147 translation quality of corresponding input tokens. Source tokens are annotated according to the accuracy of their translation, while annotations"
2021.eval4nlp-1.15,N18-1027,0,0.0134343,"ction, welladded on top of the pretrained LM to model met- suited for tasks such as machine translation as it rics in their own space, with a predefined set of results in few alignments between tokens involved sentence-level metrics M = {m1 , . . . , mg }. Each in the attention mechanism. However, in the case of 148 unsupervised sequence labeling such as token-level QE without annotated data, zero to many tokens may influence sentence-level scores given a metric. Thus, to allow more flexibility in the distribution of attention weights over input tokens and following the approach presented in (Rei and Søgaard, 2018), we replaced softmax by sigmoid . Sentence-level scores are obtained for each metric with the weighted sum of value vectors for each attention head (eqn. 4): X attn i,j = αi,j,1:n Vi , (4) where attn i,j ∈ R(d/u) , before concatenating the output of each head and projecting the result back in the dimensionality of the model (eqn. 5): 0 yjs = (attn 1,j ⊕ . . . ⊕ attn u,j ) · W O (5) 0 with W O ∈ Rd×d . Finally, we project yjs from the model dimensionality to a single score through a 0 metric specific linear layer: yjs = yjs · Wjs with Wjs ∈ Rd×1 and yjs ∈ [0, 1] ⊂ R. Token-level QE scores are"
2021.eval4nlp-1.15,P16-1162,0,0.0221314,"dels and 149 Pretrained Models Test 4.1 Two types of pretrained models were necessary to conduct our experiments: contextual embedding LMs to encode bilingual input sequences and MT models to produce synthetic data required for QE pretraining. Contextual embedding LMs used in our experiments are based on a pretrained XLM-R checkpoint, namely xlm-roberta-large from the HuggingFace Transformers library (Wolf et al., 2020). This model, initially introduced in (Conneau et al., 2020), was pretrained on 2.5TB of filtered CommonCrawl data, covering 100 languages with a vocabulary of 250k BPE tokens (Sennrich et al., 2016), 1, 024 embedding and hidden-state dimensions, 4, 096-dimensional feed-forward layers and 16 attention heads. MT models used in our experiments are transformer-based neural MT (NMT) models. For two language pairs and translation directions of the Eval4NLP 2021 shared task, namely Estonian→English (ET–EN) and Romanian→English (RO–EN), we used pretrained NMT models made available by the WMT’20 QE shared task organizers (Specia et al., 2020).1 For German→Chinese (DE–ZH) and Russian→German (RU–DE), the two zero-shot pairs of the shared task, we used the mBART50 model (Liu et al., 2020; Tang et al"
2021.eval4nlp-1.15,2006.amta-papers.25,0,0.653831,"2020). Yet, the QE layers and architectures are rarely investigated, neither for performance nor for interpretability purposes, and the center of attention is mainly on large pretrained models and generating additional (synthetic) training corpora. In this paper, we present a novel QE architecture which encompasses a metric-to-input attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE scores are usually obtained with surface-level MT metrics computed between translation outputs and human produced references or post-edits such as HTER (Snover et al., 2006), we propose to make use of several metrics simultaneously in order to model translation errors at various granularities, i.e. 1 Introduction at the character, token, and phrase levels. Second, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not av"
2021.eval4nlp-1.15,2009.eamt-1.5,0,0.0109312,"U multi gold Figure 3: Attention weights computed between individual metric embeddings, namely DA, TER, chrF and BLEU, along with the multimetric approach (see eqn. 6) and the human annotations (noted gold). Samples extracted from the ET–EN and RO–EN validation sets (top and bottom respectively). 6 Previous Work Since the shift of most NLP tasks towards using large pretrained contextual LMs as basis for taskspecific finetuning, the research community working on QE for MT moved from a classic two-step process of feature engineering followed by machine learning (Blatz et al., 2004; Quirk, 2004; Specia et al., 2009) to an end-to-end training neural-based paradigm. First attempts in this direction were conducted by Kim et al. (2017) with the predictorestimator, which inspired further work in using various types of encoders (Wang et al., 2020), enriching the model with features extracted from NMT models (Moura et al., 2020; Fomicheva et al., 2020a) or modifying the pretraining objective of contextual LMs for QE adaptation (Rubino and Sumita, 2020). 7 Conclusion This paper presented a novel QE architecture for unsupervised token-level quality prediction providing sentence-level explainable decisions from th"
2021.eval4nlp-1.15,2020.acl-main.558,0,0.0121273,"ession respectively. Qi,j = (mj · E) · WiQ However, this approach has limitations While the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and data artifacts which correlate with QE scores, instead of encoding translation-related features from source and target inputs (Sun et al., 2020). These findings corroborate with the empirical observation made by Kepler et al. (2019), where the authors obtained the best word-level QE results using BERT and ignoring target language features when predicting source quality labels and vice-versa. Additionally, most recent QE approaches do not allow for the interpretability of sentence-level QE predictions at test time and leads to the current state of QE as a set of black-box components. Furthermore, token-level error annotations is costly to produce. 3 Metric Embedding and Attention Motivated by the limitations to contextual LM based QE,"
2021.eval4nlp-1.15,2016.eamt-2.8,0,0.013544,"synthetic data and to train word alignments based on the IBM 2 model (Brown et al., 1993) and trained using fast_align (Dyer et al., 2013). Details about the synthetic data are presented in Table 2.4 The special case of DE–ZH resulting from preliminary experiments, we noticed for this language pair that the translation quality of the synthetic data was low compared to the three other language pairs. We assumed that it was due to two issues: 3 Scripts and procedure available at https://github. com/deep-spin/qe-corpus-builder 4 Parallel corpora were collected from the WMT news translation task (Tiedemann, 2016) and OPUS (Tiedemann, 2016). 150 the quality of the DE–ZH parallel corpora and the performance of the NMT model. To tackle the first issue, we generated our own DE–ZH parallel corpora by pivot-based (back-) translation, starting from a monolingual Chinese corpus composed of CommonCrawl and NewsCrawl 2018 to 2020, translating it into English using an in-house NMT model trained with Marian (Junczys-Dowmunt et al., 2018) on the WMT’21 QE ZH–EN parallel corpus, then translating the English output into German using the EN–DE NMT model released by the WMT’20 QE shared task organizers, resulting in a"
2021.eval4nlp-1.15,2021.eacl-main.50,0,0.0296166,"Missing"
2021.eval4nlp-1.15,2020.acl-main.151,0,0.166826,"present in Table 3 the results obtained on the Eval4NLP 2021 shared task as reported by the organizers, including our baselines and final submissions along with the three baselines proposed QE pretraining was conducted per language by the organizers, namely random scores, Tranpair starting from the XLM-R checkpoint presented sQuest (Ranasinghe et al., 2020b) combined with in Section 4.1 using two different random seeds and LIME (Ribeiro et al., 2016) (noted Official baseline learning rates. Additionally, four QE pretraining were conducted on the concatenation of all syn- 1), and XMoverScore (Zhao et al., 2020) combined with SHAP (Lundberg and Lee, 2017) (noted Ofthetic data using four different random seeds and ficial baseline 2). Our baselines are composed of two learning rates. Training was ran for two epochs ensembles of two finetuned language-specific QE for the language specific models and for a single pretrained models while our final submissions are epoch for the remaining ones. We restricted the composed of ensembles of eight finetuned models length of training samples to a minimum of 5 and a maximum of 128 subword tokens for the bilin- for each of token-level tasks (source and target) and"
C12-1014,2011.mtsummit-papers.35,1,0.845632,"Missing"
C12-1014,W08-0309,0,0.027292,"using n-gram precision with a brevity penalty as the score, as demonstrated in (1) BLEU(n) = n Y 1 1 PRECi n · bp (1) where n is the order of n-gram, PRECi is the i-gram precision and bp is the brevity penalty. The brevity penalty is defined as (2): bp = e x p(ma x( l en(Re f ) l en(Out) − 1, 0)) (2) where l en(Re f ) is the length of the reference and l en(Out) is the length of the output. This n-gram matching scheme makes BLEU very sensitive to small changes in the output, and fails to capture linguistic variations, especially in the case where only one reference translation is being used. (Callison-Burch et al., 2008) show that that BLEU has a lower correlation with human judgement than metrics such as TER, which take into account linguistic resources and better matching strategies. Furthermore, BLEU is designed to evaluate MT output on a document level. For this reason, we have used S-BLEU (Sentence-Level BLEU) and TER to compare individual sentences. TER is an Edit Distance-style evaluation metric that measures the amount of editing that a human post-editor would have to perform to change a system output so it matches the given reference translation. It calculates how many insertions, deletions, substitu"
C12-1014,P05-1033,0,0.0797616,"rom the error analysis should enhance feature selection methods. We will research ways to further refine statistical post-editing techniques for both RBMT and SMT systems. In previous work, we developed a contextualised SPE system that attempts to preserve the original context of the source material with a novel method of context modelling (Béchara et al., 2011). We intend to take this work further and refine our use of context information. We will also experiment with different system combinations: in addition to RBMT and PBSMT systems, we will utilise a hierarchical phrase based SMT system (Chiang, 2005). Acknowledgments This work is supported by Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University. We would like to thank Symantec, in particular Dr. Johann Roturier and Dr. Fred Hollowood, for providing us with the data and with access to the Symantec Systran Machine Translation Production System. We would like to thank Dr. Stephen Doherty, from the School of Applied Language and Intercultural Studies at Dublin City University, for providing us with access to masters in translation students to act as o"
C12-1014,W07-0732,0,0.108776,"l., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et al., 2007; Dugast et al., 2007; Potet et al., 2011), manually corrected (i.e. post-edited) MT output is used as the target side for training the SPE system (i.e. a ""mono-lingual"" SMT system trained on the output of the first stage MT system as source and the manually corrected first stage MT output as target, and then applied to the output of the first stage MT system on unseen source side input data), while the other approach (Oflazer and El-Khalout, 2007; Béchara et al., 2011; Rubino et al., 2012) simply uses available bi-text training data (such as translation memories (TMs) in industrial applications or more generic SM"
C12-1014,P07-2045,0,0.0334804,"Missing"
C12-1014,N03-1017,0,0.0101645,"o the text type and domain of the Symantec translation memory data, as described in (Roturier, 2009). 3.3 Statistical Phrase-Based Machine Translation Statistical machine translation builds statistical models based on the analysis of existing parallel corpora, both monolingual and bilingual. For our statistical machine translation system we used the PBSMT system Moses, 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in (Koehn et al., 2003). We used minimum error rate training (MERT) (Och, 2003) for tuning on the development set. During decoding, the stack size was limited to 500 hypotheses. 3.4 Statistical Post-editing The first pipeline, which combines RBMT output with SPE (statistical post-editing) system, uses Systran to translate the entire source side of the TM-based training set, and the output together with the corresponding target side of the TM is then used as the training data for the SMT-based SPE system. The second-stage system therefore produces a monolingual translation based on the output produced by the first st"
C12-1014,P03-1021,0,0.0229625,"a, as described in (Roturier, 2009). 3.3 Statistical Phrase-Based Machine Translation Statistical machine translation builds statistical models based on the analysis of existing parallel corpora, both monolingual and bilingual. For our statistical machine translation system we used the PBSMT system Moses, 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in (Koehn et al., 2003). We used minimum error rate training (MERT) (Och, 2003) for tuning on the development set. During decoding, the stack size was limited to 500 hypotheses. 3.4 Statistical Post-editing The first pipeline, which combines RBMT output with SPE (statistical post-editing) system, uses Systran to translate the entire source side of the TM-based training set, and the output together with the corresponding target side of the TM is then used as the training data for the SMT-based SPE system. The second-stage system therefore produces a monolingual translation based on the output produced by the first stage RBMT system and the target side of the TM training d"
C12-1014,J03-1002,0,0.00632206,"ion system, specifically customised with the use of 10K+ dictionary entries specific to the text type and domain of the Symantec translation memory data, as described in (Roturier, 2009). 3.3 Statistical Phrase-Based Machine Translation Statistical machine translation builds statistical models based on the analysis of existing parallel corpora, both monolingual and bilingual. For our statistical machine translation system we used the PBSMT system Moses, 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in (Koehn et al., 2003). We used minimum error rate training (MERT) (Och, 2003) for tuning on the development set. During decoding, the stack size was limited to 500 hypotheses. 3.4 Statistical Post-editing The first pipeline, which combines RBMT output with SPE (statistical post-editing) system, uses Systran to translate the entire source side of the TM-based training set, and the output together with the corresponding target side of the TM is then used as the training data for the SMT-based SPE system. The second-stage system th"
C12-1014,W07-0704,0,0.0639693,"utomatic tuning of SMT system parameters (e.g. using MERT), which may require a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et al., 2007; Dugast et al., 2007; Potet et al., 2011), manually corrected (i.e. post-edited) MT output is used as the target side for training the SPE system (i.e. a ""mono-lingual"" SMT system trained on the output of the first stage MT system as source and the"
C12-1014,P02-1040,0,0.0927948,"nglish training set. This approach will ensure that we do not translate already seen data, and that the source side of the training set for the SPE system is as close in quality to the test set source as possible. Figure 2 illustrates the SMT+SPE pipeline. MT System F (Input) Moses SPE System E' (Input) E' (Ouput) Moses E'' (Ouput) Figure 2: The Moses+Moses pipeline, using the output of Moses as the input for the second stage SMT system (Moses) 3.5 Automatic Evaluation Metrics We used two metrics for automatic evaluation; BLEU (Bilingual Evaluation Understudy) and TER (Translation Edit Rate) (Papineni et al., 2002; Snover et al., 2006). Both of these metrics depend on a reference translation to estimate the quality of machine translated output. BLEU matches n-grams between the MT output and the reference translation, using n-gram precision with a brevity penalty as the score, as demonstrated in (1) BLEU(n) = n Y 1 1 PRECi n · bp (1) where n is the order of n-gram, PRECi is the i-gram precision and bp is the brevity penalty. The brevity penalty is defined as (2): bp = e x p(ma x( l en(Re f ) l en(Out) − 1, 0)) (2) where l en(Re f ) is the length of the reference and l en(Out) is the length of the output"
C12-1014,2012.eamt-1.55,1,0.856232,"quire a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et al., 2007; Dugast et al., 2007; Potet et al., 2011), manually corrected (i.e. post-edited) MT output is used as the target side for training the SPE system (i.e. a ""mono-lingual"" SMT system trained on the output of the first stage MT system as source and the manually corrected first stage MT output as target, and then app"
C12-1014,W05-0822,0,0.0550267,"Missing"
C12-1014,N07-1064,0,0.376754,"2. 215 1 Introduction Human evaluation is a core component of shared tasks such as WMT, and is often considered the gold standard in evaluation of translation systems. Automatic evaluation metrics, however, are much less costly, much more time efficient and enable automatic tuning of SMT system parameters (e.g. using MERT), which may require a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one appr"
C12-1014,2006.amta-papers.25,0,0.0998391,"is approach will ensure that we do not translate already seen data, and that the source side of the training set for the SPE system is as close in quality to the test set source as possible. Figure 2 illustrates the SMT+SPE pipeline. MT System F (Input) Moses SPE System E' (Input) E' (Ouput) Moses E'' (Ouput) Figure 2: The Moses+Moses pipeline, using the output of Moses as the input for the second stage SMT system (Moses) 3.5 Automatic Evaluation Metrics We used two metrics for automatic evaluation; BLEU (Bilingual Evaluation Understudy) and TER (Translation Edit Rate) (Papineni et al., 2002; Snover et al., 2006). Both of these metrics depend on a reference translation to estimate the quality of machine translated output. BLEU matches n-grams between the MT output and the reference translation, using n-gram precision with a brevity penalty as the score, as demonstrated in (1) BLEU(n) = n Y 1 1 PRECi n · bp (1) where n is the order of n-gram, PRECi is the i-gram precision and bp is the brevity penalty. The brevity penalty is defined as (2): bp = e x p(ma x( l en(Re f ) l en(Out) − 1, 0)) (2) where l en(Re f ) is the length of the reference and l en(Out) is the length of the output. This n-gram matching"
C12-1014,2007.mtsummit-wpt.4,0,0.0922062,"Human evaluation is a core component of shared tasks such as WMT, and is often considered the gold standard in evaluation of translation systems. Automatic evaluation metrics, however, are much less costly, much more time efficient and enable automatic tuning of SMT system parameters (e.g. using MERT), which may require a number of iterations to converge. Statistical post-editing of the output of RBMT and SMT systems is an active field of research and RBMT + SPE pipelines are by now a commercial reality. 1 Automatic post-editing of rule-based machine translation systems (Simard et al., 2007; Terumasa, 2007; Kuhn et al., 2010) has shown (in some cases) spectacular improvements in translation quality measured in terms of automatic evaluation scores. Furthermore, SPE has also been applied to the output of statistical MT (SMT) systems (Oflazer and El-Khalout, 2007; Potet et al., 2011; Béchara et al., 2011; Rubino et al., 2012), albeit with more mixed results. However, to date, despite considerable interest in the area, the comparison between SPE pipelines and pure SMT and RBMT systems is not fully researched. Previous research can be categorised into roughly two classes: in one approach (Simard et"
C12-1014,vilar-etal-2006-error,0,0.0749684,"Missing"
C14-1194,W10-1408,1,0.899818,"Missing"
C14-1194,W12-3108,0,0.18602,"ore a variety of syntactic features extracted from the output of both a hand-crafted broad-coverage grammar/parser and a statistical constituency parser on the WMT 2012 data set. They find that the syntactic features make an important contribution to the overall system. In a framework for combining QE and automatic metrics to evaluate MT output, Specia and Gim´enez (2010) use part-of-speech (POS) tag language model probabilities of the MT output 3-grams as features for QE and features built upon syntactic chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis (2012) builds a series of models for estimating post-editing effort using syntactic features such as parse probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams, CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or disfluent. 2053 In this work, we compare the use of tree kernels and hand-crafted features extracted from the constituency and dependency trees of the source and target sides of a translation pair, as well as comparing the role of source and target syntax. In addition, we conduct a more in-d"
C14-1194,W13-2201,0,0.0307628,"and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All metrics are applied at the segment level.5 We randomly select 4500 parallel segments from the News development data sets released for the WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system, we translate the data set with the following three systems and randomly choose 1500 distinct segments from each: • ACCEPT6 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprietary rule-based system • Bing7 : an online translation system The data set is randomly split into 3000 training, 500 development and 1000 test segments. We use the development set for tuning model parameters and building hand-crafted feature s"
C14-1194,W07-0718,0,0.0379313,"an evaluation exists for just a few language pairs and domains. To the best of our knowledge, the only available English-to-French data set which contains human judgements of translation quality are as follows: • CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commission and also from the health domain. In addition to the domain (and style) difference to newswire (the domain on which our parsers are trained), a major stumbling block which prevents us from using this data set is its small size: only 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All"
C14-1194,W12-3102,0,0.145282,"h has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree fragments of both the constituency and dependency trees of the source and target sentences. Our handcrafted feature set consists of an initial set of 489 constituency and dependency features which are then reduced to a set of 144 with no significant loss in performance. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Pag"
C14-1194,candito-etal-2010-statistical,0,0.0670248,"Missing"
C14-1194,P02-1034,0,0.669376,"-speaking customer base. It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing the syntactic complexity of the source sentence, the grammaticality of the target translation and the syntactic symmetry between the source sentence and its translation. This assumption has been borne out by previous research which has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree fra"
C14-1194,W08-1301,0,0.301447,"Missing"
C14-1194,W11-2107,0,0.0412097,"y 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All metrics are applied at the segment level.5 We randomly select 4500 parallel segments from the News development data sets released for the WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system, we translate the data set with the following three systems and randomly choose 1500 distinct segments from each: • ACCEPT6 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprieta"
C14-1194,2005.eamt-1.15,0,0.0327113,"ey find that the syntactic features make an important contribution to the overall system. In a framework for combining QE and automatic metrics to evaluate MT output, Specia and Gim´enez (2010) use part-of-speech (POS) tag language model probabilities of the MT output 3-grams as features for QE and features built upon syntactic chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis (2012) builds a series of models for estimating post-editing effort using syntactic features such as parse probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams, CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or disfluent. 2053 In this work, we compare the use of tree kernels and hand-crafted features extracted from the constituency and dependency trees of the source and target sides of a translation pair, as well as comparing the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches and compare the utility of syntactic information extracted from the source side and target sides of the translation. 3 Data While there is evidence to"
C14-1194,2007.mtsummit-papers.31,0,0.0263003,"ctic information extracted from the source side and target sides of the translation. 3 Data While there is evidence to suggest that predicting human evaluation scores is superior to predicting automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation exists for just a few language pairs and domains. To the best of our knowledge, the only available English-to-French data set which contains human judgements of translation quality are as follows: • CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commission and also from the health domain. In addition to the domain (and style) difference to newswire (the domain on which our parsers are trained), a major stumbling block which prevents us from using this data set is its small size: only 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the eval"
C14-1194,W12-3112,0,0.278003,"roduced on a daily basis by a company’s Englishspeaking customers can be translated automatically into French and made available with confidence to the company’s French-speaking customer base. It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing the syntactic complexity of the source sentence, the grammaticality of the target translation and the syntactic symmetry between the source sentence and its translation. This assumption has been borne out by previous research which has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully wit"
C14-1194,I13-1153,1,0.623386,"Missing"
C14-1194,P03-1054,0,0.0123318,"aning that they can be varied by changing the value of a parameter. For example, the non-terminal label is a parameter for the non-terminal-label-count 2056 ∗1 2 ∗3 4 ∗5 ∗6 7 ∗8 9 ∗10 11 ∗12 ∗13 ∗14 ∗1 ∗2 ∗3 ∗4 ∗5 ∗6 ∗7 ∗8 ∗9 ∗10 ∗11 Constituency Label of the root node of the constituency tree Height of the constituency tree which is the number of edges from root node to the farthest terminal (leaf) node Number of nodes in the constituency tree Log probability of the constituency parse assigned by the parser Parseval F1 score of the tree with respect to a tree produced by the Stanford parser (Klein and Manning, 2003) Right hand side of the CFG production rule expanding the root node All non-lexical and lexical CFG production rules expanding the tree nodes Average arity of the non-lexical CFG production rules expanding the constituency tree nodes Counts of each non-terminal label in the tree POS unigrams, 3-grams and 5-grams POS n-gram scores against language models trained on the POS tags of the respective treebanks using the SRILM toolkit (http://www.speech.sri.com/projects/srilm/) with Witten-Bell smoothing Counts of each 12 universal POS tags (Petrov et al., 2012) Location of the first verb in the sent"
C14-1194,W04-3250,0,0.357514,"Missing"
C14-1194,J93-2004,0,0.0461628,"Missing"
C14-1194,E06-1015,0,0.703271,"It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing the syntactic complexity of the source sentence, the grammaticality of the target translation and the syntactic symmetry between the source sentence and its translation. This assumption has been borne out by previous research which has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree fragments of both the"
C14-1194,P02-1040,0,0.089815,"hich prevents us from using this data set is its small size: only 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All metrics are applied at the segment level.5 We randomly select 4500 parallel segments from the News development data sets released for the WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system, we translate the data set with the following three systems and randomly choose 1500 distinct segments from each: • ACCEPT6 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plu"
C14-1194,P06-1055,0,0.0290645,"ulting tree representation are word forms and dependency relations, omitting POS tag information. An example is shown in Figure 1. A word is a child of its dependency relation to its head. The dependency relation in turn is the child of the head word. This continues until the root of the tree. Based on preliminary experiments on our development set, we use subset tree kernels, where the tree fragments are subtrees rooted at any node in the tree so that no production rule expanding a node in the 8 https://github.com/CNGLdlab/LORG-Release. The Lorg parser is very similar to the Berkeley parser (Petrov et al., 2006), the main difference being its unknown word handling mechanism (Attia et al., 2010). 9 http://svmlight.joachims.org/ 10 http://disi.unitn.it/moschitti/Tree-Kernel.htm 2055 BLEU RMSE r 1-TER RMSE r METEOR RMSE r BM BW 0.1626 0 0.1965 0 0.1657 0 0.1601 0.1766 0.1949 0.1565 0.1625 0.2047 TK BW+TK 0.1581 0.2437 0.1888 0.2774 0.1595 0.2715 0.1570 0.2696 0.1879 0.2939 0.1576 0.3111 HC BW+HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516 0.1587 0.2418 0.1899 0.2611 0.1585 0.2964 SyQE 0.1577 0.2535 0.1887 0.2797 0.1594 0.2743 BW+SyQE 0.1568 0.2802 0.1879 0.2937 0.1576 0.3127 Table 1: QE performances measu"
C14-1194,petrov-etal-2012-universal,0,0.0684758,"Missing"
C14-1194,W06-1608,0,0.0180821,"ts the quality of the parse tree (Hypothesis 2). Although this low quality would be expected to affect the dependency trees in the same way since they are directly derived from the consistency trees, this is not the case and it appears that the problematic aspects of the French parses are abstracted away from the dependency trees. To test the first hypothesis, we investigate the role of parser accuracy in QE. For both languages, we substitute the standard parsing models used in all our prior experiments with “lower-accuracy” models trained using only a fraction of the training data (following Quirk and Corston-Oliver (2006)). The English parsing model achieves an F1 of 72.5 and the French an F1 of 66.5, representing drops of approximately 17 points from the original models. The RMSE and Pearson r of the new QE model are 0.1583 and 0.2350 compared to 0.1581 and 0.2437 of the one trained with original trees (see also the third row of Table 1). These results show that the use of these lower-accuracy models has only a minimal and statistically insignificant effect on QE performance, suggesting that intrinsic parser accuracy is not the reason why the target constituency trees are less useful than the source constitue"
C14-1194,quirk-2004-training,0,0.414736,"in performance improvements in the tasks of both QE for MT and parser accuracy prediction The rest of this paper is organised as follows: we discuss related work in using syntax in QE in Section 2, we describe the data in Section 3, and we then go on to describe the QE framework and the systems built in Section 4. We follow this with an investigation of the role of source and target syntax in Section 5 before presenting our heuristics to modify the French constituency trees in Section 6. 2 Related Work Features extracted from parser output have been used before in training QE for MT systems. Quirk (2004) uses a single syntax-based feature which indicates whether a full parse for the source sentence could be found. Hardmeier et al. (2012) employ tree kernels to predict the 1-to-5 post-editing cost of a machine-translated sentence. They use tree kernels derived from syntactic constituency and dependency trees of the source side (English) and only dependency trees of the translation side (Spanish). The tree kernels are used both alone and combined with non-syntactic features. The combined setting ranked second in the 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (201"
C14-1194,D08-1093,0,0.0575215,"Missing"
C14-1194,W12-3117,1,0.90862,"Missing"
C14-1194,2006.amta-papers.25,0,0.0606138,"crafted features extracted from the constituency and dependency trees of the source and target sides of a translation pair, as well as comparing the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches and compare the utility of syntactic information extracted from the source side and target sides of the translation. 3 Data While there is evidence to suggest that predicting human evaluation scores is superior to predicting automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation exists for just a few language pairs and domains. To the best of our knowledge, the only available English-to-French data set which contains human judgements of translation quality are as follows: • CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commission and also from the health domain. In addition to the domain (and style) difference to newswire (the domain on which our parsers are trained), a major stumbling block which prevents us from using this data set is its small size: only 1135 segment"
C14-1194,2010.amta-papers.3,0,0.581398,"Missing"
C14-1194,2009.eamt-1.5,0,0.0221037,"or does the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French Treebank does appear to have an adverse effect, and this is significantly improved by simple transformations of the French trees. Finally, we provide further evidence of the usefulness of these transformations by applying them in a separate task – parser accuracy prediction. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made regarding whether to publish a machine translation as is or to re-direct it to a translator, either for postediting or to be translated from scratch. The scores produced by a QE system can also be used to choose between translations, in a system combination framework or in n-best list reranking. The work presented here takes place in the context of a wider study, the aim of which is to develop an English-French QE system so that technical support material that is produced on a daily basis by a company’s Englishspea"
C14-1194,P12-2066,1,0.863412,"Missing"
C14-1194,2003.mtsummit-papers.52,0,0.0150045,"f quality estimation nor does the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French Treebank does appear to have an adverse effect, and this is significantly improved by simple transformations of the French trees. Finally, we provide further evidence of the usefulness of these transformations by applying them in a separate task – parser accuracy prediction. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made regarding whether to publish a machine translation as is or to re-direct it to a translator, either for postediting or to be translated from scratch. The scores produced by a QE system can also be used to choose between translations, in a system combination framework or in n-best list reranking. The work presented here takes place in the context of a wider study, the aim of which is to develop an English-French QE system so that technical support material that is produced on a daily basis by a"
C14-1194,N10-1121,0,0.215032,"Missing"
C14-1194,C04-1046,0,\N,Missing
C16-1072,W06-0903,0,0.0380558,"e show that these features outperform more traditional features, such as token or character n-grams, while leading to more compact models. We present a detailed analysis of feature informativeness in order to gain a better understanding of diachronic change on different linguistic levels. 1 Introduction Supervised classification has been applied to various natural language processing tasks over the past decades. To date, however, distinguishing between time periods has not received extensive attention. Early research on classifying time periods is presented in de Jong et al. (2005) for Dutch. Dalli and Wilks (2006) and Kumar et al. (2011) use word frequencies for temporal classification of documents, while Sagi et al. (2009) and Kim et al. (2014) predict semantic changes over time. While lexical features are commonly used for classification approaches of time periods, features based on more abstract linguistic levels have not yet been widely investigated. In our study, we use supervised classification to distinguish scientific abstracts written in the 19th and 20th century at the sentence-level. From previous work, we know that in the scientific domain, shared expertise among authors and audience affect"
C16-1072,W16-2121,1,0.764012,"inguistic levels. In corpus-linguistic work on language change, approaches are typically frequency-based (e.g. Biber and Gray (2011; Biber and Gray (2013; Biber and Gray (2016), Taavitsainen and Pahta (2012), Moskowich and Crespo (2012)) and do not inherently account for context – diachronic change being observed through the lens of unconditioned probabilities. In contrast, information density measures as we apply them here, are based on conditional probabilities and thus inherently take context into account. Based on our previous work on long-term change using information-theoretic features (Degaetano-Ortlieb and Teich, 2016), we have shown how these features help model diachronic change, further motivating their use to classify different time periods. 3 Experimental Setup The experiments presented in this paper focus on the use of sentence-level information density measures — in particular n-gram log-probabilities according to a language model and n-gram distribution according to frequency quartiles — to classify texts from different time periods. In this section, we present the supervised classification setup and the set of features as well as the data used. 3.1 Supervised Classification A linear Support Vector"
C16-1072,N01-1021,0,0.151434,"onvention-driven style with respect to grammar (Biber and Gray, 2011; Biber and Gray, 2016; Banks, 2005). Considering that language variation affects all linguistic levels — from sounds and words to syntactic structure — we investigate a set of features extracted at the lexis, part-of-speech and syntactic levels to test how well they act as predictors of time period-specific language use. Moreover, based on psycholinguistic evidence it has been shown that language users choose those linguistic options that they know to be relatively predictable in a specific context to optimize communication (Hale, 2001; Levy, 2008; Demberg and Keller, 2008). To model communication in this sense, in our research we employ features based on the information-theoretic notion of surprisal or information density. Specifically, we make use of information theory inspired features on the linguistic levels of lexis, part-of-speech and syntax. In addition, these features allow an unlexicalized dense-vector representation, which enormously reduces the amount of features used for classification. Besides achieving high performance in classification, we are particularly interested in insights on long-term diachronic lingu"
C16-1072,D08-1038,0,0.0318531,"ds. So far, these kinds of features have been successfully used in classification of Gospels (see Islam and Dundia (2015) being able to identify the Greek Gospel as the original text and the American and Georgian ones as translations) and classification of human translated texts (see Rubino et al. (2016) distinguishing original from manually translated texts of different levels of expertise). 2.3 Language Change Previous computational work on diachronic change in scientific language mostly discusses short-term change (see e.g. Blei and Lafferty (2006; 2007) on changes in scientific topics and Hall et al. (2008) on the ACL anthology corpus, both using topic models) rather than long-term change and is mostly concerned with change related to lexis (such as topical shifts) rather than change on more abstract linguistic levels. In corpus-linguistic work on language change, approaches are typically frequency-based (e.g. Biber and Gray (2011; Biber and Gray (2013; Biber and Gray (2016), Taavitsainen and Pahta (2012), Moskowich and Crespo (2012)) and do not inherently account for context – diachronic change being observed through the lens of unconditioned probabilities. In contrast, information density meas"
C16-1072,Y15-2012,0,0.0205311,"ng lower surprisal. However, not only changes in lexis will be reflected in changes of surprisal values. From studies on language change, we know that diachronically there has been, for example, a shift from a more verbal towards a more nominal style (cf. notably Biber and Gray (2011)). This will have an impact on surprisal values with respect to grammatical units (such as parts of speech or syntactic units), motivating the use of information theory inspired features to classify between time periods. So far, these kinds of features have been successfully used in classification of Gospels (see Islam and Dundia (2015) being able to identify the Greek Gospel as the original text and the American and Georgian ones as translations) and classification of human translated texts (see Rubino et al. (2016) distinguishing original from manually translated texts of different levels of expertise). 2.3 Language Change Previous computational work on diachronic change in scientific language mostly discusses short-term change (see e.g. Blei and Lafferty (2006; 2007) on changes in scientific topics and Hall et al. (2008) on the ACL anthology corpus, both using topic models) rather than long-term change and is mostly conce"
C16-1072,L16-1305,1,0.601364,"cies. Table 1: Statistics (in thousands) of the corpora used in our experiments. 3.2 Datasets Four corpora are used in our experiments, two for each time period (early 19th century: 1800-1850; late 20th century: 1970-2007). Two corpora compose our training, development and test sets (henceforth: 19cA and 20cA) while two others allow us to train language models and extract n-gram frequencies (henceforth 19cLM and 20cLM). Statistics about these corpora are presented in Table 1a and Table 1b. For the 19th century time period, we use a corpus of research articles from the Royal Society of London (Kermes et al., 2016). Abstracts are taken from this corpus to form the 19cA classification subset. For feature extraction full research articles (19cLM) are taken from the same corpus, filtering out articles with abstracts included in 19cA. For the 20th century time period, abstracts are taken from a corpus of research articles (Degaetano-Ortlieb et al., 2013) covering several disciplines1 as our 20cA classification subset. For feature extraction, we collected abstracts from several fields (20cLM) matching those of 20cA. The main difference between 19cLM and 20cLM is the type of document used to extract them, the"
C16-1072,W14-2517,0,0.0136326,"We present a detailed analysis of feature informativeness in order to gain a better understanding of diachronic change on different linguistic levels. 1 Introduction Supervised classification has been applied to various natural language processing tasks over the past decades. To date, however, distinguishing between time periods has not received extensive attention. Early research on classifying time periods is presented in de Jong et al. (2005) for Dutch. Dalli and Wilks (2006) and Kumar et al. (2011) use word frequencies for temporal classification of documents, while Sagi et al. (2009) and Kim et al. (2014) predict semantic changes over time. While lexical features are commonly used for classification approaches of time periods, features based on more abstract linguistic levels have not yet been widely investigated. In our study, we use supervised classification to distinguish scientific abstracts written in the 19th and 20th century at the sentence-level. From previous work, we know that in the scientific domain, shared expertise among authors and audience affects their language use. Over a longer time period, it drives the evolution of domain-specific language with respect to lexis (Halliday,"
C16-1072,D08-1025,0,0.133066,"iven style with respect to grammar (Biber and Gray, 2011; Biber and Gray, 2016; Banks, 2005). Considering that language variation affects all linguistic levels — from sounds and words to syntactic structure — we investigate a set of features extracted at the lexis, part-of-speech and syntactic levels to test how well they act as predictors of time period-specific language use. Moreover, based on psycholinguistic evidence it has been shown that language users choose those linguistic options that they know to be relatively predictable in a specific context to optimize communication (Hale, 2001; Levy, 2008; Demberg and Keller, 2008). To model communication in this sense, in our research we employ features based on the information-theoretic notion of surprisal or information density. Specifically, we make use of information theory inspired features on the linguistic levels of lexis, part-of-speech and syntax. In addition, these features allow an unlexicalized dense-vector representation, which enormously reduces the amount of features used for classification. Besides achieving high performance in classification, we are particularly interested in insights on long-term diachronic linguistic change"
C16-1072,P14-5010,0,0.0120413,"hematical expressions, etc. 3.3 Feature Sets We consider three sets of features: shallow base-line features, n-gram frequency features, and information density features. Both n-gram and features specifically referred to as information density features capture aspects of information density and rely on the external resources presented in Table 1b. Shallow features Here we consider popular lexical features such as bags of character and token ngrams as a baseline, as well as bags of part-of-speech (POS) n-grams (n ∈ [1; 3]). For POS tagging and syntactic parsing, we use the Stanford NLP toolkit (Manning et al., 2014).2 For bags of token n-grams, three feature sets are built: one taking into account all n-grams, one considering n-grams appearing at least 200 times in the training corpus and one keeping only n-grams appearing at least 500 times, noted Tokens All, Tokens 200 and Tokens 500 respectively. The two latter sets allow for more compact models and less sparsity in the feature vectors. Additionally, 13 surface features are used, extracted from the surface-level of each sentence, which aim to capture meta representations of sentences’ lexical form including sentence and average word lengths, the numbe"
C16-1072,P12-2051,0,0.077108,"Missing"
C16-1072,N16-1110,1,0.890051,"Missing"
C16-1072,W09-0214,0,0.0354212,"o more compact models. We present a detailed analysis of feature informativeness in order to gain a better understanding of diachronic change on different linguistic levels. 1 Introduction Supervised classification has been applied to various natural language processing tasks over the past decades. To date, however, distinguishing between time periods has not received extensive attention. Early research on classifying time periods is presented in de Jong et al. (2005) for Dutch. Dalli and Wilks (2006) and Kumar et al. (2011) use word frequencies for temporal classification of documents, while Sagi et al. (2009) and Kim et al. (2014) predict semantic changes over time. While lexical features are commonly used for classification approaches of time periods, features based on more abstract linguistic levels have not yet been widely investigated. In our study, we use supervised classification to distinguish scientific abstracts written in the 19th and 20th century at the sentence-level. From previous work, we know that in the scientific domain, shared expertise among authors and audience affects their language use. Over a longer time period, it drives the evolution of domain-specific language with respec"
E17-3002,W15-0514,0,0.030732,"r, they do not offer effective functionalities for 1) easy access to the argumentative structure of debate content, and 2) quick overviews of the various semantic facets, the polarity and the relevance of the arguments. Some platforms1 allow users to label posts as pro or con arguments, to cite external sources, to assess debate content or to create structured debates across the web, but do not offer any deeper automatic language technologybased analyses. Argumentation mining research, which could help in automatically structuring debates, has only recently been applied to web debate corpora (Boltuzic and Snajder, 2015; Petasis and Karkaletsis, 2016; Egan et al., 2016). Our goal is to address these issues by developing a debate platform that: • Supports debate participants in making substantial and clear contributions • Facilitates an overview of debate contents • Associates relevant information available on the web with debate topics • Connects regional discussions to global deliberations • Supports advanced participation in deliberations, without sacrificing transparency and usability. In this paper, we present the Common Round platform, which implements various functions towards these goals, with followi"
E17-3002,W16-2816,0,0.0376372,"Missing"
E17-3002,P09-1113,0,0.0119288,"odels to recognize standard entity types, such as persons, organizations, locations and date/time expressions. For non-standard concept types, we use SProUT (Drozdzynski et al., 2004), which implements a regular expression-like rule formalism and gazetteers for detecting domain-specific concepts in text. Relation extraction is performed by matching dependency parse trees of sentences to a set of automatically learned dependency patterns (Krause et al., 2012). Relation patterns are learned from corpora manually annotated with event type, argument types, and roles, or using distant supervision (Mintz et al., 2009). For sentiment analysis, we apply a lexicon-based approach that additionally makes use of syntactic information in order to handle negation. Text Analytics and Association with External Material The Common Round platform enriches the contents of debates and posts by extracting information about topics, sentiment, entities and relations. Topic detection helps to find semantically related debates. Sentiment analysis allows determining the emotion level in discussions. By identifying, for example, instances of the relation MayTreatDisorder in a discussion about the legalization of cannabis, we c"
E17-3002,W16-2811,0,\N,Missing
F12-2049,2011.mtsummit-papers.35,0,0.0797069,"Missing"
F12-2049,W07-0732,0,0.0439527,"Missing"
F12-2049,W08-0509,0,0.0277659,"Missing"
F12-2049,2007.mtsummit-papers.34,0,0.0798289,"Missing"
F12-2049,P07-2045,0,0.0141972,"Missing"
F12-2049,N03-1017,0,0.0162392,"Missing"
F12-2049,P03-1021,0,0.0590379,"Missing"
F12-2049,W07-0704,0,0.0361111,"Missing"
F12-2049,P02-1040,0,0.0941639,"Missing"
F12-2049,N07-1064,0,0.0504043,"Missing"
F12-2049,W07-0728,0,0.0418624,"Missing"
F12-2049,2007.tmi-papers.27,0,0.0213151,"Missing"
F12-2049,2011.mtsummit-papers.16,0,0.0373847,"Missing"
I13-1153,W10-1408,1,0.876079,"Missing"
I13-1153,W12-3108,0,0.649958,"amon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a hand-crafted broad-coverage grammar/parser and a statistical constituency parser. Avramidis (2012) builds models for estimating post-editing effort using syntactic features such as parse probabilities and label frequency. Like Hardmeier et al. (2012), we use tree kernels to represent the output of a parser, but unlike all the previous works, we explicitly examine the role of parser accuracy. There have been some attempts to investigate the role of parser accuracy in downstream applications. Johannson and Nugues (2007) introduce an English constituency-to-dependency converter and find that syntactic dependency trees produced using this converter help semantic role labelling more than depend"
I13-1153,candito-etal-2010-statistical,0,0.157828,"Missing"
I13-1153,P02-1034,0,0.152214,"the larger subsets. 3 WSJ Section 23 and the FTB test set. 4 Quality Estimation • ACCEPT5 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprietary rule-based system • Bing6 : an online translation system The translations are scored at the segment level using segment-level BLEU. The data set is randomly split into 3000 training, 500 development, and 1000 test segments. Model parameters are tuned using the development set. We encode syntactic information using tree kernels (Collins and Duffy, 2002; Moschitti, 2006) because they allow us to use all subtrees of the 4 http://www.statmt.org/wmt13 http://www.accept.unige.ch/Products/ D_4_1_Baseline_MT_systems.pdf 6 http://www.bing.com/translator 1093 5 Training size F1 100 51.06 1K 72.53 English 10K 20K 87.69 88.47 40K 89.55 100 52.85 500 66.51 French 2.5K 78.55 5K 81.85 10K 83.40 Table 1: Parser F1 s for various training set sizes: the sizes in bold are selected for the experiments. parsed sentences as features in an efficient way, thus obviating the need for manual feature engineering. We use SVMLight-TK7 (Moschitti, 2006), a support vect"
I13-1153,W08-1301,0,0.303853,"Missing"
I13-1153,W03-2806,0,0.0804443,"Missing"
I13-1153,2005.eamt-1.15,0,0.166348,", and, in particular, the role of syntactic features. We ask the following: To what extent is QE for MT influenced by the quality of the syntactic information provided to it? Does the accuracy of the parsing model used to provide the syntactic features influence the accuracy of the QE system? We compare two pairs of parsing systems which differ with respect to their Parseval f-scores by around 17 absolute points in Related Work Features extracted from parser output have been used before in QE for MT. Quirk (2004) uses a feature which indicates whether a full parse for a sentence can be found. Gamon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a hand-crafted broad-coverage grammar/parser and a statistical constituency parser. Avramidis (2012) b"
I13-1153,2011.mtsummit-papers.51,0,0.0339492,"ndency parsers in a task-based evaluation involving an answer extraction system but bigger differences be1092 International Joint Conference on Natural Language Processing, pages 1092–1096, Nagoya, Japan, 14-18 October 2013. tween the two parsers when evaluated intrinsically. Quirk and Corston-Oliver (2006) demonstrate that a syntax-enhanced MT system is sensitive to a decrease in parser accuracy obtained by training the parser on smaller training sets. Zhang et al. (2010) experiment with a different syntax-enhanced MT system and do not observe the same behaviour. Both Miyao et al. (2008) and Goto et al. (2011) evaluate a suite of state-of-the-art English statistical parsers on the tasks of protein-pair interaction identification and patent translation respectively, and find only small (albeit sometimes statistically significant) differences between the parsing systems. Our study is closest to that of Quirk and Corston-Oliver (2006) since we are taking one parser and using it to train various models with different training set sizes. However, these models fail to parse about 10 and 2 percent of our English and French data respectively. Since the failed sentences are not necessarily parallel in the s"
I13-1153,W12-3112,0,0.0644446,"tic features influence the accuracy of the QE system? We compare two pairs of parsing systems which differ with respect to their Parseval f-scores by around 17 absolute points in Related Work Features extracted from parser output have been used before in QE for MT. Quirk (2004) uses a feature which indicates whether a full parse for a sentence can be found. Gamon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a hand-crafted broad-coverage grammar/parser and a statistical constituency parser. Avramidis (2012) builds models for estimating post-editing effort using syntactic features such as parse probabilities and label frequency. Like Hardmeier et al. (2012), we use tree kernels to represent the output of a parser, but unlike all the previous works,"
I13-1153,W07-2416,0,0.0939502,"Missing"
I13-1153,W04-3250,0,0.208003,"uency and dependency parse tree kernels of the source and translation sides, exploring first the higheraccuracy parse trees. Table 2 shows the performance of this system (CD-STH ) compared to the system trained on the baseline features (B-WMT). We also compare to another baseline (B-Mean) which always predicts the mean of the segmentlevel BLEU scores of the training instances. We evaluate performance using Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). To test the statistical significance of the performance differences (at p < 0.05), we use paired bootstrap resampling (Koehn, 2004). CD-STH achieves statistically significantly bet7 http://disi.unitn.it/moschitti/ Tree-Kernel.htm 8 A word is a child of its dependency relation to its head and this dependency relation is the child of the head word. ter RMSE and Pearson r than both baselines, which shows the usefulness of tree kernels in QE. We combine CD-STH and B-WMT 9 – this system B+CD-STH performs statistically significantly better than both systems individually, suggesting that tree kernels can also be useful in synergy with non-syntactic features. B-Mean B-WMT CD-STH B+CD-STH RMSE 0.1626 0.1601 0.1581 0.1570 Pearson r"
I13-1153,2005.mtsummit-papers.11,0,0.00494316,"dency trees using the Stanford converter for English (de Marneffe and Manning, 2008) and Const2Dep (Candito et al., 2010) for French. The labels must be removed from the arcs in the dependency trees before they can be used in SVMLight-TK – the nodes in the resulting tree representation are word forms and dependency relations, omitting part-of-speech tags.8 Based on preliminary experiments on our development set, we use subset tree kernels. We build a baseline system with features provided for the WMT 2012 QE shared task (Callison-Burch et al., 2012): we use Europarl v7 and News Commentary v8 (Koehn, 2005) to extract n-gram frequency, language model and word alignment features. This is considered a strong baseline as the system that used just these features was ranked higher than many of the other systems. 5 Experiments and Results We build a QE system using constituency and dependency parse tree kernels of the source and translation sides, exploring first the higheraccuracy parse trees. Table 2 shows the performance of this system (CD-STH ) compared to the system trained on the baseline features (B-WMT). We also compare to another baseline (B-Mean) which always predicts the mean of the segment"
I13-1153,H94-1020,0,0.0268323,"splitting the treebank non-terminals, estimating probabilities for the new rules using Expectation Maximization and merging the less useful splits (Petrov et al., 2006), and which parses using the max-rule parsing algorithm (Petrov and Klein, 2007). In order to investigate the effect of parsing accuracy, we train two parsing models – one “higheraccuracy” model and one “lower-accuracy” model – for each language. We use training set size to control the accuracy. For English, the higheraccuracy model is trained on Sections 2-21 of the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Marcus et al., 1994) (approx 40k sentences). For French, the higher-accuracy model is trained on the training section of the French Treebank (FTB) (Abeill´e et al., 2003) (approx 10k sentences). For the lower-accuracy models, we first select four random subsets of varying sizes from the larger training sets for each language2 and measure the performance of the resulting models on the standard parsing test sets3 using Parseval F1 – see Table 1. All parsing models are trained with 5 split/merge cycles. The worst-performing models for each language are those trained on 100 training sentences. 1 https://github.com/CN"
I13-1153,P08-1006,0,0.0194089,"erences between two dependency parsers in a task-based evaluation involving an answer extraction system but bigger differences be1092 International Joint Conference on Natural Language Processing, pages 1092–1096, Nagoya, Japan, 14-18 October 2013. tween the two parsers when evaluated intrinsically. Quirk and Corston-Oliver (2006) demonstrate that a syntax-enhanced MT system is sensitive to a decrease in parser accuracy obtained by training the parser on smaller training sets. Zhang et al. (2010) experiment with a different syntax-enhanced MT system and do not observe the same behaviour. Both Miyao et al. (2008) and Goto et al. (2011) evaluate a suite of state-of-the-art English statistical parsers on the tasks of protein-pair interaction identification and patent translation respectively, and find only small (albeit sometimes statistically significant) differences between the parsing systems. Our study is closest to that of Quirk and Corston-Oliver (2006) since we are taking one parser and using it to train various models with different training set sizes. However, these models fail to parse about 10 and 2 percent of our English and French data respectively. Since the failed sentences are not necess"
I13-1153,E06-1015,0,0.236135,"J Section 23 and the FTB test set. 4 Quality Estimation • ACCEPT5 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprietary rule-based system • Bing6 : an online translation system The translations are scored at the segment level using segment-level BLEU. The data set is randomly split into 3000 training, 500 development, and 1000 test segments. Model parameters are tuned using the development set. We encode syntactic information using tree kernels (Collins and Duffy, 2002; Moschitti, 2006) because they allow us to use all subtrees of the 4 http://www.statmt.org/wmt13 http://www.accept.unige.ch/Products/ D_4_1_Baseline_MT_systems.pdf 6 http://www.bing.com/translator 1093 5 Training size F1 100 51.06 1K 72.53 English 10K 20K 87.69 88.47 40K 89.55 100 52.85 500 66.51 French 2.5K 78.55 5K 81.85 10K 83.40 Table 1: Parser F1 s for various training set sizes: the sizes in bold are selected for the experiments. parsed sentences as features in an efficient way, thus obviating the need for manual feature engineering. We use SVMLight-TK7 (Moschitti, 2006), a support vector machine (SVM) i"
I13-1153,N07-1051,0,0.0136047,"ed for the WMT13 translation task.4 To remain independent of any one MT system, we translate the dataset with the following three systems, randomly choosing 1500 distinct segments from each: Parsing For parsing we use the LORG parser (Attia et al., 2010)1 which learns a latent-variable probabilistic context-free grammar (PCFG-LA) from a treebank in an iterative process of splitting the treebank non-terminals, estimating probabilities for the new rules using Expectation Maximization and merging the less useful splits (Petrov et al., 2006), and which parses using the max-rule parsing algorithm (Petrov and Klein, 2007). In order to investigate the effect of parsing accuracy, we train two parsing models – one “higheraccuracy” model and one “lower-accuracy” model – for each language. We use training set size to control the accuracy. For English, the higheraccuracy model is trained on Sections 2-21 of the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Marcus et al., 1994) (approx 40k sentences). For French, the higher-accuracy model is trained on the training section of the French Treebank (FTB) (Abeill´e et al., 2003) (approx 10k sentences). For the lower-accuracy models, we first select four r"
I13-1153,P06-1055,0,0.0397184,"omly select 4500 parallel segments from the News development data sets released for the WMT13 translation task.4 To remain independent of any one MT system, we translate the dataset with the following three systems, randomly choosing 1500 distinct segments from each: Parsing For parsing we use the LORG parser (Attia et al., 2010)1 which learns a latent-variable probabilistic context-free grammar (PCFG-LA) from a treebank in an iterative process of splitting the treebank non-terminals, estimating probabilities for the new rules using Expectation Maximization and merging the less useful splits (Petrov et al., 2006), and which parses using the max-rule parsing algorithm (Petrov and Klein, 2007). In order to investigate the effect of parsing accuracy, we train two parsing models – one “higheraccuracy” model and one “lower-accuracy” model – for each language. We use training set size to control the accuracy. For English, the higheraccuracy model is trained on Sections 2-21 of the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Marcus et al., 1994) (approx 40k sentences). For French, the higher-accuracy model is trained on the training section of the French Treebank (FTB) (Abeill´e et al., 200"
I13-1153,W06-1608,0,0.0641369,"using this converter help semantic role labelling more than dependency trees produced using an older converter despite the fact that trees produced using the older converter have higher attachment scores than trees produced using the new converter. Moll´a and Hutchinson (2003) find significant differences between two dependency parsers in a task-based evaluation involving an answer extraction system but bigger differences be1092 International Joint Conference on Natural Language Processing, pages 1092–1096, Nagoya, Japan, 14-18 October 2013. tween the two parsers when evaluated intrinsically. Quirk and Corston-Oliver (2006) demonstrate that a syntax-enhanced MT system is sensitive to a decrease in parser accuracy obtained by training the parser on smaller training sets. Zhang et al. (2010) experiment with a different syntax-enhanced MT system and do not observe the same behaviour. Both Miyao et al. (2008) and Goto et al. (2011) evaluate a suite of state-of-the-art English statistical parsers on the tasks of protein-pair interaction identification and patent translation respectively, and find only small (albeit sometimes statistically significant) differences between the parsing systems. Our study is closest to t"
I13-1153,quirk-2004-training,0,0.450292,"to represent the translation pairs. The aspect of the task that we focus on is the feature set, and, in particular, the role of syntactic features. We ask the following: To what extent is QE for MT influenced by the quality of the syntactic information provided to it? Does the accuracy of the parsing model used to provide the syntactic features influence the accuracy of the QE system? We compare two pairs of parsing systems which differ with respect to their Parseval f-scores by around 17 absolute points in Related Work Features extracted from parser output have been used before in QE for MT. Quirk (2004) uses a feature which indicates whether a full parse for a sentence can be found. Gamon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a"
I13-1153,W12-3117,1,0.844745,"Missing"
I13-1153,2010.amta-papers.3,0,0.73638,"Missing"
I13-1153,2009.eamt-1.5,0,0.0569683,"LEU score of EnglishFrench translations. In order to examine the effect of the accuracy of the parse tree on the accuracy of the quality estimation system, we experiment with various parsing systems which differ substantially with respect to their Parseval f-scores. We find that it makes very little difference which system we choose to use in the quality estimation task – this effect is particularly apparent for source-side English parse trees. 1 2 Introduction Much research has been carried out on quality estimation (QE) for machine translation (MT) (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009; Callison-Burch et al., 2012), with the aim of solving the problem of how to accurately assess the quality of a translation without access to a reference translation. Approaches differ with respect to the nature of the quality scores being estimated (binary, 5-point or real-valued scales; human evaluations versus automatic metrics), the learning algorithms used or the feature set chosen to represent the translation pairs. The aspect of the task that we focus on is the feature set, and, in particular, the role of syntactic features. We ask the following: To what extent is QE for MT influenced"
I13-1153,2003.mtsummit-papers.52,0,0.232368,"ct the segment-level BLEU score of EnglishFrench translations. In order to examine the effect of the accuracy of the parse tree on the accuracy of the quality estimation system, we experiment with various parsing systems which differ substantially with respect to their Parseval f-scores. We find that it makes very little difference which system we choose to use in the quality estimation task – this effect is particularly apparent for source-side English parse trees. 1 2 Introduction Much research has been carried out on quality estimation (QE) for machine translation (MT) (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009; Callison-Burch et al., 2012), with the aim of solving the problem of how to accurately assess the quality of a translation without access to a reference translation. Approaches differ with respect to the nature of the quality scores being estimated (binary, 5-point or real-valued scales; human evaluations versus automatic metrics), the learning algorithms used or the feature set chosen to represent the translation pairs. The aspect of the task that we focus on is the feature set, and, in particular, the role of syntactic features. We ask the following: To what extent is"
I13-1153,W12-3102,0,\N,Missing
I13-1153,C04-1046,0,\N,Missing
I13-1166,P07-1038,0,0.0194114,"nguage Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013) or averaged human evaluations (Specia et al., 2009; Callison-Burch et al., 2012). As regards the learning algorithms used, several have been tried, with support vector machine and decision tree learning proving popular (Callison-Burch et al., 2012). 3 Dataset We use the datase"
I13-1166,2012.eamt-1.41,1,0.89917,"Missing"
I13-1166,P06-4018,0,0.00750392,"the following information (in the form of one feature for the source segment, one for the target segment and, where appropriate, one for the ratio between the two): - 11 Case Features the number of upper and lowercased words, the number of fully uppercased words, the number of mixed-case words and whether or not the segment begins with an uppercase letter. - 13 Punctuation Features the ratio between punctuation characters and other characters, the number of words containing a full stop, the number of sentences produced by an off-the-shelf sentence splitter for each segment (included in NLTK (Bird, 2006)), whether or not the segment contains a dash, an ellipsis, and whether or not the segment ends with a punctuation symbol. - 9 Acronym and Emotion Features the number of web and IT-domain acronyms and the number of emoticons. - 4 Linguistic Features the number of spelling mistakes flagged by the spellchecker L ANGUAGE T OOL5 and whether or not the segment starts with a verb (indicating imperatives or questions). 5 Experiments Classification models are built using the C-SVC implementation in L IB SVM (Chang and Lin, 2011) with a Radial Basis Function (RBF) kernel. Optimal hyper-parameters C and"
I13-1166,P07-2045,0,0.00300313,"ry scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013) or averaged human evaluations (Specia et al., 2009; Callison-Burch et al., 2012). As regards the learning algorithms used, several have been tried, with support vector machine and decision tree learning proving popular (Callison-Burch et al., 2012). 3 Dataset We use the dataset presented in Roturier and Bensadoun (2011), which was obtained by machinetranslating 694 English segments, harvested from the Symantec English Norton forum1 , into French using three different translators (M OSES (Koehn et al., 2007), M ICROSOFT2 (MS) and S YSTRAN). The translations were then evaluated in terms of comprehensibility (1 to 5 scale) and fidelity (binary scale) by human annotators. The source side of this data set represents user-generated content – see Banerjee et al. (2012) for a detailed description of the characteristics of this type of data and see Table 2 for some examples. For each of the three translators, we extract 500 segments from this dataset to build our training sets. The remaining 194 segments per translator are used as test sets. The distribution of the comprehensibility and fidelity classes"
I13-1166,P00-1056,0,0.181157,"e repeat this feature extraction process using four LMs built on the Symantec Translation Memories (TMs)3 and four LMs built on the monolingual Symantec forum data4 . - 15 MT Output Language Model Features A M OSES English-French PB-SMT system is trained on the Symantec TMs and the same target LM used to extract the baseline LM features. The English side of the Symantec Norton monolingual forum data is translated by this system and the output is used to build a 5-gram LM. Target features are then extracted in a similar way as the standard LM features. - 4 Word Alignment Features Using GIZA++ (Och and Ney, 2000) and the Symantec TMs, word alignment probabilities are extracted from the source and target segments. - 78 n-gram Frequency Features The number of source and target segments unigrams seen in a reference corpus plus the percentage of n-grams in frequency quartiles (n ∈ [1; 5]). The reference corpus is the same corpus used to extract the LM features. 3 ∼ 1.6M aligned segments in English and French. ∼ 3M segments in English and ∼ 40k segments in French. 4 1168 so loe and behold I get a Internet Worm Protection Signature File Version: 20090511.001. on 5/20/09 in the afternoon Start NIS 2009 > In"
I13-1166,quirk-2004-training,0,0.247742,"lass, or continuous scores. First applied at the word level (Gandrabur and Foster, 2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores"
I13-1166,2009.eamt-1.15,0,0.0175197,"tors. Our experiments show that tried-and-tested quality estimation features work well on this type of data but that extending this set can be beneficial. We also show that the performance of particular types of features depends on the type of system used to produce the translation. 1 Introduction Quality Estimation (QE) involves judging the correctness of a system output given an input without any output reference. Substantial progress has been made on QE for Machine Translation (MT), but research has been mainly conducted on wellformed, edited text (Blatz et al., 2003; Ueffing et al., 2003; Raybaud et al., 2009; Specia et al., 2009). We turn our attention to estimating the quality of user-generated content (UGC) translation – a particularly relevant use of QE since the translation process is likely to be affected by the noisy nature of the input, particularly if the MT system is trained on well-formed text. The source language content is collected from an IT Web forum in English and translated into French by three automatic systems. For each MT system, the produced translation is manually evaluated following two criteria: the translation comprehensibility and fidelity. We evaluate several feature se"
I13-1166,W12-3117,1,0.878731,"or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013) or averaged human evaluations (Specia et al., 2009; Callison-Burch et al., 2012). As regards the learning algorithms used, several have been tried, with support vector machine and decision tree learning proving popular (Callison-Burch et al., 2012). 3 Dataset We use the dataset presented in Roturier and Bensadoun (2011), which was obtained by machinetranslating 694 English segments, harvested from the Symantec English Norton for"
I13-1166,2013.mtsummit-posters.13,1,0.896472,"Missing"
I13-1166,2010.amta-papers.3,0,0.26557,"2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013)"
I13-1166,2005.eamt-1.15,0,0.0314964,"andrabur and Foster, 2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic m"
I13-1166,2009.eamt-1.5,0,0.182946,"show that tried-and-tested quality estimation features work well on this type of data but that extending this set can be beneficial. We also show that the performance of particular types of features depends on the type of system used to produce the translation. 1 Introduction Quality Estimation (QE) involves judging the correctness of a system output given an input without any output reference. Substantial progress has been made on QE for Machine Translation (MT), but research has been mainly conducted on wellformed, edited text (Blatz et al., 2003; Ueffing et al., 2003; Raybaud et al., 2009; Specia et al., 2009). We turn our attention to estimating the quality of user-generated content (UGC) translation – a particularly relevant use of QE since the translation process is likely to be affected by the noisy nature of the input, particularly if the MT system is trained on well-formed text. The source language content is collected from an IT Web forum in English and translated into French by three automatic systems. For each MT system, the produced translation is manually evaluated following two criteria: the translation comprehensibility and fidelity. We evaluate several feature sets on the UGC dataset"
I13-1166,W03-0413,0,0.306327,"Section 5. A discussion of the results, as well as a comparison with previous work, are presented in Section 6. Finally, we conclude and suggest future work in Section 7. 2 Background The main approach for QE in MT is based on estimating how correct MT output is through characteristic elements extracted from the source and the target texts and the MT system involved in the translation process. These elements, or features, are seen as predictive parameters that can be combined with machine learning methods to estimate binary, multi-class, or continuous scores. First applied at the word level (Gandrabur and Foster, 2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Speci"
I13-1166,2003.mtsummit-papers.52,0,0.596181,"delity by human annotators. Our experiments show that tried-and-tested quality estimation features work well on this type of data but that extending this set can be beneficial. We also show that the performance of particular types of features depends on the type of system used to produce the translation. 1 Introduction Quality Estimation (QE) involves judging the correctness of a system output given an input without any output reference. Substantial progress has been made on QE for Machine Translation (MT), but research has been mainly conducted on wellformed, edited text (Blatz et al., 2003; Ueffing et al., 2003; Raybaud et al., 2009; Specia et al., 2009). We turn our attention to estimating the quality of user-generated content (UGC) translation – a particularly relevant use of QE since the translation process is likely to be affected by the noisy nature of the input, particularly if the MT system is trained on well-formed text. The source language content is collected from an IT Web forum in English and translated into French by three automatic systems. For each MT system, the produced translation is manually evaluated following two criteria: the translation comprehensibility and fidelity. We evalu"
I13-1166,P03-1054,0,0.00360087,"features are the source and target segment distributions over the 10-dimensional topic space and 3 features are the distances between these distributions, using the cosine, euclidean distance and city-block metrics. - 16 Pseudo-reference Features Following Soricut et al. (2012), we compare each MT system output to the two others using sentence-level BLEU, error information provided by TER (no. of insertions, deletions, etc.) and Levenshtein. - 3 Part-of-Speech Features We count the number of POS tag types in the source and target segments, extracted from trees produced by the Stanford parser (Klein and Manning, 2003). The ratio of these two values is also included. 4.2 UGC-related Features We also experiment with features that capture the noisy nature of UGC. Some are related to the inconsistent use of character case, some to nonstandard punctuation, some to spelling mistakes and some to the tendency of sentence splitters to underperform on this type of text. From each source-target pair, we extract the following information (in the form of one feature for the source segment, one for the target segment and, where appropriate, one for the ratio between the two): - 11 Case Features the number of upper and l"
I13-1166,W12-3102,0,\N,Missing
I13-1166,C04-1046,0,\N,Missing
I13-1166,W13-2201,0,\N,Missing
I13-1166,W12-3118,0,\N,Missing
I17-1049,C12-1163,1,0.853934,"y marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (Marcu and Echihabi, 2002). While this method promised to provide almost unlimited training data, it was shown that explicit relations differ in systematic ways from implicit relations (Asr and Demberg, 2012), so that performance on implicits is very poor when learning on explicits only (Sporleder and Lascarides, 2008). The release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples, lead to substantial improvements in classification of implicit relations, and spurred The approach proposed here differs from previous approaches, because we extend our train485 of the end-to-end parser. Implicit relations in the PDTB are only ever annotated between consecutive sentences. Therefore, we specifically extract pairs of consecutive sentences on the source English s"
I17-1049,P13-2013,0,0.0441453,"no et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 201"
I17-1049,P16-1163,0,0.0343774,"012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just"
I17-1049,P17-4012,0,0.019691,"et al., 2016; R¨onnqvist et al., 2017). The model is illustrated in Figure 2, where each word from the two discourse relational arguments is represented as a vector, which is found through a look-up word embedding. Given the word representations [w1 ,w2 ,...,wn ] as the input sequence, an Machine Translation System We train an MT system to back-translate the target side of the parallel corpus to English. To produce the highest-quality back-translation, we use a neural MT system trained on the same parallel corpus. The system is implemented by Open-source Neural Machine Translation (OpenNMT) (Klein et al., 2017). Source words are first mapped to word vectors and then fed into a recurrent neural network. 4 Case sensitive BLEU implemented in mteval-v13a.pl. Test sets available at http://www.statmt.org/ wmt15/translation-task.html 5 The non-explicit sense classification module of this parser is thus not used in the proposed method. 3 All corpora are available at http://cl.haifa.ac. il/projects/translationese/ 488 Figure 2: The bidirectional LSTM Network for the task of implicit discourse relation classification. LSTM computes the state sequence [h1 ,h2 ,...,hn ] with the following equations: it = σ(Wiw"
I17-1049,P11-1132,0,0.034184,"sists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not originally present in the source text (explicitation) (Laali and Kosseim, 2014; Koppel and Ordan, 2011; Cartoni et al., 2011; Hoek and Zufferey, 2015; Zufferey, 2016). When explicitating an implicit relation, the human translator is, in other words, disambiguating the source implicit relation with an explicit DC in the target language. Our contribution is twofold: Firstly, we propose a pipeline to automatically label English implicit discourse relation samples based on explicitation of DCs in human translation, which is the target side of a parallel corpus. Secondly, we show that the extra instances mined by the proposed method improve the performance of a standard neural classifier by a large"
I17-1049,D14-1168,0,0.0447042,"or them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify r"
I17-1049,C14-1058,0,0.106724,"sed by the community, consists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not originally present in the source text (explicitation) (Laali and Kosseim, 2014; Koppel and Ordan, 2011; Cartoni et al., 2011; Hoek and Zufferey, 2015; Zufferey, 2016). When explicitating an implicit relation, the human translator is, in other words, disambiguating the source implicit relation with an explicit DC in the target language. Our contribution is twofold: Firstly, we propose a pipeline to automatically label English implicit discourse relation samples based on explicitation of DCs in human translation, which is the target side of a parallel corpus. Secondly, we show that the extra instances mined by the proposed method improve the performance of a standard neur"
I17-1049,P14-1065,0,0.120711,"Missing"
I17-1049,P13-1047,0,0.052225,"dea has been to select explicit discourse instances that are similar to implicit ones to add to the training set. Wang et al. (2012) proposed to differentiate typical and atypical examples for each discourse relation, and augment training data for implicits only by typical explicits. In a similar vein, Rutherford and Xue (2015) proposed criteria for selecting among explicitly marked relations ones that contain discourse connectives which can be omitted without changing the interpretation of the discourse. These relations are then added to the implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-l"
I17-1049,D10-1039,0,0.0212566,"a similar vein, Rutherford and Xue (2015) proposed criteria for selecting among explicitly marked relations ones that contain discourse connectives which can be omitted without changing the interpretation of the discourse. These relations are then added to the implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (M"
I17-1049,D13-1070,0,0.0145759,"Explicit, Contingency.Cause 2. [They desperately needed somebody who showed they cared for them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused"
I17-1049,P16-1135,0,0.0824352,"implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (Marcu and Echihabi, 2002). While this method promised to provide almost unlimited training data, it was shown that explicit relations differ in systematic ways from implicit relations (Asr and Demberg, 2012), so that performance on implicits is very poor when lear"
I17-1049,D09-1036,0,0.505892,"Missing"
I17-1049,W15-0205,0,0.0220059,"usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not originally present in the source text (explicitation) (Laali and Kosseim, 2014; Koppel and Ordan, 2011; Cartoni et al., 2011; Hoek and Zufferey, 2015; Zufferey, 2016). When explicitating an implicit relation, the human translator is, in other words, disambiguating the source implicit relation with an explicit DC in the target language. Our contribution is twofold: Firstly, we propose a pipeline to automatically label English implicit discourse relation samples based on explicitation of DCs in human translation, which is the target side of a parallel corpus. Secondly, we show that the extra instances mined by the proposed method improve the performance of a standard neural classifier by a large margin, when evaluated on the PDTB 2.0 benchma"
I17-1049,P14-1092,0,0.122089,"re incomplete.]Arg2 — Explicit, Contingency.Cause 2. [They desperately needed somebody who showed they cared for them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Ear"
I17-1049,P02-1047,0,0.385847,") observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (Marcu and Echihabi, 2002). While this method promised to provide almost unlimited training data, it was shown that explicit relations differ in systematic ways from implicit relations (Asr and Demberg, 2012), so that performance on implicits is very poor when learning on explicits only (Sporleder and Lascarides, 2008). The release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples, lead to substantial improvements in classification of implicit relations, and spurred The approach proposed here differs from previous approaches, because we extend our train485 of the end-to-end p"
I17-1049,Q15-1024,0,0.196373,"iran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not re"
I17-1049,E17-1027,1,0.892968,"e Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). Both streams of methods suffer from insufficient annotated data (Wang et al., 2015), since the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which is the d"
I17-1049,E14-1068,0,0.152296,"ncreasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al.,"
I17-1049,W12-1614,0,0.103706,"y, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Che"
I17-1049,P09-1077,0,0.120841,"res to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estima"
I17-1049,N15-1081,0,0.130077,"osed method improve the performance of a standard neural classifier by a large margin, when evaluated on the PDTB 2.0 benchmark test set as well as by crossvalidation (Shi and Demberg, 2017). 2 Data extension has therefore been a longstanding goal in discourse relation classification. The main idea has been to select explicit discourse instances that are similar to implicit ones to add to the training set. Wang et al. (2012) proposed to differentiate typical and atypical examples for each discourse relation, and augment training data for implicits only by typical explicits. In a similar vein, Rutherford and Xue (2015) proposed criteria for selecting among explicitly marked relations ones that contain discourse connectives which can be omitted without changing the interpretation of the discourse. These relations are then added to the implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features t"
I17-1049,C08-2022,0,0.175133,"Missing"
I17-1049,E17-2024,1,0.676276,"; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). Both streams of methods suffer from insufficient annotated data (Wang et al., 2015), since the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which is the discourse annotated resource mostly used by the community, consists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore cr"
I17-1049,prasad-etal-2008-penn,0,0.281943,"n training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). Both streams of methods suffer from insufficient annotated data (Wang et al., 2015), since the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which is the discourse annotated resource mostly used by the community, consists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not ori"
I17-1049,C16-1180,0,0.770166,"Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 20"
I17-1049,P17-1093,0,0.385933,"samples are extracted, of which 25, 086 are inter-sentential relations and 77, 228 are intrasentential7 . Inter-sentential relations are much less abundant because stricter screening strategy is applied (the end of point 3 in Section 3). From Table 1 we can also see that majority of DCs in the “→” means from source to target side. Table 1: Numbers of intra/inter-sentence samples extracted from parallel corpora. 7 A dataset containing these additional instances will be made available to researchers upon publication of the paper. 489 Models Most common class Lin et al. (2009) Qin et al. (2016) Qin et al. (2017) Rutherford et al. (2017) Shi and Demberg (2017) (no surface features) Ours 1 PDTB only PDTB + inter-sentential samples PDTB + intra-sentential samples PDTB + all samples PDTB Test Set Cross Validation 25.36 40.20 43.81 44.65 39.56 37.68 25.59 34.44 34.32 42.29 44.29 45.50 30.01 34.14 35.08 37.84 “-” means no result currently. Table 2: Accuracy of 11-way classification of implicit discourse relations on PDTB test set and by cross validation. be largely orthogonal to the choice of classification model, we are here most interested in seeing whether adding the new instances improves over the base"
I17-1049,C12-1168,0,0.19068,"Missing"
I17-1049,D16-1253,0,0.398231,"Missing"
I17-1049,K16-2001,0,0.170987,"Missing"
I17-1049,D14-1196,0,0.0899231,"ho showed they cared for them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more"
I17-1049,D15-1266,0,0.329857,"xpressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the t"
I17-1049,C12-2138,0,0.0232655,"anslation. Parallel corpora have been exploited as a resource of discourse relation data in previous work but have mostly been used with goals different from ours: Cartoni et al. (2013) and Meyer et al. (2015) use parallel corpora to label and disambiguate discourse connectives in the target language based on explicitly marked English relations, in order to help machine translation. A second application has been to project discourse annotation from English onto other languages through parallel corpora, in order to construct discourse annotated resources for the target language (Versley, 2010; Zhou et al., 2012; Laali and Kosseim, 2014). The approach that is in spirit most similar to ours is by Wu et al. (2016), who extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus and got improvements by incorporating these data via a multi-task neural network on the 4-way classification. 3 • that are identified as the Arg1 and Arg2 of an implicit discourse relation1 ; • whose corresponding back-translated target sentences are identified as the Arg1 and Arg2 of an explicit relation; • that are not part of the Arg1 or Arg2 of any other discourse relations2 . 4. La"
I17-1049,P16-2034,1,0.813794,"other types of relations, i.e. explicit relations or no relation, in order to extract implicit-to-explicit DC translation from the parallel corpus5 . On the back-translation, the end-to-end parser is applied to identify only explicitly marked discourse relations. 4.4 Implicit relation classification model We use a Bidirectional Long Short-Term Memory (LSTM) network as the implicit relation classification model to evaluate the samples extracted by the proposed method. This architecture inspects both left and right contextual information and has been proven effective in relation classification (Zhou et al., 2016; R¨onnqvist et al., 2017). The model is illustrated in Figure 2, where each word from the two discourse relational arguments is represented as a vector, which is found through a look-up word embedding. Given the word representations [w1 ,w2 ,...,wn ] as the input sequence, an Machine Translation System We train an MT system to back-translate the target side of the parallel corpus to English. To produce the highest-quality back-translation, we use a neural MT system trained on the same parallel corpus. The system is implemented by Open-source Neural Machine Translation (OpenNMT) (Klein et al.,"
I17-1049,C10-2172,0,0.160486,"n extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 201"
N16-1110,P14-2048,0,0.0398854,"stimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of a translationese classifier. 2.4 Translator Experience J¨aa¨ skel¨ainen (1997) describes translational behaviour of professionals and non-professionals who perform translation from English into Finnish. Carl and Buch-Kromann (2010) appl"
N16-1110,C04-1046,0,0.0291151,"s distributed differently in English and German (Doherty, 2006; Fabricius-Hansen, 1996). These contrasts may impact translation, and in case of source language shining through1 , we would expect to observe differences between translations and comparable originals in terms of information density. Additionally, translations are often more specialised and more conventionalised than originals (excluding translation of fictional texts). In this paper we investigate whether and to what extent information density based features are useful in human translation classification. Quality estimation (QE) (Blatz et al., 2004; Ueffing and Ney, 2005) is the attempt to learn models that predict machine translation quality without access to a reference translation at prediction time. Translation, manual or automatic, is always a process of transforming a source into a target text. This process is prone to error. In this paper we explore whether and to what extent the extensive research on QE can be brought to bear on the problem of human translation vs. originals classification, and in particular the discrimination between novice and professional translation output. Below we explore the ability of our features to dis"
N16-1110,2010.eamt-1.14,0,0.238837,"ated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of a translationese classifier. 2.4 Translator Experience J¨aa¨ skel¨ainen (1997) describes translational behaviour of professionals and non-professionals who perform translation from English into Finnish. Carl and Buch-Kromann (2010) apply psycholinguistic methods in their analysis. They present a study of translation phases and processes for student and professional translators, relating translators’ eye movements and keystrokes to the quality of the translations produced. They show that the translation behaviour of novice and professional translators differs with respect to how they use the translation phases. Englund Dimitrova (2005) develops a combined process and product analysis and compares translators with different levels of translation experience, but concentrates only on cohesive explicitness. Most of these wor"
N16-1110,J13-4008,0,0.0253462,"on density measured on translated texts compared to comparable originally authored ones in the same language. Source language interference should result in peaks of measured surprisal values in translated texts, while the information density may remain uniform in originals. According to Hale (2001), a surprisal model allows the estimation of the probability of a parse tree given a sentence prefix. Levy (2008) showed that a lexical-based surprisal measure can be obtained by computing the negative log probability of a word given its preceding context: S = − log P (wk+1 |w1 . . . wk ). Following Demberg et al. (2013), we estimate surprisal in three ways, at the word, part-of-speech and syntax levels, based on ngram language models and language models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learn"
N16-1110,W03-0413,0,0.0672265,"language models and language models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorr"
N16-1110,N01-1021,0,0.00949344,"tion. This is often referred to as the uniform information density hypothesis (Frank and Jaeger, 2008). The information conveyed by an expression can be quantified by its surprisal, a measure of how predictable an expression is given its context. Simplification and explicitation may impact the average information density measured on translated texts compared to comparable originally authored ones in the same language. Source language interference should result in peaks of measured surprisal values in translated texts, while the information density may remain uniform in originals. According to Hale (2001), a surprisal model allows the estimation of the probability of a parse tree given a sentence prefix. Levy (2008) showed that a lexical-based surprisal measure can be obtained by computing the negative log probability of a word given its preceding context: S = − log P (wk+1 |w1 . . . wk ). Following Demberg et al. (2013), we estimate surprisal in three ways, at the word, part-of-speech and syntax levels, based on ngram language models and language models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lex"
N16-1110,P11-1132,0,0.0949733,"t our results compared to previous work is given in Section 5. Finally, conclusion and future work are provided in Section 6. 1 If translations demonstrate features more typical for the source language, see e.g. Teich (2003). 961 2 Related Work We briefly review previous work on translationese, information density, machine translation quality estimation and studies on human translation expertise. 2.1 Translationese A number of corpus-based studies on translation have shown that it is possible to automatically predict whether a text is an original or a translation (Baroni and Bernardini, 2006; Koppel and Ordan, 2011). These approaches are based on the concept of translationese – a term coined to capture the specific language of translations by Gellerstam (1986). The idea is that translations exhibit properties which distinguish them from original texts, both the source texts of the translation and comparable texts originally authored in the target language. Baker (1993; 1995) claimed these properties to be universal, i.e. (source) language-independent, emphasising general effects of the process of translation. However, translationese includes features involving both source and target language. Most lingui"
N16-1110,2009.mtsummit-papers.9,0,0.563286,"method they were produced with, are different from their source texts and from originally authored comparable texts in the target language. This has been confirmed by many linguistic studies on translation properties commonly called translationese (Gellerstam, 1986). These studies show that translations tend to share a set of lexical, syntactic and/or textual features distinguishing them from non-translated texts. As most of these features can be measured quantitatively, we are able to automatically distinguish translations from originals (Baroni and Bernardini, 2006; Ozdowska and Way, 2009; Kurokawa et al., 2009). This is useful for Statistical Machine Translation (SMT), as language and translation models can be improved if Languages provide speakers with a large number of possibilities of how they may encode messages. These include the choice of phonemes, words, syntactic structures, as well as arranging sentences in discourse. Speakers’ decisions regarding these choices are influenced by diverse factors: cognitive processing limitations can impact variation in linguistic encoding across all linguistic levels. Text production conditions, including monolingual vs. multilingual settings, can influence"
N16-1110,W13-2510,1,0.895954,"datasets used in our experiments are separated into two subsets: corpora used to extract features and corpora used to train, tune and test our classifiers. The former are taken from the publicly available bilingual English-German parallel corpora consisting of parliamentary proceedings, literary works and political commentary, compiled by (Rabinovich et al., 2015). These corpora are used individually to train language models and compute n-gram frequency distributions. Basic corpus statistics are presented in Table 1. The latter ones are composed of German texts, taken from the VARTRA corpora (Lapshinova-Koltunski, 2013), which were either originally written in German (originals) or translated from English (translations). Originals and translations belong to the same genres and registers and can be considered comparable. They include a mixture of literary, tourism and popular-scientific texts, instruction manuals, commercial letters and political essays and speeches. The VARTRA translations are split in two sets: one produced by professional translators, and one produced by translator trainees. Details are presented in Table 2. We extract balanced subsets of training, tuning and testing data containing three,"
N16-1110,J12-4004,0,0.027507,"uage and translation models. Automatic classification of original vs. translated texts has applications in machine translation, especially in studies showing the impact of the nature (original vs. translation) of the text in translation and language models used in SMT. Kurokawa et al. (2009) show that taking directionality into account when training an English-to-French phrasebased SMT system leads to improved translation performance. Ozdowska & Way (2009) analyse the same language pair and demonstrate that the nature of the original source language has an impact on the quality of SMT output. Lembersky et al. (2012) show that BLEU scores can be improved by language models compiled from translated texts and not from comparable originally authored ones. 2.2 Information Density In a natural communication situation, speakers tend to exploit variations in their linguistic encoding – modulating the order, density and specificity of their expressions to avoid informational peaks and troughs that may result in inefficient communication. This is often referred to as the uniform information density hypothesis (Frank and Jaeger, 2008). The information conveyed by an expression can be quantified by its surprisal, a"
N16-1110,P14-5010,0,0.00390023,"lexical, part-of-speech and syntactic structures observed between originals and translations, as well as between different levels of translation experience. These features are extracted the same way as the suprisal features, but based on language models trained on sentence-level reversed text. The backward language model features are popular in translation quality estimation studies and show interesting results (Duchateau et al., 2002; Rubino et al., 2013b). 3.4 Preprocessing and Tools All data used in our experiments are sentence-split, lower-cased and tokenised using the C ORE NLP toolkit (Manning et al., 2014). The part-of-speech tags and syntactic trees required to extract some features are obtained with the same set of tools. For parsing, we use the probabilistic context-free grammar model trained on the Negra corpus (Brants et al., 2003) and described in (Rafferty and Manning, 2008), before flattening the trees as illustrated in Figure 1. Both part-of-speech and flattened syntac3 Originally authored texts and translations are used separately in order to model their characteristics. (S (ADV Zugleich) (VAFIN werden) (PPER wir) (VP (ADV unerbittlich) (NP (PP (APPR mit) (ART den) (VVFIN Folgen)) (AR"
N16-1110,2009.eamt-1.14,0,0.0861729,"tions, regardless of the method they were produced with, are different from their source texts and from originally authored comparable texts in the target language. This has been confirmed by many linguistic studies on translation properties commonly called translationese (Gellerstam, 1986). These studies show that translations tend to share a set of lexical, syntactic and/or textual features distinguishing them from non-translated texts. As most of these features can be measured quantitatively, we are able to automatically distinguish translations from originals (Baroni and Bernardini, 2006; Ozdowska and Way, 2009; Kurokawa et al., 2009). This is useful for Statistical Machine Translation (SMT), as language and translation models can be improved if Languages provide speakers with a large number of possibilities of how they may encode messages. These include the choice of phonemes, words, syntactic structures, as well as arranging sentences in discourse. Speakers’ decisions regarding these choices are influenced by diverse factors: cognitive processing limitations can impact variation in linguistic encoding across all linguistic levels. Text production conditions, including monolingual vs. multilingual"
N16-1110,quirk-2004-training,0,0.0435977,"962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation pro"
N16-1110,Q15-1030,0,0.36774,"Missing"
N16-1110,W08-1006,0,0.0203033,"level reversed text. The backward language model features are popular in translation quality estimation studies and show interesting results (Duchateau et al., 2002; Rubino et al., 2013b). 3.4 Preprocessing and Tools All data used in our experiments are sentence-split, lower-cased and tokenised using the C ORE NLP toolkit (Manning et al., 2014). The part-of-speech tags and syntactic trees required to extract some features are obtained with the same set of tools. For parsing, we use the probabilistic context-free grammar model trained on the Negra corpus (Brants et al., 2003) and described in (Rafferty and Manning, 2008), before flattening the trees as illustrated in Figure 1. Both part-of-speech and flattened syntac3 Originally authored texts and translations are used separately in order to model their characteristics. (S (ADV Zugleich) (VAFIN werden) (PPER wir) (VP (ADV unerbittlich) (NP (PP (APPR mit) (ART den) (VVFIN Folgen)) (ART des) (NN Geburtenrückgangs)) (VVPP konfrontiert)) Flatten ($. .)) (TOP (S (ADV Zugleich) (VAFIN werden) (PPER wir) (VP (ADV unerbittlich) (NP (PP (APPR mit) (ART den) (VVFIN Folgen)) (ART des) (NN Geburtenrückgangs)) (VVPP konfrontiert)) ($. .))) Delexicalise (TOP (S (ADV ) (VAF"
N16-1110,2013.mtsummit-posters.13,1,0.884681,"Missing"
N16-1110,I13-1166,1,0.921127,"ss of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of a translationese clas"
N16-1110,2014.eamt-1.21,0,0.0181558,"speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations."
N16-1110,2010.amta-papers.3,0,0.0187959,"ranslation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indicators of automatic translations. In the case of human translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correlation with the performance of"
N16-1110,H05-1096,0,0.0163349,"ently in English and German (Doherty, 2006; Fabricius-Hansen, 1996). These contrasts may impact translation, and in case of source language shining through1 , we would expect to observe differences between translations and comparable originals in terms of information density. Additionally, translations are often more specialised and more conventionalised than originals (excluding translation of fictional texts). In this paper we investigate whether and to what extent information density based features are useful in human translation classification. Quality estimation (QE) (Blatz et al., 2004; Ueffing and Ney, 2005) is the attempt to learn models that predict machine translation quality without access to a reference translation at prediction time. Translation, manual or automatic, is always a process of transforming a source into a target text. This process is prone to error. In this paper we explore whether and to what extent the extensive research on QE can be brought to bear on the problem of human translation vs. originals classification, and in particular the discrimination between novice and professional translation output. Below we explore the ability of our features to distinguish between 1) non-"
N16-1110,2003.mtsummit-papers.52,0,0.0354724,"e models trained on unlexicalised part-of-speech sequences and flattened syntactic trees. Note that all resulting feature vectors do not represent lexical information but information theoretic surprisal measures. 962 2.3 Quality Estimation Machine translation QE is the process of estimating how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a supervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueffing et al., 2003; Blatz et al., 2003; Scarton and Specia, 2014). Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n-best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distinguish between originals and automatic translations is correlated with the quality of the machine translated texts (Aharoni et al., 2014): low quality translation, containing grammatical and syntactic errors, as well as incorrect lexical choices, a"
N16-1110,D11-1034,0,\N,Missing
R09-2012,W06-2810,0,0.0254433,"verage. 4 4.1 Resources Comparable Corpora Using the World Wide Web as a non-parallel corpus can solve the problems of accessibility, relevance and quantity of data. Wikipedia is a well known online free collaborative encyclopedia. Many articles are domain specific, and each document represents one concept only [13]. As in [16], we use Wikipedia1 as a comparable corpus, for the abundance of the multilingual content freely available. Wikipedia is used in many natural language processing domains, like named entity disambiguation [3], the retrieval of similar sentences across different languages [1], thesaurus extraction [13], semantic relation extraction [20], etc. Although Wikipedia has a structure that can help identify translations (cross languages links, titles of pages and section, ...), we do not consider this information this study. We want to build the context vectors for terms to translate with Wikipedia articles, which are used like concept-related word lists or semantic networks. In order to extract the information we need from Wikipedia, we rely in this work on a tool called NLGbAse2 . This tool provides a search engine with cosine similarity computed between the query and t"
R09-2012,J90-2002,0,0.657712,"Missing"
R09-2012,D07-1074,0,0.0189365,"nd the seed word lexicon size, but also of the seed word lexicon coverage. 4 4.1 Resources Comparable Corpora Using the World Wide Web as a non-parallel corpus can solve the problems of accessibility, relevance and quantity of data. Wikipedia is a well known online free collaborative encyclopedia. Many articles are domain specific, and each document represents one concept only [13]. As in [16], we use Wikipedia1 as a comparable corpus, for the abundance of the multilingual content freely available. Wikipedia is used in many natural language processing domains, like named entity disambiguation [3], the retrieval of similar sentences across different languages [1], thesaurus extraction [13], semantic relation extraction [20], etc. Although Wikipedia has a structure that can help identify translations (cross languages links, titles of pages and section, ...), we do not consider this information this study. We want to build the context vectors for terms to translate with Wikipedia articles, which are used like concept-related word lists or semantic networks. In order to extract the information we need from Wikipedia, we rely in this work on a tool called NLGbAse2 . This tool provides a se"
R09-2012,P93-1003,0,0.132568,"guage built directly from corpora. This comparison can be computed with different similarity measures. Usually, the Cosine, Jaccard or Dice coefficient [9] are used. [18] obtained better results using the city-block metric than using the Cosine, Jaccard coefficient, Euclidean distance and scalar product. [14] have also made studies on the impact of different metrics to extract terms’ translation pairs. The figure 1 illustrates this projection-based approach. 2.2 Related Works Studies on parallel corpora has allowed to identify features like co-occurrence position of a word and its translation [10, 19, 21]. Switching to non-parallel corpora implies that the co-occurrence feature is not directly applicable because there are no direct correspondences between sentences or segments. Another word feature correlating words pairs, called context heterogeneity [5], can be applied to texts in different languages which are not translations of each other. Computed on comparable corpora, the context heterogeneity measure can be used to retrieve domain66 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 66–70 this aspect is very important for initial context-vector projection. 3 Experimental"
R09-2012,P07-1084,0,0.4902,"why comparable corpora [11] are studied by researchers in multilingual terminology extraction. Some authors have shown that statistical methods can make use of comparable corpora. In particular, [17] paved the way for a family of approaches that assumes that co-occurrences of words which are translations of each other are correlated in comparable corpora. Basically, we can observe that a word and its translation appear in the same lexical environment, which can be used as a context vector [5]. This projection-based translation approach can be applied to terminology extraction. For example in [14], a term extraction program coupled with a lexical alignment program are implemented to manage this task. Usually, in projection-based term extraction, a bilingual or multilingual lexicon is needed to do the projection from one language to another. This step depends on the lexicon and the context vector. To the best of our knowledge, there are no analyses on the impact of Projection-based Approach Description In the source language text, the term to be translated is surrounded by a context consisting of other terms. This information helps us build a context vector, with a flexible window aroun"
R09-2012,P95-1050,0,0.289876,"[8]. This approach, using parallel corpora, yields good results. But the lack of parallel texts is still an issue. This is particularly true for specific domains. Building parallel corpora is time-consuming and relies heavily on human translators. Even if domain specific parallel corpora exist, terminology extraction often amounts to reverse engineering the work of human translators. This is a reason why comparable corpora [11] are studied by researchers in multilingual terminology extraction. Some authors have shown that statistical methods can make use of comparable corpora. In particular, [17] paved the way for a family of approaches that assumes that co-occurrences of words which are translations of each other are correlated in comparable corpora. Basically, we can observe that a word and its translation appear in the same lexical environment, which can be used as a context vector [5]. This projection-based translation approach can be applied to terminology extraction. For example in [14], a term extraction program coupled with a lexical alignment program are implemented to manage this task. Usually, in projection-based term extraction, a bilingual or multilingual lexicon is neede"
R09-2012,P99-1067,0,0.703168,"This information helps us build a context vector, with a flexible window around the term [6, 14] for example. Then this context has to be projected in the target language. The target context vector is built thanks to a bilingual lexicon. This lexicon-based step is the basis of projection-based approaches. To retrieve translation candidates, the projected context vector has to be compared with all possible vectors in the target language built directly from corpora. This comparison can be computed with different similarity measures. Usually, the Cosine, Jaccard or Dice coefficient [9] are used. [18] obtained better results using the city-block metric than using the Cosine, Jaccard coefficient, Euclidean distance and scalar product. [14] have also made studies on the impact of different metrics to extract terms’ translation pairs. The figure 1 illustrates this projection-based approach. 2.2 Related Works Studies on parallel corpora has allowed to identify features like co-occurrence position of a word and its translation [10, 19, 21]. Switching to non-parallel corpora implies that the co-occurrence feature is not directly applicable because there are no direct correspondences between sent"
R09-2012,W95-0114,0,0.305625,"rminology extraction often amounts to reverse engineering the work of human translators. This is a reason why comparable corpora [11] are studied by researchers in multilingual terminology extraction. Some authors have shown that statistical methods can make use of comparable corpora. In particular, [17] paved the way for a family of approaches that assumes that co-occurrences of words which are translations of each other are correlated in comparable corpora. Basically, we can observe that a word and its translation appear in the same lexical environment, which can be used as a context vector [5]. This projection-based translation approach can be applied to terminology extraction. For example in [14], a term extraction program coupled with a lexical alignment program are implemented to manage this task. Usually, in projection-based term extraction, a bilingual or multilingual lexicon is needed to do the projection from one language to another. This step depends on the lexicon and the context vector. To the best of our knowledge, there are no analyses on the impact of Projection-based Approach Description In the source language text, the term to be translated is surrounded by a context"
R09-2012,H94-1027,0,0.0102046,"guage built directly from corpora. This comparison can be computed with different similarity measures. Usually, the Cosine, Jaccard or Dice coefficient [9] are used. [18] obtained better results using the city-block metric than using the Cosine, Jaccard coefficient, Euclidean distance and scalar product. [14] have also made studies on the impact of different metrics to extract terms’ translation pairs. The figure 1 illustrates this projection-based approach. 2.2 Related Works Studies on parallel corpora has allowed to identify features like co-occurrence position of a word and its translation [10, 19, 21]. Switching to non-parallel corpora implies that the co-occurrence feature is not directly applicable because there are no direct correspondences between sentences or segments. Another word feature correlating words pairs, called context heterogeneity [5], can be applied to texts in different languages which are not translations of each other. Computed on comparable corpora, the context heterogeneity measure can be used to retrieve domain66 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 66–70 this aspect is very important for initial context-vector projection. 3 Experimental"
R09-2012,P98-1069,0,0.0360949,"co-occurrences of words between different languages are correlated in non-parallel corpora. In [18], co-occurrence matrices are built from comparable corpora, and used to compare the projected vectors with all possible vectors from the initial text. In the same study, a small seed word lexicon, which does not cover the test set, is used and expanded during the experiments with the projection-based approach. Based on these studies, [14] proposed an approach to solve multi-word term translation from non-parallel corpora. They first adapt the single-word term context vector approach proposed by [7] to multi-word term. Then, an implementation of the direct context vector method is proposed and applied to terminology extraction between unrelated languages (French/Japanese). Different metrics are compared to compute similarity between the context vector of the term to translate and the context-matrix built from the initial corpora [14]. In these studies, much attention is paid to similarity metrics between context vectors, built on single or multi-word term co-occurrence values. The projectionbased approach described here requires a bilingual seed word lexicon, and it is very surprising th"
R09-2012,1994.amta-1.26,0,0.0422048,"guage built directly from corpora. This comparison can be computed with different similarity measures. Usually, the Cosine, Jaccard or Dice coefficient [9] are used. [18] obtained better results using the city-block metric than using the Cosine, Jaccard coefficient, Euclidean distance and scalar product. [14] have also made studies on the impact of different metrics to extract terms’ translation pairs. The figure 1 illustrates this projection-based approach. 2.2 Related Works Studies on parallel corpora has allowed to identify features like co-occurrence position of a word and its translation [10, 19, 21]. Switching to non-parallel corpora implies that the co-occurrence feature is not directly applicable because there are no direct correspondences between sentences or segments. Another word feature correlating words pairs, called context heterogeneity [5], can be applied to texts in different languages which are not translations of each other. Computed on comparable corpora, the context heterogeneity measure can be used to retrieve domain66 Student Research Workshop, RANLP 2009 - Borovets, Bulgaria, pages 66–70 this aspect is very important for initial context-vector projection. 3 Experimental"
R09-2012,C98-1066,0,\N,Missing
R09-2012,C94-1084,0,\N,Missing
R09-2012,P98-1074,0,\N,Missing
R09-2012,C98-1071,0,\N,Missing
rubino-etal-2014-quality,quirk-2004-training,0,\N,Missing
rubino-etal-2014-quality,2008.iwslt-papers.1,0,\N,Missing
rubino-etal-2014-quality,W12-3102,0,\N,Missing
rubino-etal-2014-quality,P02-1040,0,\N,Missing
rubino-etal-2014-quality,P07-2045,0,\N,Missing
rubino-etal-2014-quality,C04-1046,0,\N,Missing
rubino-etal-2014-quality,2009.eamt-1.5,0,\N,Missing
rubino-etal-2014-quality,P09-1018,0,\N,Missing
rubino-etal-2014-quality,2005.mtsummit-papers.11,0,\N,Missing
rubino-etal-2014-quality,W04-3250,0,\N,Missing
rubino-etal-2014-quality,2011.eamt-1.15,0,\N,Missing
rubino-etal-2014-quality,I13-1166,1,\N,Missing
rubino-etal-2014-quality,W12-5706,1,\N,Missing
rubino-etal-2014-quality,W03-0413,0,\N,Missing
rubino-etal-2014-quality,W12-3118,0,\N,Missing
W11-2154,W08-0509,0,0.0344349,"of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga Engl"
W11-2154,2005.eamt-1.19,0,0.0365934,"l when citations included in sentences have to be translated. Two configurations were tested: zone markups inclusion around quotes and wall markups inclusion within zone markups. However, the measured gains were finally too marginal to include the method in the final system. 4.2 Parallel corpus subsampling As the only news parallel corpus provided for the workshop contains 116 k sentence pairs, we must resort to parallel out-of-domain corpora in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best co"
W11-2154,W10-1713,1,0.771338,"in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best configuration tested in (Huet et al., 2010), we index the three out-of-domain corpora using L EMUR3 , and build queries from English news-s sentences where stop words are removed. The 10 top sentence pairs retrieved per query are selected and added to the new training corpus if they are not redundant with a sentence pair already collected. The process is repeated until the training parallel corpus reaches a threshold over the number of re"
W11-2154,D07-1103,0,0.0637009,"ning on mono-news-c and news-s Training on news-c, euro and UN 5-gram models Training on 10 M sentence pairs selected in news-c, euro, UN and giga Translation model Phrase table filtering Use of -monotone-at-punctuation option Table 2: Distinct features between final configurations retained for the LIG and LIA systems 3.3 Translation model training Translation models were trained from the parallel corpora news-c, euro and UN. Data were aligned at the word-level and then used to build standard phrase-based translation models. We filtered the obtained phrase table using the method described in (Johnson et al., 2007). Since this technique drastically reduces the size of the phrase table, while not degrading (and even slightly improving) the results on the development and test corpora (System 6), we decided to employ filtered phrase tables in the final configuration of the LIG system. 3.4 Tuning For decoding, the system uses a log-linear combination of translation model scores with the LM log-probability. We prevent phrase reordering over punctuation using the M OSES option -monotone-atpunctuation. As the system can be beforehand tuned by adjusting the log-linear combination weights on a development corpus"
W11-2154,P07-2045,0,0.0138313,"10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCES ) English-French Bilingual training news-c euro UN giga English Monolingual training News Commentary v6 mono-news-c Shuffled News Crawl corpus (from 2007 to 2011) news-s Europarl v6 mono"
W11-2154,J03-1002,0,0.00334518,"evoted to model tuning: test09 was used for the development of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods. 2.2 LIG and LIA system characteristics Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. KneserNey discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using G IZA ++ (Och and Ney, 2003) or its multi-threaded version MG IZA ++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with M OSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models: 1 When not specified otherwise “our” system refers to the LIGA system. 440 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440–446, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics C ORPORA News Commentary v6 Europarl v6 United Nation corpus 109 corpus D ESIGNATION S IZE ( SENTENCE"
W11-2154,P03-1021,0,0.0208191,"us (from 2007 to 2011) news-s Europarl v6 mono-euro 116 k 1.8 M 12 M 23 M 181 k 25 M 1.8 M Development newstest2008 newssyscomb2009 newstest2009 test08 testcomb09 test09 2,051 502 2,525 test10 2,489 Test newstest2010 Table 1: Used corpora processed in order to normalize a special French form (named euphonious “t”) as described in (Potet et al., 2010). • 5 translation model scores, • 1 distance-based reordering score, • 6 lexicalized reordering score, • 1 LM score and • 1 word penalty score. The score weights were optimized on the test09 corpus according to the BLEU score with the MERT method (Och, 2003). The experiments led specifically with either LIG or LIA system are respectively described in Sections 3 and 4. Unless otherwise indicated, all the evaluations were performed using case-insensitive BLEU and were computed with the mteval-v13a.pl script provided by NIST. Table 2 summarizes the differences between the final configuration of the systems. 3 The LIG machine translation system LIG participated for the second time to the WMT shared news translation task for the French-English language pair. 3.1 Pre-processing Training data were first lowercased with the P ERL script provided for the"
W12-3117,P07-1111,0,0.0122582,"timation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation. Owczarzak et al. (2007) use labelled dependencies together with WordNet to avoid penalising valid syntactic and lexical variations in MT evaluation. In what follows, we describe how we make use of syntactic information in the QE task, i.e. evaluating MT output without a reference translation. Wagner et al."
W12-3117,W02-1503,0,0.00799189,"se of syntactic information in the QE task, i.e. evaluating MT output without a reference translation. Wagner et al. (2007; 2009) use three sources of linguistic information in order to extract features which they use to judge the grammaticality of English sentences: 1. For each POS n-gram (with n ranging from 2 to 7), a feature is extracted which represents the frequency of the least frequent n-gram in the sentence according to some reference corpus. TreeTagger (Schmidt, 1994) is used to produce POS tags. 2. Features provided by a hand-crafted, broad141 coverage precision grammar of English (Butt et al., 2002) and a Lexical Functional Grammar parser (Maxwell and Kaplan, 1996). These include whether or not a sentence could be parsed without resorting to robustness measures, the number of analyses found and the parsing time. 3. Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al., 1993), one trained on a distorted version of the treebank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions"
W12-3117,P01-1020,0,0.0369169,"ent Semantic Analysis (LSA, or Latent Semantic Indexing, LSI (Deerwester et al., 1990)). More recently, some studies were conducted on the use of LDA to adapt SMT systems to specific domains (Gong et al., 2010; Gong et al., 2011) or to extract bilingual lexicon from comparable corpora (Rubino and Linar`es, 2011). Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore"
W12-3117,W07-0738,0,0.0257641,"Missing"
W12-3117,2011.mtsummit-papers.57,0,0.0136471,"Topic and Syntax-based Features In this section, we focus on the set of features that aim to capture adequacy using topic modelling and grammaticality using POS tagging and syntactic parsing. 1 http://www.microsofttranslator.com/ The list of English and Spanish rules is available at: http://languagetool.org/languages. 2 140 Topic-based Features guage model to a specific topic using Latent Semantic Analysis (LSA, or Latent Semantic Indexing, LSI (Deerwester et al., 1990)). More recently, some studies were conducted on the use of LDA to adapt SMT systems to specific domains (Gong et al., 2010; Gong et al., 2011) or to extract bilingual lexicon from comparable corpora (Rubino and Linar`es, 2011). Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4"
W12-3117,P07-2045,0,0.00253308,"h • Number of source prepositions and conjunctions word: our assumption here is that segments containing a relatively high number of prepositions and conjunctions may be more complex and difficult to translate. • Number of source out-of-vocabulary words 139 Language Model Features All the language models (LMs) used in our work are n-gram LMs with Kneser-Ney smoothing built with the SRI Toolkit (Stolcke, 2002). • Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al. (2002) • Log probability of target segments on 5-gram MT-output-based LM: using M OSES (Koehn et al., 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. This MT output is used to build a LM that models the behavior of the MT system. We assume that for a given MT output, a high n-gram probability (or a low perplexity) of the LM indicates that the MT output contains mistakes. MT-system Features • 15 scores provided by Moses: phrase-table, language model, reordering model and word penalty (weighted and unweighted) • Number of n-bests for each source segment • MT output back-translation: from Spanish"
W12-3117,W05-0904,0,0.0194657,"a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation. Owczarzak et al. (2007) use labelled dependencies together with WordNet to avoid penalising valid syntactic and lexical variations in MT evaluation. In what follows, we describe how we make use of syntacti"
W12-3117,J93-2004,0,0.0395667,"frequent n-gram in the sentence according to some reference corpus. TreeTagger (Schmidt, 1994) is used to produce POS tags. 2. Features provided by a hand-crafted, broad141 coverage precision grammar of English (Butt et al., 2002) and a Lexical Functional Grammar parser (Maxwell and Kaplan, 1996). These include whether or not a sentence could be parsed without resorting to robustness measures, the number of analyses found and the parsing time. 3. Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al., 1993), one trained on a distorted version of the treebank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. These features were originally designed to distinguish grammatical sentences from ungrammatical ones and were tested on sentences from learner corpora by Wagner et al. (2009) and Wagner (2012). In this work we extract all three sets of features from the source side of our data and the POS-based subset from the target side.3 We use the publicly available pre-trained TreeTagger models fo"
W12-3117,D09-1092,0,0.0331213,"trics in further experiments, like the Manhattan or the Euclidean distances. Some parameters related to LDA have to be studied more carefully too, such as the number of topics (dimensions in the topic space), the number of words per topic, the Dirichlet hyperparameter α, etc. In our experiments, we built a topic model composed of 10 dimensions using Gibbs sampling with 1000 iterations. We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al., 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009). In the field of machine translation, Tam et al. (2007) propose to adapt a translation and a lanSource Syntax Features Wagner et al. (2007; 2009) propose a series of features to measure sentence grammaticality. These features rely on a part-of-speech tagger, a probabilistic parser and a precision grammar/parser. We have at our disposal these tools for English and so we apply them to the source data. The features themselves are described in more detail in Section 3.2. Target Syntax Features We use a part-of-speech tag"
W12-3117,W07-0714,0,0.0262538,"Missing"
W12-3117,P02-1040,0,0.105237,"that the MT output contains mistakes. This MT output is used to build a LM that models the behavior of the MT system. We assume that for a given MT output, a high n-gram probability (or a low perplexity) of the LM indicates that the MT output contains mistakes. MT-system Features • 15 scores provided by Moses: phrase-table, language model, reordering model and word penalty (weighted and unweighted) • Number of n-bests for each source segment • MT output back-translation: from Spanish to English using M OSES trained on the provided parallel corpus, scored with TER (Snover et al., 2006), BLEU (Papineni et al., 2002) and the Levenshtein distance (Levenshtein, 1966), based on the source segments as a translation reference Topic Model Features • Probability distribution over topics: Source and target segment probability distribution over topics for a 10-dimension topic model • Cosine distance between source and target topic vectors More details about these two features are provided in Section 3.1. 2.2 Unconstrained System In addition to the features used for the constrained system, a further 238 unconstrained features were included in our unconstrained system. MT System Features 3.1 As for our constrained s"
W12-3117,quirk-2004-training,0,0.129928,"f LDA to adapt SMT systems to specific domains (Gong et al., 2010; Gong et al., 2011) or to extract bilingual lexicon from comparable corpora (Rubino and Linar`es, 2011). Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation. O"
W12-3117,2006.amta-papers.25,0,0.0363423,"orpus into Spanish, assuming that the MT output contains mistakes. This MT output is used to build a LM that models the behavior of the MT system. We assume that for a given MT output, a high n-gram probability (or a low perplexity) of the LM indicates that the MT output contains mistakes. MT-system Features • 15 scores provided by Moses: phrase-table, language model, reordering model and word penalty (weighted and unweighted) • Number of n-bests for each source segment • MT output back-translation: from Spanish to English using M OSES trained on the provided parallel corpus, scored with TER (Snover et al., 2006), BLEU (Papineni et al., 2002) and the Levenshtein distance (Levenshtein, 1966), based on the source segments as a translation reference Topic Model Features • Probability distribution over topics: Source and target segment probability distribution over topics for a 10-dimension topic model • Cosine distance between source and target topic vectors More details about these two features are provided in Section 3.1. 2.2 Unconstrained System In addition to the features used for the constrained system, a further 238 unconstrained features were included in our unconstrained system. MT System Feature"
W12-3117,D07-1012,1,0.896831,"Missing"
W12-3117,P05-1022,0,\N,Missing
W12-5706,P09-1064,0,0.121552,"Missing"
W12-5706,I11-1153,0,0.0265379,"Missing"
W12-5706,D08-1011,0,0.0381145,"Missing"
W12-5706,P08-2021,0,0.032303,"Missing"
W12-5706,W02-1019,0,0.0944556,"Missing"
W12-5706,W10-1747,0,0.0286838,"Missing"
W12-5706,E06-1005,0,0.150468,"Missing"
W12-5706,okita-2012-annotated,1,0.586832,"Missing"
W12-5706,W10-4006,1,0.742452,"Missing"
W12-5706,P02-1040,0,0.0835599,"Missing"
W12-5706,P07-1040,0,0.0849459,"Missing"
W12-5706,W12-3117,1,0.888749,"Missing"
W12-5706,2006.amta-papers.25,0,0.031404,"Missing"
W12-5706,W12-3118,0,0.0636222,"Missing"
W12-5706,2009.eamt-1.5,0,0.0836285,"Missing"
W12-5706,D08-1065,0,0.114044,"Missing"
W12-5706,C04-1046,0,\N,Missing
W13-2227,P10-2041,0,0.156028,"Missing"
W13-2227,D11-1033,0,0.177558,"Missing"
W13-2227,N07-1029,0,0.0348966,"gaword. Each LM is trained with the SRILM toolkit, before interpolating all the LMs according to their weights obtained by minimizing the perplexity on the tuning set (WMT2011 and WMT2012 test sets). As SRILM can only interpolate 10 LMs, we first interpolate a LM with Europarl, News Commentary, News Crawl (20072012, each year individually, 6 separate parts), then we interpolate a new LM with this interpolated LM and LDC Gigawords (we kept the Gigaword subsets separated according to the news sources as distributed by LDC, which leads to 7 corpus). We also use a word-level combination strategy (Rosti et al., 2007) to combine the three translation hypotheses. To combine these systems, we first use the Minimum Bayes-Risk (MBR) (Kumar and Byrne, 2004) decoder to obtain the 5 best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al., 2000). We then use IHMM (He et al., 2008) to choose the backbone build the CN and finally search for and generate the best translation. We tune the system parameters on development set with Simple-Simplex algorithm. The parameters for system weights are set equal. Other parameters like language model, length penalty and combination coefficient are"
W13-2227,P96-1041,0,0.0728056,"scribed. We investigate the use of linguistic information to select parallel data. In Section 3, we present the systems built for the French-English pair in both di2 Setting Our setup uses the M OSES toolkit, version 1.0 (Koehn et al., 2007). We use a pipeline with the phrase-based decoder with standard parameters, unless noted otherwise. The decoder uses cube pruning (-cube-pruning-pop-limit 2000 -s 2000), MBR (-mbr-size 800 -mbr-scale 1) and monotone at punctuation reordering. Individual language models (LMs), 5-gram and smoothed using a simplified version of the improved Kneser-Ney method (Chen and Goodman, 1996), are built for each monolingual corpus using IRSTLM 5.80.01 (Federico et al., 2008). These LMs are then interpolated with IRSTLM using the test set of WMT11 as the development set. Finally, the interpolated LMs are merged into one LM preserving the weights using SRILM (Stolcke, 2002). We use all the parallel corpora available for this language pair: Europarl (EU), News Commentary (NC), United Nations (UN) and Common Crawl (CC). Regarding monolingual corpora, we use the freely available monolingual corpora (EuIntroduction 1 Spanish-English http://www.nclt.dcu.ie/mt/ http://www.prompsit.com/ 21"
W13-2227,P10-4002,0,0.0227851,"rther experiments are still required to determine the minimum sample size needed to outperform both the in-domain system and the combination of the two translation models. Finally, for the German-English language pair, we presents our exploitation of long ordering problem. We compared two hierarchical models with one phrase-based model, and we also use a system combination strategy to further improve 4.2.2 Three baseline systems We use the data set up described by the former subsection and build up three baseline systems, namely PB M OSES (phrase-based), Hiero M OSES (hierarchical) and C DEC (Dyer et al., 2010). The motivation of choosing Hierarchical Models is to address the German-English’s long reorder problem. We want to test the performance of C DEC and Hiero M OSES and choose the best. PB M OSES is used as our benchmark. The three results obtained on the development and test sets for the three baseline system and the system combination are shown in the Table 6. PB M OSES Hiero M OSES C DEC Combination System Combination Test 24.0 24.4 24.4 24.8 Table 6: BLEU scores obtained by our systems on the development and test sets for the German to English translation task. From the Table 6 we can see t"
W13-2227,W13-2803,1,0.864471,"Missing"
W13-2227,D11-1020,1,0.829215,"Comb. 30.0 30.8 29.8 58.9 29.9 30.4 29.3 59.3 29.7 29.6 28.7 61.8 29.6 29.4 28.5 62.0 4 German-English In this section we describe our work on German to English subtask. Firstly we describe the Dependency tree to string method which we tried but unfortunately failed due to short of time. Secondly we discuss the baseline system and the preprocessing we performed. Thirdly a system combination method is described. 4.1 Dependency Tree to String Method Our original plan was to address the long distance reordering problem in German-English translation. We use Xie’s Dependency tree to string method(Xie et al., 2011) which obtains good results on Chinese to English translation and exhibits good performance at long distance reordering as our decoder. We use Stanford dependency parser4 to parse the English side of the data and Mate-Tool5 for the German side. The first set of experiments did not lead to encouraging results and due to insufficient time, we decide to switch to other decoders, based on statistical phrase-based and hierarchical approaches. Table 5: BLEU and TER scores obtained by our systems. BLEUdev is the score obtained on the development set given by MERT, while BLEU, BLEUcased and TER are ob"
W13-2227,W08-0509,0,0.0846195,"Missing"
W13-2227,D08-1011,0,0.0262708,"News Crawl (20072012, each year individually, 6 separate parts), then we interpolate a new LM with this interpolated LM and LDC Gigawords (we kept the Gigaword subsets separated according to the news sources as distributed by LDC, which leads to 7 corpus). We also use a word-level combination strategy (Rosti et al., 2007) to combine the three translation hypotheses. To combine these systems, we first use the Minimum Bayes-Risk (MBR) (Kumar and Byrne, 2004) decoder to obtain the 5 best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al., 2000). We then use IHMM (He et al., 2008) to choose the backbone build the CN and finally search for and generate the best translation. We tune the system parameters on development set with Simple-Simplex algorithm. The parameters for system weights are set equal. Other parameters like language model, length penalty and combination coefficient are chosen when we see a good improvement on development set. 5 Development 22.0 22.1 22.5 23.0 Conclusion This paper presented a set of experiments conducted on Spanish-English, French-English and German-English language pairs. For the SpanishEnglish pair, we have explored the use of linguisti"
W13-2227,P07-2045,0,\N,Missing
W13-2227,N04-1022,0,\N,Missing
W13-2249,P07-2045,0,0.00595314,"nslation options and the number of nodes in the decoding graph. The set of topic model features was reduced in order to keep only those that were shown to be effective on three quality estimation datasets (the details can be found in (Rubino et al. (to appear), 2013)). These features encode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and without Part-Of-Speech (POS) agreement, were included as features. Using the alignment information provided by the decoder, we POS tagged the source and target sentences with TreeTagger (S"
W13-2249,P02-1040,0,0.0981654,"ar), 2013)). These features encode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and without Part-Of-Speech (POS) agreement, were included as features. Using the alignment information provided by the decoder, we POS tagged the source and target sentences with TreeTagger (Schmidt, 1994) and the publicly available pre-trained models for English and Spanish. We mapped the tagsets of both languages by simplifying the initial tags and obtain a reduced set of 8 tags. We applied that simplification on the tagged sentences before check"
W13-2249,W12-3117,1,0.938067,"1.1 involve estimating postediting effort for English-Spanish translation pairs in the news domain. The two systems use a wide variety of features, of which the most effective are the word-alignment, n-gram frequency, language model, POS-tag-based and pseudoreferences ones. Both systems perform at a similarly high level in the two tasks of scoring and ranking translations, although there is some evidence that the systems are over-fitting to the training data. 1 2 Features Our starting point for the WMT13 QE shared task was the feature set used in the system we submitted to the WMT12 QE task (Rubino et al., 2012). This feature set, comprising 308 features in total, extended the 17 baseline features provided by the task organisers to include 6 additional surface features, 6 additional language model features, 17 additional features derived from the MT system components and the n-best lists, 138 features obtained by part-of-speech tagging and parsing the source sentences and 95 obtained by part-of-speech tagging the target sentences, 21 topic model features, 2 features produced by a grammar checker1 and 6 pseudo-source (or backtranslation) features. We made the following modifications to this 2012 featu"
W13-2249,2006.amta-papers.25,0,0.0629642,"ncode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and without Part-Of-Speech (POS) agreement, were included as features. Using the alignment information provided by the decoder, we POS tagged the source and target sentences with TreeTagger (Schmidt, 1994) and the publicly available pre-trained models for English and Spanish. We mapped the tagsets of both languages by simplifying the initial tags and obtain a reduced set of 8 tags. We applied that simplification on the tagged sentences before checking for POS agreement. To e"
W13-2249,W12-3118,0,0.0833151,"g the number of target words aligned with source words. We extracted 8 additional features from the decoder log file, including the number of discarded hypotheses, the total number of translation options and the number of nodes in the decoding graph. The set of topic model features was reduced in order to keep only those that were shown to be effective on three quality estimation datasets (the details can be found in (Rubino et al. (to appear), 2013)). These features encode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and wit"
W13-2249,2013.mtsummit-posters.13,1,\N,Missing
W13-2255,W12-3102,0,0.0592232,"Missing"
W13-2255,W12-3114,1,0.879918,"Missing"
W13-2255,W12-3118,0,0.235627,"Missing"
W14-3319,D11-1033,0,0.199736,"nt sets, to obtain our final SMT system. Our submission for the English to French translation task was ranked second amongst nine teams and a total of twenty submissions. 1 To train the LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding pa"
W14-3319,P11-1105,0,0.180288,"encodes a different degree of generalisation over the particular example it has been extracted from. Finally, the minimum set of rules which correctly reproduces all the bilingual phrases is found based on integer linear programming search (Garfinkel and Nemhauser, 1972). Once the rules have been inferred, the phrase table is built from them and the original rulebased MT dictionaries, following the method by S´anchez-Cartagena et al. (2011), which was one of winning systems4 (together with two online SMT systems) in the pairwise manual evaluation of the WMT11 English–Spanish translation task (Callison-Burch et al., 2011). This phrasetable is then interpolated with the baseline TM and the results are presented in Table 5. A slight improvement over the baseline is observed, which motivates the use of synthetic rules in our final MT system. This small improvement may be related to the small coverage of the Apertium dictionaries: the English–French bilingual dictionary has a low number of entries compared to more mature language pairs in Apertium which have around 20 times more bilingual entries. System the number of n-bests used by MERT. Results obtained on the development set newstest2013 are reported in Table"
W14-3319,W13-2212,0,0.0160957,"except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 LDC ltw 986.9 LDC nyt 5,327.7 LDC wpb 108.8 LDC xin 5,121.9 59.9 7.4 90.2 308.1 347.0 157.8 358.1 345.5"
W14-3319,W08-0509,0,0.170818,"test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we ma"
W14-3319,P13-2121,0,0.0606903,"Missing"
W14-3319,P07-2045,0,0.0136523,"with the focus on the English to French direction. Language models (LMs) and translation 171 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171–177, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2 Datasets and Tools Corpus We use all the monolingual and parallel datasets in English and French provided by the shared task organisers, as well as the LDC Gigaword for the same languages1 . For each language, a true-case model is trained using all the data, using the traintruecaser.perl script included in the M OSES toolkit (Koehn et al., 2007). Punctuation marks of all the monolingual and parallel corpora are then normalised using the script normalize-punctuation.perl provided by the organisers, before being tokenised and true-cased using the scripts distributed with the M OSES toolkit. The same pre-processing steps are applied to the development and test sets. As development sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and wit"
W14-3319,W13-2235,0,0.294214,"he LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding parameters. Introduction The reminder of this paper is organised as follows: the datasets and tools used in our experiments are described in Section 2. Then, details about the LMs and TMs a"
W14-3319,P10-2041,0,0.0671112,"ces and (ii) reduce the total amount of out-of-domain data. Second, a novel approach for the automatic extraction of translation rules and their use to enrich the phrase table is detailed. 10 Parallel Data Filtering and Vocabulary Saturation Bilingual Cross-Entropy Difference 4.1 Amongst the parallel corpora provided by the shared task organisers, only News Commentary can be considered as in-domain regarding the development and test sets. We use this training corpus to build our baseline SMT system. The other parallel corpora are individually filtered using bilingual cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011). This data filtering method relies on four LMs, two in the source and two in the target language, which aim to model particular features of in and out-ofdomain sentences. We build the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k"
W14-3319,J03-1002,0,0.01001,"ts, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich"
W14-3319,P03-1021,0,0.0469427,"rter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over se"
W14-3319,P02-1040,0,0.0911366,"d the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k 10k Figure 1: Sample of ranked sentence-pairs (10k) of each of the out-of-domain parallel corpora with bilingual cross-entropy difference The results obtained using the pseudo indomain data show B LEU (Papineni et al., 2002) scores superior or equal to the baseline score. Only the Europarl subset is slightly lower than the baseline, while the subset taken from the 109 corpus reaches the highest B LEU compared to the other systems (30.29). This is mainly due to the 3 The subsets contain the same number of sentences and the same vocabulary as News Commentary. 173 size of this subset which is ten times larger than the one taken from Europarl. The last row of Table 3 shows the B LEU score obtained after interpolating the four pseudo in-domain translation models. This system outperforms the best pseudo indomain one by"
W14-3319,2011.mtsummit-papers.64,1,0.889124,"Missing"
W14-3319,E12-1055,0,0.0325549,"Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 L"
W14-3319,2006.amta-papers.25,0,0.0323304,"r detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation leads to an overall improvement of 0.1 B LEU absolute. The results on newstest2014 show that adding synthetic rules did not help improving B LEU and degraded slightly TER (Snover et al., 2006) scores. In addition to our English→French submission, we submitted a French→English translation. Our French→English MT system is built on the alignments obtained from the English→French direction. The training processes between the two sys27.76 28.06 Table 5: BLEU scores reported by MERT on English–French newstest2013 for the baseline SMT system standalone and with automatically extracted translation rules. 5 27.76 31.93 31.90 32.21 32.10 Table 6: B LEU scores reported by MERT on English–French newstest2013 development set. BLEUdev Baseline Baseline+Rules BLEUdev Tuning and Decoding We presen"
W14-3319,D07-1080,0,0.0658355,"experiments. Adding the synthetic translation rules degrades B LEU (as indicated by the last row in the Table), thus we decide to submit two systems to the shared task: one without and one with synthetic rules. By submitting a system without synthetic rules, we also ensure that our SMT system is constrained according to the shared task guidelines. System Baseline + pseudo in + pseudo out + OSM + MERT 200-best + Rules As MERT is not suitable when a large number of features are used (our system uses 19 fetures), we switch to the Margin Infused Relaxed Algorithm (MIRA) for our submitted systems (Watanabe et al., 2007). The development set used is newstest2012, as we aim to select the best decoding parameters according to the scores obtained when decoding the newstest2013 corpus, after detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation lea"
W14-3319,W14-3320,1,\N,Missing
W15-3022,D11-1033,0,0.0118991,"used to train the translation models, after pre-processing. lation models are presented in Table 1. Figure 1 shows how different segmentation methods affect the vocabulary size; given that linguistic segmentation have larger vocabularies as statistical their contribution to translation models may be at least partially complementary. The two unconstrained parallel datasets are split into three subsets: pseudo in-domain, pseudo outof-domain top and pseudo out-of-domain bottom, henceforth in, outt and outb. We rank the sentence pairs according to bilingual cross-entropy difference on the devset (Axelrod et al., 2011) and calculate the perplexity on the devset of LMs trained on different portions of the top ranked sentences (the top 1/64, 1/32 and so on). The subset for which we obtain the lowest perplexities is kept as in (this was 1/4 for fienwac (403.89 and 3610.95 for English and Finnish, respectively), and 1/16 for osubs (702.45 and 7032.2). The remaining part of each dataset is split in two sequential parts in ranking order of same number of lines, which are kept as outt and outb. The out-of-domain part of osubs is further processed with vocabulary saturation (Lewis and Eetemadi, 2013) in order to ha"
W15-3022,P05-1033,0,0.0336499,"ipt omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Ope"
W15-3022,P14-1129,0,0.147789,"Missing"
W15-3022,P11-1105,0,0.0620347,"d model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained"
W15-3022,espla-gomis-etal-2014-comparing,1,0.815616,"Missing"
W15-3022,D08-1089,0,0.0261127,"s. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The truecaser is lexicon-based, t"
W15-3022,W08-0509,0,0.135098,"fst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014"
W15-3022,D07-1091,0,0.013807,"ters of the word alignment, phrase extraction and decoding algorithms have not been modified to take into account the nature of the segmented data. 4.1 12 using the script omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same paral"
W15-3022,2005.iwslt-1.8,0,0.121773,"t to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The tr"
W15-3022,P07-2045,0,0.00899697,"d a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The truecaser is lexicon-based, trained on all the monolingual and parallel data. In addition, we remove sentence pairs from the parallel corpora where either side is longer than 80 tokens. Corpus Europarl v8 fienwac.in fienwac.outt fienwac.outb osubs.in osubs.outt osubs.outb Sentences (k) Words (M) Finnish English Constrained System 1,901.1 36.5 Unconstrained System 640.1 9.2 838.9 12.5 838.9 13.9 492.2 3.6 1,169.6 8.8 1,169.6 7.8 50.9 13.6 18.1 18.1 5.6 14.4 13."
W15-3022,2005.mtsummit-papers.11,0,0.134429,"ne translation (SMT) systems submitted by the Abu-MaTran project for the WMT 2015 translation task. The language pair concerned is Finnish–English with a strong focus on the English-to-Finnish direction. The Finnish language is newly introduced this year as a particular translation challenge due to its rich morphology and to the lack of resources available, compared to e.g. English or French. Morphologically rich languages, and especially Finnish, are known to be difficult to translate using phrase-based SMT systems mainly because of the large diversity of word forms leading to data scarcity (Koehn, 2005). We assume that data acquisition and morphological segmentation should contribute to decrease the out-of-vocabulary rate and thus improve the performance of SMT. To gather additional data, we decide to build on previous work conducted in the Abu-MaTran project and crawl the Web looking for monolingual and parallel corpora (Toral et al., 2014). In addition, morphological segmentation of Finnish is used in our systems as pre- and post-processing steps. Four segmentation methods are proposed in this paper, two unsupervised and two rule-based. Both constrained and unconstrained translation system"
W15-3022,W13-2235,0,0.0133834,"nce on the devset (Axelrod et al., 2011) and calculate the perplexity on the devset of LMs trained on different portions of the top ranked sentences (the top 1/64, 1/32 and so on). The subset for which we obtain the lowest perplexities is kept as in (this was 1/4 for fienwac (403.89 and 3610.95 for English and Finnish, respectively), and 1/16 for osubs (702.45 and 7032.2). The remaining part of each dataset is split in two sequential parts in ranking order of same number of lines, which are kept as outt and outb. The out-of-domain part of osubs is further processed with vocabulary saturation (Lewis and Eetemadi, 2013) in order to have a more efficient and compact system (Rubino et al., 2014). We traverse the sentence pairs in the order they are ranked 187 Corpus Europarl v8 News Commentary v10 News Shuffled 2007 2008 2009 2010 2011 2012 2013 2014 Gigaword 5th Sentences (k) Words (M) 2,218.2 344.9 59.9 8.6 3 782.5 12 954.5 14 680.0 6 797.2 15 437.7 14 869.7 21 688.4 28 221.3 28,178.1 90.2 308.1 347.0 157.8 358.1 345.5 495.2 636.6 4,831.5 Corpus Constrained System News Shuffle 2014 1,378.8 Unconstrained System FiWaC 146,557.4 System and filter out those for which we have seen already each 1-gram at least 10"
W15-3022,W14-0405,1,0.869044,"Missing"
W15-3022,J04-4002,0,0.0271016,"n form of automake scriptlets at http:// github.com/flammie/autostuff-moses-smt/. 10 The parameters of the word alignment, phrase extraction and decoding algorithms have not been modified to take into account the nature of the segmented data. 4.1 12 using the script omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Ha"
W15-3022,C14-1111,0,0.168889,"Missing"
W15-3022,W13-2506,1,0.831045,"precisionoriented. In this first step, a large amount of potentially parallel data is obtained by post-processing data collected with a TLD crawl, which is not primarily aimed at finding parallel data. To make use of this resource in a more efficient way, we re-crawl some of the most promising web sites (we call them multilingual hotspots) with the ILSP-FC crawler specialised in locating parallel documents during crawling. According to Espl`a-Gomis et al. (2014), B ITEXTOR and ILSP-FC have shown to be complementary, and combining both tools leads to a larger amount of parallel data. ILSP-FC (Papavassiliou et al., 2013) is a modular crawling system allowing to easily acquire domain-specific and generic corpora from the Web.5 This crawler includes a de-duplicator which checks all documents in a pairwise manner to identify near-duplicates. This is achieved by comparing the quantised word frequencies and the paragraphs of each pair of candidate duplicate documents. A document-pair detector also examines each document in the same manner and identifies pairs of documents that could be considered parallel. The main methods used by the pair detector are URL similarity, co-occurrences of images with the same filenam"
W15-3022,P02-1040,0,0.096443,"47 0.830 0.828 0.819 0.864 0.849 Combination 14.61 0.786 13.54 0.801 Table 4: Results obtained on the development and test sets for the constrained English-to-Finnish translation task. Best individual system in bold. 2010) using default settings, except for the beam size (set to 1, 500) and radius (5 for Finnish and 7 for English), following empirical results obtained on the development set. 5.1 Constrained Results Individual systems trained on the provided data are evaluated before being combined. The results obtained for the English-to-Finnish direction are presented in Table 4.13 The BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores obtained by the system trained on compoundsegmented data (HFST Comp) show a positive impact of this method on SMT according to the development set, compared to the other individual systems. The unsupervised segmentation methods do not improve over phrase-based SMT, while the hierarchical model shows an interesting reduction of the TER score compared to a classic phrasebased approach. On the test set, the use of inflectional morph segments as well as compounds (HFST Morph) leads to the best results for the individual systems on both evaluation metrics. The"
W15-3022,W15-1844,1,0.795965,"section: morfessor produces 1-best segmentation: and ‘Kun→←ta→←liito→←ksen selvitt¨a→←misess¨a’ and flatcat ‘Kun→←tali→←itoksen selvitt¨amis→←ess¨a’ 3.1 3.3 3.2 Rule-based Segmentation Rule-based morphological segmentation is based on linguistically motivated computational descriptions of the morphology by dividing the word-forms into morphs (minimal segments carrying semantic or syntactic meaning). The rule-based approach to morphological segmentation uses a morphological dictionary of words and an implementation of the morphological grammar to analyse word-forms. In our case, we use OMORFI (Pirinen, 2015), an open-source implementation of the Finnish morphology.8 OMORFI’s segmentation produces named segment boundaries: stem, inflection, derivation, compound-word and other etymological. The two variants of rule-based segmentation we use are based on selection of the boundary points: compound segmentation uses compound segments and discards the rest (referred in tables and figures to as HFST Comp), and morph segmentation uses compound and Unsupervised Segmentation Segments in the SMT Pipeline The segmented data is used exactly as the wordform-based data during training, tuning and testing of the"
W15-3022,P13-2121,0,0.0253168,"Missing"
W15-3022,W14-3319,1,0.888215,"Missing"
W15-3022,E12-1055,0,0.0141743,".2 636.6 4,831.5 Corpus Constrained System News Shuffle 2014 1,378.8 Unconstrained System FiWaC 146,557.4 System and filter out those for which we have seen already each 1-gram at least 10 times. This results in a reduction of 3.2x on the number of sentence pairs (from 7.3M to 2.3M ) and 2.6x on the number of words (from 114M to 44M ). The resulting parallel datasets (7 in total: Europarl and 3 sets for each fienwac and osubs) are used individually to train translation and reordering models before being combined by linear interpolation based on perplexity minimisation on the development set. (Sennrich, 2012) Language Models All the Language Models (LM) used in our experiments are 5-grams modified Kneser-Ney smoothed LMs trained using KenLM (Heafield et al., 2013). For the constrained setup, the Finnish and the English LMs are trained following two different approaches. The English LM is trained on the concatenation of all available corpora while the Finnish LM is obtained by linearly interpolating individually trained LMs based on each corpus. The weights given to each individual LM is calculated by minimising the perplexity obtained on the development set. For the unconstrained setup, the Finnis"
W15-3022,2006.amta-papers.25,0,0.0135668,"Combination 14.61 0.786 13.54 0.801 Table 4: Results obtained on the development and test sets for the constrained English-to-Finnish translation task. Best individual system in bold. 2010) using default settings, except for the beam size (set to 1, 500) and radius (5 for Finnish and 7 for English), following empirical results obtained on the development set. 5.1 Constrained Results Individual systems trained on the provided data are evaluated before being combined. The results obtained for the English-to-Finnish direction are presented in Table 4.13 The BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores obtained by the system trained on compoundsegmented data (HFST Comp) show a positive impact of this method on SMT according to the development set, compared to the other individual systems. The unsupervised segmentation methods do not improve over phrase-based SMT, while the hierarchical model shows an interesting reduction of the TER score compared to a classic phrasebased approach. On the test set, the use of inflectional morph segments as well as compounds (HFST Morph) leads to the best results for the individual systems on both evaluation metrics. The combination of these 7 systems"
W15-3022,2014.eamt-1.45,1,0.822092,"Missing"
W15-3022,D07-1080,0,0.0247453,"fessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn t"
W15-4944,E06-1032,0,\N,Missing
W15-4944,W10-1751,0,\N,Missing
W15-4944,W14-3301,0,\N,Missing
W15-4944,P02-1040,0,\N,Missing
W15-4944,W14-3319,1,\N,Missing
W15-4944,P11-1105,0,\N,Missing
W15-4944,P10-2041,0,\N,Missing
W15-4944,W05-0909,0,\N,Missing
W15-4944,P07-2045,0,\N,Missing
W15-4944,W07-0718,0,\N,Missing
W15-4944,C14-1111,0,\N,Missing
W15-4944,P12-3005,0,\N,Missing
W15-4944,2012.eamt-1.67,1,\N,Missing
W15-4944,2014.eamt-1.4,1,\N,Missing
W15-4944,W14-3320,0,\N,Missing
W15-4944,2005.mtsummit-papers.11,0,\N,Missing
W15-4944,ljubesic-etal-2014-tweetcat,1,\N,Missing
W15-4944,W15-3036,1,\N,Missing
W15-4944,rubino-etal-2014-quality,1,\N,Missing
W15-4944,W15-3022,1,\N,Missing
W15-4944,W15-4903,1,\N,Missing
W15-4944,2015.eamt-1.4,1,\N,Missing
W15-4944,espla-gomis-etal-2014-comparing,1,\N,Missing
W15-4944,W15-3001,0,\N,Missing
W15-4944,ljubesic-toral-2014-cawac,1,\N,Missing
W15-4944,W14-0405,1,\N,Missing
W15-4944,2005.iwslt-1.8,0,\N,Missing
W15-4944,W16-3421,1,\N,Missing
W15-4944,D07-1078,0,\N,Missing
W15-4944,W08-0509,0,\N,Missing
W15-4944,W11-2123,0,\N,Missing
W15-4944,P14-1129,0,\N,Missing
W15-4944,W16-2347,0,\N,Missing
W15-4944,W16-2375,1,\N,Missing
W15-4944,W16-2367,1,\N,Missing
W15-4944,W16-3423,1,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-3423,agic-ljubesic-2014-setimes,0,0.0575829,"Missing"
W16-3423,D11-1033,0,0.0612128,"Missing"
W16-3423,N12-1047,0,0.0331989,"d by professional and amateur translators. 2. Compare the use of three reordering models (word-, phrase-based and hierarchical). 3. Measure the impact of using additional models recently introduced in the SMT pipeline. Specifically, the operation sequence model (OSM) and bilingual neural language models (BiNLM). 4. Assess the impact of different ways to select and combine data sets. 5. Compare our best systems to widely-used commercial systems. 2 Experimental Setting 2.1 MT Systems SMT systems are trained with Moses 3.0,4 using default settings unless mentioned otherwise, and tuned with MIRA (Cherry and Foster, 2012). Language models are of order 5 with Kneser-Ney modified smoothing. The following publicly available parallel corpora are used for training: HrEnWaC 2.0,5 the DGT Translation Memory,6 the JRC Acquis,7 SET IMES(Agi´c and Ljubeˇsi´c, 2014), T ED talks,8 OpenSubtitles 2013 cleaned (Espl`a-Gomis et al., 2014) and SrEnWaC.9 The last one is a parallel corpus for Serbian–English. The Serbian side is translated to Croatian with a rule-based system in order to get more English-Croatian parallel text.10 Language models are trained on the hrWaC corpus11 for Croatian, and on all the available English dat"
W16-3423,P14-1129,0,0.187282,"Missing"
W16-3423,P11-1105,0,0.265402,"Missing"
W16-3423,D08-1089,0,0.0386896,"human evaluation. This may indicate that the impact of the development set in our setup is too small to be of importance. 3.2 Reordering Models We compare using a word-based reordering model solely (the default in the Moses MT toolkit) to adding two additional models: 1) phrase-based (with the same three orientations as the word-based model: monotone, swap and discontinuous) and 2) hierarchical (with four orientations: non merged, discontinuous, left and right). This combination has been shown to yield the best performance in terms of automatic metrics for English–Chinese and English–Arabic (Galley and Manning, 2008). Here we evaluate it for another language and not only automatically but also manually. Results are shown in Table 2. Table 2. Results using different reordering models. Best results shown in bold. English→Croatian Croatian→English BLEU TER Human Range BLEU TER Human Range Word-based 0.2363 0.6303 -0.117 1-2 0.3392 0.5211 0.168 1 Three 0.2355 0.6336 0.117 1-2 0.3404 0.5202 -0.168 2 System The results are mixed. In terms of automatic metrics, the differences are very small and not significant. According to the human evaluation, the word-based model alone leads to significantly better results t"
W16-3423,W13-2235,0,0.0453153,"Missing"
W16-3423,P10-2041,0,0.0839258,"Missing"
W16-3423,P02-1040,0,0.110767,"Missing"
W16-3423,W14-3301,0,0.209135,"Missing"
W16-3423,E12-1055,0,0.0483699,"Missing"
W16-3423,2006.amta-papers.25,0,0.0504835,"lly (except the one in Section 3.4). We used the widely used automatic metrics BLEU (Papineni et al., 2002) and 4 5 6 7 8 9 10 11 12 https://github.com/moses-smt/mosesdecoder/tree/RELEASE-3.0 http://hdl.handle.net/11356/1058 https://ec.europa.eu/jrc/en/language-technologies/dgt-translation-memory http://tinyurl.com/CroatianAcquis http://nlp.ffzg.hr/resources/corpora/ted-talks/ http://hdl.handle.net/11356/1059 https://svn.code.sf.net/p/apertium/svn/staging/apertium-hbs HR-hbs SR/ http://nlp.ffzg.hr/resources/corpora/hrwac/ http://www.statmt.org/wmt15/translation-task.html 370 Toral et al. TER (Snover et al., 2006). Statistical significance is calculated on BLEU scores with paired bootstrap resampling (1,000 iterations and p = 0.95). The human evaluation consists of ranking MT outputs with Appraise.13 For each experiment 100 randomly selected segments were ranked. All the annotations were carried out by 2 native Croatian speakers with an advanced level of English. The following guidelines were provided to the annotators: Given translations by more than two MT systems, the task is to rank them: - Rank system A higher (rank1) than B (rank2), if the output of the first is better than the output of the seco"
W16-3423,N13-1069,0,0.136687,".14 Namely, we run 1,000 iterations of rankings followed by clustering (p = 0.95). If two systems are placed in different clusters (column “range” in results’ tables) then the one with lower range is considered significantly better. 3 Experiments 3.1 Development Sets In this experiment we aim to assess the impact of using a development set obtained by professional versus amateur translators. While professional translations should lead to a higher quality data set, its cost is in our case close to an order of magnitude (both in terms of price and time) higher than crowdsourcing. In this sense, Zbib et al. (2013) built MT systems for Arabic–English using development sets that were professionally translated and crowdsourced. They compared tuning with one reference (either professional or crowdsourced) and using both together as multiple references. The latter setup led to the best results. In our experiments we aim to corroborate these results for English-to-Croatian15 and also compare the use of professional and crowdsourced translations for tuning. Results are shown in Table 1. Following Zbib et al. (2013), we would expect the combination of both professional and crowdsourced translations to perform"
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W18-6452,D16-1025,0,0.0415751,"Missing"
W18-6452,W17-4730,0,0.0210998,"es on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary size, the system applies ad hoc pre-processing for the German language by re-implementing the pipeline developed by the best system at the WMT‘17 Translation task (Huck et al., 2017). In addition to the data released for the task, training is performed by taking advantage of both the artificial data provided by (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus (Negri et al., 2018). The submitted runs, which rely on the same multi-source architecture and pre-processing step, differ in the loss function used, which is either minimum-risk training alone (“MRT”), or its linear combination with maximum likelihood estimation (“MRT+MLE”). Participants Five participating teams submitted a total of 11 runs for the PBSMT subtask and 10 runs for the NMT subtask. Partici"
W18-6452,W16-2378,0,0.519466,"previous rounds. The TER (↓) and BLEU (↑) scores reported in Table 2 are computed using the human post-edits as reference. As discussed in (Bojar et al., 2017), numeric evidence of a higher quality of the original translations can indicate a smaller room for improvement for APE systems (having, at the same time, less to learn during training and less to corParticipants were also provided with additional training material for both the subtasks. One resource (called “Artificial” in Table 1) is the corpus of 4.5 million artificially-generated postediting triplets used by the 2016 winning system (Junczys-Dowmunt and Grundkiewicz, 2016). This corpus was widely used by participants in the 2017 round of the APE task. The other resource is the English-German section of the eSCAPE corpus (Negri et al., 2018). It comprises 14.5 million instances, which were artificially generated both via phrase-based and neural translation (7.25 millions each) of the same source sentences. Table 1 provides basic statistics about the data, which was released by the European Project QT21 (Specia et al., 2017). In addition, Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the four rounds of the APE"
W18-6452,P15-2026,1,0.890076,"e technology progress over the past. On the other side, extending the evaluation to NMTderived data was meant to explore the effectiveness of APE techniques, which now migrated to the neural paradigm, to correct data obtained with the same paradigm. In terms of participants and submitted runs, 5 teams produced respectively 11 runs for the PBSMT subtask and 10 runs for the NMT subtask. Introduction The WMT shared task on MT Automatic PostEditing (APE), this year at its fourth round, aims to evaluate systems for the automatic correction of errors in a machine-translated text. As pointed out by (Chatterjee et al., 2015), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by per1 As opposed to the 2017 round, in which both EnglishGerman and German-English data were considered. 710 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 710–725 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64079 All submissions were produced by neural APE systems. All the teams experimented"
W18-6452,W17-4773,1,0.908777,"ntion layers (4 and 8 respectively). calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “do-nothing” system that leaves all the test targets unmodified. Baseline results, the same shown in Table 2, are also reported in Tables 4 and 5 for comparison with participants’ submissions.7 For each submitted run, the statistical significance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary s"
W18-6452,P18-4020,0,0.0259359,"Missing"
W18-6452,W18-1804,1,0.845642,"Missing"
W18-6452,P17-4012,0,0.0450369,"tly trained to handle PBNMT and NMT outputs. This was achieved by adding, at the beginning of every MT segment to be corrected, a specific token indicating which type of MT system was used to produce it and from which training corpus the segment pair was extracted. (i.e. the WMT training data, the artificial training data presented in (Junczys-Dowmunt and Grundkiewicz, 2016), or the eSCAPE corpus (Negri et al., 2018)). The submitted runs were obtained with two neural architectures. One (“LSTM”) is an attentional RNN with gated units based on (Bahdanau et al., 2014) and implemented in OpenNMT (Klein et al., 2017). The other is the multi-head attention-only network (Vaswani et al., 2017) implemented in Microsoft & University of Edinburgh. MS UEdin’s neural APE system is based on the dual-source Transformer models available in Marian (Junczys-Dowmunt et al., 2018). The models are trained with tied embeddings across all embeddings matrices and shared parameters for all the encoders. The dual-source Transformer model is implemented by stacking an additional target-source multi-head component on the previ7 In addition to the do-nothing baseline, in previous rounds we also compared systems’ performance with"
W18-6452,W04-3250,0,0.0838393,"(i.e. “Transf.base” and “Transf.large”) were trained with different configurations in terms of parallel attention layers (4 and 8 respectively). calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “do-nothing” system that leaves all the test targets unmodified. Baseline results, the same shown in Table 2, are also reported in Tables 4 and 5 for comparison with participants’ submissions.7 For each submitted run, the statistical significance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias a"
W18-6452,W13-2305,0,0.0160731,"score. Average standardized scores for individual segments belonging to a given system are then computed, before the final overall DA score for that system is computed as the average of its segment scores. Figure 5: Screenshot of the direct assessment user interface. the PBSMT subtask. 6 Human evaluation In order to complement the automatic evaluation of APE submissions, a manual evaluation of the primary systems submitted (five in total) was conducted. Similarly to the manual evaluation conducted for last year APE shared task, it was carried out following the direct assessment (DA) approach (Graham et al., 2013; Graham et al., 2016). In this Section, we present the evaluation procedure as well as the results obtained. 6.1 Evaluation procedure The manual evaluation carried out this year involved 12 native German speakers with full professional proficiency in English in the IT domain, with a third of the evaluators being students in translation technologies from Saarland University and the remaining ones researchers and engineers from DFKI. Each evaluator was introduced to the evaluation task through a set of slides and a testing phase of the evaluation platform in order to be familiar with the user i"
W18-6452,L18-1004,1,0.931029,"uality of the original translations can indicate a smaller room for improvement for APE systems (having, at the same time, less to learn during training and less to corParticipants were also provided with additional training material for both the subtasks. One resource (called “Artificial” in Table 1) is the corpus of 4.5 million artificially-generated postediting triplets used by the 2016 winning system (Junczys-Dowmunt and Grundkiewicz, 2016). This corpus was widely used by participants in the 2017 round of the APE task. The other resource is the English-German section of the eSCAPE corpus (Negri et al., 2018). It comprises 14.5 million instances, which were artificially generated both via phrase-based and neural translation (7.25 millions each) of the same source sentences. Table 1 provides basic statistics about the data, which was released by the European Project QT21 (Specia et al., 2017). In addition, Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the four rounds of the APE task, we report the repetition rate of SRC, TGT and PE elements, as well as the TER (Snover et al., 2006) and the BLEU score (Papineni et al., 2002) of the TGT elements ("
W18-6452,W18-6468,0,0.100227,"Missing"
W18-6452,W13-0805,0,0.0175914,"t set consists of 2,000 newly-released instances. For the NMT subtask, the training and development set respectively consist of 13,442 and 1,000 triplets, while the test set comprises 1,023 instances. Task description Similar to previous years, participants were provided with training and development data consisting of (source, target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data 3 We used a phrase-based MT system trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 4 The NMT system was trained with generic and indomain parallel training data using the attentional encoderdecoder architecture (Bahdanau et al., 2014) implemented in the Nematus toolkit (Sennrich et al., 2017). We used bytepair encoding (Sennrich et al., 2016) for vocabulary reduction, mini-batches of 100, word embeddings of 500 dimensions, and gated recurrent unit layers of 1,024 units. Optimization was done using Adam and by re-shuffling the training set at each epoch. For this year’s round, the APE task focused on one lang"
W18-6452,P02-1040,0,0.102334,"Missing"
W18-6452,W17-4775,0,0.0193748,"ficance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary size, the system applies ad hoc pre-processing for the German language by re-implementing the pipeline developed by the best system at the WMT‘17 Translation task (Huck et al., 2017). In addition to the data released for the task, training is performed by taking advantage of both the artificial data provided by (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus ("
W18-6452,W18-6469,1,0.876946,"Missing"
W18-6452,P16-1162,0,0.0670476,"target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data 3 We used a phrase-based MT system trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 4 The NMT system was trained with generic and indomain parallel training data using the attentional encoderdecoder architecture (Bahdanau et al., 2014) implemented in the Nematus toolkit (Sennrich et al., 2017). We used bytepair encoding (Sennrich et al., 2016) for vocabulary reduction, mini-batches of 100, word embeddings of 500 dimensions, and gated recurrent unit layers of 1,024 units. Optimization was done using Adam and by re-shuffling the training set at each epoch. For this year’s round, the APE task focused on one language pair, English-German, and on data coming from the Information Technology (IT) domain. As emerged from the previous evaluations, the selected target domain is specific and repetitive enough to allow supervised systems to 2 In addition to these small in-domain training sets, which were released by the organizers over the yea"
W18-6452,E17-3017,0,0.0615879,"Missing"
W18-6452,P16-1159,0,0.0236516,"lated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary size, the system applies ad hoc pre-processing for the German language by re-implementing the pipeline developed by the best system at the WMT‘17 Translation task (Huck et al., 2017). In addition to the data released for the task, training is performed by taking advantage of both the artificial data provided by (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus (Negri et al., 2018). The submitted runs, which rely on the same multi-source a"
W18-6452,W18-6470,0,0.167966,"Missing"
W18-6452,N07-1064,0,0.0365222,"2017) implemented in Microsoft & University of Edinburgh. MS UEdin’s neural APE system is based on the dual-source Transformer models available in Marian (Junczys-Dowmunt et al., 2018). The models are trained with tied embeddings across all embeddings matrices and shared parameters for all the encoders. The dual-source Transformer model is implemented by stacking an additional target-source multi-head component on the previ7 In addition to the do-nothing baseline, in previous rounds we also compared systems’ performance with a reimplementation of the phrase-based approach firstly proposed by Simard et al. (2007), which represented the common backbone of APE systems before the spread of neural solutions. As shown in (Bojar et al., 2016; Bojar et al., 2017), the steady progress of neural APE technology has made the phrase-based solution not competitive with current methods reducing the importance of having it as an additional term of comparison. 714 ous multi-head component, one for each encoder. Each multi-head attention block is followed by a skip connection from the previous input and layer normalization. Each encoder corresponds exactly to the implementation from (Vaswani et al., 2017), but with co"
W18-6452,2006.amta-papers.25,0,0.232446,"Missing"
W18-6452,W18-6471,1,0.873456,"Missing"
W18-6469,P18-4020,0,0.0452739,"Missing"
W18-6469,D11-1033,0,0.0385676,"ed text being the source sequences and the corresponding post-edited text the target sequences, without making use of the source language (English). We did not split the machine translated data whether it was produced by a phrase-based (PBMT) or a neural (NMT) system. Instead, we added a specific token at the beginning of every source (machine translated) segment indicating which type of translation system was used to produce it. The two additional parallel resources (artificial training data and eSCAPE corpora) were filtered using the bilingual cross-entropy difference approach presented in (Axelrod et al., 2011). We used the APE training data as in-domain corpus and each additional parallel corpus individually as out-of-domain corpus. The top n sentence pairs ranked by their bilingual cross-entropy scores were kept, with n being set by calculating the perplexity obtained on the development set. The resulting corpora used contain approx. 100k, 4 Evaluation The three APE models trained for the shared task were used to post-edit the test set released by the organizers. Automatic evaluation with BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) was conducted by the organizers and the obtained sc"
W18-6469,P17-4012,0,0.0377497,"Results show better performances reached by the attention-only model over the recurrent one, significant improvement over the baseline when post-editing phrase-based MT output but degradation when applied to neural MT output. 1 2 APE Architectures The two neural network architectures used in our experiments were an attentional recurrent neural network with gated units and a multi-head attention-only network. 2.1 Introduction Recurrent Neural Network For the Recurrent Neural Network (RNN) approach, we followed the architecture presented in (Bahdanau et al., 2014) and implemented in O PEN NMT (Klein et al., 2017)1 . Both the encoder and the decoder were 2-layered monodirectional RNNs with LSTM cells. The decoder applies global attention over the source sentence and performs input feeding. The source and target word embeddings, as well as the hidden layers, had 500 dimensions. The dropout probability was set to 0.3. The source and target vocabulary size is limited to 50000 tokens. Standard stochastic gradient descent is used as optimizer with a maximum batch size of 64. These hyper-parameters are the default ones in O PEN NMT and were not tuned during the experiments presented in this paper. For the 20"
W18-6469,P07-2045,0,0.0102426,"ennrich et al., 2016a; Johnson et al., 2017) allowed for identification of the segment pairs provenance. In order to balance the amount of data coming from different sources, we oversampled the official training data to reach approximately the amount taken from the two additional resources. Similarly, we increased the amount of data produced by a NMT system to balance with the amount produced by a PBMT system. This method was inspired by the work presented in (Chu et al., 2017). The corpora which were not already tokenized were processed with the tokenizer distributed with the M OSES toolkit (Koehn et al., 2007). Additionally, all corpora were true-cased using a pretrained true-casing model provided by the WMT organizers2 . Finally, a byte-pair encoding (Sennrich et al., 2016b) model was trained on the German training data available for the WMT translation task and applied to both source and target sides of all corpora used in our experiments. For the attention only approach, we used the architecture described in (Vaswani et al., 2017) and implemented in M ARIAN (Junczys-Dowmunt et al., 2018). Two models were trained following this approach with variations in the number of heads (parallel attention l"
W18-6469,L18-1004,0,0.0635855,"itional encodings, the dropout rate was set to 0.1 and the batch size to 32. These hyper-parameters were selected in order to compare the impact of increasing the dimensionality of the encoder and decoder layers, as well as the number of heads, on the post-editing performances. 3 Data Preparation The training corpora provided for the APE shared task since 2016 were used (Bojar et al., 2016, 2017), as well as the two additional resources made available by the shared task organizers, namely the artificial training data presented in (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus (Negri et al., 2018). The target language data (German) was used for both input and output sequences in our APE models, the machine translated text being the source sequences and the corresponding post-edited text the target sequences, without making use of the source language (English). We did not split the machine translated data whether it was produced by a phrase-based (PBMT) or a neural (NMT) system. Instead, we added a specific token at the beginning of every source (machine translated) segment indicating which type of translation system was used to produce it. The two additional parallel resources (artific"
W18-6469,P02-1040,0,0.100594,"were filtered using the bilingual cross-entropy difference approach presented in (Axelrod et al., 2011). We used the APE training data as in-domain corpus and each additional parallel corpus individually as out-of-domain corpus. The top n sentence pairs ranked by their bilingual cross-entropy scores were kept, with n being set by calculating the perplexity obtained on the development set. The resulting corpora used contain approx. 100k, 4 Evaluation The three APE models trained for the shared task were used to post-edit the test set released by the organizers. Automatic evaluation with BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) was conducted by the organizers and the obtained scores on the official test set are reported in Table 1. The automatic metrics results are obtained by comparing each system output to the manually post-edited MT output (TERpe and BLEUpe ), to an independent translation (TERref and BLEUref ) and finally using both post-edited MT output and independent translation simultaneously as a multireference evaluation approach (TERpe+ref and BLEUpe+ref ). The results obtained by the nonpost-edited MT output is presented as a baseline. 2 http://data.statmt.org/wmt18/ transla"
W18-6469,N16-1005,0,0.118998,"18 edition of the WMT automatic postediting (APE) task, two novelties were added compared to the previous editions: post-editing of neural machine translation (NMT) output in addition to phrase-based (PBMT) output, and the availability of larger training sets. The DFKI-MLT systems developed for this shared task aimed at handling outputs from PBMT and NMT jointly with a single APE model. This was achieved by using artificial tokens indicating which type of MT system was used to produce the source segment and from which corpus the segment pair was extracted (inspired by (Yamagishi et al., 2016; Sennrich et al., 2016a; Johnson et al., 2017)). Two NMT architectures were used to train our APE models, one using gated recurrent layers with 1 We used the Torch version of O PEN NMT available at https://github.com/OpenNMT/OpenNMT 836 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 836–839 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64096 2.2 Attention Only 300k and 360k segment pairs taken from the eSCAPE PBMT corpus, the eSCAPE NMT corpus and the artificial training data"
W18-6469,P16-1162,0,0.454778,"18 edition of the WMT automatic postediting (APE) task, two novelties were added compared to the previous editions: post-editing of neural machine translation (NMT) output in addition to phrase-based (PBMT) output, and the availability of larger training sets. The DFKI-MLT systems developed for this shared task aimed at handling outputs from PBMT and NMT jointly with a single APE model. This was achieved by using artificial tokens indicating which type of MT system was used to produce the source segment and from which corpus the segment pair was extracted (inspired by (Yamagishi et al., 2016; Sennrich et al., 2016a; Johnson et al., 2017)). Two NMT architectures were used to train our APE models, one using gated recurrent layers with 1 We used the Torch version of O PEN NMT available at https://github.com/OpenNMT/OpenNMT 836 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 836–839 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64096 2.2 Attention Only 300k and 360k segment pairs taken from the eSCAPE PBMT corpus, the eSCAPE NMT corpus and the artificial training data"
W18-6469,P17-2061,0,0.0209174,"est sets as well. All datasets were used together to train our APE models, the artificial tokens inspired by (Yamagishi et al., 2016; Sennrich et al., 2016a; Johnson et al., 2017) allowed for identification of the segment pairs provenance. In order to balance the amount of data coming from different sources, we oversampled the official training data to reach approximately the amount taken from the two additional resources. Similarly, we increased the amount of data produced by a NMT system to balance with the amount produced by a PBMT system. This method was inspired by the work presented in (Chu et al., 2017). The corpora which were not already tokenized were processed with the tokenizer distributed with the M OSES toolkit (Koehn et al., 2007). Additionally, all corpora were true-cased using a pretrained true-casing model provided by the WMT organizers2 . Finally, a byte-pair encoding (Sennrich et al., 2016b) model was trained on the German training data available for the WMT translation task and applied to both source and target sides of all corpora used in our experiments. For the attention only approach, we used the architecture described in (Vaswani et al., 2017) and implemented in M ARIAN (Ju"
W18-6469,2006.amta-papers.25,0,0.0941682,"al cross-entropy difference approach presented in (Axelrod et al., 2011). We used the APE training data as in-domain corpus and each additional parallel corpus individually as out-of-domain corpus. The top n sentence pairs ranked by their bilingual cross-entropy scores were kept, with n being set by calculating the perplexity obtained on the development set. The resulting corpora used contain approx. 100k, 4 Evaluation The three APE models trained for the shared task were used to post-edit the test set released by the organizers. Automatic evaluation with BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) was conducted by the organizers and the obtained scores on the official test set are reported in Table 1. The automatic metrics results are obtained by comparing each system output to the manually post-edited MT output (TERpe and BLEUpe ), to an independent translation (TERref and BLEUref ) and finally using both post-edited MT output and independent translation simultaneously as a multireference evaluation approach (TERpe+ref and BLEUpe+ref ). The results obtained by the nonpost-edited MT output is presented as a baseline. 2 http://data.statmt.org/wmt18/ translation-task/preprocessed/de-en/"
W18-6469,W16-2378,0,0.0654214,", 512 dimensions were used for the embedding layers and the positional encodings, the dropout rate was set to 0.1 and the batch size to 32. These hyper-parameters were selected in order to compare the impact of increasing the dimensionality of the encoder and decoder layers, as well as the number of heads, on the post-editing performances. 3 Data Preparation The training corpora provided for the APE shared task since 2016 were used (Bojar et al., 2016, 2017), as well as the two additional resources made available by the shared task organizers, namely the artificial training data presented in (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus (Negri et al., 2018). The target language data (German) was used for both input and output sequences in our APE models, the machine translated text being the source sequences and the corresponding post-edited text the target sequences, without making use of the source language (English). We did not split the machine translated data whether it was produced by a phrase-based (PBMT) or a neural (NMT) system. Instead, we added a specific token at the beginning of every source (machine translated) segment indicating which type of translation system was used to produce it. The"
W18-6469,W16-4620,0,0.115161,"n this paper. For the 2018 edition of the WMT automatic postediting (APE) task, two novelties were added compared to the previous editions: post-editing of neural machine translation (NMT) output in addition to phrase-based (PBMT) output, and the availability of larger training sets. The DFKI-MLT systems developed for this shared task aimed at handling outputs from PBMT and NMT jointly with a single APE model. This was achieved by using artificial tokens indicating which type of MT system was used to produce the source segment and from which corpus the segment pair was extracted (inspired by (Yamagishi et al., 2016; Sennrich et al., 2016a; Johnson et al., 2017)). Two NMT architectures were used to train our APE models, one using gated recurrent layers with 1 We used the Torch version of O PEN NMT available at https://github.com/OpenNMT/OpenNMT 836 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 836–839 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64096 2.2 Attention Only 300k and 360k segment pairs taken from the eSCAPE PBMT corpus, the eSCAPE NMT corpus and the ar"
