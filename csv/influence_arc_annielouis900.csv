2020.emnlp-main.601,D18-1241,0,0.0283105,"s. 1 Table 1: A polar question with 10 indirect responses, taken from our corpus. paper, we present the first large scale corpus and models for interpreting such indirect answers. Previous attempts to interpret indirect yes/no answers have been small scale and without datadriven techniques (Green and Carberry, 1999; de Marneffe et al., 2010). However, recent success on many language understanding problems (Wang et al., 2019), the impressive generation capabilities of modern dialog systems (Zhang et al., 2019; Adiwardana et al., 2020), as well as the huge interest in yes/no question-answering (Choi et al., 2018; Clark et al., 2019) have created a conducive environment for revisiting this hard task. Introduction Humans produce and interpret complex utterances even in simple scenarios. For example, for the polar (yes/no) question ‘Want to get dinner?’, there are many perfectly natural responses in addition to ‘yes’ and ‘no’, as in Table 1. How should a dialog system interpret these INDIRECT answers? Many can be understood based on the answer text alone, e.g. ‘I would like that’ or ‘I’d rather just go to bed’. For others, the question is crucial, e.g. ‘Dinner would be lovely.’ is a positive reply here,"
2020.emnlp-main.601,N19-1300,0,0.0579156,"ar question with 10 indirect responses, taken from our corpus. paper, we present the first large scale corpus and models for interpreting such indirect answers. Previous attempts to interpret indirect yes/no answers have been small scale and without datadriven techniques (Green and Carberry, 1999; de Marneffe et al., 2010). However, recent success on many language understanding problems (Wang et al., 2019), the impressive generation capabilities of modern dialog systems (Zhang et al., 2019; Adiwardana et al., 2020), as well as the huge interest in yes/no question-answering (Choi et al., 2018; Clark et al., 2019) have created a conducive environment for revisiting this hard task. Introduction Humans produce and interpret complex utterances even in simple scenarios. For example, for the polar (yes/no) question ‘Want to get dinner?’, there are many perfectly natural responses in addition to ‘yes’ and ‘no’, as in Table 1. How should a dialog system interpret these INDIRECT answers? Many can be understood based on the answer text alone, e.g. ‘I would like that’ or ‘I’d rather just go to bed’. For others, the question is crucial, e.g. ‘Dinner would be lovely.’ is a positive reply here, but a negative answe"
2020.emnlp-main.601,W11-0609,0,0.0931706,"Missing"
2020.emnlp-main.601,N19-1423,0,0.0876,"Missing"
2020.emnlp-main.601,J99-3004,0,0.827456,"LQ-YN finetunes a BOOLQ model checkpoint (see Section 5.1) on our corpus, with a new output layer. Since BOOLQ is a Yes/No question answering system, even if developed for a different domain, we expect to learn many semantics of yes/no answers from this data. BERT-MNLI-YN is first fine-tuned on the MNLI corpus, followed by our YN data. This configuration tests if the signals we hoped to capture with the out-of-the-box MNLI model (Section 5.1) can be strengthened by training on our target task. BERT-DIS-YN. As discussed, indirect answers also have discourse relations with the speaker’s intent (Green and Carberry, 1999). We implement this idea via a discourse connective prediction task. Consider again the texts from Section 5.1: The likely connective between Q0 and A is ‘because’ as in ‘I like Italian food [because] I love Tuscan food.’. For Q0 and B, ‘but’ would be more reasonable: ‘I like Italian food [but] I prefer Mexican cuisine.’. We hypothesize that these discourse relations will help learn the yes/no meaning via transfer learning. We use 400K examples (matching the MNLI data size) of explicit connectives and their arguments (a subset of Nie et al. (2019)). We aim to predict the 5 connectives (because"
2020.emnlp-main.601,N18-2017,0,0.0450893,"Missing"
2020.emnlp-main.601,W09-3920,0,0.464633,"Missing"
2020.emnlp-main.601,P10-1018,0,0.0738745,"Missing"
2020.emnlp-main.601,P19-1442,0,0.0381978,"Missing"
2020.emnlp-main.601,S18-2023,0,0.0498751,"Missing"
2020.emnlp-main.601,Q19-1016,0,0.0487234,"Missing"
2020.emnlp-main.601,N18-1101,0,0.133957,"Missing"
2020.emnlp-main.646,N19-1423,0,0.0167135,"k Times (NYT) Annotated Corpus (Sandhaus, 2008) to extract tuples of named entities and their document context. The NYT corpus contains high-quality metadata listing the salient named entities mentioned in each article. We form our tuples from entities tagged in the metadata for the same article. We refer to a tuple of entities and its associated information as an aggregatable instance. We Related work Abstractive summarizers have gained prominence with the popularization of RNNs (Sutskever et al., 2014; Nallapati et al., 2016), and more recently Transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019). Several abstractive models have achieved state-of-the-art performances on benchmark summarization datasets in terms of ROUGE, 3 8032 The TESA dataset Data collected aggregatable instances 2100 annotators 63 annotations 6299 Preprocessed dataset aggregatable instances 1718 42 annotators annotations 4675 P ERSON entities tuples 941 (801) L OCATION entities tuples 629 (412) O RGANIZATION entities tuples 148 (123) P ERSON aggregations 2900 (951) L OCATION aggregations 2041 (505) O RGANIZATION aggregations 456 (239) first describe the components of an aggregatable instance in more detail. Then, w"
2020.emnlp-main.646,C10-1039,0,0.245029,"ess only a proxy for abstraction in the broader sense which concerns semantic generalization. We argue that it is important to also focus explicitly on semantic abstraction, as this capability is required for more difficult types of summarization which are out of reach of current methods. For example, generating a plot summary of a novel might require describing sequences of events using one sentence. Writing a survey of a scientific field would require categorizing papers and ideas, and being able to refer to them as a whole. Outside of domain-specific settings such as opinion summarization (Ganesan et al., 2010; Gerani et al., 2014, inter alia), and tasks such as sentence fusion (Barzilay and McKeown, 2005), there has been little work focusing on semantic generalization and abstraction. 8031 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8031–8050, c November 16–20, 2020. 2020 Association for Computational Linguistics In this paper, we start to tackle this issue by focusing on the specific task of semantic aggregation of entities; i.e., how to refer to a tuple of named entities using a noun phrase instead of enumerating them (See Table 1 for an example)"
2020.emnlp-main.646,D18-1206,0,0.0645614,"Missing"
2020.emnlp-main.646,N19-4009,0,0.0127795,"ntage of this approach over the previous one is that it leverages the set-up of TESA as a ranking task, and the model is exposed to both correct and incorrect aggregations during training (which, on the other hand, makes it more computationally expensive). By contrast, generative BART only sees correct ones. We thus expect the discriminative model to produce higher performance. However, this comes at a cost, as this approach cannot generate freely an aggregation, but only retrieve one from a set of candidates. For all three versions above, we built upon code that is available through fairseq (Ott et al., 2019). We use the version of BART pre-trained on the CNN/DailyMail dataset. The choice of hyperparameters is described in Appendix D. 6 Results The results of the models on TESA’s test set are presented in Table 4. We see that most models outMethod Random baseline Frequency baseline Logistic regression Pre-trained BART Generative BART Discriminative BART MAP 0.222 0.570 0.700 0.389 0.701 0.895 R@10 0.442 0.655 0.863 0.682 0.903 0.991 MRR 0.289 0.761 0.840 0.505 0.840 0.954 Table 4: Results of the different models on the TESA test set. Method Generative BART (0.701) Discriminative BART (0.895) conte"
2020.emnlp-main.646,P16-1216,0,0.0509863,"Missing"
2020.emnlp-main.646,W18-5446,0,0.0803362,"Missing"
2020.emnlp-main.646,2020.findings-emnlp.217,0,0.0296911,"We present two ways of fine-tuning BART to TESA, either in a discriminative or in a generative fashion, and compare them against simpler statistical and frequency-based methods. The simple classifier achieves decent results on TESA. It is however outperformed by a wide margin by BART, when fine-tuned on our task in a discriminative manner. When fine-tuned as a generative model, BART yields similar performance as the simple classifier. Yet, the generative model is able to freely generate entity aggregations with diversity and quality, despite some factual inconsistencies. including ProphetNet (Yan et al., 2020), PEGASUS (Zhang et al., 2019) and BART (Lewis et al., 2019). Recent work has also focused on specific issues such as preventing inappropriate repetition (Kry´sci´nski et al., 2018), word-level rewriting, and evaluating factual consistency (Kry´sci´nski et al., 2019; Maynez et al., 2020). Abstraction is critical for certain domains and applications, but has not been thoroughly explored in many. For example, in scientific article summarization the particular structure and length of scientific articles make extractive techniques much easier to apply (Agarwal et al., 2011), therefore abstractive"
C12-2075,P11-1036,0,0.0725235,"siness-related tweets. The company-related keyword is underlined. 3 Related work Most methods for summarizing tweets have either focused on tweets matching a generic search query (O’Connor et al., 2010), or on tweets related to sports and celebrity events (Sharifi et al., 2010; Chakrabarti and Punera, 2011; Liu et al., 2011; Inouye and Kalita, 2011; Nichols et al., 2012). We focus on summarizing the tweets that show up in the search for a company. In fact, our work is more related to aspect-based summarization methods commonly employed on product reviews (Hu and Liu, 2004; Gamon et al., 2005; Sauper et al., 2011; Zhai et al., 2011). These methods first obtain a set of attributes for the product. For example, for a camera, the attributes may be “lens”, “focus” and “zoom”. Then positive and negative sentences in the reviews are divided according to these attributes and their aggregate are shown for each category. Some approaches obtain the attributes through manual annotations and domain resources (Gamon et al., 2005; Zhuang et al., 2006). Others learn the attributes automatically using the review text (Hu and Liu, 2004; Titov and McDonald, 2008; Sauper et al., 2011; Zhai et al., 2011). Frequently occu"
C12-2075,N10-1100,0,0.145711,"and Exxon/Mobil. TO MEEEE! 5. Postings from other applications such as 4Square 6. Advertisements, job postings I’m at JPMorgan Chase in Lake Mary, FL http:// AZ Jobs |North Phoenix- Part time Teller - 67th Ave 7. Mentions but not really about the company My little bro just asked me was uncle Sam the owner of Sams club... Table 2: Types of business-related tweets. The company-related keyword is underlined. 3 Related work Most methods for summarizing tweets have either focused on tweets matching a generic search query (O’Connor et al., 2010), or on tweets related to sports and celebrity events (Sharifi et al., 2010; Chakrabarti and Punera, 2011; Liu et al., 2011; Inouye and Kalita, 2011; Nichols et al., 2012). We focus on summarizing the tweets that show up in the search for a company. In fact, our work is more related to aspect-based summarization methods commonly employed on product reviews (Hu and Liu, 2004; Gamon et al., 2005; Sauper et al., 2011; Zhai et al., 2011). These methods first obtain a set of attributes for the product. For example, for a camera, the attributes may be “lens”, “focus” and “zoom”. Then positive and negative sentences in the reviews are divided according to these attributes a"
C12-2075,P08-1036,0,0.0333155,"oyed on product reviews (Hu and Liu, 2004; Gamon et al., 2005; Sauper et al., 2011; Zhai et al., 2011). These methods first obtain a set of attributes for the product. For example, for a camera, the attributes may be “lens”, “focus” and “zoom”. Then positive and negative sentences in the reviews are divided according to these attributes and their aggregate are shown for each category. Some approaches obtain the attributes through manual annotations and domain resources (Gamon et al., 2005; Zhuang et al., 2006). Others learn the attributes automatically using the review text (Hu and Liu, 2004; Titov and McDonald, 2008; Sauper et al., 2011; Zhai et al., 2011). Frequently occurring phrases in the reviews and also which often tend to be associated with sentiment as chosen as attributes. But while product review archives have significant overlap in topics, the informal nature of twitter conversation creates diverse tweets and also mixes review and non-review content. Identifying frequent attributes from tweet streams becomes difficult and unreliable. So we use an external resource, news articles, to learn concepts for the business domain and use these concepts to guide clustering of tweets. Our procedure for l"
C12-2075,H05-1044,0,0.0225799,"Missing"
C12-2075,W11-0709,0,\N,Missing
D09-1032,C08-1019,0,0.0843761,"Missing"
D09-1032,P06-2020,0,0.231088,"Missing"
D09-1032,W00-0408,0,0.0887345,"nk documents according to their relevance to a given query. The summaries for each document were also ranked for relevance with respect to the same query. For good summarization systems, the relevance ranking of summaries is expected to be similar to that of the full documents. Based on this intuition, the correlation between relevance rankings of summaries and original documents was used to compare the different systems. The approach was motivated by the assumption that the distribution of terms in a good summary is similar to the distribution of terms in the original document. Even earlier, Donaway et al. (2000) suggested that there are considerable benefits to be had in adopting model-free methods of evaluation involving direct comparisons between the original document and its summary. The motivation for their work was the considerable variation in content selection choices in model summaries (Rath et al., 1961). The identity of the model writer significantly affects summary evaluations (also noted by McKeown et al. (2001), Jing et al. (1998)) and evaluations of the same systems can be rather different when different models are used. In their experiments, Donaway et al. (2000) demonstrated that the"
D09-1032,C00-1072,0,0.15036,"rs contain only topic signatures from the input and all words of the summary based on two different human models is about the same as the difference between system ranking based on one model summary and the ranking produced using input-summary similarity. Inputs and summaries were compared using only one metric: cosine similarity. Kullback Leibler (KL) divergence: The KL divergence between two probability distributions P and Q is given by D(P ||Q) = X pP (w) log2 w pP (w) pQ (w) Topic signatures are words highly descriptive of the input, as determined by the application of loglikelihood test (Lin and Hovy, 2000). Using only topic signatures from the input to represent text is expected to be more accurate because the reduced vector has fewer dimensions compared with using all the words from the input. (1) 4.2 Summary likelihood The likelihood of a word appearing in the summary is approximated as being equal to its probability in the input. We compute both a summary’s unigram probability as well as its probability under a multinomial model. Unigram summary probability: It is defined as the average number of bits wasted by coding samples belonging to P using another distribution Q, an approximate of P ."
D09-1032,N03-1020,0,0.24138,"an be used during system development when human model summaries are not available. Our results provide validation of several features that can be optimized in the development of new summarization systems when the objective is to improve content selection on average, over a collection of test inputs. However, none of the features is consistently predictive of good summary content for individual inputs. 1 Introduction The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (Lin, 2004; Lin and Hovy, 2003). ROUGE compares system summaries against one or more model summaries by computing n-gram word overlaps between the two. The wide adoption of such automatic measures is understandable because they are convenient and greatly reduce the complexity of evaluations. ROUGE scores also correlate well with manual evaluations of content based on comparison with a single model summary, as used in the early editions of the Document Understanding Conferences (Over et al., 2007). In our work, we take the idea of automatic evaluation to an extreme and explore the feasibility of developing a fully automatic"
D09-1032,N06-1059,0,0.1432,"and summary words were stopword filtered and stemmed before computing the features. 4.1 Distributional Similarity Measures of similarity between two probability distributions are a natural choice for the task at hand. One would expect good summaries to be characterized by low divergence between probability distributions of words in the input and summary, and by high similarity with the input. We experimented with three common measures: KL and Jensen Shannon divergence and cosine similarity. These three metrics have already been applied for summary evaluation, albeit in different contexts. In Lin et al. (2006), KL and JS divergences between human and machine summary distributions were used to evaluate content selection. The study found that JS divergence always outperformed KL divergence. Moreover, the performance of JS divergence was better than standard ROUGE scores for multi-document summarization when multiple human models were used for the comparison. The use of cosine similarity in Donaway et al. (2000) is more directly related to our work. They show that the difference between evaluations 3 The scores were computed after stemming but stop words were retained in the summaries. 308 2. Vectors"
D09-1032,W04-1013,0,0.348336,"measures can be used during system development when human model summaries are not available. Our results provide validation of several features that can be optimized in the development of new summarization systems when the objective is to improve content selection on average, over a collection of test inputs. However, none of the features is consistently predictive of good summary content for individual inputs. 1 Introduction The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (Lin, 2004; Lin and Hovy, 2003). ROUGE compares system summaries against one or more model summaries by computing n-gram word overlaps between the two. The wide adoption of such automatic measures is understandable because they are convenient and greatly reduce the complexity of evaluations. ROUGE scores also correlate well with manual evaluations of content based on comparison with a single model summary, as used in the early editions of the Document Understanding Conferences (Over et al., 2007). In our work, we take the idea of automatic evaluation to an extreme and explore the feasibility of developi"
D09-1032,N04-1019,1,0.864716,"herence. We do not make use of any of the linguistic quality evaluations. Our work focuses on fully automatic evaluation of content selection, so manual pyramid and responsiveness scores are used for comparison with our automatic method. The pyramid metric measures content selection exclusively, while responsiveness incorporates at least some aspects of linguistic quality. Table 1: Spearman correlation between manual scores and ROUGE-1 and ROUGE-2 recall. All correlations are highly significant with p-value &lt; 0.00001. produced by the systems. Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al., 2007). It uses multiple human models from which annotators identify semantically defined Summary Content Units (SCU). Each SCU is assigned a weight equal to the number of human model summaries that express that SCU. An ideal maximally informative summary would express a subset of the most highly weighted SCUs, with multiple maximally informative summaries being possible. The pyramid score for a system summary is equal to"
D09-1032,P03-1048,0,0.0510705,"Missing"
D09-1032,H05-1014,0,0.00941913,"he 2008 Text Analysis Conference (TAC)2 . Query-focused summaries were produced from input documents in response to a stated user information need. The update summaries require more sophistication: two sets of articles on the same topic are provided. The first set of articles represents the background of a story and users are assumed to be already familiar with the information contained in them. The update task is to produce a multi-document summary from the second set of articles that can serve as an update to the user. This task is reminiscent of the novelty detection task explored at TREC (Soboroff and Harman, 2005). 3.2 Data The test set for the TAC 2008 summarization task contains 48 inputs. Each input consists of two sets of 10 documents each, called docsets A and B. Both A and B are on the same general topic but B contains documents published later than those in A. In addition, the user’s information need associated with each input is given by a query statement consisting of a title and narrative. An example query statement is shown below. Title: Airbus A380 Narrative: Describe developments in the production and launch of the Airbus A380. A system must produce two summaries: (1) a query-focused summa"
D12-1106,J08-1001,0,0.712522,"Missing"
D12-1106,N04-1015,0,0.327173,"ference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intenti"
D12-1106,W08-2102,0,0.011,"ause in p2 . Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Productions: In this representation we view each sentence as the set of grammatica"
D12-1106,P05-1022,0,0.0181966,"organization. The next sentence has a quote from that person, where the quotation forms the topicalized clause in p2 . Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are p"
D12-1106,D10-1003,0,0.0137417,"e for syntactic coherence We first present a pilot study that confirms that adjacent sentences in discourse exhibit stable patterns of syntactic co-occurrence. This study validates our second assumption relating the syntax of adjacent sentences. Later in Section 6, we examine syntactic patterns in individual sentences (assumption 1) using a corpus of academic articles where sentences were manually annotated with communicative goals. Prior work has reported that certain grammatical productions are repeated in adjacent sentences more often than would be expected by chance (Reitter et al., 2006; Cheung and Penn, 2010). We analyze all co-occurrence patterns rather than just repetitions. We use the gold standard parse trees from the Penn Treebank (Marcus et al., 1994). Our unit of analysis is a pair of adjacent sentences (S1 , S2 ) and we choose to use Section 0 of the corpus which has 99 documents and 1727 sentence pairs. We enumerate all productions that appear in the syntactic parse of any sentence and exclude those that appear less than 25 times, resulting in a list of 197 unique productions. Then all ordered pairs2 (p1 , p2 ) of productions are formed. For each pair, we compute 2 (p1 , p2 ) and (p2 , p1"
D12-1106,R11-1059,0,0.038123,"Missing"
D12-1106,J05-1003,0,0.0193848,"nce has a quote from that person, where the quotation forms the topicalized clause in p2 . Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Prod"
D12-1106,councill-etal-2008-parscit,0,0.0187609,"rvation, Experiment, Motivation, Model, Hypothesis. For our study, we use the annotation of the introduction and the abstract sections. We divide the data into training, development and test sets. For abstracts, we have 75, 50 and 100 for these sets respectively. For introductions, this split is 75, 31, 82.6 ACL Anthology Network (AAN) Corpus: Radev et al. (2009) provides the full text of publications from ACL venues. These articles do not have any zone annotations. The AAN corpus is produced from OCR analysis and no section marking is available. To recreate these, we use the Parscit tagger7 (Councill et al., 2008). We use articles from years 1999 to 2011. For training, we randomly choose 70 articles from ACL and NAACL main conferences. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6 Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7 http://aye.comp.nus.edu.sg/parsCit/ 1164 could be successfully identified using"
D12-1106,P08-2011,0,0.653865,"Missing"
D12-1106,P11-2022,0,0.567307,"Missing"
D12-1106,N07-1055,0,0.866753,"here wi and wj are syntactic items and c(wi , wj ) is the number of sentences that contain the item wi immediately followed by a sentence that contains wj . |V |is the vocabulary size for syntactic items. 3.2.2 Global structure Now we turn to a global coherence approach that implements the assumption that sentences with similar syntax have the same communicative goal as well as captures the patterns in communicative goals in the discourse. This approach uses a Hidden Markov Model (HMM) which has been a popular implementation for modeling coherence (Barzilay and Lee, 2004; Fung and Ngai, 2006; Elsner et al., 2007). The hidden states in our model depict communicative goals by encoding a probability distribution over syntactic items. This distribution gives higher weight to syntactic items that are more likely for that communicative goal. Transitions between states record the common patterns in intentional structure for the domain. In this syntax-HMM, states hk are created by clustering the sentences from the documents in the training set by syntactic similarity. For the productions representation of syntax, the features for clustering are the number of times a given production appeared in the parse of t"
D12-1106,J86-3001,0,0.514503,"of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each sentence in the article has a communicative goal and the sequence of goals helps t"
D12-1106,D11-1025,0,0.0217339,"er et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Motivation, Model, Hy"
D12-1106,P08-1067,0,0.0125096,"is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Productions: In this representation we view each sentence as the set of grammatical productions, LHS → RHS, which a"
D12-1106,J09-1003,0,0.018057,"Missing"
D12-1106,P03-1054,0,0.00570919,"arly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6 Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7 http://aye.comp.nus.edu.sg/parsCit/ 1164 could be successfully identified using Parscit.8 This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). For each corpus and each section, we train all our syntactic models: the two local coherence models using the production and d-sequence representations and the HMM models with the two representations. These models are tuned on the respective development data, on the task of differentiating the original from a permuted section. For this purpose, we created a maximum of 30 permutations per article. 6.1 Comparison with ART Corpus zones We perform this analysis using the ART corpus. The zone annotations present in this corpus allow us to directly test our first assumption in this work, that sent"
D12-1106,P03-1069,0,0.260847,"d work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a"
D12-1106,liakata-etal-2010-corpora,0,0.0113564,"we tested for combination. In prior work, content and entity grid methods have been combined generatively (Elsner et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labe"
D12-1106,D09-1036,0,0.0591354,"Missing"
D12-1106,P11-1100,0,0.416454,"l automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each sentence in the ar"
D12-1106,D08-1020,1,0.432459,"have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each"
D12-1106,A97-1004,0,0.0332239,"domly choose 70 articles from ACL and NAACL main conferences. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6 Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7 http://aye.comp.nus.edu.sg/parsCit/ 1164 could be successfully identified using Parscit.8 This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). For each corpus and each section, we train all our syntactic models: the two local coherence models using the production and d-sequence representations and the HMM models with the two representations. These models are tuned on the respective development data, on the task of differentiating the original from a permuted section. For this purpose, we created a maximum of 30 permutations per article. 6.1 Comparison with ART Corpus zones We perform this analysis using the ART corpus. The zone annotations present in this corpus allow us"
D12-1106,P06-2103,0,0.761263,"s of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an eve"
D12-1106,W00-1302,0,0.147745,"bined generatively (Elsner et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Mo"
D12-1106,E99-1015,0,0.0194253,"simple approach that we tested for combination. In prior work, content and entity grid methods have been combined generatively (Elsner et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assig"
D12-1106,J93-2004,0,\N,Missing
D15-1178,N13-1015,1,0.81284,", we compute p(T [j] → pi |T [j]) as the probability under a unigram language model Lj which is trained on the collection of the posts from the training corpus which are dominated by Q Np T [j] nodes. p(T [j] → pi |T [j]) = k=1i Lj (wki ) i are the tokens in post pi . where w1i , w2i ...wN pi The rest of the production probabilities are learned using MLE on the training trees. In the case of LCFRS rules, the gap information is also obtained during the extraction. 6.1 6.3 6 Parsers for Conversation Trees Refining the non-terminals We use a clustering approach, akin to the spectral algorithm of Cohen et al. (2013) and Narayan and Cohen (2015),3 to create finer grained categories corresponding to GB ’s non-terminals: S, X, C and T . Each node in each tree in the training data is associated with a feature vector, which is a function of the tree and the anchor node. These vectors are clustered (for each of the non-terminals separately) and then, each node is annotated with the corresponding cluster. This process gives us the non-terminals S[i], X[j], T [k] and C[l] of GE . The features for a node nl are: depth of nl in the tree, root is at depth 0; maximum depth of the subtree under nl ; number of sibling"
D15-1178,D08-1035,0,0.143178,"identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur"
D15-1178,N09-1040,0,0.0185533,"ast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to"
D15-1178,J10-3004,0,0.669708,"nk annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama"
D15-1178,P03-1071,0,0.0606101,"typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly re"
D15-1178,D10-1084,0,0.0230044,"perform other tree generation baselines by a significant margin especially on short threads. 2 Related work The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1 Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality pr"
D15-1178,W12-4615,0,0.0337959,"Missing"
D15-1178,P06-1004,0,0.0348871,"r the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and tha"
D15-1178,P05-1010,0,0.0878193,"→ T C |T , T → p and C → X |X X |X C. The C nodes can be collapsed and its daughters attached to the parent of C to revert back to the non-binary tree. While this CFG defines the structure of conversation trees, by itself this grammar is insufficient for our task. In particular, it contains a single nonterminal of each type (S, X, T , C) and so does not distinguish between topics. We extend this grammar to create GE which has a set of nonterminals corresponding to each non-terminal in GB , these fine-grained non-terminals correspond to different topics. GE is created using latent annotations (Matsuzaki et al., 2005) on the X, T , S and C non-terminals from GB . The resulting non-terminals for GE are S[i], X[j], T [k] and C[l], such that 1 ≤ i ≤ NS , 1 ≤ j ≤ NX , 1 ≤ k ≤ NT , 1 ≤ l ≤ NC . i, j, k and l identify specific topics attached to a particular node type. Our output trees are created with GE to depict the topic segmentation of the thread and are nonbinary. The binary trees produced by our algorithms are converted by collapsing the C. As a result, conversation trees have S[i], X[j] and T [k] 2 Any context-free grammar can be converted to an equivalent CNF grammar. Our algorithms support unary rules."
D15-1178,D15-1214,1,0.77639,"i |T [j]) as the probability under a unigram language model Lj which is trained on the collection of the posts from the training corpus which are dominated by Q Np T [j] nodes. p(T [j] → pi |T [j]) = k=1i Lj (wki ) i are the tokens in post pi . where w1i , w2i ...wN pi The rest of the production probabilities are learned using MLE on the training trees. In the case of LCFRS rules, the gap information is also obtained during the extraction. 6.1 6.3 6 Parsers for Conversation Trees Refining the non-terminals We use a clustering approach, akin to the spectral algorithm of Cohen et al. (2013) and Narayan and Cohen (2015),3 to create finer grained categories corresponding to GB ’s non-terminals: S, X, C and T . Each node in each tree in the training data is associated with a feature vector, which is a function of the tree and the anchor node. These vectors are clustered (for each of the non-terminals separately) and then, each node is annotated with the corresponding cluster. This process gives us the non-terminals S[i], X[j], T [k] and C[l] of GE . The features for a node nl are: depth of nl in the tree, root is at depth 0; maximum depth of the subtree under nl ; number of siblings of nl ; number of children"
D15-1178,J03-1006,0,0.0288771,"r to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns ~ x)) to conditional probabilities p(A(~x) → φ|A(~ the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(~x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, ρ ~] where A is a non-terminal and ρ~ is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exhaustive parsing as in Viterbi parsing of CFGs. The complexity of parsing is O(n3k ) where k is the fa"
D15-1178,J86-3001,0,0.597124,"level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree, a requirement posed by many prior seg1544 mentation algorithms. The second limitation of prior studies is assuming that topics do not recur in the same document. But linguistic theories allow for non-adjacent utterances to belong to the same topic segment (Grosz and Sidner, 1986) and this fact is empirically true in chat and forum conversations (Elsner and Charniak, 2010; Wang et al., 2011). Our mo"
D15-1178,P94-1002,0,0.546745,"r and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics"
D15-1178,P10-2028,0,0.0164992,"s perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree,"
D15-1178,P01-1064,0,0.15201,", 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are dis"
D15-1178,P87-1015,0,0.840849,"Missing"
D15-1178,J13-1006,0,0.0515534,"productions of a CFG take the single spans of each non-terminal on the RHS and concatenate them in the same order to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns ~ x)) to conditional probabilities p(A(~x) → φ|A(~ the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(~x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, ρ ~] where A is a non-terminal and ρ~ is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exh"
D15-1178,D11-1002,0,0.458409,"generation baselines by a significant margin especially on short threads. 2 Related work The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1 Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein"
D15-1178,C10-1061,0,\N,Missing
D15-1178,U10-1009,0,\N,Missing
D18-1466,P98-2176,0,0.10454,"12) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the source. Earlier work (Radev, 1998) modeled the choice of the best expression (from a lexicon) to fit the specific semantic context during text generation. Supplementing these efforts, we model the progression of entities’ status from hearer-new to hearer-old as it changes across, rather than within, documents. In the social sciences, Graus et al. (2017) analyzed distributions of entity mentions, and intervals between mentions, to identify patterns of entities becoming common knowledge, but without looking at the content of the REs. The coinage and subsequent acceptance/extinction of lexical innovations is another domain that m"
D18-1466,P14-5010,0,0.00434734,"Missing"
D18-1466,J93-2004,0,0.0612252,"Business Machines Corporation, which is the secondbiggest advertiser on the Internet International Business Machines Corporation, the worlds largest computer company International Business Machines Corporation, based in Armonk, N.Y. Western Resources Inc., worth $1.7 billion The National Basketball Association in New York International Business Machines International Business Machines Corporation International Business Machines Corporation Western Resources Inc. National Basketball Association Table 1: Regular expressions (regex) for finding RE spans within an NP. The regex use Penn Treebank (Marcus et al., 1993) tags. The full phrase is the RE, the string of NNPs ‘First Lieut. Kelly Flinn’ that comprise the embedded NP is the base expression, and the remaining NP ‘the pilot’ is the descriptor. Certain adjustments had to be made. For example, NPs of the form ‘NNP of NNP’ (e.g. “University of Virginia”) are treated as base expressions. In addition, some connectives and symbols were included in the base to accommodate names such as “Food and Drug Administration”. These adjustments lead to their own errors. For instance, for “Dan Zegart of Titusville”, our exception rule will mark the full expression as"
D18-1466,P12-1084,0,0.0210443,"and discourse-old respectively), or either newly introduce the entity to an audience (hearer-new) or be part of common knowledge (hearer-old). According to Prince (1992), hearer-old entities are more often mentioned with definite expressions, since the hearer can pick out the unique referent based on background knowledge. Corpus studies (Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the source. Earlier work (Rad"
D18-1466,N03-2024,0,0.155809,"company does, whereas now such elaboration is needed rarely, if at all. This paper presents a first computational study that relates the form of an entity’s referring expressions (RE) in articles written at different times to the entity’s changing information status.2 Previous work has focused on predicting how REs for an entity vary within a single text. This type of information status can improve coreference resolution (Recasens et al., 2013) and 1 article 1386221 from Sandhaus (2008) 2 Corpus available at http://groups.inf.ed.ac. uk/cup/ref/ help generate references in automatic summaries (Nenkova and McKeown, 2003). But there has been little exploration of the change in REs to an entity over time and across articles, as the entity is accepted into common knowledge. The current work is driven by linguistic interest in characterizing REs over time. In addition, knowing the current acceptance of an entity can help in generating time-appropriate expressions. From a social science perspective, there is also great interest in capturing the birth, acceptance into common parlance, but also possible death, and subsequent reintroductions of entities. In this paper, we disambiguate and track thousands of person (P"
D18-1466,W06-1612,0,0.0489405,"tion for Computational Linguistics and discourse-old respectively), or either newly introduce the entity to an audience (hearer-new) or be part of common knowledge (hearer-old). According to Prince (1992), hearer-old entities are more often mentioned with definite expressions, since the hearer can pick out the unique referent based on background knowledge. Corpus studies (Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references i"
D18-1466,D11-1099,0,0.0277042,"tational Linguistics and discourse-old respectively), or either newly introduce the entity to an audience (hearer-new) or be part of common knowledge (hearer-old). According to Prince (1992), hearer-old entities are more often mentioned with definite expressions, since the hearer can pick out the unique referent based on background knowledge. Corpus studies (Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the so"
D18-1466,N13-1071,0,0.0670792,"Missing"
D18-1466,J11-4007,0,0.0281636,"(Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the source. Earlier work (Radev, 1998) modeled the choice of the best expression (from a lexicon) to fit the specific semantic context during text generation. Supplementing these efforts, we model the progression of entities’ status from hearer-new to hearer-old as it changes across, rather than within, documents. In the social sciences, Graus et al. (2017) analyzed di"
D18-1466,C18-1135,0,0.0149887,"eneration. Supplementing these efforts, we model the progression of entities’ status from hearer-new to hearer-old as it changes across, rather than within, documents. In the social sciences, Graus et al. (2017) analyzed distributions of entity mentions, and intervals between mentions, to identify patterns of entities becoming common knowledge, but without looking at the content of the REs. The coinage and subsequent acceptance/extinction of lexical innovations is another domain that models expressions over time, often by mapping properties of the speakers who use them within a community (see Tredici and Fernandez, 2018 and work reviewed therein). Our focus here is on REs specifically. In what follows, we explain our RE extraction (Section 3) and linguistic features (Section 4). Analysis of the REs and the model for information status prediction are in Sections 5 and 6. 3 Extracting REs over time We use the New York Times Annotated Corpus (NYTAC) (Sandhaus, 2008), containing the 1.8M articles published in the New York Times over the period 1987–2007 (20 years). Given the complexity in identifying potentially interesting entities and disambiguating references to them over time, we limited our scope to person"
D18-1466,P13-1043,0,0.0606745,"Missing"
D18-1466,D14-1162,0,0.0813896,"include the number of mentions within the one month bin, and the average time gap between consecutive mentions (multiplied by the log of number of mentions to compensate for frequency). We also include the entity’s current age (time since introduction). Context: We employ the topic metadata from NYTAC to capture a notion of world context of an entity’s mentions. Every article has topic tags (sports, finance, technology, politics, etc.) and also a section label (travel, economics, culture, etc.). We clustered the thousands of topic tags into 20 broad topics by using the Glove word embeddings (Pennington et al., 2014), and K-means clustering. A small set of 17 clusters were also created for the newspaper sections. The count of mentions belonging to articles in each cluster is a feature. 5 Features versus acceptance time We built a linear model (LM) to test which features are significantly predictive of time to acceptance. An example is the set of REs for an entity from a one month bin. The dependent variable is the time left until acceptance, i.e. the value (in months) from the current age of the entity (month were the mentions were taken from) until the acceptance age. For our corpus, the possible values"
D19-1620,D12-1091,0,0.0165501,"highest average of ROUGE-1, -2 and -L scores against the abstractive gold standard. 7 Results and Discussion Table 3 reports the F1 scores for ROUGE-1,-2 and -L (Lin, 2004). We use the pyrouge3 wrapper library to evaluate the final models, while training with a faster Python-only implementation4 . We test for significance between the baseline models and our proposed techniques using the bootstrap method. This method was first recommended for testing significance in ROUGE scores by Lin (2004), and has subsequently been advocated as an appropriate measure in works such as Dror et al. (2018) and Berg-Kirkpatrick et al. (2012). 2 We are unable to evaluate this model on the lead overlap measure due to lack of access to the model outputs. 3 www.github.com/bheinzerling/pyrouge 4 www.github.com/Diego999/py-rouge Figure 1: Training curves for BanditSum based models. Average ROUGE is the average of ROUGE-1, -2 and -L F1. The simple entropy regularizer has a small but not significant improvement, and pretraining has a similar improvement only for RNES. But the auxiliary ROUGE loss significantly (p < 0.001) improves over BanditSum, obtaining an extra 0.15 ROUGE points on average. The last column reports the percentage of s"
D19-1620,P15-2136,0,0.0374767,"Missing"
D19-1620,P16-1046,0,0.149251,"Missing"
D19-1620,D18-1409,1,0.897238,"Missing"
D19-1620,P18-1128,0,0.0141444,"rce sentences with the highest average of ROUGE-1, -2 and -L scores against the abstractive gold standard. 7 Results and Discussion Table 3 reports the F1 scores for ROUGE-1,-2 and -L (Lin, 2004). We use the pyrouge3 wrapper library to evaluate the final models, while training with a faster Python-only implementation4 . We test for significance between the baseline models and our proposed techniques using the bootstrap method. This method was first recommended for testing significance in ROUGE scores by Lin (2004), and has subsequently been advocated as an appropriate measure in works such as Dror et al. (2018) and Berg-Kirkpatrick et al. (2012). 2 We are unable to evaluate this model on the lead overlap measure due to lack of access to the model outputs. 3 www.github.com/bheinzerling/pyrouge 4 www.github.com/Diego999/py-rouge Figure 1: Training curves for BanditSum based models. Average ROUGE is the average of ROUGE-1, -2 and -L F1. The simple entropy regularizer has a small but not significant improvement, and pretraining has a similar improvement only for RNES. But the auxiliary ROUGE loss significantly (p < 0.001) improves over BanditSum, obtaining an extra 0.15 ROUGE points on average. The last"
D19-1620,E14-1075,0,0.0976512,"Missing"
D19-1620,P14-1062,0,0.0272008,"Missing"
D19-1620,D18-1208,0,0.0708357,"31.11 30.09 21.65 31.59 32.15 25.76 28.71 30.09 24.91 29.00 28.63 5.00 4.72 0.42 4.72 4.93 4.98 Table 2: BanditSum’s performance—calculated as the average between ROUGE-1,-2, and -L F1—on the validation set of the CNN/Daily Mail corpus. The sentence position information is perturbed at different levels, as explained in Section 4. worse when tested on a mismatched data perturbation. Even when the distortion is at a single lead position in insert-lead and insert-lead3, the performance on the original data is significantly lower than when trained without the distortion. These results corroborate Kedzie et al. (2018)’s findings for RL-based systems. Interestingly, the random model has the best mean performance and the lowest variation indicating that completely removing the position bias may allow a model to focus on learning robust sentence semantics. 5 Learning to Counter Position Bias We present two methods which encourage models to locate key phrases at diverse parts of the article. 5.1 Multi-Stage Training This technique is inspired by the robust results from the random model in section 4. We implement a multi-stage training method for both BanditSum and RNES where in the first few epochs, we train o"
D19-1620,D14-1181,0,0.00776977,"Missing"
D19-1620,Q18-1017,0,0.0548552,"Missing"
D19-1620,W04-1013,0,0.0309211,"eplacing LKL with the negated entropy of PM in eq. 2. This loss penalizes low entropy, helping the model explore, but it is ‘undirected’ compared to our proposed method. We present the results of Lead-3 baseline (first 3 sentences), and two other competitive models—Refresh2 (Narayan et al., 2018a) and NeuSum (Zhou et al., 2018). Lastly, we include results from an oracle summarizer, computed as the triplet of source sentences with the highest average of ROUGE-1, -2 and -L scores against the abstractive gold standard. 7 Results and Discussion Table 3 reports the F1 scores for ROUGE-1,-2 and -L (Lin, 2004). We use the pyrouge3 wrapper library to evaluate the final models, while training with a faster Python-only implementation4 . We test for significance between the baseline models and our proposed techniques using the bootstrap method. This method was first recommended for testing significance in ROUGE scores by Lin (2004), and has subsequently been advocated as an appropriate measure in works such as Dror et al. (2018) and Berg-Kirkpatrick et al. (2012). 2 We are unable to evaluate this model on the lead overlap measure due to lack of access to the model outputs. 3 www.github.com/bheinzerling"
D19-1620,K16-1028,0,0.0809297,"Missing"
D19-1620,D18-1206,0,0.170281,"d, the pretraining approach produces mixed results. We also confirm that when summary-worthy sentences appear late, there is a large performance discrepancy between the oracle summary and state-of-the-art summarizers, indicating that learning to balance lead bias with other features of news text is a noteworthy issue to tackle. 2 Related Work Modern summarization methods for news are typically based on neural network-based sequenceto-sequence learning (Kalchbrenner et al., 2014; Kim, 2014; Chung et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a; Zhou et al., 2018). In MLE-based training, extractive summarizers are trained with gradient ascent to maximize the likelihood of heuristically-generated groundtruth binary labels (Nallapati et al., 2017). Many MLE-based models do not perform as well as their reinforcement learning-based (RL) competitors that directly optimize ROUGE (Paulus et al., 2018; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018). As RL-based models represent the state of the art for extractive summarization, we analyze them in this paper. The closest work to ours is a recent study by Kedzie et al. (2018) whi"
D19-1620,N18-1158,0,0.129587,"d, the pretraining approach produces mixed results. We also confirm that when summary-worthy sentences appear late, there is a large performance discrepancy between the oracle summary and state-of-the-art summarizers, indicating that learning to balance lead bias with other features of news text is a noteworthy issue to tackle. 2 Related Work Modern summarization methods for news are typically based on neural network-based sequenceto-sequence learning (Kalchbrenner et al., 2014; Kim, 2014; Chung et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a; Zhou et al., 2018). In MLE-based training, extractive summarizers are trained with gradient ascent to maximize the likelihood of heuristically-generated groundtruth binary labels (Nallapati et al., 2017). Many MLE-based models do not perform as well as their reinforcement learning-based (RL) competitors that directly optimize ROUGE (Paulus et al., 2018; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018). As RL-based models represent the state of the art for extractive summarization, we analyze them in this paper. The closest work to ours is a recent study by Kedzie et al. (2018) whi"
D19-1620,P18-1061,0,0.105675,"oach produces mixed results. We also confirm that when summary-worthy sentences appear late, there is a large performance discrepancy between the oracle summary and state-of-the-art summarizers, indicating that learning to balance lead bias with other features of news text is a noteworthy issue to tackle. 2 Related Work Modern summarization methods for news are typically based on neural network-based sequenceto-sequence learning (Kalchbrenner et al., 2014; Kim, 2014; Chung et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a; Zhou et al., 2018). In MLE-based training, extractive summarizers are trained with gradient ascent to maximize the likelihood of heuristically-generated groundtruth binary labels (Nallapati et al., 2017). Many MLE-based models do not perform as well as their reinforcement learning-based (RL) competitors that directly optimize ROUGE (Paulus et al., 2018; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018). As RL-based models represent the state of the art for extractive summarization, we analyze them in this paper. The closest work to ours is a recent study by Kedzie et al. (2018) which showed that MLEbas"
E09-1062,J08-1001,0,0.0809535,"Missing"
E09-1062,W02-1033,0,0.101391,"Missing"
E09-1062,P06-2020,0,0.130376,"Missing"
E09-1062,P08-1080,0,0.0605695,"Missing"
E09-1062,C00-1072,0,0.136372,"Missing"
E09-1062,W07-0737,0,0.018197,"on answering for example, a system may be configured not to answer questions for which the confidence of producing a correct answer is low, and in this way increase the overall accuracy of the system whenever it does produce an answer (Brill et al., 2002; Dredze and Czuba, 2007). 1 Introduction Similarly in machine translation, some sentences might contain difficult to translate phrases, that is, portions of the input are likely to lead to garbled output if automatic translation is attempted. Automatically identifying such phrases has the potential of improving MT as shown by an oracle study (Mohit and Hwa, 2007). More recent work (Birch et al., 2008) has shown that properties of reordering, source and target language complexity and relatedness can be used to predict translation quality. In information retrieval, the problem of predicting system performance has generated considerable interest and has led to notably good results (Cronen-Townsend et al., 2002; Yom-Tov et al., 2005; Carmel et al., 2006). The input to a summarization system significantly affects the quality of the summary that can be produced for it, by either a person or an automatic method. Some inputs are difficult and summaries produc"
E09-1062,P08-1094,1,0.852784,"d utilizing more representative examples for good and bad performance. We also extend the analysis to single document summarization, for which predicting system performance turns out to be much more accurate than for multi-document summarization. We address three key questions. In summarization, researchers have recognized that some inputs might be more successfully handled by a particular subsystem (McKeown et al., 2001), but little work has been done to qualify the general characteristics of inputs that lead to suboptimal performance of systems. Only recently the issue has drawn attention: (Nenkova and Louis, 2008) present an initial analysis of the factors that influence system performance in content selection. This study was based on results from the Document Understanding Conference (DUC) evaluations (Over et al., 2007) of multi-document summarization of news. They showed that input, system identity and length of the target summary were all significant factors affecting summary quality. Longer summaries were consistently better than shorter ones for the same input, so improvements can be easy in applications where varying target size is possible. Indeed, varying summary size is desirable in many situ"
E09-1062,N01-1003,0,0.0191257,"Missing"
E09-1062,D08-1078,0,\N,Missing
E14-1017,C00-1072,0,0.0862228,"ake advantage of likely words in the domain and consistency, but which also adapt to topic shifts. 3.1 A general domain model This system seeks to bias translations towards words which occur often in biography articles. The topic cache is filled with word unigrams that are more likely to occur in biographies com156 pared to general news documents. We compare the words from 1,475 English Wikipedia biographies articles to those in a large collection (64,875 articles) of New York Times (NYT) news articles (taken from the NYT Annotated Corpus (Sandhaus, 2008)). We use a log-likelihood ratio test (Lin and Hovy, 2000) to identify words which occur with significantly higher probability in biographies compared to NYT. We collect only words indicated with 0.0001 significance by the test to be more likely in biographies. We rank this set of 18,597 words in decreasing order of frequency in the biography article set and assign to each word a score equal to 1/rank of the word. These words with their associated scores form the contents of the topic cache. In the general domain model, these same words are assumed to be useful for the full document and so the cache contents remain constant during translation of the"
E14-1017,P03-1021,0,0.0397194,"kipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (Tiedemann, 2010b). In future work, we will explore how successful optimization of baseline and cache feature weights could be done jointly. We present the findings from our grid search below. The struct-topic cache has two parameters, the number of topics T and the number of most probable words from each topic which get loaded into the cache. We ran the tuning for T = 25, 50, 100 and 200 topi"
E14-1017,P02-1040,0,0.0904727,"Missing"
E14-1017,P12-1048,0,0.0198003,"m that translates Wikipedia biographies from French to English by adapting a system 1 Corpus available at http://homepages.inf.ed. ac.uk/alouis/wikiBio.html. 155 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155–163, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics A different line of work very relevant to our study is the creation of topic-specific translations by either inferring a topic for the source document as a whole, or at the other extreme, finer topics for individual sentences (Su et al., 2012; Eidelman et al., 2012). Neither of these granularities seem intuitive in natural discourse. In this work, we propose that tailoring translations to topics associated with discourse segments in the article is likely to be beneficial for two reasons: a) subtopics of such granularity can be assumed with reasonable confidence to re-occur in documents from the same domain and b) we can hypothesize that a domain will have a small number of segment-level topics. mann (2010a). In such methods, cache(s) can be filled with relevant items for translation and translation hypotheses that match a greater"
E14-1017,P12-2023,0,0.0213013,"Wikipedia biographies from French to English by adapting a system 1 Corpus available at http://homepages.inf.ed. ac.uk/alouis/wikiBio.html. 155 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155–163, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics A different line of work very relevant to our study is the creation of topic-specific translations by either inferring a topic for the source document as a whole, or at the other extreme, finer topics for individual sentences (Su et al., 2012; Eidelman et al., 2012). Neither of these granularities seem intuitive in natural discourse. In this work, we propose that tailoring translations to topics associated with discourse segments in the article is likely to be beneficial for two reasons: a) subtopics of such granularity can be assumed with reasonable confidence to re-occur in documents from the same domain and b) we can hypothesize that a domain will have a small number of segment-level topics. mann (2010a). In such methods, cache(s) can be filled with relevant items for translation and translation hypotheses that match a greater number of cache items ar"
E14-1017,W10-2602,0,0.0596777,"rom the 1-best translations of previous sentences in the same document. Each word is associated with an age value and a score. Age indicates when a word entered the cache and introduces a ‘decay effect’. Words used in immediately previous sentences have a low age value while higher age values indicate words from sentences much prior in the document. Scores are inversely proportional to age. Both the types of caches are present in both the general domain and structured models, but the cache words and scores are computed differently. Related work The study that is closest to our work is that of Tiedemann (2010a), which proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (20"
E14-1017,D08-1035,0,0.030374,"verage BLEU score gains from a structured cache (compared to domain caches) split by different properties of documents in the test set document structure to match that handled better by the structured model. We test this hypothesis by segmenting all test documents with an ideal segment size. The model seems to perform better when each segment has around 5 to 10 sentences (longer segments are also preferred but we have few very long documents in our corpus), so we try to re-segment the articles to contain approximately 7 sentences in each segment. We use an automatic topic segmentation method (Eisenstein and Barzilay, 2008) to segment the source articles in our test corpus. For each article we request (document length)/7 segments to be created.5 We then run the structured topic and consistency models on the automatically segmented corpus using the same feature weights as before. The results are shown in Table 6. Model Struct-topic Struct-consistency BLEU (doc) 17.94 17.51 BLEU (sent) 17.94 17.46 Table 6: Translation performance on automatically segmented test corpus The struct-topic cache now reaches our best result of 0.5 BLEU improvement over the out-ofdomain model and 0.3 improvement over the unstructured dom"
E14-1017,2012.iwslt-evaluation.1,0,0.0609649,"Missing"
E14-1017,W10-1728,0,0.277657,"rom the 1-best translations of previous sentences in the same document. Each word is associated with an age value and a score. Age indicates when a word entered the cache and introduces a ‘decay effect’. Words used in immediately previous sentences have a low age value while higher age values indicate words from sentences much prior in the document. Scores are inversely proportional to age. Both the types of caches are present in both the general domain and structured models, but the cache words and scores are computed differently. Related work The study that is closest to our work is that of Tiedemann (2010a), which proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (20"
E14-1017,2010.amta-papers.24,0,0.0347941,"is harder to do on very divergent domains. In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics such as ‘childhood’, ‘schooling’, ‘career’ and ‘later life’, and a likely chronological order to these topics. These regularities become more predictable in documents from sources such as Wikipedia. This setting allows us to explore the utility of models which make translation decisions depending on the discourse structure. Translation methods for structured documents have only recently been explored in Foster et al. (2010). However, their system was developed for parlimentary proceedings and translations were adapted using separate language models based upon the identity of the speaker, text type (questions, debate, etc.) and the year when the proceedings took place. Biographies constitute a more realistic discourse context to develop structured models. This paper introduces a new corpus consisting of paired French-English translations of biography articles from Wikipedia.1 We translate this corpus by developing cache-based domain adaptation methods, a technique recently proposed by TiedeWe present a French to"
E14-1017,P06-2124,0,0.0736738,"Missing"
E14-1017,D11-1084,0,0.049085,"proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (2010b) and Gong et al. (2011), were applied for translating in-domain documents. Gong et al. (2011) introduced additional caches to store (i) words and phrase pairs from training documents most similar to a current source article, and (ii) words from topical clusters created on the training set. However, a central issue in these systems is that caches become noisy over time, since they ignore topic shifts in the documents. This paper presents cache models which not only take advantage of likely words in the domain and consistency, but which also adapt to topic shifts. 3.1 A general domain model This system seeks to bias t"
E14-1017,D12-1108,0,0.107521,"Missing"
E14-1017,N03-1017,0,0.0288526,"erage no. of segments per article Tuning 15 430 13 59 4.7 Test 30 1008 12 85 5.3 Table 1: Summary of Wikipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (Tiedemann, 2010b). In future work, we will explore how successful optimization of baseline and cache feature weights could be done jointly. We present the findings from our grid search below. The struct-topic cache has two parameters, the number of topics T and the number of most probable words from"
E14-1017,P07-2045,0,0.0170959,"ain subtopics. We select only articles that have at least 10 segments (sections) to ensure 4 http://sourceforge.net/projects/ aligner/ A filtered set of 13,400 entries from www.dict.cc 158 No. of article pairs Total sentences pairs Min. article size (in sentences) Max. article size (in sentences) Average no. of segments per article Tuning 15 430 13 59 4.7 Test 30 1008 12 85 5.3 Table 1: Summary of Wikipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (T"
E14-1017,federico-etal-2012-iwslt,0,\N,Missing
E14-1067,Q13-1028,1,0.862056,"tor (Chen and Mooney, 2008) and question answering with different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspect"
E14-1067,E06-1038,0,0.0400983,"versus specific sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to develop statistical models for sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007). It consists of pairs of sentences in an original text and a professional summary of that text. In every pair, one of the sentences (source) appeared in the original text and the other is a shorter version with the superfluous details deleted. Both sentences were produced by people. We use the dataset created by Galley and McKeown (2007). The sentences are taken from the Ziff Davis Corpus which contains articles about technology products. This data also contains alignment between the constituency parse nodes of the source and summary sentence pair. Through the align"
E14-1067,C08-1019,0,0.0335038,"l., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspects of quality and often the ratings for different aspects are highly correlated (Conroy and Dang, 2008; Pitler et al., 2010) b) Moreover, for verbosity in particular, the most appropriate data for annotation would be concise and verbose versions of the same text (possibly of similar lengths). It is more likely that people can distinguish between verbosity of these controlled pairs compared to ratings on an individual article. Such writing samples are not easily available. So we have avoided the uncertainties in annotation in this first work by adopting a simpler approach based on three key ideas. (i) We define a concise article of length l as “an article that has the appropriate types of conte"
E14-1067,P12-1094,0,0.0205826,"ith different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspects of quality and often the ratings for different aspects are hig"
E14-1067,P09-2004,1,0.755621,"tion of entities, i.e the LHS (lefthand side) of the production is a noun phrase. The count of each of these productions is added as a feature allowing us to track what type of information about the entities is conveyed in the snippet. We also add features for the most frequent 15 productions whose LHS is not a noun phrase. Discourse relations (5 features). These features are based on the hypothesis that different discourse relations would vary in their appropriateness for articles of different lengths. For example causal information may be included only in more detailed texts. We use a tool (Pitler and Nenkova, 2009) to identify all explicit discourse connectives in our snippets, along with the general semantic class of the connective (temporal, comparison, contingency and expansion). We use the number of discourse connectives of each of the four types as features, as well as the total number of connectives. Continuity (6 features). These features capture the degree to which adjacent sentences in the snippet are related and continue the topic. The amount of continuity for subtopics is likely to vary for long and short texts. We add the number of pronouns and determiners as two features. Another feature is"
E14-1067,D08-1035,0,0.0294122,"source would be of high quality overall. 5.1 Data We obtain the text of the articles from the NYT Annotated Corpus (Sandhaus, 2008). We choose the articles from the opinion section of the newspaper since they are likely to have good topic continuity and related content compared to general news which often contain lists of facts. We further use only the editorial articles to ensure that the articles are of high quality. We collect 10,724 opinion articles from years 2000 to 2007 of the NYT. We divide each article into topic segments using the unsupervised topic segmentation method developed by Eisenstein and Barzilay (2008). We use the following heuristic to decide on the number of topic segments for each article. If the article has fewer than 50 sentences, we create segments such that the expected length Classification results The task is to predict the length of the summary from which the fixed length snippet was taken, i.e. 4-way classification—50, 100, 200 or a 400 word summary. We trained an SVM classifier with a radial basis kernel on the 2001 data. The regularization and kernel parameters were tuned using 10fold cross validation on the training set. The accuracies of classification on the 2002 data are sh"
E14-1067,P10-1056,1,0.856027,"the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspects of quality and often the ratings for different aspects are highly correlated (Conroy and Dang, 2008; Pitler et al., 2010) b) Moreover, for verbosity in particular, the most appropriate data for annotation would be concise and verbose versions of the same text (possibly of similar lengths). It is more likely that people can distinguish between verbosity of these controlled pairs compared to ratings on an individual article. Such writing samples are not easily available. So we have avoided the uncertainties in annotation in this first work by adopting a simpler approach based on three key ideas. (i) We define a concise article of length l as “an article that has the appropriate types of content expected in an arti"
E14-1067,P05-1045,0,0.00936916,"n the Stanford Coreference tool (Raghunathan et al., 2010) to identity pronoun and entity coreference links within the snippet. The number of total coreference links, and the number of intra- and inter-sentence links are added as three separate features. Amount of detail (7 features). To indicate descriptive words, we compute the number of adjectives and adverbs (two features). We also include the total number of named entities (NEs), average length of NEs in words and the number of sentences that do not have any NEs. The named entities were identified using the Stanford NER recognition tool (Finkel et al., 2005). We also use the predictions of a classifier trained to identify general versus specific sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to dev"
E14-1067,D10-1048,0,0.0169272,"gives the likelihood of the text being deleted. We also add the perplexity value based on this likelihood, P −1/n where P is the likelihood and n is the number of productions from the snippet for which we have deletion information in our data.2 For training a model, we need texts which we can assume are written in a concise manner. We use two sources of data—summaries written by people and high quality news articles. the vectors of adjacent sentences and the average value of the similarity across all pairs of adjacent sentences is the feature value. We also run the Stanford Coreference tool (Raghunathan et al., 2010) to identity pronoun and entity coreference links within the snippet. The number of total coreference links, and the number of intra- and inter-sentence links are added as three separate features. Amount of detail (7 features). To indicate descriptive words, we compute the number of adjectives and adverbs (two features). We also include the total number of named entities (NEs), average length of NEs in words and the number of sentences that do not have any NEs. The named entities were identified using the Stanford NER recognition tool (Finkel et al., 2005). We also use the predictions of a cla"
E14-1067,N07-1023,0,0.0201974,"sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to develop statistical models for sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007). It consists of pairs of sentences in an original text and a professional summary of that text. In every pair, one of the sentences (source) appeared in the original text and the other is a shorter version with the superfluous details deleted. Both sentences were produced by people. We use the dataset created by Galley and McKeown (2007). The sentences are taken from the Ziff Davis Corpus which contains articles about technology products. This data also contains alignment between the constituency parse nodes of the source and summary sentence pair. Through the alignment it is possible to trac"
E14-1067,D13-1181,0,0.155135,"ration (O’Donnell, 1997), soccer commentator (Chen and Mooney, 2008) and question answering with different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it"
E14-1067,P08-1080,0,0.0333736,"and k &lt; mintj l(tj ). The mapping f is learned based on the constant length snippet only and the aim is to predict the original text length. are interspersed with the original shorter summary. The performance of a range of human-machine applications can be enhanced if they had the ability to predict the appropriate length of a system contribution and the type of content appropriate for that length. Such applications include document generation (O’Donnell, 1997), soccer commentator (Chen and Mooney, 2008) and question answering with different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to"
E14-1067,P03-1054,0,0.0273392,"easily available. So we have avoided the uncertainties in annotation in this first work by adopting a simpler approach based on three key ideas. (i) We define a concise article of length l as “an article that has the appropriate types of content expected in an article of length l”. Note that length is not equal to verbosity in our model. Our defif (sti ) → ˆl(ti ) In our work we choose to work with topical segments from documents rather than the complete documents themselves. 637 article. All the syntax based features are computed from the constituency trees produced from the Stanford Parser (Klein and Manning, 2003). Once the model is trained, we identify the verbosity for a test article as follows: Let us consider a new topic segment tx during test time. Let the length of the segment be l. We obtain a snippet stx of size k from tx . Now assume that our model predicts f (stx ) = ˆl. Case 1: ˆl ' l, the content type in tx matches the content types generally present in articles of length l. We consider such articles as concise. Case 2: ˆl  l, the type of content included in tx is really suitable for longer and detailed topic segments. Thus tx is likely conveying too much detail given its length i.e. it is"
E14-1067,I11-1068,1,0.860989,"er-sentence links are added as three separate features. Amount of detail (7 features). To indicate descriptive words, we compute the number of adjectives and adverbs (two features). We also include the total number of named entities (NEs), average length of NEs in words and the number of sentences that do not have any NEs. The named entities were identified using the Stanford NER recognition tool (Finkel et al., 2005). We also use the predictions of a classifier trained to identify general versus specific sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to develop statistical models for sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007). It consists of pairs of sentences in an original text and a professional summary of th"
I11-1068,N09-1041,0,0.020246,"tails about the topic. We show that our classifier successfully distinguishes these two types of summaries. Summarization is one task where the distinction between general and specific content is relevant. The space available for summary content is limited. So authors include some specific detail but at the same time have to generalize other content to stay within the space limit. Early work in Jing and McKeown (2000) report that when people create summaries, they generalize some of the sentences from the source text, others are made more specific. From the point of view of automatic systems, Haghighi and Vanderwende (2009) developed a topic model-based summarization system which learns the topics of the input at both overall document level as well as specific subtopics. Sentences are assumed to be generated by a combination of the general and specific topics in the input 611 texts. However, since the preference of such sentences is not known, only heuristics were applied to choose the proportions. We expect our classifier to be useful in such cases. 4.1 Data We use summaries and source texts from the Document Understanding Conference (DUC) organized by NIST in 2005.5 The task in 2005 was to create summaries tha"
I11-1068,H05-1044,0,0.00700482,"he specific ones. So we introduced two features—the number of words in the sentence and the number of nouns. Polarity. Sentences with strong opinion are typical in the general category in our examples in Table 2. For instance, the phrases “publishing sensation”, and “very slowly–if at all” are evaluative while the specific sentences in these relations present evidence which justify the general statements. So, we record for each sentence the number of positive, negative and polar (not neutral) words using two lexicons—The General Inquirer (Stone et al., 1966) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). We also add another set of features where each of these counts is normalized by the sentence length. Specificity. Specific sentences are more likely to contain specific words and details. We use two sets of features to capture specificity of words in the sentence. The first of these is based on WordNet (Miller et al., 1990) and is motivated by prior work by Resnik (1995) where hypernym relations from WordNet were used to compute specificity. For each noun and verb in a sentence, we record the length of the path from the word to the root of 606 Instantiations [1] The 40-year-old Mr. Murakami"
I11-1068,A00-2024,0,0.0529215,"luation on news summaries. Here people were asked to write general or specific summaries for a set of articles, in the first type conveying only the general ideas and in the second providing specific details about the topic. We show that our classifier successfully distinguishes these two types of summaries. Summarization is one task where the distinction between general and specific content is relevant. The space available for summary content is limited. So authors include some specific detail but at the same time have to generalize other content to stay within the space limit. Early work in Jing and McKeown (2000) report that when people create summaries, they generalize some of the sentences from the source text, others are made more specific. From the point of view of automatic systems, Haghighi and Vanderwende (2009) developed a topic model-based summarization system which learns the topics of the input at both overall document level as well as specific subtopics. Sentences are assumed to be generated by a combination of the general and specific topics in the input 611 texts. However, since the preference of such sentences is not known, only heuristics were applied to choose the proportions. We expe"
I11-1068,P03-1054,0,0.00812179,"Missing"
I11-1068,prasad-etal-2008-penn,0,0.0599945,"relations generalizes without noticeable compromise in accuracy. We describe our classifier based on discourse relations here, the annotation study is detailed in the next section. 2 2.1 Features A general vs. specific sentence classifier based on discourse relations The task of differentiating general and specific content has not been addressed in prior work, so there is no existing corpus annotated for specificity. For this reason, we first exploit indirect annotations of these distinctions in the form of certain types of discourse relations annotated in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The discourse relations we consider are Specification and Instantiation. They are defined to hold between adjacent sentences. The definitions of the relations do not talk directly about the specificity of sentences, but they seem to indirectly indicate that the first one is general and the second is specific. The exact definitions of these two relations in the PDTB are given in (Prasad et al., 2007). Some examples are shown in Table 2. The PDTB annotations cover 1 million words from Wall Street Journal (WSJ) articles. Instantiations and Specifications are fairly frequent (1403 and 2370 respe"
I11-1068,P10-1005,0,0.256259,"Missing"
I11-1068,J93-2004,0,\N,Missing
I11-1068,P02-1047,0,\N,Missing
J13-2002,P07-1038,0,0.263603,"mmaries. These system summaries or “pseudomodels” are chosen to be the ones which receive high scores based on the one available model summary. We expect that the beneﬁt of pseudomodels will be noticeable in micro-level correlations with pyramid scores. At the macro level, even with multiple human models there is no improvement in correlations compared with a single model, and the addition of less-ideal system summaries is not likely to be better than adding human summaries. 5.1 Related Work The idea of using system output for evaluation was introduced in the context of machine translation by Albrecht and Hwa (2007, 2008). In their method, Albrecht and Hwa (2007) designate some systems to act as pseudoreferences. Then, every candidate translation to be evaluated is compared to the translations produced by the pseudoreferences using a variety of similarity metrics. Each similarity value is then used as a feature and trained to predict the human assigned score for that candidate translation. They show that the scores produced by their regression metric using only system-based references correlates with human judgments to the same extent as scores produced using multiple human reference translations. Also,"
J13-2002,W08-0330,0,0.0753241,"Missing"
J13-2002,W10-1703,0,0.0554496,"Missing"
J13-2002,W11-2103,0,0.0396829,"Missing"
J13-2002,P06-2020,0,0.181305,"Missing"
J13-2002,W00-0408,0,0.268032,"Missing"
J13-2002,W04-3247,0,0.0243617,"for generic summarization (i.e., the set of documents given as input must be summarized to reﬂect the sources as best as possible; in contrast, TAC 2009 tasks can be considered as focused summarization where either a query is provided or an update is required). This method ﬁrst computes a set of topic words from the input using a loglikelihood ratio. The sentences are ranked using the score introduced by Conroy, Schlesinger, and O’Leary (2006): the ratio of the number of unique topic words in the sentence to the unique content words in the sentence. Graph centrality: This approach (Erkan and Radev 2004; Mihalcea and Tarau 2005) performs selection over a graph representation of the input sentences. Each sentence is represented in vector space using unigram word counts. Two sentences are linked when their vectors have a cosine similarity of at least 0.1. When this graph is converted into a Markov chain, we can compute the stationary distribution of the transition matrix deﬁned by the graph’s edges. This stationary distribution gives the probability of visiting each node during repeated random walks through the graph. The high probability nodes are the ones that are most visited and these corr"
J13-2002,W09-1802,0,0.0838456,"ry similarity features are computed using the input, they can be useful features to incorporate in a summarization system. The combination of systems to perform evaluation also provides a way to build a better system. The concern would be how the usefulness of these metrics will change if systems were also optimizing for them. To optimize a metric such as JS divergence exactly would be difﬁcult because the JS divergence score cannot be factored or divided among individual sentences, a necessary condition if the problem should be solved using an Integer Linear Program as in McDonald (2007) and Gillick and Favre (2009). Therefore only greedy methods are possible. In fact, KL divergence was greedily optimized in Haghighi and Vanderwende (2009) to obtain a high performance summarizer. Gaming the evaluation should carry little concern, however, as these metrics are proposed with a view to tuning systems. The metrics we presented are developed for evaluation in a new setting where model summaries are not available and to aid system development and tuning. Further, notice from the micro-level evaluation that a single metric such as JS divergence does not predict content selection performance well for all inputs."
J13-2002,W10-0722,0,0.35282,"ider the ROUGE results as the upper bound of performance for the model-free evaluations that we propose because ROUGE involves direct comparison with the gold-standard summaries. Our metrics are designed to be used when model summaries are not available. 2.5 Automatic Evaluation Without Gold-Standard Summaries All of these methods require signiﬁcant human involvement. In evaluations where goldstandard summaries are needed, assessors ﬁrst read the input documents (10 or more per input) and write a summary. Then manual comparison of system and gold standard is done, which takes additional time. Gillick and Liu (2010) hypothesize that at least 17.5 hours are needed to evaluate two systems under this set up on a standard test set. Moreover, multiple gold-standard summaries are needed for the same input, so different assessors have to read and create summaries. The more reliable evaluation 4 The scores were computed after stemming but stop words were retained in the summaries. 271 Computational Linguistics Volume 39, Number 2 Table 1 Spearman correlation between manual scores and ROUGE metrics on TAC 2009 data (53 systems). All correlations are highly signiﬁcant with p-value < 10−10 . ROUGE variant ROUGE-1 R"
J13-2002,N09-1041,0,0.202792,"em. The combination of systems to perform evaluation also provides a way to build a better system. The concern would be how the usefulness of these metrics will change if systems were also optimizing for them. To optimize a metric such as JS divergence exactly would be difﬁcult because the JS divergence score cannot be factored or divided among individual sentences, a necessary condition if the problem should be solved using an Integer Linear Program as in McDonald (2007) and Gillick and Favre (2009). Therefore only greedy methods are possible. In fact, KL divergence was greedily optimized in Haghighi and Vanderwende (2009) to obtain a high performance summarizer. Gaming the evaluation should carry little concern, however, as these metrics are proposed with a view to tuning systems. The metrics we presented are developed for evaluation in a new setting where model summaries are not available and to aid system development and tuning. Further, notice from the micro-level evaluation that a single metric such as JS divergence does not predict content selection performance well for all inputs. System developers should therefore involve other specialized features as well. Regression of similarity metrics is a better p"
J13-2002,W04-1003,0,0.0129822,"expert evaluation is the quality of the model summaries. Evaluations based on model summaries assume that the gold standards are of high quality. Through the years at TAC, considerable effort has been invested to ensure that the evaluation scores do not vary depending on the particular gold standard. In the early years of TAC only one gold-standard summary was used. During this time, papers reported ANOVA tests examining the factors that most inﬂuenced summary scores from the evaluations and found that the identity of the judge turned out to be the most signiﬁcant factor (McKeown et al. 2001; Harman and Over 2004). But it is desirable that a model summary or a human judgment be representative of important content in general and does not depict the individual biases of the person who created the summary or made the judgment. So the evaluation methodology was reﬁned to remove the inﬂuence of the assessor identity on the evaluation. The pyramid evaluation was also developed with this goal of smoothing out the variation between judges. Gillick and Liu (2010) point out that Mechanical Turk evaluations have this undesirable outcome: The identity 272 Louis and Nenkova Automatic Content Evaluation of the judge"
J13-2002,N04-1022,0,0.00886725,"nt, we ﬁnd that consensus among system summaries is indicative of important content. This result suggests that by combining the content selected by multiple systems, one might be able to build a summary that is better than each of them individually. In fact, this idea of system consensus has been utilized in the development of MT systems for quite some time. One approach in MT is rescoring the n-best list from an individual system’s decoder, and picking the (consensus) translation that is close on average to all translations. Such rescoring is implemented using a minimum Bayes risk technique (Kumar and Byrne 2004; Tromble et al. 2008). The other approach is system combination where the output from multiple systems is combined to produce a new translation. Several techniques including minimum Bayes risk have been applied to perform system combination in machine translation. Shared tasks on system combination have also been organized in recent years to encourage the development of such methods (Callison-Burch et al. 2010, 2011). Such strategies could be a useful direction to explore for summarization as well. 7. Discussion In this article, we have discussed metrics for summary evaluation when human summ"
J13-2002,W04-1013,0,0.48583,"Missing"
J13-2002,N06-1059,0,0.386894,"ting the features. 4.2.1 Distribution Similarity. Measures of similarity between two probability distributions are a natural choice for our task. One would expect good summaries to be characterized by low divergence between probability distributions of words in the input and summary, and by high similarity with the input. We experimented with three common measures: Kullback Leibler (KL) divergence, Jensen Shannon (JS) divergence, and cosine similarity. These three metrics have already been applied for summary evaluation, albeit in a different context. In their study of model-based evaluation, Lin et al. (2006) used KL and JS divergences to measure the similarity between human and machine summaries. They found that JS divergence always outperformed KL divergence. Moreover, the performance of JS divergence was better than standard ROUGE scores for multi-document summarization when multiple human models were used for the comparison. The use of input–summary similarity in Donaway, Drummey, and Mather (2000), which we described in the previous section, is more directly related to our work. But here, inputs and summaries were compared using only one metric: cosine similarity. Kullback Leibler (KL) diverg"
J13-2002,C00-1072,0,0.0493824,"the JS distance is symmetric and always deﬁned. We compute both smoothed and unsmoothed versions of the divergence as summary scores. Vector space similarity: The third metric is cosine overlap between the tf ∗ idf vector representations of input and summary contents. cosθ = vinp .vsumm ||vinp |vsumm || (5) We compute two variants: 1. Vectors contain all words from input and summary. 2. Vectors contain only topic signature words from the input and all words of the summary. Topic signatures are words highly descriptive of the input, as determined by the application of the log-likelihood test (Lin and Hovy 2000). Using only topic signatures from the input to represent text is expected to be more accurate because the reduced vector has fewer dimensions compared with using all the words from the input. 4.2.2 Summary Likelihood. For this approach, we view summaries as being generated according to word distributions in the input. Then the probability of a word in the input would be indicative of how likely it is to be emitted into a summary. Under this generative model, the likelihood of a summary’s content can be computed using different methods and we expect the likelihood to be higher for better quali"
J13-2002,N03-1020,0,0.505299,"nly the one available model. Finally, we explore the feasibility of another measure—similarity between a system summary and the pool of all other system summaries for the same input. This method of comparison with the consensus of systems produces impressively accurate rankings of system summaries, achieving correlation with human rankings above 0.9. 1. Introduction In this work, we present evaluation metrics for summary content which make use of little or no human involvement. Evaluation methods such as manual pyramid scores (Nenkova, Passonneau, and McKeown 2007) and automatic ROUGE scores (Lin and Hovy 2003) rely on multiple human summaries as a gold standard (model) against which they compare a summary to assess how informative the candidate summary is. It is desirable that evaluation of similar quality be done quickly and cheaply ∗ E-mail: lannie@seas.upenn.edu. ∗∗ University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut St., Philadelphia, PA 19104. E-mail: nenkova@seas.upenn.edu. Submission received: 18 June 2011; revised submission received: 23 March 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00123 © 2013 Association for Computational Ling"
J13-2002,D09-1032,1,0.877548,"y, Drummey, and Mather demonstrated that the correlations between manual evaluation using a gold-standard summary and a) manual evaluation using a different gold-standard summary 275 Computational Linguistics Volume 39, Number 2 b) automatic evaluation by directly comparing input and summary6 are the same. Their conclusion was that such automatic methods should be seriously considered as an alternative to evaluation protocols built around the need to compare with a gold standard. These studies, however, do not directly assess the performance of input–summary similarity for ranking systems. In Louis and Nenkova (2009a), we provided the ﬁrst study of several metrics for measuring similarity for this task and presented correlations of these metrics with human produced rankings of systems. We have released a tool, SIMetrix (Summary-Input Similarity Metrics), which computes all the similarity metrics that we explored.7 4.2 Metrics for Computing Similarity In this section, we describe a suite of similarity metrics for comparing the input and summary content. We use cosine similarity, which is standard for many applications. The other metrics fall under three main classes: distribution similarity, summary likel"
J13-2002,E09-1062,1,0.910439,"y, Drummey, and Mather demonstrated that the correlations between manual evaluation using a gold-standard summary and a) manual evaluation using a different gold-standard summary 275 Computational Linguistics Volume 39, Number 2 b) automatic evaluation by directly comparing input and summary6 are the same. Their conclusion was that such automatic methods should be seriously considered as an alternative to evaluation protocols built around the need to compare with a gold standard. These studies, however, do not directly assess the performance of input–summary similarity for ranking systems. In Louis and Nenkova (2009a), we provided the ﬁrst study of several metrics for measuring similarity for this task and presented correlations of these metrics with human produced rankings of systems. We have released a tool, SIMetrix (Summary-Input Similarity Metrics), which computes all the similarity metrics that we explored.7 4.2 Metrics for Computing Similarity In this section, we describe a suite of similarity metrics for comparing the input and summary content. We use cosine similarity, which is standard for many applications. The other metrics fall under three main classes: distribution similarity, summary likel"
J13-2002,W07-0716,0,0.0305552,"however, pseudoreferences of different quality are chosen in an oracle manner (using the human-assigned scores). This setting is not practical because it depends on the actual system scores. In later work, Albrecht and Hwa (2008) use off-the-self machine translation systems as pseudoreferences and show that they can contribute to good results. This later work is a more realistic set-up and here regression is important because we have no guarantees as to the quality of the off-the-shelf systems on the test data. A similar idea of augmenting machine output to human gold standard was explored in Madnani et al. (2007) in the context of machine translation (MT). For tuning MT systems, often multiple reference translations are required. Madnani et al. augmented reference translations of a sentence with automatically generated paraphrases of the reference. They found in the experiments that such augmentation helped in MT tuning—the number of reference translations needed could be cut in half and compensated with automatic paraphrases. 5.2 Choice of Pseudoreference Systems For this evaluation, the choice of the pseudoreference system is an important step. In this section, we detail some development experiments"
J13-2002,P08-1094,1,0.891924,"ndard. Saggion et al. (2010) report that trends can be observed in the JSD metric performance although it does not provide good evaluations for opinion and biographical type inputs. Automatic evaluations in different genres therefore have different requirements and exploring these is an avenue for future work. Input–summary similarity based only on word distribution works well for evaluating summaries of cohesive-type inputs. We can also envision a situation where we will be able to predict whether the JS divergence evaluation will be accurate or not on a particular test set. In prior work in Nenkova and Louis (2008) and Louis and Nenkova (2009b), we have explored properties of summarization inputs and provided a characterization of inputs into cohesive and less cohesive based on automatic features. The less cohesive inputs were found to be the ones where automatic systems in general performed poorly. In that work, we proposed features to predict if an input is cohesive or not. We now apply these features to the TAC’09 data with the intention of automatically identifying inputs suitable for JS divergence evaluation (the cohesive ones). The features were trained on data from previous years of TAC evaluatio"
J13-2002,N04-1019,1,0.739737,"y’s content is called coverage. These coverage scores were taken as indicators of content quality for the system summaries. Different people include very different content in their summaries, however, and so the coverage scores can vary depending on which model is used (Rath, Resnick, and Savage 1961). This problem of bias in evaluation was later addressed by the pyramid technique, which combines information from multiple model summaries to compose the reference for evaluation. Since 2005, the pyramid evaluation method has become standard. 2.2 Pyramid Evaluation The pyramid evaluation method (Nenkova and Passonneau 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova, Passonneau, and McKeown 2007). It uses multiple human models from which annotators identify semantically deﬁned Summary Content Units (SCUs). Each SCU is assigned a weight equal to the number of human model summaries that express that SCU. An ideal maximally informative summary would express a subset of the most highly weighted SCUs, with multiple maximally informative summaries being possible. The pyramid score for a system sum"
J13-2002,W09-2806,0,0.0334818,"Missing"
J13-2002,radev-etal-2004-mead,0,0.169143,"Missing"
J13-2002,D08-1065,0,0.0124538,"sus among system summaries is indicative of important content. This result suggests that by combining the content selected by multiple systems, one might be able to build a summary that is better than each of them individually. In fact, this idea of system consensus has been utilized in the development of MT systems for quite some time. One approach in MT is rescoring the n-best list from an individual system’s decoder, and picking the (consensus) translation that is close on average to all translations. Such rescoring is implemented using a minimum Bayes risk technique (Kumar and Byrne 2004; Tromble et al. 2008). The other approach is system combination where the output from multiple systems is combined to produce a new translation. Several techniques including minimum Bayes risk have been applied to perform system combination in machine translation. Shared tasks on system combination have also been organized in recent years to encourage the development of such methods (Callison-Burch et al. 2010, 2011). Such strategies could be a useful direction to explore for summarization as well. 7. Discussion In this article, we have discussed metrics for summary evaluation when human summaries are not present."
J13-2002,W03-0508,0,0.0903101,"Missing"
J13-2002,C10-2122,0,\N,Missing
J16-4011,D10-1084,0,0.081412,"Missing"
J16-4011,D14-1215,0,0.061462,"Missing"
J16-4011,P14-2110,0,0.0691627,"Missing"
J16-4011,N10-1020,0,0.113006,"Missing"
J16-4011,P14-1017,0,0.0727033,"Missing"
louis-nenkova-2012-corpus,J93-2004,0,\N,Missing
louis-nenkova-2012-corpus,prasad-etal-2008-penn,0,\N,Missing
louis-nenkova-2012-corpus,I11-1068,1,\N,Missing
louis-nenkova-2012-corpus,W11-1605,1,\N,Missing
N10-1043,J95-2003,0,0.792196,"Missing"
N10-1043,N06-2015,0,0.0305724,"Missing"
N10-1043,J04-3003,0,0.203222,"Missing"
N10-1043,prasad-etal-2008-penn,0,0.237627,"Missing"
N12-2010,W10-4236,0,0.0303015,"d recommendation for news articles. We also include a third genre—automatically generated summaries. Here, when systems produce multi-sentence text, they must ensure that the text is readable and coherent. Automatic evaluation of content and linguistic quality is therefore necessary for system development in this genre. 2 Thesis Summary and Contributions For this thesis, we only consider the discourse and style components of text quality, aspects that have received less focus in prior work. Sentence-level problems have been widely explored and recently, even specifically for academic writing (Dale and Kilgarriff, 2010). We also do not consider content in our work, for example, academic writing quality also depends on the ideas and arguments presented but these aspects are outside the scope of this thesis. As defined previously, we focus on a fixed audience 55 level. We assume a reader at the top level of the competency spectrum: an adult educated reader for science news and automatic summaries, and for academic articles, an expert on the topic. This definition has minimal focus on reader abilities and allows us to analyze textual differences exclusively. The specific contributions of this thesis are: 1. Def"
N12-2010,E09-1027,0,0.0355972,"Missing"
N12-2010,N07-1058,0,0.0187214,"om different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng 56 et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique"
N12-2010,liakata-etal-2010-corpora,0,0.0442164,"Missing"
N12-2010,P11-1100,0,0.0609797,"ty and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically generate short children’s stories using patterns of event and entity"
N12-2010,E09-1062,1,0.861822,"ad. Also if the ideas were not closely related in the article that would create additional difficulty. This aspect is important for machine generated text: an automatic summary should focus on a few main aspects rather than present a bag of many unrelated facts. In fact, in large scale evaluation workshops, automatic summaries are also manually graded for a ‘focus’ aspect. For this purpose, we want to identify 58 metrics which can indicate cohesiveness and focus of an article. In our studies so far, we have have developed cohesiveness metrics for clusters of articles (Nenkova and Louis, 2008; Louis and Nenkova, 2009). In future work, we will explore how these metrics work for individual articles. Information quality also arises in the context of source documents given for automatic summarization. Particularly for systems which summarize online news, the input is created by clustering together news on the same topic from different sources. For example, a cluster may be created for the Japanese earthquake and aftermath. When the period covered is too large or when the documents discuss many different opinions and ideas it becomes hard for a system to point out the most relevant facts. So one proxy for cohes"
N12-2010,I11-1068,1,0.890252,"Missing"
N12-2010,W11-1605,1,0.835392,"Missing"
N12-2010,D12-1106,1,0.878991,"Missing"
N12-2010,louis-nenkova-2012-corpus,1,0.88826,"Missing"
N12-2010,P09-1025,0,0.014231,"subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically generate short children’s stories using patterns of event and entity co-occurrences. They find that people judge their stories as better when the text is optimized not only for coherence and but also its interesting nature. They use a supervised approach to predict the interest value for a story during the generation process. Burstein et al. (2010) find that for predicting the coherence of student essays, better accuracies can be obtained by augmenting generic coherence metrics with features related to student writing such as word variety and spelling errors. In my own work on a"
N12-2010,P08-1094,1,0.831659,"would be difficult to read. Also if the ideas were not closely related in the article that would create additional difficulty. This aspect is important for machine generated text: an automatic summary should focus on a few main aspects rather than present a bag of many unrelated facts. In fact, in large scale evaluation workshops, automatic summaries are also manually graded for a ‘focus’ aspect. For this purpose, we want to identify 58 metrics which can indicate cohesiveness and focus of an article. In our studies so far, we have have developed cohesiveness metrics for clusters of articles (Nenkova and Louis, 2008; Louis and Nenkova, 2009). In future work, we will explore how these metrics work for individual articles. Information quality also arises in the context of source documents given for automatic summarization. Particularly for systems which summarize online news, the input is created by clustering together news on the same topic from different sources. For example, a cluster may be created for the Japanese earthquake and aftermath. When the period covered is too large or when the documents discuss many different opinions and ideas it becomes hard for a system to point out the most relevant fac"
N12-2010,D08-1020,0,0.241293,"ries of prior work on this topic. The first is studies on ‘readability’ which have proposed metrics to select texts appropriate (easy to read) for an audience of given age and education level (Flesch, 1948; Collins-Thompson and Callan, 2004). These metrics typically classify texts as suitable for adult or child, or into a more fine-grained set of 12 educational grade levels. The second line of work are recent computational metrics to predict coherence. These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzilay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of ar1. We consider a single fixed audience level and the texts that audience is typically exposed to. For example, a college educated reader of a newspaper might find some articles better written than others, even though he understands and can read nearly all of them with ease. 2. It is a holistic property of texts. At a minimum, at least four factors influence quality: the content/topic that is discussed, sentence levelgrammaticality, discourse coherence and writing style. Here writing style refers to extra properties introduced into the text by the author but do not"
N12-2010,P10-1056,1,0.70027,"dren’s stories using patterns of event and entity co-occurrences. They find that people judge their stories as better when the text is optimized not only for coherence and but also its interesting nature. They use a supervised approach to predict the interest value for a story during the generation process. Burstein et al. (2010) find that for predicting the coherence of student essays, better accuracies can be obtained by augmenting generic coherence metrics with features related to student writing such as word variety and spelling errors. In my own work on automatic evaluation of summaries (Pitler et al., 2010), I have observed the impact of genre. We consider a corpus of summaries written by people and those produced by automatic systems. Psycholinguistic metrics previously proposed for analyzing coherence of human texts work successfully on human summaries but are less accurate for system summaries. Similarly, metrics which predict the fluency of machine translations accurately, work barely above baseline for judging the grammaticality of sentences from human summaries. But they give high accuracies on machine summary sentences. So for machine and human generated text, clearly different features m"
N12-2010,P05-1065,0,0.165387,"So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng 56 et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audi"
N12-2010,P06-2103,0,0.0296252,"audience distinctions other than grade levels. Feng 56 et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications."
N12-2010,W00-1302,0,0.0954998,"Missing"
N12-2010,N04-1025,0,\N,Missing
N12-2010,N10-1099,0,\N,Missing
N12-2010,N04-1015,0,\N,Missing
N12-2010,J08-1001,0,\N,Missing
N18-2111,D15-1208,0,0.0148199,"focus on learning regular and repetitive event sequences in stories (Chambers and Jurafsky, 2009; McIntyre and Lapata, 2009), together with some information about the agent of the actions. These extractions are fairly lowlevel, in the form of noun-verb pairs. There are also models for clustering stories either based on their characters (Frermann and Szarvas, 2017), or sentiment and topic (Elsner, 2012, 2015). The above approaches mine types of actions or characters. This work focuses on infering the latent ties between actions and characters, and whether one aspect can help predict the other. Flekova and Gurevych (2015) present recent work related to this latter idea. They classify characters based on their speech and actions into an introvert or extrovert class. In contrast, we focus on attributes of characters and actions beyond such coarse traits, and when these attributes are expressed as complex descriptions. 3 A noteworthy aspect of these RPGs is that character attributes are determined by writing the descriptions before the game starts. The story thread itself then focuses predominantly on the actions and does not reiterate character attributes. Moreover, we know unambiguously which character is assoc"
N18-2111,P10-1015,0,0.0889485,"Missing"
N18-2111,N16-1149,0,0.0306833,"bers with a “NUM” token. We drop all punctuation and any text with less than 5 tokens. After these preprocessing steps, we have 1,439 stories containing 1.48M tokens for action descriptions and 2.95M for characters. 4 P (X|C) = i=1 p(xi |zi , x1 ...xi−1 , z1 ...zi−1 ), where zl is a variable indicating which character produced the token xl . For this model, we essentially augment the RNNs with the character descriptions as side information. For each token xl , the side information is the character description indicated by zl , i.e, Czl . We follow the approach by Mikolov and Zweig (2012), and Hoang et al. (2016), where a feature embedding vector e representing side information is input to both the RNN’s hidden and output layers, or to one of them. During development, we found that concatenating the feature embedding with the token embedding at the input layer, and with the hidden state at output layer gave the best performance. More formally, ACTION -LMS computes:    xi−1 hi = LSTM hi−1 , ei     hi P (xi |x1 . . . xi−1 ) = softmax Wrv + bv ei Learning character-action interactions We examine the feasibility of inferring characteraction interactions from text using neural language models (LM)"
N18-2111,P13-1035,0,0.0380066,"Missing"
N18-2111,N16-1180,0,0.033295,"Missing"
N18-2111,P14-1035,0,0.0372952,"Missing"
N18-2111,P15-1162,0,0.0686606,"Missing"
N18-2111,P09-1068,0,0.0515207,"ities for making sophisticated inference over narrative texts. 2 Related work In work on narratives, both characters and actions have received significant attention, albeit separately. There is work on inducing types of characters (Bamman et al., 2013, 2014) or relationships between characters (Chang et al., 2009; Elson et al., 2010; Chaturvedi et al., 2016; Iyyer et al., 2016). Often these approaches are based on probabilistic topic models or more recently distributed word representations computed by neural networks. Others focus on learning regular and repetitive event sequences in stories (Chambers and Jurafsky, 2009; McIntyre and Lapata, 2009), together with some information about the agent of the actions. These extractions are fairly lowlevel, in the form of noun-verb pairs. There are also models for clustering stories either based on their characters (Frermann and Szarvas, 2017), or sentiment and topic (Elsner, 2012, 2015). The above approaches mine types of actions or characters. This work focuses on infering the latent ties between actions and characters, and whether one aspect can help predict the other. Flekova and Gurevych (2015) present recent work related to this latter idea. They classify chara"
N18-2111,P09-1025,0,0.027127,"d inference over narrative texts. 2 Related work In work on narratives, both characters and actions have received significant attention, albeit separately. There is work on inducing types of characters (Bamman et al., 2013, 2014) or relationships between characters (Chang et al., 2009; Elson et al., 2010; Chaturvedi et al., 2016; Iyyer et al., 2016). Often these approaches are based on probabilistic topic models or more recently distributed word representations computed by neural networks. Others focus on learning regular and repetitive event sequences in stories (Chambers and Jurafsky, 2009; McIntyre and Lapata, 2009), together with some information about the agent of the actions. These extractions are fairly lowlevel, in the form of noun-verb pairs. There are also models for clustering stories either based on their characters (Frermann and Szarvas, 2017), or sentiment and topic (Elsner, 2012, 2015). The above approaches mine types of actions or characters. This work focuses on infering the latent ties between actions and characters, and whether one aspect can help predict the other. Flekova and Gurevych (2015) present recent work related to this latter idea. They classify characters based on their speech"
N18-2111,E12-1065,0,0.0205659,"t al., 2010; Chaturvedi et al., 2016; Iyyer et al., 2016). Often these approaches are based on probabilistic topic models or more recently distributed word representations computed by neural networks. Others focus on learning regular and repetitive event sequences in stories (Chambers and Jurafsky, 2009; McIntyre and Lapata, 2009), together with some information about the agent of the actions. These extractions are fairly lowlevel, in the form of noun-verb pairs. There are also models for clustering stories either based on their characters (Frermann and Szarvas, 2017), or sentiment and topic (Elsner, 2012, 2015). The above approaches mine types of actions or characters. This work focuses on infering the latent ties between actions and characters, and whether one aspect can help predict the other. Flekova and Gurevych (2015) present recent work related to this latter idea. They classify characters based on their speech and actions into an introvert or extrovert class. In contrast, we focus on attributes of characters and actions beyond such coarse traits, and when these attributes are expressed as complex descriptions. 3 A noteworthy aspect of these RPGs is that character attributes are determi"
P08-1094,P99-1071,0,0.0853771,"Let X be a discrete random variable taking values from the finite set V = {w1 , ..., wn } where V is the vocabulary of the input set and wi are the words that appear in the input. The probability distribution p(w) = P r(X = w) can be easily calculated using frequency counts from the input. The entropy of the input set is equal to the entropy of X: H(X) = − i=n X p(wi ) log2 p(wi ) (1) i=1 Average, minimum and maximum cosine overlap between the news articles in the input. Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006). The more similar the different documents in the input are to each other, the more likely there is repetition across documents at various granularities. Cosine similarity between the document vector representations is probably the easiest and most commonly used among the various similarity measures. We use tf*idf weights in the vector representations, with term frequency (tf) normalized by the total number of words in the document in order to remove bias resulting from high frequencies by virtue of higher document length alone. 829 The cosine similar"
P08-1094,P06-2020,0,0.186573,"Missing"
P08-1094,P07-2049,1,0.778613,"rge background collection. Then the relative entropy between the input and the collection is given by KL divergence = X w∈I pinp (w) log 2 pinp (w) pcoll (w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents. Number of topic signature terms for the input set. The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T . Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection N T ). Given T and N T , the likelihood ratio statistic (Dunning, 1994) is used to identify the topic signature terms. The probabilistic model of the data allows for statistical inference in order to decide which terms t a"
P08-1094,W05-0902,0,0.0273248,"th the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been paid to this fact in system development and no specific user studies are available to show what summary length might be most suitable for specific applications. In later editions of the DUC conference, only summaries of 100 words were produced, focusing development efforts on one of the more demanding length restrictions. The interaction between summary length and summarizer is small but significant (Table 1), with certain summarization strategies more successful at particular summary lengths than at others. Improved performance as measured by increase"
P08-1094,C00-1072,0,0.408407,"py, between the input (I) and collection language models. Let pinp (w) be the probability of the word w in the input and pcoll (w) be the probability of the word occurring in the large background collection. Then the relative entropy between the input and the collection is given by KL divergence = X w∈I pinp (w) log 2 pinp (w) pcoll (w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents. Number of topic signature terms for the input set. The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T . Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection N T ). Given T and N T , the likelihood r"
P08-1094,N03-1020,0,0.28589,"cores than summarizer identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1 The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attenti"
P08-1094,W03-0510,0,0.0215943,"cores than summarizer identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1 The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attenti"
P08-1094,W04-1013,0,0.0199578,"identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1 The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been p"
P09-1077,N07-1054,0,0.731783,"n to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 684 In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful information was obtained using nouns and adjectives. Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model."
P09-1077,C08-2022,1,0.876339,". In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 684 In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful info"
P09-1077,W01-1605,0,0.139711,"Missing"
P09-1077,N03-1030,0,0.0982978,"marked by explicit discourse connectives (also called cue words) such as “because” or “but”. It is not uncommon, though, for a discourse relation to hold between two text spans without an explicit discourse connective, as the example below demonstrates: 2 (1) The 101-year-old magazine has never had to woo advertisers with quite so much fervor before. [because] It largely rested on its hard-to-fault demographics. Related Work Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense, but often for explicits and at the sentence level. Soricut and Marcu (2003) address the task of In this paper we address the problem of automatic sense prediction for discourse relations 683 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683–691, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 3 parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the co"
P09-1077,W03-1210,0,0.262437,"Missing"
P09-1077,W06-1317,0,0.209991,"d to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations. Our final verb features were the part of speech tags (gold-standard from the Penn Treebank) of the main verb. One would expect that Expansion would link sentences with the same tense, whereas Contingency and Temporal relations would contain verbs with different tenses. First-Last, First3: The first and last words of a relation’s arguments have been found to be particularly useful for predicting its sense (Wellner et al., 2006). Wellner et al. (2006) suggest that these words are such predictive features because they are often explicit discourse connectives. In our experiments on implicits, the first and last words are not connectives. However, some implicits have been found to be related by connective-like expressions which often appear in the beginning of the second argument. In the PDTB, these are annotated as alternatively lexicalized relations (AltLexes). To capture such effects, we included the first and last words of Arg1 as features, the first word falls into according to the General Inquirer lexicon (Stone e"
P09-1077,H05-1044,0,0.0221037,"two text spans of a relation are taken from the getting anything but basic food supplies to peogold-standard annotations in the PDTB. ple remains difficult.] is created as an example of Polarity Tags: We define features that represent the Cause relation. Because of examples like this, the sentiment of the words in the two spans. Each “but-but” is a very useful word pair feature indiword’s polarity was assigned according to its encating Cause, as the but would have been removed try in the Multi-perspective Question Answering for the artifical Contrast examples. In fact, the top Opinion Corpus (Wilson et al., 2005). In this re17 features for classifying Contrast versus Other source, each sentiment word is annotated as posiall contain the word “but”, and are indications that tive, negative, both, or neutral. We use the number the relation is Other. of negated and non-negated positive, negative, and neutral sentiment words in the two text spans as These findings indicate an unexpected anomafeatures. If a writer refers to something as “nice” lous effect in the use of synthetic data. Since rein Arg1, that counts towards the positive sentiment lations are created by removing connectives, if an count (Arg1Pos"
P09-1077,J05-2005,0,0.0770325,"Missing"
P09-1077,N04-1020,0,0.0245116,"uccess, while the Marcu and Echihabi (2001) experiments report only accuracy. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 684 In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful information was obtained using nouns and adjectives. Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model. They show that (i) stemming, (ii) using a small fixed vocabulary size consisting of only the most frequent stems (which would tend to be dominated by function words) and (iii) a cutoff on the minimum frequency of a feature, all resu"
P09-1077,P02-1047,0,\N,Missing
P10-1056,J08-1001,0,0.704322,"n validated on data from NIST evaluations. In their pioneering work on automatic evaluation of summary coherence, Lapata and Barzilay (2005) provide a correlation analysis between human coherence assessments and (1) semantic relatedness between adjacent sentences and (2) measures that characterize how mentions of the same entity in different syntactic positions are spread across adjacent sentences. Several of their models exhibit a statistically significant agreement with human ratings and complement each other, yielding an even higher correlation when combined. Lapata and Barzilay (2005) and Barzilay and Lapata (2008) both show the effectiveness of entity-based coherence in evaluating summaries. However, fewer than five automatic summarizers were used in these studies. Further, both sets of experiments perform evaluations of mixed sets of human-produced and machine-produced summaries, so the results may be influenced by the ease of discriminating between a human and machine written summary. Therefore, we believe it is an open question how well these features predict the quality of automatically generated summaries. In this work, we focus on linguistic quality evaluation for automatic systems only. We analy"
P10-1056,W08-0309,0,0.0746868,"Missing"
P10-1056,P03-1054,0,0.00266968,". 2010 Association for Computational Linguistics in Section 4). We test the performance of different sets of features separately and in combination with each other (Section 5). Results are presented in Section 6, showing the robustness of each class and their abilities to reproduce human rankings of systems and summaries with high accuracy. All of the features we investigate can be computed automatically directly from text, but some require considerable linguistic processing. Several of our features require a syntactic parse. To extract these, all summaries were parsed by the Stanford parser (Klein and Manning, 2003). 2 Aspects of linguistic quality 3.1 Word choice: language models We focus on the five aspects of linguistic quality that were used to evaluate summaries in DUC: grammaticality, non-redundancy, referential clarity, focus, and structure/coherence.3 For each of the questions, all summaries were manually rated on a scale from 1 to 5, in which 5 is the best. The exact definitions that were provided to the human assessors are reproduced below. Psycholinguistic studies have shown that people read frequent words and phrases more quickly (Haberlandt and Graesser, 1985; Just and Carpenter, 1987), so t"
P10-1056,E09-1017,1,0.930837,"he discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature exploration on our development set found that under-specified 3 http://www-nlpir.nist.gov/projects/ duc/duc2006/quality-questions.txt 545 3.4 Local coherence: Cohesive devices references to people are much more disruptive to a summary than short references to organizations or locations. In fact, prior work in Nenkova and McKeown (2003) found that summaries that have been rewritten so that first mentions of people are informative desc"
P10-1056,E09-1018,0,0.0329764,"ural features will be better at detecting ungrammatical sentences than the local language model features. Coreference Steinberger et al. (2007) compare the coreference chains in input documents and in summaries in order to locate potential problems. We instead define a set of more general features related to coreference that are not specific to summarization and are applicable for any text. Our features check the existence of proper antecedents for pronouns in the summary without reference to the text of the input documents. We use the publicly available pronoun resolution system described in Charniak and Elsner (2009) to mark possible antecedents for pronouns in the summary. We then compute as features the number of times an antecedent for a pronoun was found in the previous sentence, in the same sentence, or neither. In addition, we modified the pronoun resolution system to also output the probability of the most likely antecedent and include the average antecedent probability for the pronouns in the text. Automatic coreference systems are trained on human-produced texts and we expect their accuracies to drop when applied to automatically generated summaries. However, the predictions and confidence scores"
P10-1056,C08-1019,0,0.0391937,"system scores on the five linguistic quality questions Content Gram Non-redun Ref Focus Gram .02 Non-redun -.40 * .38 * Ref .29 .25 -.07 Focus .28 .24 -.09 .89 * Struct .09 .54 * .27 .76 * .80 * Table 1: Spearman correlations between the manual ratings for systems averaged over the 50 inputs in 2006; * p < .05 We use the summaries from DUC 2006 for training and feature development and DUC 2007 served as the test set. Validating the results on consecutive years of evaluation is important, as results that hold for the data in one year might not carry over to the next, as happened for example in Conroy and Dang (2008)’s work. Following Barzilay and Lapata (2008), we report summary ranking accuracy as the fraction of correct pairwise rankings in the test set. We use a Ranking SVM (SV M light (Joachims, 2002)) to score summaries using our features. The Ranking SVM seeks to minimize the number of discordant pairs (pairs in which the gold standard has x1 ranked strictly higher than x2 , but the learner ranks x2 strictly higher than x1 ). The output of the ranker is always a real valued score, so a global rank order is always obtained. The default regularization parameter was used. 5.1 Combining predictions To"
P10-1056,P03-1069,0,0.0305634,"ce resolution. Instead, noun phrases are considered to refer to the same entity if their heads are identical. Entity coherence features are the only ones that have been previously applied with success for predicting summary coherence. They can therefore be considered to be the state-of-the-art approach for automatic evaluation of linguistic quality. Word coherence: Soricut and Marcu (2006) Word co-occurrence patterns across adjacent sentences provide a way of measuring local coherence that is not linguistically informed but which can be easily computed using large amounts of unannotated text (Lapata, 2003; Soricut and Marcu, 2006). Word coherence can be considered as the analog of language models at the inter-sentence level. Specifically, we used the two features introduced by Soricut and Marcu (2006). Soricut and Marcu (2006) make an analogy to machine translation: two words are likely to be translations of each other if they often appear in parallel sentences; in texts, two words are likely to signal local coherence if they often appear in adjacent sentences. The two features we computed are forward likelihood, the likelihood of observing the words in sentence si conditioned on si−1 , and ba"
P10-1056,N03-1020,0,0.308836,"Missing"
P10-1056,P08-2011,0,0.0534132,"bout a topic. These five questions get at different aspects of what makes a well-written text. We therefore predict each aspect of linguistic quality separately. 3.2 Reference form: Named entities This set of features examines whether named entities have informative descriptions in the summary. We focus on named entities because they appear often in summaries of news documents and are often not known to the reader beforehand. In addition, first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from pr"
P10-1056,W04-1013,0,0.0525781,"Missing"
P10-1056,N07-1055,0,0.010597,"ity to compute the overlap of words in adjacent sentences si and si+1 as a measure of continuity. vsi .vsi+1 (1) cosθ = ||vsi |vsi+1 || The dimensions of the two vectors (vsi and vsi+1 ) are the total number of word types from both sentences si and si+1 . Stop words were retained. The value of each dimension for a sentence is the number of tokens of that word type in that sentence. We compute the min, max, and average value of cosine similarity over the entire summary. 5 547 http://cohmetrix.memphis.edu/ 3.8 would differ from those in incoherent sequences. We use the Brown Coherence Toolkit7 (Elsner et al., 2007) to construct the grids. The tool does not perform full coreference resolution. Instead, noun phrases are considered to refer to the same entity if their heads are identical. Entity coherence features are the only ones that have been previously applied with success for predicting summary coherence. They can therefore be considered to be the state-of-the-art approach for automatic evaluation of linguistic quality. Word coherence: Soricut and Marcu (2006) Word co-occurrence patterns across adjacent sentences provide a way of measuring local coherence that is not linguistically informed but which"
P10-1056,N03-2024,1,0.772875,"tors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature exploration on our development set found that under-specified 3 http://www-nlpir.nist.gov/projects/ duc/duc2006/quality-questions.txt 545 3.4 Local coherence: Cohesive devices references to people are much more disruptive to a summary than short references to organizations or locations. In fact, prior work in Nenkova and McKeown (2003) found that summaries that have been rewritten so that first mentions of people are informative descriptions and subsequent mentions are replaced with more concise reference forms are overwhelmingly preferred to summaries whose entity references have not been rewritten. In coherent text, constituent clauses and sentences are related and depend on each other for their interpretation. Referring expressions such as pronouns link the current utterance to those where the entities were previously mentioned. In addition, discourse connectives such as “but” or “because” relate propositions or events e"
P10-1056,P05-1045,0,0.00216479,"a well-written text. We therefore predict each aspect of linguistic quality separately. 3.2 Reference form: Named entities This set of features examines whether named entities have informative descriptions in the summary. We focus on named entities because they appear often in summaries of news documents and are often not known to the reader beforehand. In addition, first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature ex"
P10-1056,W02-0404,0,0.0614937,"Missing"
P10-1056,J95-2003,0,0.140656,"marized and 35 summarization systems which participated in the evaluation. This included 34 automatic systems submitted by participants, and a baseline system that simply extracted the leading sentences from the most recent article. In DUC 2007, there were 45 inputs and 32 different summarization systems. Apart from the leading sentences baseline, a high performance automatic summarizer from a previous year was also used as a baseline. All these automatic systems are included in our evaluation experiments. Entity coherence: Barzilay and Lapata (2008) Linguistic theories, and Centering theory (Grosz et al., 1995) in particular, have hypothesized that the properties of the transition of attention from entities in one sentence to those in the next, play a major role in the determination of local coherence. Barzilay and Lapata (2008), inspired by Centering, proposed a method to compute the local coherence of texts on the basis of the sequences of entity mentions appearing in them. In their Entity Grid model, a text is represented by a matrix with rows corresponding to each sentence in a text, and columns to each entity mentioned anywhere in the text. The value of a cell in the grid is the entity’s gramma"
P10-1056,W09-2807,0,0.02483,"Missing"
P10-1056,P06-2103,0,0.555556,"ve (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature exploration on our development set found that under-specified 3 http://www-nlpir.nist.gov/projects/ duc/duc2006/quality-questions.txt 545 3.4 Local coherence: Cohesive devices references to people are much more disruptive to a summary than short references to organizations or locations. In fact, prior work in Nenkova and McKeown (2003) found that summaries that have been rewritten so that first mentions of people are informative descriptions and subsequent mentions are replaced with more concis"
P14-2055,P06-2020,0,0.0709746,"Missing"
P14-2055,P06-1039,0,0.106569,"Missing"
P14-2055,E12-1022,0,0.105915,"n et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to learn and use topics discussed only in background, those only in the update input and those that overlap across the two sets. Even for generic summarization, some of the best results were obtained by Conroy et al. (2006) by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by Lin and Hovy (2000). Central to this approach is the use of a likelihood ratio test to compute topic words, words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the in"
P14-2055,N09-1041,0,0.0709351,"m the topic word list before the next sentence selection. KLinp : represents the system that does not use background information. Rather the method creates a summary by optimizing for high similarity of the summary with the input word distribution. Suppose the input unigram distribution is I and the current summary is S, the method chooses the sentence sl = arg minsi KL({S ∪ si }||I) at each iteration. Since {S ∪ si } is used to compute divergence, redundancy is implicitly controlled in this approach. Such a KL objective was used in competitive systems in the past (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Input + background: These systems combine (i) a score based on the background (KLback , TS or SR) with (ii) the score based on the input only (KLinp ). For example, to combine TSsum and KLinp : for each sentence, we compute its scores based on the two methods. Then we normalize the two sets of scores for candidate sentences using zscores and compute the best sentence as arg maxsi (TSsum (si ) - KLinp (si )). Redundancy control is done similarly to the TS only systems. Systems for comparison We compare against three types of systems, (i) those which similarly to surprise, use a background cor"
P14-2055,C00-1072,0,0.136628,"ce list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to learn and use topics discussed only in background, those only in the update input and those that overlap across the two sets. Even for generic summarization, some of the best results were obtained by Conroy et al. (2006) by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by Lin and Hovy (2000). Central to this approach is the use of a likelihood ratio test to compute topic words, words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the input’s topic. In this work, we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach. We briefly explain the topic words based approach below. λ= P (D|H1 ) b(ct , N, p) = (2) P (D|H2 ) b(cI , NI , p1 ) b(cB , NB , p2 ) p, p1 and p2 are estimated by maximum lik"
P14-2055,N03-1020,0,0.122925,"to surprise, use a background corpus to identify important sentences, (ii) a system that uses information from the input set only and no background, and (iii) systems that combine scores from the input and background. KLback : represents a simple baseline for surprise computation from a background corpus. A single unigram probability distribution B is created from the background using maximum likelihood. The summary is created by greedily adding sentences which maximize KL divergence 6 Content selection results For evaluation, we compare each summary to the four manual summaries using ROUGE (Lin and Hovy, 2003; Lin, 2004). All summaries were truncated to 100 words, stemming was performed and 2 http://www-nlpir.nist.gov/projects/ duc/index.html 3 http://www.nist.gov/tac/ 336 KLback TSsum TSavg SRsum SRavg KLinp KLinp +TSsum KLinp +TSavg KLinp +SRsum KLinp +SRavg ROUGE-1 0.2276 (TS, SR) 0.3078 0.2841 (TSsum , SRsum ) 0.3120 0.3003 0.3075 (KLinp +TSavg ) 0.3250 0.3410 0.3187 (KLinp +TSavg ) 0.3220 (KLinp +TSavg ) ROUGE-2 0.0250 (TS, SR) 0.0616 0.0493 (TSsum ) 0.0580 0.0549 0.0684 0.0725 0.0795 0.0660 (KLinp +TSavg ) 0.0696 KLback TSsum TSavg SRsum SRavg KLinp KLinp +TSsum KLinp +TSavg ROUGE-1 0.2246 ("
P14-2055,W04-1013,0,0.0145304,"background corpus to identify important sentences, (ii) a system that uses information from the input set only and no background, and (iii) systems that combine scores from the input and background. KLback : represents a simple baseline for surprise computation from a background corpus. A single unigram probability distribution B is created from the background using maximum likelihood. The summary is created by greedily adding sentences which maximize KL divergence 6 Content selection results For evaluation, we compare each summary to the four manual summaries using ROUGE (Lin and Hovy, 2003; Lin, 2004). All summaries were truncated to 100 words, stemming was performed and 2 http://www-nlpir.nist.gov/projects/ duc/index.html 3 http://www.nist.gov/tac/ 336 KLback TSsum TSavg SRsum SRavg KLinp KLinp +TSsum KLinp +TSavg KLinp +SRsum KLinp +SRavg ROUGE-1 0.2276 (TS, SR) 0.3078 0.2841 (TSsum , SRsum ) 0.3120 0.3003 0.3075 (KLinp +TSavg ) 0.3250 0.3410 0.3187 (KLinp +TSavg ) 0.3220 (KLinp +TSavg ) ROUGE-2 0.0250 (TS, SR) 0.0616 0.0493 (TSsum ) 0.0580 0.0549 0.0684 0.0725 0.0795 0.0660 (KLinp +TSavg ) 0.0696 KLback TSsum TSavg SRsum SRavg KLinp KLinp +TSsum KLinp +TSavg ROUGE-1 0.2246 (TS, SR) 0.30"
P14-2055,H05-1014,0,0.030461,"ection of news articles as the background, and b) UP DATE summarization where the background is a smaller but specific set of news documents on the same topic as the input set. We find that our method performs competitively with a previous log-likelihood ratio approach which identifies words with significantly higher probability in the input compared to the background. The Bayesian approach is more advantageous in the update task, where the background corpus is smaller in size. 2 Related work Computing new information is useful in many applications. The TREC novelty tasks (Allan et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list. For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to"
Q13-1028,P98-1013,0,0.0159989,"alth, crime, ethics, can provoke emotional reactions in readers as shown in the snippet below. Medicine is a constant trade-off, a struggle to cure the disease without killing the patient first. Chemotherapy, for example, involves purposely poisoning someone – but with the expectation that the short-term injury will be outweighed by the eventual benefits. We compute affect-related features using three lexicons. The MPQA (Wilson et al., 2005) and General Inquirer (Stone et al., 1966) give lists of positive and negative sentiment words. The third resource is emotion-related words from FrameNet (Baker et al., 1998). The sizes of these lexicon are 8,221, 5,395, and 653 words respectively. We compute the counts of positive, negative, polar, and emotion words, each normalized by the total number of content words in the article (POS PROP, NEG PROP, PO LAR PROP, EMOT PROP ). We also include the proportion of emotion and polar words taken together (POLAR EMOT PROP) and the ratio between count of positive and negative words (POS BY NEG). The features with higher values in the VERY GOOD class are NEG PROP, POLAR PROP, EMOT POLAR PROP . In TYPICAL articles, POS BY NEG , EMOT PROP have higher values. VERY GOOD ar"
Q13-1028,P11-1091,0,0.0293758,"Missing"
Q13-1028,J08-1001,0,0.0436698,"Missing"
Q13-1028,P00-1037,0,0.054098,"ting is clear and well-organized but the text also contains creative use of language and a clever story-like explanation of the scientific contribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Ling"
Q13-1028,P96-1041,0,0.192558,"Missing"
Q13-1028,W04-3238,0,0.0199859,"Missing"
Q13-1028,W10-4236,0,0.0182046,"Missing"
Q13-1028,de-marneffe-etal-2006-generating,0,0.00631726,"Missing"
Q13-1028,N07-1055,0,0.0209746,"Missing"
Q13-1028,P05-1045,0,0.0107963,"Dr. Remington was born in Reedville, Va., in 1922, to Maud and P. Sheldon Remington, a school headmaster. Charles spent his boyhood chasing butterflies alongside his father, also a collector. During his graduate studies at Harvard, he founded the Lepidopterists’ Society with an equally butterfly-smitten undergraduate, Harry Clench. We approximate this facet by computing the number of explicit references to people, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns). Our third resource contains the number of times different noun phrases (NP) were followed by each of the relative pronouns ‘who’, ‘where’ and ‘which’. These counts for 664,673 noun phrases were collected by Ji and Lin (2009) from the Google Ngram Corpus (Lin et al., 2010). We use a simple heuristic to obtain a list of animate (google animate) and inanimate nouns (google inanimate) from this list. The head of each NP is taken as a candidate noun. If the noun d"
Q13-1028,Y09-1024,0,0.0169442,"omputing the number of explicit references to people, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns). Our third resource contains the number of times different noun phrases (NP) were followed by each of the relative pronouns ‘who’, ‘where’ and ‘which’. These counts for 664,673 noun phrases were collected by Ji and Lin (2009) from the Google Ngram Corpus (Lin et al., 2010). We use a simple heuristic to obtain a list of animate (google animate) and inanimate nouns (google inanimate) from this list. The head of each NP is taken as a candidate noun. If the noun does not occur with ‘who’ in any of the noun phrases where it is the head, then it is inanimate. In contrast, if it appears only with ‘who’ in all noun phrases, it is animate. Otherwise, for each NP where the noun is a head, we check whether the count of times the noun phrase appeared with ‘who’ is greater than each of the occurrences of ‘which’, ‘where’ and ‘"
Q13-1028,P03-1054,0,0.00685121,"n-gram model (AVR CHAR PERP ALL , AVR CHAR PERP 10, 20, 30). In phoneme features, we ignore words that do not have an entry in the CMU dictionary. Word pair measures: Next we attempt to detect unusual combinations of words. We do this calculation only for certain types of syntactic relations–a) nouns and their adjective modifiers, b) verbs with adverb modifiers, c) adjacent nouns in a noun phrase and d) verb and subject pairs. Counts for co-occurrence again come from NYT 1996 articles. The syntactic relations are obtained using the constituency and dependency parses from the Stanford parser (Klein and Manning, 2003; De Marneffe et al., 2006). To avoid the influence of proper names and named entities, we replace them with tags (NNP for proper names and PERSON, ORG, LOC for named entities). We treat the words for which the dependency holds as a (auxiliary word, main word) pair. For adjective-noun and adverb-verb pairs, the auxiliary is the adjective or adverb; for noun-noun pairs, it is the first noun; and for verb-subject pairs, the auxiliary is the subject. Our idea is to compute usualness scores based on frequency with which a particular pair of words appears in the background. Specifically, we compute"
Q13-1028,P03-1069,0,0.0213815,"ntribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Linguistics. zani et al., 1996; Mooney and Roy, 2000); and c) is easy to read for a target readership. Shorter words (Flesch, 1948), les"
Q13-1028,C00-1072,0,0.0372494,"and TYPICAL categories created above allow us to study writing quality without regard to topic. However a typical information retrieval scenario would involve comparison between articles of the same topic, i.e. relevant to the same query. To investigate how quality differentiation can be done within topics, we created another corpus where we paired articles of VERY GOOD and TYPICAL quality. For each article in the VERY GOOD category, we compute similarity with all articles in the TYPICAL set. This similarity is computed by comparing the topic words (computed using a loglikelihood ratio test (Lin and Hovy, 2000)) of the two articles. We retain the most similar 10 TYPICAL articles for each VERY GOOD article. We enumerate all pairs of VERY GOOD with matched up TYPICAL ARTICLES (10 in number) giving a total of 35,300 pairs. There are two distinguishing aspects of our cor343 pus. First, the average quality of articles is high. They are unlikely to have spelling, grammar and basic organization problems allowing us to investigate article quality rather than the detection of errors. Second, our corpus contains more realistic samples of quality differences for IR or article recommendation compared to prior w"
Q13-1028,lin-etal-2010-new,0,0.0172851,"ople, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns). Our third resource contains the number of times different noun phrases (NP) were followed by each of the relative pronouns ‘who’, ‘where’ and ‘which’. These counts for 664,673 noun phrases were collected by Ji and Lin (2009) from the Google Ngram Corpus (Lin et al., 2010). We use a simple heuristic to obtain a list of animate (google animate) and inanimate nouns (google inanimate) from this list. The head of each NP is taken as a candidate noun. If the noun does not occur with ‘who’ in any of the noun phrases where it is the head, then it is inanimate. In contrast, if it appears only with ‘who’ in all noun phrases, it is animate. Otherwise, for each NP where the noun is a head, we check whether the count of times the noun phrase appeared with ‘who’ is greater than each of the occurrences of ‘which’, ‘where’ and ‘when’ (taken individually) with that noun phrase"
Q13-1028,P09-1025,0,0.123395,"Missing"
Q13-1028,J88-2004,0,0.0160571,"TYPICAL class. All these trends indicate that unusual phrases are associated with the VERY GOOD category of articles. 3.4 Sub-genres There are several sub-genres within science writing (Stocking, 2010): short descriptions of discoveries, longer explanatory articles, narratives, stories about scientists, reports on meetings, review articles and blog posts. Naturally, some of these sub-genres will be more appealing to readers. To investigate this aspect, we compute scores for some sub-genres of interest—narrative, attribution and interview. Narrative texts typically have characters and events (Nakhimovsky, 1988), so we look for entities and past tense in the articles. We count the number of sentences where the first verb in surface order is in the past tense. Then among these sentences, we pick those which have either a personal pronoun or a proper noun before the target verb (again in surface order). The proportion of such sentences in the text is taken as the NARRATIVE score. We also developed a measure to identify the degree to which the article’s content is attributed to external sources as opposed to the author’s own statements. Attribution to other sources is frequent in the news domain since m"
Q13-1028,D08-1020,1,0.882749,"Missing"
Q13-1028,P09-2004,1,0.689097,"Missing"
Q13-1028,D09-1026,0,0.019944,"Missing"
Q13-1028,D10-1094,0,0.0170979,"ative use of language and a clever story-like explanation of the scientific contribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Linguistics. zani et al., 1996; Mooney and Roy, 2000); and c) is easy t"
Q13-1028,P05-1065,0,0.0735558,"Missing"
Q13-1028,C08-1109,0,0.0125707,"but the text also contains creative use of language and a clever story-like explanation of the scientific contribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Linguistics. zani et al., 1996; Mooney and"
Q13-1028,H05-1044,0,0.00189106,"ero, the score is set to zero. All three scores are significantly higher for the TYPICAL class. 3.5 Affective content Some articles, for example those detailing research on health, crime, ethics, can provoke emotional reactions in readers as shown in the snippet below. Medicine is a constant trade-off, a struggle to cure the disease without killing the patient first. Chemotherapy, for example, involves purposely poisoning someone – but with the expectation that the short-term injury will be outweighed by the eventual benefits. We compute affect-related features using three lexicons. The MPQA (Wilson et al., 2005) and General Inquirer (Stone et al., 1966) give lists of positive and negative sentiment words. The third resource is emotion-related words from FrameNet (Baker et al., 1998). The sizes of these lexicon are 8,221, 5,395, and 653 words respectively. We compute the counts of positive, negative, polar, and emotion words, each normalized by the total number of content words in the article (POS PROP, NEG PROP, PO LAR PROP, EMOT PROP ). We also include the proportion of emotion and polar words taken together (POLAR EMOT PROP) and the ratio between count of positive and negative words (POS BY NEG). T"
Q13-1028,C98-1013,0,\N,Missing
Q15-1006,J08-1001,1,0.669325,"experts or experienced users in the absence 1 The corpus can be downloaded from http: //www.homepages.inf.ed.ac.uk/alouis/ solutionComplexity.html. 75 Frequency and learn to predict an ordering for new sets of solutions. This setup is related to previous studies on information ordering where the aim is to learn statistical patterns of document structure which can be then used to order new sentences or paragraphs in a coherent manner. Some approaches approximate the structure of a document via topic and entity sequences using local dependencies such as conditional probabilities (Lapata, 2003; Barzilay and Lapata, 2008) or Hidden Markov Models (Barzilay and Lee, 2004). More recently, global approaches which directly model the permutations of topics in the document have been proposed (Chen et al., 2009b). Following this line of work, one of our models uses the Generalized Mallows Model (Fligner and Verducci, 1986) in its generative process which allows to model permutations of complexity levels in the training data. 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Solution set size Figure 1: Histogram of solution set sizes of any interaction with other users or their devices and"
Q15-1006,P06-1049,0,0.0195836,"em on a web interface and assigned a rank to each solution to create an order. No ties were allowed and a complete ordering was required to keep the annotation simple. The annotators were fluent English speakers and had some knowledge of computer hardware and software. We refrained from including novice users in our study as they are likely to have very different personal preferences resulting in more divergent rankings. Results We measured inter-annotator agreement using Kendall’s τ , a metric of rank correlation which has been reliably used in information ordering evaluations (Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007). τ ranges between −1 and +1, where +1 indicates equivalent rankings, −1 completely reverse rankings, and 0 independent rankings. Table 2 shows the pairwise inter-annotator agreement as well as the agreement between each annotator and the original FAQ order. The table shows fair agreement between the annotators confirming that this is a reasonable task for humans to do. As can be seen, there are some individual differences, with the inter-annotator agreement varying from 0.421 (for A,B) to 0.625 (for A,D). The last column in Table 2 reports the agreement 76 between our a"
Q15-1006,P09-1010,0,0.0288407,"., whether users express themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approxi"
Q15-1006,P11-1028,0,0.0135884,"gh our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approximately ordered from least to most complex) 3 Problem formulation Our aim in this work is to learn models wh"
Q15-1006,N09-1042,0,0.117762,"for new sets of solutions. This setup is related to previous studies on information ordering where the aim is to learn statistical patterns of document structure which can be then used to order new sentences or paragraphs in a coherent manner. Some approaches approximate the structure of a document via topic and entity sequences using local dependencies such as conditional probabilities (Lapata, 2003; Barzilay and Lapata, 2008) or Hidden Markov Models (Barzilay and Lee, 2004). More recently, global approaches which directly model the permutations of topics in the document have been proposed (Chen et al., 2009b). Following this line of work, one of our models uses the Generalized Mallows Model (Fligner and Verducci, 1986) in its generative process which allows to model permutations of complexity levels in the training data. 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Solution set size Figure 1: Histogram of solution set sizes of any interaction with other users or their devices and thus constitute a generic list of steps to try out. We assume that in such a situation, the solution providers are likely to suggest simpler solutions before other complex ones, leadin"
Q15-1006,D09-1100,0,0.012473,"s themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approximately ordered from least"
Q15-1006,N07-1055,0,0.0182355,"procedure. We opted for a discriminative ranking approach instead which uses the generative models to compute a rich set of features. This choice allows us to simultaneously obtain features tapping on to different aspects learned by the models and to use well-defined objective functions. Below, we briefly describe the features based on our generative models. We also present additional features used to create baselines for system comparison. Likelihood We created a Hidden Markov Model based on the sample from the posterior of our models (for a similar HMM approximation of a Bayesian model see Elsner et al. (2007)). For our model, the HMM has L states, and each state sm corresponds to a complexity level dm . We used the complexity language models φm estimated from the posterior as the emission probability distribution for the corresponding states. The transition probabilities of the HMM were computed based on the complexity level assignments for the training solution sequences in our posterior sample. The probability of transitioning to state sj from state si , p(sj |si ), is the conc(d ,d ) ditional probability p(dj |di ) computed as c(di i )j , where c(di , dj ) is the number of times the complexity"
Q15-1006,D10-1084,0,0.0292318,"a particular repair action. Our notion of complexity is conceptually similar to the cost of an action, however we learn to predict complexity levels rather than calibrate them manually. Also note that our troubleshooting task is not device specific. Our models learn from troubleshootingoriented data without any restrictions on the problems being solved. Previous work on web-based user support has mostly focused on thread analysis. The idea is to model the content structure of forum threads by analyzing the requests for information and suggested solutions in the thread data (Wang et al., 2011; Kim et al., 2010). Examples of such analysis include identifying which earlier post(s) a given post responds to and in what manner (e.g., is it a question, an answer or a confirmation). Other related work (Lui and Baldwin, 2009) identifies user characteristics in such data, i.e., whether users express themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some rela"
Q15-1006,P03-1069,1,0.673814,"ed by computer experts or experienced users in the absence 1 The corpus can be downloaded from http: //www.homepages.inf.ed.ac.uk/alouis/ solutionComplexity.html. 75 Frequency and learn to predict an ordering for new sets of solutions. This setup is related to previous studies on information ordering where the aim is to learn statistical patterns of document structure which can be then used to order new sentences or paragraphs in a coherent manner. Some approaches approximate the structure of a document via topic and entity sequences using local dependencies such as conditional probabilities (Lapata, 2003; Barzilay and Lapata, 2008) or Hidden Markov Models (Barzilay and Lee, 2004). More recently, global approaches which directly model the permutations of topics in the document have been proposed (Chen et al., 2009b). Following this line of work, one of our models uses the Generalized Mallows Model (Fligner and Verducci, 1986) in its generative process which allows to model permutations of complexity levels in the training data. 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Solution set size Figure 1: Histogram of solution set sizes of any interaction with othe"
Q15-1006,W07-2312,0,0.044981,"Missing"
Q15-1006,P14-5010,0,0.00308879,"t of detail from complex ones). We devised three features based on the number of sentences (within a solution), words, and average sentence length. Syntax/Semantics Another related class of features estimates solution complexity based on sentence structure and meaning. We obtained eight syntactic features based on the number of nouns, verbs, adjectives and adverbs, prepositions, pronouns, wh-adverbs, modals, and punctuation. Other features compute the average and maximum depth of constituent parse trees. The part-of-speech tags and parse trees were obtained using the Stanford CoreNLP toolkit (Manning et al., 2014). In addition, we computed 10 semantic features using WordNet (Miller, 1995). They are the average number of senses for each category (noun, verb, adjective/adverb), and the maximum number of senses for the same three classes. We also include the average and maximum lengths of the path to the root of the hypernym tree for nouns and verbs. This class of features roughly approximates the indicators typically used in predicting text readability (Schwarm and Ostendorf, 2005; McNamara et al., 2014). 5.2 Experimental Setup We performed 10-fold cross-validation. We trained the ranking model on 240 pr"
Q15-1006,P05-1065,0,0.0273753,"maximum depth of constituent parse trees. The part-of-speech tags and parse trees were obtained using the Stanford CoreNLP toolkit (Manning et al., 2014). In addition, we computed 10 semantic features using WordNet (Miller, 1995). They are the average number of senses for each category (noun, verb, adjective/adverb), and the maximum number of senses for the same three classes. We also include the average and maximum lengths of the path to the root of the hypernym tree for nouns and verbs. This class of features roughly approximates the indicators typically used in predicting text readability (Schwarm and Ostendorf, 2005; McNamara et al., 2014). 5.2 Experimental Setup We performed 10-fold cross-validation. We trained the ranking model on 240 problem-solution sets; 30 sets were reserved for development and 30 for 82 testing (in each fold). The most frequent 20 words in each training set were filtered as stopwords. The development data was used to tune the parameters and hyperparameters of the models and the number of complexity levels. We experimented with ranges [5– 20] and found that the best number of levels was 10 for the position model and 20 for the permutationbased model, respectively. For the expected"
Q15-1006,P10-1083,0,0.0150187,"technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approximately ordered from least to most complex) 3 Problem formulation"
Q15-1006,D11-1002,0,0.0220793,"ed by carrying out a particular repair action. Our notion of complexity is conceptually similar to the cost of an action, however we learn to predict complexity levels rather than calibrate them manually. Also note that our troubleshooting task is not device specific. Our models learn from troubleshootingoriented data without any restrictions on the problems being solved. Previous work on web-based user support has mostly focused on thread analysis. The idea is to model the content structure of forum threads by analyzing the requests for information and suggested solutions in the thread data (Wang et al., 2011; Kim et al., 2010). Examples of such analysis include identifying which earlier post(s) a given post responds to and in what manner (e.g., is it a question, an answer or a confirmation). Other related work (Lui and Baldwin, 2009) identifies user characteristics in such data, i.e., whether users express themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our w"
Q15-1006,N04-1015,0,\N,Missing
Q15-1006,J06-4002,1,\N,Missing
Q15-1006,U10-1009,0,\N,Missing
W10-1013,P98-2127,0,0.0184017,"he chosen sense of the prompt word are added as expansions. Distributionally similar words: We also consider as expansions words that appear in similar contexts as the prompt words. For example, “cordial”, “polite”, “cheerful”, “hostile”, “calm”, “lively” and “affable” often appear in the same contexts as the word “friendly”. Such related words form part of a concept like ‘behavioral characteristics of people’ and are likely to appear in a discussion of any one aspect. These expansions could comprise antonyms and other related words too. This idea of word similarity was implemented in work by Lin (1998). Similarity between two words is estimated by examining the degree of overlap of their contexts in a large corpus. We access Lin’s similarity estimates using a tool from Leacock and Chodorow (2003) that returns words with similarity values above a cutoff. Word association norms: Word associations have been of great interest in psycholinguistic research. Participants are given a target word and asked to mention words that readily come to mind. The most frequent among these are recorded as free associations for that target. They form another interesting category of expansions for our purpose be"
W10-1013,C98-2122,0,\N,Missing
W10-4310,N06-2015,0,0.01815,"ations. We use the part of speech (POS) tag associated with the head of the noun phrase to assign one of the following categories: pronoun, nominal, name or expletive. When the head does not belong to the above classes, we simply record its POS tag. We also mark whether the noun phrase is a definite description using the presence of the article ‘the’. Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S sales to remain steady at about 1,200 cars in 1990. The luxury auto maker last year sold 1,214 cars in the U.S. We use the coreference annotations from the Ontonotes corpus (version 2.9) (Hovy et al., 2006) to compute our gold-standard entity features. The WSJ portion of this corpus contains 590 articles. Here, nominalizations and temporal expressions are also annotated for coreference but we use the links between noun phrases only. We expect these features computed on the gold-standard annotations to represent an upper bound on the performance of entity features. Finally, the Penn Treebank corpus (Marcus et al., 1994) is used to obtain gold-standard parse and grammatical role information. Only adjacent sentences within the same paragraph are used in our experiments. 3 Modification. We expected"
W10-4310,D09-1036,0,0.373089,"discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
W10-4310,P09-1077,1,0.332049,"expressions. We aim to investigate the association between discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meetin"
W10-4310,prasad-etal-2008-penn,1,0.192545,"ract The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Prasad et al., 2008), that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity. Entity-related features would be expected to better separate out such relations. We present the first comprehensive study of the connection between entity features and discourse relations. We show that there are notable differences in properties of referring expressions across the different relations. Sense prediction can be done with results better than random baseline using only entity realization information. Their performance, however, is lower than a know"
W10-4310,miltsakaki-etal-2004-penn,1,\N,Missing
W10-4310,N07-1054,0,\N,Missing
W10-4310,J93-2004,0,\N,Missing
W10-4310,P02-1047,0,\N,Missing
W10-4327,C94-1056,0,0.0305994,"xical overlap provides a simple and cheap alternative to discourse for computing text structure with comparable performance for the task of content selection. 1 Introduction Discourse relations such as cause, contrast or elaboration are considered critical for text interpretation, as they signal in what way parts of a text relate to each other to form a coherent whole. For this reason, the discourse structure of a text can be seen as an intermediate representation, over which an automatic summarizer can perform computations in order to identify important spans of text to include in a summary (Ono et al., 1994; Marcu, 1998; Wolf and Gibson, 2004). In our work, we study the content selection performance of different types of discourse-based features. Discourse relations interconnect units of a text and discourse formalisms have proposed different Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147–156, c The University of Tokyo, September 24-25, 2010. 2010 Association for Computational Linguistics 147 and compare discourse-based selection to simpler non-discourse methods. 2 ple parents appear frequently in texts and do not allow a t"
W10-4327,W01-1605,0,0.196426,"a subset of the overlapping documents for which we also have human summaries available. 2.1 lated expectation, Elaboration, Example, Generalization, Attribution, Temporal sequence, Similarity, Contrast and Same. The edge between two nodes representing a relation is directed in the case of asymmetric relations such as Cause and Condition and undirected for symmetric relations like Similarity and Contrast. RST corpus RST (Mann and Thompson, 1988) proposes that coherent text can be represented as a tree formed by the combination of text units via discourse relations. The RST corpus developed by Carlson et al. (2001) contains discourse tree annotations for 385 WSJ articles from the Penn Treebank corpus. The smallest annotation units in the RST corpus are sub-sentential clauses, also called elementary discourse units (EDUs). Adjacent EDUs combine through rhetorical relations into larger spans such as sentences. The larger units recursively participate in relations with others, yielding one hierarchical tree structure covering the entire text. The discourse units participating in a RST relation are assigned either nucleus or satellite status; a nucleus is considered to be more central, or important, in the"
W10-4327,W02-0404,0,0.0124147,"Missing"
W10-4327,prasad-etal-2008-penn,1,0.917385,"002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a detailed analysis of the predictive power of different types of discourse features for content selection We present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We find that structure information is the most robust indicator of importance. Semantic sense only provides constraints on content select"
W10-4327,N03-1030,0,0.854144,"Missing"
W10-4327,C00-1072,0,0.0904035,"Missing"
W10-4327,W02-0406,0,0.0101342,"rcu, 1998; Wolf and Gibson, 2004; Uzda et al., 2008), little is known about which aspects of discourse are actually correlated with content selection power. In our work, we separate out structural and semantic features and examine their usefulness. We also investigate whether simpler intermediate representations can be used in lieu of discourse. More parsimonious, easy to compute representations of text have been proposed for summarization. For example, a text can be reduced to a set of highly descriptive topical words, the presence of which is used to signal importance for content selection (Lin and Hovy, 2002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Pra"
W10-4327,N03-1020,0,0.0285085,"Missing"
W10-4327,W04-1013,0,0.0416163,"Missing"
W10-4327,W06-1317,0,0.092053,"Missing"
W10-4327,W04-1004,0,0.271132,"for the full text, i.e. tree (Mann and Thompson, 1988) and graph (Wolf and Gibson, 2005). This structure is one source of information from discourse which can be used to compute the importance of text units. The semantics of the discourse relations between sentences could be another indicator of content importance. For example, text units connected by “cause” and “contrast” relationships might be more important content for summaries compared to those conveying “elaboration”. While previous work have focused on developing content selection methods based upon individual frameworks (Marcu, 1998; Wolf and Gibson, 2004; Uzda et al., 2008), little is known about which aspects of discourse are actually correlated with content selection power. In our work, we separate out structural and semantic features and examine their usefulness. We also investigate whether simpler intermediate representations can be used in lieu of discourse. More parsimonious, easy to compute representations of text have been proposed for summarization. For example, a text can be reduced to a set of highly descriptive topical words, the presence of which is used to signal importance for content selection (Lin and Hovy, 2002; Conroy et al"
W10-4327,J05-2005,0,0.349654,"to signal importance for content selection (Lin and Hovy, 2002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a detailed analysis of the predictive power of different types of discourse features for content selection We present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We find that structure information is the most robust indicator of importance."
W10-4327,J00-3005,0,0.583013,"Missing"
W10-4327,J93-2004,0,\N,Missing
W10-4327,P06-2020,0,\N,Missing
W10-4327,P09-1077,1,\N,Missing
W10-4327,P04-1049,0,\N,Missing
W10-4327,I05-2004,0,\N,Missing
W11-1605,J05-3002,0,0.0340575,"more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics recently, Marsi et al. (2010) manually annotated the"
W11-1605,D08-1019,0,0.0147024,"nd Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics recently, Marsi et al. (2010) manually annotated the transformations between sour"
W11-1605,N07-1023,0,0.087651,"nt gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational"
W11-1605,N09-1041,0,0.178042,"techniques. General sentences are overview statements. Specific sentences supply details. An example general and specific sentence from different parts of a news article are shown in Table 1. Prior studies have advocated that the distinction between general and specific content is relevant for text summarization. Jing and McKeown (2000) studied what edits people use to create summaries from sentences in the source text. Two of the operations they identify are generalization and specification where the source content gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube,"
W11-1605,A00-2024,0,0.0590099,"summary. In this work, we argue that the general and specific nature of the content is also taken into account by human summarizers; we show that this distinction is directly related to the quality of the summary and it also calls for the use and refinement of text-to-text generation techniques. General sentences are overview statements. Specific sentences supply details. An example general and specific sentence from different parts of a news article are shown in Table 1. Prior studies have advocated that the distinction between general and specific content is relevant for text summarization. Jing and McKeown (2000) studied what edits people use to create summaries from sentences in the source text. Two of the operations they identify are generalization and specification where the source content gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two type"
W11-1605,N03-1020,0,0.139942,"of the importance of the content present. So we performed an analysis to check the contribution of generality to the content scores in addition to the importance factor. We combine a measure of content importance Predictor (Intercept) rouge2 avgspec Mean β 0.212 1.299 -0.166 Stdev. β 0.03 0.11 0.04 t value 6.87 11.74 -4.21 p-value 2.3e-11 * &lt; 2e-16 * 3.1e-05 * sums. 202 400 79 avg specificity 0.71 0.72 0.77 Table 4: Number of summaries at extreme levels of linguistic quality scores and their average specificity values Table 3: Results from regression test from the ROUGE automatic evaluation (Lin and Hovy, 2003; Lin, 2004) with generality to predict the coverage scores. We use the same reference as used for the official coverage score evaluation and compute ROUGE-2 which is the recall of bigrams of the human summary by the system summary. Next we train a regression model on our data using the ROUGE-2 score and specificity as predictors of the coverage score. We then inspected the weights learnt in the regression model to identify the influence of the predictors. Table 3 shows the mean values and standard deviation of the beta coefficients. We also report the results from a test to determine if the b"
W11-1605,W04-1013,0,0.012092,"f the content present. So we performed an analysis to check the contribution of generality to the content scores in addition to the importance factor. We combine a measure of content importance Predictor (Intercept) rouge2 avgspec Mean β 0.212 1.299 -0.166 Stdev. β 0.03 0.11 0.04 t value 6.87 11.74 -4.21 p-value 2.3e-11 * &lt; 2e-16 * 3.1e-05 * sums. 202 400 79 avg specificity 0.71 0.72 0.77 Table 4: Number of summaries at extreme levels of linguistic quality scores and their average specificity values Table 3: Results from regression test from the ROUGE automatic evaluation (Lin and Hovy, 2003; Lin, 2004) with generality to predict the coverage scores. We use the same reference as used for the official coverage score evaluation and compute ROUGE-2 which is the recall of bigrams of the human summary by the system summary. Next we train a regression model on our data using the ROUGE-2 score and specificity as predictors of the coverage score. We then inspected the weights learnt in the regression model to identify the influence of the predictors. Table 3 shows the mean values and standard deviation of the beta coefficients. We also report the results from a test to determine if the beta coeffici"
W11-1605,E06-1038,0,0.0699902,"the source content gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Ass"
W11-1605,D08-1057,0,0.0135117,"l as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics recently, Marsi et al. (2010) manually annotated the transformations between source and compressed phrases and observe that generalization is a frequent transformation. But it is not know"
W11-1605,W02-0406,0,\N,Missing
W15-2703,A00-3008,0,0.751922,"em, we have embarked on a large crowd-sourcing experiment, the first phase of which is described in Sections 3– 5. Section 6 discusses our results to date, with further phases described in Section 7. 2 Background This is not the first work to call attention to multiple co-occurring connectives. Webber and colleagues (1999) used them to argue that discourse spans could be related by both adjacency relations and anaphoric relations. Similary, in the context of Catalan and Spanish oral narrative, Cuenca and Marin (2009) used them to argue for different patterns and degrees of discourse cohesion. Oates (2000) considers how multiple discourse connectives should be used in Natural Language Generation, noting that the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996). Fraser (2013) considers the order in which multiple CONTRASTIVE connectives co-occur, describing their patterning in terms of general contrastive discourse markers and specific contrastive discourse markers. For Turkish, Zeyrek (2014) describes patterns of multiple co-occuring connectives that signal CON TRASTIVE and/or CONCESSIVE relations. These efforts have all been directed a"
W15-2703,Q14-1025,0,0.0266132,"sage presentation Figure 4: Screen shot of a participant being asked to indicate whether or not their choice of a conjunction that fits with respect to its sense — in this case, “but” — sounds natural 26 with other participants, as well as 3 trials in which a participant selected the response before, which was intended for use in only the catch trials. The resulting dataset of responses from 58 participants comprises 2665 judgments over the 46 target passages (ignoring the four catch trials). The results reported below are raw counts, and do not yet take account of potential participant bias (Passonneau and Carpenter, 2014). Considering the dataset as a whole, we can ask how often a participant’s response matched the author’s original choice. (Note that this can only be assessed on explicit passages – that is, ones where the author expicitly used a pair of co-occurring connectives, cf. Section 3). Table 3 shows the pattern of participant responses for passages for which the authors themselves had included an explicit conjunction before the adverbial. Recall that participants always saw a gap before the discourse adverbial, regardless of the author’s original choice to use or not use a conjunction, meaning the ex"
W15-2703,prasad-etal-2008-penn,1,0.78783,"rhetorical relations (Mann and Thompson, 1988). Such relations between what we will call here discourse spans can be signaled explicitly via discourse connectives or specific lexico-syntactic contructions, or conveyed implicitly, via inference on the part of a comprehender. But when does the latter happen? Previously, it was assumed that relations are conveyed implicitly when they are not signalled explicitly. But consider Ex. 1a-b, each with two explicit connectives conveying distinct relations: 1 The sense labels used here (in small caps) are short forms of the labels used in the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014). (1) a. Let’s eat dinner now because otherwise we’ll miss the film. 22 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 22–31, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. gap or choose None.2 In half the passages (10 per adverbial), the author had used one of these conjunctions before the adverbial (which Jiang then removed), and in the other half (including Ex. 3– 4), the author had used no conjunction before the adverbial. The only criteria used in selecting t"
W15-2703,J14-4007,1,\N,Missing
W16-1707,J08-4004,0,0.373054,"Missing"
W16-1707,P87-1023,0,0.758335,"ng annotators what additional relation they infer (besides that associated with instead itself), one still needs to ask: • For clauses starting with discourse adverbials other than instead, is the relation signalled by the adverbial all there is, or can an additional relation be inferred with the previous text? In the former case, no additional annotation is required; in the latter, it is. Existing work highlights the importance of understanding discourse relations in context, showing a range of phenomena that are sensitive to the semantic connection that holds between two spans of discourse (Hirschberg and Litman, 1987; Kehler and Rohde, 2013). Such connections can be made explicit in text via an overt connective or marked syntax; otherwise they must be inferred. Various contextual cues have been identified that guide the 49 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 49–58, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics • If another relation can be inferred, can it be inferred deterministically based on the adverbial alone? If so, no additional work is required, as the relation can be annotated automatically. • If it can’t be inferred based on t"
W16-1707,N13-1132,0,0.0250313,"the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a bo"
W16-1707,N13-1000,0,0.223269,"Missing"
W16-1707,P12-3029,0,0.0207026,"adverbial was replaced with a slash. Each passage contained one of the following discourse adverbials after the gap: actually, after all, first of all, for example, for instance, however, in fact, in general, in other words, indeed, instead, nevertheless, nonetheless, on the one hand, on the other hand, otherwise, specifically, then, therefore, and thus. These represent a sampling of high-frequency adverbials, which belong to a variety of semantic classes and which showed a range of conjunction co-occurrence patterns in counts extracted from the Google Books Ngram Corpus (Michel et al., 2011; Lin et al., 2012). Half the target passages originally contained a conjunction before the adverbial. For those explicit passages, we excised the conjunction and replaced it with a gap. For excerpts that were originally implicit passages, we simply inserted a gap before the adverbial. For each of the 20 adverbials, participants saw 25 explicit passages and 25 implicit passages, with the exception of however, which appeared in 25 implicit passages and 1 explicit passage (due to the rarity of conjunctions that naturally occur directly before however). The distribution of original (author-chosen) conjunctions in t"
W16-1707,A00-3008,0,0.104179,"r and annotate its relation, if any, to the previous sentence. This reflected the common assumption, noted earlier, that the situation is “either/or” – if a discourse relation is marked, there is nothing to infer. With respect to research on explicit multiple cooccurring connectives, over 15 years ago, Webber et al. (1999) used them to argue that discourse spans could be related by both adjacency relations and anaphoric relations. Similary, in the context of Catalan and Spanish oral narrative, Cuenca and Marín (2009) used them to argue for different patterns and degrees of discourse cohesion. Oates (2000) considered how multiple discourse connectives should be used in Natural Language Generation, noting that the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for probl"
W16-1707,Q14-1025,0,0.10735,"they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a boxcar has been attached to a train"
W16-1707,W05-0311,0,0.587643,"number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a boxcar has been attached to a train engine. The next sentence specified what should then be done. Over half their participants interpreted the pronoun it in this next 50 sentence as referring to the boxcar, while others interpreted it to refer to the engine. But the situation associated with these two different interpretations was the same in both cases, since the engine"
W16-1707,miltsakaki-etal-2004-penn,1,0.830866,"Missing"
W16-1707,W15-2703,1,0.830948,"lds between two spans of discourse. It may not be simple to identify or infer that relation, but once achieved, the task is taken to be done. But properties of the discourse adverbial instead (Webber, 2013) have challenged this assumption. In particular, sentence-initial instead supports the inference of another discourse relation, with the specific relation depending on properties of the spans. This can be seen through what coordinating conjunction makes the relation explicit—compare: The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isn’t exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to a larger set of 20 discourse adverbials by eliciting ≈28K conjunction completions via crowdsourcing. Our data replicate and extend Rohde et al.’s findings that discourse adverbials do indeed license inferred conjunctions. Further, the diverse patterns observed for the adverbials include cases in which more than one valid connection can be inferred, each one endorsed by a substantial number of participants; suc"
W16-1707,W13-0124,1,0.96323,"er}@ed.ac.uk, {anna.y.dickinson, chrisclark272}@gmail.com, nschneid@inf.ed.ac.uk, aplouis@essex.ac.uk Abstract establishment of discourse relations (Hirschberg and Litman, 1987; Kehler, 2002; Webber, 2013). When it comes to producing resources annotated with discourse relations—e.g., the Penn Discourse Treebank (PDTB; Prasad et al., 2008)—it is commonly assumed that at most a single discourse relation holds between two spans of discourse. It may not be simple to identify or infer that relation, but once achieved, the task is taken to be done. But properties of the discourse adverbial instead (Webber, 2013) have challenged this assumption. In particular, sentence-initial instead supports the inference of another discourse relation, with the specific relation depending on properties of the spans. This can be seen through what coordinating conjunction makes the relation explicit—compare: The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isn’t exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to"
W16-1707,prasad-etal-2008-penn,1,\N,Missing
W17-0906,W17-0908,0,0.124042,"Missing"
W17-0906,W17-0909,0,0.387095,"d. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as wo"
W17-0906,W17-0912,0,0.164607,"ls can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the"
W17-0906,H05-1044,0,0.0509919,"Missing"
W17-0906,H89-1033,0,0.679051,"Missing"
W17-0906,N16-1098,1,0.310191,"esident Marco was excited to be a registered voter. He thought long and hard about who to vote for. Finally he had decided on his favorite candidate. He placed his vote for that candidate. Marco was proud that he had finally voted. Spaghetti Sauce Tina made spaghetti for her boyfriend. It took a lot of work, but she was very proud. Her boyfriend ate the whole plate and said it was good. Tina tried it herself, and realized it was disgusting. She was touched that he pretended it was good to spare her feelings. Table 2: Example ROCStories instances from the Winter 2017 release. 3 As described in Mostafazadeh et al. (2016), the SCT cases are collected through Amazon Mechanical Turk (Mturk) on the basis of the ROCStories corpus, a collection of five-sentence everyday life stories which are full of stereotypical sequence of events. To construct SCT cases, they randomly sampled complete five-sentence stories from the ROCStories corpus and presented only the first four sentences of each story to the Mturk workers. Then, for each story, a worker was asked to write a ‘right ending’ and a ‘wrong ending’. This resulting set was further filtered by human verification: they compile each SCT case into two independent five"
W17-0906,W17-0911,0,0.0613484,"2017). Linguistic features include aspects of sentiment, negation, pronominalization and n-gram overlap between the story and possible endings. 98,159 1,871 1,871 Table 3: The size of the provided shared task datasets. uate the systems in terms of accuracy, which we #correct measure as #test Any other details recases . garding our shared task can be accessed via our shared task page http://cs.rochester. edu/nlp/rocstories/LSDSem17/. 4 roemmele (University of Southern California). Binary classifier based on a recurrent neural network that operates over (sentence-level) Skipthought embeddings (Roemmele et al., 2017). For training, different data augmentation methods are explored. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they l"
W17-0906,W17-0910,0,0.073945,"Missing"
W17-0906,W17-0907,0,0.273047,"exically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the same vector space. This model is called Deep Structured Semantic Model (DSSM) (Huang et al., 2013) and had outperformed all the other baselines reported in Mostafazadeh et al. (2016). cogcomp"
W17-6814,W15-2703,1,0.745006,"erent discourse connectives are used to realize particular types of coherence relations remains unresolved. While some connectives show nearly one-to-one mappings with individual coherence relations, other connectives permit much more flexible usage across contexts. One early enterprise targeting the above question was Alistair Knott’s systematic assessment of the conditions that permit one connective to substitute for another (Knott, 1996). Substitutability, along with categories of coherence relations, then predicts the behavior of individual connectives. Another such enterprise is our own (Rohde et al., 2015, 2016, 2017) on implicit connectives in the context of explicit discourse adverbials. Using naturally occurring passages, we have gathered judgments from multiple participants as to what connective, if any, they could insert into a particular passage immediately before an existing discourse adverbial, to make explicit the author’s intended message. For example, when shown the passage It’s too far to walk. Instead let’s take the bus., a participant might insert so to express what she takes to be the intended causal reading. Our findings show variation across participant responses. Such diverge"
W17-6814,W16-1707,1,0.859986,"Missing"
