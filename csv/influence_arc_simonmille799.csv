2008.eamt-1.18,W03-2008,0,0.212445,"focal position detecting circuit. A sentence of this length and complexity is difficult to process even for native speakers of English, let alone for foreigners who do not master English well. Given that professionals have to sift through the claims of a large number of patents returned as response to a search in a patent DB (which makes a quick assessment of the relevance of patent essential), it is not surprising that multilingual summarization of patent claims is very attractive to practitioners in the field. Nonetheless, only little work has been done so far in the area; cf. as an example [1], who proposes a reading aid based on the segmentation of claims into smaller and simpler sentences. The focus has been on the machine translation – especially in the light of the recently dramatically increased prominence of patents in languages not widely spoken in the West (e.g., Korean and Chinese). As far as summarization of patent material is concerned, up to date, the overwhelming share of it is manual.1 One explication for this unsatisfactory state of affairs is that the peculiarities of the genre of patent claims require new approaches to summarization: the application of surface leve"
2008.eamt-1.18,W97-0713,0,0.0775597,"orithms are not able to cope with a reasonable outcome with sentences of such a length, a prior two-step simplification procedure of the original is needed: (i) segmentation into simpler chunks and (ii) repair of chunks which are not grammatical clauses by introducing missing constituents or referential links, or by modifying available constituents. The output of the simplification can serve for two extraction based summarization strategies: (a) discourse structure oriented summarization; (b) syntactic structure oriented summarization. Discourse structure oriented summarization as proposed by [3] uses the depth of the subtree “controlled” by an element of a discourse relation in the sense of the Rhetorical Structure Theory [4] – under the assumption that the nucleus of a relation controls an elementary tree formed by the nucleus and satellite of a relation. See [5] for the application of this strategy to the summarization of patent claims. The syntactic structure based summarization often uses syntactic dependency criteria which indicate the importance of syntactic tree branches, drawing on dependency relations [6,7]. To the best of our knowledge, the syntax oriented strategy has not"
2008.eamt-1.18,W03-2007,0,0.353926,"ng constituents or referential links, or by modifying available constituents. The output of the simplification can serve for two extraction based summarization strategies: (a) discourse structure oriented summarization; (b) syntactic structure oriented summarization. Discourse structure oriented summarization as proposed by [3] uses the depth of the subtree “controlled” by an element of a discourse relation in the sense of the Rhetorical Structure Theory [4] – under the assumption that the nucleus of a relation controls an elementary tree formed by the nucleus and satellite of a relation. See [5] for the application of this strategy to the summarization of patent claims. The syntactic structure based summarization often uses syntactic dependency criteria which indicate the importance of syntactic tree branches, drawing on dependency relations [6,7]. To the best of our knowledge, the syntax oriented strategy has not been applied so far to patents. In PATExpert, three different summarization strategies are implemented: (i) a strategy based on the claim structure, (ii) a strategy based on the discourse structure, and (iii) a strategy based on the deep-syntactic (or shallow semantic) stru"
2008.eamt-1.18,W00-1436,1,0.744236,"marization is most suitable for multilingual summarization. It presupposes two preprocessing stages: (a) claim 122 12th EAMT conference, 22-23 September 2008, Hamburg, Germany dependency structure determination, simplification, and discourse analysis, and (b) full parsing of the simplified claim sentences. For parsing, we use MINIPAR [8]. Despite some shortcomings such as systematic right-attachment, we chose MINIPAR since it produces syntactic structures which roughly correspond to the Surface Syntactic Structures (SSyntSs) of the linguistic framework underlying the linguistic workbench MATE [9] we use for generation: the Meaning-Text Theory, MTT [10]. The summarization and multilingual transfer stages are performed on the DeepSyntactic Structures (DSyntSs) of the MTT, such that prior to these stages, the MINIPAR structures are mapped onto SSyntSs and the SSyntSs onto DSyntSs; for details on the preprocessing stages, see [11]. The abstract nature of the DSyntS, which eliminates the surface-syntactic idiosyncrasies of the linguistic constructions, ensures, on the one hand, quasi-semantic criteria for summarization, and, on the other hand, simplified transfer between the structures of"
2008.eamt-1.18,mille-wanner-2008-making,1,0.911241,"[8]. Despite some shortcomings such as systematic right-attachment, we chose MINIPAR since it produces syntactic structures which roughly correspond to the Surface Syntactic Structures (SSyntSs) of the linguistic framework underlying the linguistic workbench MATE [9] we use for generation: the Meaning-Text Theory, MTT [10]. The summarization and multilingual transfer stages are performed on the DeepSyntactic Structures (DSyntSs) of the MTT, such that prior to these stages, the MINIPAR structures are mapped onto SSyntSs and the SSyntSs onto DSyntSs; for details on the preprocessing stages, see [11]. The abstract nature of the DSyntS, which eliminates the surface-syntactic idiosyncrasies of the linguistic constructions, ensures, on the one hand, quasi-semantic criteria for summarization, and, on the other hand, simplified transfer between the structures of different languages; cf., e.g., [12]. 3 Multilingual Summarization of Patent Claims Starting from the DSyntSs of the simplified claims, the multilingual summarization of patents consists of the following steps: (1) summarization of the original claims, (2) transfer of DSyntS of the source language to the target language, (3) generation"
2008.eamt-1.18,W04-1013,0,0.00474356,"Missing"
2008.eamt-1.18,J85-2001,0,0.247219,"-23 September 2008, Hamburg, Germany tongue produced by PATExpert with summarization switched off (such that only simplification, transfer and regeneration were effective) against the online-Google translation of the original claims as baseline.5 Given that the recall of our multilingual generator is still very much hampered by the shortage of multilingual resources, we consider this evaluation a general indication of the potential of “deep” translation techniques when combined with the preprocessing of the claims. The evaluation was based on a questionnaire which has been largely inspired by [14]. It consists of three categories: “intelligibility”, “simplicity” and “accuracy”. The first two deal with the quality of the transferred text; both have a five value scale. The third category, which has a seven value scale, captures how the content from the English input is conveyed in the transferred text. Due to the lack of space, we do not cite here the questionnaire itself. Table 1 shows the accuracy regarding each of the three quality categories for PATExpert and the baseline. Table 1. Accuracy of the PATExpert Multilingual Summarizer against a baseline Intelligibility Simplicity Accurac"
2008.eamt-1.18,lenci-etal-2002-multilingual,0,0.0229069,"one hand, to perform the summarization at a rather abstract level and thus to use “deep” summarization criteria, and, on the other hand, to reduce the transfer to a large extent to lexical transfer. The results are encouraging. Still, the three central components involved in the process: summarization, transfer and generation, are continuously being extended and improved, such that in the full paper, we will be able to present evaluation figures that are likely to be considerably superior to those presented above. There are some related works. The most similar ones are the MUSI-summarizer by [15] and the summarizer within VERBMOBIL described in [16]. As our strategy, 5 Since our goal was to evaluate the multilingual output of our system with the original claims as input, we consider it correct to run the Google translator on the original claims. 128 12th EAMT conference, 22-23 September 2008, Hamburg, Germany MUSI implies a deep analysis stage and a regeneration stage. However, MUSI’s summarization strategy consists in sentence extraction using surface-oriented criteria (cue phases and positions of sentences). The analysis is applied to the extracted sentences and the resulting syntac"
2008.eamt-1.18,W00-1420,0,0.0305217,"stract level and thus to use “deep” summarization criteria, and, on the other hand, to reduce the transfer to a large extent to lexical transfer. The results are encouraging. Still, the three central components involved in the process: summarization, transfer and generation, are continuously being extended and improved, such that in the full paper, we will be able to present evaluation figures that are likely to be considerably superior to those presented above. There are some related works. The most similar ones are the MUSI-summarizer by [15] and the summarizer within VERBMOBIL described in [16]. As our strategy, 5 Since our goal was to evaluate the multilingual output of our system with the original claims as input, we consider it correct to run the Google translator on the original claims. 128 12th EAMT conference, 22-23 September 2008, Hamburg, Germany MUSI implies a deep analysis stage and a regeneration stage. However, MUSI’s summarization strategy consists in sentence extraction using surface-oriented criteria (cue phases and positions of sentences). The analysis is applied to the extracted sentences and the resulting syntactic structures are mapped onto conceptual representati"
2020.inlg-1.23,W18-6537,0,0.0302491,"experimental setup has a substantial impact on the reliability of human quality judgements (Novikova et al., 2018; Santhanam and Shaikh, 2019). Moreover, there is little consensus about how human evaluations should be designed and reported. Methods employed and details reported vary widely, issues including missing details (e.g. number of evaluators, outputs evaluated, and ratings collected), lack of proper analysis of results obtained (e.g. effect size and statistical significance), and much variation in names and definitions of evaluated aspects of output quality (van der Lee et al., 2019; Amidei et al., 2018). However, we currently lack a complete picture of the prevailing consensus, or lack thereof, regarding approaches to human evaluation, experimental design and terminology. Our goal in this work, therefore, is to investigate the extent of the above issues and provide a clear picture of the human evaluations NLG currently employs, how they are reported, and in what respects they are in need of improvement. To this end, we examined 20 years of NLG papers that reported some form of human evaluation, capturing key information about the systems, the quality criteria employed, and how these criteria"
2020.inlg-1.23,J08-4004,0,0.107861,"ells, we replaced those with ‘blank.’ We also removed papers not meeting the conditions from Section 2. Calculating agreement: The data resulting from annotation was a 10 (papers) × n (quality criteria identified by annotator in paper) × 16 (attribute value pairs) data frame, for each of the annotators. The task for IAA assessment was to measure the agreement across multiple data frames (one for each annotator) allowing for different numbers of criteria being identified by different authors. We did this by calculating Krippendorff’s alpha using Jaccard for the distance measure (recommended by Artstein and Poesio 2008). Scores for the seven closed-class attributes are shown in Table 2 for each of the two IAA tests (column headings as explained in the preceding section). The consensus annotations (‘duo’) required pairs of annotators to reach agreement about selected attribute values. This reduced disagreement and improved consistency with the guidelines, the time it took was prohibitive. For the attributes task, data type, and type of rating instrument (shortened to ‘instrument’ in the table), we consider the ‘5 best’ IAA to be very good (0 indicating chance-level agreement). For system input and output, IAA"
2020.inlg-1.24,W12-1510,0,0.173708,"at criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind criteria names, as is the case for the following names, all defined as the usefulness of the output text for completing a particular task: Dialogue efficiency (Qu and Green, 2002), Usefulness (Miliaev et al., 2003), Task completion (Varges, 2006), Productivity (Allman et al., 2012). 2.2 Other aspects of evaluations A vanishingly small number of papers provide full details of human evaluation experiments. It is common for papers not to report how many system outputs or evaluators were used, what information was given to them, what questions asked, etc. Our survey of 468 individual human evaluations in NLG (Howcroft et al., 2020) indicates that in about 2/3 of cases reports do not provide the question/prompt evaluators were shown, over half do not define the quality criterion assessed, and around 1/5 do not name the quality criterion. Missing information about experimenta"
2020.inlg-1.24,W17-3505,0,0.0131544,"See our survey of 20 years of human evaluations in NLG (Howcroft et al., 2020). 4 Note that the examples in this section were chosen at random, not because they vary most widely. Wan, 2020); “measures ability of the dialogue system to produce responses consistent with the topic of conversation” (Santhanam and Shaikh, 2019); “measures how much the response is comprehensible and relevant to a user’s request” (Yi et al., 2019); “refers to the meaning of the generated sentence, so that a sentence with no meaning would be rated with a 1 and a sentence with a full meaning would be rated with a 5” (Barros et al., 2017); “measures [a conversation’s] grammaticality and fluency” (Juraska et al., 2019); “concerns coherence and readability” (Murray et al., 2010). The inverse is also common, where the same definition is used with different criterion names. E.g. Chen et al. (2020) define Language Naturalness as “whether the generated text is grammatically correct and fluent, regardless of factual correctness”, while Juraska et al. (2019) give essentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical Engl"
2020.inlg-1.24,W09-0603,0,0.0192925,"he point is that we need to know how similar evaluations are, and in what respects, to inform expectations of similarity between their results. Conversely, when results are reported for different criteria (names), we may expect metaevaluation and correlation analysis to yield distinguishable results. This can be the case, e.g. Belz and Reiter (2006) report high Pearson correlation with all metrics for Fluency (of weather forecasts), but no correlations with any metrics for Accuracy (of the meteorological information). However, extreme positive correlations (r = 0.93..0.99) are often reported (Belz and Kow, 2009; Gardent et al., 2017; Duˇsek et al., 2020) for pairs of apparently very different quality criteria (e.g. Readability/Meaning Similarity), even when assessed separately for the express purpose of avoiding conflation (Mille et al., 2018, 2019; Duˇsek et al., 2020). What is clear, if nothing else, is that some evaluations are less similar, and others more, than meets the eye, and that we do not currently have a systematic way of telling in what respects (in terms of which properties) evaluations are the same and in what respects they are different. In order to be able to do this, we need a syst"
2020.inlg-1.24,W10-4201,0,0.21729,"t al., 2019). In this section we focus on those aspects that make it hard to compare different evaluations. 2.1 Quality criterion names Different papers use the same quality criterion name with different definitions, and the same definitions with different names. Even for less problematic criteria names such as Readability,4 substantial variation exists. Some definitions are about reading ease: “Ease of reading” (Forrest et al., 2018); “a summary is readable if it is easy to read and understand” (Di Fabbrizio et al., 2014). Others veer towards fluency: “how fluent and readable [the text is]” (Belz and Kow, 2010); “readability concerns fluency of the textual data” (Mahapatra et al., 2016). Yet others combine multiple aspects of quality: “measures the linguistic quality of text and helps quantify the difficulty of understanding the text for a reader” (Santhanam and Shaikh, 2019); “[r]eadability is [...] concerned with the fluency and coherence of the texts.” (Zang and Wan, 2017). A far messier criterion name is Coherence, some definitions referring to structure (underlined text below) and theme/topic (dotted underline), some just to one of the two, and others to neither (last three examples): “[whether"
2020.inlg-1.24,P11-2040,0,0.0227507,"d classification system. Fluency some authors might take that to relate to both form and content. As things stand, it is often impossible to tell, because (a) there is not enough information provided in papers, and (b) even if there is, it is not described in shared terms. A related question is how well evaluators understand what they are being asked to evaluate. It is often assumed that aspects of quality like Fluency and Clarity, and the differences between them, are intuitively clear to evaluators, but how certain is this when good intra and inter-evaluator agreement is so hard to achieve (Belz and Kow, 2011), and correlations between apparently very different criteria are so often in the high nineties (Section 3)? That researchers struggle to explain what to evaluate is also clear from definitions and prompts reported in papers which often define one quality criterion in terms of others (e.g. Rows 2, 3, 5 in Tables 1 and 2), and use inconsistent language in quality criterion name, definition, and prompts. A shared classification system helps address both the above, (a) making clear what needs to be included in reports to convey what was evaluated, and (b) providing a basis for conveying to evalua"
2020.inlg-1.24,E06-1040,0,0.279694,"that more readable texts are faster to read), and in the other, participants were asked to explicitly rate the readability of outputs on a 5-point scale? And what if we are then told that definitions of Readability and questions put to evaluators differed in each case? The point is that we need to know how similar evaluations are, and in what respects, to inform expectations of similarity between their results. Conversely, when results are reported for different criteria (names), we may expect metaevaluation and correlation analysis to yield distinguishable results. This can be the case, e.g. Belz and Reiter (2006) report high Pearson correlation with all metrics for Fluency (of weather forecasts), but no correlations with any metrics for Accuracy (of the meteorological information). However, extreme positive correlations (r = 0.93..0.99) are often reported (Belz and Kow, 2009; Gardent et al., 2017; Duˇsek et al., 2020) for pairs of apparently very different quality criteria (e.g. Readability/Meaning Similarity), even when assessed separately for the express purpose of avoiding conflation (Mille et al., 2018, 2019; Duˇsek et al., 2020). What is clear, if nothing else, is that some evaluations are less s"
2020.inlg-1.24,2020.acl-main.100,0,0.0725964,"ectness”, while Juraska et al. (2019) give essentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical English”, while Harrison and Walker (2018) use a very similar definition for Grammaticality: “adherence to rules of syntax, use of the wrong wh-word, verb tense consistency, and overall legitimacy as an English sentence.” In some cases where criterion names are different, it is slightly more evident that criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind criteria names, as is the case for the following names, all defined as the usefulness of the output text for completing a particular task: Dialogue efficiency (Qu and Green, 2002), Usefulness (Miliaev et al., 2003), Task completion (Varges, 2006), Productivity (Allman et al., 2012). 2.2 Other aspects of evaluations A vanishingly small number of papers provide f"
2020.inlg-1.24,2020.acl-main.21,0,0.0178487,"Missing"
2020.inlg-1.24,2020.acl-main.18,0,0.0299747,"nt with the topic of conversation” (Santhanam and Shaikh, 2019); “measures how much the response is comprehensible and relevant to a user’s request” (Yi et al., 2019); “refers to the meaning of the generated sentence, so that a sentence with no meaning would be rated with a 1 and a sentence with a full meaning would be rated with a 5” (Barros et al., 2017); “measures [a conversation’s] grammaticality and fluency” (Juraska et al., 2019); “concerns coherence and readability” (Murray et al., 2010). The inverse is also common, where the same definition is used with different criterion names. E.g. Chen et al. (2020) define Language Naturalness as “whether the generated text is grammatically correct and fluent, regardless of factual correctness”, while Juraska et al. (2019) give essentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical English”, while Harrison and Walker (2018) use a very similar definition for Grammaticality: “adherence to rules of syntax, use of the wrong wh-word, verb tense consistency, and overall legitimacy as an English sentence.” In some cases where criterion names are di"
2020.inlg-1.24,2020.acl-main.223,0,0.0679691,"Missing"
2020.inlg-1.24,W14-4408,0,0.069203,"Missing"
2020.inlg-1.24,W18-6522,0,0.0266058,"sues in Comparing Human Evaluations in NLG Human evaluations in NLG currently paint a confused picture3 with very poor standards for designing and reporting evaluations (van der Lee et al., 2019). In this section we focus on those aspects that make it hard to compare different evaluations. 2.1 Quality criterion names Different papers use the same quality criterion name with different definitions, and the same definitions with different names. Even for less problematic criteria names such as Readability,4 substantial variation exists. Some definitions are about reading ease: “Ease of reading” (Forrest et al., 2018); “a summary is readable if it is easy to read and understand” (Di Fabbrizio et al., 2014). Others veer towards fluency: “how fluent and readable [the text is]” (Belz and Kow, 2010); “readability concerns fluency of the textual data” (Mahapatra et al., 2016). Yet others combine multiple aspects of quality: “measures the linguistic quality of text and helps quantify the difficulty of understanding the text for a reader” (Santhanam and Shaikh, 2019); “[r]eadability is [...] concerned with the fluency and coherence of the texts.” (Zang and Wan, 2017). A far messier criterion name is Coherence, so"
2020.inlg-1.24,W17-3518,0,0.0299588,"need to know how similar evaluations are, and in what respects, to inform expectations of similarity between their results. Conversely, when results are reported for different criteria (names), we may expect metaevaluation and correlation analysis to yield distinguishable results. This can be the case, e.g. Belz and Reiter (2006) report high Pearson correlation with all metrics for Fluency (of weather forecasts), but no correlations with any metrics for Accuracy (of the meteorological information). However, extreme positive correlations (r = 0.93..0.99) are often reported (Belz and Kow, 2009; Gardent et al., 2017; Duˇsek et al., 2020) for pairs of apparently very different quality criteria (e.g. Readability/Meaning Similarity), even when assessed separately for the express purpose of avoiding conflation (Mille et al., 2018, 2019; Duˇsek et al., 2020). What is clear, if nothing else, is that some evaluations are less similar, and others more, than meets the eye, and that we do not currently have a systematic way of telling in what respects (in terms of which properties) evaluations are the same and in what respects they are different. In order to be able to do this, we need a system that specifies what"
2020.inlg-1.24,W08-1108,0,0.0570783,"for comparison of evaluations across papers, meta-evaluation experiments, reproducibility testing. 1 Introduction Human evaluations play a central role in Natural Language Generation (NLG), a field which has always been wary of automatic evaluation metrics and their limitations (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). NLG has trusted human evaluations perhaps more than any other NLP subfield, and has always gauged the trustworthiness of automatic evaluation metrics in terms of how well, and how consistently, they correlate with human evaluation scores (Over et al., 2007; Gatt and Belz, 2008; Bojar et al., 2016; Shimorina et al., 2018; Ma et al., 2019; Mille et al., 2019; Duˇsek et al., 2020). If they do not, even in isolated cases, the reliability of the metric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human evaluation involved aimed to assess the same thing. More generalised conclusions are sometimes drawn, for example that BLEU scores do not correlate well with human judgements of specific quality David M. Howcroft Heriot-Watt University, UK d.howcroft@hw.ac.uk criteria1 such as ‘Fluency,’ ‘Naturalness,’ ‘Readability’ or"
2020.inlg-1.24,W18-6536,0,0.0125844,"[a conversation’s] grammaticality and fluency” (Juraska et al., 2019); “concerns coherence and readability” (Murray et al., 2010). The inverse is also common, where the same definition is used with different criterion names. E.g. Chen et al. (2020) define Language Naturalness as “whether the generated text is grammatically correct and fluent, regardless of factual correctness”, while Juraska et al. (2019) give essentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical English”, while Harrison and Walker (2018) use a very similar definition for Grammaticality: “adherence to rules of syntax, use of the wrong wh-word, verb tense consistency, and overall legitimacy as an English sentence.” In some cases where criterion names are different, it is slightly more evident that criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind c"
2020.inlg-1.24,W19-8623,0,0.12708,"4 Note that the examples in this section were chosen at random, not because they vary most widely. Wan, 2020); “measures ability of the dialogue system to produce responses consistent with the topic of conversation” (Santhanam and Shaikh, 2019); “measures how much the response is comprehensible and relevant to a user’s request” (Yi et al., 2019); “refers to the meaning of the generated sentence, so that a sentence with no meaning would be rated with a 1 and a sentence with a full meaning would be rated with a 5” (Barros et al., 2017); “measures [a conversation’s] grammaticality and fluency” (Juraska et al., 2019); “concerns coherence and readability” (Murray et al., 2010). The inverse is also common, where the same definition is used with different criterion names. E.g. Chen et al. (2020) define Language Naturalness as “whether the generated text is grammatically correct and fluent, regardless of factual correctness”, while Juraska et al. (2019) give essentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical English”, while Harrison and Walker (2018) use a very similar definition for Grammati"
2020.inlg-1.24,W19-8643,0,0.135453,"Missing"
2020.inlg-1.24,W19-5302,0,0.0270323,"eriments, reproducibility testing. 1 Introduction Human evaluations play a central role in Natural Language Generation (NLG), a field which has always been wary of automatic evaluation metrics and their limitations (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). NLG has trusted human evaluations perhaps more than any other NLP subfield, and has always gauged the trustworthiness of automatic evaluation metrics in terms of how well, and how consistently, they correlate with human evaluation scores (Over et al., 2007; Gatt and Belz, 2008; Bojar et al., 2016; Shimorina et al., 2018; Ma et al., 2019; Mille et al., 2019; Duˇsek et al., 2020). If they do not, even in isolated cases, the reliability of the metric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human evaluation involved aimed to assess the same thing. More generalised conclusions are sometimes drawn, for example that BLEU scores do not correlate well with human judgements of specific quality David M. Howcroft Heriot-Watt University, UK d.howcroft@hw.ac.uk criteria1 such as ‘Fluency,’ ‘Naturalness,’ ‘Readability’ or ‘Overall Quality’2 in the general case (Novikova et al., 2017"
2020.inlg-1.24,W16-6624,0,0.0122514,"to compare different evaluations. 2.1 Quality criterion names Different papers use the same quality criterion name with different definitions, and the same definitions with different names. Even for less problematic criteria names such as Readability,4 substantial variation exists. Some definitions are about reading ease: “Ease of reading” (Forrest et al., 2018); “a summary is readable if it is easy to read and understand” (Di Fabbrizio et al., 2014). Others veer towards fluency: “how fluent and readable [the text is]” (Belz and Kow, 2010); “readability concerns fluency of the textual data” (Mahapatra et al., 2016). Yet others combine multiple aspects of quality: “measures the linguistic quality of text and helps quantify the difficulty of understanding the text for a reader” (Santhanam and Shaikh, 2019); “[r]eadability is [...] concerned with the fluency and coherence of the texts.” (Zang and Wan, 2017). A far messier criterion name is Coherence, some definitions referring to structure (underlined text below) and theme/topic (dotted underline), some just to one of the two, and others to neither (last three examples): “[whether] the poem [is] thematically structured” (Van de Cruys, 2020); “measures if a"
2020.inlg-1.24,2020.acl-main.448,0,0.0619037,"tric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human evaluation involved aimed to assess the same thing. More generalised conclusions are sometimes drawn, for example that BLEU scores do not correlate well with human judgements of specific quality David M. Howcroft Heriot-Watt University, UK d.howcroft@hw.ac.uk criteria1 such as ‘Fluency,’ ‘Naturalness,’ ‘Readability’ or ‘Overall Quality’2 in the general case (Novikova et al., 2017; May and Priyadarshi, 2017; Reiter, 2018; Shimorina et al., 2018; Duˇsek et al., 2020; Sellam et al., 2020; Mathur et al., 2020). However, such comments make the assumption that, and only really make sense if, multiple evaluations of, say, ‘Fluency’ do in fact assess the same aspect of quality in the output texts. We argue that we do not currently have a way of establishing whether any two evaluations, metric or human, do or do not assess the same thing. In fact, we have plenty of evidence (Section 2) that in many cases, when two evaluations use the same name for a quality criterion, they do in fact assess different aspects of quality, even for seemingly straightforward criteria like ‘Fluency’ and ‘Readability.’ And co"
2020.inlg-1.24,S17-2090,0,0.0135418,"Mille et al., 2019; Duˇsek et al., 2020). If they do not, even in isolated cases, the reliability of the metric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human evaluation involved aimed to assess the same thing. More generalised conclusions are sometimes drawn, for example that BLEU scores do not correlate well with human judgements of specific quality David M. Howcroft Heriot-Watt University, UK d.howcroft@hw.ac.uk criteria1 such as ‘Fluency,’ ‘Naturalness,’ ‘Readability’ or ‘Overall Quality’2 in the general case (Novikova et al., 2017; May and Priyadarshi, 2017; Reiter, 2018; Shimorina et al., 2018; Duˇsek et al., 2020; Sellam et al., 2020; Mathur et al., 2020). However, such comments make the assumption that, and only really make sense if, multiple evaluations of, say, ‘Fluency’ do in fact assess the same aspect of quality in the output texts. We argue that we do not currently have a way of establishing whether any two evaluations, metric or human, do or do not assess the same thing. In fact, we have plenty of evidence (Section 2) that in many cases, when two evaluations use the same name for a quality criterion, they do in fact assess different as"
2020.inlg-1.24,W03-2308,0,0.158001,"s where criterion names are different, it is slightly more evident that criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind criteria names, as is the case for the following names, all defined as the usefulness of the output text for completing a particular task: Dialogue efficiency (Qu and Green, 2002), Usefulness (Miliaev et al., 2003), Task completion (Varges, 2006), Productivity (Allman et al., 2012). 2.2 Other aspects of evaluations A vanishingly small number of papers provide full details of human evaluation experiments. It is common for papers not to report how many system outputs or evaluators were used, what information was given to them, what questions asked, etc. Our survey of 468 individual human evaluations in NLG (Howcroft et al., 2020) indicates that in about 2/3 of cases reports do not provide the question/prompt evaluators were shown, over half do not define the quality criterion assessed, and around 1/5 do n"
2020.inlg-1.24,W18-3601,1,0.837732,"uation and correlation analysis to yield distinguishable results. This can be the case, e.g. Belz and Reiter (2006) report high Pearson correlation with all metrics for Fluency (of weather forecasts), but no correlations with any metrics for Accuracy (of the meteorological information). However, extreme positive correlations (r = 0.93..0.99) are often reported (Belz and Kow, 2009; Gardent et al., 2017; Duˇsek et al., 2020) for pairs of apparently very different quality criteria (e.g. Readability/Meaning Similarity), even when assessed separately for the express purpose of avoiding conflation (Mille et al., 2018, 2019; Duˇsek et al., 2020). What is clear, if nothing else, is that some evaluations are less similar, and others more, than meets the eye, and that we do not currently have a systematic way of telling in what respects (in terms of which properties) evaluations are the same and in what respects they are different. In order to be able to do this, we need a system that specifies what those properties are, and provides definitions that make it possible to determine whether evaluations are the same or different in terms of each property. Identifying such properties is a major challenge, with cur"
2020.inlg-1.24,D19-6301,1,0.832092,"cibility testing. 1 Introduction Human evaluations play a central role in Natural Language Generation (NLG), a field which has always been wary of automatic evaluation metrics and their limitations (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). NLG has trusted human evaluations perhaps more than any other NLP subfield, and has always gauged the trustworthiness of automatic evaluation metrics in terms of how well, and how consistently, they correlate with human evaluation scores (Over et al., 2007; Gatt and Belz, 2008; Bojar et al., 2016; Shimorina et al., 2018; Ma et al., 2019; Mille et al., 2019; Duˇsek et al., 2020). If they do not, even in isolated cases, the reliability of the metric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human evaluation involved aimed to assess the same thing. More generalised conclusions are sometimes drawn, for example that BLEU scores do not correlate well with human judgements of specific quality David M. Howcroft Heriot-Watt University, UK d.howcroft@hw.ac.uk criteria1 such as ‘Fluency,’ ‘Naturalness,’ ‘Readability’ or ‘Overall Quality’2 in the general case (Novikova et al., 2017; May and Priyadarsh"
2020.inlg-1.24,W16-6621,0,0.0568377,"Missing"
2020.inlg-1.24,W10-4211,0,0.0327425,"om, not because they vary most widely. Wan, 2020); “measures ability of the dialogue system to produce responses consistent with the topic of conversation” (Santhanam and Shaikh, 2019); “measures how much the response is comprehensible and relevant to a user’s request” (Yi et al., 2019); “refers to the meaning of the generated sentence, so that a sentence with no meaning would be rated with a 1 and a sentence with a full meaning would be rated with a 5” (Barros et al., 2017); “measures [a conversation’s] grammaticality and fluency” (Juraska et al., 2019); “concerns coherence and readability” (Murray et al., 2010). The inverse is also common, where the same definition is used with different criterion names. E.g. Chen et al. (2020) define Language Naturalness as “whether the generated text is grammatically correct and fluent, regardless of factual correctness”, while Juraska et al. (2019) give essentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical English”, while Harrison and Walker (2018) use a very similar definition for Grammaticality: “adherence to rules of syntax, use of the wrong wh-w"
2020.inlg-1.24,W16-6620,0,0.0667279,"Missing"
2020.inlg-1.24,D17-1238,0,0.13799,"Missing"
2020.inlg-1.24,2020.acl-main.135,0,0.0217043,"Missing"
2020.inlg-1.24,W02-2118,0,0.0610845,"n English sentence.” In some cases where criterion names are different, it is slightly more evident that criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind criteria names, as is the case for the following names, all defined as the usefulness of the output text for completing a particular task: Dialogue efficiency (Qu and Green, 2002), Usefulness (Miliaev et al., 2003), Task completion (Varges, 2006), Productivity (Allman et al., 2012). 2.2 Other aspects of evaluations A vanishingly small number of papers provide full details of human evaluation experiments. It is common for papers not to report how many system outputs or evaluators were used, what information was given to them, what questions asked, etc. Our survey of 468 individual human evaluations in NLG (Howcroft et al., 2020) indicates that in about 2/3 of cases reports do not provide the question/prompt evaluators were shown, over half do not define the quality crit"
2020.inlg-1.24,J18-3002,0,0.0453219,"system for evaluations based on disentangling (i) what is being evaluated (which aspect of quality), and (ii) how it is evaluated in specific (a) evaluation modes and (b) experimental designs. We show that this approach provides a basis for determining comparability, hence for comparison of evaluations across papers, meta-evaluation experiments, reproducibility testing. 1 Introduction Human evaluations play a central role in Natural Language Generation (NLG), a field which has always been wary of automatic evaluation metrics and their limitations (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). NLG has trusted human evaluations perhaps more than any other NLP subfield, and has always gauged the trustworthiness of automatic evaluation metrics in terms of how well, and how consistently, they correlate with human evaluation scores (Over et al., 2007; Gatt and Belz, 2008; Bojar et al., 2016; Shimorina et al., 2018; Ma et al., 2019; Mille et al., 2019; Duˇsek et al., 2020). If they do not, even in isolated cases, the reliability of the metric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human evaluation involved aimed to assess the sa"
2020.inlg-1.24,J09-4008,0,0.173159,"xamples from NLG, we propose a classification system for evaluations based on disentangling (i) what is being evaluated (which aspect of quality), and (ii) how it is evaluated in specific (a) evaluation modes and (b) experimental designs. We show that this approach provides a basis for determining comparability, hence for comparison of evaluations across papers, meta-evaluation experiments, reproducibility testing. 1 Introduction Human evaluations play a central role in Natural Language Generation (NLG), a field which has always been wary of automatic evaluation metrics and their limitations (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). NLG has trusted human evaluations perhaps more than any other NLP subfield, and has always gauged the trustworthiness of automatic evaluation metrics in terms of how well, and how consistently, they correlate with human evaluation scores (Over et al., 2007; Gatt and Belz, 2008; Bojar et al., 2016; Shimorina et al., 2018; Ma et al., 2019; Mille et al., 2019; Duˇsek et al., 2020). If they do not, even in isolated cases, the reliability of the metric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human eval"
2020.inlg-1.24,W19-8610,0,0.0418043,". Even for less problematic criteria names such as Readability,4 substantial variation exists. Some definitions are about reading ease: “Ease of reading” (Forrest et al., 2018); “a summary is readable if it is easy to read and understand” (Di Fabbrizio et al., 2014). Others veer towards fluency: “how fluent and readable [the text is]” (Belz and Kow, 2010); “readability concerns fluency of the textual data” (Mahapatra et al., 2016). Yet others combine multiple aspects of quality: “measures the linguistic quality of text and helps quantify the difficulty of understanding the text for a reader” (Santhanam and Shaikh, 2019); “[r]eadability is [...] concerned with the fluency and coherence of the texts.” (Zang and Wan, 2017). A far messier criterion name is Coherence, some definitions referring to structure (underlined text below) and theme/topic (dotted underline), some just to one of the two, and others to neither (last three examples): “[whether] the poem [is] thematically structured” (Van de Cruys, 2020); “measures if a question is coherent with previous ones” (Chai and 3 See our survey of 20 years of human evaluations in NLG (Howcroft et al., 2020). 4 Note that the examples in this section were chosen at ran"
2020.inlg-1.24,2020.acl-main.704,0,0.0259691,"reliability of the metric is seen as doubtful, regardless of the quality of the human evaluation, or whether the metric and human evaluation involved aimed to assess the same thing. More generalised conclusions are sometimes drawn, for example that BLEU scores do not correlate well with human judgements of specific quality David M. Howcroft Heriot-Watt University, UK d.howcroft@hw.ac.uk criteria1 such as ‘Fluency,’ ‘Naturalness,’ ‘Readability’ or ‘Overall Quality’2 in the general case (Novikova et al., 2017; May and Priyadarshi, 2017; Reiter, 2018; Shimorina et al., 2018; Duˇsek et al., 2020; Sellam et al., 2020; Mathur et al., 2020). However, such comments make the assumption that, and only really make sense if, multiple evaluations of, say, ‘Fluency’ do in fact assess the same aspect of quality in the output texts. We argue that we do not currently have a way of establishing whether any two evaluations, metric or human, do or do not assess the same thing. In fact, we have plenty of evidence (Section 2) that in many cases, when two evaluations use the same name for a quality criterion, they do in fact assess different aspects of quality, even for seemingly straightforward criteria like ‘Fluency’ and"
2020.inlg-1.24,W06-1404,0,0.0525163,"t is slightly more evident that criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind criteria names, as is the case for the following names, all defined as the usefulness of the output text for completing a particular task: Dialogue efficiency (Qu and Green, 2002), Usefulness (Miliaev et al., 2003), Task completion (Varges, 2006), Productivity (Allman et al., 2012). 2.2 Other aspects of evaluations A vanishingly small number of papers provide full details of human evaluation experiments. It is common for papers not to report how many system outputs or evaluators were used, what information was given to them, what questions asked, etc. Our survey of 468 individual human evaluations in NLG (Howcroft et al., 2020) indicates that in about 2/3 of cases reports do not provide the question/prompt evaluators were shown, over half do not define the quality criterion assessed, and around 1/5 do not name the quality criterion. M"
2020.inlg-1.24,2020.acl-main.101,0,0.0694477,"fluent, regardless of factual correctness”, while Juraska et al. (2019) give essentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical English”, while Harrison and Walker (2018) use a very similar definition for Grammaticality: “adherence to rules of syntax, use of the wrong wh-word, verb tense consistency, and overall legitimacy as an English sentence.” In some cases where criterion names are different, it is slightly more evident that criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind criteria names, as is the case for the following names, all defined as the usefulness of the output text for completing a particular task: Dialogue efficiency (Qu and Green, 2002), Usefulness (Miliaev et al., 2003), Task completion (Varges, 2006), Productivity (Allman et al., 2012). 2.2 Other aspects of evaluations A vanishingl"
2020.inlg-1.24,W16-6608,0,0.0202091,"Missing"
2020.inlg-1.24,2020.acl-main.26,0,0.0327247,"Missing"
2020.inlg-1.24,W17-3526,0,0.0123308,"s are about reading ease: “Ease of reading” (Forrest et al., 2018); “a summary is readable if it is easy to read and understand” (Di Fabbrizio et al., 2014). Others veer towards fluency: “how fluent and readable [the text is]” (Belz and Kow, 2010); “readability concerns fluency of the textual data” (Mahapatra et al., 2016). Yet others combine multiple aspects of quality: “measures the linguistic quality of text and helps quantify the difficulty of understanding the text for a reader” (Santhanam and Shaikh, 2019); “[r]eadability is [...] concerned with the fluency and coherence of the texts.” (Zang and Wan, 2017). A far messier criterion name is Coherence, some definitions referring to structure (underlined text below) and theme/topic (dotted underline), some just to one of the two, and others to neither (last three examples): “[whether] the poem [is] thematically structured” (Van de Cruys, 2020); “measures if a question is coherent with previous ones” (Chai and 3 See our survey of 20 years of human evaluations in NLG (Howcroft et al., 2020). 4 Note that the examples in this section were chosen at random, not because they vary most widely. Wan, 2020); “measures ability of the dialogue system to produc"
2020.inlg-1.24,2020.acl-main.639,0,0.158145,"ssentially the same definition (see preceding paragraph) for Coherence. Wubben et al. (2016) define Fluency as “the extent to which a sentence is in proper, grammatical English”, while Harrison and Walker (2018) use a very similar definition for Grammaticality: “adherence to rules of syntax, use of the wrong wh-word, verb tense consistency, and overall legitimacy as an English sentence.” In some cases where criterion names are different, it is slightly more evident that criteria are in fact closely related, as with Wang et al. (2020)’s Faithfulness, Cao et al. (2020)’s Content similarity, and Zhou et al. (2020)’s Content preservation, all of which measure the extent to which the content of an output overlaps with that of the input. However, in many cases similarities are unguessably obscured behind criteria names, as is the case for the following names, all defined as the usefulness of the output text for completing a particular task: Dialogue efficiency (Qu and Green, 2002), Usefulness (Miliaev et al., 2003), Task completion (Varges, 2006), Productivity (Allman et al., 2012). 2.2 Other aspects of evaluations A vanishingly small number of papers provide full details of human evaluation experiments."
2020.msr-1.1,W13-3520,0,0.0265768,". Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track are UD structures as in T1 from"
2020.msr-1.1,P11-2040,0,0.0129219,"R’18 (Mille et al., 2018) and SR’19 (Mille et al., 2019). The evaluation method is Direct Assessment (DA) (Graham et al., 2016), as used by the WMT competitions to produce the official ranking of machine translation systems (Barrault et al., 2020) and video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019). We ran the evaluation on Mechanical Turk,6 assessing two quality criteria, in separate evaluation experiments, but using the same method: Readability and Meaning Similarity. We used continuous sliders as rating tools, the evidence being that raters tend to prefer them (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the gray text’), was:7,8 The meaning of the gray text is adequately expressed by t"
2020.msr-1.1,W11-2832,0,0.0178935,"of the SR’20 tracks, data and evaluation methods, as well as brief summaries of the participating systems. Full descriptions of the participating systems can be found in separate system reports elsewhere in this volume. 1 Introduction SR’20 is the fourth in a line of shared tasks focused on surface realisation, the name originally given to the last stage in the first-generation (pre-statistical and pre-neural) Natural Language Generation (NLG) pipeline, mapping from semantic representations to fully realised surface word strings. When we ran the first Surface Realisation Shared Task in 2011 (Belz et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al.,"
2020.msr-1.1,K17-3005,0,0.0265528,"tense, verbal finiteness, etc.). The test data sets can be grouped into three types: (i) in-domain test data, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some histori"
2020.msr-1.1,W19-8652,0,0.0338279,"map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back o"
2020.msr-1.1,W18-3604,1,0.862544,"redicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1, and 13 for T2 (for a summary overview, see Table 2, top 3 sections of the table). The datasets were selected from the available collection of 4"
2020.msr-1.1,D19-1055,0,0.0142412,"ear, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system"
2020.msr-1.1,D19-6304,0,0.216682,"er proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs word order restoration by learning Interpreted Regular Tree Grammar (IRTG) rules encoding the correspondence between UD-subgraphs and word orderings. The grammars build strings and UD graphs simultaneously, using pairs of operations each connecting a set of dependents to their common head while concatenating the corresponding words. Rule 3 Data type Dataset Track train dev test In-domain arabic_padt (ar) chinese_gsd (zh) english_ewt (en) english_gum (en) english_lines (en) english_partut (en) french_gsd (fr) french_partut (fr) french_sequoia (fr) hindi_hdtb (hi) indonesian_gsd (id) j"
2020.msr-1.1,2020.acl-main.703,0,0.0144601,"mmon head while concatenating the corresponding words. The approach extends the team’s 2019 system by allowing rules to reference lemmas in addition to POS-tags and by giving preference to derivations that use a smaller number of more specific rules to construct a particular UD graph. Word order restoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augm"
2020.msr-1.1,K18-2014,0,0.0243276,"ta, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some historical figures. On the extracted sentences, we applied extensive filtering to achieve reasonably good text qu"
2020.msr-1.1,D17-1262,1,0.833898,"look at improvements this year compared to 2019, we see for instance, on the English_ewt test set, last year’s top BLEU score in T1 (the Shallow Track) was 82.98 (IMS); in 2020, it goes up to 86.16 in the restricted track (IMS), and 87.5 in the open track (ADAPT). In T2 (the Deep Track), top BLEU scores also increased, from 54.75 (IMS) to 58.84 in the restricted track, and 58.66 in the unrestricted track (both IMS). We next look at overall improvements of team submissions across all test sets they submitted outputs bias into the evaluation revealing no significant evidence of reference-bias (Ma et al., 2017). 8 ADAPT 20a 20b –BLEU-4– T1_ar_padt T1_en_ewt T2_en_ewt T1_en_gum T2_en_gum T1_en_lines T2_en_lines T1_en_partut T2_en_partut T1_es_ancora T2_es_ancora T1_es_gsd T2_es_gsd T1_fr_gsd T2_fr_gsd T1_fr_partut T2_fr_partut T1_fr_sequoia T2_fr_sequoia T1_hi_hdtb T1_id_gsd T1_ja_gsd T1_ko_gsd T1_ko_kaist T1_pt_bosque T1_pt_gsd T1_ru_gsd T1_ru_syntagrus T1_zh_gsd BME 20a 19 26 57.25 26.4 59.22 60.77 57.57 55.98 48.78 57.96 61.37 59.32 61.09 54.6 53.74 43.21 43.8 52.46 49.17 45.25 46.72 57.2 59.16 50.89 58.37 57.05 39.89 30.68 54.28 54.79 50.58 63.63 54.22 49.53 46.08 47.23 39.53 30.39 54.58 50.91 58"
2020.msr-1.1,W17-3517,1,0.829222,"et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al., 2017), Universal Dependencies (UDs) had emerged as a convenient standard in parsing, with many associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisatio"
2020.msr-1.1,W18-3601,1,0.732438,"ny associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Mult"
2020.msr-1.1,D19-6301,1,0.741899,"ets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are"
2020.msr-1.1,W15-4719,0,0.0195724,"rojective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1,"
2020.msr-1.1,P02-1040,0,0.108533,"-20-multilingual 2 5 Figure 1: Sample UD structure (without the last two columns). Figure 2: Sample T1 input structure (without the last two columns). Figure 3: Sample T2 input structure (without the last two columns). and Deep Track inputs is available on GitLab.4 Figures 1, 2 and 3 shown sample UD, Track 1 and Track 2 structures respectively, taken from the parsed Wikipedia English dataset. 5 Evaluation Methods 5.1 Automatic methods We used BLEU, NIST, BERT, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST5 is a related n-gram similarity metric weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single) reference text. The resulting number is then divided by the number of characters"
2020.msr-1.1,N18-1202,0,0.0268272,"rd order and inflecting words. a. Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track"
2020.msr-1.1,2020.acl-demos.14,0,0.0252606,"xt quality. We skipped sentences that include special characters, contain unusual tokens (e.g. ISBN), or have unbalanced quotation marks or brackets. Furthermore, we took only sentences with more than 5 tokens and shorter than 50 tokens. After the initial filtering, quite a few malformed sentences remained. In order to remove those, we scored the sentences with BERT and kept only the best scored half. Finally, via manual inspection we identified patterns and expressions to reduce the number of malformed sentences still further. We parsed the cleaned Wikipedia sentences with the Stanza parser (Qi et al., 2020), using the trained models provided for the respective languages; the Stanza parser gets very competitive results on a large set of languages (see Table 3). For each language, we executed the parser with the processors for Tokenisation and Sentence Split, Multi-word Token Expansion, Part-of-Speech and Morphological Tagging, Lemmatisation and Dependency Parsing. The performance of the parser for all six languages in terms of Labelled Attachment Score and lemmatisation, two of the crucial aspects for our task, is provided in Table 3; for reference, we also provide the LAS and lemma scores of the"
2020.msr-1.1,K18-2011,1,0.865135,"Missing"
2020.msr-1.1,D19-6306,0,0.147951,", T2 T1, T2 T1 T1 T1 T1, T2 - - 1,795 1,032 1,675 2,287 471 1,723 Automatically parsed Wikipedia english_wikiST Z (en) french_wikiST Z (fr) korean_wikiST Z (ko) portuguese_wikiST Z (pt) russian_wikiST Z (ru) spanish_wikiST Z (es) T1, T2 T1, T2 T1 T1 T1 T1, T2 - - 1,313 1,313 530 1,135 1,291 1,280 Table 2: SR’20 dataset sizes for training, development and test sets (number of sentences). weights are proportional to the observed frequency of each pattern in the training data. The inflection step uses a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search, then combining the trees into a full projective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group w"
2020.msr-1.1,2020.acl-main.134,0,0.342113,"estoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs wo"
2020.msr-1.1,W19-3404,0,0.023759,"ferent again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system training, and cros"
2020.webnlg-1.1,W11-2832,0,0.0388806,"f., e.g., (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018) for overviews and the WebNLG challenge (Gardent et al., 2017a) for state-of-the-art works. In general, there are three main approaches to generating texts from ontologies: (i) filling slot values in predefined sentence templates (McRoy et al., 2003), (ii) applying grammars that encode different types of linguistic knowledge (Varges and Mellish, 2001; Wanner et al., 2010; Bouayad-Agha et al., 2012; Androutsopoulos et al., 2013), and (iii) predicting the most appropriate output based on machine learning models (Gardent et al., 2017b; Belz et al., 2011). Template-based generators are very robust, but also limited in terms of portability since new templates need to be defined for ev3 System and dataset overview Let us first introduce the architecture of our system and then outline the creation of the datasets used for development and testing. 3.1 General system architecture The workflow of our system is illustrated in Figure 1. The initial input is the topic entity on which the text is to be generated. Based on this, the data collection module harvests relevant content from the Web. The resources of interest are images from the Flickr website"
2020.webnlg-1.1,N01-1001,0,0.19552,"d via a multi-modality ontology, which is further enriched by DBpedia triples for the purpose of semantics-driven image retrieval. On the other side, text generation from ontological structures is on the rise; cf., e.g., (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018) for overviews and the WebNLG challenge (Gardent et al., 2017a) for state-of-the-art works. In general, there are three main approaches to generating texts from ontologies: (i) filling slot values in predefined sentence templates (McRoy et al., 2003), (ii) applying grammars that encode different types of linguistic knowledge (Varges and Mellish, 2001; Wanner et al., 2010; Bouayad-Agha et al., 2012; Androutsopoulos et al., 2013), and (iii) predicting the most appropriate output based on machine learning models (Gardent et al., 2017b; Belz et al., 2011). Template-based generators are very robust, but also limited in terms of portability since new templates need to be defined for ev3 System and dataset overview Let us first introduce the architecture of our system and then outline the creation of the datasets used for development and testing. 3.1 General system architecture The workflow of our system is illustrated in Figure 1. The initial i"
2020.webnlg-1.3,D19-1052,1,0.864153,"Missing"
2020.webnlg-1.3,W05-0909,0,0.0280667,"tion of text generated outputs. ChrF++ has shown a good correlation with human rankings of different MT outputs, especially for morphologically rich target languages. Additionally, it is language- and tokenisation- independent.7 3.2.1.1 N-gram-based metrics BLEU (Papineni et al., 2002) is widely chosen for evaluating text generation outputs due to its low costs. BLEU uses a modified precision metric for comparing the hypotheses with the references. For the sake of comparison, BENG uses two implementations of BLEU: (1) Multi-bleu-detok from Moses,4 (2) BLEU-NLTK from the NLTK library.5 METEOR (Banerjee and Lavie, 2005) relies on semantic features to improve correlation quality between system hypotheses and human references. To this end, METEOR considers the synonymy overlap through a shared WordNet synset of the words to overcome some weaknesses of BLEU and TER (Snover et al., 2006) is different from the aforementioned metrics. TER measures the number of necessary edits in an MT/NLG output to match the reference text exactly. The edits consist of insertions, deletions, substitutions and shift of words, as well as capitalisation and punctuation. The TER score is calculated by computing the number of edits di"
2020.webnlg-1.3,P17-1017,1,0.930658,"g platform GERBIL, is opensource and is publicly available along with the data it contains. 1 Introduction NLG is the process of generating coherent natural language text from non-linguistic data (Reiter and Dale, 2000). A large number of approaches with distinct inputs have been employed for NLG systems over the last years (Gatt and Krahmer, 2018). After having been addressed in only a few papers at the beginning of the last decade (Ell et al., 2012; Ngonga Ngomo et al., 2013), the generation of natural language from Resource Description Framework (RDF) data has gained substantial attention (Gardent et al., 2017b). The RDF-totext task has hence been proposed to investigate the quality of automatically generated texts from RDF Knowledge Graphs (KGs) (Colin et al., 2016; Moussallem et al., 2020). Recent studies have focused on comparing systematically neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples (Ferreira et al., 2019). However, a transparent comparison of RDFbased NLG systems is costly and prone to failure when relying only on benchmarking datasets such as WebNLG without a proper benchmarking platform. Recent works have hence proposed evaluation to"
2020.webnlg-1.3,W11-2832,0,0.0213396,"s service-oriented architecture. We in3.3 Datasets tegrated new experiment types, datasets, and measures. The main advantages of BENG are that it BENG includes the WebNLG datasets for the follows the FAIR Guiding Principles and provides RDF2Text task (refer to Table 2). WebNLG2017 a web-based frontend that allows for several use 9 https://github.com/Tiiiger/bert_score 10 cases enabling lay people and expert users to perhttps://github.com/google-research/ bleurt form informed comparisons of annotation tools. In 30 future work, we plan to include other popular NLG benchmarks such as E2E and SR (Belz et al., 2011; Novikova et al., 2017; Mille et al., 2018) and extend the experiment types as well as include the web-services for models instead of uploading the hypotheses. Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th INLG conference, pages 163–167. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of th"
2020.webnlg-1.3,W17-3518,1,0.928991,"g platform GERBIL, is opensource and is publicly available along with the data it contains. 1 Introduction NLG is the process of generating coherent natural language text from non-linguistic data (Reiter and Dale, 2000). A large number of approaches with distinct inputs have been employed for NLG systems over the last years (Gatt and Krahmer, 2018). After having been addressed in only a few papers at the beginning of the last decade (Ell et al., 2012; Ngonga Ngomo et al., 2013), the generation of natural language from Resource Description Framework (RDF) data has gained substantial attention (Gardent et al., 2017b). The RDF-totext task has hence been proposed to investigate the quality of automatically generated texts from RDF Knowledge Graphs (KGs) (Colin et al., 2016; Moussallem et al., 2020). Recent studies have focused on comparing systematically neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples (Ferreira et al., 2019). However, a transparent comparison of RDFbased NLG systems is costly and prone to failure when relying only on benchmarking datasets such as WebNLG without a proper benchmarking platform. Recent works have hence proposed evaluation to"
2020.webnlg-1.3,W18-1817,0,0.0128354,"es, videos, texts) sources and multiple text references to provide a detailed picture of system evaluations. In MT, compare-mt (Neubig et al., 2019) and MT-ComparEval (Klejch et al., 2015) are related tools for comparative analysis with automatic measures that provide a high-level view of major differences between MT outputs. In turn, Vis-Eval Metric Viewer (Steele and Specia, 2018) and iBLEU (Madnani, 2011) present metric scores as a visual interface. Other tools focus on the interpretability of the text generation process and language model parameters such as the OpenNMT visualisation tool (Klein et al., 2018), LM (Rong and Adar, 2016), and Seq2Seq (Strobelt et al., 2019). Although MT and NLG tasks rely on the same metrics for evaluating their outputs, none of the aforementioned tools rely on FAIR principles for the sake of reproducible research. Therefore, BENG is the first evaluation tool that abides by the FAIR principles for the text generation task. 3 Framework BENG addresses the problem of comparing different NLG systems using automatic metric results while relying on FAIR principles. It is based on a service-oriented architecture that reuses components from the FAIR benchmarking platform GER"
2020.webnlg-1.3,W18-6521,1,0.894335,"Missing"
2020.webnlg-1.3,W18-3601,1,0.827371,"Datasets tegrated new experiment types, datasets, and measures. The main advantages of BENG are that it BENG includes the WebNLG datasets for the follows the FAIR Guiding Principles and provides RDF2Text task (refer to Table 2). WebNLG2017 a web-based frontend that allows for several use 9 https://github.com/Tiiiger/bert_score 10 cases enabling lay people and expert users to perhttps://github.com/google-research/ bleurt form informed comparisons of annotation tools. In 30 future work, we plan to include other popular NLG benchmarks such as E2E and SR (Belz et al., 2011; Novikova et al., 2017; Mille et al., 2018) and extend the experiment types as well as include the web-services for models instead of uploading the hypotheses. Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th INLG conference, pages 163–167. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:"
2020.webnlg-1.3,2020.acl-main.704,0,0.104713,"upload the candidate triples and reference triples. The results are presented in the generated URI, not in the leaderboard as there is no common dataset. Name 3.2 Automatic Evaluation Metrics Experiment Type Lang. Texts WebNLG2017 RDF2Text EN DE WebNLG2019 RDF2Text RU EN WebNLG2020 RDF2Text/Text2RDF RU Sets T /S D 25,298 9,674 20,370 7,812 7 15 7 15 20,800 5,185 7 45,032 16,677 20,800 5,185 7 19 7 9 9 3.2.1 Metrics for Text Generation BENG includes the most used metrics according to Gatt and Krahmer (2018) and the metrics which correlate better with human evaluations based on recent findings (Sellam et al., 2020). We briefly explain below the automatic evaluation metrics present in BENG. NIST (Doddington, 2002). BENG relies on the latest METEOR version.6 chrF++ (Popovi´c, 2015, 2016) proposes the use of character n-gram precision and recall (F-score) for automatic evaluation of text generated outputs. ChrF++ has shown a good correlation with human rankings of different MT outputs, especially for morphologically rich target languages. Additionally, it is language- and tokenisation- independent.7 3.2.1.1 N-gram-based metrics BLEU (Papineni et al., 2002) is widely chosen for evaluating text generation ou"
2020.webnlg-1.3,2006.amta-papers.25,0,0.0587913,"t al., 2002) is widely chosen for evaluating text generation outputs due to its low costs. BLEU uses a modified precision metric for comparing the hypotheses with the references. For the sake of comparison, BENG uses two implementations of BLEU: (1) Multi-bleu-detok from Moses,4 (2) BLEU-NLTK from the NLTK library.5 METEOR (Banerjee and Lavie, 2005) relies on semantic features to improve correlation quality between system hypotheses and human references. To this end, METEOR considers the synonymy overlap through a shared WordNet synset of the words to overcome some weaknesses of BLEU and TER (Snover et al., 2006) is different from the aforementioned metrics. TER measures the number of necessary edits in an MT/NLG output to match the reference text exactly. The edits consist of insertions, deletions, substitutions and shift of words, as well as capitalisation and punctuation. The TER score is calculated by computing the number of edits divided by the average referenced words.8 6 rb.gy/6q5zsv https://github.com/m-popovic/chrF 8 https://github.com/roy-ht/pyter 4 7 rb.gy/zaffdt 5 https://www.nltk.org/ 29 Figure 2: Screenshot of the Leaderboard - RDF2Text is a semantically varied corpus containing diverse"
2020.webnlg-1.3,N19-4007,0,0.0128991,"s which convert text into a set of RDF triples. The evaluation script uses Precision, Recall, F1-score as metrics. The evaluation algorithm relies on four types of matches (exact, partial, strict, and Enttype3 ) to compare the candidate triples with the reference triples. In the WebNLG Text2RDF experiment type, the users can upload the candidate triples and select the text generation tasks, for example, Machine Translation (MT) or NLG. VizSeq supports multimodal (images, videos, texts) sources and multiple text references to provide a detailed picture of system evaluations. In MT, compare-mt (Neubig et al., 2019) and MT-ComparEval (Klejch et al., 2015) are related tools for comparative analysis with automatic measures that provide a high-level view of major differences between MT outputs. In turn, Vis-Eval Metric Viewer (Steele and Specia, 2018) and iBLEU (Madnani, 2011) present metric scores as a visual interface. Other tools focus on the interpretability of the text generation process and language model parameters such as the OpenNMT visualisation tool (Klein et al., 2018), LM (Rong and Adar, 2016), and Seq2Seq (Strobelt et al., 2019). Although MT and NLG tasks rely on the same metrics for evaluatin"
2020.webnlg-1.3,N18-5015,0,0.011764,"ate triples with the reference triples. In the WebNLG Text2RDF experiment type, the users can upload the candidate triples and select the text generation tasks, for example, Machine Translation (MT) or NLG. VizSeq supports multimodal (images, videos, texts) sources and multiple text references to provide a detailed picture of system evaluations. In MT, compare-mt (Neubig et al., 2019) and MT-ComparEval (Klejch et al., 2015) are related tools for comparative analysis with automatic measures that provide a high-level view of major differences between MT outputs. In turn, Vis-Eval Metric Viewer (Steele and Specia, 2018) and iBLEU (Madnani, 2011) present metric scores as a visual interface. Other tools focus on the interpretability of the text generation process and language model parameters such as the OpenNMT visualisation tool (Klein et al., 2018), LM (Rong and Adar, 2016), and Seq2Seq (Strobelt et al., 2019). Although MT and NLG tasks rely on the same metrics for evaluating their outputs, none of the aforementioned tools rely on FAIR principles for the sake of reproducible research. Therefore, BENG is the first evaluation tool that abides by the FAIR principles for the text generation task. 3 Framework BE"
2020.webnlg-1.3,W17-5525,0,0.0200914,"architecture. We in3.3 Datasets tegrated new experiment types, datasets, and measures. The main advantages of BENG are that it BENG includes the WebNLG datasets for the follows the FAIR Guiding Principles and provides RDF2Text task (refer to Table 2). WebNLG2017 a web-based frontend that allows for several use 9 https://github.com/Tiiiger/bert_score 10 cases enabling lay people and expert users to perhttps://github.com/google-research/ bleurt form informed comparisons of annotation tools. In 30 future work, we plan to include other popular NLG benchmarks such as E2E and SR (Belz et al., 2011; Novikova et al., 2017; Mille et al., 2018) and extend the experiment types as well as include the web-services for models instead of uploading the hypotheses. Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th INLG conference, pages 163–167. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu"
2020.webnlg-1.3,P02-1040,0,0.107682,"ter with human evaluations based on recent findings (Sellam et al., 2020). We briefly explain below the automatic evaluation metrics present in BENG. NIST (Doddington, 2002). BENG relies on the latest METEOR version.6 chrF++ (Popovi´c, 2015, 2016) proposes the use of character n-gram precision and recall (F-score) for automatic evaluation of text generated outputs. ChrF++ has shown a good correlation with human rankings of different MT outputs, especially for morphologically rich target languages. Additionally, it is language- and tokenisation- independent.7 3.2.1.1 N-gram-based metrics BLEU (Papineni et al., 2002) is widely chosen for evaluating text generation outputs due to its low costs. BLEU uses a modified precision metric for comparing the hypotheses with the references. For the sake of comparison, BENG uses two implementations of BLEU: (1) Multi-bleu-detok from Moses,4 (2) BLEU-NLTK from the NLTK library.5 METEOR (Banerjee and Lavie, 2005) relies on semantic features to improve correlation quality between system hypotheses and human references. To this end, METEOR considers the synonymy overlap through a shared WordNet synset of the words to overcome some weaknesses of BLEU and TER (Snover et al"
2020.webnlg-1.3,W15-3049,0,0.0272179,"Missing"
2020.webnlg-1.3,D19-3043,0,0.0531727,"roposed to investigate the quality of automatically generated texts from RDF Knowledge Graphs (KGs) (Colin et al., 2016; Moussallem et al., 2020). Recent studies have focused on comparing systematically neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples (Ferreira et al., 2019). However, a transparent comparison of RDFbased NLG systems is costly and prone to failure when relying only on benchmarking datasets such as WebNLG without a proper benchmarking platform. Recent works have hence proposed evaluation tools for text generation such as VizSeq (Wang et al., 2019) and MT-ComparEval (Klejch et al., 2015). However, none of these tools abides by the FAIR principles (Wilkinson et al., 2016), which are now widely regarded as a key first step to ensure reproducible research in scientific experiments. Moreover, none of the tools aforementioned involves RDF data for the knowledge (relations and entities) extraction task (KE). In this paper, we address this gap by presenting BENG, a FAIR benchmarking platform for NLG and Knowledge Extraction systems. BENG is available as an online instance with a user-friendly interface that can be freely used by researchers to"
2020.webnlg-1.3,W16-2341,0,0.0850823,"Missing"
2020.webnlg-1.7,2020.webnlg-1.17,0,0.0322949,"ts (Russian native speakers), and they edited delexicalised templates from the data by replacing the texts one more time if needed. RDF subjects and objects with placeholders and Based on this procedure, we assume that the identifying their text counterparts using the JaroRussian data is of a decent quality. However, based Winkler similarlity metrics. on manual inspections, some texts may still be 3.2 Mono-lingual, Mono-task, Neural lacking in terms of fluency and correctness. Note Approaches also that the Russian version was derived from the English WebNLG version 2.0, where some errors med. Blinov (2020) focuses on generation into in semantic content realisation were present. Russian. They used the pre-trained Russian GPT-2 Train 3 Dev Test (D2T) Test (SP) Participating Systems The WebNLG+ data was downloaded more than 100 times, 17 teams submitted 48 system runs. language model (Radford et al., 2019) augmented with a classification head and fine-tuned on the WebNLG+ RDF-to-Russian dataset. The author experimented with various sampling methods and Team med RALI-Universit´e de Montr´eal ORANGE-NLG cuni-ufal TGen bt5 UPC-POE DANGNT-SGU Huawei Noah’s Ark Lab Amazon AI (Shanghai) NILC NUIG-DSI Cy"
2020.webnlg-1.7,W14-3346,0,0.0336319,"Missing"
2020.webnlg-1.7,2020.webnlg-1.19,0,0.0612738,"Missing"
2020.webnlg-1.7,P17-1017,1,0.933665,"(KBs) and natural language. There is a clear parallel between open information extraction (Open IE) and RDF-based semantic parsing, and between RDFto-Text generation and KB verbalisation. Yet the interaction between NLP and Semantic Web research remains limited. By highlighting the NLP tasks involved in mapping RDF triples and natural language, we aim to stimulate cross-fertilisation between NLP and Semantic Web research. WebNLG datasets align sets of RDF triples with text. While the 2017 WebNLG shared task required participating systems to generate English text from a set of DBpedia triples (Gardent et al., 2017b), the 2020 WebNLG+ challenge additionally includes generation into Russian and semantic parsing of English and Russian texts. Thus the WebNLG+ challenge encompasses four tasks: RDF-to-English, RDF-to-Russian, English-to-RDF and Russian-toRDF. Timeline. The training and development data was released on April 15, 2020, preliminary evaluation scripts on April, 30th and final evaluation scripts on May, 30th. The test data was made available on September, 13th and the deadline for submitting system results was September, 27th. Automatic evaluation results were announced on October, 9th and the fi"
2020.webnlg-1.7,W17-3518,1,0.843878,"(KBs) and natural language. There is a clear parallel between open information extraction (Open IE) and RDF-based semantic parsing, and between RDFto-Text generation and KB verbalisation. Yet the interaction between NLP and Semantic Web research remains limited. By highlighting the NLP tasks involved in mapping RDF triples and natural language, we aim to stimulate cross-fertilisation between NLP and Semantic Web research. WebNLG datasets align sets of RDF triples with text. While the 2017 WebNLG shared task required participating systems to generate English text from a set of DBpedia triples (Gardent et al., 2017b), the 2020 WebNLG+ challenge additionally includes generation into Russian and semantic parsing of English and Russian texts. Thus the WebNLG+ challenge encompasses four tasks: RDF-to-English, RDF-to-Russian, English-to-RDF and Russian-toRDF. Timeline. The training and development data was released on April 15, 2020, preliminary evaluation scripts on April, 30th and final evaluation scripts on May, 30th. The test data was made available on September, 13th and the deadline for submitting system results was September, 27th. Automatic evaluation results were announced on October, 9th and the fi"
2020.webnlg-1.7,2020.webnlg-1.8,0,0.302633,"nce. To help bridge the gap between the input graph and the output linear structure, a second phase of pre-training is applied using DocRED, a noisy parallel corpus of sentences and their automatically extracted relation (17K entries). Lexicalisation of RDF properties are also curated from the WebNLG+ and the DocRED datasets. 3.4 Bi-Directional, Monolingual Approaches Amazon AI (Shanghai). Zhao et al. (2020) introduced a two-step model for RDF-to-Text generation which combines a planner trained to learn the order in which triples should be verbalised and a decoder for verbalising each triple. Guo et al. (2020a) train Zhao et al. (2020)’s planner on the WebNLG+ dataset and use the pre-trained T5-large model to verbalise the linearised triples. For the Text-to-RDF task, entity linking is applied to the text and DBpedia is queried to retrieve the corresponding triples. CycleGT. Guo et al. (2020b) present a weakly supervised method where generation and semantic parsing models are learned by bootstrapping from purely text and purely RDF data and iteratively mapping between the two forms. The T5 pre-trained sequence-to-sequence model is used to bootstrap the generation model. For semantic parsing, the a"
2020.webnlg-1.7,2020.webnlg-1.20,0,0.329984,"Missing"
2020.webnlg-1.7,2020.webnlg-1.18,0,0.0265484,"Missing"
2020.webnlg-1.7,2020.webnlg-1.16,0,0.135144,"ion followed the proceIn what follows, we summarise the primary subdure below: missions of the 15 participating teams. 1. Russian WebNLG was translated from the English WebNLG version 2.0 with the MT system 3.1 Monolingual, Mono-Task, of Sennrich et al. (2017), as described in Shimorina Template-based Approaches et al. (2019). Among all system submissions, two of them 2. It was then post-edited using crowdsourcing used templates: RALI-Universit´e de Montr´eal and on the Yandex.Toloka platform in two steps: DANGNT-SGU. • we asked people to post-edit Russian texts RALI-Universit´e de Montr´eal. Lapalme (2020) given original English texts and provided them implements a symbolic approach which captures with some pointers for translation of entities the various substeps of NLG programmatically. (the links described above). Crowdworkers The input set of RDF triples is partitioned and orwere asked to use the pointers as much as dered into sentence sized subsets. Each subset possible. is then transformed into a sentence using Python • given the post-edited sentences, we asked peo- procedures designed to encode 200 manually defined sentence templates. Aggregation is handled ple to check if the text was t"
2020.webnlg-1.7,W07-0734,0,0.720903,"Missing"
2020.webnlg-1.7,2020.acl-main.703,0,0.110317,"Missing"
2020.webnlg-1.7,2020.tacl-1.47,0,0.0691849,"Missing"
2020.webnlg-1.7,W02-0109,0,0.0192729,"Missing"
2020.webnlg-1.7,P14-5010,0,0.00278563,"architect PRED architect COR COR SUB Capers OBJ Super Capers INC PAR MIS SPU INC COR COR INC MIS SPU INC INC COR INC Table 7: Examples of possible error types for semantic parsing, and how these are interpreted by the measures. COR = correct, INC = incorrect, PAR = partial, MIS = missed, SPU = spurious. For development purposes, the evaluation script also provided information about the number of correct, incorrect, partial missed, spurious, possible, and actual matches for the four measures. Baselines. A baseline was constructed by using Stanford CoreNLP’s Open Information Extraction module (Manning et al., 2014) on the texts in the test set. This module allows for the extraction of subjects, relations, and objects in a string without any training necessary. Extraction of these triples was limited to 10 per text, to avoid memory overflow errors when running the evaluation script. As this Open Information Extraction module was only developed for English, the Russian sentences were translated to English using DeepL,7 before extracting the RDF triples using Stanford CoreNLP’s Open Information Extraction module. 5 Results of Automatic Evaluation In this section, we present the automatic scores on English"
2020.webnlg-1.7,W19-8659,1,0.935546,"lcoxon Rank-Sum Test to evaluate whether there is a statistically significant difference between the average evaluation scores of the systems. The result is shown as a system’s rank, which was set measuring the pair-wise statistical tests between the averaged z-score results of a top-performing systems with the results of each of its lowerperforming ones. • We computed final human evaluation results for (i) the whole set of sampled test set outputs per system, (ii) for outputs per each test set type (seen categories, unseen entities, unseen categories). Baselines. We used the FORGe generator (Mille et al., 2019a) as a baseline, an all-purpose grammarand template-based generator that takes predicateargument structures as input. FORGe was adapted to triple-based inputs such as the E2E and several DBpedia-oriented datasets — including WebNLG and WebNLG+ — with the addition of a module for the mapping of RDF to predicate-argument (external module) and a module for aggregation. It consists of 16 graph-transduction grammars that perform the following tasks as a pipeline: (i) aggregation of predicate-argument templates, (ii) definition of sentence structure for each resulting aggregated graph, (iii) introd"
2020.webnlg-1.7,2020.webnlg-1.9,0,0.0276582,"Missing"
2020.webnlg-1.7,P02-1040,0,0.107432,"Missing"
2020.webnlg-1.7,2020.webnlg-1.15,0,0.227841,"Missing"
2020.webnlg-1.7,W17-4770,0,0.177685,"Missing"
2020.webnlg-1.7,2020.acl-demos.14,0,0.0152759,"ao et al. (2020)’s planner on the WebNLG+ dataset and use the pre-trained T5-large model to verbalise the linearised triples. For the Text-to-RDF task, entity linking is applied to the text and DBpedia is queried to retrieve the corresponding triples. CycleGT. Guo et al. (2020b) present a weakly supervised method where generation and semantic parsing models are learned by bootstrapping from purely text and purely RDF data and iteratively mapping between the two forms. The T5 pre-trained sequence-to-sequence model is used to bootstrap the generation model. For semantic parsing, the authors use Qi et al. (2020) entity extraction model to identify all entities present in the input text and a multi-label classifier to predict the relation between pairs of entities. Each input text and each input graph is aligned with its back-translated version and the resulting aligned data for training. The two models are improved by repeatedly alternating the optimisation of each model. The text and the RDF data used to bootstrap the model are the WebNLG+ 2020 dataset, shuffled to ensure that the data is fully non parallel (text and RDF in each of the datasets are not aligned). 3.5 Bi-directional, Bi-lingual Approa"
2020.webnlg-1.7,S13-2056,0,0.0357247,"Missing"
2020.webnlg-1.7,2020.acl-main.704,0,0.155201,"Missing"
2020.webnlg-1.7,W19-3706,1,0.842449,"Missing"
2020.webnlg-1.7,2006.amta-papers.25,0,0.292963,"Missing"
2020.webnlg-1.7,2020.webnlg-1.12,0,0.185192,"Missing"
2020.webnlg-1.7,2020.acl-main.224,0,0.302039,"cketed representations) are compared. Multi-tasking and pipeline architectures are also examined to analyse how different ways of integrating generation with document planning (triples order) impact performance. To help bridge the gap between the input graph and the output linear structure, a second phase of pre-training is applied using DocRED, a noisy parallel corpus of sentences and their automatically extracted relation (17K entries). Lexicalisation of RDF properties are also curated from the WebNLG+ and the DocRED datasets. 3.4 Bi-Directional, Monolingual Approaches Amazon AI (Shanghai). Zhao et al. (2020) introduced a two-step model for RDF-to-Text generation which combines a planner trained to learn the order in which triples should be verbalised and a decoder for verbalising each triple. Guo et al. (2020a) train Zhao et al. (2020)’s planner on the WebNLG+ dataset and use the pre-trained T5-large model to verbalise the linearised triples. For the Text-to-RDF task, entity linking is applied to the text and DBpedia is queried to retrieve the corresponding triples. CycleGT. Guo et al. (2020b) present a weakly supervised method where generation and semantic parsing models are learned by bootstrap"
2020.webnlg-1.7,2020.webnlg-1.22,0,0.0889317,"Missing"
2021.findings-acl.333,Q16-1037,0,0.215026,"al., 2019b), DistilBERT (Sanh et al., 2019), XLNet (Yang et al., 2019), etc. are excellent learners. They have been shown to capture a range of different types of linguistic information, from morphological (Edmiston, 2020) over syntactic (Hewitt and Manning, 2019) to lexico-semantic (Joshi et al., 2020). A particularly significant number of works study the degree to which these models capture and generalize over (i.e., learn to instantiate correctly in different contexts) syntactic phenomena, including, e.g., subject-verb agreement, long distance dependencies, garden path constructions, etc. (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2019; Wilcox et al., 2019a). However, most of these works focus on monolingual models, and, if the coverage of syntactic phenomena is considered systematically and in detail, it is mainly for English, as, e.g., (Hu et al., 2020a). This paper aims to shift the attention from monolingual to multilingual models and to emphasize the importance to also consider the syntactic phenomena of languages other than English when assessing the generalization potential of a model. More specifically, it systematically assesses how well multilingual models are capable"
2021.findings-acl.333,N19-1112,0,0.331757,"otential of the models on English and Spanish tests, comparing the syntactic abilities of monolingual and multilingual models on the same language (English), and of multilingual models on two different languages (English and Spanish). For English, we use the available SyntaxGym test suite; for Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntactic generalization capabilities of language models through the SyntaxGym online platform. 1 Introduction Transformer-based neural models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), DistilBERT (Sanh et al., 2019), XLNet (Yang et al., 2019), etc. are excellent learners. They have been shown to capture a range of different types of linguistic information, from morphological (Edmiston, 2020) over syntactic (Hewitt and Manning, 2019) to lexico-semantic (Joshi et al., 2020). A particularly significant number of works study the degree to which these models capture and generalize over (i.e., learn to instantiate correctly in different contexts) syntactic phenomena, including, e.g., subject-verb agreement, long distance dependencies, garden path constructions, etc. (Linzen et"
2021.gem-1.10,2020.acl-main.424,0,0.0251404,"les WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling prog"
2021.gem-1.10,2020.acl-main.766,0,0.0426234,"sh datasets were ranked lower, with only WebNLG and MLSum among the top 15 datasets. We grouped all datasets by their high-level tasks and selected a group that would not violate the selection principles (e.g., only high-resource tasks). If two datasets fit, we picked the one with a higher interest rating. Among the 11 datasets, we have 18different languages, and the dataset sizes range from 5,000 examples to 1.5M, with most datasets between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining (Anastasopoulos and Neubig, 2020). The high-level tasks include Dialog, Summarization, Data-to-Text, and Simplification. About half of the datasets have multiple references and more than half had post-processing steps applied to them to ensure high data quality. 3.1 GEMifying the data We produce data cards (Bender and Friedman, 2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-specific template.7 In addition to describing the data itself, the cards acknowledge potential limitations of a dataset regarding its creation process and describe its real-world use cases to ensure that the research is c"
2021.gem-1.10,W05-0909,0,0.165721,"otebook within 2-3 hours. 4.2 avoid overfitting to known metrics, we will use metrics on the test submissions that are not included in this initial writeup. Consequentially, the baseline results are an incomplete list which will be expanded upon the announcement of the test metrics. The set of metrics can be computed via the framework described at https://gem-benchmark. com/shared_task which comprises metrics in the following categories: Lexical Similarity. We include multiple “traditional” metrics as baseline metrics, notably BLEU (Papineni et al., 2002), ROUGE-1/2/L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). These metrics can often be gamed, for example, ROUGE can be improved by increased the output length of the model (Sun et al., 2019). Moreover, the reliability of these metrics depends on the quality and number of the references (Mathur et al., 2020a; Freitag et al., 2020). However, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 10"
2021.gem-1.10,W11-2832,0,0.0915033,"Missing"
2021.gem-1.10,2020.inlg-1.24,1,0.864971,"human evaluation standards. In recent work, Howcroft et al. (2020) investigated NLG papers from the last 2 For a more complete description of recent developments in NLG evaluation, we refer to the survey by Celikyilmaz et al. (2020). 99 twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most cases, it is not even mentioned what the human evaluation aims to measure and that definitions of measures like “accuracy” or “fluency” are inconsistent. They thus suggest reporting standards for criteria and methods, following a classification system proposed by Belz et al. (2020). In addition, regularly scheduled shared tasks like WMT have lead to standardization of human evaluation setups and enabled controlled experimentation with them. GEM has the opportunity to develop reproducible standards for how human evaluation for NLG tasks beyond translation should be conducted while at the same time incorporating lessons from related work. Acting on the same need, the recently proposed GENIE (Khashabi et al., 2021) system aims to automate and standardize the human evaluation of different NLG systems, however with the contrasting goal of reducing the evaluating to a leaderb"
2021.gem-1.10,Q18-1041,0,0.236179,"han half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard ("
2021.gem-1.10,W17-4755,0,0.0383891,"Missing"
2021.gem-1.10,W16-2302,0,0.0489314,"Missing"
2021.gem-1.10,N18-2097,0,0.0498492,"Missing"
2021.gem-1.10,N19-1423,0,0.0210653,"provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation appr"
2021.gem-1.10,P19-1483,1,0.811795,"tions. 106 Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek"
2021.gem-1.10,K19-1037,0,0.027404,"Missing"
2021.gem-1.10,P17-1123,0,0.0287395,"Missing"
2021.gem-1.10,2020.acl-main.454,1,0.84177,"LEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen"
2021.gem-1.10,W19-8652,1,0.897973,"Missing"
2021.gem-1.10,C16-1191,0,0.0653297,"Missing"
2021.gem-1.10,P18-1082,0,0.0705277,"Missing"
2021.gem-1.10,W16-3622,1,0.897392,"Missing"
2021.gem-1.10,P16-2008,1,0.869379,"Missing"
2021.gem-1.10,W19-8670,1,0.884454,"Missing"
2021.gem-1.10,2020.emnlp-main.393,0,0.179031,"le to be able to address newly found limitations, and that the benchmark should focus on climbing a leaderboard. Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench,1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able mu"
2021.gem-1.10,N19-1395,0,0.0587235,"Missing"
2021.gem-1.10,2020.emnlp-main.751,0,0.503468,"nly on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover,"
2021.gem-1.10,P19-1346,1,0.897232,"Missing"
2021.gem-1.10,2020.webnlg-1.7,1,0.843551,"Missing"
2021.gem-1.10,2020.findings-emnlp.195,1,0.835947,"Missing"
2021.gem-1.10,2020.emnlp-main.5,0,0.126308,"e machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differently across tasks, setups, and languages, a multi-task NLG benchmark has the opportunity to act as a testbed to evaluate how the latest advances in automated metrics perform on these different tasks. The benchmark can facilitate this research through the release of system outputs and associated human annotations, which is what we are planning to do with GEM. Moreover, we allow the integrat"
2021.gem-1.10,2020.emnlp-main.489,0,0.0422168,"les 3 and 4. Our interactive system is centered around a parallel coordinates plot (Inselberg, 1985) which shows all results as lines through parallel axes. Every line intersects the axes at the corresponding mapped value. For instance, see the red line representing the results for task “ToTTo” of baseline “t5-small”. Filters can be applied along axes (see BLEURT axis in Figure 2) and the filtered selection is highlighted through bold lines. A selection can be a set of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The closest prior work is by Fu et al. (2020) for namedentity recognition which allows similar filtering and sorting, but presents the results in a table. However, the parallel coordinates approach can scale to a much greater number of metrics than a table. Moreover, by using a parallel coordinates plot instead of a table, it is easy to spot patterns that span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that, for the T5 baseline on ToTTo, the diversity metrics score higher than other systems while scoring lower on reference-based metrics. Since we only have a single baseline for ToTTo, it i"
2021.gem-1.10,W17-3518,1,0.863605,"Missing"
2021.gem-1.10,N19-1169,1,0.923485,"eiter and Dale, 2000). These texts aim to fulfill an underlying communicative goal (e.g., to produce a summary of an article) while remaining faithful to the input information, fluent, grammatical, and natural-looking. An NLG system needs to be robust to shifts in the data distribution and be able to produce text in many different languages. Finally, it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent. These optimization objectives can often be conflicting (Hashimoto et al., 2019) and, as a result, evaluations that focus only on a single aspect may fail to recognize the drawbacks of a particular method. To demonstrate this trade-off, consider an improvement on the CNN-DM summarization dataset (Hermann et al., 2015; Nallapati et al., 2016) measured by the ROUGE-L metWe introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent"
2021.gem-1.10,2020.ngt-1.1,0,0.0253511,"first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution"
2021.gem-1.10,2020.inlg-1.23,1,0.841029,"Missing"
2021.gem-1.10,D15-1229,0,0.0462438,"Missing"
2021.gem-1.10,2020.acl-main.709,1,0.887061,"Missing"
2021.gem-1.10,2020.acl-main.560,0,0.077305,"on does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b; Liu et al., 2020a; McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and Wiki"
2021.gem-1.10,2020.evalnlgeval-1.4,0,0.0163557,"ation tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These inclu"
2021.gem-1.10,D18-1208,0,0.0650198,"Missing"
2021.gem-1.10,Q18-1023,0,0.0642232,"Missing"
2021.gem-1.10,2020.findings-emnlp.360,1,0.934785,"e a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling progress. and conducting in-depth ana"
2021.gem-1.10,D16-1128,0,0.0680679,"Missing"
2021.gem-1.10,2020.acl-main.703,0,0.214186,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,2020.acl-main.653,0,0.233553,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,N16-1014,0,0.0410993,"t at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These include the Shannon Entropy (Shannon and Weaver, 1963) over unigrams and bigrams (H1 , H2 ), the mean segmented type token ratio over segment lengths of 100 (MSTTR, Johnson, 1944), the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that only appear once across the entire test output (Unique1,2 , Li et al., 2016). focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated with a judgment. As part of this, we collect the number of parameters of a system, as suggested by Ethayarajh and Jurafsky (2020). For each task, we additionally report the vocabulary size over the output (|V|) and the mean output length of a system (Sun et al., 2019). 5 Results One of the central aims of GEM is to measure the progress in NLG without misrepresenting the complex interactions between the sometimes contradicting measures. We t"
2021.gem-1.10,2020.findings-emnlp.165,0,0.0883829,"Missing"
2021.gem-1.10,W04-1013,0,0.355472,"n be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets"
2021.gem-1.10,2020.acl-main.465,0,0.123121,"; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress. As datasets, metrics, and models improve, the benchmark environment will improve as well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics, and addressing discovered flaws in the experimental setup, as demonstrated in Figure 1. Making all model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption and incre"
2021.gem-1.10,2020.tacl-1.47,0,0.129414,"ich has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able much richer evaluation (as described in the next sections), and promote non-English datasets. In addition, it can ensure that the datasets created for those shared tasks continue being evaluated. Increasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is t"
2021.gem-1.10,2021.ccl-1.108,0,0.0502233,"Missing"
2021.gem-1.10,W15-4640,0,0.0777152,"Missing"
2021.gem-1.10,W18-6450,0,0.0170006,"ever, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automa"
2021.gem-1.10,W19-5302,0,0.0535855,"Missing"
2021.gem-1.10,2020.coling-main.420,0,0.022745,"s many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured quantities (Howcroft et al., 2020). Improving Data Improving Metrics Evaluation with gameable metrics Improving Models Consistent Human Eval Non-repeatable human evaluation Figure 1: The opportunities of living benchmarks and pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Ot"
2021.gem-1.10,2020.acl-main.448,0,0.212993,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.wmt-1.77,0,0.124839,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.acl-main.173,1,0.838186,"to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicti"
2021.gem-1.10,W18-3601,1,0.834682,"where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way"
2021.gem-1.10,2020.msr-1.1,1,0.821875,"Missing"
2021.gem-1.10,C18-1147,1,0.871186,"Missing"
2021.gem-1.10,2020.emnlp-main.466,0,0.0504036,"Missing"
2021.gem-1.10,D15-1238,0,0.013053,"d this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the in"
2021.gem-1.10,K16-1028,0,0.065799,"Missing"
2021.gem-1.10,D18-1206,1,0.894493,"Missing"
2021.gem-1.10,W17-5525,1,0.880333,"Missing"
2021.gem-1.10,P02-1040,0,0.114424,"n NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achi"
2021.gem-1.10,2020.emnlp-main.89,1,0.895725,"Missing"
2021.gem-1.10,W17-3537,1,0.910798,"trics saturate, we need to evaluate them on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmar"
2021.gem-1.10,2020.emnlp-main.185,0,0.0484252,"Missing"
2021.gem-1.10,2021.acl-long.186,0,0.0851423,"Missing"
2021.gem-1.10,P19-1195,0,0.0585453,"Missing"
2021.gem-1.10,N18-1012,0,0.0173282,"akers of low-resourced languages through a participatory research approach, as suggested by (∀ et al., 2020). Toward this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the result"
2021.gem-1.10,Q19-1016,0,0.0583227,"Missing"
2021.gem-1.10,J18-3002,0,0.0872491,"for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated"
2021.gem-1.10,2020.acl-main.442,0,0.137138,"hem on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generatio"
2021.gem-1.10,2020.emnlp-main.647,0,0.0370713,"Missing"
2021.gem-1.10,2021.emnlp-main.529,0,0.024648,"on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291 63.5 64.0 32.5 29.4 55.1 54.5 27.5 26.4 0.943 0.942 -0.400 -0.412 mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++ 0.229 0.23 0.233 0.229 0.152 0.151 0.167 47.3 48.1 51.3 52.1 13.6 13.8 9.7 28.6 28.8 30.0 31.3 0.0 0.0 0.0 43.0 44.2 46.4 47.3 13.6 13.8 9.7 17.9 17.1 17.5 17.0 0.03 0.03 0.03 0.895 0.898 0.902 0.905 0.6"
2021.gem-1.10,D19-1320,0,0.0210218,"ence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTSc"
2021.gem-1.10,2020.acl-main.704,1,0.824209,"er, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang e"
2021.gem-1.10,P19-1212,0,0.0526263,"Missing"
2021.gem-1.10,P19-1646,0,0.0614703,"Missing"
2021.gem-1.10,Q16-1029,1,0.821779,"in a news article en *25k Articles WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmar"
2021.gem-1.10,W15-3031,0,0.06329,"Missing"
2021.gem-1.10,W19-2303,0,0.167787,"of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured"
2021.gem-1.10,2020.nlp4convai-1.13,0,0.0861626,"Missing"
2021.gem-1.10,D16-1033,0,0.064867,"Missing"
2021.gem-1.10,2020.acl-main.450,0,0.015505,"2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291"
2021.gem-1.10,P18-1205,0,0.0245839,"nalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the input of the wider NLG research community. To do so, we will set up a yearly selec"
2021.inlg-1.25,2020.inlg-1.24,1,0.634095,"been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submis"
2021.inlg-1.25,2020.webnlg-1.7,1,0.833183,"Missing"
2021.inlg-1.25,P19-1483,0,0.0185913,"e to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and di"
2021.inlg-1.25,W19-8652,1,0.842123,"., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either"
2021.inlg-1.25,2020.inlg-1.19,1,0.814881,"e et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 acc"
2021.inlg-1.25,2020.findings-emnlp.76,0,0.032576,"ll achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 acc"
2021.inlg-1.25,2021.inlg-1.26,0,0.0115062,"3, and truncate 3 Experiments 6 https://pypi.org/project/text2num/ https://pypi.org/project/num2words/ 8 Due to space constraints, we do not list the results of 7 5 https://spacy.io 262 Error Type Mistake R P 5 Token R P NAME NUMBER WORD CONTEXT NOT_CHECKABLE OTHER 0.750 0.777 0.514 0.000 0.000 0.000 0.846 0.750 0.483 - 0.759 0.759 0.465 0.000 0.000 0.000 0.862 0.752 0.529 - Overall 0.691 0.756 0.550 0.769 Our Charles-UPF submission achieved the best results in the automatic metrics category, but there is still a gap with what humans can achieve, as shown by the Laval University submission’s (Garneau and Lamontagne, 2021) overall 0.841 recall and 0.879 precision. One way to improve our system would be to enrich the reference fact descriptions, by either inferring more information from the raw data, or by extracting additional data from external databases.2 Another option would be to add surrounding sentences to the context – this could help to resolve coreferences (e.g., if a player is referred to as ""He"") and to detect the CONTEXT errors. We also note that our approach requires the real system outputs manually annotated with errors in order to work well – using only synthetic data results in low recall (see T"
2021.inlg-1.25,2020.coling-main.218,0,0.0358257,"Missing"
2021.inlg-1.25,2020.inlg-1.14,0,0.0275566,"Missing"
2021.inlg-1.25,2020.webnlg-1.16,0,0.0372828,"NLG (Tian et al., 2019; Harkous et al., 2020; Filippova, 2020; Rebuffel et al., 2021). Neural systems are particularly unreliable on complex datasets such as Rotowire (Wiseman et al., 2017), where the task is to generate basketball match summaries from tabular data. Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluat"
2021.inlg-1.25,W07-0734,0,0.0155492,"n training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale we"
2021.inlg-1.25,W19-8643,0,0.0460402,"Missing"
2021.inlg-1.25,2021.ccl-1.108,0,0.0266333,"Missing"
2021.inlg-1.25,W19-8659,1,0.927622,"., 2020; Filippova, 2020; Rebuffel et al., 2021). Neural systems are particularly unreliable on complex datasets such as Rotowire (Wiseman et al., 2017), where the task is to generate basketball match summaries from tabular data. Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasne"
2021.inlg-1.25,D17-1238,1,0.833037,"models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternati"
2021.inlg-1.25,P02-1040,0,0.109437,"longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et"
2021.inlg-1.25,D19-1410,0,0.0426386,"Missing"
2021.inlg-1.25,2020.inlg-1.28,0,0.0195068,"guage inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submission for the shared task falls into the first category: we developed an automatic metric for token-le"
2021.inlg-1.25,W19-8610,0,0.0126242,"llam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less re"
2021.inlg-1.25,2020.acl-main.704,0,0.01747,"n recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shai"
2021.inlg-1.25,2020.inlg-1.22,0,0.160963,"content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submission for the shared task f"
2021.inlg-1.25,2021.inlg-1.23,0,0.0361876,"ifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submission for the shared task falls into the first category: we developed an automatic metric for token-level error annotation which"
2021.inlg-1.25,N19-1101,0,0.0546357,"Missing"
2021.inlg-1.25,D15-1199,0,0.0768463,"Missing"
2021.inlg-1.25,D17-1239,0,0.0256655,"on with a model trained on a mixture of human-annotated and synthetic data. 1 Introduction Recent neural NLG systems can easily generate fluent texts from linearized structured data (Zhao et al., 2020; Kale and Rastogi, 2020; Castro Ferreira et al., 2020). However, the systems cannot guarantee that the output is properly grounded in the input – hallucination (outputs not supported by input data) is a notorious problem in neural NLG (Tian et al., 2019; Harkous et al., 2020; Filippova, 2020; Rebuffel et al., 2021). Neural systems are particularly unreliable on complex datasets such as Rotowire (Wiseman et al., 2017), where the task is to generate basketball match summaries from tabular data. Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Cas"
2021.inlg-1.25,D19-5011,0,0.0120759,"e semantic similarity between gi and the evaluated sentence s using Sentence Transformers (Reimers and Gurevych, 2019).4 In particular, we embed the sentence tokens by applying mean pooling on the output of 4 https://www.sbert.net/ LM-based Error Tagger We use a RoBERTa LM (Liu et al., 2019) with a token-level classification head as our errorchecking model. Unlike unsupervised approaches based on examining attention values (Thorne et al., 2019; Li et al., 2020) or input perturbations (Kim et al., 2020), we train the model directly to predict error categories using annotated data, similarly to Yoosuf and Yang (2019). The model receives an input X = (C, s), composed of the context C, i.e., relevant background facts selected by context retrieval in Section 2.2, and the generated sentence s to be tagged. The inputs are separated by the delimiter </s>. The model is trained to annotate each token in s either with an OK label, or with a label corresponding to one of the error categories. We experiment with two data sources for training the model: (1) gold-standard annotated data 261 Generator Data synth Simple synth + human synth Compact synth + human c R EMR = 0.25 P F1 R EMR = 0.5 P F1 R EMR = 0.75 P F1 5 10"
2021.inlg-1.25,2020.acl-main.224,0,0.0904673,"Missing"
2021.inlg-1.30,2020.inlg-1.29,1,0.92967,"or Fluency, and Clarity had higher standard deviation. Clarity and Fluency ratings were higher, and their standard deviations lower, in the reproduction study than in the original study by substantial margins. Clarity had a higher degree of reproducibility than Fluency, as measured by the coefficient of variation. Data and code are publicly available.1 1 2 Introduction Recent years have seen growing interest in, and concern about, reproducibility across the Natural Language Processing (NLP) field. The ReproGen Shared Task on Reproducibility of Human Evaluations in Natural Language Generation (Belz et al., 2020a) was the first shared task to focus on reproducibility of human evaluations (rather than metrics). We report on our participation in ReproGen, where our contribution was in Track A, the Summary of the Evaluated System PASS (Personalized Automated Soccer texts System) is a modular data-to-text system that produces Dutch summaries of football matches and is a partial re-implementation of the GoalGetter system (Theune et al., 2001). Like GoalGetter, PASS is a template and rule-based system. Unlike GoalGetter, PASS (i) tailors the tone of football reports for supporters of one of the clubs in a"
2021.inlg-1.30,2020.inlg-1.24,1,0.903,"or Fluency, and Clarity had higher standard deviation. Clarity and Fluency ratings were higher, and their standard deviations lower, in the reproduction study than in the original study by substantial margins. Clarity had a higher degree of reproducibility than Fluency, as measured by the coefficient of variation. Data and code are publicly available.1 1 2 Introduction Recent years have seen growing interest in, and concern about, reproducibility across the Natural Language Processing (NLP) field. The ReproGen Shared Task on Reproducibility of Human Evaluations in Natural Language Generation (Belz et al., 2020a) was the first shared task to focus on reproducibility of human evaluations (rather than metrics). We report on our participation in ReproGen, where our contribution was in Track A, the Summary of the Evaluated System PASS (Personalized Automated Soccer texts System) is a modular data-to-text system that produces Dutch summaries of football matches and is a partial re-implementation of the GoalGetter system (Theune et al., 2001). Like GoalGetter, PASS is a template and rule-based system. Unlike GoalGetter, PASS (i) tailors the tone of football reports for supporters of one of the clubs in a"
2021.inlg-1.30,W16-6612,0,0.028565,"roGen, where our contribution was in Track A, the Summary of the Evaluated System PASS (Personalized Automated Soccer texts System) is a modular data-to-text system that produces Dutch summaries of football matches and is a partial re-implementation of the GoalGetter system (Theune et al., 2001). Like GoalGetter, PASS is a template and rule-based system. Unlike GoalGetter, PASS (i) tailors the tone of football reports for supporters of one of the clubs in a match, (ii) has a modular architecture, and (iii) uses templates informed by the MEmo FC (Multilingual Emotional Football Corpus) corpus (Braun et al., 2016). Data and Language Sources: Automatically scraped football match data from Goal.com,2 subsequently stored in XML-format, is used as input data, and the MEmo FC corpus as reference data. System Architecture: The PASS3 architecture is a data-to-text pipeline consisting of the following modules: (1) the governing module (used in slightly different versions for different report parts) processes topics one by one, and interacts with the other modules as necessary; (2) the topic collection module extracts topics from the match data and orders them; (3) the lookup module retrieves all matching templ"
2021.inlg-1.30,2020.inlg-1.23,1,0.822935,"Missing"
2021.inlg-1.30,W17-3513,0,0.0471286,"Missing"
bott-etal-2012-text,W09-1210,0,\N,Missing
bott-etal-2012-text,C96-2183,0,\N,Missing
bott-etal-2012-text,C10-1152,0,\N,Missing
bott-etal-2012-text,W00-1436,0,\N,Missing
bott-etal-2012-text,W11-2802,0,\N,Missing
bott-etal-2012-text,W11-1601,0,\N,Missing
bott-etal-2012-text,W11-1603,1,\N,Missing
bott-etal-2012-text,mille-wanner-2008-making,1,\N,Missing
bott-etal-2012-text,taule-etal-2008-ancora,0,\N,Missing
bott-etal-2012-text,W05-0610,0,\N,Missing
C10-1012,C00-1007,0,0.036318,"ent. Its disadvantage is that it requires at least syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators"
C10-1012,P98-1026,0,0.035183,"Missing"
C10-1012,P01-1024,0,0.0300827,"om the analyzer, which makes both approaches not directly comparable. 5.3 Discussion The overall performance of our SVM-based deep sentence generator ranges between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words within constituents, and the sec"
C10-1012,W96-0501,0,0.0268062,"ubsequent integration of other generation tasks such as referring expression generation, ellipsis generation, and aggregation. As a matter of fact, this generator instantiates the Reference Architecture for Generation Systems (Mellish et al., 2006) for linguistic generation. A more practical advantage of the presented deep stochastic sentence generator (as, in principle, of all stochastic generators) is that, if trained on a representative corpus, it is domainindependent. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al., 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources. Deep stochastic sentence realizers have the potential to become real off-the-shelf modules. Our realizer is freely available for download at http://www.recerca.upf.edu/taln. 3 We are currently working on a generation-oriented multilevel annotation of corpora for a number of languages. The corpora will be made available to the community. 105 Acknowledgments Many thanks to"
C10-1012,D08-1019,0,0.0243456,"syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, mul"
C10-1012,N09-2057,0,0.382889,"is the head, 2 if w2 is the head, etc. and else 0; dist is the position within the constituent; contains-? is a boolean value which is true if the sentence contains a question mark and false otherwise; pos-head is the position of the head in the constituent) 4.2 Dependency Tree Linearization Since we use unordered dependency trees as syntactic structures, our realizer has to find the optimal linear order for the lexemes of each dependency tree. Algorithm 4 shows our linearization algorithm. To order the dependency tree, we use a one classifier-approach for all languages—in contrast to, e.g., Filippova and Strube (2009), who use a two-classifier approach for German.1 The algorithm is again a beam search. It starts with an elementary list for each node of the dependency tree. Each elementary list is first extended by the children of the node in the list; then, the lists are extended stepwise by the children of the newly added nodes. If the number of lists during this procedure exceeds the threshold of 1000, the lists are sorted in accordance with their score, and the first 1000 are kept. The remaining lists are removed. Afterwards, the score of each list is adjusted according to a global score function which"
C10-1012,P01-1029,0,0.0295193,"or is directly derived from the analyzer, which makes both approaches not directly comparable. 5.3 Discussion The overall performance of our SVM-based deep sentence generator ranges between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words wi"
C10-1012,W09-1201,0,0.0950376,"Missing"
C10-1012,P09-1091,0,0.394975,"ore (ULA) is the proportion of correct tokens that are assigned the correct head. To assess the quality of linearization, we use three different evaluation metrics. The first metric is the per-phrase/per-clause accuracy (acc snt.), which facilitates the automatic evaluation of results: acc = correct constituents all constituents As second evaluation metric, we use a metric related to the edit distance: di = 1 − m total number of words (with m as the minimum number of deletions combined with insertions to obtain the correct order (Ringger et al., 2004)). To be able to compare our results with (He et al., 2009) and (Ringger et al., 2004), we use the BLEU score as a third metric. For the asessment of the quality of the word form generation, we use the accuracy score. The accuracy is the ratio between correctly generated word forms and the entire set of generated word forms. For the evaluation of the sentence realizer as a whole, we use the BLEU metric. 5.2 Experimental Results Table 4 displays the results obtained for the isolated stages of sentence realization and of the realization as a whole, with reference to a baseline and to some state-of-the-art works. The baseline is the deep sentence realiza"
C10-1012,P95-1034,0,0.032851,"syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increa"
C10-1012,P98-1116,0,0.894255,"suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increasingly available for multiple"
C10-1012,W02-2103,0,0.227142,"t it requires at least syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically an"
C10-1012,A97-1039,0,0.0615238,"ion tasks such as referring expression generation, ellipsis generation, and aggregation. As a matter of fact, this generator instantiates the Reference Architecture for Generation Systems (Mellish et al., 2006) for linguistic generation. A more practical advantage of the presented deep stochastic sentence generator (as, in principle, of all stochastic generators) is that, if trained on a representative corpus, it is domainindependent. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al., 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources. Deep stochastic sentence realizers have the potential to become real off-the-shelf modules. Our realizer is freely available for download at http://www.recerca.upf.edu/taln. 3 We are currently working on a generation-oriented multilevel annotation of corpora for a number of languages. The corpora will be made available to the community. 105 Acknowledgments Many thanks to the three anonymous reviewers for thei"
C10-1012,W00-0306,0,0.0191148,"Missing"
C10-1012,J05-1004,0,0.0546622,"0’ for “A0 realized as a relative clause”, and ‘AM-MNR’ for “manner modifier”. As can be seen, 6 out of the total of 14 edges in the complete representation of this example have been added by Algorithm 1. We still did not finish the formal evaluation of the principal changes necessary to adapt the PropBank annotation for generation, nor the quality of our completion algorithm. However, the need of an annotation with generation in mind is obvious. Completing the Semantic Annotation The semantic annotation of sentences in CoNLL ’09 shared task corpora follows the PropBank annotation guidelines (Palmer et al., 2005). Prob99 a Algorithm 1: Complete semantic graph //si is a semantic graph and yi a dependency tree // si = hNsi , Lsi , Esi i, where Nsi is the set of nodes // Lsi the set of edge labels // Esi ⊆ Ns × Ns × Ls is the set of edges for i ← 1 to |I |// iteration over the training examples let ry ∈ yi be the root node of the dependency tree // initialization of the queue nodeQueue ← children(ry ) while nodeQueue 6= ∅ do ny ← removeFirst(nodeQueue) // breath first: add nodes at the end of the queue nodeQueue ← nodeQueue ∪ children(ny ) nys ← sem(ny ); pys ← sem(parent(ny )) //get the semantic equival"
C10-1012,C04-1097,0,0.73441,"n The morphological realization algorithm selects the edit script in accordance with the highest score for each lemma of a sentence obtained during training (see Algorithm 2 above) and applies then the scripts to obtain the word forms; cf. Algorithm 5. Table 2 lists the feature schemas used for morphological realization. 5 Experiments To evaluate the performance of our realizer, we carried out experiments on deep generation of Chinese, English, German and Spanish, starting from CoNLL ’09 shared task corpora. The size of the test sets is listed in Table 3.2 2 As in (Langkilde-Geary, 2002) and (Ringger et al., 2004), we used Section 23 of the WSJ corpus as test set for English. 102 Algorithm 3: Semantic generation Algorithm 4: Dependency tree linearization //si , y semantic graph and its dependency tree for i ← 1 to |I |// iteration over the training examples // build an initial tree for all n1 ∈ si do trees ← {} // initialize the constructed trees list for all n2 ∈ si do if n1 6= n2 then for all l ∈ dependency-labels do trees = trees ∪ {(synt(n1 ),synt(n2 ),l)} trees ← sort-trees-descending-to-score(trees) trees ← look-forward(1000,sublist(trees,20)) //assess at most 1000 edges of the 20 best trees tree"
C10-1012,E09-1097,0,0.189557,"s between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words within constituents, and the second (which is a maximum entropy classifier) determines the order of constituents that depend on a finite verb. For English, we achieve with our"
C10-1012,C98-1112,0,\N,Missing
C10-1012,C98-1026,0,\N,Missing
C10-1012,W01-0520,0,\N,Missing
C12-2082,W09-1210,0,0.0321107,"tion of the corpus with the most detailed tagset of 60 SyntRels has been obtained from the original annotation in AnCora (Taulé et al., 2008), which has been adapted, revised and enriched manually. Starting from the most fine-grained annotation, we derived automatically the other three, ending up with four different treebanks for the same corpus. Four reference parsers have been used. Three of them are the top three parsers for Spanish in the CoNLL Shared Task 2009 (Hajiˇc et al., 2009): Che’s (Che et al., 2009), henceforth Che, Merlo’s (Gesmundo et al., 2009), henceforth Merlo, and Bohnet’s (Bohnet, 2009), henceforth Bohnet. The fourth, the Malt Parser (Nivre et al., 2007), henceforth Malt, has been chosen because it is a very broadly used syntactic dependency parser. Malt and Merlo are transition based, while Bohnet and Che are graph based. In our experiments, all of them processed non-projective dependency trees. Each parser contains its own configuration options, which depend on the parsing approach, the learning techniques, etc. Therefore, it was not possible to apply the same setup to all parsers. Instead, we used for each parser its own default configuration, which does not guarantee an"
C12-2082,bosco-etal-2000-building,0,0.131616,"rs and show that the precision and recall of hard-to-parse relations can be quite different, depending on the tag granularity in the annotation, that is, if the annotation contains or not morphological and/or semantic information. In contrast, our goal is to provide evidence that the creation of annotations that capture significant fine-grained distinctive features of the grammar (and only the grammar) of a language does not need to harm significantly the performance of the parsers. Consider as two 3 Some other works present a hierarchical organization of grammatical relations (in particular (Bosco et al., 2000), (Briscoe et al., 2002), and (Marneffe et al., 2006)), but those hierarchies are not used to test the impact of the tagset granularity on the results of a parser. 843 such fine-grained distinctive features the relations modal and direct-object in the following two sentences. As indicated, only the direct object can be pronominalized by a clitic pronoun and moved before the governing verb, without that a pro-verb is needed: Juan puede-modal→ venir mañana, lit. ‘John might come tomorrow’ (Juan lo puede *(hacer)), and Juan puede-dobj→ venir mañana, lit. ‘John is able to come tomorrow’ (Juan lo p"
C12-2082,bosco-etal-2010-comparing,0,0.0506274,"Missing"
C12-2082,W06-2920,0,0.135168,"toolkit provided with the parser. For the other parsers, we used the official CoNLL’06 evaluation toolkit. The LAS figures for each parser and for each version of the annotation are 4 One can always imagine some statistical “disambiguation” based on the context in which the construction is used, but the amount of data needed could be prohibitive—at least for Spanish—and eventually, the only way would probably be to imply human experts for the revision of the annotation. 5 Bohnet’s parser uses CoNLL’09 14-column format, while the other three need to be trained on the CoNLL’06 10-column format (Buchholz and Marsi, 2006), but the available information is exactly the same, whatever the format: word positions, word forms, PoS, lemmas, (all of which kept the same in our experiments), and dependencies. 844 shown in Table 2. The graphic on the right of Table 2 shows how each parser reacts to and how its performance varies with the increasing number of relations in the tagset. We can observe that all four parsers behave similarly: their accuracy is very constant from 15 to 44 SyntRels, and decreases with 60 SyntRels. We also notice that there is a significant difference between Bohnet, Merlo and Malt’s LAS progress"
C12-2082,W09-1207,0,0.0292797,"of the experiments In our experiments, we used the four tagsets introduced in Section 2. The annotation of the corpus with the most detailed tagset of 60 SyntRels has been obtained from the original annotation in AnCora (Taulé et al., 2008), which has been adapted, revised and enriched manually. Starting from the most fine-grained annotation, we derived automatically the other three, ending up with four different treebanks for the same corpus. Four reference parsers have been used. Three of them are the top three parsers for Spanish in the CoNLL Shared Task 2009 (Hajiˇc et al., 2009): Che’s (Che et al., 2009), henceforth Che, Merlo’s (Gesmundo et al., 2009), henceforth Merlo, and Bohnet’s (Bohnet, 2009), henceforth Bohnet. The fourth, the Malt Parser (Nivre et al., 2007), henceforth Malt, has been chosen because it is a very broadly used syntactic dependency parser. Malt and Merlo are transition based, while Bohnet and Che are graph based. In our experiments, all of them processed non-projective dependency trees. Each parser contains its own configuration options, which depend on the parsing approach, the learning techniques, etc. Therefore, it was not possible to apply the same setup to all parse"
C12-2082,W09-1205,0,0.0323256,"sed the four tagsets introduced in Section 2. The annotation of the corpus with the most detailed tagset of 60 SyntRels has been obtained from the original annotation in AnCora (Taulé et al., 2008), which has been adapted, revised and enriched manually. Starting from the most fine-grained annotation, we derived automatically the other three, ending up with four different treebanks for the same corpus. Four reference parsers have been used. Three of them are the top three parsers for Spanish in the CoNLL Shared Task 2009 (Hajiˇc et al., 2009): Che’s (Che et al., 2009), henceforth Che, Merlo’s (Gesmundo et al., 2009), henceforth Merlo, and Bohnet’s (Bohnet, 2009), henceforth Bohnet. The fourth, the Malt Parser (Nivre et al., 2007), henceforth Malt, has been chosen because it is a very broadly used syntactic dependency parser. Malt and Merlo are transition based, while Bohnet and Che are graph based. In our experiments, all of them processed non-projective dependency trees. Each parser contains its own configuration options, which depend on the parsing approach, the learning techniques, etc. Therefore, it was not possible to apply the same setup to all parsers. Instead, we used for each parser its own defa"
C12-2082,W07-2416,0,0.0815438,"s to control the level of granularity of the tagset. We do not orientate our scheme towards any particular linguistic theory; the selected criteria are dictated by syntactic behaviour observed in the language in question (in our case, Spanish). For instance, for dependents of verbs, we need to capture whether they can be cliticized, promoted 1 The dependency annotation scheme of the Penn Treebank has served as blueprint for annotation schemes of a series of treebanks in different languages and is thus a de facto standard. See (Marcus et al., 1993) for the original consituency annotation, and (Johansson and Nugues, 2007) for the conversion to one-word-per-line dependency representations. 2 “Minimal” refers here not only to the number of tags, but also to the level of precision of the syntactic tags. Indeed, many corpora mix several levels of representation (e.g., syntax, semantics, lexicon, etc.) such that the number of syntactic relations does not necessarily reflect the level of idiosyncracy of the annotation. 842 or demoted, etc. For any kind of dependent, we need to capture the canonical order with respect to its governor, the part-of-speech of the governor, the part-of-speech of the prototypical element"
C12-2082,P03-1054,0,0.00546165,"obl-obj1/2/3 and noun-compl. In the third column (31 SyntRels), obl-obj and agent are fused into one relation obl-obj, defined as “prepositional object which cannot be pronominalized”. Finally, in the last column (15 SyntRels), one tag OOBJ gathers any object which cannot be pronominalized, as opposed to IOBJ and DOBJ, which can be replaced by a dative and an accusative pronoun, respectively. 3 Experiments 3.1 Background A number of experiments on different granularities of annotation and their impact on the performance of probabilistic parsers are known from the literature; see in particular Klein and Manning (2003) and Petrov et al. (2006), who show the benefits of splitting generic part-of-speech tags (e.g., NP, VP, etc.) into more precise subcategories for the derivation of accurate probabilistic context-free grammars (PCFG). Our proposal differs from these works in that they focus on constituency parsing and part-of-speech tags, whereas we tackle dependency parsing and edge labels.3 But more importantly, the goals are different. Thus, they target the improvement of parsing accuracy, and for that they infer, with simple rules, from the training data (categorial) information which is more specific than"
C12-2082,J93-2004,0,0.0388651,"into the scheme. Using more or less fine-grained criteria allows us to control the level of granularity of the tagset. We do not orientate our scheme towards any particular linguistic theory; the selected criteria are dictated by syntactic behaviour observed in the language in question (in our case, Spanish). For instance, for dependents of verbs, we need to capture whether they can be cliticized, promoted 1 The dependency annotation scheme of the Penn Treebank has served as blueprint for annotation schemes of a series of treebanks in different languages and is thus a de facto standard. See (Marcus et al., 1993) for the original consituency annotation, and (Johansson and Nugues, 2007) for the conversion to one-word-per-line dependency representations. 2 “Minimal” refers here not only to the number of tags, but also to the level of precision of the syntactic tags. Indeed, many corpora mix several levels of representation (e.g., syntax, semantics, lexicon, etc.) such that the number of syntactic relations does not necessarily reflect the level of idiosyncracy of the annotation. 842 or demoted, etc. For any kind of dependent, we need to capture the canonical order with respect to its governor, the part-"
C12-2082,de-marneffe-etal-2006-generating,0,0.0641374,"to-parse relations can be quite different, depending on the tag granularity in the annotation, that is, if the annotation contains or not morphological and/or semantic information. In contrast, our goal is to provide evidence that the creation of annotations that capture significant fine-grained distinctive features of the grammar (and only the grammar) of a language does not need to harm significantly the performance of the parsers. Consider as two 3 Some other works present a hierarchical organization of grammatical relations (in particular (Bosco et al., 2000), (Briscoe et al., 2002), and (Marneffe et al., 2006)), but those hierarchies are not used to test the impact of the tagset granularity on the results of a parser. 843 such fine-grained distinctive features the relations modal and direct-object in the following two sentences. As indicated, only the direct object can be pronominalized by a clitic pronoun and moved before the governing verb, without that a pro-verb is needed: Juan puede-modal→ venir mañana, lit. ‘John might come tomorrow’ (Juan lo puede *(hacer)), and Juan puede-dobj→ venir mañana, lit. ‘John is able to come tomorrow’ (Juan lo puede (hacer)). If the annotation of the relations doe"
C12-2082,P06-1055,0,0.0293544,"In the third column (31 SyntRels), obl-obj and agent are fused into one relation obl-obj, defined as “prepositional object which cannot be pronominalized”. Finally, in the last column (15 SyntRels), one tag OOBJ gathers any object which cannot be pronominalized, as opposed to IOBJ and DOBJ, which can be replaced by a dative and an accusative pronoun, respectively. 3 Experiments 3.1 Background A number of experiments on different granularities of annotation and their impact on the performance of probabilistic parsers are known from the literature; see in particular Klein and Manning (2003) and Petrov et al. (2006), who show the benefits of splitting generic part-of-speech tags (e.g., NP, VP, etc.) into more precise subcategories for the derivation of accurate probabilistic context-free grammars (PCFG). Our proposal differs from these works in that they focus on constituency parsing and part-of-speech tags, whereas we tackle dependency parsing and edge labels.3 But more importantly, the goals are different. Thus, they target the improvement of parsing accuracy, and for that they infer, with simple rules, from the training data (categorial) information which is more specific than what is directly availab"
C12-2082,D07-1066,0,0.022075,"Missing"
C12-2082,taule-etal-2008-ancora,0,0.460298,"t it is possible to reach a good balance between the accuracy of a parser and the richness of the linguistic annotation. They also show that the principles that we applied when designing the hierarchical annotation schema are valid and may be used for the design of other annotation schemes in the future. 2 Hierarchical syntactic annotation scheme The hierarchical annotation scheme in Table 1 has been developed for Spanish on a small corpus of 3513 sentences (100892 words, see (Mille et al., 2009); corpus available at UPF–TALN webpage), which constitutes a section of the Spanish corpus AnCora (Taulé et al., 2008). The general idea underlying this scheme is to apply only syntactic (rather than also semantic) criteria in order to identify each grammatical tag that is to be introduced into the scheme. Using more or less fine-grained criteria allows us to control the level of granularity of the tagset. We do not orientate our scheme towards any particular linguistic theory; the selected criteria are dictated by syntactic behaviour observed in the language in question (in our case, Spanish). For instance, for dependents of verbs, we need to capture whether they can be cliticized, promoted 1 The dependency"
C12-2082,W09-1201,0,\N,Missing
C14-1133,W13-2322,0,0.0603372,"Missing"
C14-1133,D12-1133,1,0.846199,"tically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set. POS LEMMA LAS UAS 96.14 91.10 78.64 86.49 Table 1: Results of Bohnet and Nivre’s surface-syntactic parser on the development set In what follows, we first present the realization of the SSyntS–DSyntS transducer and then the realization of the baseline. 3.1 SSyntS–DSyntS transducer As outlined in Section 2.2, the SSyntS–DSyntS transducer is composed of three s"
C14-1133,W06-2920,0,0.0316683,"c parsing pipeline 5 Related Work To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated synta"
C14-1133,J08-1003,0,0.0551117,"Missing"
C14-1133,W09-1207,0,0.0198429,"also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules i"
C14-1133,J07-4004,0,0.0802815,"Missing"
C14-1133,fillmore-etal-2002-framenet,0,0.0465585,"matical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attributive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank and Semantic Frame structures are not always connected, may contain either individual lexical items or phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or sentential). In other words, they"
C14-1133,W09-1205,0,0.0190178,"int parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997). However, this is not to say that the idea of the surface→surface syntax→deep syntax pipeline is"
C14-1133,P03-1046,0,0.0820135,"Missing"
C14-1133,W12-3602,0,0.0984375,", publish-COORD→or-II→perish, and so on. APPEND subsumes all parentheticals, interjections, direct addresses, etc., as, e.g., in Listen, John!: listen-APPEND→John. DSyntSs thus show a strong similarity with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated; (ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv) they are connected.4 A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al., 2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows: Definition 2 (DSyntS) An DSyntS of a language L is a quintuple TDS = hN, A, λls →n , ρrs →a , γn→g i defined over the full lexical items Ld of L, the set of semantic grammemes Gsem , and the set of deepsyntactic relations Rdsynt , where • the set N of nodes and the set A of directed arcs form a connected tree, • λls →n assigns to each n ∈ N an ls ∈ Ld , • ρrs →a assigns to each a ∈ A an r ∈ Rdsynt , and • γn→g assigns to each λls →n (n) a set of grammemes Gt ∈ Gsem . Consider in Figure 1 an example for an"
C14-1133,W07-2416,0,0.483157,"(forests of trees defined over individual lexemes or phrasal chunks and abstract semantic role labels which capture the argument structure of predicative elements, dropping all attributive and coordinative dependencies). We propose a parser that delivers deep syntactic structures as output. 1 Introduction Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from l"
C14-1133,W08-2123,0,0.0217204,"rsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but"
C14-1133,P86-1038,0,0.257327,"proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt , i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for verbs. The value of lexssynt can be any (either full or functional) lexical item; in graphical representations of SSyntSs, usually only the value of lexssynt is shown. The edge labels of a SSyntS are grammatical functions ‘subj’, ‘dobj’, ‘det’, ‘modif’, etc. In other words, SSyntSs are syntactic structures of the kind as encountered"
C14-1133,C12-2082,1,0.889085,"ucer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf."
C14-1133,W13-3724,1,0.686421,"less than a dozen grammemes, etc. 3 Experiments In order to validate the outlined SSyntS–DSyntS transduction and to assess its performance in combination with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of 1405 experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentenc"
C14-1133,C69-0101,0,0.518574,"r outcome. Section 5 summarizes the related work, before in Section 6 some conclusions and plans for future work are presented. 2 Fundamentals of SSyntS–DSyntS transduction Before we set out to discuss the principles of the SSyntS–DSynt transduction, we must specify the DSyntSs and SSyntSs as used in our experiments. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt ,"
C14-1133,W08-2121,0,0.228819,"Missing"
C14-1133,taule-etal-2008-ancora,0,0.0989872,"Missing"
C14-1133,P08-1101,0,0.0611179,"Missing"
C14-1133,W09-1201,0,\N,Missing
C14-1133,P01-1033,0,\N,Missing
C14-1133,J05-1004,0,\N,Missing
C14-1133,Q13-1018,0,\N,Missing
C14-1133,D07-1096,0,\N,Missing
C14-1133,P13-2017,0,\N,Missing
C14-1133,ballesteros-nivre-2012-maltoptimizer-system,1,\N,Missing
D19-6301,W13-3520,0,0.0329649,"e, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track structures using a series of gra"
D19-6301,P11-2040,1,0.829216,"availability of evaluators: four Shallow Track in-domain datasets (Chinese-GSD, English-EWT, RussianSynTagRus, Spanish-AnCora), one Shallow Track dataset coming from parsed data (SpanishAnCoraHIT ) and one (in-domain) Deep Track dataset (English-EWT). As in SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), we assessed two quality criteria in the human evaluations, in separate evaluation experiments, Readability and Meaning Similarity, and used continuous sliders as rating tools, the evidence being that raters tend to prefer them 14 Thank you to Yevgeniy Puzikov for pointing this out. (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were first given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the grey text’), was as follows: The meaning of the grey text is adequately"
D19-6301,W11-2832,1,0.65823,"ncies.org/ Bernd Bohnet Google Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from whic"
D19-6301,K17-3005,0,0.0303018,"bed by syntactic structure or agreement (such as verbal finiteness or verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the tra"
D19-6301,D19-6302,0,0.0736613,"tion is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions app"
D19-6301,D19-6303,0,0.0332354,"ighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised sta"
D19-6301,W18-3606,0,0.194017,"andard scores (or z-scores) computed on the set of all raw scores by the given evaluator using each evaluator’s mean and standard deviation. For both raw and standard scores, we compute the mean of sentence-level scores. Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.15 4 Overview of Submitted Systems ADAPT is a sequence to sequence model with dependency features attached to word embeddings. A BERT sentence classifier was used as a reranker to choose between different hypotheses. The implementation is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ord"
D19-6301,W18-3604,0,0.0728895,"nto a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. Baseline: In order to set a lower boundary for the automatic and human evaluations, a simple English baseline consisting of 7 lines of python code was implemented16 . It generates from a UD file with an in-order traversal of the tree read by pyconll and outputting the form of each node. 5 Evaluation results There were 14 submissions to the task, of which two were withdrawn; 9 teams participa"
D19-6301,D19-6310,0,0.021759,"tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. B"
D19-6301,D19-6304,0,0.339584,"Missing"
D19-6301,K18-2014,0,0.041076,"verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the training data, the alignments with the tokens of the Shallow Track struct"
D19-6301,D19-6311,0,0.0501754,"ter RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projec"
D19-6301,W04-2705,0,0.268698,"Missing"
D19-6301,W18-3601,1,0.502146,"Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from which functional words (in particul"
D19-6301,W15-4719,0,0.125325,"The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, whic"
D19-6301,J05-1004,0,0.0811396,"n the English-gum dataset);8 3. The lines corresponding to combined lexical units (e.g. Spanish “del” &lt;de+el&gt; lit. ’of.the’) and the contents of columns [9] and [10] were removed; 4. Information about the relative order of components of named entities, multiple coordinations and punctuation signs was added in the FEATS column (dependency relations compound, compound:prt, compound:svc, flat, flat:foreign, flat:name, fixed, conj, punct); For the Deep Track, the following steps were additionally carried out: 5. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the 8 Thank you to Guy Lapalme for spotting this. syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation. See also the inventory of relations in Table 2. 6. Functional prepositions"
D19-6301,P02-1040,0,0.108135,"a que los nuevos miembros del CNE deben tener experiencia para “dirigir procesos complejos”. In the original UD files, the reference sentences are by default detokenised. In order to carry out the evaluations of the tokenised outputs, we built a tokenised version of the reference sentences by concatenating the words of the second column of the UD structures (see Figure 1) separated by a whitespace. 3 Evaluation Methods 3.1 Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST13 is a related n-gram similarity metric 13 http://www.itl.nist.gov/iad/mig/ tests/mt/doc/ngram-study.pdf; http:// www.itl.nist.gov/iad/mig/tests/mt/2009/ weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn t"
D19-6301,N18-1202,0,0.019743,"however, permissible. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track"
D19-6301,D19-6312,0,0.0504468,"els use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to group the words to be contracted, and then generate the contracted word form of each group with a seq2seq model. The LORIA submission (Shimorina and Gardent, 2019) presents a modular approach to surface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. Th"
D19-6301,K18-2011,1,0.820368,"Missing"
D19-6301,D19-6309,0,0.0269112,"rface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. The OSU-FB pipeline for generation (Upasani et al., 2019) starts by generating inflected word forms in the tree using character seq2seq models. These inflected syntactic trees are then linearised as constituent trees by converting the relations to non-terminals. The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity)."
D19-6301,D19-6306,0,0.0857639,"weighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to g"
D19-6313,W13-2322,0,0.662131,"eiter and Dale, 2000). This area has gained relevance in the Natural Language Processing community and in the industry in the last years. There are several works and efforts in NLG for English.1 Recently, shared-tasks focused on NLG from semantic representations have gained the attention of the NLG community. Thus, several representations have emerged for attending different contexts. For example, the RDF-based representation presented by Gardent et al. (2017) in its WebNLG challenge, the dialog-act-based representation presented by Novikova et al. (2016), and Abstract Meaning Representation (Banarescu et al., 2013). 1 Most of the work may be found at https://aclweb. org/anthology/sigs/siggen/. 2 Available at https://catalog.ldc.upenn. 94 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 94–103 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chinese corpus, containing 10,149 annotated sentences in its first version.3 This difficulty to get large corpora with this kind of annotation (due to the difficult and expensive annotation task that it represents) constrains the development of research in other"
D19-6313,W17-3501,0,0.133797,"t the performance decreases in each cut as the cut quality decreases as well. • training on cut 1 plus each cut included progressively. At the beginning, the training set was composed by the cut 1. Then, a lower quality cut was added to the training set at each training phase until all the cuts were included. The goal of this experiment was to evaluate how the method performance varied when lower quality data was inserted into the training set. Experiments Experiments were performed using the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) methods provided by Castro Ferreira et al. (2017) to compare how each method behaved in the evaluated context. The SMT method used the same parameters proposed by Castro Ferreira et al. (2017) and a 5-gram language model trained on the BP corpus provided by Hartmann et al. (2017). Also, the AMR graph pre-processing comprised a compression and a pre-ordering step without delexicalization (described as -Delex+Compress+Preorder in the original paper) as this configuration got one of the best results. The NMT method used similar parameters to Castro Ferreira et al. (2017). The encoder was bidirectional RNN with GRU, each with a 1024D hidden unit"
D19-6313,N18-1104,0,0.343797,"e fact that there are less datasets in the deep track is directly related to the higher complexity of the conversion compared to the shallow track, for which a superficial processing (basically order randomization) is sufficient. Among the efforts to build or adapt semantic representations for non-English languages, it is possible to cite Abstract Meaning Representation (AMR) as an example. Although AMR was not born as an interlingua, several works have tried to use it in that way to annotate sentences in other languages like Chinese and Czech (Xue et al., 2014), Italian, Spanish, and German (Damonte and Cohen, 2018) and Brazilian Portuguese (Anchiˆeta and Pardo, 2018). Other works have tried to adapt the English AMR guidelines to Spanish and Brazilian Portuguese with some success (Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). However, most of these works report a small number of AMRannotated sentences (compared to the English corpus) and are restricted to some domains like tales (“The Little Prince”). To the best of our knowledge, the only AMR-annotated corpus comparable (in terms of size) to the English corpus2 is the This paper presents an exploratory study that aims to evaluate"
D19-6313,S15-1026,0,0.0228902,"Missing"
D19-6313,N13-1073,0,0.0260316,"Missing"
D19-6313,D18-1045,0,0.0604478,"Missing"
D19-6313,P17-1017,0,0.0225501,"is the research area that aims to give to the computers the ability to generate texts in human language from some underlying representation of information (Reiter and Dale, 2000). This area has gained relevance in the Natural Language Processing community and in the industry in the last years. There are several works and efforts in NLG for English.1 Recently, shared-tasks focused on NLG from semantic representations have gained the attention of the NLG community. Thus, several representations have emerged for attending different contexts. For example, the RDF-based representation presented by Gardent et al. (2017) in its WebNLG challenge, the dialog-act-based representation presented by Novikova et al. (2016), and Abstract Meaning Representation (Banarescu et al., 2013). 1 Most of the work may be found at https://aclweb. org/anthology/sigs/siggen/. 2 Available at https://catalog.ldc.upenn. 94 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 94–103 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chinese corpus, containing 10,149 annotated sentences in its first version.3 This difficulty to get lar"
D19-6313,L18-1157,1,0.884612,"Missing"
D19-6313,N18-3017,0,0.0217459,"nglish languages on it. Our methodology for generating corpus and the experiments performed are presented in §4. Furthermore, §5 contains the results and a discussion about the results. Finally, the conclusions and future work are presented in §6. 2 Related Work Several works have proven the usefulness of translating corpora to increase the dataset size and improve the performance of their models. For example, Klinger and Cimiano (2015) used Phrasebased MT and some quality estimation measures to build a corpus with the best translations and use it in Sentiment Analysis. Misu et al. (2012) and Gaspers et al. (2018) explored back-translation in Natural Language Understanding systems using different measures. Misu et al. (2012) showed that BLEU is not a good quality measure and Gaspers et al. (2018) used measures from alignments, machine translation and language models to select the best sentences to be included in the corpus. Monsalve et al. (2019) also explored some quality measures (BLEU and METEOR) to select the best sentences and build a non-English corpus for Reading Comprehension and Word Sense Disambiguation. Among the results, they showed that despite the introduction of low-quality sentences, th"
D19-6313,J05-1004,0,0.050963,"malism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph (Banarescu et al., 2013). This representation includes information about semantic roles, named entities, spatial-temporal information, and co-references, among other information. AMR-annotated sentences may be represented using logic forms, PENMAN notation, and graphs (Figure 1). AMR has gained relevance in the research community due to its attempt to abstract away from syntactic idiosyncrasies5 and its wide use of other comprehensive linguistic resources, such as PropBank (Palmer et al., 2005).6 The current AMR-annotated corpus for English contains 39,260 sentences. Some efforts have been performed to build a corpus for Non-English languages leveraging the alignments and the parallel corpora that exist and trying to consider AMR an interlingua (Xue et al., 2014; Damonte and Cohen, 2018; Anchiˆeta and Pardo, 2018). Other works tried to adapt the AMR guidelines to other languages (Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). For Brazilian Portuguese, there are two AMRannotated corpora, one automatically built from the alignments between the sentences of the “T"
D19-6313,W17-6615,0,0.156579,"Missing"
D19-6313,P02-1040,0,0.10381,"(2017). The encoder was bidirectional RNN with GRU, each with a 1024D hidden unit. Source and target word embeddings were 300D each and both were trained jointly with It is worth noting that each configuration was performed using the cuts generated by F and METEOR to evaluate the quality measure in the corpus selection task. The test was performed on the automatically generated test set described in §4.1.1. In order to compare the results in a real context, the methods were also evaluated on the AMR-annotated BP dataset described in §3. Similar to Castro Ferreira et al. (2017), we used BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as metrics to evaluate fluency, adequacy and postediting efforts of the models, respectively. 98 Eu0 posso1 trabalhar2 no3 meu 4 tópico 5 de6 pesquisa 7 atual 8 . 9 I 0 can 1 work 2 on 3 my 4 current 5 research 6 topic 7 . 8 (a) Alignments between English and Brazilian Portuguese sentences (p / possible-01~e.1 :ARG1 (w / work-01~e.2 :ARG0 (i / i~e.0,4) :ARG1~e.3 (t / topic~e.7 :mod (r / research-01~e.6 :ARG0 i) :time (c / current~e.5)))) (p / possible-01~e.1 :ARG1 (w / trabalhar-01~e.2 :ARG0 (i / eu-eu~e.4,0) :ARG1~e.3 (t / tópic"
D19-6313,K15-1016,0,0.0347778,"arget language. However, the quality of the translations depends on the language pair. Thus, it is important to filter out some translations according to their quality. This may be accomplished by applying backtranslation and performing a quality evaluation (using some quality measures like BLEU or METEOR) in English. In Machine Translation, Backtranslation consists of translating a target sentence (in our case, Portuguese) into a source language (in our case, English). This approach has shown good performance in some classification tasks like Sentiment Analysis and Word Sense Disambiguation (Klinger and Cimiano, 2015; Monsalve et al., 2019). Furthermore, Monsalve et al. (2019) show that despite the introduction of sentences with low quality (according to quality measures), the performance of the classifiers continues improving. Also, this approach has been successful in the context of neural machine translation (Sennrich et al., 2016). In the case of NLG from semantic representations, it would be expected that quality is critical since low-quality sentences may lead to models learning incorrect language. Additionally, other issues that may impact the performance of this task are the translation of the sem"
D19-6313,P18-1080,0,0.0285281,"involve language generation, it is noted that back-translation has been widely, and successfully, used in neural machine translation. The aim was to generate synthetic source sentences to increase the parallel training dataset (Sennrich et al., 2016; Edunov et al., edu/LDC2017T10. 3 Available at https://catalog.ldc.upenn. edu/LDC2019T07 4 A cut consists of a set of sentences of the corpus with a similar quality. 95 ∃ d, m, m1, d1: instance(d, describe-01) ∧ instance(m, man) ∧ instance(m1, mission) ∧ instance(d1, disaster) ∧ ARG0(d, m) ∧ ARG1(d, m1) ∧ ARG2(d, d1) (a) Logic format 2018). Also, Prabhumoye et al. (2018) applied back-translation to perform style transfer with good results. Concerning the described work, a question emerges: How can back-translation influence NLG from semantic representations? It is important to note that not only English sentences will be translated into BP ones, but its corresponding semantic representations will be translated to handle representations for Portuguese. Thus, several issues related to alignments may affect the performance (in addition to the quality translation). The following sections show the influence of backtranslation in NLG. 3 (d / describe-01 :ARG0 (m /"
D19-6313,W07-0734,0,0.0557334,"tional RNN with GRU, each with a 1024D hidden unit. Source and target word embeddings were 300D each and both were trained jointly with It is worth noting that each configuration was performed using the cuts generated by F and METEOR to evaluate the quality measure in the corpus selection task. The test was performed on the automatically generated test set described in §4.1.1. In order to compare the results in a real context, the methods were also evaluated on the AMR-annotated BP dataset described in §3. Similar to Castro Ferreira et al. (2017), we used BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as metrics to evaluate fluency, adequacy and postediting efforts of the models, respectively. 98 Eu0 posso1 trabalhar2 no3 meu 4 tópico 5 de6 pesquisa 7 atual 8 . 9 I 0 can 1 work 2 on 3 my 4 current 5 research 6 topic 7 . 8 (a) Alignments between English and Brazilian Portuguese sentences (p / possible-01~e.1 :ARG1 (w / work-01~e.2 :ARG0 (i / i~e.0,4) :ARG1~e.3 (t / topic~e.7 :mod (r / research-01~e.6 :ARG0 i) :time (c / current~e.5)))) (p / possible-01~e.1 :ARG1 (w / trabalhar-01~e.2 :ARG0 (i / eu-eu~e.4,0) :ARG1~e.3 (t / tópico~e.5 :mod (r / research-01~e.7,6"
D19-6313,P16-1009,0,0.200901,"ine Translation, Backtranslation consists of translating a target sentence (in our case, Portuguese) into a source language (in our case, English). This approach has shown good performance in some classification tasks like Sentiment Analysis and Word Sense Disambiguation (Klinger and Cimiano, 2015; Monsalve et al., 2019). Furthermore, Monsalve et al. (2019) show that despite the introduction of sentences with low quality (according to quality measures), the performance of the classifiers continues improving. Also, this approach has been successful in the context of neural machine translation (Sennrich et al., 2016). In the case of NLG from semantic representations, it would be expected that quality is critical since low-quality sentences may lead to models learning incorrect language. Additionally, other issues that may impact the performance of this task are the translation of the semantic representation and the alignments between language pairs. In this context, this paper presents an exploratory study that aims to evaluate the usefulness of back-translation in NLG from semantic representations for non-English languages. Specifically, AMR and Brazilian Portuguese (BP) are chosen as semantic representa"
D19-6313,L18-1486,0,0.0225367,"Missing"
D19-6313,2006.amta-papers.25,0,0.0144211,"024D hidden unit. Source and target word embeddings were 300D each and both were trained jointly with It is worth noting that each configuration was performed using the cuts generated by F and METEOR to evaluate the quality measure in the corpus selection task. The test was performed on the automatically generated test set described in §4.1.1. In order to compare the results in a real context, the methods were also evaluated on the AMR-annotated BP dataset described in §3. Similar to Castro Ferreira et al. (2017), we used BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as metrics to evaluate fluency, adequacy and postediting efforts of the models, respectively. 98 Eu0 posso1 trabalhar2 no3 meu 4 tópico 5 de6 pesquisa 7 atual 8 . 9 I 0 can 1 work 2 on 3 my 4 current 5 research 6 topic 7 . 8 (a) Alignments between English and Brazilian Portuguese sentences (p / possible-01~e.1 :ARG1 (w / work-01~e.2 :ARG0 (i / i~e.0,4) :ARG1~e.3 (t / topic~e.7 :mod (r / research-01~e.6 :ARG0 i) :time (c / current~e.5)))) (p / possible-01~e.1 :ARG1 (w / trabalhar-01~e.2 :ARG0 (i / eu-eu~e.4,0) :ARG1~e.3 (t / tópico~e.5 :mod (r / research-01~e.7,6 :ARG0 i) :time (c / atual~e.8)"
D19-6313,W18-3601,1,0.874759,"slation as Strategy to Tackle the Lack of Corpus in Natural Language Generation from Semantic Representations Marco Antonio Sobrevilla Cabezudo♣ Simon Mille♠ Thiago Alexandre Salgueiro Pardo♣ ♣ Interinstitutional Center for Computational Linguistics (NILC) Institute of Mathematical and Computer Sciences, University of S˜ao Paulo. S˜ao Carlos/SP, Brazil ♠ Universitat Pompeu Fabra. Barcelona, Spain msobrevillac@usp.br, simon.mille@upf.edu, taspardo@icmc.usp.br Abstract There are not as many works for languages other than English: in 2018, the first multilingual surface realization was proposed (Mille et al., 2018). This event proposed two tasks, one focused on reordering a dependency tree and generating inflected words (called shallow track), and the other one focused on generating sentences from a deepsyntax representation similar to a semantic representation (called deep track). It is important to note that while NLG methods were evaluated in corpora for ten different languages in the shallow track, the deep track was limited to evaluating NLG methods on three languages (English, Spanish, and French). The fact that there are less datasets in the deep track is directly related to the higher complexity"
D19-6313,W19-4028,1,0.875699,"the efforts to build or adapt semantic representations for non-English languages, it is possible to cite Abstract Meaning Representation (AMR) as an example. Although AMR was not born as an interlingua, several works have tried to use it in that way to annotate sentences in other languages like Chinese and Czech (Xue et al., 2014), Italian, Spanish, and German (Damonte and Cohen, 2018) and Brazilian Portuguese (Anchiˆeta and Pardo, 2018). Other works have tried to adapt the English AMR guidelines to Spanish and Brazilian Portuguese with some success (Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). However, most of these works report a small number of AMRannotated sentences (compared to the English corpus) and are restricted to some domains like tales (“The Little Prince”). To the best of our knowledge, the only AMR-annotated corpus comparable (in terms of size) to the English corpus2 is the This paper presents an exploratory study that aims to evaluate the usefulness of backtranslation in Natural Language Generation (NLG) from semantic representations for nonEnglish languages. Specifically, Abstract Meaning Representation and Brazilian Portuguese (BP) are chosen as semantic representa"
D19-6313,xue-etal-2014-interlingua,0,0.035807,"Missing"
D19-6313,W16-6644,0,0.020963,"anguage from some underlying representation of information (Reiter and Dale, 2000). This area has gained relevance in the Natural Language Processing community and in the industry in the last years. There are several works and efforts in NLG for English.1 Recently, shared-tasks focused on NLG from semantic representations have gained the attention of the NLG community. Thus, several representations have emerged for attending different contexts. For example, the RDF-based representation presented by Gardent et al. (2017) in its WebNLG challenge, the dialog-act-based representation presented by Novikova et al. (2016), and Abstract Meaning Representation (Banarescu et al., 2013). 1 Most of the work may be found at https://aclweb. org/anthology/sigs/siggen/. 2 Available at https://catalog.ldc.upenn. 94 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 94–103 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chinese corpus, containing 10,149 annotated sentences in its first version.3 This difficulty to get large corpora with this kind of annotation (due to the difficult and expensive annotation task that"
mille-wanner-2008-making,C88-2088,0,\N,Missing
mille-wanner-2008-making,W07-1414,0,\N,Missing
mille-wanner-2008-making,N03-1003,0,\N,Missing
mille-wanner-2008-making,W00-1436,1,\N,Missing
mille-wanner-2008-making,P06-1048,0,\N,Missing
mille-wanner-2008-making,P79-1016,0,\N,Missing
mille-wanner-2010-syntactic,megyesi-etal-2008-swedish,0,\N,Missing
mille-wanner-2010-syntactic,de-marneffe-etal-2006-generating,0,\N,Missing
mille-wanner-2010-syntactic,nivre-etal-2006-talbanken05,0,\N,Missing
mille-wanner-2010-syntactic,bohnet-wanner-2010-open,1,\N,Missing
mille-wanner-2010-syntactic,J93-2004,0,\N,Missing
mille-wanner-2010-syntactic,W09-1210,0,\N,Missing
mille-wanner-2010-syntactic,W03-1712,0,\N,Missing
mille-wanner-2010-syntactic,W00-1436,1,\N,Missing
mille-wanner-2010-syntactic,W07-2441,0,\N,Missing
mille-wanner-2010-syntactic,apresjan-etal-2006-syntactically,0,\N,Missing
N15-1042,P13-2009,0,0.00851502,"ic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and"
N15-1042,2004.tmi-1.14,0,0.0251845,"y avoided. We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filipp"
N15-1042,C14-1133,1,0.883628,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,W14-4416,1,0.917886,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,C00-1007,0,0.43582,"eep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a t"
N15-1042,W11-2832,0,0.444251,"SyntSs). While SSyntSs and linearized structures are isomorphic, the difference in the linguistic abstraction of the DSyntSs and SSyntSs leads to divergences that impede the isomorphy between the two and make the first mapping a challenge for statistical generation. Therefore, we focus in this section on 388 the presentation of the DSyntSs and SSyntSs and the mapping between them. 2.1 2.1.1 DSyntSs and SSyntSs Input DSyntSs DSyntSs are very similar to the PropBank (Babko-Malaya, 2005) structures and the structures as used for the deep track of the First Surface Realization Shared Task (SRST, (Belz et al., 2011)) annotations. DSyntSs are connected trees that contain only meaning-bearing lexical items and both predicate-argument (indicated by Roman numbers: I, II, III, IV, . . . ) and lexico-structural, or deepsyntactic, (ATTR(ibutive), APPEND(itive) and COORD(inative)) relations. In other words, they do not contain any punctuation and functional nodes, i.e., governed elements, auxiliaries and determiners. Governed elements such governed prepositions and subordinating conjunctions are dropped because they are imposed by sub-categorization restrictions of the predicative head and void of own meaning— a"
N15-1042,W05-1601,0,0.0234085,"focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach."
N15-1042,C10-1012,1,0.761097,"neration still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates t"
N15-1042,W11-2835,1,0.945822,"projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et"
N15-1042,de-marneffe-etal-2006-generating,0,0.0170839,"Missing"
N15-1042,P07-1041,0,0.125277,"sentations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portabi"
N15-1042,D08-1019,0,0.28662,", 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module rai"
N15-1042,W11-2833,0,0.0446441,"Missing"
N15-1042,W13-2131,0,0.0280973,"th rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use it for generation of spoken discourse in Arabic, Polish and Turkish. We believe"
N15-1042,P09-1091,0,0.108813,"their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this pap"
N15-1042,W07-2416,0,0.0989154,"starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summarize related work, before in Section 6 some conclusions are drawn and future work is outlined. 2 The Fu"
N15-1042,C12-1083,0,0.0809232,"ent a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008"
N15-1042,P95-1034,0,0.355379,"ans that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov e"
N15-1042,P98-1116,0,0.439226,"to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine"
N15-1042,W02-2103,0,0.194135,"see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and"
N15-1042,P10-1157,0,0.0362809,"Missing"
N15-1042,C12-2082,1,0.839143,"he experiments Spanish Treebank For the validation of the performance of our generator on Spanish, we use the AnCora-UPF treebank, which contains only about 100,000 tokens, but which has been manually annotated and validated on the SSyntS- and DSyntS-layers, such that its quality is rather high. The deep annotation does not contain any functional prepositions since they have been removed for all predicates of the corpus, and the DSyntS-relations have been edited following annotation guidelines. AnCora-UPF SSyntSs are annotated with fine-grained dependencies organized in a hierarchical scheme (Mille et al., 2012), in a similar fashion as the dependencies of the Stanford Scheme (de Marneffe et al., 2006).7 Thus, it is possible to use the full set of labels or to reduce it according to our needs. We performed preliminary experiments in order to assess which tag granularity is better suited for generation and came up with the 31-label tagset. 7 The main difference with the Stanford scheme is that in AnCora-UPF no distinction is explicitly made between argumental and non-argumental dependencies. 2.3.2 English Treebank For the validation of the generator on English, we use the dependency Penn TreeBank (abo"
N15-1042,W13-3724,1,0.879189,"o, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summari"
N15-1042,W11-2836,0,0.029485,"ybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use i"
N15-1042,C04-1097,0,0.0373783,"state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learni"
N15-1042,P04-1011,0,0.0319245,"on.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline t"
N15-1042,E09-1097,0,0.0213842,"nd number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a f"
N15-1042,N07-1022,0,0.00899511,"lassifiers for data-driven generators. As already mentioned in Section 1, most of the state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between mean"
N15-1042,C08-1038,0,\N,Missing
N15-1042,C98-1112,0,\N,Missing
N15-1042,W09-1201,0,\N,Missing
N15-3012,I13-2007,1,0.859674,"d simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it"
N15-3012,C14-1133,1,0.913287,"structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As s"
N15-3012,N15-1042,1,0.837746,"million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joi"
N15-3012,D12-1133,1,0.928036,"are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguis"
N15-3012,bohnet-wanner-2010-open,1,0.822011,"3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of"
N15-3012,E14-2003,0,0.0630282,"Missing"
N15-3012,S10-1059,0,0.0176587,"best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it allows for the display of any additional structural information provided by an extended parsing pipeline. For instance, if the obtained deep-syntactic structure is projected onto a frame-like structure (Chen et al., 2010) with semantic roles as arc labels, this frame structure can be displayed as well. We are currently working on such an extension. Furthermore, we aim to expand our visualization interface to facilitate active exploration of linguistic structures with Brat and thus add to the static display of structures the dimension of Visual Analytics (Keim et al., 2008). Acknowledgments This work has been partially funded by the European Union’s Seventh Framework and Horizon 2020 Research and Innovation Programmes under the Grant Agreement numbers FP7-ICT-610411, FP7-SME606163, and H2020-RIA-645012. Referen"
N15-3012,P08-5006,0,0.0225905,"plicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that"
N15-3012,N10-1011,0,0.0251457,"ructure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntacti"
N15-3012,D08-1019,0,0.0297568,"ions between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output"
N15-3012,W07-2416,0,0.13618,"ing of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics adv adv quant quant subj analyt perf (a) almost 1.2 million jobs have analyt pass prepos det prepos det been created by the state in that time ATTR ATTR agent ATTR ATTR II I II ATT"
N15-3012,C12-1083,0,0.016108,"ucture of a sentence. More precisely, a deep-syntactic structure (DSyntS) is a dependency tree that captures the argumentative, attributive and coordinative relations between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-synta"
N15-3012,nilsson-nivre-2008-malteval,0,0.0344817,"structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an opera"
N15-3012,J05-1004,0,0.0170667,", 2012). Brat takes an annotation file, which is produced by transforming the CoNLL files that the parsers output into Brat’s native format, and generates the graphical interface for the dependency trees. Figure 2 shows three sample surface syntactic structures in Brat. In Figure 3, their equivalent deepsyntactic structures are displayed. As already Figure 1, the figures illustrate the difference of both types of structures with respect to the abstraction of linguistic phenomena. The DSyntSs are clearly much closer to semantics. As a matter of fact, they are equivalent to PropBank structures (Palmer et al., 2005). However, this does not mean that they must per se be “simpler” than their corresponding surface-syntactic structures—compare, for instance, the structures (3a) and (3b) in Figures 2 and 3, where both SSyntS and DSyntS contain the same number of nodes, i.e., are isomorphic. The structures (2a) and (2b) illustrate the capacity of the deep parser to correctly identify the arguments of a lexical item without that explicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-sy"
N15-3012,E12-2021,0,0.106271,"Missing"
N15-3012,P13-4010,0,0.0552154,"Missing"
S17-2158,N15-1042,1,0.845391,"of the input graphs and the resolution of morphological agreements, and (ii) an off-theshelf statistical linearization component. 1 1 2 3 4 5 6 7 8 Setup of the system Layermtt ConS #rul. N/A SemS 190 SemSpos 96 DSyntS 267 SSyntS 294 DMorphS SMorphS Text Textf inal 85 N/A 1 4 Table 1: Overview of the AMR-to-text pipeline. The generator we presented for Task 9.2 of SemEval is a pipeline of graph transducers called Fabra Open Rule-based Generator (FORGe).1 It is built upon work presented, e.g., in (Bohnet, 2006; Wanner et al., 2010). It can be also considered an extended rule-based version of (Ballesteros et al., 2015). The current generator has been mainly developed on the dependency Penn Treebank (Johansson and Nugues, 2007) automatically converted to predicate-argument structures, and adapted to the AMR inputs using SemEval’s training and evaluation sets. 1.1 Step Conversion of AMRs format into CoNLL’09 format Mapping of AMRs onto predicate-argument graphs Assignment of parts of speech Derivation of deep syntactic structure Introduction of function words Resolution of agreements Linearization Retrieval of surface forms Post-processing 1.2 Input format conversion Since our generator cannot read the provid"
S17-2158,W11-2832,0,0.0456219,"tion of the sentence is the introduction of all idiosyncratic words and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004); see (Mille and Wanner, 2015). For instance, the entry corresponding to “peek” would contain the following information: 2.6 The surface-syntactic structures are linearized with an off-the-shelf tool used in the first SRST (Belz et al., 2011), a statistical tree linearizer that orders bottom-up each head and its children (Bohnet et al., 2011). 2.7 It indicates that, according to PropBank, the second argument of “peek” needs the preposition “at”. Hence, this preposition is introduced in the surface-syntactic structure, as shown in the followNMOD ing example: NMOD IOBJ PMOD NMOD SBJ he peek at dog the black bark that AMRs are underspecified in terms of tense, aspect, number, and definiteness. For the task, a past progressive is equally correct as a simple present. Our generator is able to introduce all types of auxiliaries and/or mo"
S17-2158,kingsbury-palmer-2002-treebank,0,0.216658,", and the root of this tree is the main node (the root) of the sentence, this step can seem redundant. But since during Step 1 some 921 2.4 data used for the linearizer, and question and exclamation marks are introduced. Introduction of function words The next step towards the realization of the sentence is the introduction of all idiosyncratic words and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004); see (Mille and Wanner, 2015). For instance, the entry corresponding to “peek” would contain the following information: 2.6 The surface-syntactic structures are linearized with an off-the-shelf tool used in the first SRST (Belz et al., 2011), a statistical tree linearizer that orders bottom-up each head and its children (Bohnet et al., 2011). 2.7 It indicates that, according to PropBank, the second argument of “peek” needs the preposition “at”. Hence, this preposition is introduced in the surface-syntactic structure, as shown in the followNMOD ing example: NM"
S17-2158,W04-2705,0,0.127656,"ode (the root) of the sentence, this step can seem redundant. But since during Step 1 some 921 2.4 data used for the linearizer, and question and exclamation marks are introduced. Introduction of function words The next step towards the realization of the sentence is the introduction of all idiosyncratic words and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004); see (Mille and Wanner, 2015). For instance, the entry corresponding to “peek” would contain the following information: 2.6 The surface-syntactic structures are linearized with an off-the-shelf tool used in the first SRST (Belz et al., 2011), a statistical tree linearizer that orders bottom-up each head and its children (Bohnet et al., 2011). 2.7 It indicates that, according to PropBank, the second argument of “peek” needs the preposition “at”. Hence, this preposition is introduced in the surface-syntactic structure, as shown in the followNMOD ing example: NMOD IOBJ PMOD NMOD SBJ he peek at d"
W11-2835,C10-1012,1,0.800448,"atical function relation labels (as SSyntR).1 The system thus realizes the following steps: 1. Semantic graph → Deep-syntactic tree 2. Deep-syntactic tree → Surface-syntactic tree 3. Surface-syntactic tree → Linearized structure 4. Linearized structure → Surface In addition, two auxiliary steps are carried out. The first one is part-of-speech tagging; it is carried out after step 3. The second one is introduction of commata; it is done after step 4. Each step is implemented as a decoder that uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x)"
W11-2835,P04-1015,0,0.0509894,"the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further processing is obsolete since the correct solution cannot be reached extending any elements of the beam. When we reach a final state, i.e. a tree spanning over all words and the correct solution is in the beam but not ranked first, we perform an update as well, since the correct element should have ranked first in the beam. Algorithm 1 displays the algorithm for the generation of the DSyntR from the SemR."
W11-2835,C96-1058,0,0.0241739,"uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further proces"
W12-1506,C00-1007,0,0.0623427,"ose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoN"
W12-1506,W11-2832,0,0.222768,"for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that th"
W12-1506,C10-3009,1,0.871223,"Missing"
W12-1506,C10-1012,1,0.93562,"erators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a ce"
W12-1506,W11-2835,1,0.926256,"thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order pr"
W12-1506,P98-1116,0,0.715111,"oriented annotation and propose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion o"
W12-1506,W02-2103,0,0.0370789,"standard annotation, with the goal to obtain an increasingly more semantic annotation, can only be accepted if the quality of (deep) stochastic generation does not unacceptably decrease. To assess this aspect, we converted automatically the PropBank annotation of the WSJ journal as used in the CoNLL shared task 2009 into an annotation that complies with all of the principles sketched above 28 for deep statistical generation and trained (Bohnet et al., 2010)’s generator on this new annotation.11 For our experiments, we used the usual training, development and test data split of the WSJ corpus (Langkilde-Geary, 2002; Ringger et al., 2004; Bohnet et al., 2010); Table 1 provides an overview of the used data. set training development test section 2 - 21 24 23 # sentences 39218 1334 2400 Table 1: Data split of the used data in the WSJ Corpus The resulting BLEU score of our experiment was 0.64, which is comparable with the accuracy reported in (Bohnet et al., 2010) (namely, 0.659), who used an annotation that still contained all functional nodes (such that their generation task was considerably more syntactic and thus more straightforward). To assess furthermore whether the automatically converted PropBank al"
W12-1506,P10-1157,0,0.0356542,"Missing"
W12-1506,W00-0306,0,0.0630775,"t is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdea"
W12-1506,J05-1004,0,0.0754015,"results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Lang"
W12-1506,C04-1097,0,0.22939,"th the goal to obtain an increasingly more semantic annotation, can only be accepted if the quality of (deep) stochastic generation does not unacceptably decrease. To assess this aspect, we converted automatically the PropBank annotation of the WSJ journal as used in the CoNLL shared task 2009 into an annotation that complies with all of the principles sketched above 28 for deep statistical generation and trained (Bohnet et al., 2010)’s generator on this new annotation.11 For our experiments, we used the usual training, development and test data split of the WSJ corpus (Langkilde-Geary, 2002; Ringger et al., 2004; Bohnet et al., 2010); Table 1 provides an overview of the used data. set training development test section 2 - 21 24 23 # sentences 39218 1334 2400 Table 1: Data split of the used data in the WSJ Corpus The resulting BLEU score of our experiment was 0.64, which is comparable with the accuracy reported in (Bohnet et al., 2010) (namely, 0.659), who used an annotation that still contained all functional nodes (such that their generation task was considerably more syntactic and thus more straightforward). To assess furthermore whether the automatically converted PropBank already offers some adva"
W12-1506,P04-1011,0,0.0192458,"more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order predicate logic structures. However, to the best of our knowledge, 1 Trained on the original ConLL 2009 corpora, (Bohnet et al., 2010)’s SVM-based generator reached a BLEU score of 0.12 for Chinese, 0.18 for English, 0.11 for German and 0.14 for Spanish. Joining the unconnected parts of the sentence annotations to connected trees (as required by a stochastic realizer) improv"
W12-1506,W08-2121,0,0.0817605,"Missing"
W12-1506,N07-1022,0,0.0136454,"dly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order predicate logic structures. However, to the best of our knowledge, 1 Trained on the original ConLL 2009 corpora, (Bohnet et al., 2010)’s SVM-based generator reached a BLEU score of 0.12 for Chinese, 0.18 for English, 0.11 for German and 0.14 for Spanish. Joining the unconnected parts of the sentence annotations to connected trees (as required by a stochastic realizer) improved the performance to a B"
W12-1506,P95-1034,0,\N,Missing
W12-1506,C98-1112,0,\N,Missing
W12-1525,W04-2705,0,\N,Missing
W12-1525,C10-1012,1,\N,Missing
W12-1525,W08-2121,0,\N,Missing
W12-1525,W11-2832,1,\N,Missing
W12-1525,W07-2416,0,\N,Missing
W12-1525,J05-1004,0,\N,Missing
W12-1525,W12-1528,1,\N,Missing
W12-1525,W04-3250,0,\N,Missing
W12-1525,kow-belz-2012-lg,1,\N,Missing
W12-1525,W12-1527,1,\N,Missing
W13-3703,C12-1007,0,0.145344,"Missing"
W13-3703,ballesteros-nivre-2012-maltoptimizer-system,1,0.935927,"igurations of morphosyntactic features which would allow for optimizing the parsing of Spanish texts, and to evaluate the impact that each feature has, independently and in combination with others. 1 Introduction As shown in natural language processing (NLP) research, a careful selection of the linguistic information is relevant in order to produce an impact on the results. In this paper, we want to look into different sets of morphosyntactic features in order to test their effect on the quality of parsing for Spanish. To this end, we apply MaltParser (Nivre et al., 2007b), and MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a), which is a system capable of exploring and exploiting the different feature sets that can be extracted from the data and used over the models generated for MaltParser. Starting from a corpus annotated with finegrained language-specific information, we can use all, or a part of the morphosyntactic features to build different models and see the impact of each feature set on the Labeled Attachment Score (henceforth LAS) of the parser. 13 We decided to use MaltOptimizer in order to answer the following questions: (i) is the inclusion of all morphological features"
W13-3703,E12-2012,1,0.938995,"igurations of morphosyntactic features which would allow for optimizing the parsing of Spanish texts, and to evaluate the impact that each feature has, independently and in combination with others. 1 Introduction As shown in natural language processing (NLP) research, a careful selection of the linguistic information is relevant in order to produce an impact on the results. In this paper, we want to look into different sets of morphosyntactic features in order to test their effect on the quality of parsing for Spanish. To this end, we apply MaltParser (Nivre et al., 2007b), and MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a), which is a system capable of exploring and exploiting the different feature sets that can be extracted from the data and used over the models generated for MaltParser. Starting from a corpus annotated with finegrained language-specific information, we can use all, or a part of the morphosyntactic features to build different models and see the impact of each feature set on the Labeled Attachment Score (henceforth LAS) of the parser. 13 We decided to use MaltOptimizer in order to answer the following questions: (i) is the inclusion of all morphological features"
W13-3703,W09-3822,0,0.0248559,"yze in depth whether it is useful to incorporate morphological information as independent features. Eryigit et al. (2008) have already contributed to this topic by testing different morphosyntactic combinations and their effect on MaltParser when applied to Turkish: they point out that some features do not make the dependency parser improve (in their case, number and person), and that Labeled and Unlabeled Attachment Scores (LAS/UAS) are unequally impacted by the feature variation (inflectional features affect more the labeled than the unlabeled accuracy). We also find interesting the work of Bengoetxea and Gojenola (2009) and Atutxa et al. (2012), which have respectively tried to include semantic classes and feature propagation between different parsing models, with the intention of improving the parsing results for Basque. However, none of these works made use of MaltOptimizer in their experiments, for the simple reason that it was not available at the time. Spanish may not be as morphologically rich as other languages such as Hebrew, Turkish or Basque, but it involves enough morphological interactions to allow our research to contribute to 14 such important discussion (Tsarfaty et al., 2010). For instance, d"
W13-3703,E12-1009,0,0.0228251,"Missing"
W13-3703,D12-1133,0,0.0332467,"Missing"
W13-3703,W06-2920,0,0.115763,". ] ... (some hidden transitions) L EFT-A RC subj [ ROOT ] { Eso } [ es lo que hicieron . ] R IGHT-A RC 3.1 MaltParser, MalOptimizer and the CoNLL Data Format ROOT subj MaltParser (Nivre et al., 2007b) is a transitionbased dependency parser generator that requires as an input a training set annotated in CoNLL-X data format,2 and provides models capable of producing the dependency parsing of new sentences. MaltParser implements four different transitionbased parsers families and provides high and stable performance (see, e.g., (Mille et al., 2012)). In the CoNLL Shared Tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a), it was one of the best parsers, achieving either the first or the second place for most of the languages. A transition-based parser is based on a state machine over mainly two data structures: (i) a buffer that stores the words to be processed and (ii) a stack that stores the ones that are being processed (see Figure 1 for details). The different transitions are shown in Figure 2; as can be observed, the state machine transitions manage the input words in order to assign dependencies between them. The transition-based parsers implemented in MaltParser use a model learne"
W13-3703,H05-1100,0,0.0239908,"iii) involving gender, number and sometimes person sharing; furthermore, some features are required on some verbs by their syntactic governor, such as a certain type of finiteness (gerund, participle, infinitive, finite) or mood. All those properties are encoded in the tagset used for the annotation of the AnCora-UPF corpus (see (Burga et al., 2011; Mille et al., 2013) for details about how the tagset was designed), so we expect that the presence or absence of one or more of these features in the training corpus will have a clear impact on the quality of the parsing. In this way, the work of (Cowan and Collins, 2005) makes a step exploring how specific morphologic features (encoded as different PoS) affect the parsing results in Spanish. Even though the authors use a constituent-based treebank and not a dependency-based one, they find that number and mood (verbal feature that overlaps our mood and finiteness) are the features that most affect the parser’s behaviour. 3 Experimental Setup Here are the five steps we followed: 1. The corpus was divided into a training set (3263 sentences, 93803 tokens, 28.7 tokens/sentence) and a test set (250 sentences, 7089 tokens, 28.4 tokens/sentence); 2. 82 different ver"
W13-3703,C12-2082,1,0.866977,"Missing"
W13-3703,W13-3724,1,0.934811,"n use all, or a part of the morphosyntactic features to build different models and see the impact of each feature set on the Labeled Attachment Score (henceforth LAS) of the parser. 13 We decided to use MaltOptimizer in order to answer the following questions: (i) is the inclusion of all morphological features found in an annotation useful for Spanish parsing?; (ii) what are the optimal configurations of morphological features?; (iii) can we explain why different features are more or less important for the parser? For this purpose, we used the UPF version of a subsection of the AnCora corpus (Mille et al., 2013) (see also Section 3.2), which includes features such as number, gender, person, mood, tense, finiteness, and coarse- and fine-grained part-ofspeech (PoS). The impact of each feature or combination of features on subsets of dependency relations is also analyzed; for this, a fine-grained annotation of the syntactic layer is preferred since it allows for a more detailed analysis. The version of the AnCora-UPF corpus that we use contains 41 language-specific syntactic tags and thus is perfectly suitable for our task. In the rest of the paper, we situate our goals within the state-of-the-art (Sect"
W13-3703,W03-3017,0,0.0587315,"ure model –the ones included in the default feature model- and the ones selected or rejected by MaltOptimizer. However, our intention is to study the effect of the features included in the FEATS column, and the interaction with the other features is actually the real case scenario. By performing an automatic search of the linguistic annotation with MaltOptimizer, we are sure that all the morphosyntactic annotation included in the FEATS column is studied and tested by MaltOptimizer. After running MaltOptimizer for Phase 1 and Phase 2, the best parser for (all) our data sets is Nivre arc-eager (Nivre, 2003), which behaves as shown in Figure 1; we were therefore ready to run the feature selection implemented in the Phase 3 POSTAG CC CD DT of MaltOptimizer. Furthermore, the experiments performed by MaltOptimizer ensure that our features are tested in the last steps of the optimization process (Ballesteros and Nivre, 2012a). IN JJ NN NP PP RB 3.2 The AnCora-UPF dependency corpus This corpus, presented by Mille et al. (2013), consists of a small section (3513 sentences, 100892 tokens) of the Spanish dependency corpus AnCora (Taul´e et al., 2008). Mille et al. (2009) explain the partially automatic m"
W13-3703,C12-1147,0,0.0275872,"Missing"
W13-3703,W12-5205,0,0.158172,"ecific syntactic tags and thus is perfectly suitable for our task. In the rest of the paper, we situate our goals within the state-of-the-art (Section 2), we describe the experimental setup, i.e. MaltParser, MaltOptimizer, the corpus used and the experiments that we carried out (Section 3), we report and discuss the results of the experiments (Section 4), and finally present the conclusions and some suggestions for further work (Section 5). 2 Motivation and Related Work Other researchers have already applied MaltOptimizer to their datasets, with different objectives in mind. Thus, the work of Seraji et al. (2012) shows that, for Persian, the parser results improve when following the model suggested by the optimizer. Tsarfaty et al. (2012a) work with Hebrew –a morphologically rich language- and incorporate the optimization offered by MaltOptimizer for presenting novel metrics that allow for jointly evaluating syntactic parsing and morphological segmentation. Mambrini and Passarotti (2012) use the opProceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 13–22, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013."
W13-3703,taule-etal-2008-ancora,0,0.153347,"Missing"
W13-3703,J08-3003,0,0.0224605,"12b) present three different parsing challenges, broadly described as: (i) the architectural challenge, which focuses on how and when to introduce morphological segmentation; (ii) the modeling challenge, focused on how and where the morphological information should be encoded; and (iii) the lexical challenge, which faces the question of how to deal with morphological variants of a word that are not included in the corpus. Our work is directly related to the modeling challenge, given that we analyze in depth whether it is useful to incorporate morphological information as independent features. Eryigit et al. (2008) have already contributed to this topic by testing different morphosyntactic combinations and their effect on MaltParser when applied to Turkish: they point out that some features do not make the dependency parser improve (in their case, number and person), and that Labeled and Unlabeled Attachment Scores (LAS/UAS) are unequally impacted by the feature variation (inflectional features affect more the labeled than the unlabeled accuracy). We also find interesting the work of Bengoetxea and Gojenola (2009) and Atutxa et al. (2012), which have respectively tried to include semantic classes and fe"
W13-3703,W10-1401,0,0.0484842,"Missing"
W13-3703,P03-1054,0,0.0111841,"specifies the POSTAG column and can be used in order to improve the parsing; however, it does not work the other way around: the Tree Tagger PoS tags in the FEATS column do not bring any new information to that one already introduced in the POSTAG column, and thus are ignored by MaltOptimizer. Also, MaltOptimizer follows a stepwise procedure, under this scenario it starts with a higher baseline and it is therefore difficult to get improvements during the optimization steps by testing new features, and thus the features are not selected. There is therefore less room for improvement. Klein and Manning (2003) present similar improvements when splitting the IN tag during their experiments on constituency parsing with a PCFG; we can see now that it is probably the case for dependency parsing too. The best configuration for MaltParser and AnCora-UPF corpus is [finiteness gender number spos]. For parsing purposes, then, it seems enough to enrich the morphosyntactic annotation just with these features, at least in the case of Spanish. These features not only work well together, but also very often improve the results when are individually added to any combination of features. On the one hand, there is"
W13-3703,E12-1006,0,0.0750063,"tate-of-the-art (Section 2), we describe the experimental setup, i.e. MaltParser, MaltOptimizer, the corpus used and the experiments that we carried out (Section 3), we report and discuss the results of the experiments (Section 4), and finally present the conclusions and some suggestions for further work (Section 5). 2 Motivation and Related Work Other researchers have already applied MaltOptimizer to their datasets, with different objectives in mind. Thus, the work of Seraji et al. (2012) shows that, for Persian, the parser results improve when following the model suggested by the optimizer. Tsarfaty et al. (2012a) work with Hebrew –a morphologically rich language- and incorporate the optimization offered by MaltOptimizer for presenting novel metrics that allow for jointly evaluating syntactic parsing and morphological segmentation. Mambrini and Passarotti (2012) use the opProceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 13–22, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. timizer not only to capture the feature model that fits best Ancient Greek, but also to evaluate how the genre used in the trai"
W13-3703,H05-1066,0,0.170482,"Missing"
W13-3703,D08-1059,0,0.0777577,"Missing"
W13-3703,P11-2033,0,0.0625344,"Missing"
W13-3703,J13-1003,0,\N,Missing
W13-3703,J08-4010,0,\N,Missing
W13-3703,D07-1096,0,\N,Missing
W13-3724,apresjan-etal-2006-syntactically,0,0.0811369,"ic, but also with semantic information. This need implies that dependency treebanks must be annotated with both syntactic and semantic information, as, e.g., the Prague Dependency Treebank (PDT) 2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sent"
W13-3724,W13-3703,1,0.884674,"Missing"
W13-3724,W11-2832,0,0.0721964,"transparent semantic frames, in the sense that no difference is made between external or internal arguments. 221 (d) SemS Figure 1: The four levels of annotation for the sentence El documento propone que este contrato afecte a las personas que engrosen las listas del paro ‘The document suggests that this contract affect the persons who make the unemployment lists swell’ sented in a single standard 14-column CoNLL file. The deep-syntactic layer is also provided in a separate CoNLL file, while the semantic layer is presented in the HFG format used in the SurfaceRealization Shared Task in 2011 (Belz et al., 2011). The different layers are connected thanks to the IDs of the nodes. 3 Multilayered annotation in practice Annotating such a corpus manually can seem too costly at the first sight. In this section, we show that a solid theoretical framework and the use of adequate tools can allow for significant reduction of the manual workload. 3.1 The advantages of our theoretical framework As already mentioned, our annotation model is strongly influenced by the Meaning-Text Theory (Mel’ˇcuk, 1988). Its rich stratification facilitates a clear separation of different types of linguistic phenomena and thus a s"
W13-3724,W00-1436,1,0.628518,"a given level to the corresponding representations at the adjacent levels. This has an interesting consequence as far as corpus annotation is concerned: starting from a given stratum and a manually created mapping grammar (the coverage does not need to be broad at first), the annotations at the adjacent strata can be easily obtained, and they can on their turn be used to derive the annotations at the next strata, and so on. In other words, with a corpus of SSyntSs, it is straightforward to derive parallel corpora of DSyntSs and SemSs using an adequate tool, such as the graph transducer MATE (Bohnet et al., 2000). The process of annotation can be reduced to a minimal manual revision of automatically created structures. For the surface-syntactic annotation, we use our detailed annotation schema that allows for relatively easy dependency relation identification, based on easy-to-use criteria. The annotation schema has been defined taking into account that (a) the schema should cover only criteria that are 222 related to the syntactic behaviour of the nodes; (b) the granularity of the schema should be balanced in the sense that it should be fine-grained enough to capture language-specific syntactic idios"
W13-3724,C10-1012,1,0.912806,"Missing"
W13-3724,W09-1210,0,0.0260248,"Missing"
W13-3724,W07-2416,0,0.0691859,"e labeling or semantic analysis, sentence generation, abstractive summarization, etc.) to deal not only with syntactic, but also with semantic information. This need implies that dependency treebanks must be annotated with both syntactic and semantic information, as, e.g., the Prague Dependency Treebank (PDT) 2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the lin"
W13-3724,J93-2004,0,0.0427188,"layer is a simple chain of surface lexical units bearing morpho-syntactic information. Surface lexical units are all the items of the vocabulary, that is, words as they appear in any monolingual dictionary, and their inflected variants. In Table 1, all possible values of all morphosyntactic features used in our annotation are detailed. In addition to features such as gender and number, we use three different tagsets for Part-ofSpeech: a coarse-grained one, dpos, and two finegrained ones: pos and spos. The difference between pos, which is a subset of the PoS tagset from the Penn TreeBank set (Marcus et al., 1993), and spos is minor, although, for instance, in parsing 1 It includes the 3,510 sentences that AnCora comprised at the time we launched this project back in early 2008, and three additional sentences we used for early tests. For downloads, see http://www.taln.upf. edu/content/resources/495. Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 217–226, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. Features dpos Possible values A, Adv, N, V adjective, adverb, auxiliary, conjunction, copula, deter"
W13-3724,W04-2705,0,0.0932115,"2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sentence level: morphological, surface-syntactic, deep-syntactic, and semantic. 2.1 Morphological layer The morphological layer is a simple chain of surface lexical units bearing morpho-syntactic"
W13-3724,C12-2082,1,0.823944,"dence with the nodes of the morphological level. The 47 language-specific surface-syntactic relations used for the annotation of this layer are given and briefly explained in Table 4.3 In the corpus, 14 of these relations occur more than a thousand times; these are, from the most frequent to the less frequent: prepos, det, punc, adv, modif, subj, obl_obj, dobj, conj, coord, aux_phras, attr, copul, and relat. Depending on the application, one can need more or less tags in the annotation. In order to allow for tuning the granularity of the tagset, we organized the relations in a hierarchy (see (Mille et al., 2012) for illustration). 2.3 Deep-syntactic (DSynt) layer The structures at this layer are dependency trees in which labelled dependencies link pairs of deep lexical units. To the lexical units, deep-syntactic grammemes are assigned. The deep-syntactic dependency relations (cf. Table 5) are languageindependent and thus also more abstract than the surface-syntactic ones. In our corpus, the deepsyntactic layer contains only 66,980 nodes since all punctuation signs and functional nodes have been removed. In the following, the four particular cases of node-removal are listed.4 (a) Governed elements The"
W13-3724,J05-1004,0,0.0981899,"ague Dependency Treebank (PDT) 2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sentence level: morphological, surface-syntactic, deep-syntactic, and semantic. 2.1 Morphological layer The morphological layer is a simple chain of surface lexical u"
W13-3724,N06-2015,0,0.0881324,"Missing"
W13-3724,taule-etal-2008-ancora,0,0.425113,"Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sentence level: morphological, surface-syntactic, deep-syntactic, and semantic. 2.1 Morphological layer The morphological layer is a simple chain of surface lexical units bearing morpho-syntactic information. Surface lexical un"
W14-4416,W02-2103,0,\N,Missing
W14-4416,C10-1012,1,\N,Missing
W14-4416,N01-1001,0,\N,Missing
W14-4416,D08-1019,0,\N,Missing
W14-4416,W08-2102,0,\N,Missing
W14-4416,C00-1007,0,\N,Missing
W14-4416,C10-3009,0,\N,Missing
W14-4416,W11-2832,0,\N,Missing
W14-4416,D08-1008,0,\N,Missing
W14-4416,W13-3724,1,\N,Missing
W14-4416,W02-2105,0,\N,Missing
W15-2107,de-marneffe-etal-2014-universal,0,0.0487752,"Missing"
W15-2107,P84-1058,0,0.665272,"d not individual nodes. The degree of “semanticity” of DSyntSs can be directly compared to Prague’s tectogrammatical structures (Hajiˇc et al., 2006), which contain autosemantic words only, leaving out synsemantic elements such as determiners, auxiliaries, (all) prepositions and conjunctions. Collapsed SDs (de Marneffe et al., 2006) differ from the DSyntSs in that they collapse only (but all) prepositions, conjunctions and possessive clitics, they do not involve any removal of (syntactic) information, and they do not add semantic information compared to the surface annotation. 7 As, e.g., in (Gross, 1984), and the Explanatory Combinatorial Dictionary (Mel’ˇcuk, 1988). 53 nen et al. (2013) point out. If the syntactic behavior is not different when a dependent is an adverb or a noun, only one syntactic relation should be needed. uses the SD scheme adapted to Finnish. The second layer inserts additional dependencies over the first layer. This second layer tries, on the one hand, to cover more semantic phenomena (conjunct propagation for coordinations, and external subjects), but, on the other hand, it aims at covering some syntactic phenomena–gaps resulting from the first layer annotation–such as"
W15-2107,C14-1133,1,0.897425,"Missing"
W15-2107,N15-1042,1,0.887809,"Missing"
W15-2107,W12-3602,0,0.0218237,"kii . . . ‘Metla predicts that the birch will be in bloom . . . ’ for the latter. Thanks to this lexicon, rules can check in the input SSyntS if a word has a dependent of the type described in its entry, and perform the adequate mapping. For instance, if a dependent of ennustaa is a noun in the nominative case with the depen8 The lexicon furthermore contains additional information about the entries which is not related to subcategorization, such as morphological invariability, as well as the values for some lexical functions. 9 A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al., 2012) for an overview of deep dependency structures. In particular, DSyntSs show some resemblance, but also some important differences, with PropBank structures, mainly due to the fact that the latter concern phrasal chunks and not individual nodes. The degree of “semanticity” of DSyntSs can be directly compared to Prague’s tectogrammatical structures (Hajiˇc et al., 2006), which contain autosemantic words only, leaving out synsemantic elements such as determiners, auxiliaries, (all) prepositions and conjunctions. Collapsed SDs (de Marneffe et al., 2006) differ from the DSyntSs in that they collaps"
W15-2107,W00-1436,1,0.475204,"ent); for instance, subj and dobj in Figure 1 map to argumental relations in Figure 2 (respectively I and II), while relat and adv are mapped to the non-argumental relation ATTR. 6 Even if it is possible to find sentences with the two nominal elements at the same side of the copula, they are not interpreted as neutral copulative sentences, but are communicatively marked. In other words, during the mapping between surface- and deep-syntax, functional elements and 52 predicate-argument relations have to be identified. Thanks to the existence of dedicated tools such as the graph-transducer MATE (Bohnet et al., 2000), the mapping of the SSynt-annotation onto the DSynt-annotation is facilitated. For instance, Mille et al. (2013) describe how they obtain the DSynt annotation of a Spanish treebank. To make the mapping straightforward, predicate-argument information is included in the tags of surfacesyntactic annotation, enriching surface-syntactic relations with semantic information. Thus, for instance, instead of simply annotating the relation obl obj when this relation is identified, specifying the argument number in the label is also required: obl obj0 corresponds to the first argument, obl obj1 to the se"
W15-2107,W13-3724,1,0.836354,"tation of the Stanford Dependency (SD) schema for 2 A surface-syntactic annotation of Finnish Our annotation schema for Finnish follows the methodology adopted for the elaboration of the 1 According to KORP -https://korp.csc.fi- the FTB with all its versions joined contains 4,386,152 sentences (76,532,636 tokens). However, the limited number of relations makes an in-depth analysis and/or comparison difficult. 48 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 48–57, Uppsala, Sweden, August 24–26 2015. schema of the Spanish AnCora-UPF treebank (Mille et al., 2013). Taking into account a series of clearly cut syntactically-motivated criteria, a tagset of Finnish syntactic dependencies has been established. In what follows, we first present the SSynt relation tagset, and then discuss some of the main criteria applied for the identification of selected tags. 2.1 DepRel adjunct adv appos attr aux aux phras bin junct Distinctive properties mobile sentential adverbial mobile verbal adverbial right-sided apposed element genitive complement of nouns non finite V governed by auxiliary verbs multi-word marker relates binary constructions non-independent adjacent"
W15-2107,W08-1301,0,0.159535,"Missing"
W15-2107,voutilainen-etal-2012-specifying,0,0.0643061,"Missing"
W15-2107,de-marneffe-etal-2006-generating,0,0.143549,"Missing"
W17-3517,W11-2832,1,0.948241,"Missing"
W17-3517,kow-belz-2012-lg,1,0.830242,"em, synonym and paraphrase matches. We will apply text normalization before scoring. For n-best ranked system outputs, we will compute a single score for all outputs by computing the weighted sum of their individual scores, with a weight assigned to an output in inverse proportion to its rank. For a subset of the test data we may obtain additional alternative realizations via Mechanical Turk for use in the automatic evaluations. 8 http://universaldependencies.org/ format.html For the human-assessed evaluation, we are planning to use a type of evaluation that is based on preference judgements (Kow and Belz, 2012, p.4035), using the existing evaluation interface described in Kow and Belz’s paper. As in SR’11, we plan to use students in the third year of an undergraduate degree, from Cambridge, Oxford and Edinburgh. Two candidate outputs9 will be presented to the evaluators, who will assess them for Clarity, Fluency and Meaning Similarity. For each criterion, they will be asked not only to state which system output they prefer, but also how strong is their preference. We plan to organize a workshop collocated with ACL ’18, COLING ’18, or EMNLP ’18 at which the results of the SR’18 will be presented. To"
W17-3517,S17-2090,0,0.0229781,"rained PoS and morphological information will be removed from the input trees. The first shared task on Surface Realization was carried out in 2011 with a similar setup, with a focus on English. We think that it is time for relaunching such a shared task effort in view of the arrival of Universal Dependencies annotated treebanks for a large number of languages on the one hand, and the increasing dominance of Deep Learning, which proved to be a game changer for NLP, on the other hand. 1 Introduction In 2017, three shared tasks on Natural Language Generation (NLG) take place: Task 9 of SemEval (May and Priyadarshi, 2017), WebNLG1 and E2E2 . The first starts from Abstract Meaning Representations (AMRs), the second from RDF triples, and the third from dialog act-based Meaning Representations (MRs) respectively. With these efforts, the focus is put on “real-life” generation, since the respective inputs come from existing analyzers (for AMRs) or existing databases (for RDF triples and 1 http://talc1.loria.fr/webnlg/stories/ challenge.html 2 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ MRs). This shows that the research on NLG is on the right track and that there is an interest in large scale “deep” NLG. However,"
W17-3517,W04-2705,0,0.187846,"Missing"
W17-3517,L16-1262,0,0.0611377,"Missing"
W17-3517,J05-1004,0,0.0504225,"their lemmas or stems, depending on the availability of lemmatization and stemming tools, respectively. For the Deep Track, additionally: 3. functional prepositions and conjunctions that can be inferred from other lexical units or from the syntactic structure will be removed, as e.g., “by” and “of” in Figure 2; 4. determiners and auxiliaries will be replaced (when needed) by attribute/value pairs, as, e.g., “Definiteness” and “Aspect” in Figure 3; 5. edge labels will be generalized into predicate argument labels, following the PropBank/NomBank edge label nomenclature (Meyers and et al., 2004; Palmer et al., 2005), with three main differences: (i) there will be no special label for external arguments (i.e., no “A0”), which means that all first arguments of a predicate will be mapped to A1, and the rest of the arguments will be labeled starting from A2; (ii) all modifier edges “AM-...” will be generalized to “AM”; (iii) there will be a coordinative relation; and (iv) any relation that does not fall into the first three cases will be assigned an underspecified edge label. 6. morphological information coming from the syntactic structure or from agreements will be removed; in other words, only “semantic” i"
W17-3539,N15-1042,1,0.84383,"Some other rules order governordependent pairs and siblings with one another. We then match the triple &lt;lemma&gt;&lt;POS&gt;&lt;morphosyntactic features&gt; with an entry of a morphological dictionary and simply replace the triple by the surface form. The final sentence corresponding to the running example would be He peeks at the black dog that barks. 3 Acknowledgments The work described in this paper has been partially funded by the European Commission under the contract numbers FP7-ICT-610411, H2020-645012RIA, H2020-700024-RIA, and H2020-700475-RIA. References SBJ he peek at dog the black bark that 2.4 (Ballesteros et al., 2015) or the linearization step (Bohnet et al., 2011), in order to overcome a possible lack of coverage of the rules. During the demo session, participants will be encouraged to play with the generator through a graphical interface, in order to see all the details of a generation process (in English, with some examples in German and Polish). A flexible multilingual generation pipeline The presented pipeline is flexible from several perspectives. First, it is quite easily adaptable to different types of inputs; for instance, it took only one week to adapt it to the AMRs of SemEval’17. Second, many r"
W17-3539,W11-2835,1,0.828323,"iblings with one another. We then match the triple &lt;lemma&gt;&lt;POS&gt;&lt;morphosyntactic features&gt; with an entry of a morphological dictionary and simply replace the triple by the surface form. The final sentence corresponding to the running example would be He peeks at the black dog that barks. 3 Acknowledgments The work described in this paper has been partially funded by the European Commission under the contract numbers FP7-ICT-610411, H2020-645012RIA, H2020-700024-RIA, and H2020-700475-RIA. References SBJ he peek at dog the black bark that 2.4 (Ballesteros et al., 2015) or the linearization step (Bohnet et al., 2011), in order to overcome a possible lack of coverage of the rules. During the demo session, participants will be encouraged to play with the generator through a graphical interface, in order to see all the details of a generation process (in English, with some examples in German and Polish). A flexible multilingual generation pipeline The presented pipeline is flexible from several perspectives. First, it is quite easily adaptable to different types of inputs; for instance, it took only one week to adapt it to the AMRs of SemEval’17. Second, many rules are language-independent, and others can be"
W17-3539,W07-2416,0,0.027111,"alana de Recerca i Estudis Avanc¸ats (ICREA), Lluis Companys 23, 08010 Barcelona, Spain firstname.lastname@upf.edu Abstract 2.1 This demo paper presents the multilingual deep sentence generator developed by the TALN group at Universitat Pompeu Fabra, implemented as a series of rule-based graphtransducers. 1 Introduction FORGe (Mille et al., 2017)1 is a pipeline of graph transducers which, coupled with lexical resources, allows for generating texts, starting from a variety of abstract input structures. The current generator has been mainly developed for English on the dependency Penn Treebank (Johansson and Nugues, 2007) automatically converted to predicateargument structures, and on Abstract Meaning Representations, using the SemEval’17 data (May and Priyadarshi, 2017). It is currently being adapted to languages such as Spanish, German French, and Polish, in the context of ontology-to-text generation as part of a dialogue system. Our generator follows the theoretical model of the Meaning-Text Theory (Mel’ˇcuk, 1988), and performs the following actions: (i) syntacticization of predicate-argument graphs; (ii) introduction of function words; (iii) linearization and retrieval of surface forms. 2 Overview of the"
W17-3539,kingsbury-palmer-2002-treebank,0,0.105846,"tacticization of predicate-argument graphs; (ii) introduction of function words; (iii) linearization and retrieval of surface forms. 2 Overview of the system In this section, we briefly describe the input to the system and the successive transductions . 1 See this paper for an evaluation of the system in the context of the SemEval AMR-to-text generation challenge. Inputs The input structures can be trees or acyclic graphs that contain linguistic information only, which includes meaning bearing units and predicateargument relations such as ARG0 (if licensing external arguments, as in PropBank (Kingsbury and Palmer, 2002)), ARG1, ARG2, . . . , ARGn). In order to allow for more compact representations, the generator can also handle “non-core” predicates as edges, be it with a generic label nonCore, or with a typed label such as purpose; see, for example two alternative representations of a purpose meaning between two nodes N1 and N2 : ARG1 ARG2 purpose N1 Npurpose N2 N1 N2 2.2 Generation of the deep syntactic structure First of all, parts of speech are assigned to each node of the structure. Then, during this transduction, a top-down recursive syntacticization of the semantic graph is performed. It looks for th"
W17-3539,S17-2090,0,0.0142368,"the multilingual deep sentence generator developed by the TALN group at Universitat Pompeu Fabra, implemented as a series of rule-based graphtransducers. 1 Introduction FORGe (Mille et al., 2017)1 is a pipeline of graph transducers which, coupled with lexical resources, allows for generating texts, starting from a variety of abstract input structures. The current generator has been mainly developed for English on the dependency Penn Treebank (Johansson and Nugues, 2007) automatically converted to predicateargument structures, and on Abstract Meaning Representations, using the SemEval’17 data (May and Priyadarshi, 2017). It is currently being adapted to languages such as Spanish, German French, and Polish, in the context of ontology-to-text generation as part of a dialogue system. Our generator follows the theoretical model of the Meaning-Text Theory (Mel’ˇcuk, 1988), and performs the following actions: (i) syntacticization of predicate-argument graphs; (ii) introduction of function words; (iii) linearization and retrieval of surface forms. 2 Overview of the system In this section, we briefly describe the input to the system and the successive transductions . 1 See this paper for an evaluation of the system"
W17-3539,W04-2705,0,0.0722544,"al Natural Language Generation conference, pages 245–246, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics 2.3 Introduction of function words The next step towards the realization of the sentence is the introduction of all idiosyncratic words (prepositions, auxiliaries, determiners, etc.) and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank and NomBank (Meyers et al., 2004). During this transduction, anaphora are resolved, and personal pronouns are introduced in the tree (this includes possessive, relative and personal pronouns). See, e.g., how the preposition “at” is introduced in the following surface-syntactic structure: SBJ NMOD NMOD IOBJ PMODNMOD Resolution of morpho-syntactic agreements, linearization, and retrieval of surface forms In order to resolve agreements, the rules for this transduction check the governor/dependent pairs, together with the syntactic relation that links them together. Some other rules order governordependent pairs and siblings with"
W17-3539,S17-2158,1,0.79536,"Missing"
W18-3601,P11-2040,1,0.888439,"Missing"
W18-3601,W11-2832,1,0.629164,"e languages may also work for others. The SR’18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation. SR’18 also addresses questions about just how suitable and useful the notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc"
W18-3601,W17-4755,1,0.838761,"gy) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a ‘damaged’ version and (iii) some are replaced by their corresponding reference texts. In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high. For full details of how these additional texts are created and thresholds applied, please refer to Bojar et al. (2017a). Below we report QA figures for the MTurk evaluations (Section 3.2.1). Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.10 3.2.2 Google Data Compute Evaluation In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google’s internal ‘Data Compute’ system evaluation service, where experienced evaluators carefully assess each system output. We used an interface that matches the WMT’17 interface above, as closely as was possible within the constraints of th"
W18-3601,P17-1017,0,0.0630018,"he notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD str"
W18-3601,D14-1020,1,0.831205,"Missing"
W18-3601,S17-2090,0,0.0582517,"h is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD structures in which word order information has b"
W18-3601,W04-2705,0,0.458005,"Missing"
W18-3601,L16-1262,0,0.0728681,"Missing"
W18-3601,W17-5525,0,0.0981398,"Missing"
W18-3601,J05-1004,0,0.102132,"in CoNLL-U format, with no meta-information.7 Figures 1, 2 and 3 show 6 universaldependencies.org 7 http://universaldependencies.org/ a sample original UD annotation for English, and the corresponding shallow and deep input structures derived from it. To create inputs to the Shallow Track, the UD structures were processed as follows: 1. Word order information was removed by randomised scrambling; 2. Words were replaced by their lemmas. For the Deep Track, the following steps were additionally carried out: 3. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation; see the inventory of relations in Table 1. 4. Functional prepositions and conjunctions in argument position (i.e. prepos"
W18-3601,P02-1040,0,0.102984,"nt Example fall→ the ball the ball→ fall fall→ last night fall→ [and] bounce Tower→ Eiffel N/A Table 1: Deep labels. train dev test ar 6,016 897 676 cs 66,485 9,016 9,876 en 12,375 1,978 2,061 es 14,289 1,651 1,719 fi 12,030 1,336 1,525 fr 14,529 1,473 416 it 12,796 562 480 nl 12,318 720 685 pt 8,325 559 476 ru 48,119 6,441 6,366 Table 2: SR’18 dataset sizes for training, development and test sets. 3 3.1 Evaluation Methods Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST9 is a related n-gram similarity metric weighted in favour of less frequent n-grams which are taken to be more informative. Inverse, normalised, character-based string-edit distance (DIST in the tables below) starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single)"
W18-6527,W10-4226,1,0.705033,"atically parsed data mentioned above shows. It is conceivable that a future shared task in NLG will involve paired (structured) data and text, plus an automatically created intermediate level of representation comprising underspecified UD (UUD) structures enriched with additional information obtained from the structured data level. This would correspond to three linked tracks (data-to-text, data-to-UUD, and UUD-totext) where one track is the end-to-end task, and the other two tracks are subtasks that can be combined to solve the end-to-end task, similar to the GREC’10 shared task competition (Belz and Kow, 2010). Or it could be argued, perhaps controversially still, that the days of structured linguistic representations in NLG are numbered anyway. The rapid development and spread of highly successful neural approaches to diverse NLG tasks, and the limited success so far of attempts to inject linguistic knowledge directly into neural networks, certainly lends some strength to this point of view. In the meantime, the above tripartite shared-task structure has the potential to accommodate both systems that map directly from data to text without structured representations, and two-component systems with"
W18-6527,W11-2832,1,0.853605,"arious ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependency structures offer a more flexible input structure and statistical systems, in principle, offer more robustness, the uptake of such systems as comp"
W18-6527,W04-2705,0,0.339346,"Missing"
W18-6527,W18-3601,1,0.855834,"as Inputs for Multilingual Surface Realisation Simon Mille Universitat Pompeu Fabra Barcelona, Spain simon.mille@upf.edu Anja Belz University of Brighton Brighton, UK a.s.belz@brighton.ac.uk Bernd Bohnet Google Inc. London, UK bohnetbd@google.com Leo Wanner ICREA and Universitat Pompeu Fabra Barcelona, Spain leo.wanner@upf.edu Abstract resolved. The success of SimpleNLG (Gatt and Reiter, 2009) which had much reduced grammatical coverage, but accepted radically simpler inputs demonstrated the importance of this issue. The recently completed first Multilingual Surface Realisation Task (SR’18) (Mille et al., 2018) used for the first time inputs derived from the Universal Dependencies (UDs) (de Marneffe et al., 2014), a framework which was devised with the aim of facilitating cross-linguistically consistent grammatical annotation, and which has grown into a large-scale community effort involving more than 200 contributors, who have created over 100 treebanks in over 70 languages between them.1 UDs provide a more general and potentially flexible input representation for surface realisation (SR). However, their use for NLG has not so far been demonstrated. In this paper, we present the UD datasets used in"
W18-6527,bohnet-wanner-2010-open,1,0.85998,"Missing"
W18-6527,P06-1130,0,0.111814,"Missing"
W18-6527,W96-0501,0,0.474787,"In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process. 1 Introduction There has long been an assumption in Natural Language Generation (NLG) that surface realisation can be treated as an independent subtask for which stand-alone, plug-and-play tools can, and should, be created. Early surface realisers such as KPML (Bateman, 1997) and FUF/Surge (Elhadad and Robin, 1996) were ambitious, independent surface realisation tools for English with wide grammatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG"
W18-6527,P17-1017,0,0.0215344,"mmatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG more generally. Some conclusions are presented in Section 7. • WebNLG dataset (Gardent et al., 2017): DBpedia triples covering properties of 15 DBpedia categories; 2 • E2E dataset (Novikova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journa"
W18-6527,W05-1510,0,0.0204745,"ibute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn Tree"
W18-6527,W09-0613,0,0.204092,"Missing"
W18-6527,W17-5525,0,0.0700189,"Missing"
W18-6527,W02-2103,0,0.0384065,"ova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the"
W18-6527,J05-1004,0,0.700717,"0 2 3 4 A2 ROOT A1 A2 AM Figure 3: Deep input (Track 2) derived from UD structure in Figure 1. (left: CoNLL-U, righ: graphical) ures 1 and 2 show an original UD structure and a SR’18 Shallow input, respectively. • the in the head can be seen as a marker for nominal definiteness; 3.2 • the conjunction (complementiser) that in, e.g., I demand that you apologise, appears because it connects a finite verb apologise as an argument of another verb demand. Deep inputs The Deep Track input structures are trees that contain only content words linked by predicateargument edges, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. The Deep inputs can be seen as closer to a realistic application context for NLG systems, in which the component that generates the inputs presumably would not have access to syntactic or language-specific information. At the same time, we used only information found in the UD structures to create the Deep inputs, and tried to keep their structure simple. In Deep inputs, words are not disambiguated, full (semantically loaded) prepositions may be missing, and some argument relations may be underspecified or missing. The next two subsections provide more details a"
W18-6527,P09-1011,0,0.0151157,"ncies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 2017): abstract predicate-argument graphs that cover several genres; 200 1 2 3 4 5 6 7 8 9 10 11 12 13 The third was being run by the head of an investment firm . the third be be run by the head of a investment firm . DET ADJ AUX AUX VERB ADP DET NOUN ADP DET NOUN NOUN PUNCT DT JJ VBD VBG VBN IN DT NN IN DT NN NN . Definite=Def|PronType=Art Degree=Pos|NumType=Ord Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin VerbForm=Ger Tense=Past|VerbForm=Part|Voice=Pass Definite=Def"
W18-6527,W08-2121,0,0.136007,"Missing"
W18-6527,de-marneffe-etal-2014-universal,0,0.0679075,"Missing"
W18-6527,D09-1043,0,0.027139,"ground With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependen"
W18-6527,K17-3001,0,0.0322515,"a Shallow Track, starting from syntactic structures in which word order information has been removed and tokens have been lemmatised, and a Deep Track, which starts from more abstract structures from which, additionally, functional words (in particular, auxiliaries, functional prepositions and conjunctions) and surface-oriented morphological information have been removed. Taking advantage of the growing availability of multilingual treebanks annotated with Universal Dependencies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 201"
W18-6542,N16-1087,0,0.0197341,"eebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arises. In the traditional generation task distribution, sen350 Proceedin"
W18-6542,W13-2111,0,0.013224,"se subgraphs. We interpret the problem of sentence packaging as a community detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, th"
W18-6542,R11-1012,0,0.028271,"mon Mille DTIC, UPF simon.mille@upf.edu Abstract tence packaging is largely avoided. It is assumed that the text planning module creates a text plan from selected elementary statements (elementary discourse units), establishing discourse relations between them. The sentence planning module then either aggregates the elementary statements contained in the text plan into more complex statements or keeps them as separate simple statements, depending on the language, style, preferences of the targeted reader, etc. (Shaw, 1998; Dalianis, 1999; Stone et al., 2003). Even if datadriven, as, e.g., in (Bayyarapu, 2011), this strategy may suggest itself mainly for input representations with a limited number of elementary elements and simple sentential structures as target. In the context of scalable report (or any other narration) generation, which can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statem"
W18-6542,W11-2832,0,0.0506235,"Missing"
W18-6542,J17-1001,0,0.0150892,"., AMRs; cf., e.g., (May and Priyadarshi, 2017; Song et al., 2018). For these generators, the problem of sentence packaging or aggregation is obviously obsolete. As already mentioned in the Introduction, in setups that start from input that is not yet cast into sentence structures, traditional NLTG foresees the task of (content) aggregation, which is dealt with as part of sentence planning (or microplanning): the elementary content elements, as assumed to be present in the text plan, are aggregated into more complex elements; see, among others, (Shaw, 1998; Dalianis, 1999; Stone et al., 2003; Gardent and Perez-Beltrachini, 2017). Our work is more in line with Konstas and LaFor illustration, consider in Figure 5 a subgraph obtained from a larger initial graph, which is shown in Figure 6 (the obtained subgraph is circled). The subgraph corresponds to the ground truth subgraph with a precision of 0.938 and a recall of 0.882. It might be seen that the obtained subgraph contains enough information to generate a sentence with a similar meaning as the original one. The original sentence that corresponds to the subgraph in Figure 5 is He said the company is experimenting with the technique on alfalfa, and plans to include co"
W18-6542,P17-1017,0,0.0228317,"ommunity detection algorithms (and focus only on the problem of sentence packaging), they view the entire problem of the verbalization of a hypergraph as a graph traversal problem. The difference in the size of the input data (and thus the number of the resulting sentences) is also a distinctive feature of our proposal when we compare it to other works that deal with sentence packaging. For instance, Narayan et al. (2017) split in their experiments on text simplification complex sentences into 2 to 3 more simple sentences. As content representation, they use the WebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the corefere"
W18-6542,C10-1012,1,0.790405,"ure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arises. In the traditional generation task distri"
W18-6542,W09-0613,0,0.0471738,"Missing"
W18-6542,W16-6626,0,0.0164395,"entence packaging as a community detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into"
W18-6542,W02-2101,0,0.0313778,"VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arises. In the traditional g"
W18-6542,N12-1093,0,0.0227533,"calable report (or any other narration) generation, which can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statements, sentence packaging on semantic representations appears as an alternative that is worth to be explored. More recent data-driven concept-to-text approaches to NLTG, e.g., (Konstas and Lapata, 2012), text simplification, e.g., (Narayan et al., 2017), dialogue act realization, e.g., (Mairesse and Young, 2014; Wen et al., 2015), deal with sentence packaging, but, as a rule, all of them concern inputs of limited size, with at most 3 to 5 resulting sentence packages, while realistic large input semantic graphs may give rise to dozens. In what follows, we present a model for sentence packaging of large semantic graphs, which contain up to 75 sentences. In general, the problem of sentence packaging consists in the optimal decomposition of a given An increasing amount of research tackles the ch"
W18-6542,W02-2105,0,0.119288,"eriments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arise"
W18-6542,J14-4003,0,0.016096,"-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statements, sentence packaging on semantic representations appears as an alternative that is worth to be explored. More recent data-driven concept-to-text approaches to NLTG, e.g., (Konstas and Lapata, 2012), text simplification, e.g., (Narayan et al., 2017), dialogue act realization, e.g., (Mairesse and Young, 2014; Wen et al., 2015), deal with sentence packaging, but, as a rule, all of them concern inputs of limited size, with at most 3 to 5 resulting sentence packages, while realistic large input semantic graphs may give rise to dozens. In what follows, we present a model for sentence packaging of large semantic graphs, which contain up to 75 sentences. In general, the problem of sentence packaging consists in the optimal decomposition of a given An increasing amount of research tackles the challenge of text generation from abstract ontological or semantic structures, which are in their very nature po"
W18-6542,P14-5010,0,0.00680384,"xt of numerous applications, including biomedicine (e.g., for protein interaction network (Bader and Hogue, 2003) or brain connectivity analysis (Hagmann et al., 2008)), web mining (Sarıyuece et al., 2015), influence analysis (Ugander et al., 2012), community detection (Asim et al., 2017), etc. Our model is inspired by the work on community detection. The model has been validated in experiments on the VerbNet/FrameNet annotated version of the Penn TreeBank (Mille et al., 2017), in which coreferences in the individual texts of the corpus have been identified using the Stanford CoreNLP toolkit (Manning et al., 2014) and fused to obtain a graph representation. The experiments show that we achieve an F1 -score of 0.738 (with a precision of 0.792 and a recall of 0.73), which means that our model is able to cope with the problem of sentence packaging in NLTG. The remainder of the paper is structured as follows. In Section 2, we introduce the semantic graphs that are assumed to be decomposed and analyze them. Section 3 outlines the experiments we carried out, and Section 4 discusses the outcome of these experiments. In Section 5, we briefly review the work that is related to ours. In Section 6, finally, we dr"
W18-6542,S17-2090,0,0.0604259,"Missing"
W18-6542,N01-1001,0,0.113406,"ith post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to a"
W18-6542,W18-3601,1,0.818738,"ebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the coreference resolution has been entirely automatic, with no subsequent thorough validation and manual correction. Both would be needed to ensure high quality of the resulting dataset. 6 Acknowledgments The presented work was supported by the European Commission under the contract numbers H2020-645012-RIA, H2020-7000024-RIA, H2020-700475-IA, and H2020-779962-RIA and by the Russian Foundation for Basic Research under the contract number 18-37-00198. Many thanks to the three anonymous reviewers, whose insighful comments helped to improve the final version of the paper. Re"
W18-6542,D15-1199,0,0.0215971,"Missing"
W18-6542,W18-6527,1,0.823291,"ebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the coreference resolution has been entirely automatic, with no subsequent thorough validation and manual correction. Both would be needed to ensure high quality of the resulting dataset. 6 Acknowledgments The presented work was supported by the European Commission under the contract numbers H2020-645012-RIA, H2020-7000024-RIA, H2020-700475-IA, and H2020-779962-RIA and by the Russian Foundation for Basic Research under the contract number 18-37-00198. Many thanks to the three anonymous reviewers, whose insighful comments helped to improve the final version of the paper. Re"
W18-6542,D17-1064,0,0.0926301,"ch can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statements, sentence packaging on semantic representations appears as an alternative that is worth to be explored. More recent data-driven concept-to-text approaches to NLTG, e.g., (Konstas and Lapata, 2012), text simplification, e.g., (Narayan et al., 2017), dialogue act realization, e.g., (Mairesse and Young, 2014; Wen et al., 2015), deal with sentence packaging, but, as a rule, all of them concern inputs of limited size, with at most 3 to 5 resulting sentence packages, while realistic large input semantic graphs may give rise to dozens. In what follows, we present a model for sentence packaging of large semantic graphs, which contain up to 75 sentences. In general, the problem of sentence packaging consists in the optimal decomposition of a given An increasing amount of research tackles the challenge of text generation from abstract ontologica"
W18-6542,W17-5525,0,0.0177525,"ntation, they use the WebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the coreference resolution has been entirely automatic, with no subsequent thorough validation and manual correction. Both would be needed to ensure high quality of the resulting dataset. 6 Acknowledgments The presented work was supported by the European Commission under the contract numbers H2020-645012-RIA, H2020-7000024-RIA, H2020-700475-IA, and H2020-779962-RIA and by the Russian Foundation for Basic Research under the contract number 18-37-00198. Many thanks to the three anonymous reviewers, whose insighful comments helped to improve the final vers"
W18-6542,A00-2026,0,0.0106883,"detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we"
W18-6542,W98-1415,0,0.536168,"Community Detection Problem Alexander Shvets DTIC, UPF alexander.shvets@upf.edu Simon Mille DTIC, UPF simon.mille@upf.edu Abstract tence packaging is largely avoided. It is assumed that the text planning module creates a text plan from selected elementary statements (elementary discourse units), establishing discourse relations between them. The sentence planning module then either aggregates the elementary statements contained in the text plan into more complex statements or keeps them as separate simple statements, depending on the language, style, preferences of the targeted reader, etc. (Shaw, 1998; Dalianis, 1999; Stone et al., 2003). Even if datadriven, as, e.g., in (Bayyarapu, 2011), this strategy may suggest itself mainly for input representations with a limited number of elementary elements and simple sentential structures as target. In the context of scalable report (or any other narration) generation, which can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse"
W18-6542,P18-1150,0,0.0587581,"Missing"
W19-8659,P17-1017,0,0.139318,"a tremendous amount of structured knowledge has been made publicly available as languageindependent triples; the Linked Open Data (LOD) cloud currently contains over one thousand interlinked datasets (e.g., DBpedia, Wikidata), which cover a large range of domains and amount to billions of different triples. The verbalization of LOD triples, i.e., their mapping onto sentences in natural languages, has been attracting a growing interest in the past years, as shown by the organization of dedicated events such as the WebNLG 2016 workshop (Gardent and Gangemi, 2016) and the 2017 WebNLG challenge (Gardent et al., 2017b). As a result, a variety of new NLG systems designed specifically for handling structured data have emerged, most of them statistical, as seen in the 2017 WebNLG challenge, although a number of rule-based generators have also been presented. All systems focus on English, mainly because no training data other than for English are available as yet. Given the high cost for the creation of training data, this state of affairs is likely to persist for some time. Therefore, the question on the competitiveness of rule-based generators arises. Statistical generators increasingly dominate the researc"
W19-8659,W17-3518,0,0.159541,"a tremendous amount of structured knowledge has been made publicly available as languageindependent triples; the Linked Open Data (LOD) cloud currently contains over one thousand interlinked datasets (e.g., DBpedia, Wikidata), which cover a large range of domains and amount to billions of different triples. The verbalization of LOD triples, i.e., their mapping onto sentences in natural languages, has been attracting a growing interest in the past years, as shown by the organization of dedicated events such as the WebNLG 2016 workshop (Gardent and Gangemi, 2016) and the 2017 WebNLG challenge (Gardent et al., 2017b). As a result, a variety of new NLG systems designed specifically for handling structured data have emerged, most of them statistical, as seen in the 2017 WebNLG challenge, although a number of rule-based generators have also been presented. All systems focus on English, mainly because no training data other than for English are available as yet. Given the high cost for the creation of training data, this state of affairs is likely to persist for some time. Therefore, the question on the competitiveness of rule-based generators arises. Statistical generators increasingly dominate the researc"
W19-8659,W09-0613,0,0.288225,"appeared in the training data (‘Astronaut’, ‘Building’, ‘University’, etc.), i.e., were “seen”, and five categories were “unseen”, i.e., they did not appear in the training data (‘Athlete’, ‘Artist’, etc.). At the time of the challenge, the WebNLG dataset contained about 10K distinct inputs and 25K data-text pairs; a sample data-text pair is shown in Figure 1. The neural generator ADAPT (Elder et al., 2018) performed best on seen data, and FORGe on unseen data and overall. In what follows, we aim to improve the performance of FORGe on seen data for English and furthermore port it to Spanish. (Gatt and Reiter, 2009) demonstrated that a welldefined generation infrastructure, along with a transparent, easy to handle rule and structure format, is a key for its take up and use for creation of generation modules for multiple languages. In what follows, we aim to demonstrate that the FORGe generator can also well serve as a multilingual portable text generator for verbalization of structured data and that its lexical and grammatical resources can be easily extended to reach a higher coverage of linguistic constructions. For this, we extend its publicly available resources for English, so as to improve the qual"
W19-8659,W11-2832,0,0.0259419,"lability of large scale syntactically annotated corpora and the lack of publicly available knowledge repositories, the focus had shifted to statistical surface generation. However, thanks to Semantic Web (SW) initiatives such as the W3C Linking Open Data Project,1 1 https://www.w3.org/wiki/SweoIG/ TaskForces/CommunityProjects/ LinkingOpenData 473 Proceedings of The 12th International Conference on Natural Language Generation, pages 473–483, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ing statistically the most appropriate output (Gardent et al., 2017b; Belz et al., 2011). Templatebased systems are very robust, but also limited in terms of portability since new templates need to be defined for every new domain, style, language, etc. Statistical systems have the best coverage, but the relevance and the quality of the produced texts cannot be ensured. Furthermore, they are fully dependent on the available (still scarce and mostly monolingual) training data. The development of grammar-based systems is time-consuming and they usually have coverage issues. However, they do not require training material, allow for a greater control over the outputs (e.g. for mitigat"
W19-8659,kingsbury-palmer-2002-treebank,0,0.511563,"nality and number information labels are also assigned. Last, in the case of multiple triple inputs, the triples are ordered (as a preliminary step for the subsequent aggregation) based on the number of appearances of their subjects and on whether a subject of a triple serves also as an object in another triple. For the population of the templates of Figure 2, the subject and object placeholders are simply replaced by the corresponding subjects and objects of Figure 1, without cleaning or further modification. Mapping properties to PredArg templates Predicate-argument templates in a PropBank (Kingsbury and Palmer, 2002; Babko-Malaya, 2005) fashion were defined taking into account the property as well as the type of the subject and object values.2 Thus, each of the properties found in the evaluation triples was associated to one of these templates. Parts of speech (e.g., NP –proper noun), grammatical features (e.g., verbal tense or nominal definiteness), or information from DBpedia (e.g., classes), for instance, can be specified in the template.3 Figure 2 shows sample PredArg templates for the DBpedia properties leader and language respectively;4 318 templates were used for the 373 properties of WebNLG. 3.2"
W19-8659,bohnet-wanner-2010-open,1,0.704242,"organization of the linguistic resources can be an adequate choice for NLG applications. 1 Introduction One of the rule-based generators presented at WebNLG was FORGe (Mille and Dasiopoulou, 2017), which ranked first with respect to overall quality in the human evaluation. FORGe is grounded in the linguistic model of the MeaningText Theory (Mel’ˇcuk, 1988). The multistratal nature of this model allows for a modular organization of blocks of graph-transduction rules, from blocks that are universal, i.e., multilingual, to blocks that are language-specific. The graphtransduction framework MATE (Bohnet and Wanner, 2010) furthermore facilitates a systematic hierarchical rule writing and testing. SimpleNLG The origins of Natural Language Generation (NLG) are in rule-based sentence/text generation from numerical data or deep semantic structures. With the availability of large scale syntactically annotated corpora and the lack of publicly available knowledge repositories, the focus had shifted to statistical surface generation. However, thanks to Semantic Web (SW) initiatives such as the W3C Linking Open Data Project,1 1 https://www.w3.org/wiki/SweoIG/ TaskForces/CommunityProjects/ LinkingOpenData 473 Proceeding"
W19-8659,L18-1478,0,0.0150211,"twerp and Belgium in Figure 3, which are merged at the end of the process, c.f. Figure 4. During linguistic generation, this results in the introduction of postnominal modifiers such as relative and participial clauses or appositions (see next section). In order to avoid the formation of heavy nominal groups, at most one aggregation is allowed per argument. Figure 5: Deep-syntactic structures gorisation lexicon. For this purpose, lexical resources derived from PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) or VerbNet (Schuler, 2005) are used; see (Mille and Wanner, 2015; Lareau et al., 2018). Personal and relative pronouns are introduced using the coreference relations (dotted arrows) and the class feature, which allows for distinguishing between human and non-human antecedents. Finally, morpho-syntactic agreements are resolved, the syntactic tree is linearized through the ordering of (i) governor/dependent and (ii) dependents with each other, and the surface forms are retrieved. Post-processing rules are then applied: upper casing, replacement of underscores by spaces, etc. Figure 4: Aggregated PredArg structures 3.4 Linguistic generation The next and last step is the rendering"
W19-8659,W14-4412,0,0.0557466,"Missing"
W19-8659,W16-6630,0,0.0154321,"lts of the automatic evaluation of the extended system, and Section 6 a qualitative evaluation of the outputs in both languages. Section 7, finally, draws some conclusions and presents the future work. 2 Related work The most prominent recent illustration of the portability of a generation framework is SimpleNLG. Originally developed for generation of English in practical applications (Gatt and Reiter, 2009), in the meantime it has been ported to generate, among others, in Brasilian Portuguese (De Oliveira and Sripada, 2014), Dutch (de Jong and Theune, 2018), German (Bollmann, 2011), Italian (Mazzei et al., 2016), and Spanish (Soto et al., 2017). However, while SimpleNLG is a framework for surface generation, usually with a limited coverage, we are interested in a portable multilingual framework for large scale text generation from structured data, more precisely, from DBpedia properties (Lehmann et al., 2015). Although most existing NLG generators combine different techniques, there are three main approaches to generating texts from an input sequence of structured data (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018): (i) filling slot values in predefined sentence templates (Androutsopoulos et al."
W19-8659,W18-6556,0,0.0202,"systems of types (ii) and (iii) have been presented. The task consisted in generating texts from up to 7 DBpedia triples from 15 categories, covering in total 373 distinct DBpedia properties. Nine categories appeared in the training data (‘Astronaut’, ‘Building’, ‘University’, etc.), i.e., were “seen”, and five categories were “unseen”, i.e., they did not appear in the training data (‘Athlete’, ‘Artist’, etc.). At the time of the challenge, the WebNLG dataset contained about 10K distinct inputs and 25K data-text pairs; a sample data-text pair is shown in Figure 1. The neural generator ADAPT (Elder et al., 2018) performed best on seen data, and FORGe on unseen data and overall. In what follows, we aim to improve the performance of FORGe on seen data for English and furthermore port it to Spanish. (Gatt and Reiter, 2009) demonstrated that a welldefined generation infrastructure, along with a transparent, easy to handle rule and structure format, is a key for its take up and use for creation of generation modules for multiple languages. In what follows, we aim to demonstrate that the FORGe generator can also well serve as a multilingual portable text generator for verbalization of structured data and t"
W19-8659,W04-2705,0,0.053654,"dentified, the PredArg structures are merged by fusing the common argument; see e.g. Antwerp and Belgium in Figure 3, which are merged at the end of the process, c.f. Figure 4. During linguistic generation, this results in the introduction of postnominal modifiers such as relative and participial clauses or appositions (see next section). In order to avoid the formation of heavy nominal groups, at most one aggregation is allowed per argument. Figure 5: Deep-syntactic structures gorisation lexicon. For this purpose, lexical resources derived from PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) or VerbNet (Schuler, 2005) are used; see (Mille and Wanner, 2015; Lareau et al., 2018). Personal and relative pronouns are introduced using the coreference relations (dotted arrows) and the class feature, which allows for distinguishing between human and non-human antecedents. Finally, morpho-syntactic agreements are resolved, the syntactic tree is linearized through the ordering of (i) governor/dependent and (ii) dependents with each other, and the surface forms are retrieved. Post-processing rules are then applied: upper casing, replacement of underscores by spaces, etc. Figure 4: Aggregate"
W19-8659,W16-3500,0,0.138673,"Pompeu Fabra Barcelona, Spain leo.wanner@upf.edu Abstract a tremendous amount of structured knowledge has been made publicly available as languageindependent triples; the Linked Open Data (LOD) cloud currently contains over one thousand interlinked datasets (e.g., DBpedia, Wikidata), which cover a large range of domains and amount to billions of different triples. The verbalization of LOD triples, i.e., their mapping onto sentences in natural languages, has been attracting a growing interest in the past years, as shown by the organization of dedicated events such as the WebNLG 2016 workshop (Gardent and Gangemi, 2016) and the 2017 WebNLG challenge (Gardent et al., 2017b). As a result, a variety of new NLG systems designed specifically for handling structured data have emerged, most of them statistical, as seen in the 2017 WebNLG challenge, although a number of rule-based generators have also been presented. All systems focus on English, mainly because no training data other than for English are available as yet. Given the high cost for the creation of training data, this state of affairs is likely to persist for some time. Therefore, the question on the competitiveness of rule-based generators arises. Stat"
W19-8659,W13-3724,1,0.848275,"noun and get morphological agreement features from it (third person singular), while NMOD towards a preposi5 Note that the node brought together during the previous step are not necessarily split up at this level. 476 tion causes the opposite order and no agreement, etc.: Charles Michel3sg &gt; is3sg &gt; the &gt; leader &gt;of &gt;Belgium. The final sentence generated for the four triples is The Antwerp International Airport serves Antwerp, which is in Belgium. Charles Michel is the leader of Belgium, in which the German language is spoken. 4 For designing the rules, we followed the approach of AnCora-UPF (Mille et al., 2013), a Spanish dataset in which each dependency relation is associated with a set of syntactic properties. For instance, a subject is characterized by being linearized to the left of its governing verb (by default), by being removable, by triggering the number and person agreements on the verb, etc. During the linguistic generation stage, 27 out of the 47 relations proposed in AnCora-UPF 8 are currently supported. In order to generalize the ordering rules across languages, the dependencies were introduced in the lexicon with details about how they are linearized with respect to their governor (ve"
W19-8659,P02-1040,0,0.104053,"structions containing all and only the entities and relations in the triples. The reference texts were written by one of the authors, a native Spanish speaker, having at hand the English references from the WebNLG challenge to serve as a potential model. Evaluation 5.3 In this section, we detail how we built a new dataset for evaluating the outputs, and describe the results of the automatic evaluations. 5.1 Reference sentences Automatic evaluation The predicted outputs in English and Spanish were compared to the reference sentences in the corresponding language; three metrics were used: BLEU (Papineni et al., 2002), which matches exact words, METEOR (Banerjee and Lavie, 2005), which matches also synonyms, and TER (Snover et al., 2006), which reflects the amount of edits needed to transform the predicted output into the reference output. Table 1 shows the results of the automatic evaluation on the English and Spanish Selection of triples for evaluation For evaluation purposes, we compiled a benchmark dataset of 200 inputs, i.e., sets of DBpe9 Note that we exclude from the count all rules than simply transfer individual attributes at each level, which amount to about 250. There are more English-specific r"
W19-8659,C16-1141,0,0.0239632,"Deep-Syntactic graphs, apply for both languages. When getting closer to the surface, the rules are less languageindependent, representing about half of the DSyntSSynt rules (108/239) and of the linearization and agreement resolution rules (66/129). 5 dia triples, with sizes ranging from 1 to 7 triples, using as reference pool the WebNLG challenge test set. The reason for using as reference basis the WebNLG challenge dataset is that it is the most recent and comprehensive dataset with respect to text generation from RDF data that has been specifically designed to promote data and text variety (Perez-Beltrachini et al., 2016). Moreover, it allows the direct comparison with the generators that participated in the challenge. In order to ensure future comparisons with machine learning-based systems in terms of their best obtained performance, only the seen categories subset of the original test set has been considered, i.e., only inputs with entities that belonged to DBpedia categories that were contained in the training data. The compilation methodology for our benchmark dataset implements a twofold goal. On one hand, we want to ensure that all properties appearing in the seen categories subset are included. On the"
W19-8659,2006.amta-papers.25,0,0.0889691,"authors, a native Spanish speaker, having at hand the English references from the WebNLG challenge to serve as a potential model. Evaluation 5.3 In this section, we detail how we built a new dataset for evaluating the outputs, and describe the results of the automatic evaluations. 5.1 Reference sentences Automatic evaluation The predicted outputs in English and Spanish were compared to the reference sentences in the corresponding language; three metrics were used: BLEU (Papineni et al., 2002), which matches exact words, METEOR (Banerjee and Lavie, 2005), which matches also synonyms, and TER (Snover et al., 2006), which reflects the amount of edits needed to transform the predicted output into the reference output. Table 1 shows the results of the automatic evaluation on the English and Spanish Selection of triples for evaluation For evaluation purposes, we compiled a benchmark dataset of 200 inputs, i.e., sets of DBpe9 Note that we exclude from the count all rules than simply transfer individual attributes at each level, which amount to about 250. There are more English-specific rules simply because the coverage of the English generator is higher. 479 extensions proposed in this paper using for each"
W19-8659,W17-3521,0,0.0221256,"the extended system, and Section 6 a qualitative evaluation of the outputs in both languages. Section 7, finally, draws some conclusions and presents the future work. 2 Related work The most prominent recent illustration of the portability of a generation framework is SimpleNLG. Originally developed for generation of English in practical applications (Gatt and Reiter, 2009), in the meantime it has been ported to generate, among others, in Brasilian Portuguese (De Oliveira and Sripada, 2014), Dutch (de Jong and Theune, 2018), German (Bollmann, 2011), Italian (Mazzei et al., 2016), and Spanish (Soto et al., 2017). However, while SimpleNLG is a framework for surface generation, usually with a limited coverage, we are interested in a portable multilingual framework for large scale text generation from structured data, more precisely, from DBpedia properties (Lehmann et al., 2015). Although most existing NLG generators combine different techniques, there are three main approaches to generating texts from an input sequence of structured data (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018): (i) filling slot values in predefined sentence templates (Androutsopoulos et al., 2013), (ii) applying grammars ("
