2021.naacl-main.304,Learning to Learn to be Right for the Right Reasons,2021,-1,-1,4,0,4151,pride kavumba,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline."
2021.findings-acl.10,{S}y{GNS}: A Systematic Generalization Testbed Based on Natural Language Semantics,2021,-1,-1,3,1,7510,hitomi yanaka,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.266,{ {SHAPE} }: {S}hifted Absolute Position Embedding for Transformers,2021,-1,-1,4,1,4615,shun kiyono,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Position representation is crucial for building position-aware representations in Transformers. Existing position representations suffer from a lack of generalization to test data with unseen lengths or high computational cost. We investigate shifted absolute position embedding (SHAPE) to address both issues. The basic idea of SHAPE is to achieve shift invariance, which is a key property of recent successful position representations, by randomly shifting absolute positions during training. We demonstrate that SHAPE is empirically comparable to its counterpart while being simpler and faster."
2021.emnlp-main.308,Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution,2021,-1,-1,5,1,9337,ryuto konno,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Masked language models (MLMs) have contributed to drastic performance improvements with regard to zero anaphora resolution (ZAR). To further improve this approach, in this study, we made two proposals. The first is a new pretraining task that trains MLMs on anaphoric relations with explicit supervision, and the second proposal is a new finetuning method that remedies a notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese ZAR demonstrated that our two proposals boost the state-of-the-art performance, and our detailed analysis provides new insights on the remaining challenges."
2021.emnlp-main.335,Transformer-based Lexically Constrained Headline Generation,2021,-1,-1,6,0,8399,kosuke yamada,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a headline including a given phrase by providing the encoder with additional information corresponding to the given phrase. However, these methods cannot always include the phrase in the generated headline. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated headline, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous strategies."
2021.emnlp-main.373,{I}ncorporating {R}esidual and {N}ormalization {L}ayers into {A}nalysis of {M}asked {L}anguage {M}odels,2021,-1,-1,4,0,9470,goro kobayashi,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers{'} progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available."
2021.emnlp-main.490,Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension,2021,-1,-1,5,1,7295,naoya inoue,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"How can we generate concise explanations for multi-hop Reading Comprehension (RC)? The current strategies of identifying supporting sentences can be seen as an extractive question-focused summarization of the input text. However, these extractive explanations are not necessarily concise i.e. not minimally sufficient for answering a question. Instead, we advocate for an abstractive approach, where we propose to generate a question-focused, abstractive summary of input paragraphs and then feed it to an RC system. Given a limited amount of human-annotated abstractive explanations, we train the abstractive explainer in a semi-supervised manner, where we start from the supervised model and then train it further through trial and error maximizing a conciseness-promoted reward function. Our experiments demonstrate that the proposed abstractive explainer can generate more compact explanations than an extractive explainer with limited supervision (only 2k instances) while maintaining sufficiency."
2021.emnlp-main.766,Exploring Methods for Generating Feedback Comments for Writing Learning,2021,-1,-1,3,1,5977,kazuaki hanawa,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"The task of generating explanatory notes for language learners is known as feedback comment generation. Although various generation techniques are available, little is known about which methods are appropriate for this task. Nagata (2019) demonstrates the effectiveness of neural-retrieval-based methods in generating feedback comments for preposition use. Retrieval-based methods have limitations in that they can only output feedback comments existing in a given training data. Furthermore, feedback comments can be made on other grammatical and writing items than preposition use, which is still unaddressed. To shed light on these points, we investigate a wider range of methods for generating many feedback comments in this study. Our close analysis of the type of task leads us to investigate three different architectures for comment generation: (i) a neural-retrieval-based method as a baseline, (ii) a pointer-generator-based generation method as a neural seq2seq method, (iii) a retrieve-and-edit method, a hybrid of (i) and (ii). Intuitively, the pointer-generator should outperform neural-retrieval, and retrieve-and-edit should perform best. However, in our experiments, this expectation is completely overturned. We closely analyze the results to reveal the major causes of these counter-intuitive results and report on our findings from the experiments."
2021.eacl-main.78,Exploring Transitivity in Neural {NLI} Models through Veridicality,2021,-1,-1,3,1,7510,hitomi yanaka,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Despite the recent success of deep neural networks in natural language processing, the extent to which they can demonstrate human-like generalization capacities for natural language understanding remains unclear. We explore this issue in the domain of natural language inference (NLI), focusing on the transitivity of inference relations, a fundamental property for systematically drawing inferences. A model capturing transitivity can compose basic inference patterns and draw new inferences. We introduce an analysis method using synthetic and naturalistic NLI datasets involving clause-embedding verbs to evaluate whether models can perform transitivity inferences composed of veridical inferences and arbitrary inference types. We find that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples. The data and code for our analysis are publicly available at https://github.com/verypluming/transitivity."
2021.eacl-main.153,"Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",2021,-1,-1,2,0.797101,4152,benjamin heinzerling,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major benefit of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases."
2021.eacl-main.321,Two Training Strategies for Improving Relation Extraction over Universal Graph,2021,-1,-1,4,1,10996,qin dai,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies: (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-of-the-art result on the NYT10 dataset. The code and datasets used in this paper are available at https://github.com/baodaiqin/UGDSRE."
2021.argmining-1.6,Exploring Methodologies for Collecting High-Quality Implicit Reasoning in Arguments,2021,-1,-1,5,0,12281,keshav singh,Proceedings of the 8th Workshop on Argument Mining,0,"Annotation of implicit reasoning (i.e., warrant) in arguments is a critical resource to train models in gaining deeper understanding and correct interpretation of arguments. However, warrants are usually annotated in unstructured form, having no restriction on their lexical structure which sometimes makes it difficult to interpret how warrants relate to any of the information given in claim and premise. Moreover, assessing and determining better warrants from the large variety of reasoning patterns of unstructured warrants becomes a formidable task. Therefore, in order to annotate warrants in a more interpretative and restrictive way, we propose two methodologies to annotate warrants in a semi-structured form. To the best of our knowledge, we are the first to show how such semi-structured warrants can be annotated on a large scale via crowdsourcing. We demonstrate through extensive quality evaluation that our methodologies enable collecting better quality warrants in comparison to unstructured annotations. To further facilitate research towards the task of explicating warrants in arguments, we release our materials publicly (i.e., crowdsourcing guidelines and collected warrants)."
2021.acl-long.405,Lower Perplexity is Not Always Human-Like,2021,-1,-1,6,1,9471,tatsuki kuribayashi,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization {---}\textit{the lower perplexity a language model has, the more human-like the language model is}{---} in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models."
2020.sustainlp-1.6,Efficient Estimation of Influence of a Training Instance,2020,-1,-1,4,1,9187,sosuke kobayashi,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,0,"Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model{'}s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization."
2020.nlpcovid19-2.13,A System for Worldwide {COVID}-19 Information Aggregation,2020,-1,-1,6,0,5182,akiko aizawa,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories."
2020.lrec-1.42,Creating Corpora for Research in Feedback Comment Generation,2020,-1,-1,2,0,5976,ryo nagata,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we report on datasets that we created for research in feedback comment generation {---} a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning. There has been almost no such corpus open to the public and accordingly there has been a very limited amount of work on this task. In this paper, we first discuss the principle and guidelines for feedback comment annotation. Then, we describe two corpora that we have manually annotated with feedback comments (approximately 50,000 general comments and 6,700 on preposition use). A part of the annotation results is now available on the web, which will facilitate research in feedback comment generation"
2020.lantern-1.3,Seeing the World through Text: Evaluating Image Descriptions for Commonsense Reasoning in Machine Reading Comprehension,2020,-1,-1,5,0,18557,diana galvansosa,Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN),0,"Despite recent achievements in natural language understanding, reasoning over commonsense knowledge still represents a big challenge to AI systems. As the name suggests, common sense is related to perception and as such, humans derive it from experience rather than from literary education. Recent works in the NLP and the computer vision field have made the effort of making such knowledge explicit using written language and visual inputs, respectively. Our premise is that the latter source fits better with the characteristics of commonsense acquisition. In this work, we explore to what extent the descriptions of real-world scenes are sufficient to learn common sense about different daily situations, drawing upon visual information to answer script knowledge questions."
2020.findings-emnlp.26,A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction,2020,-1,-1,5,1,5978,masato mita,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Existing approaches for grammatical error correction (GEC) largely rely on supervised learning with manually created GEC datasets. However, there has been little focus on verifying and ensuring the quality of the datasets, and on how lower-quality data might affect GEC performance. We indeed found that there is a non-negligible amount of {``}noise{''} where errors were inappropriately edited or left uncorrected. To address this, we designed a self-refinement method where the key idea is to denoise these datasets by leveraging the prediction consistency of existing models, and outperformed strong denoising baseline methods. We further applied task-specific techniques and achieved state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks. We then analyzed the effect of the proposed denoising method, and found that our approach leads to improved coverage of corrections and facilitated fluency edits which are reflected in higher recall and overall performance."
2020.emnlp-main.68,Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness,2020,-1,-1,4,1,20121,reina akama,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness. The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities. We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality. Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality. We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation."
2020.emnlp-main.236,Word Rotator{'}s Distance,2020,-1,-1,5,1,9472,sho yokoi,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity. However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover{'}s distance (optimal transport), which we refer to as word rotator{'}s distance. Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method. On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines. The source code is avaliable at https://github.com/eumesy/wrd"
2020.emnlp-main.574,Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,2020,30,0,4,0,9470,goro kobayashi,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers."
2020.emnlp-demos.28,Langsmith: An Interactive Academic Text Revision System,2020,-1,-1,5,1,13281,takumi ito,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Despite the current diversity and inclusion initiatives in the academic community, researchers with a non-native command of English still face significant obstacles when writing papers in English. This paper presents the Langsmith editor, which assists inexperienced, non-native researchers to write English papers, especially in the natural language processing (NLP) field. Our system can suggest fluent, academic-style sentences to writers based on their rough, incomplete phrases or sentences. The system also encourages interaction between human writers and the computerized revision system. The experimental results demonstrated that Langsmith helps non-native English-speaker students write papers in English. The system is available at https://emnlp-demo.editor. langsmith.co.jp/."
2020.coling-main.160,Modeling Event Salience in Narratives via Barthes{'} Cardinal Functions,2020,-1,-1,6,0,21244,takaki otake,Proceedings of the 28th International Conference on Computational Linguistics,0,"Events in a narrative differ in salience: some are more important to the story than others. Estimating event salience is useful for tasks such as story generation, and as a tool for text analysis in narratology and folkloristics. To compute event salience without any annotations, we adopt Barthes{'} definition of event salience and propose several unsupervised methods that require only a pre-trained language model. Evaluating the proposed methods on folktales with event salience annotation, we show that the proposed methods outperform baseline methods and find fine-tuning a language model on narrative texts is a key factor in improving the proposed methods."
2020.coling-main.435,An Empirical Study of Contextual Data Augmentation for {J}apanese Zero Anaphora Resolution,2020,-1,-1,6,1,9337,ryuto konno,Proceedings of the 28th International Conference on Computational Linguistics,0,"One critical issue of zero anaphora resolution (ZAR) is the scarcity of labeled data. This study explores how effectively this problem can be alleviated by data augmentation. We adopt a state-of-the-art data augmentation method, called the contextual data augmentation (CDA), that generates labeled training instances using a pretrained language model. The CDA has been reported to work well for several other natural language processing tasks, including text classification and machine translation. This study addresses two underexplored issues on CDA, that is, how to reduce the computational cost of data augmentation and how to ensure the quality of the generated data. We also propose two methods to adapt CDA to ZAR: [MASK]-based augmentation and linguistically-controlled masking. Consequently, the experimental results on Japanese ZAR show that our methods contribute to both the accuracy gainand the computation cost reduction. Our closer analysis reveals that the proposed method can improve the quality of the augmented training data when compared to the conventional CDA."
2020.coling-main.521,{P}he{MT}: A Phenomenon-wise Dataset for Machine Translation Robustness on User-Generated Contents,2020,-1,-1,7,0,21640,ryo fujii,Proceedings of the 28th International Conference on Computational Linguistics,0,"Neural Machine Translation (NMT) has shown drastic improvement in its quality when translating clean input, such as text from the news domain. However, existing studies suggest that NMT still struggles with certain kinds of input with considerable noise, such as User-Generated Contents (UGC) on the Internet. To make better use of NMT for cross-cultural communication, one of the most promising directions is to develop a model that correctly handles these expressions. Though its importance has been recognized, it is still not clear as to what creates the great gap in performance between the translation of clean input and that of UGC. To answer the question, we present a new dataset, PheMT, for evaluating the robustness of MT systems against specific linguistic phenomena in Japanese-English translation. Our experiments with the created dataset revealed that not only our in-house models but even widely used off-the-shelf systems are greatly disturbed by the presence of certain phenomena."
2020.acl-srw.30,Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition,2020,-1,-1,6,0,22502,takuma kato,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"In general, the labels used in sequence labeling consist of different types of elements. For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person). However, while most sequence labeling models do not consider such label components, the shared components across labels, such as Person, can be beneficial for label prediction. In this work, we propose to integrate label component information as embeddings into models. Through experiments on English and Japanese fine-grained named entity recognition, we demonstrate that the proposed method improves performance, especially for instances with low-frequency labels."
2020.acl-srw.32,Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation,2020,-1,-1,7,0,22504,hiroaki funayama,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Many recent Short Answer Scoring (SAS) systems have employed Quadratic Weighted Kappa (QWK) as the evaluation measure of their systems. However, we hypothesize that QWK is unsatisfactory for the evaluation of the SAS systems when we consider measuring their effectiveness in actual usage. We introduce a new task formulation of SAS that matches the actual usage. In our formulation, the SAS systems should extract as many scoring predictions that are not critical scoring errors (CSEs). We conduct the experiments in our new task formulation and demonstrate that a typical SAS system can predict scores with zero CSE for approximately 50{\%} of test data at maximum by filtering out low-reliablility predictions on the basis of a certain confidence estimation. This result directly indicates the possibility of reducing half the scoring cost of human raters, which is more preferable for the evaluation of SAS systems."
2020.acl-main.47,Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in {J}apanese,2020,31,0,4,1,9471,tatsuki kuribayashi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LM-based method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments."
2020.acl-main.55,Evaluating Dialogue Generation Systems via Response Selection,2020,20,0,5,0,22572,shiki sato,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses. Through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU."
2020.acl-main.391,Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction,2020,35,0,5,0.297619,3204,masahiro kaneko,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec."
2020.acl-main.543,Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?,2020,39,0,4,1,7510,hitomi yanaka,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition. We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits. A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets. However, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set. This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set."
2020.acl-main.575,Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition,2020,47,0,7,1,9339,hiroki ouchi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance."
2020.acl-main.602,{R}4{C}: A Benchmark for Evaluating {RC} Systems to Get the Right Answer for the Right Reason,2020,15,0,3,1,7295,naoya inoue,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets. This prevents the community from reliably measuring the progress of RC systems. To address this issue, we introduce R4C, a new task for evaluating RC systems{'} internal reasoning. R4C requires giving not only answers but also derivations: explanations that justify predicted answers. We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations. We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e. 13.8k derivations). Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that R4C assesses different skills from an existing benchmark."
W19-8606,Diamonds in the Rough: Generating Fluent Sentences from Early-Stage Drafts for Academic Writing Assistance,2019,38,0,7,1,13281,takumi ito,Proceedings of the 12th International Conference on Natural Language Generation,0,"The writing process consists of several stages such as drafting, revising, editing, and proofreading. Studies on writing assistance, such as grammatical error correction (GEC), have mainly focused on sentence editing and proofreading, where surface-level issues such as typographical errors, spelling errors, or grammatical errors should be corrected. We broaden this focus to include the earlier revising stage, where sentences require adjustment to the information included or major rewriting and propose Sentence-level Revision (SentRev) as a new writing assistance task. Well-performing systems in this task can help inexperienced authors by producing fluent, complete sentences given their rough, incomplete drafts. We build a new freely available crowdsourced evaluation dataset consisting of incomplete sentences authored by non-native writers paired with their final versions extracted from published academic papers for developing and evaluating SentRev models. We also establish baseline performance on SentRev using our newly built evaluation dataset."
W19-8641,A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation,2019,0,0,7,1,9399,yuta hitomi,Proceedings of the 12th International Conference on Natural Language Generation,0,"Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to news production. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two corpora, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for headlines of three different lengths composed by professional editors. We report new findings on these corpora; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems."
W19-4804,Can Neural Networks Understand Monotonicity Reasoning?,2019,33,1,4,1,7510,hitomi yanaka,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55{\%}, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning."
W19-4433,Analytic Score Prediction and Justification Identification in Automated Short Answer Scoring,2019,0,0,7,0.628235,22506,tomoya mizumoto,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"This paper provides an analytical assessment of student short answer responses with a view to potential benefits in pedagogical contexts. We first propose and formalize two novel analytical assessment tasks: analytic score prediction and justification identification, and then provide the first dataset created for analytic short answer scoring research. Subsequently, we present a neural baseline model and report our extensive empirical results to demonstrate how our dataset can be used to explore new and intriguing technical challenges in short answer scoring. The dataset is publicly available for research purposes."
W19-2601,Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention,2019,0,0,5,1,10996,qin dai,Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications,0,"The increased demand for structured scientific knowledge has attracted considerable attention in extracting scientific relation from the ever growing scientific publications. Distant supervision is widely applied approach to automatically generate large amounts of labelled data with low manual annotation cost. However, distant supervision inevitably accompanies the wrong labelling problem, which will negatively affect the performance of Relation Extraction (RE). To address this issue, (Han et al., 2018) proposes a novel framework for jointly training both RE model and Knowledge Graph Completion (KGC) model to extract structured knowledge from non-scientific dataset. In this work, we firstly investigate the feasibility of this framework on scientific dataset, specifically on biomedical dataset. Secondly, to achieve better performance on the biomedical dataset, we extend the framework with other competitive KGC models. Moreover, we proposed a new end-to-end KGC model to extend the framework. Experimental results not only show the feasibility of the framework on the biomedical dataset, but also indicate the effectiveness of our extensions, because our extended model achieves significant and consistent improvements on distant supervised RE as compared with baselines."
W19-2605,Annotating with Pros and Cons of Technologies in Computer Science Papers,2019,0,0,4,0,24654,hono shirai,Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications,0,"This paper explores a task for extracting a technological expression and its pros/cons from computer science papers. We report ongoing efforts on an annotated corpus of pros/cons and an analysis of the nature of the automatic extraction task. Specifically, we show how to adapt the targeted sentiment analysis task for pros/cons extraction in computer science papers and conduct an annotation study. In order to identify the challenges of the automatic extraction task, we construct a strong baseline model and conduct an error analysis. The experiments show that pros/cons can be consistently annotated by several annotators, and that the task is challenging due to domain-specific knowledge. The annotated dataset is made publicly available for research purposes."
S19-2185,The Sally Smedley Hyperpartisan News Detector at {S}em{E}val-2019 Task 4,2019,0,0,5,1,5977,kazuaki hanawa,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This paper describes our system submitted to the formal run of SemEval-2019 Task 4: Hyperpartisan news detection. Our system is based on a linear classifier using several features, i.e., 1) embedding features based on the pre-trained BERT embeddings, 2) article length features, and 3) embedding features of informative phrases extracted from by-publisher dataset. Our system achieved 80.9{\%} accuracy on the test set for the formal run and got the 3rd place out of 42 teams."
S19-1027,{HELP}: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning,2019,24,3,4,1,7510,hitomi yanaka,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model architectures themselves. To investigate this issue, we introduce a new dataset, called HELP, for handling entailments with lexical and logical phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonicity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data augmentation while others are immune to it."
P19-2053,Unsupervised Learning of Discourse-Aware Text Representation for Essay Scoring,2019,0,0,5,0,12282,farjana mim,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Existing document embedding approaches mainly focus on capturing sequences of words in documents. However, some document classification and regression tasks such as essay scoring need to consider discourse structure of documents. Although some prior approaches consider this issue and utilize discourse structure of text for document classification, these approaches are dependent on computationally expensive parsers. In this paper, we propose an unsupervised approach to capture discourse structure in terms of coherence and cohesion for document embedding that does not require any expensive parser or annotation. Extrinsic evaluation results show that the document representation obtained from our approach improves the performance of essay Organization scoring and Argument Strength scoring."
P19-1464,An Empirical Study of Span Representations in Argumentation Structure Parsing,2019,0,1,7,1,9471,tatsuki kuribayashi,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"For several natural language processing (NLP) tasks, span representation design is attracting considerable attention as a promising new technique; a common basis for an effective design has been established. With such basis, exploring task-dependent extensions for argumentation structure parsing (ASP) becomes an interesting research direction. This study investigates (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. Our extensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed."
N19-1132,Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models {---} Is Single-Corpus Evaluation Enough?,2019,22,0,5,1,5978,masato mita,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models. GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark. However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics. To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models{'} rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models."
N19-1353,{S}ubword-based {C}ompact {R}econstruction of {W}ord {E}mbeddings,2019,0,0,3,1,22505,shota sasaki,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The idea of subword-based word embeddings has been proposed in the literature, mainly for solving the out-of-vocabulary (OOV) word problem observed in standard word-based word embeddings. In this paper, we propose a method of reconstructing pre-trained word embeddings using subword information that can effectively represent a large number of subword embeddings in a considerably small fixed space. The key techniques of our method are twofold: memory-shared embeddings and a variant of the key-value-query self-attention mechanism. Our experiments show that our reconstructed subword-based embeddings can successfully imitate well-trained word embeddings in a small fixed space while preventing quality degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks."
D19-6610,Improving Evidence Detection by Leveraging Warrants,2019,-1,-1,5,0,12281,keshav singh,Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER),0,"Recognizing the implicit link between a claim and a piece of evidence (i.e. warrant) is the key to improving the performance of evidence detection. In this work, we explore the effectiveness of automatically extracted warrants for evidence detection. Given a claim and candidate evidence, our proposed method extracts multiple warrants via similarity search from an existing, structured corpus of arguments. We then attentively aggregate the extracted warrants, considering the consistency between the given argument and the acquired warrants. Although a qualitative analysis on the warrants shows that the extraction method needs to be improved, our results indicate that our method can still improve the performance of evidence detection."
D19-6119,Inject Rubrics into Short Answer Grading System,2019,0,0,5,0,26472,tianqi wang,Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019),0,"Short Answer Grading (SAG) is a task of scoring students{'} answers in examinations. Most existing SAG systems predict scores based only on the answers, including the model used as base line in this paper, which gives the-state-of-the-art performance. But they ignore important evaluation criteria such as rubrics, which play a crucial role for evaluating answers in real-world situations. In this paper, we present a method to inject information from rubrics into SAG systems. We implement our approach on top of word-level attention mechanism to introduce the rubric information, in order to locate information in each answer that are highly related to the score. Our experimental results demonstrate that injecting rubric information effectively contributes to the performance improvement and that our proposed model outperforms the state-of-the-art SAG model on the widely used ASAP-SAS dataset under low-resource settings."
D19-6004,"When Choosing Plausible Alternatives, Clever Hans can be Clever",2019,23,1,6,0,4151,pride kavumba,Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing,0,"Pretrained language models, such as BERT and RoBERTa, have shown large improvements in the commonsense reasoning benchmark COPA. However, recent work found that many improvements in benchmarks of natural language understanding are not due to models learning the task, but due to their increasing ability to exploit superficial cues, such as tokens that occur more often in the correct answer than the wrong one. Are BERT{'}s and RoBERTa{'}s good performance on COPA also caused by this? We find superficial cues in COPA, as well as evidence that BERT exploits these cues.To remedy this problem, we introduce Balanced COPA, an extension of COPA that does not suffer from easy-to-exploit single token cues. We analyze BERT{'}s and RoBERTa{'}s performance on original and Balanced COPA, finding that BERT relies on superficial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that BERT learns the task to a certain degree when forced to. In contrast, RoBERTa does not appear to rely on superficial cues."
D19-3039,{TEASPN}: Framework and Protocol for Integrated Writing Assistance Environments,2019,0,0,5,0,752,masato hagiwara,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with writing software. We propose TEASPN, a protocol and an open-source framework for achieving integrated writing assistance environments. The protocol standardizes the way writing software communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in natural language processing (NLP) with low cost. As a result, users can enjoy the integrated experience in their favorite writing software. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text."
D19-1054,Select and Attend: Towards Controllable Content Selection in Text Generation,2019,0,4,3,0,5981,xiaoyu shen,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Many text generation tasks naturally contain two steps: content selection and surface realization. Current neural encoder-decoder models conflate both steps into a black-box architecture. As a result, the content to be described in the text cannot be explicitly controlled. This paper tackles this problem by decoupling content selection from the decoder. The decoupled content selection is human interpretable, whose value can be manually manipulated to control the content of generated text. The model can be trained end-to-end without human annotations by maximizing a lower bound of the marginal likelihood. We further propose an effective way to trade-off between performance and controllability with a single adjustable hyperparameter. In both data-to-text and headline generation tasks, our model achieves promising results, paving the way for controllable content selection in text generation."
D19-1119,An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction,2019,0,2,5,1,4615,shun kiyono,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F0.5=65.0) and the official test set of the BEA-2019 shared task (F0.5=70.2) without making any modifications to the model architecture."
D19-1379,Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis,2019,0,0,3,1,9339,hiroki ouchi,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In transductive learning, an unlabeled test set is used for model training. Although this setting deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, transductive learning is underexplored in natural language processing. Here we conduct an empirical study of transductive learning for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings."
Y18-1001,Multi-dialect Neural Machine Translation and Dialectometry,2018,0,0,4,0,295,kaori abe,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1015,Improving Scientific Relation Classification with Task Specific Supersense,2018,0,1,4,1,10996,qin dai,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1034,Reducing Odd Generation from Neural Headline Generation,2018,0,1,5,1,4615,shun kiyono,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1074,Suspicious News Detection Using Micro Blog Text,2018,25,0,7,0,27558,tsubasa tagami,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,"We present a new task, suspicious news detection using micro blog text. This task aims to support human experts to detect suspicious news articles to be verified, which is costly but a crucial step before verifying the truthfulness of the articles. Specifically, in this task, given a set of posts on SNS referring to a news article, the goal is to judge whether the article is to be verified or not. For this task, we create a publicly available dataset in Japanese and provide benchmark results by using several basic machine learning techniques. Experimental results show that our models can reduce the cost of manual fact-checking process."
W18-5607,Investigating the Challenges of Temporal Relation Extraction from Clinical Text,2018,0,3,4,0,27896,diana galvan,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Temporal reasoning remains as an unsolved task for Natural Language Processing (NLP), particularly demonstrated in the clinical domain. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate: the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning."
W18-5410,Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models,2018,0,1,5,1,4615,shun kiyono,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"Developing a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention matrix to interpret how Encoder-Decoder-based models translate a given source sentence to the corresponding target sentence. However, recent studies have empirically revealed that an attention matrix is not optimal for token-wise translation analyses. We propose a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism."
W18-5210,Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument Templates,2018,0,1,4,1,24185,paul reisert,Proceedings of the 5th Workshop on Argument Mining,0,"Most of the existing works on argument mining cast the problem of argumentative structure identification as classification tasks (e.g. attack-support relations, stance, explicit premise/claim). This paper goes a step further by addressing the task of automatically identifying reasoning patterns of arguments using predefined templates, which is called \textit{argument template (AT) instantiation}. The contributions of this work are three-fold. First, we develop a simple, yet expressive set of easily annotatable ATs that can represent a majority of writer{'}s reasoning for texts with diverse policy topics while maintaining the computational feasibility of the task. Second, we create a small, but highly reliable annotated corpus of instantiated ATs on top of reliably annotated support and attack relations and conduct an annotation study. Third, we formulate the task of AT instantiation as structured prediction constrained by a feasible set of templates. Our evaluation demonstrates that we can annotate ATs with a reasonably high inter-annotator agreement, and the use of template-constrained inference is useful for instantiating ATs with only partial reasoning comprehension clues."
P18-2091,Unsupervised Learning of Style-sensitive Word Vectors,2018,14,0,5,1,20121,reina akama,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner. We propose extending the continuous bag of words (CBOW) embedding model (Mikolov et al., 2013b) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent. In addition, we introduce a novel task to predict lexical stylistic similarity and to create a benchmark dataset for this task. Our experiment with this dataset supports our assumption and demonstrates that the proposed extensions contribute to the acquisition of style-sensitive word embeddings."
P18-1200,Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder,2018,0,2,3,0,10997,ryo takahashi,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base. Intuitively, a relation can be modeled by a matrix mapping entity vectors. However, relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices {--} for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency{\_}of{\_}country and country{\_}of{\_}film usually matches currency{\_}of{\_}film{\_}budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2=M3). In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an autoencoder leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at \url{github.com/tianran/glimvec}."
N18-2073,Cross-Lingual Learning-to-Rank with Shared Representations,2018,0,13,5,1,22505,shota sasaki,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Cross-lingual information retrieval (CLIR) is a document retrieval task where the documents are written in a language different from that of the user{'}s query. This is a challenging problem for data-driven approaches due to the general lack of labeled training data. We introduce a large-scale dataset derived from Wikipedia to support CLIR research in 25 languages. Further, we present a simple yet effective neural learning-to-rank model that shares representations across languages and reduces the data requirement. This model can exploit training data in, for example, Japanese-English CLIR to improve the results of Swahili-English CLIR."
N18-1015,A Melody-Conditioned Lyrics Language Model,2018,0,6,5,1,16335,kento watanabe,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"This paper presents a novel, data-driven language model that produces entire lyrics for a given input melody. Previously proposed models for lyrics generation suffer from the inability of capturing the relationship between lyrics and melody partly due to the unavailability of lyrics-melody aligned data. In this study, we first propose a new practical method for creating a large collection of lyrics-melody aligned data and then create a collection of 1,000 lyrics-melody pairs augmented with precise syllable-note alignments and word/sentence/paragraph boundaries. We then provide a quantitative analysis of the correlation between word/sentence/paragraph boundaries in lyrics and melodies. We then propose an RNN-based lyrics language model conditioned on a featurized melody. Experimental results show that the proposed model generates fluent lyrics while maintaining the compatibility between boundaries of lyrics and melody structures."
D18-1203,Pointwise {HSIC}: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions,2018,0,0,5,1,9472,sho yokoi,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI). As well as deriving PMI from mutual information, we derive this new measure from the Hilbert{--}Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels. Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNN-based PMI while outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs."
D18-1453,What Makes Reading Comprehension Questions Easier?,2018,0,26,2,0,5181,saku sugawara,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC."
C18-1009,Distance-Free Modeling of Multi-Predicate Interactions in End-to-End {J}apanese Predicate-Argument Structure Analysis,2018,26,0,2,1,9338,yuichiroh matsubayashi,Proceedings of the 27th International Conference on Computational Linguistics,0,"Capturing interactions among multiple predicate-argument structures (PASs) is a crucial issue in the task of analyzing PAS in Japanese. In this paper, we propose new Japanese PAS analysis models that integrate the label prediction information of arguments in multiple PASs by extending the input and last layers of a standard deep bidirectional recurrent neural network (bi-RNN) model. In these models, using the mechanisms of pooling and attention, we aim to directly capture the potential interactions among multiple PASs, without being disturbed by the word order and distance. Our experiments show that the proposed models improve the prediction accuracy specifically for cases where the predicate and argument are in an indirect dependency relation and achieve a new state of the art in the overall $F_1$ on a standard benchmark corpus."
C18-1286,Predicting Stances from Social Media Posts using Factorization Machines,2018,0,1,4,1,30916,akira sasaki,Proceedings of the 27th International Conference on Computational Linguistics,0,"Social media provide platforms to express, discuss, and shape opinions about events and issues in the real world. An important step to analyze the discussions on social media and to assist in healthy decision-making is stance detection. This paper presents an approach to detect the stance of a user toward a topic based on their stances toward other topics and the social media posts of the user. We apply factorization machines, a widely used method in item recommendation, to model user preferences toward topics from the social media data. The experimental results demonstrate that users{'} posts are useful to model topic preferences and therefore predict stances of silent users."
Y17-1045,A Crowdsourcing Approach for Annotating Causal Relation Instances in {W}ikipedia,2017,20,0,4,1,5977,kazuaki hanawa,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
W17-6937,Handling Multiword Expressions in Causality Estimation,2017,6,7,5,1,22505,shota sasaki,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-4208,Analyzing the Revision Logs of a {J}apanese Newspaper for Article Quality Assessment,2017,2,0,4,1,9400,hideaki tamori,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,We address the issue of the quality of journalism and analyze daily article revision logs from a Japanese newspaper company. The revision logs contain data that can help reveal the requirements of quality journalism such as the types and number of edit operations and aspects commonly focused in revision. This study also discusses potential applications such as quality assessment and automatic article revision as our future research directions.
P17-1037,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,2017,31,0,4,1,30916,akira sasaki,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We presents in this paper our approach for modeling inter-topic preferences of Twitter users: for example, {``}those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade{''}. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, electoral prediction, electoral campaigns, and online debates. In order to extract users{'} preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., {``}A is completely wrong{''}). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users{'} preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences."
I17-2022,Revisiting the Design Issues of Local Models for {J}apanese Predicate-Argument Structure Analysis,2017,0,1,2,1,9338,yuichiroh matsubayashi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"The research trend in Japanese predicate-argument structure (PAS) analysis is shifting from pointwise prediction models with local features to global models designed to search for globally optimal solutions. However, the existing global models tend to employ only relatively simple local features; therefore, the overall performance gains are rather limited. The importance of designing a local model is demonstrated in this study by showing that the performance of a sophisticated local model can be considerably improved with recent feature embedding methods and a feature combination learning based on a neural network, outperforming the state-of-the-art global models in F1 on a common benchmark dataset."
I17-2058,Reference-based Metrics can be Replaced with Reference-less Metrics in Evaluating Grammatical Error Correction Systems,2017,10,5,3,0.952381,24169,hiroki asano,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In grammatical error correction (GEC), automatically evaluating system outputs requires gold-standard references, which must be created manually and thus tend to be both expensive and limited in coverage. To address this problem, a reference-less approach has recently emerged; however, previous reference-less metrics that only consider the criterion of grammaticality, have not worked as well as reference-based metrics. This study explores the potential of extending a prior grammaticality-based method to establish a reference-less evaluation method for GEC systems. Further, we empirically show that a reference-less metric that combines fluency and meaning preservation with grammaticality provides a better estimate of manual scores than that of commonly used reference-based metrics. To our knowledge, this is the first study that provides empirical evidence that a reference-less metric can replace reference-based metrics in evaluating GEC systems."
I17-2069,Generating Stylistically Consistent Dialog Responses with Transfer Learning,2017,12,7,5,1,20121,reina akama,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We propose a novel, data-driven, and stylistically consistent dialog response generation system. To create a user-friendly system, it is crucial to make generated responses not only appropriate but also stylistically consistent. For leaning both the properties effectively, our proposed framework has two training stages inspired by transfer learning. First, we train the model to generate appropriate responses, and then we ensure that the responses have a specific style. Experimental results demonstrate that the proposed method produces stylistically consistent responses while maintaining the appropriateness of the responses learned in a general domain."
I17-2074,Proofread Sentence Generation as Multi-Task Learning with Editing Operation Prediction,2017,15,1,4,1,9399,yuta hitomi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper explores the idea of robot editors, automated proofreaders that enable journalists to improve the quality of their articles. We propose a novel neural model of multi-task learning that both generates proofread sentences and predicts the editing operations required to rewrite the source sentences and create the proofread ones. The model is trained using logs of the revisions made professional editors revising draft newspaper articles written by journalists. Experiments demonstrate the effectiveness of our multi-task learning approach and the potential value of using revision logs for this task."
I17-1048,A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse,2017,34,3,3,1,9187,sosuke kobayashi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities."
E17-1119,Neural Architectures for Fine-grained Entity Type Classification,2017,12,21,3,1,33057,sonse shimaoka,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"In this work, we investigate several neural network architectures for fine-grained entity type classification and make three key contributions. Despite being a natural comparison and addition, previous work on attentive neural architectures have not considered hand-crafted features and we combine these with learnt features and establish that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism learns to attend over syntactic heads and the phrase containing the mention, both of which are known to be strong hand-crafted features for our task. We introduce parameter sharing between labels through a hierarchical encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We demonstrate that the choice of training data has a drastic impact on performance, which decreases by as much as 9.85{\%} loose micro F1 score for a previously proposed method. Despite this discrepancy, our best model achieves state-of-the-art results with 75.36{\%} loose micro F1 score on the well-established Figer (GOLD) dataset and we report the best results for models trained using publicly available data for the OntoNotes dataset with 64.93{\%} loose micro F1 score."
Y16-3027,Neural Joint Learning for Classifying {W}ikipedia Articles into Fine-grained Named Entity Types,2016,20,3,5,0,33283,masatoshi suzuki,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
Y16-2022,Recognizing Open-Vocabulary Relations between Objects in Images,2016,23,0,7,1,17721,masayasu muraoka,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
Y16-2026,Toward the automatic extraction of knowledge of usable goods,2016,-1,-1,4,0,33301,mei uemura,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
W16-1313,An Attentive Neural Architecture for Fine-grained Entity Type Classification,2016,12,7,3,1,33057,sonse shimaoka,Proceedings of the 5th Workshop on Automated Knowledge Base Construction,0,"In this work we propose a novel attention-based neural network model for the task of fine-grained entity type classification that unlike previously proposed models recursively composes representations of entity mention contexts. Our model achieves state-of-the-art performance with 74.94% loose micro F1-score on the well-established FIGER dataset, a relative improvement of 2.59%. We also investigate the behavior of the attention mechanism of our model and observe that it can learn contextual linguistic expressions that indicate the fine-grained category memberships of an entity."
S16-1065,Tohoku at {S}em{E}val-2016 Task 6: Feature-based Model versus Convolutional Neural Network for Stance Detection,2016,3,8,5,0,34254,yuki igarashi,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
P16-3021,Building a Corpus for {J}apanese Wikification with Fine-Grained Entity Classes,2016,13,3,4,0,34395,davaajav jargalsaikhan,Proceedings of the {ACL} 2016 Student Research Workshop,0,"In this research, we build a Wikification corpus for advancing Japanese Entity Linking. This corpus consists of 340 Japanese newspaper articles with 25,675 entity mentions. All entity mentions are labeled by a fine-grained semantic classes (200 classes), and 19,121 mentions were successfully linked to Japanese Wikipedia articles. Even with the fine-grained semantic classes, we found it hard to define the target of entity linking annotations and to utilize the fine-grained semantic classes to improve the accuracy of entity linking."
P16-1121,Learning Semantically and Additively Compositional Distributional Representations,2016,52,9,3,1,20138,ran tian,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art."
P16-1215,Composing Distributed Representations of Relational Patterns,2016,28,6,3,1,4614,sho takase,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1099,Dynamic Entity Representation with Max-pooling Improves Machine Reading,2016,19,27,4,1,9187,sosuke kobayashi,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset (Hermann et al., 2015), our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network"
L16-1734,Question-Answering with Logic Specific to Video Games,2016,0,0,3,0,35435,corentin dumont,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a corpus and a knowledge database aiming at developing Question-Answering in a new context, the open world of a video game. We chose a popular game called {`}Minecraft{'}, and created a QA corpus with a knowledge database related to this game and the ontology of a meaning representation that will be used to structure this database. We are interested in the logic rules specific to the game, which may not exist in the real world. The ultimate goal of this research is to build a QA system that can answer natural language questions from players by using inference on these game-specific logic rules. The QA corpus is partially composed of online quiz questions and partially composed of manually written variations of the most relevant ones. The knowledge database is extracted from several wiki-like websites about Minecraft. It is composed of unstructured data, such as text, that will be structured using the meaning representation we defined, and already structured data such as infoboxes. A preliminary examination of the data shows that players are asking creative questions about the game, and that the QA corpus can be used for clustering verbs and linking them to predefined actions in the game."
C16-1184,Modeling Discourse Segments in Lyrics Using Repeated Patterns,2016,13,1,5,1,16335,kento watanabe,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This study proposes a computational model of the discourse segments in lyrics to understand and to model the structure of lyrics. To test our hypothesis that discourse segmentations in lyrics strongly correlate with repeated patterns, we conduct the first large-scale corpus study on discourse segments in lyrics. Next, we propose the task to automatically identify segment boundaries in lyrics and train a logistic regression model for the task with the repeated pattern and textual features. The results of our empirical experiments illustrate the significance of capturing repeated patterns in predicting the boundaries of discourse segments in lyrics."
C16-1266,Modeling Context-sensitive Selectional Preference with Distributed Representations,2016,25,3,5,1,7295,naoya inoue,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper proposes a novel problem setting of selectional preference (SP) between a predicate and its arguments, called as context-sensitive SP (CSP). CSP models the narrative consistency between the predicate and preceding contexts of its arguments, in addition to the conventional SP based on semantic types. Furthermore, we present a novel CSP model that extends the neural SP model (Van de Cruys, 2014) to incorporate contextual information into the distributed representations of arguments. Experimental results demonstrate that the proposed CSP model successfully learns CSP and outperforms the conventional SP model in coreference cluster ranking."
Y15-1012,Fast and Large-scale Unsupervised Relation Extraction,2015,29,3,3,1,4614,sho takase,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"A common approach to unsupervised relation extraction builds clusters of patterns expressing the same relation. In order to obtain clusters of relational patterns of good quality, we have two major challenges: the semantic representation of relational patterns and the scalability to large data. In this paper, we explore various methods for modeling the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to unsupervised relation extraction. The experimental results show that approximate frequency counting and dimension reduction not only speeds up similarity computation but also improves the quality of pattern vectors."
Y15-1013,Reducing Lexical Features in Parsing by Word Embeddings,2015,26,3,4,0,34255,hiroya komatsu,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words."
Y15-1063,Recognizing Complex Negation on {T}witter,2015,14,1,7,1,16768,junta mizuno,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"After the Great East Japan Earthquake in 2011, an abundance of false rumors were disseminated on Twitter that actually hindered rescue activities. This work presents a method for recognizing the negation of predicates on Twitter to find Japanese tweets that refute false rumors. We assume that the predicate xe2x80x9coccurxe2x80x9d is negated in the sentence xe2x80x9cThe guy who tweeted that a nuclear explosion occurred has watched too many SF movies.xe2x80x9d The challenge is in the treatment of such complex negation. We have to recognize a wide range of complex negation expressions such as xe2x80x9cit is theoretically impossible that...xe2x80x9d and xe2x80x9cThe guy who... watched too many SF movies.xe2x80x9d We tackle this problem using a combination of a supervised classifier and clusters of n-grams derived from large un-annotated corpora. The n-gram clusters give us a gain of about 22% in F-score for complex negations."
W15-1606,Semantic Annotation of {J}apanese Functional Expressions and its Impact on Factuality Analysis,2015,14,1,5,0,37010,yudai kamioka,Proceedings of The 9th Linguistic Annotation Workshop,0,"Recognizing the meaning of functional expressions is essential for natural language understanding. This is a difficult task, owing to the lack of a sufficient corpus for machine learning and evaluation. In this study, we design a new annotation scheme and construct a corpus containing 2,327 Japanese sentences and 8,775 functional expressions. Our scheme achieves high inter-annotator agreement with kappa score of 0.85. In the experiments, we confirmed that machine learning-based functional expression analysis contributes to factuality analysis."
W15-1609,Annotating Geographical Entities on Microblog Text,2015,16,6,4,1,18558,koji matsuda,Proceedings of The 9th Linguistic Annotation Workshop,0,"This paper presents a discussion of the problems surrounding the task of annotating geographical entities on microblogs and reports the preliminary results of our efforts to annotate Japanese microblog texts. Unlike prior work, we not only annotate geographical location entities but also facility entities, such as stations, restaurants, shopping stores, hospitals and schools. We discuss ways in which to build a gazetteer, the types of ambiguities that need to be considered, reasons why the annotator tends to disagree, and the problems that need to be solved to automate the task of annotating the geographical entities. All the annotation data and the annotation guidelines are publicly available for research purposes from our web site."
W15-0507,A Computational Approach for Generating Toulmin Model Argumentation,2015,20,6,4,1,24185,paul reisert,Proceedings of the 2nd Workshop on Argumentation Mining,0,"Automatic generation of arguments is an important task that can be useful for many applications. For instance, the ability to generate coherent arguments during a debate can be useful when determining strengths of supporting evidence. However, with limited technologies that automatically generate arguments, the development of computational models for debates, as well as other areas, is becoming increasingly important. For this task, we focused on a promising argumentation model: the Toulmin model. The Toulmin model is both well-structured and general, and has been shown to be useful for policy debates. In this preliminary work we attempted to generate, with a given topic motion keyword or phrase, Toulmin model arguments by developing a computational model for detecting arguments spanned across multiple documents. This paper discusses our subjective results, observations, and future work."
W15-0512,Learning Sentence Ordering for Opinion Generation of Debate,2015,15,5,8,0,32288,toshihiko yanase,Proceedings of the 2nd Workshop on Argumentation Mining,0,"We propose a sentence ordering method to help compose persuasive opinions for debating. In debate texts, support of an opinion such as evidence and reason typically follows the main claim. We focused on this claimsupport structure to order sentences, and developed a two-step method. First, we select from among candidate sentences a first sentence that is likely to be a claim. Second, we order the remaining sentences by using a ranking-based method. We tested the effectiveness of the proposed method by comparing it with a general-purpose method of sentence ordering and found through experiment that it improves the accuracy of first sentence selection by about 19 percentage points and had a superior performance over all metrics. We also applied the proposed method to a constructive speech generation task."
Y14-1010,Finding The Best Model Among Representative Compositional Models,2014,29,11,6,1,17721,masayasu muraoka,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality. We implement and compare these models under the same conditions. The experimentally obtained results demonstrate that the model using different composition matrices for different dependency relations achieved state-ofthe-art performance on a dataset for two-word compositions (Mitchell and Lapata, 2010)."
Y14-1042,An Example-Based Approach to Difficult Pronoun Resolution,2014,30,1,4,0.465617,8038,canasai kruengkrai,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,A Winograd schema is a pair of twin sentences containing a referential ambiguity that is easy for a human to resolve but difficult for a computer. This paper explores the characteristics of world knowledge necessary for resolving such a schema. We observe that people tend to avoid ambiguous antecedents when using pronouns in writing. We present a method for automatically acquiring examples that are similar to Winograd schemas but have less ambiguity. We generate a concise search query that captures the essential parts of a given source sentence and then find the alignments of the source sentence and its retrieved examples. Our experimental results show that the existing sentences on the Web indeed contain instances of world knowledge useful for difficult pronoun resolution.
Y14-1049,Modeling Structural Topic Transitions for Automatic Lyrics Generation,2014,12,9,3,1,16335,kento watanabe,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"By adopting recent advances in music creation technologies, such as digital audio workstations and singing voice synthesizers, people can now create songs in their personal computers. Computers can also assist in creating lyrics or generating them automatically, although this aspect has been less thoroughly researched and is limited to rhyme and meter. This study focuses on the structural relations in Japanese lyrics. We present novel generation models that capture the topic transitions between units peculiar to the lyrics, such as verse/chorus and line. These transitions are modeled by a Hidden Markov Model (HMM) for representing topics and topic transitions. To verify that our models generate contextsuitable lyrics, we evaluate the models using a log probability of lyrics generation and fill-in-the-blanks-type test. The results show that the language model is far more effective than HMM-based models, but the HMM-based approach successfully captures the inter-verse/chorus and inter-line relations. In the result of experimental evaluation, our approach captures the inter-verse/chorus and inter-line relations."
W14-4910,A Corpus Study for Identifying Evidence on Microblogs,2014,13,2,5,1,24185,paul reisert,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising popularity, little attention has been given towards determining the properties of discourse relations for the rapid, large-scale microblog data. Therefore, given their importance for various NLP tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations. As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics."
W13-4502,Computer-assisted Structuring of Emergency Management Information: A Project Note,2013,-1,-1,2,1,5308,yotaro watanabe,Proceedings of the Workshop on Language Processing and Crisis Information 2013,0,None
W13-4505,Extracting and Aggregating False Information from Microblogs,2013,2,4,5,0.572041,4956,naoaki okazaki,Proceedings of the Workshop on Language Processing and Crisis Information 2013,0,"During the 2011 East Japan Earthquake and Tsunami Disaster, we had found a number of false information spread on Twitter, e.g., xe2x80x9cThe Cosmo Oil explosion causes toxic rain.xe2x80x9d This paper extracts pieces of false information exhaustively from all the tweets within one week after the earthquake. Designing a set of linguistic patterns that correct false information, this paper proposes a method for detecting false information. More specifically, the method extracts text passages that match to the correction patterns, clusters the passages into topics of false information, and selects, for each topic, a passage explaining the false information the most suitably. In the experiment, we report the performance of the proposed method on the data set extracted manually from Web sites that are specialized in collecting false information."
P13-3016,Detecting Chronic Critics Based on Sentiment Polarity and User{'}s Behavior in Social Media,2013,14,2,5,1,4614,sho takase,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"There are some chronic critics who always complain about the entity in social media. We are working to automatically detect these chronic critics to prevent the spread of bad rumors about the reputation of the entity. In social media, most comments are informal, and, there are sarcastic and incomplete contexts. This means that it is difficult for current NLP technology such as opinion mining to recognize the complaints. As an alternative approach for social media, we can assume that users who share the same opinions will link to each other. Thus, we propose a method that combines opinion mining with graph analysis for the connections between users to identify the chronic critics. Our experimental results show that the proposed method outperforms analysis based only on opinion mining techniques."
P13-1038,Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web,2013,22,10,5,0,41475,katsuma narisawa,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution of numbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distribution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches."
I13-2008,{NICT} Disaster Information Analysis System,2013,4,7,6,0.61279,35675,kiyonori ohtake,The Companion Volume of the Proceedings of {IJCNLP} 2013: System Demonstrations,0,"Immediately after the 2011 Great East Japan Earthquake, the Internet was flooded by a huge amount of information concerning the damage and problems caused by the earthquake, the tsunami, and the nuclear disaster. Many reports about aid efforts and advice to victims were also transmitted into cyberspace. However, since most people were overwhelmed by the massive amounts of information, they could not make proper decisions, and much confusion was caused. Furthermore, false rumors spread on the Internet and fanned such confusion. We demonstrate NICTxe2x80x99s prototype disaster information analysis system, which was designed to properly organize such a large amount of disaster-related information on social media during future large-scale disasters to help people understand the situation and make correct decisions. We are going to deploy it using a large-scale computer cluster in fiscal year 2014."
I13-1067,A Lexicon-based Investigation of Research Issues in {J}apanese Factuality Analysis,2013,10,4,3,0,37011,kazuya narita,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Event factuality is information about whether events mentioned in natural language correspond to either actual events that have occurred in the real world or events that are of uncertain interpretation. Factuality analysis is useful for information extraction and textual entailment recognition, among others, but sufficient performance has not yet been achieved by the machine learning-based approach. It is now important to take a closer look at the linguistics phenomena involved in factuality analysis and identify the technical research issues more precisely. In this paper, we discuss issues regarding lexical knowledge through error analysis of a Japanese factuality analyzer based on lexical knowledge and compositionality."
Y12-1057,Set Expansion using Sibling Relations between Semantic Categories,2012,18,0,3,1,4614,sho takase,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Most set expansion algorithms assume to acquire new instances of different semantic categories independently even when we have seed instances of multiple semantic categories. However, in the setting of set expansion with multiple semantic categories, we might leverage other types of prior knowledge about semantic categories. In this paper, we present a method of set expansion when ontological information related to target semantic categories is available. More specifically, the proposed method makes use of sibling relations between semantic categories as an additional type of prior knowledge. We demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from Wikipedia in a semi-automatic manner."
C12-1079,Coreference Resolution with {ILP}-based Weighted Abduction,2012,45,13,3,1,7295,naoya inoue,Proceedings of {COLING} 2012,0,"This paper presents the first systematic study of the coreference resolution problem in a general inference-based discourse processing framework. Employing the mode of inference called weighted abduction, we propose a novel solution to the overmerging problem inherent to inference-based frameworks. The overmerging problem consists in erroneously assuming distinct entities to be identical. In discourse processing, overmerging causes establishing wrong coreference links. In order to approach this problem, we extend Hobbs et al. (1993)xe2x80x99s weighted abduction by introducing weighted unification and show how to learn the unification weights by applying machine learning techniques. For making large-scale processing and parameter learning in an abductive logic framework feasible, we employ a new efficient implementation of weighted abduction based on Integer Linear Programming. We then propose several linguistically motivated features for blocking incorrect unifications and employ different large-scale world knowledge resources for establishing unification via inference. We provide a large-scale evaluation on the CoNLL-2011 shared task dataset, showing that all features and almost all knowledge components improve the performance of our system."
C12-1171,A Latent Discriminative Model for Compositional Entailment Relation Recognition using Natural Logic,2012,26,8,5,1,5308,yotaro watanabe,Proceedings of {COLING} 2012,0,"Recognizing semantic relations between sentences, such as entailment and contradiction, is a challenging task that requires detailed analysis of the interaction between diverse linguistic phenomena. In this paper, we propose a latent discriminative model that unifies a statistical framework and a theory of Natural Logic to capture complex interactions between linguistic phenomena. The proposed approach jointly models alignments, their local semantic relations, and a sentence-level semantic relation, and has hidden variables including alignment edits between sentences and their semantic relations, only requires sentences pairs annotated with sentence-level semantic relations as training data to learn appropriate alignments. In evaluation on a dataset including diverse linguistic phenomena, our proposed method achieved a competitive results on alignment prediction, and significant improvements on a sentence-level semantic relation recognition task compared to an alignment supervised model. Our analysis did not provide evidence that directly learning alignments and their labels using gold standard alignments contributed to semantic relation recognition performance and instead suggests that they can be detrimental to performance if used in a manner that prevents the learning of globally optimal alignments."
W11-0123,Recognizing Confinement in Web Texts,2011,9,6,7,0,44459,megumi ohki,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"In the Recognizing Textual Entailment (RTE) task, sentence pairs are classified into one of three semantic relations: Entailment, Contradiction or Unknown. While we find some sentence pairs hold full entailments or contradictions, there are a number of pairs that partially entail or contradict one another depending on a specific situation. These partial contradiction sentence pairs contain useful information for opinion mining and other such tasks, but it is difficult for Internet users to access this knowledge because current frameworks do not differentiate between full contradictions and partial contradictions. In this paper, under current approaches to semantic relation recognition, we define a new semantic relation known as Confinement in order to recognize this useful information. This information is classified as either Contradiction or Entailment. We provide a series of semantic templates to recognize Confinement relations in Web texts, and then implement a system for recognizing Confinement between sentence pairs. We show that our proposed system can obtains a F-score of 61% for recognizing Confinement in Japanese-language Web texts, and it outperforms a baseline which does not use a manually compiled list of lexico-syntactic patterns to instantiate the semantic templates."
W10-3904,Automatic Classification of Semantic Relations between Facts and Opinions,2010,19,7,8,1,15888,koji murakami,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"Classifying and identifying semantic relations between facts and opinions on the Web is of utmost importance for organizing information on the Web, however, this requires consideration of a broader set of semantic relations than are typically handled in Recognizing Textual Entailment (RTE), Cross-document Structure Theory (CST), and similar tasks. In this paper, we describe the construction and evaluation of a system that identifies and classifies semantic relations in Internet data. Our system targets a set of semantic relations that have been inspired by CST but that have been generalized and broadened to facilitate application to mixed fact and opinion data from the Internet. Our system identifies these semantic relations in Japanese Web texts using a combination of lexical, syntactic, and semantic information and evaluate our system against gold standard data that was manually constructed for this task. We will release all gold standard data used in training and evaluation of our system this summer."
W10-3201,A Thesaurus of Predicate-Argument Structure for {J}apanese Verbs to Deal with Granularity of Verb Meanings,2010,9,7,2,0,17433,koichi takeuchi,Proceedings of the Eighth Workshop on {A}sian Language Resouces,0,"In this paper we propose a framework of verb semantic description in order to organize different granularity of similarity between verbs. Since verb meanings highly depend on their arguments we propose a verb thesaurus on the basis of possible shared meanings with predicate-argument structure. Motivations of this work are to (1) construct a practical lexicon for dealing with alternations, paraphrases and entailment relations between predicates, and (2) provide a basic database for statistical learning system as well as a theoretical lexicon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several granularities of semantic classes to characterize verb meanings. The thesaurus form allows us to provide several granularities of shared meanings; thus, this gives us a further revision for applying more detailed analyses of verb meanings."
N10-1120,Dependency Tree-based Sentiment Classification using {CRF}s with Hidden Variables,2010,18,243,2,0,27752,tetsuji nakagawa,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features."
matsuyoshi-etal-2010-annotating,"Annotating Event Mentions in Text with Modality, Focus, and Source Information",2010,10,23,5,1,29950,suguru matsuyoshi,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Many natural language processing tasks, including information extraction, question answering and recognizing textual entailment, require analysis of the polarity, focus of polarity, tense, aspect, mood and source of the event mentions in a text in addition to its predicate-argument structure analysis. We refer to modality, polarity and other associated information as extended modality. In this paper, we propose a new annotation scheme for representing the extended modality of event mentions in a sentence. Our extended modality consists of the following seven components: Source, Time, Conditional, Primary modality type, Actuality, Evaluation and Focus. We reviewed the literature about extended modality in Linguistics and Natural Language Processing (NLP) and defined appropriate labels of each component. In the proposed annotation scheme, information of extended modality of an event mention is summarized at the core predicate of the event mention for immediate use in NLP applications. We also report on the current progress of our manual annotation of a Japanese corpus of about 50,000 event mentions, showing a reasonably high ratio of inter-annotator agreement."
C10-2061,Identifying Contradictory and Contrastive Relations between Statements to Outline Web Information on a Given Topic,2010,23,13,2,0.116468,3202,daisuke kawahara,Coling 2010: Posters,0,"We present a method for producing a bird's-eye view of statements that are expressed on Web pages on a given topic. This method aggregates statements that are relevant to the topic, and shows contradictory and contrastive relations among them. This view of contradictions and contrasts helps users acquire a top-down understanding of the topic. To realize this, we extract such statements and relations, including cross-document implicit contrastive relations between statements, in an unsupervised manner. Our experimental results indicate the effectiveness of our approach."
Y09-2021,Interpolated {PLSI} for Learning Plausible Verb Arguments,2009,13,3,2,0,29747,hiram calvo,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Learning Plausible Verb Arguments allows to automatically learn what kind of activities, where and how, are performed by classes of entities from sparse argument co%occurrences with a verb; this informati on it is useful for sentence reconstruction tasks. Calvo et al. (2009b) propose a non language%dependent model based on the Word Space Model for calculating the plausibility of candidate arguments given one verb and one argument, and compare with the single latent variable PLSI algorithm method, outperforming it. In this work we replicate their experiments with a different corpus, and explore variants to the PLSI method in order to explore further capabilities of this latter widely used technique. Particularly, we propose using an interpolated PLSI scheme that allows the combination of multiple latent semantic variables, and validate it in a task of identifying the real dependency%pair triple with re gard to an artificially created one, obtaining up to 83% recall."
W09-3027,Annotating Semantic Relations Combining Facts and Opinions,2009,9,10,5,1,15888,koji murakami,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"As part of the Statement Map project, we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efficiently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called statements and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter."
P09-4001,{WISDOM}: A Web Information Credibility Analysis Systematic,2009,6,23,5,0,47164,susumu akamine,Proceedings of the {ACL}-{IJCNLP} 2009 Software Demonstrations,0,"We demonstrate an information credibility analysis system called WISDOM. The purpose of WISDOM is to evaluate the credibility of information available on the Web from multiple viewpoints. WISDOM considers the following to be the source of information credibility: information contents, information senders, and information appearances. We aim at analyzing and organizing these measures on the basis of semantics-oriented natural language processing (NLP) techniques."
P09-1073,Capturing Salience with a Trainable Cache Model for Zero-anaphora Resolution,2009,28,11,2,1,12930,ryu iida,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,This paper explores how to apply the notion of caching introduced by Walker (1996) to the task of zero-anaphora resolution. We propose a machine learning-based implementation of a cache model to reduce the computational cost of identifying an antecedent. Our empirical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it.
I08-1065,Acquiring Event Relation Knowledge by Learning Cooccurrence Patterns and Fertilizing Cooccurrence Samples with Verbal Nouns,2008,13,8,2,0,48682,shuya abe,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Aiming at acquiring semantic relations between events from a large corpus, this paper proposes several extensions to a state-of-theart method originally designed for entity relation extraction, reporting on the present results of our experiments on a Japanese Web corpus. The results show that (a) there are indeed specific cooccurrence patterns useful for event relation acquisition, (b) the use of cooccurrence samples involving verbal nouns has positive impacts on both recall and precision, and (c) over five thousand relation instances are acquired from a 500M-sentence Web corpus with a precision of about 66% for action-effect relations."
C08-1001,Two-Phased Event Relation Acquisition: Coupling the Relation-Oriented and Argument-Oriented Approaches,2008,13,19,2,0,48682,shuya abe,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Addressing the task of acquiring semantic relations between events from a large corpus, we first argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argument-oriented approach. We then propose a two-phased approach, which first uses lexico-syntactic patterns to acquire predicate pairs and then uses two types of anchors to identify shared arguments. The present results of our empirical evaluation on a large-scale Japanese Web corpus have shown that (a) the anchor-based filtering extensively improves the accuracy of predicate pair acquisition, (b) the two types of anchors are almost equally contributive and combining them improves recall without losing accuracy, and (c) the anchor-based method also achieves high accuracy in shared argument identification."
C08-1111,Emotion Classification Using Massive Examples Extracted from the Web,2008,21,111,2,0,48744,ryoko tokuhisa,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper, we propose a data-oriented method for inferring the emotion of a speaker conversing with a dialog system from the semantic content of an utterance. We first fully automatically obtain a huge collection of emotion-provoking event instances from the Web. With Japanese chosen as a target language, about 1.3 million emotion provoking event instances are extracted using an emotion lexicon and lexical patterns. We then decompose the emotion classification task into two sub-steps: sentiment polarity classification (coarsegrained emotion classification), and emotion classification (fine-grained emotion classification). For each subtask, the collection of emotion-proviking event instances is used as labelled examples to train a classifier. The results of our experiments indicate that our method significantly outperforms the baseline method. We also find that compared with the single-step model, which applies the emotion classifier directly to inputs, our two-step model significantly reduces sentiment polarity errors, which are considered fatal errors in real dialog applications."
W07-1522,Annotating a {J}apanese Text Corpus with Predicate-Argument and Coreference Relations,2007,13,66,3,1,12930,ryu iida,Proceedings of the Linguistic Annotation Workshop,0,"In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA-Tagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1, which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006)."
D07-1114,Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining,2007,21,133,2,0,40256,nozomi kobayashi,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"The technology of opinion extraction allows users to retrieve and analyze peoplexe2x80x99s opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues. Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks."
P06-1079,Exploiting Syntactic Patterns as Clues in Zero-Anaphora Resolution,2006,26,40,2,1,12930,ryu iida,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zero-anaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues. Taking Japanese as a target language, we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora, which consequently improves the overall performance of zero-anaphora resolution."
inui-etal-2006-augmenting,Augmenting a Semantic Verb Lexicon with a Large Scale Collection of Example Sentences,2006,0,0,1,1,4154,kentaro inui,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"One of the crucial issues in semantic parsing is how to reduce costs of collecting a sufficiently large amount of labeled data. This paper presents a new approach to cost-saving annotation of example sentences with predicate-argument structure information, taking Japanese as a target language. In this scheme, a large collection of unlabeled examples are first clustered and selectively sampled, and for each sampled cluster, only one representative example is given a label by a human annotator. The advantages of this approach are empirically supported by the results of our preliminary experiments, where we use an existing similarity function and naive sampling strategy."
I05-5004,A Class-oriented Approach to Building a Paraphrase Corpus,2005,9,9,2,1,5049,atsushi fujita,Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005),0,"Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement. Our preliminary experiments on building a paraphrase corpus have so far been producing promising results, which we have evaluated according to cost-efficiency, exhaustiveness, and reliability."
I05-2030,Opinion Extraction Using a Learning-Based Anaphora Resolution Technique,2005,9,35,3,0,40256,nozomi kobayashi,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"This paper addresses the task of extracting opinions from a given document collection. Assuming that an opinion can be represented as a tuple xe3x80x88Subject, Attribute, Valuexe3x80x89, we propose a computational method to extract such tuples from texts. In this method, the main task is decomposed into (a) the process of extracting Attribute-Value pairs from a given text and (b) the process of judging whether an extracted pair expresses an opinion of the author. We apply machine-learning techniques to both subtasks. We also report on the results of our experiments and discuss future directions."
I05-1079,Exploiting Lexical Conceptual Structure for Paraphrase Generation,2005,16,6,2,1,5049,atsushi fujita,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Lexical Conceptual Structure (LCS) represents verbs as semantic structures with a limited number of semantic predicates. This paper attempts to exploit how LCS can be used to explain the regularities underlying lexical and syntactic paraphrases, such as verb alternation, compound word decomposition, and lexical derivation. We propose a paraphrase generation model which transforms LCSs of verbs, and then conduct an empirical experiment taking the paraphrasing of Japanese light-verb constructions as an example. Experimental results justify that syntactic and semantic properties of verbs encoded in LCS are useful to semantically constrain the syntactic transformation in paraphrase generation."
W04-0402,Paraphrasing of {J}apanese Light-verb Constructions Based on Lexical Conceptual Structure,2004,14,13,3,1,5049,atsushi fujita,Proceedings of the Workshop on Multiword Expressions: Integrating Processing,0,"Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge. In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases."
W03-2604,Incorporating Contextual Cues in Trainable Models for Coreference Resolution,2003,-1,-1,2,1,12930,ryu iida,Proceedings of the 2003 {EACL} Workshop on The Computational Treatment of Anaphora,0,None
W03-1602,Text Simplification for Reading Assistance: A Project Note,2003,19,79,1,1,4154,kentaro inui,Proceedings of the Second International Workshop on Paraphrasing,0,"This paper describes our ongoing research project on text simplification for congenitally deaf people. Text simplification we are aiming at is the task of offering a deaf reader a syntactic and lexical paraphrase of a given text for assisting her/him to understand what it means. In this paper, we discuss the issues we should address to realize text simplification and report on the present results in three different aspects of this task: readability assessment, paraphrase representation and post-transfer error detection."
W01-0814,A Paraphrase-Based Exploration of Cohesiveness Criteria,2001,22,7,1,1,4154,kentaro inui,Proceedings of the {ACL} 2001 Eighth {E}uropean Workshop on Natural Language Generation ({EWNLG}),0,"This paper proposes an empirical approach to the development of a computational model for assessing texts according to cohesiveness. We argue that the NLG technologies for the generation of structural paraphrases can be used to efficiently create what we call a cohesion-variant parallel corpus, which would serve as a good resource for empirical acquisition of cohesiveness criteria. We also present our pilot case study, in which we took a particular type of paraphrasing that separates a relative clause from a sentence. We have so far created a cohesion-variant parallel corpus containing 499 cohesive instances and 841 incohesive instances. Based on this corpus, we conducted a preliminary experiment on cohesion evaluation, obtaining encouraging results."
C00-1051,Committee-based Decision Making in Probabilistic Partial Parsing,2000,14,4,2,0,14555,takashi inui,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This paper explores two directions for the next step beyond the state of the art of statistical parsing: probabilistic partial parsing and committee-based decision making. Probabilistic partial parsing is a probabilistic extension of the existing notion of partial parsing, which enables fine-grained arbitrary choice on the trade-off between accuracy and coverage. Committee-based decision making is to combine the out-puts from different systems to make a better decision. While various committee-based techniques for NLP have recently been investigated, they would need to be further extended so as to be applicable to probabilistic partial parsing. Aiming at this coupling, this paper gives a general framework to committee-based decision making, which consists of a set of weighting functions and a combination function, and discusses how it can be coupled with probabilistic partial parsing. Our experiments have so far been producing promising results."
W98-1510,An Empirical Evaluation on Statistical Parsing of {J}apanese Sentences Using Lexical Association Statistics,1998,11,7,2,0,29633,kiyoaki shirai,Proceedings of the Third Conference on Empirical Methods for Natural Language Processing,0,"We are proposing a new framework of statistical language modeling which integrates lexical association statistics with syntactic preference, while maintaining the modularity of those different statistics types, facilitating both training of the model and analysis of its behavior. In this paper, we report the result of an empirical evaluation of our model, where the model is applied to disambiguation of dependency structures of Japanese sentences. We also discussed the room remained for further improvement based on our error analysis."
J98-4002,Selective Sampling for Example-based Word Sense Disambiguation,1998,49,95,2,1,37781,atsushi fujii,Computational Linguistics,0,"This paper proposes an efficient example sampling method for example-based word sense disambiguation systems. To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required. In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search). To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation. Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectiveness of our method through experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system."
W96-0105,Selective Sampling of Effective Example Sentence Sets for Word Sense Disambiguation,1996,18,4,2,0,37781,atsushi fujii,Fourth Workshop on Very Large Corpora,0,None
C96-2175,The {I}nternet a {``}natural{''} channel for language learning,1996,0,0,1,1,4154,kentaro inui,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,None
C96-2176,The {I}nternet a {``}natural{''} channel for language learning,1996,0,0,1,1,4154,kentaro inui,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,None
C96-1012,To what extent does case contribute to verb sense disambiguation?,1996,13,7,2,0,37781,atsushi fujii,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Word sense disambugation has recently been utillized in corpus-based approaches, reflecting the growth in the number of machine readable texts. One category of approaches disambiguates an input verb sense based on the similarity between its governing case fillers and those in given examples. In this paper, we introduce the degree of contribution of case to verb sense disambiguation into this existing method. In this, greater diversity of semantic range of case filler examples will lead to that case contributing to verb sense disambiguation more. We also report the result of a comparative experiment, in which the performance of disambiguation is improved by considering this notion of semantic contribution."
