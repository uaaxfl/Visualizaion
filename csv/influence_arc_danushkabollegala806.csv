2020.aacl-main.87,W06-1615,0,0.396767,"wo cross-domain sentiment classification datasets show that the proposed method reports consistently good performance across domains, and at times outperforming more complex prior proposals. Moreover, the computed domain-attention scores enable us to find explanations for the predictions made by the proposed method. 1 1 Introduction Domain adaptation (DA) considers the problem of generalising a model learnt using the data from a particular source domain to a different target domain (Zhang et al., 2015). Although most DA methods consider adapting to a target domain from a single source domain (Blitzer et al., 2006, 2007; Ganin et al., 2016), often it is difficult to find a suitable single source to adapt from, and one must consider multiple sources. For example, in sentiment classification, each product category is considered as a domain (Blitzer et al., 2006), resulting in a multi-domain adaptation setting. Unsupervised DA (UDA) is a special case of DA where labelled instances are not available for ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associate"
2020.aacl-main.87,P11-1014,1,0.837883,"nto methods that: (a) first select a source domain and then select instances from that source domain to adapt to a given target domain test instance (Ganin et al., 2016; Kim et al., 2017; Zhao et al., 2018; Guo et al., 2018); (b) pool all source domain instances together and from this pool select instances to adapt to a given target domain test instance (Chattopadhyay et al., 2012); (c) pick a source domain and use all instances in that source (source domain selection) (Schultz et al., 2018); and (d) pick all source domains and use all instances (utilising all instances) (Aue and Gamon, 2005; Bollegala et al., 2011; Wu and Huang, 2016). In contrast, we propose a multi-source UDA 873 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 873–883 c December 4 - 7, 2020. 2020 Association for Computational Linguistics method that systematically addresses the various challenges in multi-source UDA. • Although in UDA we have labelled instances in each source domain, its number is significantly smaller than that of the unlabelled instances in the same domain. For example, in"
2020.aacl-main.87,P07-1056,0,0.315073,"a multi-source UDA 873 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 873–883 c December 4 - 7, 2020. 2020 Association for Computational Linguistics method that systematically addresses the various challenges in multi-source UDA. • Although in UDA we have labelled instances in each source domain, its number is significantly smaller than that of the unlabelled instances in the same domain. For example, in the Amazon product review dataset released by Blitzer et al. (2007) there are 73679 unlabelled instances in total across the four domains, whereas there are only 4800 labelled instances. To increase the labelled instances in a source domain, we induce pseudo-labels for the unlabelled instances in each source domain using self-training as in § 3.1. • In UDA, we have no labelled data for the target domain. To address this challenge, we infer pseudo-labels for the target domain’s unlabelled training instances by majority voting over the classifiers trained from each source domain, using both labelled and pseudolabelled instances as in § 3.1. • Given that the pse"
2020.aacl-main.87,D18-1498,0,0.0370709,"Missing"
2020.aacl-main.87,P17-1060,0,0.0657555,"he loss between the original inputs and their reconstructions. Not all of the source domains are appropriate for learning transferable projections for a particular target domain. Adapting from an unrelated source can result in poor performance on the given target, which is known as negative transfer (Rosenstein et al., 2005; Pan and Yang, 2010; Guo et al., 2018). Prior proposals for multi-source UDA can be broadly classified into methods that: (a) first select a source domain and then select instances from that source domain to adapt to a given target domain test instance (Ganin et al., 2016; Kim et al., 2017; Zhao et al., 2018; Guo et al., 2018); (b) pool all source domain instances together and from this pool select instances to adapt to a given target domain test instance (Chattopadhyay et al., 2012); (c) pick a source domain and use all instances in that source (source domain selection) (Schultz et al., 2018); and (d) pick all source domains and use all instances (utilising all instances) (Aue and Gamon, 2005; Bollegala et al., 2011; Wu and Huang, 2016). In contrast, we propose a multi-source UDA 873 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computati"
2020.aacl-main.87,K16-1006,0,0.0223882,"ns, we use the pseudolabelled target domain instances T L∗ and source domains’ labelled instances SiL to learn a relatedness map, ψi , between a target domain instance xT (∈ T L∗ ) and a source domain labelled instance xLi (∈ SiL ) as given by (3). exp(xT &gt; xLi ) &gt; 0 x0 ∈S L exp(xT x ) ψi (xT , xLi ) = P (3) i Using ψi , we can determine how well each instance in a source domain contributes to the prediction of the label of a target domain’s instance. (1) 3.4 x∈T In the case of text documents x, their embeddings, x, can be computed using numerous approaches such as using bi-directional LSTMs (Melamud et al., 2016) or transformers (Reimers and Gurevych, 2019). In our experiments, we use the Smoothed Inversed Frequency (SIF; Arora et al., 2017), which computes document embeddings as the weighted-average of the pre-trained word embeddings for the words in a document. Despite being unsupervised, SIF has shown strong performance in numerous semantic textual similarity benchmarks (Agirre et al., 2015). Using the centroid computed in (1), similarity for target instance to the centroid is computed using (2) Other distance measures such as the Euclidean distance can also be used. We use cosine similarity here f"
2020.aacl-main.87,D18-1063,0,0.0206126,"., 2015), and automatically capture pivots using an attention mechanism. Guo et al. (2018) proposed an UDA method using a mixture of experts for each domain. They model the domain relations 874 using a point-to-set distance metric to the encoded training matrix for source domains. Next, they perform joint training over all domain-pairs to update the parameters in the model by meta-training. However, they ignore the available unlabelled instances for the source domain. Adversarial training methods have shown to be sensitive to the hyper parameter values and require problem-specific techniques (Mukherjee et al., 2018). Kim et al. (2017) modeled domain relations using exampleto-domain based on an attention mechanism. However, the attention weights are learnt using source domain training data in a supervised manner. Following a self-training approach, Chattopadhyay et al. (2012) proposed a two-stage weighting framework for multi-source DA that first computes the weights for features from different source domains using Maximum Mean Discrepancy (MMD; Borgwardt et al., 2006). Next, they generate pseudo labels for the target unlabelled instances using a classifier learnt from the multiple source domains. Finally"
2020.aacl-main.87,D14-1162,0,0.0865508,"Missing"
2020.aacl-main.87,D19-1410,0,0.0150652,"main instances T L∗ and source domains’ labelled instances SiL to learn a relatedness map, ψi , between a target domain instance xT (∈ T L∗ ) and a source domain labelled instance xLi (∈ SiL ) as given by (3). exp(xT &gt; xLi ) &gt; 0 x0 ∈S L exp(xT x ) ψi (xT , xLi ) = P (3) i Using ψi , we can determine how well each instance in a source domain contributes to the prediction of the label of a target domain’s instance. (1) 3.4 x∈T In the case of text documents x, their embeddings, x, can be computed using numerous approaches such as using bi-directional LSTMs (Melamud et al., 2016) or transformers (Reimers and Gurevych, 2019). In our experiments, we use the Smoothed Inversed Frequency (SIF; Arora et al., 2017), which computes document embeddings as the weighted-average of the pre-trained word embeddings for the words in a document. Despite being unsupervised, SIF has shown strong performance in numerous semantic textual similarity benchmarks (Agirre et al., 2015). Using the centroid computed in (1), similarity for target instance to the centroid is computed using (2) Other distance measures such as the Euclidean distance can also be used. We use cosine similarity here for its simplicity. We predict the labels for"
2020.aacl-main.87,P18-1096,0,0.0166932,"lly, we predict a pseudo-label for a target domain instance as the majority vote, f ∗ ∈ {0, 1}, over the predictions made by the individual classifiers fi . 2 Although we consider binary sentiment classification as an evaluation task in this paper, the proposed method can be easily extended to multi-class classification settings by making 1-vs-rest prediction tasks (Rifkin and Klautau, 2004). 875 3.2 Prototype Selection Selecting the highest confident pseudo-labelled instances for training a classifier for the target domain as done in prior work (Zhou and Li, 2005; Abney, 2007; Søgaard, 2010; Ruder and Plank, 2018) does not guarantee that those instances will be the most Algorithm 1 Multi-Source Self-Training Input: source domains’ labelled instances L S1L , . . . , SN , source domains’ unlabelled instances U U S1 , . . . , SN and target domain’s unlabelled instances T U , target classes Y, base learner Γ and the classification confidence threshold τ . Output: multi-source self-training classifier f ∗ 1: for i = 1 to N do 2: Li ← SiL 3: fi ← Γ(Li ) 4: for x ∈ SiU do 5: yˆ = arg maxy∈Y fi (x, y) 6: if fi (x, yˆ) &gt; τ then 7: Li ← Li ∪ {(x, yˆ)} 8: end if 9: end for 10: fi ← Γ(Li ) 11: end for 12: return m"
2020.aacl-main.87,P10-2038,0,0.141459,"instances. Finally, we predict a pseudo-label for a target domain instance as the majority vote, f ∗ ∈ {0, 1}, over the predictions made by the individual classifiers fi . 2 Although we consider binary sentiment classification as an evaluation task in this paper, the proposed method can be easily extended to multi-class classification settings by making 1-vs-rest prediction tasks (Rifkin and Klautau, 2004). 875 3.2 Prototype Selection Selecting the highest confident pseudo-labelled instances for training a classifier for the target domain as done in prior work (Zhou and Li, 2005; Abney, 2007; Søgaard, 2010; Ruder and Plank, 2018) does not guarantee that those instances will be the most Algorithm 1 Multi-Source Self-Training Input: source domains’ labelled instances L S1L , . . . , SN , source domains’ unlabelled instances U U S1 , . . . , SN and target domain’s unlabelled instances T U , target classes Y, base learner Γ and the classification confidence threshold τ . Output: multi-source self-training classifier f ∗ 1: for i = 1 to N do 2: Li ← SiL 3: fi ← Γ(Li ) 4: for x ∈ SiU do 5: yˆ = arg maxy∈Y fi (x, y) 6: if fi (x, yˆ) &gt; τ then 7: Li ← Li ∪ {(x, yˆ)} 8: end if 9: end for 10: fi ← Γ(Li )"
2020.aacl-main.87,P16-1029,0,0.321404,"rst select a source domain and then select instances from that source domain to adapt to a given target domain test instance (Ganin et al., 2016; Kim et al., 2017; Zhao et al., 2018; Guo et al., 2018); (b) pool all source domain instances together and from this pool select instances to adapt to a given target domain test instance (Chattopadhyay et al., 2012); (c) pick a source domain and use all instances in that source (source domain selection) (Schultz et al., 2018); and (d) pick all source domains and use all instances (utilising all instances) (Aue and Gamon, 2005; Bollegala et al., 2011; Wu and Huang, 2016). In contrast, we propose a multi-source UDA 873 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 873–883 c December 4 - 7, 2020. 2020 Association for Computational Linguistics method that systematically addresses the various challenges in multi-source UDA. • Although in UDA we have labelled instances in each source domain, its number is significantly smaller than that of the unlabelled instances in the same domain. For example, in the Amazon product r"
2020.aacl-main.87,S15-2045,0,\N,Missing
2020.acl-main.73,D18-1403,0,0.0201291,"ine the topic. For each topic k, if the cumulative proportion of topics over descenP dants, j∈Des(k) pj , is less than a threshold, the k-th topic and its descendants are removed (Des(k) denotes the set of topic k and its descendants). We also remove topics with no children at the bottom. 802 4 Experiments NPMI 4.1 Datasets In our experiments, we use the 20NewsGroups and the Amazon product reviews. The 20NewsGroups is a collection of 20 different news groups containing 11, 258 training and 7, 487 testing documents2 . For the Amazon product reviews, we use the domain of Laptop Bags provided by Angelidis and Lapata (2018), with 31, 943 training, 385 validation and 416 testing documents3 . We use the provided test documents in our evaluations, while randomly splitting the remainder of the documents into training and validation sets. 4.2 Baseline Methods As baselines, we use a tree-structured topic model based on the nested Chinese restaurant process (nCRP) with collapsed Gibbs sampling (Blei et al., 2010). In addition, we use a flat neural topic model, i.e. the recurrent stick-breaking process (RSB), which constructs the unbounded flat topic distribution via an RNN (Miao et al., 2017). 4.3 Implementation Detail"
2020.acl-main.73,P10-1084,0,0.0464517,"s, not organizing them into coherent groups or hierarchies. Treestructured topic models (Griffiths et al., 2004), which detect the latent tree structure of topics, can overcome this limitation. These models induce a tree with an infinite number of nodes and assign a generic topic to the root and more detailed topics to the leaf nodes. In Figure 1, we show an example of topics induced by our model. Such characteristics are preferable for several downstream tasks, such as document retrieval (Weninger et al., 2012), aspect-based sentiment analysis (Kim et al., 2013) and extractive summarization (Celikyilmaz and Hakkani-Tur, 2010), because they provide succinct information from multiple viewpoints. For instance, in the case of document retrieval of product reviews, some users are interested in the general opinions about bag covers, while others pay more attention to specific topics such as the hardness or color of the covers. The tree structure can navigate users to the documents with desirable granularity. However, it is difficult to use tree-structured topic models with neural models for downstream tasks. While neural models require a large amount of data for training, conventional inference algorithms, such as colla"
2020.acl-main.73,D18-1096,0,0.0343488,"Missing"
2020.acl-main.73,P17-1036,0,0.0197921,"plain the word distribution assigned to each topic1 . We introduce the embeddings of the k-th topic, tk ∈ RH , and words, U ∈ RV ×H , to obtain the word distribution, βk ∈ ∆V −1 , by (13). (13) τl 1 where τ l is a temperature value and produces more sparse probability distribution over words as the level l gets to be deeper (Hinton et al., 2014). As the number of topics is unbounded, the word distributions must be generated dynamically. Hence, we introduce another DRNN to generate topic embeddings as tk = DRNN(tpar(k) , tk−1 ). Several neural topic models (Xie et al., 2015; Miao et al., 2017; He et al., 2017) have introduced diversity regularizer to eliminate redundancy in the topics. While they force all topics to be orthogonal, this is not suitable for tree-structured topic models, which admit the correlation between a parent and its children. Hence, we introduce a tree-specific diversity regularizer with t¯ki = ti − tk as:  > 2 X X t¯ki · t¯kj −1 (14) kt¯ki kkt¯kj k k∈Leaf / i,j∈Chi(k):i6=j where Leaf and Chi(k) denote the set of the topics with no children and the children of the k-th topic, respectively. By adding this regularizer to the variational objective, each child topic becomes ortho"
2020.acl-main.73,E14-1056,0,0.11007,"m 4 The code to reproduce the results is available at: https: //github.com/misonuma/tsntm. RSB (Miao et al., 2017) nCRP (Blei et al., 2010) TSNTM (Our Model) 20News Amazon 0.201 0.198 0.220 0.102 0.112 0.121 Table 1: Average NPMI of the induced topics. Perplexity RSB (Miao et al., 2017) nCRP (Blei et al., 2010) TSNTM (Our Model) 20News Amazon 931 681 886 472 303 460 Table 2: Average perplexity of each model. 4.4 Evaluating Topic Interpretability Several works (Chang et al., 2009; Newman et al., 2010) pointed out that perplexity is not suitable for evaluating topic interpretability. Meanwhile, Lau et al. (2014) showed that the normalized pointwise mutual information (NPMI) between all pairs of words in each topic closely corresponds to the ranking of topic interpretability by human annotators. Thus, we use NPMI instead of perplexity as the primary evaluation measure following Srivastava and Sutton (2017); Ding et al. (2018). Table 1 shows the average NPMI of the topics induced by each model. Our model is competitive with the nCRP-based model and the RSB for each dataset. This indicates that our model can induce interpretable topics similar to the other models. As a note, we also show the average per"
2020.acl-main.73,N10-1012,0,0.254812,"ary provided at https://github.com/akashgit/ autoencoding_vi_for_topic_models. 3 https://github.com/stangelid/oposum 4 The code to reproduce the results is available at: https: //github.com/misonuma/tsntm. RSB (Miao et al., 2017) nCRP (Blei et al., 2010) TSNTM (Our Model) 20News Amazon 0.201 0.198 0.220 0.102 0.112 0.121 Table 1: Average NPMI of the induced topics. Perplexity RSB (Miao et al., 2017) nCRP (Blei et al., 2010) TSNTM (Our Model) 20News Amazon 931 681 886 472 303 460 Table 2: Average perplexity of each model. 4.4 Evaluating Topic Interpretability Several works (Chang et al., 2009; Newman et al., 2010) pointed out that perplexity is not suitable for evaluating topic interpretability. Meanwhile, Lau et al. (2014) showed that the normalized pointwise mutual information (NPMI) between all pairs of words in each topic closely corresponds to the ranking of topic interpretability by human annotators. Thus, we use NPMI instead of perplexity as the primary evaluation measure following Srivastava and Sutton (2017); Ding et al. (2018). Table 1 shows the average NPMI of the topics induced by each model. Our model is competitive with the nCRP-based model and the RSB for each dataset. This indicates tha"
2020.acl-main.73,N19-1015,0,0.0266663,"respectively the ancestors and siblings. 800 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 800–806 c July 5 - 10, 2020. 2020 Association for Computational Linguistics sampling a level Experimental results show that the TSNTM achieves competitive performance against a prior work (Blei et al., 2010) when inducing latent topics and tree structures. The TSNTM scales to larger datasets and allows for end-to-end training with neural models of several tasks such as aspect-based sentiment analysis (Esmaeili et al., 2019) and abstractive summarization (Wang et al., 2019). 2 β11 β111 cd,1 wd,2 β12 wd,1 β112 zd,2 wd,4 wd,3 zd,1 β121 cd,2 cd,3 zd,4 zd,3 cd,4 sampling a path Figure 2: Sampling process of a topic for each word. Related Works Following the pioneering work of tree-structured topic models by Griffiths et al. (2004), several extended models have been proposed (Ghahramani et al., 2010; Zavitsanos et al., 2011; Kim et al., 2012; Ahmed et al., 2013; Paisley et al., 2014). Our model is based on the modeling assumption of Wang and Blei (2009); Blei et al. (2010), while parameterizing a topic distribution with AEVB. In the context of applying AEVB to flat d"
2020.coling-main.149,P12-1092,0,0.0704095,"construction error. This result contradicts prior work (Mu and Viswanath, 2018) that proposed to remove the top principal components from pre-trained embeddings. We experimentally verify our theoretical claims and show that retaining the top principal components is indeed useful for improving pre-trained word embeddings, without requiring access to additional linguistic resources or labeled data. 1 Introduction Pre-trained word embeddings have been successfully used as features for representing input texts in many NLP tasks (Dhillon et al., 2015; Mnih and Hinton, 2009; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014). Mu and Viswanath (2018) showed that the accuracy of pre-trained word embeddings can be further improved in a post-processing step, without requiring additional training data, by removing the mean of the word embeddings (centering) computed over the set of words (i.e. vocabulary) and projecting onto the directions defined by the principal component vectors, excluding the top principal components. They empirically showed that pre-trained word embeddings are distributed in a narrow cone around the mean embedding vector, and centering and projectio"
2020.coling-main.149,2020.repl4nlp-1.19,0,0.0366207,"posal by Mu and Viswanath (2018) to project the word embeddings away from the largest principal component vectors, which is motivated as a method to improve isotropy in the word embedding space. They provided experimental evidence to the effect that largest principal component vectors encode word frequency and removal of them is not detrimental to semantic tasks such as semantic similarity measurement and analogy detection. However, the frequency of a word is an important piece of information for tasks that require differentiating stop words and content words such as in information retrieval. Raunak et al. (2020) demonstrated that removing the top principal components does not necessarily lead to performance improvement. Moreover, contextualised word embeddings such as BERT (Devlin et al., 2019) and Elmo (Peters et al., 2018) have shown to be anisotropic despite their superior performance in a wide-range of NLP tasks (Ethayarajh, 2019). Therefore, it is not readily obvious whether removing the largest principal components to satisfy isotropy is a universally valid strategy. On the other hand, our experimental results show that by autoencoding not only we obtain better embeddings than Mu and Viswanath"
2020.coling-main.565,P19-1024,0,0.123919,"more informative features useful for relation extraction. Our experimental results show that the proposed method achieves superior performance over existing GCN-based models achieving stateof-the-art performance on cross-sentence n-ary relation extraction and SemEval 2010 Task 8 sentence-level relation extraction task. Our model also achieves a comparable performance to the SoTA on the TACRED dataset. 1 Introduction In recent times, Graph Convolutional Network (GCN) (Kipf and Welling, 2016) based models such as Contextualised GCN (C-GCN) (Zhang et al., 2018) and Attention Guided GCN (AGGCN) (Guo et al., 2019) are shown to be useful for relation extraction. The C-GCN model employs pruned dependency trees obtained using a path-centric pruning distance to filter irrelevant nodes from sentence graph to aid in relation extraction. Experiments with C-GCN model shows that the performance of the C-GCN model significantly decreases with the inclusion of all nodes in the sentence graph obtained from the dependency graph (Zhang et al., 2018), thus making it important to use a right pruning distance to achieve optimum performance. The AGGCN model (Guo et al., 2019) on the other hand, instead of using pruned d"
2020.coling-main.565,P16-1105,0,0.38947,"based features and surface-level lexical features were successfully used with support vector machines to improve relation extraction (Rink and Harabagiu, 2010). Sequence-based Neural network models such as Recurrent Neural Networks (RNNs), LSTMs, BiLSTMs have been used with graph-based features for relation extraction (Socher et al., 2012). Shortest dependency path (SDP) between entities combined with RNNs are also shown to be useful for relation extraction (Xu et al., 2015). Further, word sequence and dependency tree substructures are combined to jointly model entity and relation extraction (Miwa and Bansal, 2016). More recently, graph-based LSTM networks have been shown particularly useful in the context of n-ary cross-sentence relation extraction (Peng et al., 2017; Song et al., 2018). GCN-based models such C-GCN (Zhang et al., 2018) and AGGCN (Guo et al., 2019) are shown to achieve SoTA performance on sentence-level and cross-sentence relation extraction. A combined neural network model that brings together the usefulness of LSTM networks in learning from longer sequences and CNNs to capture salient features is also proposed cross-sentence n-ary relation extraction (Mandya et al., 2018). The main fo"
2020.coling-main.565,Q17-1008,0,0.292355,"y considers smaller graphs associated with entities to learn representation for important nodes in the sentence. Further, while the pruning strategy used by the C-GCN model (Zhang et al., 2018) is applicable for sentence-level relation extraction, it is not suitable for cross-sentence n-ary relation extraction as there could be more than two entities across multiple sentences. The challenges arise in obtaining a single dependency path in the LCA sub-tree connecting n-ary entities across sentences for cross-sentence relation extraction. Previous work on cross-sentence relation extraction task (Peng et al., 2017; Song et al., 2018; Guo et al., 2019) have largely made use of full dependency graph with additional features such as adjacent words and co-reference links (Peng et al., 2017). The usage of full dependency tree significantly increases network parameters, requiring high computational power. In contrast to these works on n-ary relation extraction, we propose in this paper a novel method to construct sub-graphs using connections ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the Univers"
2020.coling-main.565,D14-1162,0,0.0883012,"Missing"
2020.coling-main.565,S10-1057,0,0.26841,"nt areas such as bioinformatics (Borgwardt et al., 2005), chemoinformatics (Duvenaud et al., 2015), social network analysis (Backstrom and Leskovec, 2011), urban computing (Bao et al., 2017) and natural language processing (Borgwardt et al., 2005). With specific application to relation extraction, several studies have examined the use of graph-based mechanisms to improve relation extraction. For example, a combination of dependency graphs, PropBank and FrameNet based features and surface-level lexical features were successfully used with support vector machines to improve relation extraction (Rink and Harabagiu, 2010). Sequence-based Neural network models such as Recurrent Neural Networks (RNNs), LSTMs, BiLSTMs have been used with graph-based features for relation extraction (Socher et al., 2012). Shortest dependency path (SDP) between entities combined with RNNs are also shown to be useful for relation extraction (Xu et al., 2015). Further, word sequence and dependency tree substructures are combined to jointly model entity and relation extraction (Miwa and Bansal, 2016). More recently, graph-based LSTM networks have been shown particularly useful in the context of n-ary cross-sentence relation extraction"
2020.coling-main.565,D12-1110,0,0.0715163,"17) and natural language processing (Borgwardt et al., 2005). With specific application to relation extraction, several studies have examined the use of graph-based mechanisms to improve relation extraction. For example, a combination of dependency graphs, PropBank and FrameNet based features and surface-level lexical features were successfully used with support vector machines to improve relation extraction (Rink and Harabagiu, 2010). Sequence-based Neural network models such as Recurrent Neural Networks (RNNs), LSTMs, BiLSTMs have been used with graph-based features for relation extraction (Socher et al., 2012). Shortest dependency path (SDP) between entities combined with RNNs are also shown to be useful for relation extraction (Xu et al., 2015). Further, word sequence and dependency tree substructures are combined to jointly model entity and relation extraction (Miwa and Bansal, 2016). More recently, graph-based LSTM networks have been shown particularly useful in the context of n-ary cross-sentence relation extraction (Peng et al., 2017; Song et al., 2018). GCN-based models such C-GCN (Zhang et al., 2018) and AGGCN (Guo et al., 2019) are shown to achieve SoTA performance on sentence-level and cro"
2020.coling-main.565,D18-1246,0,0.0300279,"graphs associated with entities to learn representation for important nodes in the sentence. Further, while the pruning strategy used by the C-GCN model (Zhang et al., 2018) is applicable for sentence-level relation extraction, it is not suitable for cross-sentence n-ary relation extraction as there could be more than two entities across multiple sentences. The challenges arise in obtaining a single dependency path in the LCA sub-tree connecting n-ary entities across sentences for cross-sentence relation extraction. Previous work on cross-sentence relation extraction task (Peng et al., 2017; Song et al., 2018; Guo et al., 2019) have largely made use of full dependency graph with additional features such as adjacent words and co-reference links (Peng et al., 2017). The usage of full dependency tree significantly increases network parameters, requiring high computational power. In contrast to these works on n-ary relation extraction, we propose in this paper a novel method to construct sub-graphs using connections ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool an"
2020.coling-main.565,P15-1150,0,0.135021,"Missing"
2020.coling-main.565,D15-1206,0,0.490291,"the use of graph-based mechanisms to improve relation extraction. For example, a combination of dependency graphs, PropBank and FrameNet based features and surface-level lexical features were successfully used with support vector machines to improve relation extraction (Rink and Harabagiu, 2010). Sequence-based Neural network models such as Recurrent Neural Networks (RNNs), LSTMs, BiLSTMs have been used with graph-based features for relation extraction (Socher et al., 2012). Shortest dependency path (SDP) between entities combined with RNNs are also shown to be useful for relation extraction (Xu et al., 2015). Further, word sequence and dependency tree substructures are combined to jointly model entity and relation extraction (Miwa and Bansal, 2016). More recently, graph-based LSTM networks have been shown particularly useful in the context of n-ary cross-sentence relation extraction (Peng et al., 2017; Song et al., 2018). GCN-based models such C-GCN (Zhang et al., 2018) and AGGCN (Guo et al., 2019) are shown to achieve SoTA performance on sentence-level and cross-sentence relation extraction. A combined neural network model that brings together the usefulness of LSTM networks in learning from lon"
2020.coling-main.565,D17-1004,0,0.318869,"sub-graphs around entities using the dependency tree for relation extraction; (c) provide evidence to substantiate the use of multiple sub-graphs instead of a single graph for relation extraction; and (d) evaluate C-GCN-MG model on standard datasets for relation extraction and show that C-GCN-MG achieves SoTA performance on cross-sentence n-ary relation extraction dataset (Peng et al., 2017) and SemEval 2010 Task 8 dataset (Hendrickx et al., 2019) to outperform C-GCN and AGGCN models. We also show that C-GCN-MG achieves comparable performance against C-GCN and AGGCN models on TACRED dataset (Zhang et al., 2017). 2 Related Work GCNs have been successfully applied for various information retrieval tasks in different areas such as bioinformatics (Borgwardt et al., 2005), chemoinformatics (Duvenaud et al., 2015), social network analysis (Backstrom and Leskovec, 2011), urban computing (Bao et al., 2017) and natural language processing (Borgwardt et al., 2005). With specific application to relation extraction, several studies have examined the use of graph-based mechanisms to improve relation extraction. For example, a combination of dependency graphs, PropBank and FrameNet based features and surface-leve"
2020.coling-main.565,D18-1244,0,0.0665346,"ormed over the resulting multiple sub-graphs to obtain more informative features useful for relation extraction. Our experimental results show that the proposed method achieves superior performance over existing GCN-based models achieving stateof-the-art performance on cross-sentence n-ary relation extraction and SemEval 2010 Task 8 sentence-level relation extraction task. Our model also achieves a comparable performance to the SoTA on the TACRED dataset. 1 Introduction In recent times, Graph Convolutional Network (GCN) (Kipf and Welling, 2016) based models such as Contextualised GCN (C-GCN) (Zhang et al., 2018) and Attention Guided GCN (AGGCN) (Guo et al., 2019) are shown to be useful for relation extraction. The C-GCN model employs pruned dependency trees obtained using a path-centric pruning distance to filter irrelevant nodes from sentence graph to aid in relation extraction. Experiments with C-GCN model shows that the performance of the C-GCN model significantly decreases with the inclusion of all nodes in the sentence graph obtained from the dependency graph (Zhang et al., 2018), thus making it important to use a right pruning distance to achieve optimum performance. The AGGCN model (Guo et al."
2020.lrec-1.248,P17-1171,0,0.0562696,"Missing"
2020.lrec-1.248,D18-1241,0,0.0534569,"Missing"
2020.lrec-1.248,D14-1162,0,0.0897838,"Missing"
2020.lrec-1.248,N18-1202,0,0.014793,"ndard training method used in CoQA, shown in Figure 2(a). As seen in Figure 2(a), the contextual paragraph, ci , the current question, qi , previous questions, q1 , .., qi−1 , and the previous answer utterances, a1 , .., ai−1 , are used as inputs to train the CoQA model. Using a representation layer, the inputs are transformed into low dimensional vectors using pre-trained word embeddings such as GloVe (Pennington et al., 2014), as used in Seq2Seq, PGNet, DrQA, DrQA+PGNet (Reddy et al., 2018), FlowQA (Huang et al., 2018) and SDNet (Zhu et al., 2018) models. Contextual embeddings such as ELMo (Peters et al., 2018) and CoVE (McCann et al., 2017) are also used in FlowQA (Huang et al., 2018) and BERT (Devlin et al., 2018) is used in SDNet (Zhu et al., 2018) model. In our experiments, we use pretrained GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2018) embeddings with SDNet. The word representation is then provided as the input to a reasoning layer, which primarily comprises of numerous sequence based networks such as LSTMs (Hochreiter and Schmidhuber, 1997) or self-attention (Vaswani et al., 2017) to encode the context and question. The final output vector from the reasoning layer is then pass"
2020.lrec-1.248,P17-1099,0,0.015063,"cted answers in CoQA models) and, (b) propose and evaluate various SS techniques to overcome exposure bias in CoQA systems. 2. Related Work The Conversational Question Answering (CoQA) dataset (Reddy et al., 2018) was developed for evaluating systems for answering questions in the form of a continuous dialogue. The key characteristic of this dataset is that it provides human-like conversational questions and preserves the naturalness of answers present in conversations. Several baseline systems such as sequence-to-sequence (Seq2Seq) (Sutskever et al., 2011), pointer-generator network (PGNet) (See et al., 2017), Document Reader Question Answering (DrQA) (Chen et al., 2017) and a combined DrQA+PGNet model were initially proposed for CoQA (Reddy et al., 2018). The BiDAF++ model (Yatskar, 2018), based on the Bi-directional attention flow (BiDAF) (Seo et al., 2016), used self-attention (Clark and Gardner, 2017) to answer conversational questions. F LOW QA (Huang et al., 2018) used a mechanism to incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. In comparison to previous approaches that simply conca"
2020.lrec-1.248,N19-1241,0,\N,Missing
2020.lrec-1.248,Q19-1016,0,\N,Missing
2020.lrec-1.475,Q16-1028,0,0.0236222,"cients, computed between human similarity ratings and cosine similarities between the words computed using their subword-composed embeddings. 5. Results Performances of different tokenisation methods and composition methods across languages are summarised in Table 2. Among the composition methods, we see that weighted+PC removal (SIF) consistently outperforms both unweighted and weighted for all languages. To the best of our knowledge, SIF has not been used before for creating word embeddings from subword embeddings. Ethayarajh (2018) showed that by modifying the random walk model proposed by Arora et al. (2016) such that the probability of generating a word given its discourse is proportional not with the inner-product between embeddings, but with their angular distance, vector length confounding effects in SIF can be rectified to create more accurate sentence embeddings. Given such developments, an interesting future research direction would be to apply sentence embedding methods to learn better word embeddings given a subtokenisation. From Table 2, we see that the best performances are reported by LST for all languages except for de and fa where respectively LM and BPE are the best. Interestingly,"
2020.lrec-1.475,W18-1810,0,0.0240077,"Missing"
2020.lrec-1.475,S17-2002,0,0.0254606,"s only for 300 dimensional embeddings. A word can be tokenised into multiple subwords by both Language weighted + PC removal: After creating word embeddings using (1), we substract the first Principal Component (PC) as suggested by Arora et al. (2017) to remove information that is common to all words, thereby emphasising the relative semantic differences among words. 4.2. Datasets and Evaluation Measures To evaluate the word embeddings created using different tokenisation and composition methods described above, we use the datasets created for en, de, fr, es, it and fa in SemEval 2017 Task 2 (Camacho-Collados et al., 2017) monolingual word similarity evaluation task. For ja we used the dataset created by Kodaira et al. (2016) via crowd sourcing for evaluating lexical simplification rules, which covers word-pairs categorised into different PoS categories. For th, we used Thai SimLex-999 dataset created by Netisopakul 3854 et al. (2019). They first translated the word-pairs in the English SimLex-999 (Hill et al., 2015) and then asked 16 annotators, who are native Thai speakers, score the word-pairs for similarity, following the guidelines of SimLex-999 (Hill et al., 2015). For tr, we used the AnlamVer dataset (Er"
2020.lrec-1.475,D18-1366,0,0.0224925,"entations. Both BPE and unigram language model can be trained using untokenised text corpora. Moreover, both methods can be used independently of the language, which make them ideal candidates for tokenising resource poor languages. Because of those reasons BPE and unigram language model are considered as LIT methods to compare in this paper. 3. Related Work Learning embeddings for the subtokens produced by LIT methods has shown to be an effective method to overcome data sparseness issues encountered when training named entity recognisers for low-resource languages such as Uyghur and Bengali (Chaudhary et al., 2018). By modelling a word as a bag of subtokens and combining pre-trained subtoken embeddings to represent rare out-ofvocabulary (OOV) words, Zhao et al. (2018) obtained SoTA results for joint prediction of POS tagging and morphosyntactic attributes in 23 languages. These prior work show that LIT can be used to overcome OOV and rare word related issues and is especially effective for resource poor languages, but did not perform a systematic comparison between LIT vs LST methods for those tasks. Zhu et al. (2019) compared supervised morphological segmentation (SMS) by CHIPMUNK, Morfessor (a family"
2020.lrec-1.475,D15-1141,0,0.0744676,"Missing"
2020.lrec-1.475,C18-1323,0,0.0154505,"7) monolingual word similarity evaluation task. For ja we used the dataset created by Kodaira et al. (2016) via crowd sourcing for evaluating lexical simplification rules, which covers word-pairs categorised into different PoS categories. For th, we used Thai SimLex-999 dataset created by Netisopakul 3854 et al. (2019). They first translated the word-pairs in the English SimLex-999 (Hill et al., 2015) and then asked 16 annotators, who are native Thai speakers, score the word-pairs for similarity, following the guidelines of SimLex-999 (Hill et al., 2015). For tr, we used the AnlamVer dataset (Ercan and Yıldız, 2018) contains relatedness and similarity ratings for 500 Turkish word-pairs, annotated by 12 human annotators. Following the official evaluation measure used in SemEval 2017 Task 2, on all datasets we report the harmonic mean of the Spearman and the Pearson correlation coefficients, computed between human similarity ratings and cosine similarities between the words computed using their subword-composed embeddings. 5. Results Performances of different tokenisation methods and composition methods across languages are summarised in Table 2. Among the composition methods, we see that weighted+PC remov"
2020.lrec-1.475,W18-3012,0,0.127017,"27 62,444,210 40,105,530 Table 1: Sizes of corpora used in the experiments LST and LIT methods. For the purpose of composing the embedding of a word from the embeddings of its subwords, Zhu et al. (2019) compared vector addition, elementiwse multiplication and self-attention-based composition (Lin et al., 2017). They found vector addition to outperform other composition methods across languages and tasks. On the other hand, prior work on sentence embedding have shown that a weighted-average of word embeddings to produce simple yet surprisingly accurate sentence embeddings (Arora et al., 2017; Ethayarajh, 2018). Inspired by these prior findings, we propose and compare three methods for composing a word embedding from its subword embeddings as follows: unweighted: This is the simple unweighted vector addition that reported the best performance in Zhu et al. (2019). weighted: We use the Smoothed Inverse Frequency (SIF) (Arora et al., 2017), where a word embedding w is computed as the sum of its constituent set of subwords, S(w), weighted by their inverse unigram probabilities, p(x), for subwords x ∈ S(w) as given by (1). X a x (1) w= a + p(w) x∈S(w) Here, the smoothing parameter a is set to 0.001 foll"
2020.lrec-1.475,J15-4004,0,0.234669,"luate the word embeddings created using different tokenisation and composition methods described above, we use the datasets created for en, de, fr, es, it and fa in SemEval 2017 Task 2 (Camacho-Collados et al., 2017) monolingual word similarity evaluation task. For ja we used the dataset created by Kodaira et al. (2016) via crowd sourcing for evaluating lexical simplification rules, which covers word-pairs categorised into different PoS categories. For th, we used Thai SimLex-999 dataset created by Netisopakul 3854 et al. (2019). They first translated the word-pairs in the English SimLex-999 (Hill et al., 2015) and then asked 16 annotators, who are native Thai speakers, score the word-pairs for similarity, following the guidelines of SimLex-999 (Hill et al., 2015). For tr, we used the AnlamVer dataset (Ercan and Yıldız, 2018) contains relatedness and similarity ratings for 500 Turkish word-pairs, annotated by 12 human annotators. Following the official evaluation measure used in SemEval 2017 Task 2, on all datasets we report the harmonic mean of the Spearman and the Pearson correlation coefficients, computed between human similarity ratings and cosine similarities between the words computed using th"
2020.lrec-1.475,kawahara-etal-2002-construction,0,0.0441103,"Missing"
2020.lrec-1.475,P16-3001,0,0.0609006,"PC removal: After creating word embeddings using (1), we substract the first Principal Component (PC) as suggested by Arora et al. (2017) to remove information that is common to all words, thereby emphasising the relative semantic differences among words. 4.2. Datasets and Evaluation Measures To evaluate the word embeddings created using different tokenisation and composition methods described above, we use the datasets created for en, de, fr, es, it and fa in SemEval 2017 Task 2 (Camacho-Collados et al., 2017) monolingual word similarity evaluation task. For ja we used the dataset created by Kodaira et al. (2016) via crowd sourcing for evaluating lexical simplification rules, which covers word-pairs categorised into different PoS categories. For th, we used Thai SimLex-999 dataset created by Netisopakul 3854 et al. (2019). They first translated the word-pairs in the English SimLex-999 (Hill et al., 2015) and then asked 16 annotators, who are native Thai speakers, score the word-pairs for similarity, following the guidelines of SimLex-999 (Hill et al., 2015). For tr, we used the AnlamVer dataset (Ercan and Yıldız, 2018) contains relatedness and similarity ratings for 500 Turkish word-pairs, annotated b"
2020.lrec-1.475,D18-2012,0,0.0291443,"Missing"
2020.lrec-1.475,W04-3230,0,0.351106,"reas some such as world are. Therefore, the effect of subtokenisation on downstream NLP tasks that require the semantics of the original input string to be retained remains unclear. The complexity of the tokenisation problem is language dependent. For example, punctuation rules, delimiter characters etc. have found to be adequate to tokenise nonagglutinative languages such as English or Italian (Moreau and Vogel, 2018), whereas non-white space delimited languages such as Japanese or Chinese require more sophisticated methods that jointly perform Part of Speech (PoS) tagging with tokenisation (Kudo et al., 2004). Moreover, hyphenated words, acronyms that use punctuations must be treated as single tokens in most NLP applications, which makes tokenisation a complex problem. Tokenisation methods can be classified into languagespecific tokenisation (LST) and language-independent tokenisation (LIT). LST methods require lexicons for the language under consideration and are often trained on manually tokenised corpora. The accuracy of LST depends on the coverage and quality of the linguistic resources used to train them. In particular, when the coverage of the training resources are poor such as for rare wor"
2020.lrec-1.475,P18-1007,0,0.0745259,"ic resources used to train them. In particular, when the coverage of the training resources are poor such as for rare words, named entities or neologisms, the accuracy of tokenisation of out of vocabulary (OOV) words can be low. LST methods have been trained using different sequence labelling methods such as hidden Markov models (HMMs) (Jurish and W¨urzner, 2013), conditional random fields (CRFs) (Kudo et al., 2004) and recurrent neural networks (RNNs) (Morita et al., 2015). LIT has gained popularity as an alternative to LST (Sennrich et al., 2016; Zhu et al., 2019; Kudo and Richardson, 2018; Kudo, 2018; Schuster and Nakajima, 2012) because, unlike LST, LIT methods do not require predefined vocabularies nor manually tokenised texts, and operate on statistical information obtained from a large text corpora. For example, text compression methods such as byte pair encoding (BPE) (Gage, 1994; Sennrich et al., 2016) and language modelling (LM) methods (Kudo, 2018) automatically select frequent subwords as tokens, and segment a given text such that some loss function (e.g. negative likelihood or code length) is minimised. LIT has become the de-facto standard in text generation applications such as"
2020.lrec-1.475,P14-5010,0,0.00265018,"mental results described in the paper. Several prior work have already investigated the effect of subtokenisation for different NLP tasks. We describe these related prior work in Section 3. and highlight the important differences between the findings reported in this paper. Evaluation protocol and experimental results comparing LST vs. LIT methods are described in Section 4.. 2. 2.1. Background Language Specific Tokenisation LST methods use language-specific resources such as lexicons, manually tokenised corpora and/or languagespecific rules. Earlier versions of the Stanford Core NLP toolkit (Manning et al., 2014) internally used JFLex1 , a meta language for specifying tokenisation rules based on regular expressions and procedures, to execute when a rule matches. Unlike the statistical tokenisers, rule-based tokenisers are easier to debug and their behaviour is deterministic. For example, a product name might be required to tokenise in a specific manner, which is easier to specify as a rule rather than having to prepare numerous manually tokenised examples of contexts to train a model. For those reasons, rule-based tokenisers have been used extensively in industrial NLP applications either as a standal"
2020.lrec-1.475,H94-1020,0,0.0929909,"s paper, we propose the use of smoothed inverse frequency (SIF), which was originally proposed by Arora et al. (2017) for creating sentence embeddings from word embeddings, for the purpose creating word embeddings from subword embeddings. 4. Evaluation Protocol Evaluating tokenisation methods is a challenging task because there is no universally agreed gold standard for tokenisation (Habert et al., 1998; Webster and Kit, 1992). Tokenisation depends both on the language as well as the task for which it is used. Although there are some manually tokenised texts such as the Penn Treebank dataset (Marcus et al., 1994) for English and Kyoto University corpus (Kawahara et al., 2002) for Japanese that can be used to train and evaluate LST methods, no such resources are available for LIT evaluation. Indeed, given that the subtokens produced by LIT methods are arbitrary and depends on the size of the vocabulary specified by the user and the statistics in the corpus used to train the LIT method, what is a valid LIT of a given text remains undefined in the first place. Therefore, following prior work comparing LST and LIT methods, we resort to an extrinsic evaluation approach where we use the tokenised output pro"
2020.lrec-1.475,N13-1090,0,0.264698,"lts for joint prediction of POS tagging and morphosyntactic attributes in 23 languages. These prior work show that LIT can be used to overcome OOV and rare word related issues and is especially effective for resource poor languages, but did not perform a systematic comparison between LIT vs LST methods for those tasks. Zhu et al. (2019) compared supervised morphological segmentation (SMS) by CHIPMUNK, Morfessor (a family of generative probabilistic models for unsupervised morphological segmentation) and BPE. They train word and subword embeddings using skip-gram with negative sampling (SGNS) (Mikolov et al., 2013a). They used multilingual word similarity, universal dependency parsing and fine-grained entity typing as the evaluation tasks. They found that subword SGNS embeddings outperform subword-agnostic SGNS embeddings for morphologically richer languages such as Finnish and Turkish. SMS, which is trained according to the readily available gold standard morphological segmentations, performs best for word similarity but worst for entity typing. Compared to BPE, which produces short and nonsensical subwords, Morfessor is a conservative segmenter that captures longer subwords. Consequently, Morfessor r"
2020.lrec-1.475,L18-1180,0,0.064437,"Missing"
2020.lrec-1.475,D15-1276,0,0.0606248,"Missing"
2020.lrec-1.475,H94-1054,0,0.0916877,"those reasons, rule-based tokenisers have been used extensively in industrial NLP applications either as a standalone module or in conjunction with statistical tokenisers (Remus et al., 2016). Statistical or machine learning-based tokenisation methods model tokenisation as a sequence labelling problem where we must predict whether a token boundary must be placed at a given position in an input text string. For example, information about the current token and its context such as previous or following tokens can be used as features for training a sequence labeller such as a hidden Markov model (Papageorgiou, 1994), conditional random field (Kudo et al., 2004) or a recurrent neural network (Chen et al., 2015). In languages such as Japanese or Chinese where multiple possible tokenisations of the input string exist, one must find the most likely sequence of tokens (Kudo et al., 2004). This can be modelled as a dynamic programming problem and solved efficiently via forward-backward inference methods. Moreover, token boundaries as well as morphological properties of the tokens such as their part-of-speech (POS) tags can be simultaneously determined, which is known as morphological analysis. To train statist"
2020.lrec-1.475,D14-1162,0,0.0868533,"information and the noise introduced by LIT out weights the benefits of using a small and fixed vocabulary, enabling us to overcome OOV issues across typologically diverse languages. To empirically answer this question, we compare LST vs. LIT for multilingual lexical semantic similarity prediction for the eight languages: English (en), German (de), Spanish (es), Farsi (fa), Italian (it), Japanese (ja), Turkish (tr) and Thai (th). Our contributions and findings in this paper can be summarised as follows: • We independently conduct LST and LIT on eight languages and use Global Vectors (GloVe) (Pennington et al., 2014) to learn word embeddings. We then predict the semantic similarity between two words using the learnt word embeddings, and measure the correlation against human similarity ratings across a suite of benchmark datasets. • We evaluate different methods to compose word embeddings from subword embeddings and find that Smoothed Inverse Frequency (SIF) (Arora et al., 2017) to outperform simple averaging, which has shown to be a strong baseline in prior work. • For LIT methods, for the first time, we compare BPE and LM in terms of both tokenisation speed and their accuracies for predicting semantic si"
2020.lrec-1.475,P19-1493,0,0.0292127,"s, has received much attention lately with their effectiveness in deep learning-based NLP models. For example, named entities, cognates/loanwords, and morphologically complex words that contain multiple morphemes are extremely challenging to properly tokenise because the occurrences of such terms are rare even in large training datasets. On the other hand, substrings of such terms are likely to be more frequent. Tokenising texts into subtokens has been sufficient for a broad range of NLP 1 3852 https://jflex.de/ tasks such as machine translation (Sennrich et al., 2016) and language modelling (Pires et al., 2019), where tokens are represented using lower-dimensional embedding vectors and fed into deep learning architectures. Sennrich et al. (2016) proposed a subtokenisation method inspired by BPE, which is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single unused byte. Specifically, the set of symbols (i.e. symbol vocabulary) is initialised with the set of characters and each word is represented as a sequence of characters, plus a special end-of-word symbol. This is useful if we want to restore the original tokenisation after subtokenisin"
2020.lrec-1.475,W16-2613,0,0.0203789,"okenisation rules based on regular expressions and procedures, to execute when a rule matches. Unlike the statistical tokenisers, rule-based tokenisers are easier to debug and their behaviour is deterministic. For example, a product name might be required to tokenise in a specific manner, which is easier to specify as a rule rather than having to prepare numerous manually tokenised examples of contexts to train a model. For those reasons, rule-based tokenisers have been used extensively in industrial NLP applications either as a standalone module or in conjunction with statistical tokenisers (Remus et al., 2016). Statistical or machine learning-based tokenisation methods model tokenisation as a sequence labelling problem where we must predict whether a token boundary must be placed at a given position in an input text string. For example, information about the current token and its context such as previous or following tokens can be used as features for training a sequence labeller such as a hidden Markov model (Papageorgiou, 1994), conditional random field (Kudo et al., 2004) or a recurrent neural network (Chen et al., 2015). In languages such as Japanese or Chinese where multiple possible tokenisat"
2020.lrec-1.475,J18-3005,0,0.062308,"Missing"
2020.lrec-1.475,P16-1162,0,0.195959,"accuracy of LST depends on the coverage and quality of the linguistic resources used to train them. In particular, when the coverage of the training resources are poor such as for rare words, named entities or neologisms, the accuracy of tokenisation of out of vocabulary (OOV) words can be low. LST methods have been trained using different sequence labelling methods such as hidden Markov models (HMMs) (Jurish and W¨urzner, 2013), conditional random fields (CRFs) (Kudo et al., 2004) and recurrent neural networks (RNNs) (Morita et al., 2015). LIT has gained popularity as an alternative to LST (Sennrich et al., 2016; Zhu et al., 2019; Kudo and Richardson, 2018; Kudo, 2018; Schuster and Nakajima, 2012) because, unlike LST, LIT methods do not require predefined vocabularies nor manually tokenised texts, and operate on statistical information obtained from a large text corpora. For example, text compression methods such as byte pair encoding (BPE) (Gage, 1994; Sennrich et al., 2016) and language modelling (LM) methods (Kudo, 2018) automatically select frequent subwords as tokens, and segment a given text such that some loss function (e.g. negative likelihood or code length) is minimised. LIT has become the"
2020.lrec-1.475,C92-4173,0,0.213187,"found that addition to be an extremely robust composition function across languages and tasks. Surprisingly, the more sophisticated selfattention reports poor performance in many tasks. In this paper, we propose the use of smoothed inverse frequency (SIF), which was originally proposed by Arora et al. (2017) for creating sentence embeddings from word embeddings, for the purpose creating word embeddings from subword embeddings. 4. Evaluation Protocol Evaluating tokenisation methods is a challenging task because there is no universally agreed gold standard for tokenisation (Habert et al., 1998; Webster and Kit, 1992). Tokenisation depends both on the language as well as the task for which it is used. Although there are some manually tokenised texts such as the Penn Treebank dataset (Marcus et al., 1994) for English and Kyoto University corpus (Kawahara et al., 2002) for Japanese that can be used to train and evaluate LST methods, no such resources are available for LIT evaluation. Indeed, given that the subtokens produced by LIT methods are arbitrary and depends on the size of the vocabulary specified by the user and the statistics in the corpus used to train the LIT method, what is a valid LIT of a given"
2020.lrec-1.475,C14-1212,0,0.0224769,". We see that various inflections of masu are listed as the top nearest neighbours such as its past tense (mashita), negation (masen) and the volitional form (mashou). We also see that other frequent sentence ending forms such as desu and kudasai are also listed as nearest neighbours. Similar trends have been reported with distributional word-level embeddings, where both semantically similar as well as related/associated words are often found as the nearest neighbours for a given word when the cosine similarity between word embeddings is used as the neighbourhood criterion (Hill et al., 2015; Weeds et al., 2014). Table 7 shows the nearest neighbours for the Turkish suffixes iyor and miyor, which respectively denote the present tense and its negation. Likewise in English and Japanese results, we see related words are listed as the nearest neighbours for those suffixes. However, the nearest neighbours retrieved in the case of Turkish are more noisier compared to that for English and Japanese. We believe this is due to the comparatively smaller corpora used for Turkish. Word embedding spaces learnt by word2vec and GloVe have shown to demonstrate a surprisingly high degree of relational structure, which"
2020.lrec-1.475,W18-5605,0,0.0307351,"bility of a given text getting over-tokenised into many smaller tokens increases with the size of the vocabulary for LIT. Creating the embedding for a word using embeddings for its subtokens becomes difficult when the word is split into many subtokens, some of which might be too small to retain the semantics of the original word. Recall that SIF method creates word embeddings as the weightedaverage of the subtoken embeddings, ignoring the position of the subtoken in the word. Incorporating characterlevel embeddings via LSTMs has shown to improve performance for named entity recognition tasks (Zhai et al., 2018). Therefore, applying more sophisticated supervised composition methods such as a recurrent neural network might help to create word embeddings from subtoken embeddings under such situations. We defer this line of investigation for future work. We conclude here that the size of the vocabulary is a hyperparameter of LIT methods that must be carefully set considering the performance of the target task. 5.2. Nearest Neighbour Analysis Given that some subtokens correspond to character ngrams representing morphology such as inflections, it remains an interesting qualitative analysis to study whethe"
2020.lrec-1.475,D18-1059,0,0.0248724,"hich make them ideal candidates for tokenising resource poor languages. Because of those reasons BPE and unigram language model are considered as LIT methods to compare in this paper. 3. Related Work Learning embeddings for the subtokens produced by LIT methods has shown to be an effective method to overcome data sparseness issues encountered when training named entity recognisers for low-resource languages such as Uyghur and Bengali (Chaudhary et al., 2018). By modelling a word as a bag of subtokens and combining pre-trained subtoken embeddings to represent rare out-ofvocabulary (OOV) words, Zhao et al. (2018) obtained SoTA results for joint prediction of POS tagging and morphosyntactic attributes in 23 languages. These prior work show that LIT can be used to overcome OOV and rare word related issues and is especially effective for resource poor languages, but did not perform a systematic comparison between LIT vs LST methods for those tasks. Zhu et al. (2019) compared supervised morphological segmentation (SMS) by CHIPMUNK, Morfessor (a family of generative probabilistic models for unsupervised morphological segmentation) and BPE. They train word and subword embeddings using skip-gram with negativ"
2020.lrec-1.475,N19-1097,0,0.035878,"Missing"
2021.eacl-main.107,2020.acl-main.431,0,0.485259,"texts it occurs, contextualised embeddings ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. use dynamic context dependent vectors for representing a word in a specific context. Unfortunately however, it has been shown that, similar to their non-contextual counterparts, contextualised text embeddings also encode various types of unfair biases (Zhao et al., 2019; Bordia and Bowman, 2019; May et al., 2019; Tan and Celis, 2019; Bommasani et al., 2020; Kurita et al., 2019). This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and"
2021.eacl-main.107,N19-3002,0,0.0358314,"t al., 2013) that represent a word by a single vector in all contexts it occurs, contextualised embeddings ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. use dynamic context dependent vectors for representing a word in a specific context. Unfortunately however, it has been shown that, similar to their non-contextual counterparts, contextualised text embeddings also encode various types of unfair biases (Zhao et al., 2019; Bordia and Bowman, 2019; May et al., 2019; Tan and Celis, 2019; Bommasani et al., 2020; Kurita et al., 2019). This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing stati"
2021.eacl-main.107,S17-2001,0,0.063784,"d embedding model. Following prior work, we compare the proposed debiasing method in two sentence-level tasks: Sentence Encoder Association Test (SEAT; May et al., 2019) and Multi-genre co-reference-based Natural Language Inference (MNLI; Dev et al., 2020). Experimental results show that the proposed method not only debiases all contextualised word embedding models compared, but also preserves useful semantic information for solving downstream tasks such as sentiment classification (Socher et al., 2013), paraphrase detection (Dolan and Brockett, 2005), semantic textual similarity measurement (Cer et al., 2017), natural language inference (Dagan et al., 2005; Bar-Haim et al., 2006) and solving Winograd schema (Levesque et al., 2012). We consider gender bias as a running example throughout this paper and evaluate the proposed method with respect to its ability to overcome gender bias in contextualised word embeddings, and defer extensions to other types of biases to future work. 2 Related Work Prior work on debiasing word embeddings can be broadly categorised into two groups depending on whether they consider static or contextualised word embeddings. Although we focus on contextualised embeddings in"
2021.eacl-main.107,N19-1423,0,0.529008,"d in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off. 1 Introduction Contextualised word embeddings have significantly improved performance in numerous natural language processing (NLP) applications (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020) and have established as the de facto standard for input text representations. Compared to static word embeddings (Pennington et al., 2014; Mikolov et al., 2013) that represent a word by a single vector in all contexts it occurs, contextualised embeddings ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. use dynamic context dependent vectors for representing a word in a specific context."
2021.eacl-main.107,I05-5002,0,0.337936,"anularities and on different layers in the pre-trained contextualised embedding model. Following prior work, we compare the proposed debiasing method in two sentence-level tasks: Sentence Encoder Association Test (SEAT; May et al., 2019) and Multi-genre co-reference-based Natural Language Inference (MNLI; Dev et al., 2020). Experimental results show that the proposed method not only debiases all contextualised word embedding models compared, but also preserves useful semantic information for solving downstream tasks such as sentiment classification (Socher et al., 2013), paraphrase detection (Dolan and Brockett, 2005), semantic textual similarity measurement (Cer et al., 2017), natural language inference (Dagan et al., 2005; Bar-Haim et al., 2006) and solving Winograd schema (Levesque et al., 2012). We consider gender bias as a running example throughout this paper and evaluate the proposed method with respect to its ability to overcome gender bias in contextualised word embeddings, and defer extensions to other types of biases to future work. 2 Related Work Prior work on debiasing word embeddings can be broadly categorised into two groups depending on whether they consider static or contextualised word em"
2021.eacl-main.107,D19-1635,0,0.0165092,"or the protected attributes. They conclude that adversarial learning alone does not guarantee invariant representations for the protected attributes. Ravfogel et al. (2020) found that iteratively projecting word embeddings to the null space of the gender direction to further improve the debiasing performance. Benchmarks for biases in Static Embeddings: Word Embedding Association Test (WEAT; Caliskan et al., 2017) quantifies various biases (e.g. gender, race and age) using semantic similarities between word embeddings. Word Association Test (WAT) measures gender bias over a large set of words (Du et al., 2019) by calculating the gender information vector for each word in a word association graph created in the Small World of Words project (SWOWEN; Deyne et al., 2019) by propagating masculine and feminine words via a random walk (Zhou et al., 2003). SemBias dataset (Zhao et al., 2018b) contains three types of word-pairs: (a) Definition, a gender-definition word pair (e.g. hero – heroine), (b) Stereotype, a gender-stereotype word pair (e.g., manager – secretary) and (c) None, two other word-pairs with similar meanings unrelated to gender (e.g., jazz – blues, pencil – pen). It uses the cosine similari"
2021.eacl-main.107,D18-1002,0,0.274764,"extual counterparts, contextualised text embeddings also encode various types of unfair biases (Zhao et al., 2019; Bordia and Bowman, 2019; May et al., 2019; Tan and Celis, 2019; Bommasani et al., 2020; Kurita et al., 2019). This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and Bollegala, 2019; Zhao et al., 2018b; Bolukbasi et al., 2016; Ravfogel et al., 2020) and adversarial methods (Xie et al., 2017; Gonen and Goldberg, 2019). In contrast, despite multiple studies reporting that contextualised embeddings to be unfairly biased, methods for debiasing contextualised embeddings are relatively under explored (Dev et al., 2020; Nadeem et al., 2020; Nangia et al., 2020). Compared to static word embe"
2021.eacl-main.107,W19-3621,0,0.0181532,"s that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and Bollegala, 2019; Zhao et al., 2018b; Bolukbasi et al., 2016; Ravfogel et al., 2020) and adversarial methods (Xie et al., 2017; Gonen and Goldberg, 2019). In contrast, despite multiple studies reporting that contextualised embeddings to be unfairly biased, methods for debiasing contextualised embeddings are relatively under explored (Dev et al., 2020; Nadeem et al., 2020; Nangia et al., 2020). Compared to static word embeddings, debiasing contextualised embeddings is significantly more challenging due to several reasons as we discuss next. First, compared to static word embedding models where the semantic representation of a word is limited to a single vector, contextualised embedding models have a significantly large number of parameters rela"
2021.eacl-main.107,N19-1000,0,0.227617,"Missing"
2021.eacl-main.107,P18-2005,0,0.0598198,"fore, if the classifier erroneously predicts a stereotypical word as gender-definitional, it would not get debiased. Zhao et al. (2018b) modified the original GloVe (Pennington et al., 2014) objective to learn gender-neutral word embeddings (GNGloVe) from a given corpus. Unlike the above1257 mentioned methods, Kaneko and Bollegala (2019) proposed GP-GloVe, a post-processing method to preserve gender-related information with autoencoder (Kaneko and Bollegala, 2020), while removing discriminatory biases from stereotypical cases. Adversarial learning (Xie et al., 2017; Elazar and Goldberg, 2018; Li et al., 2018) for debiasing first encode the inputs and then two classifiers are jointly trained – one predicting the target task (for which we must ensure high prediction accuracy) and the other for protected attributes (that must not be easily predictable). Elazar and Goldberg (2018) showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inputs can still manage to reach substantially high accuracies for the protected attributes. They conclude that adversarial learning alone does not gua"
2021.eacl-main.107,2021.ccl-1.108,0,0.052229,"Missing"
2021.eacl-main.107,D19-1530,0,0.0621907,"d require specialised hardware such as GPU/TPU clusters. On the other hand, fine-tuning a pre-trained contextualised embedding model for a particular task (possibly using labelled data for the target task) is relatively less expensive. Consequently, the standard practice in the NLP community has been to share1 pre-trained contextualised embedding models and fine-tune as needed. Therefore, it is desirable that a debiasing method proposed for contextualised embedding models can be applied as a fine-tuning method. In this view, counterfactual data augmentation methods (Zmigrod et al., 2019; Hall Maudslay et al., 2019; Zhao et al., 2019) that swap gender pronouns in the training corpus for creating a gender balanced version of the training data are less attractive when debiasing contextualised embeddings because we must retrain those models on the balanced corpora, which is more expensive compared to fine-tuning. Using gender-bias as a running example, we address the above-mentioned challenges by proposing a debiasing method that fine-tunes pre-trained contextualised word embeddings2 . Our proposed method retains the semantic information learnt by the contextualised embedding model with respect to gender-r"
2021.eacl-main.107,N19-1063,0,0.36027,"nt a word by a single vector in all contexts it occurs, contextualised embeddings ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. use dynamic context dependent vectors for representing a word in a specific context. Unfortunately however, it has been shown that, similar to their non-contextual counterparts, contextualised text embeddings also encode various types of unfair biases (Zhao et al., 2019; Bordia and Bowman, 2019; May et al., 2019; Tan and Celis, 2019; Bommasani et al., 2020; Kurita et al., 2019). This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings"
2021.eacl-main.107,P19-1160,1,0.924444,"Missing"
2021.eacl-main.107,2020.coling-main.149,1,0.732908,"gender-definitional words during the subsequent debiasing process, and focus only on words that are not predicted as gender-definitional by a classifier. Therefore, if the classifier erroneously predicts a stereotypical word as gender-definitional, it would not get debiased. Zhao et al. (2018b) modified the original GloVe (Pennington et al., 2014) objective to learn gender-neutral word embeddings (GNGloVe) from a given corpus. Unlike the above1257 mentioned methods, Kaneko and Bollegala (2019) proposed GP-GloVe, a post-processing method to preserve gender-related information with autoencoder (Kaneko and Bollegala, 2020), while removing discriminatory biases from stereotypical cases. Adversarial learning (Xie et al., 2017; Elazar and Goldberg, 2018; Li et al., 2018) for debiasing first encode the inputs and then two classifiers are jointly trained – one predicting the target task (for which we must ensure high prediction accuracy) and the other for protected attributes (that must not be easily predictable). Elazar and Goldberg (2018) showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inp"
2021.eacl-main.107,W19-3806,0,0.0331376,"nd obtain the debiased embedding by averaging them. It can only be applied to feature-based embeddings, so it cannot be applied to fine-tuning based embeddings like BERT. We directly debias the contextual embeddings. Additionally, data augmentation requires re-training of the embeddings, which is often costly compared to fine-tuning. Kurita et al. (2019) created masked templates such as “ is a nurse” and used BERT to predict the masked gender pronouns. They used the log-odds between male and female pronoun predictions as an evaluation measure and showed that BERT to be biased according to it. Karve et al. (2019) learnt conceptor matrices using class definitions in the WEAT and used the negated conceptors to debias ELMo and BERT. Although their method was effective for ELMo, the results on BERT were mixed. This method can only be applied to context-independent vectors, and it requires the creation of static embeddings from BERT and ELMo as a pre-processing step for debi1258 Figure 1: Types of hidden states in E considered in the proposed method. The blue boxes in the middle correspond to the hidden states of the target token. asing the context-dependent vectors. Therefore, we do not compare against th"
2021.eacl-main.107,W19-3823,0,0.134449,"ualised embeddings ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. use dynamic context dependent vectors for representing a word in a specific context. Unfortunately however, it has been shown that, similar to their non-contextual counterparts, contextualised text embeddings also encode various types of unfair biases (Zhao et al., 2019; Bordia and Bowman, 2019; May et al., 2019; Tan and Celis, 2019; Bommasani et al., 2020; Kurita et al., 2019). This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and Bollegala, 2019; Zhao"
2021.eacl-main.107,2020.emnlp-main.154,0,0.14684,"018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and Bollegala, 2019; Zhao et al., 2018b; Bolukbasi et al., 2016; Ravfogel et al., 2020) and adversarial methods (Xie et al., 2017; Gonen and Goldberg, 2019). In contrast, despite multiple studies reporting that contextualised embeddings to be unfairly biased, methods for debiasing contextualised embeddings are relatively under explored (Dev et al., 2020; Nadeem et al., 2020; Nangia et al., 2020). Compared to static word embeddings, debiasing contextualised embeddings is significantly more challenging due to several reasons as we discuss next. First, compared to static word embedding models where the semantic representation of a word is limited to a single vector, contextualised embedding models have a significantly large number of parameters related in complex ways. For example, BERT-large model (Devlin et al., 2019) contains 24 layers, 16 attention heads and 340M parameters. Therefore, it is not obvious which parameters are responsible for the unfair biases related to a partic1256 P"
2021.eacl-main.107,D14-1162,0,0.0844824,"Missing"
2021.eacl-main.107,2020.acl-main.647,0,0.221132,"such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and Bollegala, 2019; Zhao et al., 2018b; Bolukbasi et al., 2016; Ravfogel et al., 2020) and adversarial methods (Xie et al., 2017; Gonen and Goldberg, 2019). In contrast, despite multiple studies reporting that contextualised embeddings to be unfairly biased, methods for debiasing contextualised embeddings are relatively under explored (Dev et al., 2020; Nadeem et al., 2020; Nangia et al., 2020). Compared to static word embeddings, debiasing contextualised embeddings is significantly more challenging due to several reasons as we discuss next. First, compared to static word embedding models where the semantic representation of a word is limited to a single vector, contextualised"
2021.eacl-main.107,N18-2002,0,0.0704632,"Missing"
2021.eacl-main.107,D13-1170,0,0.0235175,"level, enabling us to debias at different granularities and on different layers in the pre-trained contextualised embedding model. Following prior work, we compare the proposed debiasing method in two sentence-level tasks: Sentence Encoder Association Test (SEAT; May et al., 2019) and Multi-genre co-reference-based Natural Language Inference (MNLI; Dev et al., 2020). Experimental results show that the proposed method not only debiases all contextualised word embedding models compared, but also preserves useful semantic information for solving downstream tasks such as sentiment classification (Socher et al., 2013), paraphrase detection (Dolan and Brockett, 2005), semantic textual similarity measurement (Cer et al., 2017), natural language inference (Dagan et al., 2005; Bar-Haim et al., 2006) and solving Winograd schema (Levesque et al., 2012). We consider gender bias as a running example throughout this paper and evaluate the proposed method with respect to its ability to overcome gender bias in contextualised word embeddings, and defer extensions to other types of biases to future work. 2 Related Work Prior work on debiasing word embeddings can be broadly categorised into two groups depending on wheth"
2021.eacl-main.107,P19-1161,0,0.0148767,"cratch is time consuming and require specialised hardware such as GPU/TPU clusters. On the other hand, fine-tuning a pre-trained contextualised embedding model for a particular task (possibly using labelled data for the target task) is relatively less expensive. Consequently, the standard practice in the NLP community has been to share1 pre-trained contextualised embedding models and fine-tune as needed. Therefore, it is desirable that a debiasing method proposed for contextualised embedding models can be applied as a fine-tuning method. In this view, counterfactual data augmentation methods (Zmigrod et al., 2019; Hall Maudslay et al., 2019; Zhao et al., 2019) that swap gender pronouns in the training corpus for creating a gender balanced version of the training data are less attractive when debiasing contextualised embeddings because we must retrain those models on the balanced corpora, which is more expensive compared to fine-tuning. Using gender-bias as a running example, we address the above-mentioned challenges by proposing a debiasing method that fine-tunes pre-trained contextualised word embeddings2 . Our proposed method retains the semantic information learnt by the contextualised embedding mo"
2021.eacl-main.107,W18-5446,0,0.0535237,"Missing"
2021.eacl-main.107,N18-1101,0,0.0358504,"Moreover, Li can be computed only for the target words in a sentence x as in (1), or can be summed up for all words in w ∈ x (i.e. 2 P P P > t∈Vt x∈Ω(t) w∈x v i (a) Ei (w, x; θ e ) ). We refer to the former as token-level debiasing and latter sentence-level debiasing. Collectively this gives us six different settings for the proposed debiasing method, which we evaluate experimentally in § 4.3. 4 4.1 Experiments Datasets We used SEAT (May et al., 2019) 6, 7 and 8 to evaluate gender bias. We use NLI as a downstream evaluation task and use the Multi-Genre Natural Language Inference data (MNLI; Williams et al., 2018) for training and development following Dev et al. (2020). In NLI, the task is to classify a given hypothesis and premise sentence-pair as entailing, contradicting, or neutral. We programmatically generated the evaluation set following Dev et al. (2020) by filling occupation words and gender words in template sentences. The templates take the form “The subject verb a/an object.” and the created sentence-pairs are assumed to be neutral. We used the word lists created by Zhao et al. (2018b) for the attribute list of feminine and masculine words. As for the stereotype word list for target words,"
2021.eacl-main.107,N19-1064,0,0.0655291,"Missing"
2021.eacl-main.107,N18-2003,0,0.433702,"tely however, it has been shown that, similar to their non-contextual counterparts, contextualised text embeddings also encode various types of unfair biases (Zhao et al., 2019; Bordia and Bowman, 2019; May et al., 2019; Tan and Celis, 2019; Bommasani et al., 2020; Kurita et al., 2019). This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and Bollegala, 2019; Zhao et al., 2018b; Bolukbasi et al., 2016; Ravfogel et al., 2020) and adversarial methods (Xie et al., 2017; Gonen and Goldberg, 2019). In contrast, despite multiple studies reporting that contextualised embeddings to be unfairly biased, methods for debiasing contextualised embeddings are relatively under explored (Dev et al., 2020;"
2021.eacl-main.107,D18-1521,0,0.370219,"tely however, it has been shown that, similar to their non-contextual counterparts, contextualised text embeddings also encode various types of unfair biases (Zhao et al., 2019; Bordia and Bowman, 2019; May et al., 2019; Tan and Celis, 2019; Bommasani et al., 2020; Kurita et al., 2019). This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings. Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar and Goldberg, 2018; Kaneko and Bollegala, 2019). As discussed later in § 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods (Kaneko and Bollegala, 2019; Zhao et al., 2018b; Bolukbasi et al., 2016; Ravfogel et al., 2020) and adversarial methods (Xie et al., 2017; Gonen and Goldberg, 2019). In contrast, despite multiple studies reporting that contextualised embeddings to be unfairly biased, methods for debiasing contextualised embeddings are relatively under explored (Dev et al., 2020;"
2021.eacl-main.133,Q16-1028,0,0.447175,"Yoshida3 and Ken-ichi Kawarabayashi3 1 University of Liverpool, Amazon. 2 Taif University, Saudi Arabia 3 National Institute of Informatics, Japan danushka@liverpool.ac.uk, hahakami@tu.edu.sa {yyoshida,k keniti}@nii.ac.jp Abstract Embedding entities and relations of a knowledge graph in a low-dimensional space has shown impressive performance in predicting missing links between entities. Although progresses have been achieved, existing methods are heuristically motivated and theoretical understanding of such embeddings is comparatively underdeveloped. This paper extends the random walk model (Arora et al., 2016a) of word embeddings to Knowledge Graph Embeddings (KGEs) to derive a scoring function that evaluates the strength of a relation R between two entities h (head) and t (tail). Moreover, we show that marginal loss minimisation, a popular objective used in much prior work in KGE, follows naturally from the loglikelihood ratio maximisation under the probabilities estimated from the KGEs according to our theoretical relationship. We propose a learning objective motivated by the theoretical analysis to learn KGEs from a given knowledge graph. Using the derived objective, accurate KGEs are learnt fr"
2021.eacl-main.133,P15-1067,0,0.0265199,"the orthogonality property as expected by the theoretical analysis. 2 Related Work At a high-level of abstraction, KGE methods can be seen as differing in their design choices for the following two main problems: (a) how to represent entities and relations, and (b) how to model the interaction between two entities and a relation that holds between them. Next, we briefly discuss prior proposals to those two problems (refer to Wang et al. (2017); Nguyen (2017); Nickel et al. (2015) for an extended survey on KGE). A popular choice for representing entities is to use vectors (Bordes et al., 2013; Ji et al., 2015; Yang et al., 2015), whereas relations have been represented by vectors, matrices (Bordes et al., 2011; Nguyen et al., 2016; Nickel et al., 2011) or tensors (Socher et al., 2013). ComplEx (Trouillon et al., 2016) introduced complex vectors for KGEs to capture the asymmetry in semantic relations. Ding et al. (2018) further improved CompIEx by imposing non-negativity and entailment constraints to ComplEx. Given entity and relation embeddings, a scoring function evaluates the strength of a triple (h, R, t). Scoring functions that encode various intuitions have been proposed such as the `1 or `2"
2021.eacl-main.133,P16-1219,0,0.0689188,"Missing"
2021.eacl-main.133,N16-1105,0,0.016521,"mplex vectors for KGEs to capture the asymmetry in semantic relations. Ding et al. (2018) further improved CompIEx by imposing non-negativity and entailment constraints to ComplEx. Given entity and relation embeddings, a scoring function evaluates the strength of a triple (h, R, t). Scoring functions that encode various intuitions have been proposed such as the `1 or `2 norms of the vector formed by a translation of the head entity embedding by the relation embedding over the target embedding, or by first performing a projection from the entity embedding space to the relation embedding space (Yoon et al., 2016). As an alternative to using vector norms as scoring functions, DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) use the component-wise multi-linear dot product. Lacroix et al. (2018) proposed the use of nuclear 3-norm regularisers instead of the popular Frobenius norm for canonical tensor decomposition. Table 1 shows the scoring functions along with algebraic structures for entities and relations proposed in selected prior work in KGE learning. Given a scoring function, KGEs are learnt that assign higher scores to relational triples in existing KGs over triples where the rela"
2021.eacl-main.133,2020.acl-main.238,0,0.0322456,"-like distributions for the parFigure 2: Results for the approximated relation embeddings for link prediction on WN18RR. tition functions for different relations for which smaller standard deviations indicate stronger concentration around the mean. Interestingly, from Table 3 we see a negative correlation between H@10 and the standard deviations indicating that the performance of RelWalk depends on the validity of the concentration assumption. 5.2 Compression of Embeddings To reduce the amount of memory required for KGEs, especially with a large KG, compressing KGEs has been studied recently (Sachan, 2020). RelWalk uses (orthogonal) matrices to represent relations, which require more parameters compared to a vector representation of the same dimensionality of a relation. Prior work studying lowerrank decomposition of KGEs have shown that, although linear embeddings of graphs can require prohibitively large dimensionality to model certain types of relations (Nickel et al., 2014) (e.g. sameAs), nonlinear embeddings can mitigate this 1558 problem (Bouchard et al., 2015). In this section, we propose memory-efficient low-rank approximations to the RelWalk embeddings. From the definition of orthogona"
2021.eacl-main.16,Q17-1010,0,0.288069,"be seen as using the dictionary as a guideline for what to retain during debiasing. We evaluate the proposed method using four standard benchmark datasets for evaluating the biases in word embeddings: Word Embedding Association Test (WEAT; Caliskan et al., 2017), Word Association Test (WAT; Du et al., 2019), SemBias (Zhao et al., 2018b) and WinoBias (Zhao et al., 2018a). Our experimental results show that the proposed debiasing method accurately removes unfair biases from three widely used pre-trained embeddings: Word2Vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). Moreover, our evaluations on semantic similarity and word analogy benchmarks show that the proposed debiasing method preserves useful semantic information in word embeddings, while removing unfair biases. 2 Related Work Dictionaries have been popularly used for learning word embeddings (Budanitsky and Hirst, 2006, 2001; Jiang and Conrath, 1997). Methods that use both dictionaries (or lexicons) and corpora to jointly learn word embeddings (Tissier et al., 2017; Alsuhaibani et al., 2019; Bollegala et al., 2016) or post-process (Glavaˇs and Vuli´c, 2018; Faruqui et al., 2015) have also been pro"
2021.eacl-main.16,N19-3002,0,0.0146204,"ormation with autoencoder (Kaneko and Bollegala, 2020), while removing discriminatory biases from stereotypical cases (GP-GloVe). However, all prior debiasing methods require us to pre-define the biases in the form of explicit word lists containing gender and stereotypical word associations. In contrast we use dictionaries as a source of bias-free semantic definitions of words and do not require pre-defining the biases to be removed. Although we focus on static word embeddings in this paper, unfair biases have been found in contextualised word embeddings as well (Zhao et al., 2019; Vig, 2019; Bordia and Bowman, 2019; May et al., 2019). Adversarial learning methods (Xie et al., 2017; Elazar and Goldberg, 2018; Li et al., 2018) for debiasing first encode the inputs and then two classifiers are jointly trained – one predicting the target task (for which we must ensure high prediction accuracy) and the other protected attributes (that must not be easily predictable). However, Elazar and Goldberg (2018) showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inputs can still manage to reach s"
2021.eacl-main.16,D18-1181,0,0.0165266,"icly available pre-trained word embeddings: Word2Vec3 (300-dimensional embeddings for ca. 3M words learned from Google News corpus (Mikolov et al., 2013a)), GloVe4 (300dimensional embeddings for ca. 2.1M words learned from the Common Crawl (Pennington et al., 2014)), and fastText5 (300-dimensional embeddings for ca. 1M words learned from Wikipedia 2017, UMBC webbase corpus and statmt.org news (Bojanowski et al., 2017)). As the dictionary definitions, we used the glosses in the WordNet (Fellbaum, 1998), which has been popularly used to learn word embeddings in prior work (Tissier et al., 2017; Bosc and Vincent, 2018; Washio et al., 2019). However, we note that our proposed method does not depend on any WordNetspecific features, thus in principle can be applied to any dictionary containing definition sentences. Words that do not appear in the vocabulary of the pre-trained embeddings are ignored when computing s(w) for the headwords w in the dictionary. Therefore, if all the words in a dictionary definition are ignored, then the we remove the corresponding headwords from training. Consequently, we are left with 54,528, 64,779 and 58,015 words respectively for Word2Vec, GloVe and fastText embeddings in the"
2021.eacl-main.16,P12-1015,0,0.0783523,"Missing"
2021.eacl-main.16,N15-1184,0,0.080141,"Missing"
2021.eacl-main.16,N19-1423,0,0.00918472,"expressed using the basis vectors as in (4). (1) Following our assumption that the dictionary definition, s(w), of w is a concise and unbiased description of the meaning of w, we would like to ensure that the encoded version of w is similar to s(w). We refer to this constraint as dictionary agreement. To formalise dictionary agreement empirically, we first represent s(w) by a sentence embedding vector s(w) ∈ Rn . Different sentence embedding methods can be used for this purpose such as convolutional neural networks (Kim, 2014), recurrent neural networks (Peters et al., 2018) or transformers (Devlin et al., 2019). For the simplicity, we use the smoothed inverse frequency (SIF; Arora et al., 2017) for creating s(w) in this paper. SIF computes the embedding of a sentence as the weighted average of the pre-trained word embeddings of the words in the sentence, where the weights are computed as the inverse unigram probability. Next, the first principal component vector of the sentence embeddings are removed. The dimensionality of the sentence embeddings created using SIF is equal to that of the pre-trained word embeddings used. Therefore, in our case we have both w, s(w) ∈ Rn . We decode the debiased embed"
2021.eacl-main.16,D19-1635,0,0.101427,"do not require pre-compiled word lists specifying the biases. The dictionary acts as a clean, unbiased source of word meaning that can be considered as positive examples of debiased meanings. In contrast to the existing debiasing methods that require us to predefine what to remove, the proposed method can be seen as using the dictionary as a guideline for what to retain during debiasing. We evaluate the proposed method using four standard benchmark datasets for evaluating the biases in word embeddings: Word Embedding Association Test (WEAT; Caliskan et al., 2017), Word Association Test (WAT; Du et al., 2019), SemBias (Zhao et al., 2018b) and WinoBias (Zhao et al., 2018a). Our experimental results show that the proposed debiasing method accurately removes unfair biases from three widely used pre-trained embeddings: Word2Vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). Moreover, our evaluations on semantic similarity and word analogy benchmarks show that the proposed debiasing method preserves useful semantic information in word embeddings, while removing unfair biases. 2 Related Work Dictionaries have been popularly used for learning word embeddi"
2021.eacl-main.16,D18-1002,0,0.0209792,"from stereotypical cases (GP-GloVe). However, all prior debiasing methods require us to pre-define the biases in the form of explicit word lists containing gender and stereotypical word associations. In contrast we use dictionaries as a source of bias-free semantic definitions of words and do not require pre-defining the biases to be removed. Although we focus on static word embeddings in this paper, unfair biases have been found in contextualised word embeddings as well (Zhao et al., 2019; Vig, 2019; Bordia and Bowman, 2019; May et al., 2019). Adversarial learning methods (Xie et al., 2017; Elazar and Goldberg, 2018; Li et al., 2018) for debiasing first encode the inputs and then two classifiers are jointly trained – one predicting the target task (for which we must ensure high prediction accuracy) and the other protected attributes (that must not be easily predictable). However, Elazar and Goldberg (2018) showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inputs can still manage to reach substantially high accuracies for the protected attributes. They conclude that adversarial lear"
2021.eacl-main.16,P19-1166,0,0.0177575,"g training, a post-hoc classifier trained on the encoded inputs can still manage to reach substantially high accuracies for the protected attributes. They conclude that adversarial learning alone does not guarantee invariant representations for the protected attributes. Ravfogel et al. (2020) found that iteratively projecting word embeddings to the null space of the gender direction to further improve the debiasing performance. To evaluate biases, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT) inspired by the Implicit Association Test (IAT; Greenwald et al., 1998). Ethayarajh et al. (2019) showed that WEAT to be systematically overestimating biases and proposed a correction. The ability to correctly answer gender-related word analogies (Zhao et al., 2018b) and resolve genderrelated coreferences (Zhao et al., 2018a; Rudinger et al., 2018) have been used as extrinsic tasks for evaluating the bias in word embeddings. We describe these evaluation benchmarks later in § 4.3. 3 Dictionary-based Debiasing Let us denote the n-dimensional pre-trained word embedding of a word w by w ∈ Rn trained on some resource C such as a text corpus. Moreover, let us assume that we are given a dictiona"
2021.eacl-main.16,P18-1004,0,0.0466157,"Missing"
2021.eacl-main.16,W19-3621,0,0.0173314,"sts of male and female pronouns for defining the gender direction. However, gender bias is only one of the many biases that exist in pre-trained word embeddings. It is inconvenient to prepare lists of words covering all different types of biases we must remove from pretrained word embeddings. Moreover, such pre212 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 212–223 April 19 - 23, 2021. ©2021 Association for Computational Linguistics compiled word lists are likely to be incomplete and inadequately cover some biases. Indeed, Gonen and Goldberg (2019) showed empirical evidence that such debiasing methods do not remove all discriminative biases from word embeddings. Unfair biases have adversely affected several NLP tasks such as machine translation (Vanmassenhove et al., 2018) and language generation (Sheng et al., 2019). Racial biases have also been shown to affect criminal prosecutions (Manzini et al., 2019) and career adverts (Lambrecht and Tucker, 2016). These findings show the difficulty of defining different biases using pre-compiled lists of words, which is a requirement in previously proposed debiasing methods for static word embedd"
2021.eacl-main.16,C18-1211,1,0.855566,"od removes unfair biases, while retaining (and sometimes further improving) the semantic information contained in the original word embeddings. We also see that for GloVe embeddings the performance has improved after debiasing whereas for Word2Vec and fastText embeddings the opposite is true. Similar drop in performance in word analogy tasks have been reported in prior work (Zhao et al., 2018b). Besides CosAdd there are multiple alternative methods proposed for solving analogies using pre-trained word embeddings such as CosMult, PairDiff and supervised operators (Bollegala et al., 2015, 2014; Hakami et al., 2018). Moreover, there have been concerns raised about the protocols used in prior work evaluating word embeddings on word analogy tasks and the correlation with downstream tasks (Schluter, 2018). Therefore, we defer further investigation in this behaviour to future work. 5.5 Visualising the Outcome of Debiasing We analyse the effect of debiasing by calculating the cosine similarity between neutral occupational #» # » # » words and gender (he − she), race (Caucasoid − # » # » # » N egroid) and age (elder − youth) directions. The neutral occupational words list is based on Bolukbasi et al. (2016) an"
2021.eacl-main.16,D19-1530,0,0.0156997,"subspace spanned by the basis vectors in the pre-trained word embedding space that corresponds to discriminatory biases (bias orthogonality). We implement the semantic preservation and dictionary agreement using two decoders, whereas the bias orthogonality is enforced by a parameter-free projection. The debiasing encoder and the decoders are learnt endto-end by a joint optimisation method. Our proposed method is agnostic to the details of the algorithms used to learn the input word embeddings. Moreover, unlike counterfactual data augmentation methods for debiasing (Zmigrod et al., 2019; Hall Maudslay et al., 2019), we do not require access to the original training resources used for learning the input word embeddings. Our proposed method overcomes the abovedescribed challenges as follows. First, instead of learning a lexicalised debiasing model, we operate on the word embedding space when learning the encoder. Therefore, we can use the words that are in the intersection of the vocabularies of the pre-trained word embeddings and the dictionary to learn the encoder, enabling us to generalise to the 1 Code and debiased embeddings: https://github. com/kanekomasahiro/dict-debias words not in the dictionary."
2021.eacl-main.16,J15-4004,0,0.0285931,"formation than necessary from the original word embeddings, performance will drop when those debiased embeddings are used in NLP applications. Therefore, to evaluate the semantic information preserved after debiasing, we use semantic similarity and word analogy benchmarks as described next. Semantic Similarity: The semantic similarity between two words is calculated as the cosine similarity between their word embeddings and compared against the human ratings using the Spearman correlation coefficient. The following datasets are used: Word Similarity 353 (WS; Finkelstein et al., 2001), SimLex (Hill et al., 2015), Rubenstein-Goodenough (RG; Rubenstein and Goodenough, 1965), MTurk (Halawi et al., Word Analogy: In word analogy, we predict d that completes the proportional analogy “a is to b as c is to what?”, for four words a, b, c and d. We use CosAdd (Levy and Goldberg, 2014), which determines d by maximising the cosine similarity between the two vectors (b − a + c) and d. Following Zhao et al. (2018b), we evaluate on MSR (Mikolov et al., 2013c) and Google analogy datasets (Mikolov et al., 2013a) as shown in Table 6. From Table 6 we see that for all word embeddings, debiased using the proposed method"
2021.eacl-main.16,O97-1002,0,0.283621,"Bias (Zhao et al., 2018a). Our experimental results show that the proposed debiasing method accurately removes unfair biases from three widely used pre-trained embeddings: Word2Vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). Moreover, our evaluations on semantic similarity and word analogy benchmarks show that the proposed debiasing method preserves useful semantic information in word embeddings, while removing unfair biases. 2 Related Work Dictionaries have been popularly used for learning word embeddings (Budanitsky and Hirst, 2006, 2001; Jiang and Conrath, 1997). Methods that use both dictionaries (or lexicons) and corpora to jointly learn word embeddings (Tissier et al., 2017; Alsuhaibani et al., 2019; Bollegala et al., 2016) or post-process (Glavaˇs and Vuli´c, 2018; Faruqui et al., 2015) have also been proposed. However, learning embeddings from dictionaries alone results in coverage and data sparseness issues (Bollegala et al., 2016) and does not guarantee bias-free embeddings (Lauscher and Glavas, 2019). To the best of our knowledge, we are the first to use dictionaries for debiasing pre-trained word embeddings. Bolukbasi et al. (2016) proposed"
2021.eacl-main.16,P19-1160,1,0.876676,"Missing"
2021.eacl-main.16,2020.coling-main.149,1,0.695363,"as gender-definitional by the classifier. Therefore, if the classifier erroneously predicts a stereotypical word as a genderdefinitional word, it would not get debiased. Zhao et al. (2018b) modified the GloVe (Pennington et al., 2014) objective to learn gender-neutral word embeddings (GN-GloVe) from a given corpus. They maximise the squared `2 distance between gender-related sub-vectors, while simultaneously minimising the GloVe objective. Unlike, the above-mentioned methods, Kaneko and Bollegala (2019) proposed a post-processing method to preserve gender-related information with autoencoder (Kaneko and Bollegala, 2020), while removing discriminatory biases from stereotypical cases (GP-GloVe). However, all prior debiasing methods require us to pre-define the biases in the form of explicit word lists containing gender and stereotypical word associations. In contrast we use dictionaries as a source of bias-free semantic definitions of words and do not require pre-defining the biases to be removed. Although we focus on static word embeddings in this paper, unfair biases have been found in contextualised word embeddings as well (Zhao et al., 2019; Vig, 2019; Bordia and Bowman, 2019; May et al., 2019). Adversaria"
2021.eacl-main.16,D14-1181,0,0.00419932,". . . , bk to be B ⊆ Rn . The projection v B of a vector v ∈ Rn onto B can be expressed using the basis vectors as in (4). (1) Following our assumption that the dictionary definition, s(w), of w is a concise and unbiased description of the meaning of w, we would like to ensure that the encoded version of w is similar to s(w). We refer to this constraint as dictionary agreement. To formalise dictionary agreement empirically, we first represent s(w) by a sentence embedding vector s(w) ∈ Rn . Different sentence embedding methods can be used for this purpose such as convolutional neural networks (Kim, 2014), recurrent neural networks (Peters et al., 2018) or transformers (Devlin et al., 2019). For the simplicity, we use the smoothed inverse frequency (SIF; Arora et al., 2017) for creating s(w) in this paper. SIF computes the embedding of a sentence as the weighted average of the pre-trained word embeddings of the words in the sentence, where the weights are computed as the inverse unigram probability. Next, the first principal component vector of the sentence embeddings are removed. The dimensionality of the sentence embeddings created using SIF is equal to that of the pre-trained word embedding"
2021.eacl-main.16,S19-1010,0,0.0267216,"while removing unfair biases. 2 Related Work Dictionaries have been popularly used for learning word embeddings (Budanitsky and Hirst, 2006, 2001; Jiang and Conrath, 1997). Methods that use both dictionaries (or lexicons) and corpora to jointly learn word embeddings (Tissier et al., 2017; Alsuhaibani et al., 2019; Bollegala et al., 2016) or post-process (Glavaˇs and Vuli´c, 2018; Faruqui et al., 2015) have also been proposed. However, learning embeddings from dictionaries alone results in coverage and data sparseness issues (Bollegala et al., 2016) and does not guarantee bias-free embeddings (Lauscher and Glavas, 2019). To the best of our knowledge, we are the first to use dictionaries for debiasing pre-trained word embeddings. Bolukbasi et al. (2016) proposed a postprocessing approach that projects gender-neutral words into a subspace, which is orthogonal to the gender dimension defined by a list of genderdefinitional words. They refer to words associated with gender (e.g., she, actor) as gender-definitional words, and the remainder gender-neutral. They proposed a hard-debiasing method where the gender direction is computed as the vector difference between the embeddings of the correspond213 ing gender-def"
2021.eacl-main.16,D17-1018,0,0.0137742,"f cosine similarities between masculine and feminine words. |L|  1 X i f (w, wm ) − f (w, wfi ) |L| However, in Type 2, these tests can be resolved using syntactic information and understanding of the pronoun. It involves two conditions: the pro-stereotyped (pro) condition links pronouns to occupations dominated by the gender of the pronoun, and the anti-stereotyped (anti) condition links pronouns to occupations not dominated by the gender of the pronoun. For a correctly debiased set of word embeddings, the difference between pro and anti is expected to be small. We use the model proposed by Lee et al. (2017) and implemented in AllenNLP (Gardner et al., 2017) as the coreference resolution method. We used a bias comparing code6 to evaluate WEAT dataset. Since the WAT code was not published, we contacted the authors to obtain the code and used it for evaluation. We used the evaluation code from GP-GloVe7 to evaluate SemBias dataset. We used AllenNLP8 to evaluate WinoBias and OntoNotes datasets. We used evaluate word pairs function and evaluate word analogies in gensim9 to evaluate word embedding benchmarks. (11) 5 Results i=1 5.1 SemBias: SemBias dataset (Zhao et al., 2018b) contains three types of"
2021.eacl-main.16,W14-1618,0,0.041819,"benchmarks as described next. Semantic Similarity: The semantic similarity between two words is calculated as the cosine similarity between their word embeddings and compared against the human ratings using the Spearman correlation coefficient. The following datasets are used: Word Similarity 353 (WS; Finkelstein et al., 2001), SimLex (Hill et al., 2015), Rubenstein-Goodenough (RG; Rubenstein and Goodenough, 1965), MTurk (Halawi et al., Word Analogy: In word analogy, we predict d that completes the proportional analogy “a is to b as c is to what?”, for four words a, b, c and d. We use CosAdd (Levy and Goldberg, 2014), which determines d by maximising the cosine similarity between the two vectors (b − a + c) and d. Following Zhao et al. (2018b), we evaluate on MSR (Mikolov et al., 2013c) and Google analogy datasets (Mikolov et al., 2013a) as shown in Table 6. From Table 6 we see that for all word embeddings, debiased using the proposed method accurately preserves the semantic information in the original embeddings. In fact, except for Word2Vec embeddings on WS dataset, we see that the accuracy of the embeddings have improved after the debiasing process, which is a desirable side-effect. We believe this is"
2021.eacl-main.16,P18-2005,0,0.0215314,"GP-GloVe). However, all prior debiasing methods require us to pre-define the biases in the form of explicit word lists containing gender and stereotypical word associations. In contrast we use dictionaries as a source of bias-free semantic definitions of words and do not require pre-defining the biases to be removed. Although we focus on static word embeddings in this paper, unfair biases have been found in contextualised word embeddings as well (Zhao et al., 2019; Vig, 2019; Bordia and Bowman, 2019; May et al., 2019). Adversarial learning methods (Xie et al., 2017; Elazar and Goldberg, 2018; Li et al., 2018) for debiasing first encode the inputs and then two classifiers are jointly trained – one predicting the target task (for which we must ensure high prediction accuracy) and the other protected attributes (that must not be easily predictable). However, Elazar and Goldberg (2018) showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inputs can still manage to reach substantially high accuracies for the protected attributes. They conclude that adversarial learning alone does no"
2021.eacl-main.16,W13-3512,0,0.146631,"Missing"
2021.eacl-main.16,N19-1062,0,0.0180932,"apter of the Association for Computational Linguistics, pages 212–223 April 19 - 23, 2021. ©2021 Association for Computational Linguistics compiled word lists are likely to be incomplete and inadequately cover some biases. Indeed, Gonen and Goldberg (2019) showed empirical evidence that such debiasing methods do not remove all discriminative biases from word embeddings. Unfair biases have adversely affected several NLP tasks such as machine translation (Vanmassenhove et al., 2018) and language generation (Sheng et al., 2019). Racial biases have also been shown to affect criminal prosecutions (Manzini et al., 2019) and career adverts (Lambrecht and Tucker, 2016). These findings show the difficulty of defining different biases using pre-compiled lists of words, which is a requirement in previously proposed debiasing methods for static word embeddings. We propose a method that uses a dictionary as a source of bias-free definitions of words for debiasing pre-trained word embeddings1 . Specifically, we learn an encoder that filters-out biases from the input embeddings. The debiased embeddings are required to simultaneously satisfy three criteria: (a) must preserve all non-discriminatory information in the p"
2021.eacl-main.16,N19-1063,0,0.0220583,"(Kaneko and Bollegala, 2020), while removing discriminatory biases from stereotypical cases (GP-GloVe). However, all prior debiasing methods require us to pre-define the biases in the form of explicit word lists containing gender and stereotypical word associations. In contrast we use dictionaries as a source of bias-free semantic definitions of words and do not require pre-defining the biases to be removed. Although we focus on static word embeddings in this paper, unfair biases have been found in contextualised word embeddings as well (Zhao et al., 2019; Vig, 2019; Bordia and Bowman, 2019; May et al., 2019). Adversarial learning methods (Xie et al., 2017; Elazar and Goldberg, 2018; Li et al., 2018) for debiasing first encode the inputs and then two classifiers are jointly trained – one predicting the target task (for which we must ensure high prediction accuracy) and the other protected attributes (that must not be easily predictable). However, Elazar and Goldberg (2018) showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inputs can still manage to reach substantially high a"
2021.eacl-main.16,N13-1090,0,0.446422,"s that require us to predefine what to remove, the proposed method can be seen as using the dictionary as a guideline for what to retain during debiasing. We evaluate the proposed method using four standard benchmark datasets for evaluating the biases in word embeddings: Word Embedding Association Test (WEAT; Caliskan et al., 2017), Word Association Test (WAT; Du et al., 2019), SemBias (Zhao et al., 2018b) and WinoBias (Zhao et al., 2018a). Our experimental results show that the proposed debiasing method accurately removes unfair biases from three widely used pre-trained embeddings: Word2Vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). Moreover, our evaluations on semantic similarity and word analogy benchmarks show that the proposed debiasing method preserves useful semantic information in word embeddings, while removing unfair biases. 2 Related Work Dictionaries have been popularly used for learning word embeddings (Budanitsky and Hirst, 2006, 2001; Jiang and Conrath, 1997). Methods that use both dictionaries (or lexicons) and corpora to jointly learn word embeddings (Tissier et al., 2017; Alsuhaibani et al., 2019; Bollegala et al., 2016) or post-p"
2021.eacl-main.16,D14-1162,0,0.0850831,"Missing"
2021.eacl-main.16,N18-1202,0,0.0324062,"n v B of a vector v ∈ Rn onto B can be expressed using the basis vectors as in (4). (1) Following our assumption that the dictionary definition, s(w), of w is a concise and unbiased description of the meaning of w, we would like to ensure that the encoded version of w is similar to s(w). We refer to this constraint as dictionary agreement. To formalise dictionary agreement empirically, we first represent s(w) by a sentence embedding vector s(w) ∈ Rn . Different sentence embedding methods can be used for this purpose such as convolutional neural networks (Kim, 2014), recurrent neural networks (Peters et al., 2018) or transformers (Devlin et al., 2019). For the simplicity, we use the smoothed inverse frequency (SIF; Arora et al., 2017) for creating s(w) in this paper. SIF computes the embedding of a sentence as the weighted average of the pre-trained word embeddings of the words in the sentence, where the weights are computed as the inverse unigram probability. Next, the first principal component vector of the sentence embeddings are removed. The dimensionality of the sentence embeddings created using SIF is equal to that of the pre-trained word embeddings used. Therefore, in our case we have both w, s("
2021.eacl-main.16,2020.acl-main.647,0,0.0192217,"jointly trained – one predicting the target task (for which we must ensure high prediction accuracy) and the other protected attributes (that must not be easily predictable). However, Elazar and Goldberg (2018) showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inputs can still manage to reach substantially high accuracies for the protected attributes. They conclude that adversarial learning alone does not guarantee invariant representations for the protected attributes. Ravfogel et al. (2020) found that iteratively projecting word embeddings to the null space of the gender direction to further improve the debiasing performance. To evaluate biases, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT) inspired by the Implicit Association Test (IAT; Greenwald et al., 1998). Ethayarajh et al. (2019) showed that WEAT to be systematically overestimating biases and proposed a correction. The ability to correctly answer gender-related word analogies (Zhao et al., 2018b) and resolve genderrelated coreferences (Zhao et al., 2018a; Rudinger et al., 2018) have been used"
2021.eacl-main.16,N18-2002,0,0.0458246,"Missing"
2021.eacl-main.16,N18-2039,0,0.0165774,"ce has improved after debiasing whereas for Word2Vec and fastText embeddings the opposite is true. Similar drop in performance in word analogy tasks have been reported in prior work (Zhao et al., 2018b). Besides CosAdd there are multiple alternative methods proposed for solving analogies using pre-trained word embeddings such as CosMult, PairDiff and supervised operators (Bollegala et al., 2015, 2014; Hakami et al., 2018). Moreover, there have been concerns raised about the protocols used in prior work evaluating word embeddings on word analogy tasks and the correlation with downstream tasks (Schluter, 2018). Therefore, we defer further investigation in this behaviour to future work. 5.5 Visualising the Outcome of Debiasing We analyse the effect of debiasing by calculating the cosine similarity between neutral occupational #» # » # » words and gender (he − she), race (Caucasoid − # » # » # » N egroid) and age (elder − youth) directions. The neutral occupational words list is based on Bolukbasi et al. (2016) and is listed in the Supplementary. Figure 1 shows the visualisation result for Word2Vec. We see that original Word2Vec shows some gender words are especially away from the 219 (a) Original Wo"
2021.eacl-main.16,D19-1339,0,0.0211949,"d embeddings. Moreover, such pre212 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 212–223 April 19 - 23, 2021. ©2021 Association for Computational Linguistics compiled word lists are likely to be incomplete and inadequately cover some biases. Indeed, Gonen and Goldberg (2019) showed empirical evidence that such debiasing methods do not remove all discriminative biases from word embeddings. Unfair biases have adversely affected several NLP tasks such as machine translation (Vanmassenhove et al., 2018) and language generation (Sheng et al., 2019). Racial biases have also been shown to affect criminal prosecutions (Manzini et al., 2019) and career adverts (Lambrecht and Tucker, 2016). These findings show the difficulty of defining different biases using pre-compiled lists of words, which is a requirement in previously proposed debiasing methods for static word embeddings. We propose a method that uses a dictionary as a source of bias-free definitions of words for debiasing pre-trained word embeddings1 . Specifically, we learn an encoder that filters-out biases from the input embeddings. The debiased embeddings are required to simultane"
2021.eacl-main.16,D17-1024,0,0.263175,"racial or religious biases (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. and Goldberg, 2018; Kaneko and Bollegala, 2019). On the other hand, human-written dictionaries act as an impartial, objective and unbiased source of word meaning. Although methods that learn word embeddings by purely using dictionaries have been proposed (Tissier et al., 2017), they have coverage and data sparseness related issues because precompiled dictionaries do not capture the meanings of neologisms or provide numerous contexts as in a corpus. Consequently, prior work has shown that word embeddings learnt from large text corpora to outperform those created from dictionaries in downstream NLP tasks (Alsuhaibani et al., 2019; Bollegala et al., 2016). We must overcome several challenges when using dictionaries to debias pre-trained word embeddings. First, not all words in the embeddings will appear in the given dictionary. Dictionaries often have limited coverage"
2021.eacl-main.16,D18-1334,0,0.0190536,"nt types of biases we must remove from pretrained word embeddings. Moreover, such pre212 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 212–223 April 19 - 23, 2021. ©2021 Association for Computational Linguistics compiled word lists are likely to be incomplete and inadequately cover some biases. Indeed, Gonen and Goldberg (2019) showed empirical evidence that such debiasing methods do not remove all discriminative biases from word embeddings. Unfair biases have adversely affected several NLP tasks such as machine translation (Vanmassenhove et al., 2018) and language generation (Sheng et al., 2019). Racial biases have also been shown to affect criminal prosecutions (Manzini et al., 2019) and career adverts (Lambrecht and Tucker, 2016). These findings show the difficulty of defining different biases using pre-compiled lists of words, which is a requirement in previously proposed debiasing methods for static word embeddings. We propose a method that uses a dictionary as a source of bias-free definitions of words for debiasing pre-trained word embeddings1 . Specifically, we learn an encoder that filters-out biases from the input embeddings. The"
2021.eacl-main.16,D19-1357,0,0.0179958,"ed word embeddings: Word2Vec3 (300-dimensional embeddings for ca. 3M words learned from Google News corpus (Mikolov et al., 2013a)), GloVe4 (300dimensional embeddings for ca. 2.1M words learned from the Common Crawl (Pennington et al., 2014)), and fastText5 (300-dimensional embeddings for ca. 1M words learned from Wikipedia 2017, UMBC webbase corpus and statmt.org news (Bojanowski et al., 2017)). As the dictionary definitions, we used the glosses in the WordNet (Fellbaum, 1998), which has been popularly used to learn word embeddings in prior work (Tissier et al., 2017; Bosc and Vincent, 2018; Washio et al., 2019). However, we note that our proposed method does not depend on any WordNetspecific features, thus in principle can be applied to any dictionary containing definition sentences. Words that do not appear in the vocabulary of the pre-trained embeddings are ignored when computing s(w) for the headwords w in the dictionary. Therefore, if all the words in a dictionary definition are ignored, then the we remove the corresponding headwords from training. Consequently, we are left with 54,528, 64,779 and 58,015 words respectively for Word2Vec, GloVe and fastText embeddings in the training dataset. We r"
2021.eacl-main.16,N19-1064,0,0.0273476,"Missing"
2021.eacl-main.16,D17-1323,0,0.0164325,"in the embeddings will appear in the given dictionary. Dictionaries often have limited coverage and will not cover neologisms, orthographic variants of words etc. that are likely to appear in large corpora. A lexicalised debiasing method would generalise poorly to the words not in the dictionary. Second, it is not known apriori what biases are hidden inside a set of pretrained word embedding vectors. Depending on the source of documents used for training the embeddings, different types of biases will be learnt and amplified by different word embedding learning algorithms to different degrees (Zhao et al., 2017). Prior work on debiasing required that the biases to be pre-defined (Kaneko and Bollegala, 2019). For example, Hard-Debias (HD; Bolukbasi et al., 2016) and Gender Neutral Glove (GN-GloVe; Zhao et al., 2018b) require lists of male and female pronouns for defining the gender direction. However, gender bias is only one of the many biases that exist in pre-trained word embeddings. It is inconvenient to prepare lists of words covering all different types of biases we must remove from pretrained word embeddings. Moreover, such pre212 Proceedings of the 16th Conference of the European Chapter of the"
2021.eacl-main.16,N18-2003,0,0.0741935,"remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics. 1 Introduction Despite pre-trained word embeddings are useful due to their low dimensionality, memory and compute efficiency, they have shown to encode not only the semantics of words but also unfair discriminatory biases such as gender, racial or religious biases (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. and Goldberg, 2018; Kaneko and Bollegala, 2019). On the other hand, human-written dictionaries act as an impartial, objective and unbiased source of word meaning. Although methods that learn word embeddings by purely using dictionaries have been proposed (Tissier et al., 2017), they have coverage and data sparseness related"
2021.eacl-main.16,D18-1521,0,0.115942,"remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics. 1 Introduction Despite pre-trained word embeddings are useful due to their low dimensionality, memory and compute efficiency, they have shown to encode not only the semantics of words but also unfair discriminatory biases such as gender, racial or religious biases (Bolukbasi et al., 2016; Zhao et al., 2018a; Rudinger et al., 2018; Zhao et al., 2018b; Elazar ∗ Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. and Goldberg, 2018; Kaneko and Bollegala, 2019). On the other hand, human-written dictionaries act as an impartial, objective and unbiased source of word meaning. Although methods that learn word embeddings by purely using dictionaries have been proposed (Tissier et al., 2017), they have coverage and data sparseness related"
2021.eacl-main.16,P19-1161,0,0.01698,") must be orthogonal to the subspace spanned by the basis vectors in the pre-trained word embedding space that corresponds to discriminatory biases (bias orthogonality). We implement the semantic preservation and dictionary agreement using two decoders, whereas the bias orthogonality is enforced by a parameter-free projection. The debiasing encoder and the decoders are learnt endto-end by a joint optimisation method. Our proposed method is agnostic to the details of the algorithms used to learn the input word embeddings. Moreover, unlike counterfactual data augmentation methods for debiasing (Zmigrod et al., 2019; Hall Maudslay et al., 2019), we do not require access to the original training resources used for learning the input word embeddings. Our proposed method overcomes the abovedescribed challenges as follows. First, instead of learning a lexicalised debiasing model, we operate on the word embedding space when learning the encoder. Therefore, we can use the words that are in the intersection of the vocabularies of the pre-trained word embeddings and the dictionary to learn the encoder, enabling us to generalise to the 1 Code and debiased embeddings: https://github. com/kanekomasahiro/dict-debias"
2021.emnlp-main.568,C18-1140,1,0.895418,"Missing"
2021.emnlp-main.568,2020.lrec-1.475,1,0.814223,"-1280 https://huggingface.co/xlm-roberta-base XLM-RoBERTa) perform the best across all datsets except for JP . For JP the best performance is obtained by an SVM model with BoN features. This could indicate that for Japanese, a language-specific tokenisation works for the lexicalised (BoN) models better than the languageindependent subtokenisation methods such as Byte Pair Encoding (BPE; Sennrich et al., 2016) that are used when training contextualised transformerbased sentence encoders. The former preserves more information than the latter at the expense of a sparser and larger feature space (Bollegala et al., 2020). Transformer-based masked language models on the other hand require subtokenisation as they must use a smaller vocabulary to make the token prediction task efficient (Yang et al., 2018; Li et al., 2019). In general, unlike the simpler word embedding and bag of words approaches, large pretrained contextualized embeddings maintain high test performance according to the reported evaluation metrics. We note that these also converged after a few epochs using a relatively small number of labelled instances, based on the model with the best 5-fold validation accuracy. Hence, contextualized embedding"
2021.emnlp-main.568,2020.acl-main.431,0,0.0119064,"view dataset. In particular, we do not collect or release any additional product reviews as part of this paper. Moreover, we have manually verified that the sentences in our dataset do not contain any customer sensitive information. However, product reviews do often contain subjective opinions, which can sometimes be socially biased. We do not filter out any such biases. We use two pretrained sentence encoders, mBERT and XLM-RoBERTa, when training the CFD models. It has been reported that pretrained masked language model encode unfair social biases such as gender, racial and religious biases (Bommasani et al., 2020). Although we have evaluated ourselves the mBERT and XLM-RoBERTa based CFD models that we use in our experiments, we suspect any social biases encoded in these pretrained masked language models could propagate into the CFD models that we train. In particular, these social biases could be further amplified during the CFD model training process, if the counterfactual statements in the training data also contain such biases. Debiasing masked language models is an active research field (Kaneko and Bollegala, 2021) and we plan to evaluate the social biases in CFD models in our future work. Referenc"
2021.emnlp-main.568,D14-1162,0,0.0854569,"Missing"
2021.emnlp-main.568,P16-1162,0,0.0052207,"cate that in both cases the classifier relies on the clue words. Overall transformer-based models (especially 11 10 Mask 12 https://fasttext.cc/docs/en/crawl-vectors.html 7099 https://huggingface.co/xlm-mlm-100-1280 https://huggingface.co/xlm-roberta-base XLM-RoBERTa) perform the best across all datsets except for JP . For JP the best performance is obtained by an SVM model with BoN features. This could indicate that for Japanese, a language-specific tokenisation works for the lexicalised (BoN) models better than the languageindependent subtokenisation methods such as Byte Pair Encoding (BPE; Sennrich et al., 2016) that are used when training contextualised transformerbased sentence encoders. The former preserves more information than the latter at the expense of a sparser and larger feature space (Bollegala et al., 2020). Transformer-based masked language models on the other hand require subtokenisation as they must use a smaller vocabulary to make the token prediction task efficient (Yang et al., 2018; Li et al., 2019). In general, unlike the simpler word embedding and bag of words approaches, large pretrained contextualized embeddings maintain high test performance according to the reported evaluatio"
2021.emnlp-main.568,P17-2103,0,0.336951,"cause all the tokens in the query (i.e. iPhone, with, warranty) 1 Introduction occur in the review sentence. Detecting counterCounterfactual statements are an essential tool of factuals can also be a precursor to capturing causal human thinking and are often found in natural lan- inferences (Wood-Doughty et al., 2018) and interguages. Counterfactual statements may be identi- actions, which have shown to be effective in fields fied as statements of the form – If p was true, then such as health sciences (H¨ofler, 2005). Janocko q would be true (i.e. assertions whose antecedent et al. (2016) and Son et al. (2017) studied CFD in (p) and consequent (q) are known or assumed to social media for automatic psychological assessbe false) (Milmed, 1957). In other words, a coun- ment of large populations. terfactual statement describes an event that may CFD is often modelled as a binary classificanot, did not, or cannot take place, and the subse- tion task (Son et al., 2017; Yang et al., 2020a). A quent consequence(s) or alternative(s) did not take manually annotated sentence-level counterfactual place. For example, consider the counterfactual dataset was introduced in SemEval-2020 (Yang statement – I would hav"
2021.emnlp-main.568,D18-1488,0,0.0606581,"Missing"
2021.emnlp-main.686,2020.clinicalnlp-1.26,0,0.0998718,"Missing"
2021.emnlp-main.686,J07-1005,0,0.0750563,"Missing"
2021.emnlp-main.686,N19-1423,0,0.0146706,"ing the label for each word in OSD and another for predicting the outcome type in OC. LCAM is designed to jointly learn contextualised label attention-based distributions at word- and sentence-levels in order to capture which label/s a word or a sentence is more semantically related to. We call them contextualised because they are enriched by global contextual representations of the abstracts to which the sentences belongs. Label attention incorporates label sparsity information and hence semantic correlation between documents and labels. A baseline BiLSTM and or clinically informed BERTbase (Devlin et al., 2019) models are used at the encoding stage of our model and later for decoding with sigmoid prediction layers. We also use a multi-label prediction (MLP) layer for the two tasks (i.e. OSD and OC), with a relaxed constraint at token-level that ensures only the top (most relevant) prediction is retained, whereas all predicted (relevant) outcome types are retained at the sentence-level during OC. We use an MLP layer because some annotated outcomes belong to multiple outcome types. For example, depression belongs to both “Physiological” and “Life-Impact” outcome types. HOD remains a challenging task d"
2021.emnlp-main.686,2020.aacl-main.11,0,0.0387006,".0 81.4 73.4 78.0 73.0 78.4 78.1 76.0 59.0 74.6 66.5 65.0 64.0 83.0 75.7 72.0 74.0 77.5 77.2 83.0 61.0 69.9 65.3 66.0 65.0 82.2 74.5 75.0 Table 4: Outcome span detection (OSD) and Outcome classification (OC) results in terms of F1 on the three datasets. Baseline, is a LCAM architecture with a BiLSTM sequence encoder. dropout of 0.1, 10 epochs, hidden state dimension for the BiLSTM and BioBERT encoders was set to 300 and 768 respectively. For the BioBERT model, we used features from BioBERT’s ultimate layer, a practice that has been endorsed in the past (Naseem et al., 2020; Yoon et al., 2019; Hao et al., 2020). We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001. Experiments were performed using a Titan RTX 24GB GPU. 5.2 Setup predictions that is present in the ground truth) and Normalized Discounted Cumulated Gain at top n (nDCG@n). 5.3 Results The first set of results we report in Table 4 are based on the independent test sets (Table 2) for each of the datasets. The joint LCAM-BioBERT and standalone BioBERT models are not only competitive but they consistently outperform the baseline model for both OSD and OC tasks. We observe the LCAM-BioBERT model outperform the other"
2021.emnlp-main.686,2020.emnlp-demos.6,0,0.0346701,"Missing"
2021.emnlp-main.686,D19-1044,0,0.018896,"RT model, we focus on the OC task results alone where the EBM-COMET classifier returns the outcome types given an out+EBM-NLP come span, and compare MLP performance to the baseline and another related MLP model, labelTable 6: OSD and OC performance percentage decline specific attention network (LSAN) (Xiao et al., when either the attention mechanism or the abstract 2019), that learns biLSTM representations for representation are eliminated from the joint learning multi-label classification of sentences. For commodel (LCAM-BioBERT). parison, we compute P@n and nDCG@n using formulas similar to (Xiao et al., 2019). As illustrated a token or an outcome span embedded into an ab- in Figure 2, the LCAM model outperforms its counstract representation. This therefore justifies inclu- terparts for all datasets, and most notably for P@1. sion of both these components. Our joint BiLSTM baseline model performs comTo evaluate the proposed label alignment method parably with LSAN, and indeed outperforms it on (subsection 3.1), we train a model using the aligned the EBM-COMET dataset for P@1, nDCG@1 and dataset (EBM-COMET+EBM-NLP) and evaluate nDCG@3. We attribute LCAMs superior perforit on the test sets of the ori"
2021.emnlp-main.686,N19-1242,0,0.0136686,"between the OSD, OC and Joint OSD & OC tasks. Specifically, in the first sentence, OSD extracts all outcomes i.e. wheezing and shortness of breath, OC classifies the text into an outcome type, Physiological, and then Joint OSD & OC extracts an outcome span and classifies it concurrently i.e. it extracts wheezing and also classifies it as a Physiological outcome. Motivated by the recent success in joint modelling of tasks such as aspect extraction (AE) and aspect sentiment classification (ASC), which together make a customer sentiment analysis task called Aspect Based Sentiment Analysis (ABSA; Xu et al., 2019), we model HOD as a joint task involving both OSD and OC. HOD can be formally defined as follows: coders – one for predicting the label for each word in OSD and another for predicting the outcome type in OC. LCAM is designed to jointly learn contextualised label attention-based distributions at word- and sentence-levels in order to capture which label/s a word or a sentence is more semantically related to. We call them contextualised because they are enriched by global contextual representations of the abstracts to which the sentences belongs. Label attention incorporates label sparsity inform"
2021.emnlp-main.686,P18-1019,0,0.124006,"tasks given two sample sentences. OSD retrieves the outcome spans, OC classifies the text span into a set of outcome types, and Joint OSD & OC retrieves outcomes and classifies them into outcome types. Introduction (2017) define an outcome as a measurement or an observation used to capture and assess the effect of treatment such as assessment of side effects (risk) or effectiveness (benefits). With the rapid growth of literature that reports outcomes, researchers have acknowledged and addressed the need to automate the extraction of outcomes from systematic reviews (Jonnalagadda et al., 2015; Nye et al., 2018) and ∗ Danushka Bollegala holds concurrent appointments as answering clinical questions (Demner-Fushman a Professor at University of Liverpool and as an Amazon and Lin, 2007). Jin and Szolovits (2018) mention Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. that automated Health Outcomes Detection (HOD) 8709 Access to the best available evidence in context of patient’s individual conditions enables healthcare professionals to administer optimal patient care (Demner-Fushman et al., 2006). Healthcare professionals identify outcomes as"
2021.emnlp-main.686,D14-1162,0,0.0851635,"ntaining wn belongs. Outcome Span Detection (OSD) |S| Given a set of sentences S = {si }i=1 within an abstract a, each si having N words, si = w1 , . . . , wN , with each word tagged to a label lw and use BIO tagging scheme (Sang and Veenstra, 1999). OSD aims to extract one or more outcome spans within si . For example, in Figure 1, OSD extracts the outcome span “incisional hernia” given the input sentence. Encoder: In our OSD task setting, we initially implement a baseline LCAM using a BiLSTM to encode input tokens (that are represented by d-dimensional word embeddings we obtain using GloVe (Pennington et al., 2014)3 ) into hidden representations for every word within an input sentence. We then consider generating each input words hidden representation using a pre-trained clinically informed BERTbase model called BioBERT (Lee et al., 2020). The LCAM model learns (3), 4.2 Abstract Hidden State Context hcn = hn + f (AbsEncoder(a)) (4) where f is a function computing the average pooled representation of the encoded abstract, AbsEncoder ∈ {BiLSTM, BioBERT}, AbsEncoder(a) ∈ Rk×|a |, |a |is the length of the abstract (measured by the number of tokens contained in it) and f (AbsEncoder(a)) ∈ Rk×1 . 4.3 Label-wo"
C18-1060,M92-1003,0,0.740127,"Missing"
C18-1060,Q16-1026,0,0.039607,"ormance, as described in Section 3.4. 3.3 LSTM+CNN+CRF model for FG-NER We re-implemented the LSTM+CNN+CRF NER model described by Ma and Hovy (2016) and adjust the model to work with FG-NER. The LSTM+CNN+CRF model originally described by (Ma and Hovy, 2016) is for NER problem with few NE categories. It first uses Convolutional Neural Network (CNN) to learn character level embeddings in the training process. For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively (Ma and Hovy, 715 2016; dos Santos and Guimar˜aes, 2015; Chiu and Nichols, 2016). The model then concatenates the character level embeddings with word embeddings to create a feature vector for each token in the input sentence. The input sentence is then fed to a BiLSTM network (Bi-directional Long-Short Term Memory network). Finally, CRF is used at the top layer of the BiLSTM to explore the correlations between outputs and jointly decode the best sequence of labels (i.e., NE categories). For both English and Japanese FG-NER task, we use pre-trained word embeddings as input for our models. Previous studies have shown that GloVe achieves the best performance for English NER"
C18-1060,W15-3904,0,0.0457865,"Missing"
C18-1060,N16-1030,0,0.0742374,"Missing"
C18-1060,P16-1101,0,0.488972,"lect the best model for different settings of training data size and target language. In FG-NER, because the number of NE categories is large, some categories might face with the data sparseness problem, whereas some other categories might have a large number of training samples in a dataset. Hence, it would be worth investigating the relation between dataset size and the performance of the system. The current state-of-the-art method for English NER (coarse-grained NER) is a neural network-based method, which uses convolutional neural network (CNN) to calculate the character level embeddings (Ma and Hovy, 2016). This leads to the question whether this method works well for languages with a large number of character types, such as Japanese. In this paper, we first investigate the relationship between the F-score of various FG-NER algorithms with the size of training datasets for both English and Japanese. Second, we suggest the direction to choose an appropriate FG-NER algorithm for appropriate target language and training data size. We show that the state-of-the-art method for English NER also performs well for English FG-NER. On the other hand, for Japanese FG-NER, the state-of-the-art method does"
C18-1060,W03-0430,0,0.0211231,"put for our models. Previous studies have shown that GloVe achieves the best performance for English NER task (Reimers and Gurevych, 2017). Consequently, we use the embeddings based on GloVe for English5 . For Japanese, we use pretrained word2vec6 embeddings. The vector dimension is 300 for English and 200 for Japanese. We use the default hyperparameters by Ma and Hovy (2016) in our model: learning rate = 0.01, batch size = 10 and decay rate = 0.09. 3.4 Incorporating dictionary information Dictionary information (gazetteer feature) has been proved to be efficient in many NER and FG-NER tasks (McCallum and Li, 2003; Sekine and Nobata, 2004; Yosef et al., 2012). While there are previous studies that use dictionary for CRF (McCallum and Li, 2003) or SVM (Yosef et al., 2012) in the NER/FGNER tasks, we believe that dictionary information would be useful in both sequence labelling and entity category disambiguation phase in the CRF+SVM method. Furthermore, the dictionary information can also be used in LSTM+CNN+CRF method. Consequently, we propose a method that efficiently utilizes dictionary information in the method LSTM+CNN+CRF and in both sequence labelling (CRF) and entity category disambiguation (SVM)"
C18-1060,W17-4114,0,0.0298835,"Missing"
C18-1060,D17-1035,0,0.0147056,"odel then concatenates the character level embeddings with word embeddings to create a feature vector for each token in the input sentence. The input sentence is then fed to a BiLSTM network (Bi-directional Long-Short Term Memory network). Finally, CRF is used at the top layer of the BiLSTM to explore the correlations between outputs and jointly decode the best sequence of labels (i.e., NE categories). For both English and Japanese FG-NER task, we use pre-trained word embeddings as input for our models. Previous studies have shown that GloVe achieves the best performance for English NER task (Reimers and Gurevych, 2017). Consequently, we use the embeddings based on GloVe for English5 . For Japanese, we use pretrained word2vec6 embeddings. The vector dimension is 300 for English and 200 for Japanese. We use the default hyperparameters by Ma and Hovy (2016) in our model: learning rate = 0.01, batch size = 10 and decay rate = 0.09. 3.4 Incorporating dictionary information Dictionary information (gazetteer feature) has been proved to be efficient in many NER and FG-NER tasks (McCallum and Li, 2003; Sekine and Nobata, 2004; Yosef et al., 2012). While there are previous studies that use dictionary for CRF (McCallu"
C18-1060,D11-1141,0,0.061483,"ed classification to recognize a music band name to answer the question “Which band was Paul in”, from the information shown in Figure 1. A fine-grained named entity recognition (FG-NER) model refers to a NER model that can recognize and classify a large number of entity categories (e.g., hundreds of NE categories). In classical coarse-grained named entity (NE) definition, often less than ten named entity categories are defined. For example, in the CoNLL-2003 Named Entity Recognition task, there are four NE categories: Person, Location, Organization and Miscellaneous (Sang and Meulder, 2003). Ritter et al. (2011) proposed a NER algorithm to recognize ten categories of entities from Twitter text. On the other hand, in FG-NER, there are hundreds of NE categories, which are fine-grained classification of coarse-grained categories. *) Equally contributed to the paper This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 711 Proceedings of the 27th International Conference on Computational Linguistics, pages 711–722 Santa Fe, New Mexico, USA, August 20-26, 2018. Paul, a former member of The Beatles, known for &quot;Let"
C18-1060,sekine-nobata-2004-definition,1,0.820793,"category Country and Organization Other. This is because the category Country is very easy to recognize, as there are only about 200 entities frequently used in this category, whereas, recognizing Organization Other or Car Stop is very difficult because of the ambiguity. This also indicates that the performance of an FG-NER system tends to depend on the categories and we can confirm this in the experimental results in the next sections. 3 Fine-Grained Named Entity Recognition Methods 3.1 Dictionary and Rule-based FG-NER The simplest method for FG-NER is using a dictionary and a set of rules. Sekine and Nobata (2004) presented a dictionary and rule-based Japanese FG-NER system that contains more than 1400 rules to recognize 140 entity categories. In this work, we added 200 rules to the existing 1400 rules by Sekine and Nobata to create a rule set of 1600 rules to classify 200 NE categories in the Sekine’s Extended Named Entity Hierarchy. We then built a rule-based Japanese FG-NER model to recognize 200 NE categories based on these 1600 rules. We use a Japanese FG-NER dictionary containing 1.6 million Wikipedia entities in this model. In the 1.6 million entities in the dictionary, only 70 thousand entities"
C18-1060,sekine-etal-2002-extended,1,0.613956,"egories from a knowledge base such as Freebase (Bollacker et al., 2008) or YAGO (Suchanek et al., 2007), filtering out the categories with a small number of entities and merging the categories with similar semantic meaning into one FG-NER category (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). The second method is to manually build an entity hierarchy to cover important domains in the real world. Following the second method, Sekine et al. proposed an Extended Named Entity Hierarchy (ENEH), which contains 200 entity categories in a three-layer hierarchy, as shown in Figure 2 (Sekine et al., 2002; Sekine, 2008). In this paper, we use the entity hierarchy described by Sekine (2008), which contains 200 NE categories at the leaf-level, as our tag set3 . At the top level of the hierarchy, there are about twenty coarse-grained named entity categories, such as Person, Organization, Location, Facility, Product, Event, . . . Each top-level categories is further divided into several second-level categories as shown in Figure 2. Each second-level category is in turn divided into several leaf-level categories. We use this hierarchy because it is carefully designed by humans, it does not ignore i"
C18-1060,sekine-2008-extended,1,0.793193,"International Conference on Computational Linguistics, pages 711–722 Santa Fe, New Mexico, USA, August 20-26, 2018. Paul, a former member of The Beatles, known for &quot;Let It Be”, Paul, a former member of The Beatles, known for &quot;Let It Be”, Person Person Artifact Organization Org > Show_Org will be holding a concert at Carnegie Hall in New York. Location Location Product > Art > Music will be holding a concert at Carnegie Hall in New York. Facilty > GOE > Theatre (a) Named entity recognition result Location > GPE > City (b) Fine-grained NER result Figure 1: Example of NER and FG-NER For example, Sekine (2008) divided the coarse-grained category Organization into the fine-grained categories such as Political Party, Military, Sports Organization, Show Organization, as shown in Figure 2. Person International Org Family Government ... Location Organization ... Show Org Political Party Time Numx Political Org Cabinet Military Other Political Org Figure 2: Sekine’s Extended Named Entity (ENE) hierarchy FG-NER is still an open research domain, with little information concerning the state-of-the-art performance, the relation between training data size and performance, and how to select the best model for"
C18-1060,W02-2029,0,0.218657,"Missing"
C18-1060,P15-2048,0,0.023901,"eral SVM models (each for a top-level category) to classify the entities into the leaf-level categories. We use the following features for both SVM and CRF: bag-of-words, POS-tag, the number of digits in the word, the Brown cluster of the current word, the appearance of the word as a substring of a word in the Wikipedia ENE dictionary, the orthography features (the word is written in Kanji, Hiragana, Katakana or Romanji), is capital letter, and the last 2-3 characters. Those features are proved to be useful in previous work on named entity recognition (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Suzuki et al., 2016). Once we have the sequence labelling result, we have already known the surfaces and the top-level categories of the entities in the input sentence. We then use SVM to classify the entities into leaf-level categories. Because the number of leaf-level categories in each top-level categories is also not too large (e.g., less than 15), SVM can achieve a reasonable performance at this step. We also propose a method to incorporate dictionary information in both CRF and SVM step to improve the entire performance, as described in Section 3.4. 3.3 LSTM+CNN+CRF model for FG-NER We"
C18-1060,C12-2133,0,0.31149,"nd Datasets 2.1 FG-NER tag set The first challenge in FG-NER is defining a comprehensive tag set with a very large number of entity categories (Ling and Weld, 2012). There are two methods for defining a tag set (i.e., set of entity categories to recognize) in previous studies on FG-NER. The first method is to take the entity categories from a knowledge base such as Freebase (Bollacker et al., 2008) or YAGO (Suchanek et al., 2007), filtering out the categories with a small number of entities and merging the categories with similar semantic meaning into one FG-NER category (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). The second method is to manually build an entity hierarchy to cover important domains in the real world. Following the second method, Sekine et al. proposed an Extended Named Entity Hierarchy (ENEH), which contains 200 entity categories in a three-layer hierarchy, as shown in Figure 2 (Sekine et al., 2002; Sekine, 2008). In this paper, we use the entity hierarchy described by Sekine (2008), which contains 200 NE categories at the leaf-level, as our tag set3 . At the top level of the hierarchy, there are about twenty coarse-grained named entity categories, such as Perso"
C18-1060,P02-1060,0,0.404785,"Missing"
C18-1140,P16-1177,0,0.0364356,"Missing"
C18-1140,P14-2131,0,0.0298991,"impressive performances in numerous NLP tasks such as sentiment classification (Socher et al., 2013), and machine translation (Zou et al., 2013). Previous works studying the differences in word embedding learning methods (Chen et al., 2013; Yin and Sch¨utze, 2016) have shown that word embeddings learnt using different methods and from different resources have significant variation in quality and characteristics of the semantics captured. For example, Hill et al. (2014; 2015) showed that the word embeddings trained from monolingual vs. bilingual corpora capture different local neighbourhoods. Bansal et al. (2014) showed that an ensemble of different word representations improves the accuracy of dependency parsing, implying the complementarity of the different word embeddings. This suggests the importance of meta-embedding – creating a new embedding by combining different existing embeddings. We refer to the input word embeddings to the meta-embedding process as the source embeddings. Yin and Sch¨utze (2016) showed that by meta-embedding five different pre-trained word embeddings, we can overcome the out-of-vocabulary problem, and improve the accuracy of cross-domain part-of-speech (POS) tagging. Encou"
C18-1140,P12-1015,0,0.031082,"ddings. The computed semantic similarity scores are compared against human-rated similarity scores using the Spearman correlation coefficient. A high degree of correlation with the human ratings is considered as an indication of the accuracy of the word embeddings. We use the following benchmark datasets for this evaluation: Word Similarity 353 dataset (WS, 2023 word pairs) (Finkelstein et al., 2002), Rubenstein-Goodenough dataset (RG, 65 word pairs) (Rubenstein and Goodenough, 1965), Miller-Charles dataset (MC, 30 word pairs) (Miller and Charles, 1998), and the MEN dataset (3000 word pairs) (Bruni et al., 2012). Word Analogy: Word analogy task consists of questions like “a to b is c to what?”. Different methods have been proposed in the literature for finding fourth word d that completes an analogy. In our experiments we use the CosAdd method, which finds d such that the cosine similarity between the vector (b − a + c) and d is maximised. We use the following benchmark datasets for this task: Google dataset (GL) (Mikolov et al., 2013b), MSR dataset (Levy and Goldberg, 2014), SemEval 2012 Task 2 dataset (SE) (Jurgens et al., 2012), and the SAT (Slack, 1980) dataset. The evaluation measure is the rati"
C18-1140,N18-2031,1,0.723336,"can set λ2 > λ1 . The coefficients are also tuned experimentally using validation data.  X L(E1 , E2 , D1 , D2 ) = λ1 ||ˆ s1 (w) − s1 (w)||2 + λ2 ||ˆ s2 (w) − s2 (w)||2 (7) w∈V Compared to DAEME, CAEME imposes a tighter integration between the two sources in their metaembedding. We jointly learn E1 , E2 , D1 and D2 that minimises the total reconstruction error given by (7). 3.3 Averaged Autoencoded Meta-Embedding (AAEME) AAEME can be seen as a special case of CAEME, where we compute the meta-embedding by averaging the two encoded sources in (1) instead by their concatenation. A recent work (Coates and Bollegala, 2018) shows that for approximately orthogonal source embedding spaces, averaging performs comparably to concatenation, without increasing the dimensionality. However, our AAEME can be seen as a more general version of averaging in the sense that we first transform each source embedding independently using two encoders before we compute their average. This operation has the benefit that we can transform the sources such that they could be averaged in the same vector space, and also guarantees orthogonality between the encoded vectors. AAEME computes the meta-embedding of a word w from its two source"
C18-1140,P12-1092,0,0.270076,"the existing state-of-the-art metaembeddings in multiple tasks. 1 Introduction Representing the meanings of words is a fundamental task in Natural Language Processing (NLP). A popular approach to represent the meaning of a word is to embed it in some fixed-dimensional vector space (Turney and Pantel, 2010). In contrast to sparse and high-dimensional counting-based distributional word representation methods that use co-occurring contexts of a word as its representation, dense and low-dimensional prediction-based distributed word representations (Pennington et al., 2014; Mikolov et al., 2013a; Huang et al., 2012; Collobert and Weston, 2008; Mnih and Hinton, 2009) have obtained impressive performances in numerous NLP tasks such as sentiment classification (Socher et al., 2013), and machine translation (Zou et al., 2013). Previous works studying the differences in word embedding learning methods (Chen et al., 2013; Yin and Sch¨utze, 2016) have shown that word embeddings learnt using different methods and from different resources have significant variation in quality and characteristics of the semantics captured. For example, Hill et al. (2014; 2015) showed that the word embeddings trained from monoling"
C18-1140,W14-1618,0,0.0459469,"and Goodenough, 1965), Miller-Charles dataset (MC, 30 word pairs) (Miller and Charles, 1998), and the MEN dataset (3000 word pairs) (Bruni et al., 2012). Word Analogy: Word analogy task consists of questions like “a to b is c to what?”. Different methods have been proposed in the literature for finding fourth word d that completes an analogy. In our experiments we use the CosAdd method, which finds d such that the cosine similarity between the vector (b − a + c) and d is maximised. We use the following benchmark datasets for this task: Google dataset (GL) (Mikolov et al., 2013b), MSR dataset (Levy and Goldberg, 2014), SemEval 2012 Task 2 dataset (SE) (Jurgens et al., 2012), and the SAT (Slack, 1980) dataset. The evaluation measure is the ratio of the questions that were answered correctly in each dataset using a particular word embedding. Relation Classification: The DiffVec dataset (DV) (Vylomova et al., 2016) contains 12,458 triples of the form (r, w1 , w2 ), where the relation r exists between the two words w1 and w2 . DiffVec dataset contains tuples covering 15 different relation types. The task is to predict the relation that exists between two words w1 and w2 from the 15 relation types in the datase"
C18-1140,D13-1054,0,0.0152118,"learning method must be able to learn meta-embeddings from the given set of source embeddings without assuming the availability of the training resources. Autoencoders (Kingma and Welling, 2014; Vincent et al., 2008) have gained popularity as a method for learning feature representations from unlabelled data that can then be used for supervised learning tasks. Autoencoders have been successfully applied in various NLP tasks such as domain adaptation (Chen et al., 2012; Ziser and Reichart, 2016), similarity measurement (Li et al., 2015; Amiri et al., 2016), machine translation (P et al., 2014; Li et al., 2013) and sentiment analysis (Socher et al., 2011). Autoencoder attempts to reconstruct an input from a possibly noisy version of the input via a non-linear transformation. The intermediate representation used by the autoencoder captures the essential information about the input such that it can be accurately reconstructed. This setting is closely related to the meta-embedding learning where we must reconstruct the information contained in individual source embeddings using a single meta-embedding. However, unlike typical autoencoder learning where we have a single input, in meta-embedding learning"
C18-1140,P15-1107,0,0.0232311,"bedding learning methods on the same resource. Therefore, a meta-embedding learning method must be able to learn meta-embeddings from the given set of source embeddings without assuming the availability of the training resources. Autoencoders (Kingma and Welling, 2014; Vincent et al., 2008) have gained popularity as a method for learning feature representations from unlabelled data that can then be used for supervised learning tasks. Autoencoders have been successfully applied in various NLP tasks such as domain adaptation (Chen et al., 2012; Ziser and Reichart, 2016), similarity measurement (Li et al., 2015; Amiri et al., 2016), machine translation (P et al., 2014; Li et al., 2013) and sentiment analysis (Socher et al., 2011). Autoencoder attempts to reconstruct an input from a possibly noisy version of the input via a non-linear transformation. The intermediate representation used by the autoencoder captures the essential information about the input such that it can be accurately reconstructed. This setting is closely related to the meta-embedding learning where we must reconstruct the information contained in individual source embeddings using a single meta-embedding. However, unlike typical a"
C18-1140,N16-1050,0,0.0562854,"Missing"
C18-1140,P05-1015,0,0.533545,"Missing"
C18-1140,D14-1162,0,0.0903007,"proposed autoencoded meta-embeddings outperform the existing state-of-the-art metaembeddings in multiple tasks. 1 Introduction Representing the meanings of words is a fundamental task in Natural Language Processing (NLP). A popular approach to represent the meaning of a word is to embed it in some fixed-dimensional vector space (Turney and Pantel, 2010). In contrast to sparse and high-dimensional counting-based distributional word representation methods that use co-occurring contexts of a word as its representation, dense and low-dimensional prediction-based distributed word representations (Pennington et al., 2014; Mikolov et al., 2013a; Huang et al., 2012; Collobert and Weston, 2008; Mnih and Hinton, 2009) have obtained impressive performances in numerous NLP tasks such as sentiment classification (Socher et al., 2013), and machine translation (Zou et al., 2013). Previous works studying the differences in word embedding learning methods (Chen et al., 2013; Yin and Sch¨utze, 2016) have shown that word embeddings learnt using different methods and from different resources have significant variation in quality and characteristics of the semantics captured. For example, Hill et al. (2014; 2015) showed tha"
C18-1140,D11-1014,0,0.0686361,"a-embeddings from the given set of source embeddings without assuming the availability of the training resources. Autoencoders (Kingma and Welling, 2014; Vincent et al., 2008) have gained popularity as a method for learning feature representations from unlabelled data that can then be used for supervised learning tasks. Autoencoders have been successfully applied in various NLP tasks such as domain adaptation (Chen et al., 2012; Ziser and Reichart, 2016), similarity measurement (Li et al., 2015; Amiri et al., 2016), machine translation (P et al., 2014; Li et al., 2013) and sentiment analysis (Socher et al., 2011). Autoencoder attempts to reconstruct an input from a possibly noisy version of the input via a non-linear transformation. The intermediate representation used by the autoencoder captures the essential information about the input such that it can be accurately reconstructed. This setting is closely related to the meta-embedding learning where we must reconstruct the information contained in individual source embeddings using a single meta-embedding. However, unlike typical autoencoder learning where we have a single input, in meta-embedding learning we must reconstruct multiple source embeddin"
C18-1140,D13-1170,0,0.0103554,"ng (NLP). A popular approach to represent the meaning of a word is to embed it in some fixed-dimensional vector space (Turney and Pantel, 2010). In contrast to sparse and high-dimensional counting-based distributional word representation methods that use co-occurring contexts of a word as its representation, dense and low-dimensional prediction-based distributed word representations (Pennington et al., 2014; Mikolov et al., 2013a; Huang et al., 2012; Collobert and Weston, 2008; Mnih and Hinton, 2009) have obtained impressive performances in numerous NLP tasks such as sentiment classification (Socher et al., 2013), and machine translation (Zou et al., 2013). Previous works studying the differences in word embedding learning methods (Chen et al., 2013; Yin and Sch¨utze, 2016) have shown that word embeddings learnt using different methods and from different resources have significant variation in quality and characteristics of the semantics captured. For example, Hill et al. (2014; 2015) showed that the word embeddings trained from monolingual vs. bilingual corpora capture different local neighbourhoods. Bansal et al. (2014) showed that an ensemble of different word representations improves the accuracy"
C18-1140,D14-1101,0,0.0151996,"ddings for out-of-vocabulary words in a particular source embedding, using the known word embeddings. Next, 1TON method is applied to learn the meta-embeddings for the union of the vocabularies covered by all of the source embeddings. Experimental results in semantic similarity prediction, word analogy detection, and cross-domain POS tagging tasks show the effectiveness of both 1TON and 1TON+. Although not learning any meta-embedding, several prior works have shown that incorporating multiple word embeddings learnt using different methods improve performance in various NLP tasks. For example, Tsuboi (2014) showed that by using both word2vec and GloVe embeddings together in a POS tagging task, it is possible to improve the tagging accuracy, if we had used only one of those embeddings. Similarly, Turian et al. (2010) collectively used Brown clusters, CW and HLBL embeddings, to improve the performance of named entity recognition and chucking tasks. Luo et al. (2014) proposed a multi-view word embedding learning method that uses a two-sided neural network. They adapt pre-trained CBOW (Mikolov et al., 2013b) embeddings from Wikipedia and clickthrough data from a search engine. Their problem setting"
C18-1140,P10-1040,0,0.107518,"ll of the source embeddings. Experimental results in semantic similarity prediction, word analogy detection, and cross-domain POS tagging tasks show the effectiveness of both 1TON and 1TON+. Although not learning any meta-embedding, several prior works have shown that incorporating multiple word embeddings learnt using different methods improve performance in various NLP tasks. For example, Tsuboi (2014) showed that by using both word2vec and GloVe embeddings together in a POS tagging task, it is possible to improve the tagging accuracy, if we had used only one of those embeddings. Similarly, Turian et al. (2010) collectively used Brown clusters, CW and HLBL embeddings, to improve the performance of named entity recognition and chucking tasks. Luo et al. (2014) proposed a multi-view word embedding learning method that uses a two-sided neural network. They adapt pre-trained CBOW (Mikolov et al., 2013b) embeddings from Wikipedia and clickthrough data from a search engine. Their problem setting is different from ours because their source embeddings are trained using the same word embedding learning method but on different resources 1651 whereas, we consider source embeddings trained using different word"
C18-1140,P16-1158,0,0.0135214,"g fourth word d that completes an analogy. In our experiments we use the CosAdd method, which finds d such that the cosine similarity between the vector (b − a + c) and d is maximised. We use the following benchmark datasets for this task: Google dataset (GL) (Mikolov et al., 2013b), MSR dataset (Levy and Goldberg, 2014), SemEval 2012 Task 2 dataset (SE) (Jurgens et al., 2012), and the SAT (Slack, 1980) dataset. The evaluation measure is the ratio of the questions that were answered correctly in each dataset using a particular word embedding. Relation Classification: The DiffVec dataset (DV) (Vylomova et al., 2016) contains 12,458 triples of the form (r, w1 , w2 ), where the relation r exists between the two words w1 and w2 . DiffVec dataset contains tuples covering 15 different relation types. The task is to predict the relation that exists between two words w1 and w2 from the 15 relation types in the dataset. The relation classification accuracy is computed as the ratio of the correctly predicted instances to the total instances in the 2 https://github.com/CongBao/AutoencodedMetaEmbedding 1655 DiffVec dataset. We represent the relation between two words as the vector offset, w1 −w2 , between the corre"
C18-1140,D13-1141,0,0.0274785,"eaning of a word is to embed it in some fixed-dimensional vector space (Turney and Pantel, 2010). In contrast to sparse and high-dimensional counting-based distributional word representation methods that use co-occurring contexts of a word as its representation, dense and low-dimensional prediction-based distributed word representations (Pennington et al., 2014; Mikolov et al., 2013a; Huang et al., 2012; Collobert and Weston, 2008; Mnih and Hinton, 2009) have obtained impressive performances in numerous NLP tasks such as sentiment classification (Socher et al., 2013), and machine translation (Zou et al., 2013). Previous works studying the differences in word embedding learning methods (Chen et al., 2013; Yin and Sch¨utze, 2016) have shown that word embeddings learnt using different methods and from different resources have significant variation in quality and characteristics of the semantics captured. For example, Hill et al. (2014; 2015) showed that the word embeddings trained from monolingual vs. bilingual corpora capture different local neighbourhoods. Bansal et al. (2014) showed that an ensemble of different word representations improves the accuracy of dependency parsing, implying the compleme"
C18-1211,D12-1050,0,0.0351408,"elational embeddings is to apply some operator on two word embeddings to compose the embedding for the relation that exits between those two words, if any. In contrast to the first approach, we do not have to learn relational embeddings and hence this can be considered as an unsupervised setting, where the compositional operator is predefined. A popular operator for composing a relational embedding from two word embeddings is PairDiff, which is the vector difference (offset) of the word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2016; Bollegala et al., 2015b; Blacoe and Lapata, 2012). Specifically, given two words a and b represented by their word embeddings respectively a and b, the relation between a and b is given by a − b under the PairDiff operator. Mikolov et al. (2013b) showed that PairDiff can accurately solve analogy equations such as # » # » # » = queen, # » where we have used the top arrows to denote the embeddings of the king − man + woman corresponding words. Bollegala et al. (2015a) showed that PairDiff can be used as a proxy for learning better word embeddings and Vylomova et al. (2016) conducted an extensive empirical comparison of PairDiff using a dataset"
C18-1211,D09-1084,1,0.76086,"rator on several benchmark analogy datasets. 1 Introduction Different types of semantic relations exist between words such as HYPERNYMY between ostrich and bird, or ANTONYMY between hot and cold. If we consider entities1 , we can observe even a richer diversity of relations such as FOUNDER-OF between Bill Gates and Microsoft, or CAPITAL-OF between Tokyo and Japan. Identifying the relations between words and entities is important for various Natural Language Processing (NLP) tasks such as automatic knowledge base completion (Socher et al., 2013), analogical reasoning (Turney and Littman, 2005; Bollegala et al., 2009) and relational information retrieval (Duc et al., 2010). For example, to solve a word analogy problem of the form “a is to b as c is to ?”, the relationship between the two words in the pair (a, b) must be correctly identified in order to find candidates d that have similar relations with c. For example, given the query “Bill Gates is to Microsoft as Steve Jobs is to ?”, a relational search engine must retrieve Apple Inc. because the FOUNDER-OF relation exists between the first and the second entity pairs. Two main approaches for creating relation embeddings can be identified in the literatur"
C18-1211,C16-1332,0,0.0192925,"ity of a relation. Complex Embeddings (Trouillon et al., 2016) overcome this limitation of DistMult by using complex embeddings and defining the score to be the real part of r h ¯t, where ¯t denotes the complex conjugate of t. The observation made by Mikolov et al. (2013b) that the relation between two words can be represented by the difference between their word embeddings sparked a renewed interest in methods that compose relational embeddings using word embeddings. Word analogy datasets such as Google dataset (Mikolov et al., 2013b), SemEval 2012 Task2 dataset (Jurgens et al., 2012), BATS (Drozd et al., 2016) etc. have established as benchmarks for evaluating word embedding learning methods. Different methods have been proposed to measure the similarity between the relations that exist between two given word pairs such as CosMult, CosAdd and PairDiff (Levy and Goldberg, 2014; Bollegala et al., 2015a). Vylomova et al. (2016) studied as to what extent the vectors generated using simple PairDiff encode different relation types. Under supervised classification settings, they conclude that PairDiff can cover a wide range of semantic relation types. Holographic embeddings proposed by Nickel et al. (2016"
C18-1211,P15-1144,0,0.141874,", under regularised settings (§3.1), the bilinear operator further simplifies to a linear combination of the input embeddings, and the expected loss over positive and negative instances becomes zero. In §4.1, we empirically validate the uncorrelation assumption for different pre-trained word embeddings such as the Continuous Bag-of-Words Model (CBOW) (Mikolov et al., 2013a), Skip-Gram with negative sampling (SG) (Mikolov et al., 2013a), Global Vectors (GloVe) (Pennington et al., 2014), word embeddings created using Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Sparse Coding (HSC) (Faruqui et al., 2015; Yogatama et al., 2015), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). This empirical evidence implies that our theoretical analysis is applicable to relational representations composed from a wide-range of word embedding learning methods. Moreover, our experimental results show that a bilinear operator reaches its optimal performance in two different word-analogy benchmark datasets, when it satisfies the requirements of the PairDiff operator. We hope that our theoretical analysis will expand the understanding of relational embedding methods, and inspire future research on accura"
C18-1211,N16-2002,0,0.0132503,", we see that crossdimensional correlations are distributed in a narrow range with an almost zero mean. This result empirically validates the uncorrelation assumption we used in our theoretical analysis. Moreover, this result indicates that Theorem 1 can be applied to a wide-range of existing word embeddings. 4.2 Learning Relation Representations Our theoretical analysis in §3 claims that the performance of the bilinear relational embedding is independent of the tensor operator A. To empirically verify this claim, we conduct the following experiment. For this purpose, we use the BATS dataset (Gladkova et al., 2016) that contains of 40 semantic and syntactic relation types8 , and generate positive examples by pairing word-pairs that have the same relation types. Approximately each relation type has 1,225 word-pairs, which enables us to generate a total of 48k positive training instances (analogous word-pairs) of the form ((h, t), (h0 , t0 )). For each pair (h, t) related by a relation r, we randomly select pairs (h0 , t0 ) with a different relation type r0 , according to the `2 distance between the two pairs to create negative (nonanalogous) instances.9 We collectively refer both positive and negative tr"
C18-1211,D16-1019,0,0.0209535,"nship between the two words in the pair (a, b) must be correctly identified in order to find candidates d that have similar relations with c. For example, given the query “Bill Gates is to Microsoft as Steve Jobs is to ?”, a relational search engine must retrieve Apple Inc. because the FOUNDER-OF relation exists between the first and the second entity pairs. Two main approaches for creating relation embeddings can be identified in the literature. In the first approach, from given corpora or knowledge bases, word and relation embeddings are jointly learnt such that some objective is optimised (Guo et al., 2016; Yang et al., 2015; Nickel et al., 2016; Bordes et al., 2013; Rocktäschel et al., 2016; Minervini et al., 2017; Trouillon et al., 2016). In this approach, word and relation embeddings are considered to be independent parameters that must be learnt by the embedding method. For example, TransE (Bordes et al., 2013) learns the word and relation embeddings such that we can accurately predict relations (links) in a given knowledge base using the learnt word and relation embeddings. Because relations are learnt independently from the words, we refer to methods that are based on this approach as ind"
C18-1211,P17-2088,0,0.0308551,"l., 2015a). Vylomova et al. (2016) studied as to what extent the vectors generated using simple PairDiff encode different relation types. Under supervised classification settings, they conclude that PairDiff can cover a wide range of semantic relation types. Holographic embeddings proposed by Nickel et al. (2016) use circular convolution to mix the embeddings of two words to create an embedding for the relation that exist between those words. It can be showed that circular correlation is indeed an elementwise product in the Fourier space and is mathematically equivalent to complex embeddings (Hayashi and Shinbo, 2017). Although PairDiff operator has been widely used in prior work for computing relation embeddings from word embeddings, to the best of our knowledge, no theoretical analysis has been conducted so far explaining why and under what conditions PairDiff is optimal, which is the focus of this paper. 3 Bilinear Relation Representations Let us consider the problem of representing the semantic relation r(h, t) between two given words h and t. We assume that h and t are already represented in some d-dimensional space respectively by their word embeddings h, t ∈ Rd . The relation between two words can b"
C18-1211,S12-1047,0,0.0268899,"lt cannot capture directionality of a relation. Complex Embeddings (Trouillon et al., 2016) overcome this limitation of DistMult by using complex embeddings and defining the score to be the real part of r h ¯t, where ¯t denotes the complex conjugate of t. The observation made by Mikolov et al. (2013b) that the relation between two words can be represented by the difference between their word embeddings sparked a renewed interest in methods that compose relational embeddings using word embeddings. Word analogy datasets such as Google dataset (Mikolov et al., 2013b), SemEval 2012 Task2 dataset (Jurgens et al., 2012), BATS (Drozd et al., 2016) etc. have established as benchmarks for evaluating word embedding learning methods. Different methods have been proposed to measure the similarity between the relations that exist between two given word pairs such as CosMult, CosAdd and PairDiff (Levy and Goldberg, 2014; Bollegala et al., 2015a). Vylomova et al. (2016) studied as to what extent the vectors generated using simple PairDiff encode different relation types. Under supervised classification settings, they conclude that PairDiff can cover a wide range of semantic relation types. Holographic embeddings prop"
C18-1211,W14-1618,0,0.256765,"Fe, New Mexico, USA, August 20-26, 2018. A second approach for creating relational embeddings is to apply some operator on two word embeddings to compose the embedding for the relation that exits between those two words, if any. In contrast to the first approach, we do not have to learn relational embeddings and hence this can be considered as an unsupervised setting, where the compositional operator is predefined. A popular operator for composing a relational embedding from two word embeddings is PairDiff, which is the vector difference (offset) of the word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2016; Bollegala et al., 2015b; Blacoe and Lapata, 2012). Specifically, given two words a and b represented by their word embeddings respectively a and b, the relation between a and b is given by a − b under the PairDiff operator. Mikolov et al. (2013b) showed that PairDiff can accurately solve analogy equations such as # » # » # » = queen, # » where we have used the top arrows to denote the embeddings of the king − man + woman corresponding words. Bollegala et al. (2015a) showed that PairDiff can be used as a proxy for learning better word embeddings and Vylomova et al. (201"
C18-1211,Q15-1016,0,0.0356146,"g training. 4 Experimental Results 4.1 Cross-dimensional Correlations A key assumption in our theoretical analysis is the uncorrelations between different dimensions in word embeddings. Here, we empirically verify the uncorrelation assumption for different input word embeddings. For this purpose, we create SG, CBOW and GloVe embeddings from the ukWaC corpus5 . We use a context window of 5 tokens and select words that occur at least 6 times in the corpus. We use the publicly available implementations for those methods by the original authors and set the parameters to the recommended values in (Levy et al., 2015) to create 50-dimensional word embeddings. As a representative of counting-based word embeddings, we create a word co-occurrence matrix weighted by the positive pointwise mutual information (PPMI) and apply singular value decomposition (SVD) to obtain 50-dimensional embeddings, which we refer to as the Latent Semantic Analysis (LSA) embeddings. We use Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to create a topic model, and represent each word by its distribution over the set of topics. Ideally, each topic will capture some semantic category and the topic distribution provides a seman"
C18-1211,N13-1090,0,0.810353,"pages 2493–2504 Santa Fe, New Mexico, USA, August 20-26, 2018. A second approach for creating relational embeddings is to apply some operator on two word embeddings to compose the embedding for the relation that exits between those two words, if any. In contrast to the first approach, we do not have to learn relational embeddings and hence this can be considered as an unsupervised setting, where the compositional operator is predefined. A popular operator for composing a relational embedding from two word embeddings is PairDiff, which is the vector difference (offset) of the word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2016; Bollegala et al., 2015b; Blacoe and Lapata, 2012). Specifically, given two words a and b represented by their word embeddings respectively a and b, the relation between a and b is given by a − b under the PairDiff operator. Mikolov et al. (2013b) showed that PairDiff can accurately solve analogy equations such as # » # » # » = queen, # » where we have used the top arrows to denote the embeddings of the king − man + woman corresponding words. Bollegala et al. (2015a) showed that PairDiff can be used as a proxy for learning better word embedding"
C18-1211,D14-1162,0,0.095157,"ar relational compositional operators are independent of bilinear pairwise interactions between the two input word embeddings. Moreover, under regularised settings (§3.1), the bilinear operator further simplifies to a linear combination of the input embeddings, and the expected loss over positive and negative instances becomes zero. In §4.1, we empirically validate the uncorrelation assumption for different pre-trained word embeddings such as the Continuous Bag-of-Words Model (CBOW) (Mikolov et al., 2013a), Skip-Gram with negative sampling (SG) (Mikolov et al., 2013a), Global Vectors (GloVe) (Pennington et al., 2014), word embeddings created using Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Sparse Coding (HSC) (Faruqui et al., 2015; Yogatama et al., 2015), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). This empirical evidence implies that our theoretical analysis is applicable to relational representations composed from a wide-range of word embedding learning methods. Moreover, our experimental results show that a bilinear operator reaches its optimal performance in two different word-analogy benchmark datasets, when it satisfies the requirements of the PairDiff operator. We hope"
C18-1211,P16-1158,0,0.296982,"st 20-26, 2018. A second approach for creating relational embeddings is to apply some operator on two word embeddings to compose the embedding for the relation that exits between those two words, if any. In contrast to the first approach, we do not have to learn relational embeddings and hence this can be considered as an unsupervised setting, where the compositional operator is predefined. A popular operator for composing a relational embedding from two word embeddings is PairDiff, which is the vector difference (offset) of the word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014; Vylomova et al., 2016; Bollegala et al., 2015b; Blacoe and Lapata, 2012). Specifically, given two words a and b represented by their word embeddings respectively a and b, the relation between a and b is given by a − b under the PairDiff operator. Mikolov et al. (2013b) showed that PairDiff can accurately solve analogy equations such as # » # » # » = queen, # » where we have used the top arrows to denote the embeddings of the king − man + woman corresponding words. Bollegala et al. (2015a) showed that PairDiff can be used as a proxy for learning better word embeddings and Vylomova et al. (2016) conducted an extensi"
D09-1084,P99-1008,0,0.0565097,"rds such as the WordNet, semantic relations (i.e. hypernymy, meronymy, synonymy etc.) between words can be directly looked up in the taxonomy. Alternatively, the labels of the edges in the path connecting two words can be used as semantic relations. However, in this paper we do not assume the availability of manually created resources such as dictionaries or taxonomies. We represent semantic relations using automatically extracted lexical patterns. Lexical patterns have been successfully used to represent various semantic relations between words such as hypernymy (Hearst, 1992), and meronymy (Berland and Charniak, 1999). Following these previous approaches, we represent R(a, b) as a set of lexical patterns. Moreover, we denote the frequency of a lexical pattern r for a word pair (a, b) by f (r, a, b). So far we have not defined the functional form of Ξ. A straightforward approach is to use a linearly weighted combination of relations as shown below, Ξ(R(a, b)) = X wi × f (ri , a, b). (2) ri ∈R(a,b) Here, wi is the weight associated with the lexical pattern ri and can be determined using training data. However, this formulation has two fundamental drawbacks. First, the number of weight parameters wi is equal"
D09-1084,P98-2127,0,0.351958,"@mi.ci.i. u-tokyo.ac.jp Yutaka Matsuo matsuo@biz-model. t.u-tokyo.ac.jp The University of Tokyo 7-3-1, Hongo, Tokyo, 113-8656, Japan Abstract ishizuka@i. u-tokyo.ac.jp known for the existing category. As the similarity between two objects X and Y increases, so does the probability of correctly inferring that Y has the property T upon knowing that X has T (Tenenbaum, 1999). Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002). In information retrieval, similar or related words are used to expand user queries to improve recall (Sahami and Heilman, 2006). Semantic similarity is a context dependent and dynamic phenomenon. New words are constantly being created and existing words are assigned with new senses on the Web. To decide whether two words are semantically similar, it is important to know the semantic relations that hold between the words. For example, the words horse and cow can be considered semantically similar because both horses and cows are useful anima"
D09-1084,P06-1127,0,0.0605329,"erm vector. Each vector is L2 normalized and the centroid of the set of vectors is computed. Semantic similarity between two queries is then defined as the inner product between the corresponding centroid vectors. They did not compare their similarity measure with taxonomybased similarity measures. 3 Relational Model of Similarity We propose a model to compute the semantic similarity between two words a and b using the set of semantic relations R(a, b) that hold between a and b. We call the proposed model the relational model of semantic similarity and it is defined by the following equation, Chen et al., (2006) propose a web-based doublechecking model to compute the semantic similarity between words. For two words X and Y , they collect snippets for each word from a web search engine. Then they count the number of occurrences of X in the snippets for Y , and Y in the snippets for X. The two values are combined nonlinearly to compute the similarity between X and Y . This method heavily depends on the search engine’s ranking algorithm. Although two words X and Y may be very similar, there is no reason to believe that one can find Y in the snippets for X, or vice versa. This observation is confirmed by"
D09-1084,W02-1029,0,0.0113604,"-model. t.u-tokyo.ac.jp The University of Tokyo 7-3-1, Hongo, Tokyo, 113-8656, Japan Abstract ishizuka@i. u-tokyo.ac.jp known for the existing category. As the similarity between two objects X and Y increases, so does the probability of correctly inferring that Y has the property T upon knowing that X has T (Tenenbaum, 1999). Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002). In information retrieval, similar or related words are used to expand user queries to improve recall (Sahami and Heilman, 2006). Semantic similarity is a context dependent and dynamic phenomenon. New words are constantly being created and existing words are assigned with new senses on the Web. To decide whether two words are semantically similar, it is important to know the semantic relations that hold between the words. For example, the words horse and cow can be considered semantically similar because both horses and cows are useful animals in agriculture. Similarly, a horse and a car can"
D09-1084,C92-2082,0,0.217432,"Missing"
D09-1084,O97-1002,0,\N,Missing
D09-1084,C98-2122,0,\N,Missing
D10-1039,W01-1605,0,0.483697,"eful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class classification problem. Some of the relations corresponding to these classes are relatively more frequent in the"
D10-1039,P09-1075,0,0.03857,"cur very rarely, such as T OPIC -C OMMENT [S][N] (2 instances), or E VALUATION [N][N] (3 instances). A similar phenomenon can be observed in PDTB, in which 15 level-two relations are employed: Some, such as E XPANSION .C ONJUNCTION, occur as often as 8759 times throughout the corpus, whereas the remainder of the relations, such as E XPANSION .E XCEPTION and C OMPARISON .P RAGMATIC CONCESSION, can appear as rarely as 17 and 12 times respectively. Although supervised approaches to discourse relation learning achieve good results on frequent relations, performance is poor on rare relation types (duVerle and Prendinger, 2009). Nonetheless, certain infrequent relation types might be important for specific tasks. For instance, 1 We use the notation [N] and [S] respectively to denote the nucleus and satellite in a RST discourse relation. 399 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399–409, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics capturing the RST T OPIC -C OMMENT [S][N] and E VALUATION [N][N] relations can be useful for sentiment analysis (Pang and Lee, 2008). Another situation where detection of lowoccurring re"
D10-1039,P08-1017,0,0.0228353,"ix. This feature selection procedure can efficiently reduce the dimensions of the feature co-occurrence matrix to N × N . Because the feature co-occurrence matrix is symmetric, we must only store the elements for the upper (or lower) triangular portion of it. 3.2 Feature Vector Extension Once the feature co-occurrence matrix is computed using unlabeled data as described in Section 3.1, we can use it to extend a feature vector during training and testing. The proposed feature vector extension method is inspired by query expansion in the field of Information Retrieval (Salton and Buckley, 1983; Fang, 2008). One of the reasons that a classifier might perform poorly on a test instance is that there are features in the test instance that were not observed during training. We call FU = {fi } the set of features that were not observed by the classifier during training (i.e. occurring in test data but not in training data). For each of those features, we use the feature co-occurrence matrix to find the set of co-occurring features, Fc (fi ). Let us denote the feature vector corresponding to a training or test instance x by fx . We use the superscript notation, fxi to denote the i-th feature in fx . M"
D10-1039,D09-1036,0,0.242628,"r. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class"
D10-1039,W02-0109,0,0.0249341,"se the dimension too much, it is possible to retain only candidate co-occurring features of Fc (fi ) possessing a co-occurrence value C(i,j) above a certain threshold. In the experiments of Section 4 however, we experienced dimension increase of 10000 at most, which did not require us to use thresholding. 3.3 Features We use three types of features: Word pairs, production rules from the parse tree, as well as features encoding the lexico-syntactic context at the border between two units of text (Soricut and Marcu, 2003). Our word pairs are lemmatized using the Wordnetbased lemmatizer of NLTK (Loper and Bird, 2002). Figure 1 shows the parse tree for a sentence composed of two discourse units, which serve as arguments of a discourse relation we want to generate a feature vector from. Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets. Surrounded by dots is, for each argument, the minimal set of subparse trees containing strictly all the words of the argument. We first extract all possible lemmatized wordpairs from the two arguments, such as (Mr., when), (decline, ask) or (comment, sale). Next, we extract from left and right argument separately"
D10-1039,P95-1037,0,0.0147493,"t, which did not require us to use thresholding. 3.3 Features We use three types of features: Word pairs, production rules from the parse tree, as well as features encoding the lexico-syntactic context at the border between two units of text (Soricut and Marcu, 2003). Our word pairs are lemmatized using the Wordnetbased lemmatizer of NLTK (Loper and Bird, 2002). Figure 1 shows the parse tree for a sentence composed of two discourse units, which serve as arguments of a discourse relation we want to generate a feature vector from. Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets. Surrounded by dots is, for each argument, the minimal set of subparse trees containing strictly all the words of the argument. We first extract all possible lemmatized wordpairs from the two arguments, such as (Mr., when), (decline, ask) or (comment, sale). Next, we extract from left and right argument separately, all production rules from the sub-parse trees, such as NP 7→ NNP NNP, NNP 7→ “Sherry” or TO 7→ “to”. Finally, we encode in our features three nodes of the parse tree, which capture the local context at the connection point between the two arguments:"
D10-1039,P02-1047,0,0.671765,"Missing"
D10-1039,J93-2004,0,0.0340167,"ales) DT NNS the sales . Argument 2 Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them—lexical heads are indicated between brackets. employed. For the PDTB classifier, we conform to the guidelines of Prasad et al. (2008b, 5): The portion of the corpus corresponding to sections 2–21 of the WSJ is used for training the classifier, while the portion corresponding to WSJ section 23 is used for testing. In order to extract syntactic features, all training and test data are furthermore aligned with their corresponding parse trees in the Penn Treebank (Marcus et al., 1993). Because in the PDTB an instance can be annotated with several discourse relations simultaneously—called ‘senses’ in Prasad et al. (2008b)—for each instance with n senses in the corpus, we create n identical feature vectors, each being labeled by one of the instance’s senses. However, in the RST framework, only one relation is allowed to hold between two EDUs. Consequently, each instance from the RSTDT is labeled with a single discourse relation, from which a single feature vector is created. For RSTDT, we extract 25078 training vectors and 1633 test vectors. For PDTB we extract 49748 trainin"
D10-1039,C08-2022,0,0.322557,"Missing"
D10-1039,P09-1077,0,0.204379,"a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. 1 Introduction Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations mu"
D10-1039,prasad-etal-2008-penn,0,0.614774,"Automatic detection of discourse relations in natural language text is important for numerous tasks in NLP, such as sentiment analysis (Somasundaran et al., 2009), text summarization (Marcu, 2000) and dialogue generation (Piwek et al., 2007). However, most of the recent work employing discourse relation classifiers are based on fully-supervised machine learning approaches (duVerle and Prendinger, 2009; Pitler et al., 2009; Lin et al., 2009). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008a), which are both based on the Wall Street Journal (WSJ) corpus. In the RSTDT, annotation is done using 78 fine-grained discourse relations, which are usually grouped into 18 coarser-grained relations. Each of these relations has furthermore several possible configurations for its arguments—its ‘nuclearity’ (Mann and Thompson, 1988). In practice, a classifier trained on these coarse-grained relations must solve a 41-class classification problem. Some of the relations corresponding to these classes are relatively more frequent in the corpus, such as the E LAB ORATION [N][S] relation (4441 inst"
D10-1039,D09-1018,0,0.052854,"Missing"
D10-1039,N03-1030,0,0.748728,"ssifier. 2 Related Work Since the release in 2001 of the RSTDT corpus, several fully-supervised discourse parsers have been built in the RST framework. In the recent work of duVerle and Prendinger (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. SVMs are employed to train two classifiers: One, binary, for determining the presence of a relation, and another, multi-class, for determining the relation label between related text spans. For the discourse relation classifier, shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. For relation classification, they report an accuracy of 0.668, and an F-score of 0.509 for the creation of the full discourse tree. 400 The unsupervised method of Marcu and Echihabi (2002) was the first that tried to detect implicit relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. For example, the word pair (sell, hold) indicates a C ON TRAST relation. Discourse relation classifiers have also been trained using PDTB. Pitler et"
D10-1039,P06-4018,0,\N,Missing
I05-1055,P03-1069,0,0.590306,"tage. Chronological ordering; ordering sentences according to the published date of the documents they belong to [6], is one solution to this problem. However, showing that this approach is insuﬃcient, Barzilay [1] proposed an reﬁned algorithm which integrates chronology ordering with topical relatedness of documents. Okazaki [7] proposes a improved chronological ordering algorithm using precedence relations among sentences. His algorithm searches for an order which satisﬁes the precedence relations among sentences. In addition to these studies which make use of chronological ordering, Lapata [3] proposes a probabilistic model of text structuring and its application to the sentence ordering. Her system calculates the conditional probabilities between sentences from a corpus and uses a greedy ordering algorithm to arrange sentences according to the conditional probabilities. Even though these previous studies proposed diﬀerent strategies to decide the sentence ordering, the appropriate way to combine these diﬀerent methods to obtain more robust and coherent text remains unknown. In addition to these existing sentence ordering heuristics, we propose a new method which we shall call succ"
I05-1055,P02-1040,0,0.0778618,"arseness of higher order n-grams Pn decreases in an exponential-like curve with n. Therefore, we deﬁne Average Continuity as the logrithmic average of Pn as follows: 1 log(Pn )) 3 n=2 4 Average Continuity = exp( (21) We add a small quantity α to numerator and denominator of Pn in equation 20 so that the logarithm will not diverge when n-grams count is zero. We used α = 0.01 in our evaluations. Experimental results showed that taking n-grams up to four gave contrasting results because the n-grams tend to be sparse for larger n values. BLEU(BiLingual Evaluation Understudy) proposed by Papineni [8] for the task of evaluating machine translations has an analogical form to our average continuity. In BLEU, a machine translation is compared against multiple reference translations and precision values are calculated using word n-grams. BLEU is then deﬁned as the logarithmic average of these precision values. 4 Results We used the 3rd Text Summarization Challenge (TSC) corpus for our experiments. TSC1 corpus contains news articles taken from two leading Japanese newspapers; Mainichi and Yomiuri. TSC-3 corpus contains human selected extracts for 30 diﬀerent topics. However, in the TSC corpus t"
I05-1055,W98-1411,0,\N,Missing
I05-1055,W98-1123,0,\N,Missing
I05-1055,J00-3005,0,\N,Missing
I05-1055,J02-4006,0,\N,Missing
I05-1055,N04-1015,0,\N,Missing
I08-2123,J93-1003,0,0.0884437,"Simplest of all ranking scores is the link frequency (lf ). We define link frequency of an candidate x as the number of different urls in which x and p cooccur. This is exactly the value of k in Table 1. Link frequency is biased towards highly frequent words. A word that has a high frequency in anchor texts can also report a high co-occurrence with p. tfidf measure which is popularly used in information retrieval can be used to normalize this bias. tfidf is computed from Table 1 as follows, tf idf (nj ) = k log N . K +1 From Table 1 we compute co-occurrence measures; log likelihood ratio LLR (Dunning, 1993), chi-squared measure CS, point-wise mutual information PMI (Church and Hanks, 1991) and hyper geometric distribution HG (Hisamitsu and Niwa, 2001). Each of these measures is used to rank candidate aliases of a given name. Because of the limited availability of space, we omit the definitions of these measures. Furthermore, we define popular set overlap measures; cosine measure, overlap coefficient and Dice coefficient from Table 1 as follows, 867 k √ , cosine(p, x) = √ n+ K overlap(p, x) = Dice(p, x) = k , min(n, K) 2k . n+K 3.4 Hub weighting A frequently observed phenomenon on the Web is that"
I08-2123,W04-3230,0,0.0155134,"e prepare a list of words that are commonly used in navigational menus, such as top, last, next, previous, links, etc and remove anchor texts that contain those words. In addition we remove any links that point to pages within the same site. All urls with only one inbound anchor text are removed from the dataset. After the above mentioned processing, the dataset contains 24, 456, 871 anchor texts pointing to 8, 023, 364 urls. The average number of inbound anchor texts per url is 3.05 and its standard deviation is 54.02. We tokenize anchor texts using the Japanese morphological analyzer MeCab (Kudo et al., 2004) and select nouns as nodes in the co-occurrence graph. For training and evaluation purposes we manually assigned aliases for 441 Japanese celebrities. The name-alias dataset covers people from various fields 868 Table 2: Mean Reciprocal Rank Method SVM (RBF) SVM (Linear) SVM (Quad) SVM (Cubic) tfidf(h) LLR(h) cosine(h) lf(h) HG(h) Dice(h) CS(h) MRR 0.5625 0.5186 0.4898 0.4087 0.3957 0.3879 0.3701 0.3677 0.3297 0.2905 0.1186 Method lf cosine tfidf Dice overlap(h) PMI(h) LLR HG CS PMI overlap MRR 0.0839 0.0761 0.0757 0.0751 0.0750 0.0624 0.0604 0.0399 0.0079 0.0072 0.0056 of cinema, sports, poli"
I08-2123,P02-1053,0,0.00822549,"ity of Tokyo 7-3-1, Hongo, Tokyo, 113-8656, Japan danushka@mi.ci.i.utokyo.ac.jp Yutaka Matsuo National Institute of Advanced Industrial Science and Technology 1-18-13, Sotokanda, Tokyo, 101-0021, Japan Mitsuru Ishizuka The University of Tokyo 7-3-1, Hongo, Tokyo, 113-8656, Japan ishizuka@i.utokyo.ac.jp y.matsuo@aist.go.jp Abstract the Web. For example, the famous Japanese major league baseball player Hideki Matsui is often called as Godzilla in web contents. Identifying aliases of a name is important in various tasks such as information retrieval (Salton and McGill, 1986), sentiment analysis (Turney, 2002) and name disambiguation (Bekkerman and McCallum, 2005). A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval, sentiment analysis and name disambiguation. We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts. Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name, its neighboring nodes in the graph are considered as candid"
I08-2123,J90-1003,0,\N,Missing
L18-1033,2014.lilt-9.5,0,0.0658613,"amental requirement for many natural language processing (NLP) tasks. By using accurate word representations, it is possible to improve the performance of downstream NLP applications such as name entity recognition (NER) (Turian et al., 2010), word similarity measurement (Huang et al., 2012), sentiment analysis (Dhillon et al., 2015), word analogy detection (Bollegala et al., 2014), syntactic parsing (Socher et al., 2013) and dependency parsing (Bansal et al., 2014). Moreover, compositional approaches can be used to compute phrase-, sentence- or document-level embeddings from word embeddings (Baroni et al., 2014). Consequently, various methods have been proposed recently that embed words in lower-dimensional dense vector spaces, for example, using word co-occurrence information such as skip-gram with negative sampling (SGNS), continuous bag-of-words model (CBOW) (Mikolov et al., 2013) and Global Vectors (GloVe) (Pennington et al., 2014), to name a few. A common limitation associated with existing predictionbased word embedding learning methods is that they represent each word by a single vector, ignoring the potentially multiple senses of a word. For example, consider the ambiguous word bank that coul"
L18-1033,P12-1015,0,0.019675,"ficient between human similarity ratings and computed cosine similarities. A higher correlation with human similarity ratings implies that the word embeddings learnt by the proposed method accurately capture the semantics of the words. We use several benchmark datasets in our evaluations: WordSim353 (WS, 353 word-pairs) (Finkelstein et al., 2002), Miller-Charles (MC, 30 word-pairs) (Miller and Charles, 1998), rare words dataset (RW, 2034 wordpairs) (Luong et al., 2013), Stanford’s contextual word similarities (SCWS, 2023 word-pairs) (Huang et al., 2012), MEN test collection (3000 word-pairs) (Bruni et al., 2012) and the SimLex-999 (SimLex, 999 word-pairs) (Hill et al., 2016). In Table 3, we compare several word embedding learn1 Setting λ1 = λ2 = λ3 = 10 performed consistently well in our experiments. 226 Words Nearest Neighbours Unique Identifier Nearest Neighbours joint (labelled+unlabelled) corpora (ambigious word) (unlabelled corpus) sense#1 sense#2 sense#3 sense#4 nationalroad forests,woods, regional,local, international, scottish, lane,junction, roads,belief, british,wales international, regional, local, scottish, wales,british, association, institute, government,agency, european, scotland lane,"
L18-1033,N15-1059,0,0.0201815,"induced clusters to learn a fixed number of sense embeddings for each word. In contrast, a nonparametric version of MSSG (NPMSSG) (Neelakantan et al., 2014) estimates the number of senses per word and learn the corresponding sense embeddings. On the other hand, Iacobacci et al. (2015) used a Word Sense Disambiguation (WSD) tool to sense annotate a large text corpus and then used an existing predictionbased word embedding learning method to learn sense and word embeddings with the help of the sense information obtained from the BabelNet (Navigli and Ponzetto, 2010) sense inventory. Similarly, Camacho-Collados et al. (2015) used the knowledge in two different lexical resources: WordNet (Miller, 1995) and Wikipedia. They use the contextual information of a particular concept from Wikipedia and WordNet synsets prior to learning two separate vector representations for each concept. Above-mentioned methods for learning word and sense embeddings require either (a) sense inventories (dictionaries defining the different senses of a word), and (b) word sense taggers that can be applied on unlabelled corpora to generate sense-labelled training data, or (c) manually senseannotated corpora. Unfortunately, such resources ar"
L18-1033,P12-1092,0,0.723852,"proposed method outperform several previously proposed competitive word embedding learning methods on word similarity and short-text classification benchmark datasets. Keywords: Sense Embeddings, Word embeddings, Labelled Data, Unlabelled Data 1. Introduction The ability to accurately represent the meanings of words is a fundamental requirement for many natural language processing (NLP) tasks. By using accurate word representations, it is possible to improve the performance of downstream NLP applications such as name entity recognition (NER) (Turian et al., 2010), word similarity measurement (Huang et al., 2012), sentiment analysis (Dhillon et al., 2015), word analogy detection (Bollegala et al., 2014), syntactic parsing (Socher et al., 2013) and dependency parsing (Bansal et al., 2014). Moreover, compositional approaches can be used to compute phrase-, sentence- or document-level embeddings from word embeddings (Baroni et al., 2014). Consequently, various methods have been proposed recently that embed words in lower-dimensional dense vector spaces, for example, using word co-occurrence information such as skip-gram with negative sampling (SGNS), continuous bag-of-words model (CBOW) (Mikolov et al.,"
L18-1033,P15-1010,0,0.0221945,"SGNG. Unlike SGNG, which updates the gradient of the word vector according to the context, MSSG predicts the nearest sense first, and then updates the gradient of the sense vector. Aforementioned methods apply a form of word sense discrimination by clustering a word contexts, before learning sense-specific word embeddings based on the induced clusters to learn a fixed number of sense embeddings for each word. In contrast, a nonparametric version of MSSG (NPMSSG) (Neelakantan et al., 2014) estimates the number of senses per word and learn the corresponding sense embeddings. On the other hand, Iacobacci et al. (2015) used a Word Sense Disambiguation (WSD) tool to sense annotate a large text corpus and then used an existing predictionbased word embedding learning method to learn sense and word embeddings with the help of the sense information obtained from the BabelNet (Navigli and Ponzetto, 2010) sense inventory. Similarly, Camacho-Collados et al. (2015) used the knowledge in two different lexical resources: WordNet (Miller, 1995) and Wikipedia. They use the contextual information of a particular concept from Wikipedia and WordNet synsets prior to learning two separate vector representations for each conc"
L18-1033,W13-3512,0,0.0320461,"t, we measure the cosine similarity between two words in human similarity benchmarks using their embeddings, and measure Spearman correlation coefficient between human similarity ratings and computed cosine similarities. A higher correlation with human similarity ratings implies that the word embeddings learnt by the proposed method accurately capture the semantics of the words. We use several benchmark datasets in our evaluations: WordSim353 (WS, 353 word-pairs) (Finkelstein et al., 2002), Miller-Charles (MC, 30 word-pairs) (Miller and Charles, 1998), rare words dataset (RW, 2034 wordpairs) (Luong et al., 2013), Stanford’s contextual word similarities (SCWS, 2023 word-pairs) (Huang et al., 2012), MEN test collection (3000 word-pairs) (Bruni et al., 2012) and the SimLex-999 (SimLex, 999 word-pairs) (Hill et al., 2016). In Table 3, we compare several word embedding learn1 Setting λ1 = λ2 = λ3 = 10 performed consistently well in our experiments. 226 Words Nearest Neighbours Unique Identifier Nearest Neighbours joint (labelled+unlabelled) corpora (ambigious word) (unlabelled corpus) sense#1 sense#2 sense#3 sense#4 nationalroad forests,woods, regional,local, international, scottish, lane,junction, roads,"
L18-1033,H93-1061,0,0.379299,"horse, cats,dogs, rat, girl, breed, sheep, horses, boy executive, chief, committee, director, treasurer, secretary, john, vice, officer, turner, superintendent, deputy Table 1: Nearest Neighbours of the learnt sense (two senses) and word embeddings. Figure 1: t-SNE projection of word/sense embeddings. Green labels show the two sense embeddings for dogychairman, whereas yellow and red labels show the nearest neighbours for the two senses. Best viewed in colour. 3.2. Word Similarity To empirically compare the proposed method against prior work, we use ukWaC as the unlabelled corpus and SemCor (Miller et al., 1993) as the sense-tagged corpus, and learn word and sense embeddings using the proposed method. We set the context window to 10 tokens to the right and left of a word in the sentence. We used 5 negative samples for both words lm and senses smg with 0.75 as a uniform sampling rate. The proposed model converged to a solution with 20 training epochs. We used the RubensteinGoodenough (RG, 65 word-pairs) (Rubenstein and Goodenough, 1965) dataset as a validation dataset to tune the hyperparameters λ1 , λ2 and λ3 given in (5). In particular, we vary the values of the coefficients λ1 , λ2 and λ3 and learn"
L18-1033,P10-1023,0,0.0388798,"ore learning sense-specific word embeddings based on the induced clusters to learn a fixed number of sense embeddings for each word. In contrast, a nonparametric version of MSSG (NPMSSG) (Neelakantan et al., 2014) estimates the number of senses per word and learn the corresponding sense embeddings. On the other hand, Iacobacci et al. (2015) used a Word Sense Disambiguation (WSD) tool to sense annotate a large text corpus and then used an existing predictionbased word embedding learning method to learn sense and word embeddings with the help of the sense information obtained from the BabelNet (Navigli and Ponzetto, 2010) sense inventory. Similarly, Camacho-Collados et al. (2015) used the knowledge in two different lexical resources: WordNet (Miller, 1995) and Wikipedia. They use the contextual information of a particular concept from Wikipedia and WordNet synsets prior to learning two separate vector representations for each concept. Above-mentioned methods for learning word and sense embeddings require either (a) sense inventories (dictionaries defining the different senses of a word), and (b) word sense taggers that can be applied on unlabelled corpora to generate sense-labelled training data, or (c) manual"
L18-1033,D14-1113,0,0.128412,"ncial institution or a river-bank. The two senses of bank are significantly different, and embedding both senses to the same point is inadequate. Several solutions have been proposed in the literature to overcome this limitation and learn sense embeddings, which capture the sense related information of words. For example, Reisinger and Mooney (2010) proposed a method for learning sense-specific high dimensional distributional vector representations of words, which was later extended by Huang et al. (2012) using global and local context to learn multiple sense embeddings for an ambiguous word. Neelakantan et al. (2014) proposed multi-sense skip-gram (MSSG), an online cluster-based sense-specific word representations learning method, by extending SGNG. Unlike SGNG, which updates the gradient of the word vector according to the context, MSSG predicts the nearest sense first, and then updates the gradient of the sense vector. Aforementioned methods apply a form of word sense discrimination by clustering a word contexts, before learning sense-specific word embeddings based on the induced clusters to learn a fixed number of sense embeddings for each word. In contrast, a nonparametric version of MSSG (NPMSSG) (Ne"
L18-1033,P04-1035,0,0.0351805,"for evaluating the word embeddings. We followed the same experimental settings used in subsection 3.2. to learn word and sense embeddings using the proposed method. Next, we used the following four binary short-text classification datasets to solve the classification task: customer reviews dataset (CR) (Hu and Liu, 2004) (1494 instances (925 positive and 569 negative)), Stanford sentiment treebank (TR)2 (1806 test instances (903 positive and 903 negative)), movie reviews dataset (MR) (Pang and Lee, 2005) (10662 instances (5331 positive and 5331 negative)), and the subjectivity dataset (SUBJ) (Pang and Lee, 2004) (10000 instances (5000 positive and 5000 negative)). Each review is represented as a bag-of-words. Next, for each bag, we compute the centroid of the embeddings to represent the review. We then train a binary logistic regression classifier with the train data portion of each dataset, and measure the classification accuracy using the corresponding test data portion. Table 4 shows the result of our model against other methods on the short-text classification task. Overall, from Table 4, we can see that the proposed method reports the best performance results for most of the benchmark datasets."
L18-1033,P05-1015,0,0.211643,"learnt by the proposed method against prior work, we used shorttext classification as another extrinsic task for evaluating the word embeddings. We followed the same experimental settings used in subsection 3.2. to learn word and sense embeddings using the proposed method. Next, we used the following four binary short-text classification datasets to solve the classification task: customer reviews dataset (CR) (Hu and Liu, 2004) (1494 instances (925 positive and 569 negative)), Stanford sentiment treebank (TR)2 (1806 test instances (903 positive and 903 negative)), movie reviews dataset (MR) (Pang and Lee, 2005) (10662 instances (5331 positive and 5331 negative)), and the subjectivity dataset (SUBJ) (Pang and Lee, 2004) (10000 instances (5000 positive and 5000 negative)). Each review is represented as a bag-of-words. Next, for each bag, we compute the centroid of the embeddings to represent the review. We then train a binary logistic regression classifier with the train data portion of each dataset, and measure the classification accuracy using the corresponding test data portion. Table 4 shows the result of our model against other methods on the short-text classification task. Overall, from Table 4,"
L18-1033,D14-1162,0,0.0879426,"l., 2015), word analogy detection (Bollegala et al., 2014), syntactic parsing (Socher et al., 2013) and dependency parsing (Bansal et al., 2014). Moreover, compositional approaches can be used to compute phrase-, sentence- or document-level embeddings from word embeddings (Baroni et al., 2014). Consequently, various methods have been proposed recently that embed words in lower-dimensional dense vector spaces, for example, using word co-occurrence information such as skip-gram with negative sampling (SGNS), continuous bag-of-words model (CBOW) (Mikolov et al., 2013) and Global Vectors (GloVe) (Pennington et al., 2014), to name a few. A common limitation associated with existing predictionbased word embedding learning methods is that they represent each word by a single vector, ignoring the potentially multiple senses of a word. For example, consider the ambiguous word bank that could mean either a financial institution or a river-bank. The two senses of bank are significantly different, and embedding both senses to the same point is inadequate. Several solutions have been proposed in the literature to overcome this limitation and learn sense embeddings, which capture the sense related information of words."
L18-1033,N10-1013,0,0.0333319,"w. A common limitation associated with existing predictionbased word embedding learning methods is that they represent each word by a single vector, ignoring the potentially multiple senses of a word. For example, consider the ambiguous word bank that could mean either a financial institution or a river-bank. The two senses of bank are significantly different, and embedding both senses to the same point is inadequate. Several solutions have been proposed in the literature to overcome this limitation and learn sense embeddings, which capture the sense related information of words. For example, Reisinger and Mooney (2010) proposed a method for learning sense-specific high dimensional distributional vector representations of words, which was later extended by Huang et al. (2012) using global and local context to learn multiple sense embeddings for an ambiguous word. Neelakantan et al. (2014) proposed multi-sense skip-gram (MSSG), an online cluster-based sense-specific word representations learning method, by extending SGNG. Unlike SGNG, which updates the gradient of the word vector according to the context, MSSG predicts the nearest sense first, and then updates the gradient of the sense vector. Aforementioned"
L18-1033,P13-1045,0,0.0376876,"classification benchmark datasets. Keywords: Sense Embeddings, Word embeddings, Labelled Data, Unlabelled Data 1. Introduction The ability to accurately represent the meanings of words is a fundamental requirement for many natural language processing (NLP) tasks. By using accurate word representations, it is possible to improve the performance of downstream NLP applications such as name entity recognition (NER) (Turian et al., 2010), word similarity measurement (Huang et al., 2012), sentiment analysis (Dhillon et al., 2015), word analogy detection (Bollegala et al., 2014), syntactic parsing (Socher et al., 2013) and dependency parsing (Bansal et al., 2014). Moreover, compositional approaches can be used to compute phrase-, sentence- or document-level embeddings from word embeddings (Baroni et al., 2014). Consequently, various methods have been proposed recently that embed words in lower-dimensional dense vector spaces, for example, using word co-occurrence information such as skip-gram with negative sampling (SGNS), continuous bag-of-words model (CBOW) (Mikolov et al., 2013) and Global Vectors (GloVe) (Pennington et al., 2014), to name a few. A common limitation associated with existing predictionbas"
L18-1033,P10-1040,0,0.0762865,"word. Moreover, the word embeddings learnt by our proposed method outperform several previously proposed competitive word embedding learning methods on word similarity and short-text classification benchmark datasets. Keywords: Sense Embeddings, Word embeddings, Labelled Data, Unlabelled Data 1. Introduction The ability to accurately represent the meanings of words is a fundamental requirement for many natural language processing (NLP) tasks. By using accurate word representations, it is possible to improve the performance of downstream NLP applications such as name entity recognition (NER) (Turian et al., 2010), word similarity measurement (Huang et al., 2012), sentiment analysis (Dhillon et al., 2015), word analogy detection (Bollegala et al., 2014), syntactic parsing (Socher et al., 2013) and dependency parsing (Bansal et al., 2014). Moreover, compositional approaches can be used to compute phrase-, sentence- or document-level embeddings from word embeddings (Baroni et al., 2014). Consequently, various methods have been proposed recently that embed words in lower-dimensional dense vector spaces, for example, using word co-occurrence information such as skip-gram with negative sampling (SGNS), cont"
L18-1099,W11-1701,0,0.0210315,"t in debates. We differ from this work, since we define a support relation based on structured argumentation using three different components – sentiment, stance and specificity that can also predict entailment. Previously, Grosse et al. (2012) have explored constructing opinion analysis trees that aggregate opinions present in a Twitter dataset based on the specificity property. Our work is not to aggregate opinions but to construct argument structures that are able to persuade an audience towards a particular conclusion. Stance classification (Mohammad et al., 2016; Augenstein et al., 2016; Anand et al., 2011) relates to classifying whether a given statement is for or against a known target, which is explicitly stated or not. Sobhani et al. (2016) investigate the relation between stance and sentiment on a set of Twitter data where the target need not be present explicitly. Ebrahimi et al. (2016) propose a model that integrates stance, sentiment and target features jointly as a three way interaction for classifying stance in a set of tweets. We use sentiment as a way of identifying stance present in opinions where the target is explicitly present. Also, we are interested in how the stance is express"
L18-1099,D16-1084,0,0.0167636,"between arguments present in debates. We differ from this work, since we define a support relation based on structured argumentation using three different components – sentiment, stance and specificity that can also predict entailment. Previously, Grosse et al. (2012) have explored constructing opinion analysis trees that aggregate opinions present in a Twitter dataset based on the specificity property. Our work is not to aggregate opinions but to construct argument structures that are able to persuade an audience towards a particular conclusion. Stance classification (Mohammad et al., 2016; Augenstein et al., 2016; Anand et al., 2011) relates to classifying whether a given statement is for or against a known target, which is explicitly stated or not. Sobhani et al. (2016) investigate the relation between stance and sentiment on a set of Twitter data where the target need not be present explicitly. Ebrahimi et al. (2016) propose a model that integrates stance, sentiment and target features jointly as a three way interaction for classifying stance in a set of tweets. We use sentiment as a way of identifying stance present in opinions where the target is explicitly present. Also, we are interested in how"
L18-1099,W14-2107,0,0.04986,"Missing"
L18-1099,P12-2041,0,0.0319387,"2012; Villalba and Saint-Dizier, 2012). Using computational argumentation techniques to deal with real-world problems such as opinion mining (Dragoni et al., 2016), sentiment analysis (Rajendran et al., 2016) and detecting deceptive reviews (Cocarascu and Toni, 2016) has been tackled so far. Boltuzic et al. (2014) combine stance, textual entailment and semantic similarity to identify relations between arguments and comments presented in a debate. We propose a way of detecting support-based entailment such that a specific opinion supports as well as entails a corresponding generalised opinion. Cabrio and Villata (2012) consider entailment to be a form of support relation that occurs between arguments present in debates. We differ from this work, since we define a support relation based on structured argumentation using three different components – sentiment, stance and specificity that can also predict entailment. Previously, Grosse et al. (2012) have explored constructing opinion analysis trees that aggregate opinions present in a Twitter dataset based on the specificity property. Our work is not to aggregate opinions but to construct argument structures that are able to persuade an audience towards a part"
L18-1099,C16-1250,0,0.0199151,"trees that aggregate opinions present in a Twitter dataset based on the specificity property. Our work is not to aggregate opinions but to construct argument structures that are able to persuade an audience towards a particular conclusion. Stance classification (Mohammad et al., 2016; Augenstein et al., 2016; Anand et al., 2011) relates to classifying whether a given statement is for or against a known target, which is explicitly stated or not. Sobhani et al. (2016) investigate the relation between stance and sentiment on a set of Twitter data where the target need not be present explicitly. Ebrahimi et al. (2016) propose a model that integrates stance, sentiment and target features jointly as a three way interaction for classifying stance in a set of tweets. We use sentiment as a way of identifying stance present in opinions where the target is explicitly present. Also, we are interested in how the stance is expressed and use this as a feature to identify support-based entailment relation. Textual entailment deals with identifying whether a hypothesis can be inferred from a given text, which is directional and differs from semantic similarity measures. Yokote et al. (2012) propose a model that transfo"
L18-1099,W07-1401,0,0.223677,"Missing"
L18-1099,P14-5008,0,0.0475515,"Missing"
L18-1099,marelli-etal-2014-sick,0,0.0863313,"Missing"
L18-1099,N16-1165,0,0.031543,"Missing"
L18-1099,L16-1623,0,0.0202213,"rt relation that occurs between arguments present in debates. We differ from this work, since we define a support relation based on structured argumentation using three different components – sentiment, stance and specificity that can also predict entailment. Previously, Grosse et al. (2012) have explored constructing opinion analysis trees that aggregate opinions present in a Twitter dataset based on the specificity property. Our work is not to aggregate opinions but to construct argument structures that are able to persuade an audience towards a particular conclusion. Stance classification (Mohammad et al., 2016; Augenstein et al., 2016; Anand et al., 2011) relates to classifying whether a given statement is for or against a known target, which is explicitly stated or not. Sobhani et al. (2016) investigate the relation between stance and sentiment on a set of Twitter data where the target need not be present explicitly. Ebrahimi et al. (2016) propose a model that integrates stance, sentiment and target features jointly as a three way interaction for classifying stance in a set of tweets. We use sentiment as a way of identifying stance present in opinions where the target is explicitly present. Also,"
L18-1099,W14-2105,0,0.0251144,"computational argumentation, an argument can be defined as a collection of premises together (linked argument) or individually (convergent argument) which are related to a conclusion (Palau and Moens, 2009). Each premise provides a support in the form of logical reasoning for, or evidence in support of, the conclusion to which it is connected. It has been suggested that, in natural language texts, this support relation can be interpreted as meaning either (a) one premise is inferred from another premise (Janier et al., 2014) or (b) one premise provides evidence that supports another premise (Park and Cardie, 2014). In either case, it is natural to interpret the relationship as a form of entailment. In this paper, we consider a subtype of entailment, which we term support-based entailment, where a support relation exists between the text and the hypothesis. Despite the unstructured nature of natural language texts, they provide meta-linguistic attributes such as stance, sentiment, and specificity that can be exploited for detecting supportbased entailment. We create a dataset of text-hypothesis pairs from opinions collected from a set of hotel reviews where the text provides support to the corresponding"
L18-1099,D14-1162,0,0.0798714,"Missing"
L18-1099,S16-2021,0,0.0202598,"nts – sentiment, stance and specificity that can also predict entailment. Previously, Grosse et al. (2012) have explored constructing opinion analysis trees that aggregate opinions present in a Twitter dataset based on the specificity property. Our work is not to aggregate opinions but to construct argument structures that are able to persuade an audience towards a particular conclusion. Stance classification (Mohammad et al., 2016; Augenstein et al., 2016; Anand et al., 2011) relates to classifying whether a given statement is for or against a known target, which is explicitly stated or not. Sobhani et al. (2016) investigate the relation between stance and sentiment on a set of Twitter data where the target need not be present explicitly. Ebrahimi et al. (2016) propose a model that integrates stance, sentiment and target features jointly as a three way interaction for classifying stance in a set of tweets. We use sentiment as a way of identifying stance present in opinions where the target is explicitly present. Also, we are interested in how the stance is expressed and use this as a feature to identify support-based entailment relation. Textual entailment deals with identifying whether a hypothesis c"
L18-1099,C14-1053,0,0.0226294,"tion or six inclusion rules (b) Non-SER — TH pairs that do not satisfy any of the 12 rules. (c) Subsumption and Inclusion — TH pairs satisfying each individual rule and (d) Random sentiment — assigning sentiment of opinions present in TH pairs of SER and Non-SER randomly. Accuracy is reported. 3. Unannotated (UA) Reviews from 30 different hotels that are unannotated and not present in the ArguAna corpus are used. Here, the reviews are unbalanced. For each statement, local sentiment is automatically classfied as positive, negative or objective using the SVM-based classfier used in the ArguAna (Wachsmuth et al., 2014a) tool. All statements predicted as positive or negative were considered as opinions. We extract a list of aspects manually annotated in the ArguAna corpus and use this to identify aspects present in opinions. The opinions are automatically classfied as implicit or explicit as mentioned for the previous dataset. There are 564 explicit opinions and 5933 implicit opinions present. The SER rules predicted 16314 TH pairs with support-based entailment. 6. 6.1. Experiments and Results Performance of SER In each of the above datasets, we predicted support-based entailment relation using the SER and"
L18-1099,W05-1207,0,0.0621876,"y interaction for classifying stance in a set of tweets. We use sentiment as a way of identifying stance present in opinions where the target is explicitly present. Also, we are interested in how the stance is expressed and use this as a feature to identify support-based entailment relation. Textual entailment deals with identifying whether a hypothesis can be inferred from a given text, which is directional and differs from semantic similarity measures. Yokote et al. (2012) propose a model that transforms similarity measures into a non-linear transformation for predicting textual entailment. Zanzotto et al. (2005) investigate on identifying patterns based on subject verb relation to identifying entailment. In their paper, they argue that the logical entailment present between the text and hypothesis is not captured properly. In contrast, we are interested in a subtype of entailment that can predict the support relation based on argumentation theory. 3. Support-based Entailment The three components of the proposed method are explained below. Based on these, we manually identify a set of support-based entailment rules (SER) for predicting the support-based entailment between a text (T) and a hypothesis ("
L18-1246,P04-1054,0,0.132958,"raction. Comparing the results of the present work on iner-sentence relation extraction with previous work on intra-sentence relation extraction, the study suggests the need for more sophisticated models to handle long-range information between entities across sentences. Keywords: Inter-sentence Relation Extraction, Relation Extraction, Inter-sentence Relation Extraction Dataset, Distant Supervision for Inter-sentence Relation Extraction 1. Introduction In recent times, the field of relation extraction has received significant research attention due to its importance in information retrieval (Culotta and Sorensen, 2004; Mintz et al., 2009; Banko et al., 2007; Etzioni et al., 2011). The key task in relation extraction is to recognise the semantic relation that exist between two given entities. Depending on the scope of the co-occurrences of the two entities, relation extraction methods can be broadly categorised into two groups: (a) intra-sentence relation extraction methods (Brin, 1998; Banko et al., 2007; Mintz et al., 2009; Riedel et al., 2010), and (b) inter-sentence relation extraction methods (Swampillai and Stevenson, 2010; Gu et al., 2017; Quirk and Poon, 2016; Peng et al., 2017). While intra-sentenc"
L18-1246,dsouza-ng-2014-annotating,0,0.0309308,"Missing"
L18-1246,P05-1053,0,0.063446,"tion mentions for each of the 17 relations. b. Present performance of baseline models such as the bag-of-words model and sequence-based neural network models on the developed dataset. 2. Related Work The related work for the present study can be grouped into the following three strands: Intra-sentence relation extraction. Mintz et al. (2009) identify atleast three paradigms applied for the task of intrasentence relation extraction. These are: (a) supervised learning approaches focussing on creating hand-labeled data and experimenting with a variety of lexical, syntactic and sematnic features (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007); (b) unsupervised learning methods aiming to cluster strings of words exracted from large collections of text (Shinyama and Sekine, 2006; Banko et al., 2007); and (c) bootstrapping methods employing small seed sets that focus on pattern-based relation extraction (Brin, 1998; Riloff et al., 1999). Recently, deep learning models such as CNN (Zeng et al., 2014; Santos et al., 2015; Xu et al., 2015a), Recurrent neural networks based models such as LSTM model (Miwa and Bansal, 2016; Xu et al., 2015b) and BiLSTM model (Wu et al., 2017) are shown to be quite useful for"
L18-1246,P16-1101,0,0.0381162,"hrough time (Rumelhart et al., 1988), usually results in the vanishing gradient problem (Bengio et al., 1994), wherein the gradient propagated through the network over time either decays or grows exponentially. The Long Short-Term Memory (LSTM) model was proposed to overcome the vanishing gradient by regulating the information in a cell state using input, output and forget gates and thereby learn long-term dependencies (Hochreiter and Schmidhuber, 1997). Recently, LSTM and BidirectionalLSTM (BiLSTM) models have also been successfully applied for the task of intra-sentence relation extraction (Ma and Hovy, 2016). Inter-sentence relation classification can be considered as a sequence classification problem, with the task to predict the relation given the sequence of words across the sentences. Theoretically, LSTM should be helpful for inter-sentence relation extraction, with its capability of handling long-term dependencies from long sequences of words. Further, while LSTM model captures the context only in the forward direction, BiLSTM models process the data in both directions with two separate hidden layers, which are then provided to the output layer. Accordingly, the following LSTM-based models a"
L18-1246,P09-1113,0,0.256658,"Missing"
L18-1246,P16-1105,0,0.0180575,"eled data and experimenting with a variety of lexical, syntactic and sematnic features (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007); (b) unsupervised learning methods aiming to cluster strings of words exracted from large collections of text (Shinyama and Sekine, 2006; Banko et al., 2007); and (c) bootstrapping methods employing small seed sets that focus on pattern-based relation extraction (Brin, 1998; Riloff et al., 1999). Recently, deep learning models such as CNN (Zeng et al., 2014; Santos et al., 2015; Xu et al., 2015a), Recurrent neural networks based models such as LSTM model (Miwa and Bansal, 2016; Xu et al., 2015b) and BiLSTM model (Wu et al., 2017) are shown to be quite useful for intra-sentence relation extraction. Inter-sentence relation extraction. As explained in the previous section, several studies have focussed on relation extraction across sentences due to its contribution to th eoverall task of relation extraction. Further, comparing intra-sentence and inter-sentence features for clinical research relationship extraction, Roberts et al. (2008) show that intra-sentence features are not very useful for intersentence relation extraction. Swampillai and Stevenson (2010) employed"
L18-1246,I13-1189,0,0.0599858,"Missing"
L18-1246,Q17-1008,0,0.330005,"n retrieval (Culotta and Sorensen, 2004; Mintz et al., 2009; Banko et al., 2007; Etzioni et al., 2011). The key task in relation extraction is to recognise the semantic relation that exist between two given entities. Depending on the scope of the co-occurrences of the two entities, relation extraction methods can be broadly categorised into two groups: (a) intra-sentence relation extraction methods (Brin, 1998; Banko et al., 2007; Mintz et al., 2009; Riedel et al., 2010), and (b) inter-sentence relation extraction methods (Swampillai and Stevenson, 2010; Gu et al., 2017; Quirk and Poon, 2016; Peng et al., 2017). While intra-sentence relation extraction attempts to extract relations between two entities that co-occur within the same sentence, inter-sentence relation extraction methods consider entities that might not necessarily co-occur in the same sentence. In more detail, the distinction between intra and intersentence relation extraction tasks can be illustrated as follows. Let us assume that a relation r takes e1 as the first argument and e2 as the second argument. Further, let us also assume that e1 is included in a sentence si and e2 is included in a sentence sj . Then, we define intra-sentenc"
L18-1246,W08-0602,0,0.013708,"ls such as CNN (Zeng et al., 2014; Santos et al., 2015; Xu et al., 2015a), Recurrent neural networks based models such as LSTM model (Miwa and Bansal, 2016; Xu et al., 2015b) and BiLSTM model (Wu et al., 2017) are shown to be quite useful for intra-sentence relation extraction. Inter-sentence relation extraction. As explained in the previous section, several studies have focussed on relation extraction across sentences due to its contribution to th eoverall task of relation extraction. Further, comparing intra-sentence and inter-sentence features for clinical research relationship extraction, Roberts et al. (2008) show that intra-sentence features are not very useful for intersentence relation extraction. Swampillai and Stevenson (2010) employed features drawn from combining parse trees of sentences for extracting relations across sentences in the MUC6 dataset. Targeting inter-sentence time-event relation extraction, Moschitti et al. (2013) proposed an SVM-model using tree kernels, whichwere evaluated on Machine Reading Program (MRP) and TimeBank datasets. Tree kernels are also shown to be useful for inter-sentence relation extraction in the Chemical-Induced-Disease domain (Nagesh, 2016). More recently"
L18-1246,P15-1061,0,0.144684,"e relation extraction. These are: (a) supervised learning approaches focussing on creating hand-labeled data and experimenting with a variety of lexical, syntactic and sematnic features (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007); (b) unsupervised learning methods aiming to cluster strings of words exracted from large collections of text (Shinyama and Sekine, 2006; Banko et al., 2007); and (c) bootstrapping methods employing small seed sets that focus on pattern-based relation extraction (Brin, 1998; Riloff et al., 1999). Recently, deep learning models such as CNN (Zeng et al., 2014; Santos et al., 2015; Xu et al., 2015a), Recurrent neural networks based models such as LSTM model (Miwa and Bansal, 2016; Xu et al., 2015b) and BiLSTM model (Wu et al., 2017) are shown to be quite useful for intra-sentence relation extraction. Inter-sentence relation extraction. As explained in the previous section, several studies have focussed on relation extraction across sentences due to its contribution to th eoverall task of relation extraction. Further, comparing intra-sentence and inter-sentence features for clinical research relationship extraction, Roberts et al. (2008) show that intra-sentence feature"
L18-1246,N06-1039,0,0.0379589,"e developed dataset. 2. Related Work The related work for the present study can be grouped into the following three strands: Intra-sentence relation extraction. Mintz et al. (2009) identify atleast three paradigms applied for the task of intrasentence relation extraction. These are: (a) supervised learning approaches focussing on creating hand-labeled data and experimenting with a variety of lexical, syntactic and sematnic features (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007); (b) unsupervised learning methods aiming to cluster strings of words exracted from large collections of text (Shinyama and Sekine, 2006; Banko et al., 2007); and (c) bootstrapping methods employing small seed sets that focus on pattern-based relation extraction (Brin, 1998; Riloff et al., 1999). Recently, deep learning models such as CNN (Zeng et al., 2014; Santos et al., 2015; Xu et al., 2015a), Recurrent neural networks based models such as LSTM model (Miwa and Bansal, 2016; Xu et al., 2015b) and BiLSTM model (Wu et al., 2017) are shown to be quite useful for intra-sentence relation extraction. Inter-sentence relation extraction. As explained in the previous section, several studies have focussed on relation extraction acro"
L18-1246,D12-1042,0,0.0405638,"The process of creating inter-sentence relation extraction dataset is described in this section. 3.1. Approach In the past, Freebase relations have been successfully used for examining relation extraction (Mintz et al., 2009; Bordes et al., 2013; Wanf et al., 2014). The initial work on using distant supervision for relation extraction was proposed by Mintz et al. (2009). The authors developed a large dataset comprising 1.8 million instances using 102 Freebase relations, connecting 940,000 entities. Since then the dataset has been extensively used for evaluation purposes (Riedel et al., 2010; Surdeanu et al., 2012). Thus, given the usefulness of the dataset developed by Mintz et al. (2009), this study proposes to use this resource (102 Freebase relations) for developing a benchmark dataset for inter-sentence relation extraction. Using this resource, 1560 Relation american/football/football/position/players architecture/structure/architect automotive/model/year/body/styles automotive/model/year/engines automotive/model/year/exterior/colors automotive/model/year/make automotive/model/year/model automotive/model/year/next/model/year automotive/model/year/previous/model/year automotive/model/year/transmissi"
L18-1246,swampillai-stevenson-2010-inter,0,0.336062,"ived significant research attention due to its importance in information retrieval (Culotta and Sorensen, 2004; Mintz et al., 2009; Banko et al., 2007; Etzioni et al., 2011). The key task in relation extraction is to recognise the semantic relation that exist between two given entities. Depending on the scope of the co-occurrences of the two entities, relation extraction methods can be broadly categorised into two groups: (a) intra-sentence relation extraction methods (Brin, 1998; Banko et al., 2007; Mintz et al., 2009; Riedel et al., 2010), and (b) inter-sentence relation extraction methods (Swampillai and Stevenson, 2010; Gu et al., 2017; Quirk and Poon, 2016; Peng et al., 2017). While intra-sentence relation extraction attempts to extract relations between two entities that co-occur within the same sentence, inter-sentence relation extraction methods consider entities that might not necessarily co-occur in the same sentence. In more detail, the distinction between intra and intersentence relation extraction tasks can be illustrated as follows. Let us assume that a relation r takes e1 as the first argument and e2 as the second argument. Further, let us also assume that e1 is included in a sentence si and e2 i"
L18-1246,D15-1062,0,0.364065,". These are: (a) supervised learning approaches focussing on creating hand-labeled data and experimenting with a variety of lexical, syntactic and sematnic features (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007); (b) unsupervised learning methods aiming to cluster strings of words exracted from large collections of text (Shinyama and Sekine, 2006; Banko et al., 2007); and (c) bootstrapping methods employing small seed sets that focus on pattern-based relation extraction (Brin, 1998; Riloff et al., 1999). Recently, deep learning models such as CNN (Zeng et al., 2014; Santos et al., 2015; Xu et al., 2015a), Recurrent neural networks based models such as LSTM model (Miwa and Bansal, 2016; Xu et al., 2015b) and BiLSTM model (Wu et al., 2017) are shown to be quite useful for intra-sentence relation extraction. Inter-sentence relation extraction. As explained in the previous section, several studies have focussed on relation extraction across sentences due to its contribution to th eoverall task of relation extraction. Further, comparing intra-sentence and inter-sentence features for clinical research relationship extraction, Roberts et al. (2008) show that intra-sentence features are not very us"
L18-1246,D15-1206,0,0.31055,". These are: (a) supervised learning approaches focussing on creating hand-labeled data and experimenting with a variety of lexical, syntactic and sematnic features (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007); (b) unsupervised learning methods aiming to cluster strings of words exracted from large collections of text (Shinyama and Sekine, 2006; Banko et al., 2007); and (c) bootstrapping methods employing small seed sets that focus on pattern-based relation extraction (Brin, 1998; Riloff et al., 1999). Recently, deep learning models such as CNN (Zeng et al., 2014; Santos et al., 2015; Xu et al., 2015a), Recurrent neural networks based models such as LSTM model (Miwa and Bansal, 2016; Xu et al., 2015b) and BiLSTM model (Wu et al., 2017) are shown to be quite useful for intra-sentence relation extraction. Inter-sentence relation extraction. As explained in the previous section, several studies have focussed on relation extraction across sentences due to its contribution to th eoverall task of relation extraction. Further, comparing intra-sentence and inter-sentence features for clinical research relationship extraction, Roberts et al. (2008) show that intra-sentence features are not very us"
L18-1246,C14-1220,0,0.0583214,"Missing"
N07-1043,P98-1012,0,0.365202,"r to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 categories : tennis players, golfers, actors, politicians and scientists, (10 names from each category) from the dmoz directory 7 . For each pair of names in our dataset, we measure the association between the two names using the proposed method and baselines. We use group-average agglomerative hierarchical clustering to cluster the names in our dataset into five clusters. We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. As summarized in Table 5 the proposed method outperforms all the baselines with a statistically significant (p ≤ 0.01 Tukey HSD) F score of 0.7897. 5 Conclusion We propose an SVM-based approach to combine page counts and lexico-syntactic patterns extracted from snippets to leverage a robust web-based semantic similarity measure. The proposed similarity measure outperforms existing web-based similarity measures and competes with models trained on WordNet. It requires just 2500 synonymous word-pairs, automatically extracted from WordNet synsets, for training."
N07-1043,P06-1127,0,0.128597,"eb hits for each individual name and their conjunction. Sahami et al., (2006) measure semantic similarity between two queries using the snippets returned for those queries by a search engine. For each query, they collect snippets from a search engine and represent each snippet as a TF-IDF weighted term vector. Each vector is L2 normalized and the centroid of the set of vectors is computed. Semantic similarity between two queries is then defined as the inner product between the corresponding centroid vectors. They do not compare their similarity measure with taxonomy based similarity measures. Chen et al., (2006) propose a web-based doublechecking model to compute semantic similarity between words. For two words P and Q, they collect snippets for each word from a web search engine. Then they count the number of occurrences of word P in the snippets for word Q and the number of occurrences of word Q in the snippets for word P . These values are combined non-linearly to compute the similarity between P and Q. This method heavily depends on the search engine’s ranking algorithm. Although two words P and Q may be very similar, there is no reason to believe that one can find Q in the snippets for P , or vi"
N07-1043,J03-3002,0,0.0444876,"uses of existing words. For example, apple is frequently associated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible. The Web can be regarded as a large-scale, dynamic corpus of text. Regarding the Web as a live corpus has become an active research topic recently. Simple, unsupervised models have shown to perform better when n-gram counts are obtained from the Web rather than from a large corpus (Keller and Lapata, 2003; Lapata and Keller, 2005). Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Turney (2001) defines a point wise mutual information (PMI-IR) measure using the number of hits returned by a Web search engine to recognize synonyms. Matsuo et. al, (2006b) follows a similar 1 http://wordnet.princeton.edu/ 340 Proceedings of NAACL HLT 2007, pages 340–347, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics approach to measure the similarity between words and apply their method in a graph-based word clustering algorithm. Due to the huge number of documents and"
N07-1043,W02-1029,0,0.0141278,"significantly improves the accuracy (F measure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product names) and the new uses of existing words. For example, apple is frequently associat"
N07-1043,C92-2082,0,0.0408974,"hat contain the query words 2 . A snippet is a brief window of text extracted by a search engine around the query term in a document. Snippets provide useful information about the immediate context of the query term. This paper proposes a Web-based semantic similarity metric which combines page counts and snippets using support vector machines. We extract lexico-syntactic patterns from snippets. For example, X is a Y indicates there is a high semantic similarity between X and Y. Automatically extracted lexico-syntactic patterns have been successfully employed in various term extraction tasks (Hearst, 1992). Our contributions are summarized as follows: • We propose a lexico-syntactic patterns-based approach to compute semantic similarity using snippets obtained from a Web search engine. • We integrate different Web-based similarity scores using WordNet synsets and support vector machines to create a robust semantic similarity measure. The integrated measure outperforms all existing Web-based semantic similarity measures in a benchmark dataset and a named entity clustering task. To the best of our knowledge, this is the first attempt to combine both WordNet synsets and Web content to leverage a r"
N07-1043,J03-3005,0,0.0380318,"l names, location names, product names) and the new uses of existing words. For example, apple is frequently associated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible. The Web can be regarded as a large-scale, dynamic corpus of text. Regarding the Web as a live corpus has become an active research topic recently. Simple, unsupervised models have shown to perform better when n-gram counts are obtained from the Web rather than from a large corpus (Keller and Lapata, 2003; Lapata and Keller, 2005). Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Turney (2001) defines a point wise mutual information (PMI-IR) measure using the number of hits returned by a Web search engine to recognize synonyms. Matsuo et. al, (2006b) follows a similar 1 http://wordnet.princeton.edu/ 340 Proceedings of NAACL HLT 2007, pages 340–347, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics approach to measure the similarity between words and apply their method in a graph-based word clustering"
N07-1043,P98-2127,0,0.263353,"eover, the proposed semantic similarity measure significantly improves the accuracy (F measure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product names) and the new uses of exist"
N07-1043,N03-1032,0,0.0756057,"Q in the snippets for P , or vice versa. This observation is confirmed by the experimental results in their paper which reports 0 similarity scores for many pairs of words in the Miller and Charles (1998) data set. 3 Method In this section we will describe the various similarity features we use in our model. We utilize page counts and snippets returned by the Google 3 search engine for simple text queries to define various similarity scores. 3.1 Page Counts-based Similarity Scores For the rest of this paper we use the notation H(P ) to denote the page count for the query P in a search engine. Terra and Clarke (2003) compare various similarity scores for measuring similarity between words in a corpus. We modify the traditional Jaccard, overlap (Simpson), Dice and PMI measures for the purpose of measuring similarity using page counts. WebJaccard coefficient between words (or phrases) P and Q, WebJaccard(P, Q), is defined by, WebJaccard(P, Q) ( 0 = H(P ∩Q) H(P )+H(Q)−H(P ∩Q) if H(P ∩ Q) ≤ c (1). otherwise Here, P ∩ Q denotes the conjunction query P AND Q. Given the scale and noise in the Web, some words might occur arbitrarily, i.e. by random chance, on some pages. Given the scale and noise in web data, it"
N07-1043,W06-1664,1,0.921358,"d using the Brown corpus. Li et al., (2003) combines structural semantic information from a lexical taxonomy and information content from a corpus in a non-linear model. They propose a similarity measure that uses shortest path length, depth and local density in a taxonomy. Their experiments using WordNet and the Brown corpus reports a Pearson correlation coefficient of 0.8914 on the Miller and Charles’ (1998) benchmark dataset. They do not evaluate their method on similarities between named entities. Recently, some work has been carried out on measuring semantic similarity using web content. Matsuo et al., (2006a) propose the use of Web hits for the extraction of communities on the Web. They measure the association between two personal names using the overlap coefficient, calculated based on the number of Web hits for each individual name and their conjunction. Sahami et al., (2006) measure semantic similarity between two queries using the snippets returned for those queries by a search engine. For each query, they collect snippets from a search engine and represent each snippet as a TF-IDF weighted term vector. Each vector is L2 normalized and the centroid of the set of vectors is computed. Semantic"
N07-1043,O97-1002,0,\N,Missing
N07-1043,C98-1012,0,\N,Missing
N18-2005,N18-1175,0,0.0248186,"mportant to understand whether the content that is left implicit in natural language texts are to be dealt as enthymemes or not. In our earlier work (Rajendran et al., 2016b), we propose an approach for reconstructing structures similar to enthymemes in opinions that are present in online reviews. However, the annotated dataset used in our approach was small and not useful for deep learning models. Recent work in argument mining is able to achieve better performance for the argument identification task using neural network models with the availability of a large corpus of annotated arguments (Habernal et al., 2018; Eger et al., 2017). Annotating a large corpus by hand is a tedious task and little existing work in argument mining has explored alternative ways to We report experiments that are carried out using two different approaches — weakly-supervised and semi-supervised learning (Section. 4). In the weakly-supervised approach, we randomly divide the manually annotated implicit/explicit opinions into different training sets that are used to train SVM classifiers for automatically labelling unannotated opinions. The unannotated opinions are labelled based on different voting criteria — Fully-Strict, P"
N18-2005,W14-2107,0,0.0244494,"iversity of Liverpool1 , King’s College London2 Abstract An argument can be defined in two different ways — (1) abstract arguments which do not refer to any internal structure (Dung, 1995) and (2) structured arguments where an argument is a collection of premises leading to a conclusion. One major problem that is faced by argument mining researchers is the variation in the definition of an argument, which is highly dependent on the data at hand. Previous work in argument mining has mostly focussed on a particular domain (Grosse et al., 2015; Villalba and SaintDizier, 2012; Ghosh et al., 2014; Boltuzic and Snajder, 2014; Park and Cardie, 2014; Cabrio and Villata, 2012). Furthermore, an argument can be defined in a variety of ways depending on the problem being solved. As a result, we focus on the specific domain of opinionated texts such as those found in online reviews. Prior work (Carstens et al., 2014; Rajendran et al., 2016a) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models. However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requiring us to manually annotate ar"
N18-2005,L16-1200,0,0.068182,"Missing"
N18-2005,N16-1165,0,0.0126448,"omain of opinionated texts such as those found in online reviews. Prior work (Carstens et al., 2014; Rajendran et al., 2016a) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models. However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requiring us to manually annotate argument components such that they can be used by supervised learning techniques. Because of the heterogenous nature of user-based content, this labelling task is time-consuming and expensive (Khatib et al., 2016; Habernal and Gurevych, 2015) and often domain-dependent. Here, we are interested in analysing the problem of using supervised learning where the quantity of human-annotated or labelled data is small, and investigating how this issue can be handled by using weakly-supervised and semi-supervised techniques. We build on our prior work (Rajendran et al., 2016b), which created a small manually annotated dataset for the supervised binary classification of opinions present in online reviews, based Online reviews have become a popular portal among customers making decisions about purchasing products"
N18-2005,P12-2041,0,0.208199,"tract An argument can be defined in two different ways — (1) abstract arguments which do not refer to any internal structure (Dung, 1995) and (2) structured arguments where an argument is a collection of premises leading to a conclusion. One major problem that is faced by argument mining researchers is the variation in the definition of an argument, which is highly dependent on the data at hand. Previous work in argument mining has mostly focussed on a particular domain (Grosse et al., 2015; Villalba and SaintDizier, 2012; Ghosh et al., 2014; Boltuzic and Snajder, 2014; Park and Cardie, 2014; Cabrio and Villata, 2012). Furthermore, an argument can be defined in a variety of ways depending on the problem being solved. As a result, we focus on the specific domain of opinionated texts such as those found in online reviews. Prior work (Carstens et al., 2014; Rajendran et al., 2016a) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models. However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requiring us to manually annotate argument components such that they can be used by su"
N18-2005,I13-1052,0,0.0305599,"which is accurate enough to be useful for deep-learning methods. The dataset is publicly available at https://goo.gl/Bym2Vz. We conduct a second experiment to test the combination of both labelled (1244 opinions) and unlabelled (4931 opinions) data using the following popular semi-supervised learning methods. Self-training method We train an SVM using the labelled data D and use this to annotate the unannotated data U . The annotated opinions from U which are labelled with the highest probability are then added to D. This process is repeated m times. Reserved method Here we use the method of Liu et al. (2013), where a portion of the training data R is reserved, and the remainder is used for training the SVM. The resulting classifier is run on the combination of U and R. The annotated opinions from U with the highest probability and the opinions from R that have the lowest probability of having a correct label generated by the SVM are appended to the training dataset. This operation is repeated m times. We chose 222 explicit opinions and 287 implicit opinions as the training data, and took 273 explicit opinions and 462 implicit opinions as the reserved portion. After the final iteration, the final"
N18-2005,N03-1023,0,0.0326572,"eature in the classifier. |S| 1 X v= si |S| Self-training Reserved Size Accuracy Size Accuracy 22 2110 2574 3600 3613 4931 49.43 80.86 81.83 82.71 82.71 82.71 511 1717 2194 3152 3708 4931 67.68 68.24 70.25 70.98 68.81 64.22 Table 3: Accuracy of the LSTM model on annotated data using a set of automatically labelled unannotated opinions of Size. Voting-Based Again, each training set T1 , T2 and T3 is used to train separate SVM classifiers, which are used to label the unlabelled opinions, giving corresponding annotated opinion sets U1 , U2 and U3 . We then followed an approach that is similar to Ng and Cardie (2003) to combine the opinions in U1 , U2 and U3 into a single set, denoted by UF , using the following voting criteria: Fully-Strict An opinion is included in UF if all three SVM classifiers predict the same stance label. Partially-Strict An opinion is included in UF if all three SVM classifiers identify it as explicit, or if at least two of them classify it as implicit. No-Strict An opinion is included in UF as implicit if at least one of the classifiers predict it to be implicit, otherwise it is included in UF as explicit. UF was then used to train an LSTM classifier and this was tested on the or"
N18-2005,P17-1002,0,0.0222756,"whether the content that is left implicit in natural language texts are to be dealt as enthymemes or not. In our earlier work (Rajendran et al., 2016b), we propose an approach for reconstructing structures similar to enthymemes in opinions that are present in online reviews. However, the annotated dataset used in our approach was small and not useful for deep learning models. Recent work in argument mining is able to achieve better performance for the argument identification task using neural network models with the availability of a large corpus of annotated arguments (Habernal et al., 2018; Eger et al., 2017). Annotating a large corpus by hand is a tedious task and little existing work in argument mining has explored alternative ways to We report experiments that are carried out using two different approaches — weakly-supervised and semi-supervised learning (Section. 4). In the weakly-supervised approach, we randomly divide the manually annotated implicit/explicit opinions into different training sets that are used to train SVM classifiers for automatically labelling unannotated opinions. The unannotated opinions are labelled based on different voting criteria — Fully-Strict, Partially-Strict and"
N18-2005,W14-2105,0,0.0259132,"g’s College London2 Abstract An argument can be defined in two different ways — (1) abstract arguments which do not refer to any internal structure (Dung, 1995) and (2) structured arguments where an argument is a collection of premises leading to a conclusion. One major problem that is faced by argument mining researchers is the variation in the definition of an argument, which is highly dependent on the data at hand. Previous work in argument mining has mostly focussed on a particular domain (Grosse et al., 2015; Villalba and SaintDizier, 2012; Ghosh et al., 2014; Boltuzic and Snajder, 2014; Park and Cardie, 2014; Cabrio and Villata, 2012). Furthermore, an argument can be defined in a variety of ways depending on the problem being solved. As a result, we focus on the specific domain of opinionated texts such as those found in online reviews. Prior work (Carstens et al., 2014; Rajendran et al., 2016a) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models. However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requiring us to manually annotate argument components such"
N18-2005,W14-2106,0,0.0267053,", Simon Parsons2 University of Liverpool1 , King’s College London2 Abstract An argument can be defined in two different ways — (1) abstract arguments which do not refer to any internal structure (Dung, 1995) and (2) structured arguments where an argument is a collection of premises leading to a conclusion. One major problem that is faced by argument mining researchers is the variation in the definition of an argument, which is highly dependent on the data at hand. Previous work in argument mining has mostly focussed on a particular domain (Grosse et al., 2015; Villalba and SaintDizier, 2012; Ghosh et al., 2014; Boltuzic and Snajder, 2014; Park and Cardie, 2014; Cabrio and Villata, 2012). Furthermore, an argument can be defined in a variety of ways depending on the problem being solved. As a result, we focus on the specific domain of opinionated texts such as those found in online reviews. Prior work (Carstens et al., 2014; Rajendran et al., 2016a) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models. However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requirin"
N18-2005,D14-1162,0,0.0812439,"and implicit opinions that are randomly sampled from the labelled data to be trained by the SVM classifier. For each of the weakly supervised approach, we give size, the number of the predicted labels that are used to train an LSTM-based model. This model was then tested on the entire labelled data, and the accuracy of this LSTM model is reported. feature. Thus there are k.l combined Noun + Adjective features in total for each opinion. Iterations 1 5 10 15 20 25 Average-based sentence embedding We compute the mean of the 300-dimensional pre-trained word embedding vectors trained using GloVe (Pennington et al., 2014) to create a sentence embedding, and use each dimension in the sentence embedding as a feature in the classifier. |S| 1 X v= si |S| Self-training Reserved Size Accuracy Size Accuracy 22 2110 2574 3600 3613 4931 49.43 80.86 81.83 82.71 82.71 82.71 511 1717 2194 3152 3708 4931 67.68 68.24 70.25 70.98 68.81 64.22 Table 3: Accuracy of the LSTM model on annotated data using a set of automatically labelled unannotated opinions of Size. Voting-Based Again, each training set T1 , T2 and T3 is used to train separate SVM classifiers, which are used to label the unlabelled opinions, giving corresponding"
N18-2005,D15-1255,0,0.495821,"texts such as those found in online reviews. Prior work (Carstens et al., 2014; Rajendran et al., 2016a) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models. However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requiring us to manually annotate argument components such that they can be used by supervised learning techniques. Because of the heterogenous nature of user-based content, this labelling task is time-consuming and expensive (Khatib et al., 2016; Habernal and Gurevych, 2015) and often domain-dependent. Here, we are interested in analysing the problem of using supervised learning where the quantity of human-annotated or labelled data is small, and investigating how this issue can be handled by using weakly-supervised and semi-supervised techniques. We build on our prior work (Rajendran et al., 2016b), which created a small manually annotated dataset for the supervised binary classification of opinions present in online reviews, based Online reviews have become a popular portal among customers making decisions about purchasing products. A number of corpora of revie"
N18-2005,W16-2804,1,0.909333,"ed by argument mining researchers is the variation in the definition of an argument, which is highly dependent on the data at hand. Previous work in argument mining has mostly focussed on a particular domain (Grosse et al., 2015; Villalba and SaintDizier, 2012; Ghosh et al., 2014; Boltuzic and Snajder, 2014; Park and Cardie, 2014; Cabrio and Villata, 2012). Furthermore, an argument can be defined in a variety of ways depending on the problem being solved. As a result, we focus on the specific domain of opinionated texts such as those found in online reviews. Prior work (Carstens et al., 2014; Rajendran et al., 2016a) in identifying arguments in online reviews has considered sentence-level statements to be arguments based on abstract argumentation models. However, to extract arguments at a finer level based on the idea of structured arguments is a harder task, requiring us to manually annotate argument components such that they can be used by supervised learning techniques. Because of the heterogenous nature of user-based content, this labelling task is time-consuming and expensive (Khatib et al., 2016; Habernal and Gurevych, 2015) and often domain-dependent. Here, we are interested in analysing the prob"
N18-2005,P16-1150,0,0.0128281,"mplicit/explicit classification task and also for other related tasks that depend on this classification. In our investigation, we are interested in automatically labelling such a dataset using the previously proposed supervised approach described in (Rajendran et al., 2016b). Related work Research in argument mining attempts to automatically identify arguments and their relations that are present in natural language texts. Lippi and Torroni (2016) present a detailed survey of existing work in argument mining. This is carried out on different domains such as debates (Cabrio and Villata, 2012; Habernal and Gurevych, 2016), reviews (Wyner et al., 2012; Gabbriellini and Santini, 2015), tweets (Bosc et al., 2016), and dialogues (Biran and Rambow, 2011). Amgoud et al. (2015) find arguments in such texts as not formally structured with most of the content left implicit. An argument, in general, is treated as a set of premises that are linked to a claim or conclusion and, those arguments in which the major premises are left implicit are termed as enthymemes. It is important to understand whether the content that is left implicit in natural language texts are to be dealt as enthymemes or not. In our earlier work (Raj"
N18-2031,N13-1090,0,0.737275,"1 Danushka Bollegala Department of Computer Science University of Liverpool danushka@liverpool.ac.uk Introduction Distributed vector representations of words, henceforth referred to as word embeddings, have been shown to exhibit strong performance on a variety of NLP tasks (Turian et al., 2010; Zou et al., 2013). Methods for producing word embedding sets exploit the distributional hypothesis to infer semantic similarity between words within large bodies of text, in the process they have been found to additionally capture more complex linguistic regularities, such as analogical relationships (Mikolov et al., 2013c). A variety of methods now exist for the production of word embeddings (Collobert and Weston, 2008; Mnih and Hinton, 2009; Huang et al., 2012; Pennington et al., 2014; Mikolov et al., 2013a). Comparative work has illustrated a variation in performance between methods across evaluative tasks (Chen et al., 2013; Yin and Sch¨utze, 2016). Methods of “meta-embedding”, as first proposed by Yin and Sch¨utze (2016), aim to conduct a complementary combination of information from an ensemble of distinct word embedding 194 Proceedings of NAACL-HLT 2018, pages 194–198 c New Orleans, Louisiana, June 1 -"
N18-2031,D14-1162,0,0.0987243,"forth referred to as word embeddings, have been shown to exhibit strong performance on a variety of NLP tasks (Turian et al., 2010; Zou et al., 2013). Methods for producing word embedding sets exploit the distributional hypothesis to infer semantic similarity between words within large bodies of text, in the process they have been found to additionally capture more complex linguistic regularities, such as analogical relationships (Mikolov et al., 2013c). A variety of methods now exist for the production of word embeddings (Collobert and Weston, 2008; Mnih and Hinton, 2009; Huang et al., 2012; Pennington et al., 2014; Mikolov et al., 2013a). Comparative work has illustrated a variation in performance between methods across evaluative tasks (Chen et al., 2013; Yin and Sch¨utze, 2016). Methods of “meta-embedding”, as first proposed by Yin and Sch¨utze (2016), aim to conduct a complementary combination of information from an ensemble of distinct word embedding 194 Proceedings of NAACL-HLT 2018, pages 194–198 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics the resulting embeddings of any word u ∈ S1 ∩S2 , zero as uzero S1 and uS2 respectively. Now, combining our sour"
N18-2031,P10-1040,0,0.581132,"han more complex meta-embedding learning methods. The result seems counter-intuitive given that vector spaces in different source embeddings are not comparable and cannot be simply averaged. We give insight into why averaging can still produce accurate meta-embedding despite the incomparability of the source vector spaces. 1 Danushka Bollegala Department of Computer Science University of Liverpool danushka@liverpool.ac.uk Introduction Distributed vector representations of words, henceforth referred to as word embeddings, have been shown to exhibit strong performance on a variety of NLP tasks (Turian et al., 2010; Zou et al., 2013). Methods for producing word embedding sets exploit the distributional hypothesis to infer semantic similarity between words within large bodies of text, in the process they have been found to additionally capture more complex linguistic regularities, such as analogical relationships (Mikolov et al., 2013c). A variety of methods now exist for the production of word embeddings (Collobert and Weston, 2008; Mnih and Hinton, 2009; Huang et al., 2012; Pennington et al., 2014; Mikolov et al., 2013a). Comparative work has illustrated a variation in performance between methods acros"
N18-2031,J15-4004,0,0.0493745,"ks. Best performances bolded per task. Dimensionality of the meta embedding is shown next to the source embedding names. Figure 1: Distribution of angles between embeddings within GloVe ∩ CBOW. Embeddings Embeddings Evaluation Tasks 3.3.1 Semantic Similarity We measure the similarity between words by calculating the cosine similarity between their embeddings; we then calculate Spearman correlation against human similarity scores. The following datasets are used: RG (Rubenstein and Goodenough, 1965), MC (Miller and Charles, 1991), WS (Finkelstein et al., 2001), RW (Luong et al., 2013), and SL (Hill et al., 2015). 4 Conclusion We have presented an argument for averaging as a valid meta-embedding technique, and found experimental performance to be close to, or in some cases better than that of concatenation, with the additional benefit of reduced dimensionality. We propose that when conducting meta-embedding, both concatenation and averaging should be considered as methods of combining embedding spaces, and their individual advantages considered. 3.3.2 Word Analogy Using the Google dataset GL (Mikolov et al., 2013b) (19544 analogy questions), we solve questions of the form a is to b as c is to what?, u"
N18-2031,P16-1128,0,0.406397,"Missing"
N18-2031,P12-1092,0,0.0384842,"ions of words, henceforth referred to as word embeddings, have been shown to exhibit strong performance on a variety of NLP tasks (Turian et al., 2010; Zou et al., 2013). Methods for producing word embedding sets exploit the distributional hypothesis to infer semantic similarity between words within large bodies of text, in the process they have been found to additionally capture more complex linguistic regularities, such as analogical relationships (Mikolov et al., 2013c). A variety of methods now exist for the production of word embeddings (Collobert and Weston, 2008; Mnih and Hinton, 2009; Huang et al., 2012; Pennington et al., 2014; Mikolov et al., 2013a). Comparative work has illustrated a variation in performance between methods across evaluative tasks (Chen et al., 2013; Yin and Sch¨utze, 2016). Methods of “meta-embedding”, as first proposed by Yin and Sch¨utze (2016), aim to conduct a complementary combination of information from an ensemble of distinct word embedding 194 Proceedings of NAACL-HLT 2018, pages 194–198 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics the resulting embeddings of any word u ∈ S1 ∩S2 , zero as uzero S1 and uS2 respectively"
N18-2031,D13-1141,0,0.0402141,"-embedding learning methods. The result seems counter-intuitive given that vector spaces in different source embeddings are not comparable and cannot be simply averaged. We give insight into why averaging can still produce accurate meta-embedding despite the incomparability of the source vector spaces. 1 Danushka Bollegala Department of Computer Science University of Liverpool danushka@liverpool.ac.uk Introduction Distributed vector representations of words, henceforth referred to as word embeddings, have been shown to exhibit strong performance on a variety of NLP tasks (Turian et al., 2010; Zou et al., 2013). Methods for producing word embedding sets exploit the distributional hypothesis to infer semantic similarity between words within large bodies of text, in the process they have been found to additionally capture more complex linguistic regularities, such as analogical relationships (Mikolov et al., 2013c). A variety of methods now exist for the production of word embeddings (Collobert and Weston, 2008; Mnih and Hinton, 2009; Huang et al., 2012; Pennington et al., 2014; Mikolov et al., 2013a). Comparative work has illustrated a variation in performance between methods across evaluative tasks"
N18-2031,W14-1618,0,0.0401268,"e of performance on evaluative tasks. Our aim is to highlight the validity of averaging across distinct word embedding sets, such that it may be considered as a tool in future meta-embedding endeavours. 2  Analysis zero uzero S1 + uS2 To evaluate semantic similarity between word embeddings we consider the Euclidean distance measure. For `2 normalised word embeddings, Euclidean distance is a monotonically decreasing function of the cosine similarity, which is a popular choice in NLP tasks that use word embeddings such as semantic similarity prediction and analogy detection (Levy et al., 2015; Levy and Goldberg, 2014). We defer the analysis of other types of distance measures to future work. By evaluating the relationship between the Euclidean distances of pairs of words in the source embedding sets and their corresponding Euclidean distances in the meta-embedding space we can obtain a view as to how the meta-embedding procedure is combining semantic information. We begin by examining concatenation through this lens, before moving on to averaging. 2.1 uS2 (1) uS2 (2) .. .               uS2 uS2 (dS )   2 = = uS1  uS1 (1)     uS1 (2)    ..     . uS1 (dS ) (1) 1 Note that the ze"
N18-2031,Q15-1016,0,0.0613237,"ding a good baseline of performance on evaluative tasks. Our aim is to highlight the validity of averaging across distinct word embedding sets, such that it may be considered as a tool in future meta-embedding endeavours. 2  Analysis zero uzero S1 + uS2 To evaluate semantic similarity between word embeddings we consider the Euclidean distance measure. For `2 normalised word embeddings, Euclidean distance is a monotonically decreasing function of the cosine similarity, which is a popular choice in NLP tasks that use word embeddings such as semantic similarity prediction and analogy detection (Levy et al., 2015; Levy and Goldberg, 2014). We defer the analysis of other types of distance measures to future work. By evaluating the relationship between the Euclidean distances of pairs of words in the source embedding sets and their corresponding Euclidean distances in the meta-embedding space we can obtain a view as to how the meta-embedding procedure is combining semantic information. We begin by examining concatenation through this lens, before moving on to averaging. 2.1 uS2 (1) uS2 (2) .. .               uS2 uS2 (dS )   2 = = uS1  uS1 (1)     uS1 (2)    ..     . uS1 (d"
N18-2031,W13-3512,0,0.0492885,"imilarity, and analogical tasks. Best performances bolded per task. Dimensionality of the meta embedding is shown next to the source embedding names. Figure 1: Distribution of angles between embeddings within GloVe ∩ CBOW. Embeddings Embeddings Evaluation Tasks 3.3.1 Semantic Similarity We measure the similarity between words by calculating the cosine similarity between their embeddings; we then calculate Spearman correlation against human similarity scores. The following datasets are used: RG (Rubenstein and Goodenough, 1965), MC (Miller and Charles, 1991), WS (Finkelstein et al., 2001), RW (Luong et al., 2013), and SL (Hill et al., 2015). 4 Conclusion We have presented an argument for averaging as a valid meta-embedding technique, and found experimental performance to be close to, or in some cases better than that of concatenation, with the additional benefit of reduced dimensionality. We propose that when conducting meta-embedding, both concatenation and averaging should be considered as methods of combining embedding spaces, and their individual advantages considered. 3.3.2 Word Analogy Using the Google dataset GL (Mikolov et al., 2013b) (19544 analogy questions), we solve questions of the form a"
P06-1049,N04-1015,0,0.534047,"are their method with chronological ordering as an application of multi-document summarization. As described above, several good strategies/heuristics to deal with the sentence ordering problem have been proposed. In order to integrate multiple strategies/heuristics, we have formalized them in a machine learning framework and have considered an algorithm to arrange sentences using the integrated strategy. and Hovy, 2001; Barzilay et al., 2002; Okazaki et al., 2004); and learning the natural order of sentences from large corpora not necessarily based on chronological information (Lapata, 2003; Barzilay and Lee, 2004). A newspaper usually disseminates descriptions of novel events that have occurred since the last publication. For this reason, ordering sentences according to their publication date is an effective heuristic for multidocument summarization (Lin and Hovy, 2001; McKeown et al., 1999). Barzilay et al. (2002) have proposed an improved version of chronological ordering by first grouping sentences into sub-topics discussed in the source documents and then arranging the sentences in each group chronologically. 3 Method We define notation a Â b to represent that sentence a precedes sentence b. We use"
P06-1049,P03-1069,0,0.238431,"y did not compare their method with chronological ordering as an application of multi-document summarization. As described above, several good strategies/heuristics to deal with the sentence ordering problem have been proposed. In order to integrate multiple strategies/heuristics, we have formalized them in a machine learning framework and have considered an algorithm to arrange sentences using the integrated strategy. and Hovy, 2001; Barzilay et al., 2002; Okazaki et al., 2004); and learning the natural order of sentences from large corpora not necessarily based on chronological information (Lapata, 2003; Barzilay and Lee, 2004). A newspaper usually disseminates descriptions of novel events that have occurred since the last publication. For this reason, ordering sentences according to their publication date is an effective heuristic for multidocument summarization (Lin and Hovy, 2001; McKeown et al., 1999). Barzilay et al. (2002) have proposed an improved version of chronological ordering by first grouping sentences into sub-topics discussed in the source documents and then arranging the sentences in each group chronologically. 3 Method We define notation a Â b to represent that sentence a pr"
P06-1049,C04-1108,1,0.899356,"Computational Linguistics orderings. The evaluation results showed that their method outperformed Lapata’s approach by a wide margin. They did not compare their method with chronological ordering as an application of multi-document summarization. As described above, several good strategies/heuristics to deal with the sentence ordering problem have been proposed. In order to integrate multiple strategies/heuristics, we have formalized them in a machine learning framework and have considered an algorithm to arrange sentences using the integrated strategy. and Hovy, 2001; Barzilay et al., 2002; Okazaki et al., 2004); and learning the natural order of sentences from large corpora not necessarily based on chronological information (Lapata, 2003; Barzilay and Lee, 2004). A newspaper usually disseminates descriptions of novel events that have occurred since the last publication. For this reason, ordering sentences according to their publication date is an effective heuristic for multidocument summarization (Lin and Hovy, 2001; McKeown et al., 1999). Barzilay et al. (2002) have proposed an improved version of chronological ordering by first grouping sentences into sub-topics discussed in the source documents"
P06-1049,P02-1040,0,0.076067,"ing a precision of continuous sentences in an ordering against the reference ordering. We define Pn to measure the precision of (14) Here, k is a parameter to control the range of the logarithmic average; and α is a small value in case if Pn is zero. We set k = 4 (ie, more than five continuous sentences are not included for evaluation) and α = 0.01. Average Continuity becomes 0 when evaluation and reference orderings share no continuous sentences and 1 when the two orderings are identical. In Figure 7, Average Continuity is calculated as 0.63. The underlying idea of Formula 14 was proposed by Papineni et al. (2002) as the BLEU metric for the semi-automatic evaluation of machine-translation systems. The original definition of the BLEU metric is to compare a machine-translated text with its reference translation by using the word n-grams. 4.3 Results of semi-automatic evaluation Table 2 reports the resemblance of orderings produced by six algorithms to the human-made ones with three metrics, Spearman’s rank correlation, Kendall’s rank correlation, and Average Continuity. The proposed method (AGL) outperforms the 391 Precision Pn RND 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 TOP PRE SUC AGL CHR Acknowledgment We"
P06-1049,J98-3005,0,\N,Missing
P06-1049,W07-2312,0,\N,Missing
P06-1049,P00-1010,0,\N,Missing
P06-1049,H05-1079,0,\N,Missing
P06-1049,J00-3005,0,\N,Missing
P06-1049,W06-1662,0,\N,Missing
P06-1049,W01-1313,0,\N,Missing
P06-1049,N03-2019,0,\N,Missing
P06-1049,P06-1051,0,\N,Missing
P06-1049,J06-4002,0,\N,Missing
P06-1049,P01-1023,0,\N,Missing
P06-1049,W05-1621,0,\N,Missing
P06-1049,W02-2111,0,\N,Missing
P06-1049,W02-2112,0,\N,Missing
P11-1014,W06-1615,0,0.614812,"number of baselines and previous cross-domain sentiment classification techniques using the benchmark dataset. For all previous techniques we give the results reported in the original papers. The No Thesaurus baseline simulates the effect of not performing any feature expansion. We simply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. This can be considered to be a lower bound that does not perform domain adaptation. SCL is the structural correspondence learning technique of Blitzer et al. (2006). In SCL-MI, features are selected using the mutual information between a feature (unigram or bigram) and a domain label. After selecting salient features, the SCL algorithm is used to train a binary classifier. SFA is the spectral feature alignment technique of Pan et al. (2010). Both the LSA and FALSA techniques are based on latent semantic analysis (Pan et al., 2010). For the Within-Domain baseline, we train a binary classifier using the labeled data from the target domain. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for"
P11-1014,P07-1056,0,0.937463,"Missing"
P11-1014,P06-4020,1,0.770277,"ines and previous cross-domain sentiment classification techniques using the benchmark dataset. For all previous techniques we give the results reported in the original papers. The No Thesaurus baseline simulates the effect of not performing any feature expansion. We simply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. This can be considered to be a lower bound that does not perform domain adaptation. SCL is the structural correspondence learning technique of Blitzer et al. (2006). In SCL-MI, features are selected using the mutual information between a feature (unigram or bigram) and a domain label. After selecting salient features, the SCL algorithm is used to train a binary classifier. SFA is the spectral feature alignment technique of Pan et al. (2010). Both the LSA and FALSA techniques are based on latent semantic analysis (Pan et al., 2010). For the Within-Domain baseline, we train a binary classifier using the labeled data from the target domain. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for"
P11-1014,P08-1017,0,0.0228875,"use of unlabeled data enables us to accurately estimate the distribution of words in source and target domains. Our method can learn from a large amount of unlabeled data to leverage a robust cross-domain sentiment classifier. We model the cross-domain sentiment classification problem as one of feature expansion, where we append additional related features to feature vectors that represent source and target domain reviews in order to reduce the mismatch of features between the two domains. Methods that use related features have been successfully used in numerous tasks such as query expansion (Fang, 2008), and document classification (Shen et al., 2009). However, feature expansion techniques have not previously been applied to the task of cross-domain sentiment classification. In our method, we use the automatically created 133 thesaurus to expand feature vectors in a binary classifier at train and test times by introducing related lexical elements from the thesaurus. We use L1 regularized logistic regression as the classification algorithm. (However, the method is agnostic to the properties of the classifier and can be used to expand feature vectors for any binary classifier). L1 regularizati"
P11-1014,P97-1023,0,0.11935,"sensitive thesaurus for feature expansion. Given a labeled or an unlabeled review, we first split the review into individual sentences. We carry out part-of-speech (POS) tagging and lemmatization on each review sentence using the RASP sys134 tem (Briscoe et al., 2006). Lemmatization reduces the data sparseness and has been shown to be effective in text classification tasks (Joachims, 1998). We then apply a simple word filter based on POS tags to select content words (nouns, verbs, adjectives, and adverbs). In particular, previous work has identified adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). Following previous work in cross-domain sentiment classification, we model a review as a bag of words. We select unigrams and bigrams from each sentence. For the remainder of this paper, we will refer to unigrams and bigrams collectively as lexical elements. Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier (Blitzer et al., 2007). We note that it is possible to create lexical elements both from source domain labeled reviews as well as from unlabeled reviews in source and target domains. Next, we rep"
P11-1014,W02-1011,0,0.0328941,"learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products. 1 Introduction Users express opinions about products or services they consume in blog posts, shopping sites, or review sites. It is useful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment 132 expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully us"
P11-1014,N04-1041,0,0.0331408,"have similar distributions are semantically similar. We compute f (u, w) as the pointwise mutual information between a lexical element u and a feature w as follows: c(u,w) N P Pn m j=1 c(u,j) i=1 c(i,w) × N N f (u, w) = log ! (1) Here, c(u, w) denotes the number of review sentences in which a lexical element u and a feature w co-occur, n and m respectively denote the total number of lexical elements and the total number of Pn P m features, and N = i=1 j=1 c(i, j). Pointwise mutual information is known to be biased towards infrequent elements and features. We follow the discounting approach of Pantel & Ravichandran (2004) to overcome this bias. Next, for two lexical elements u and v (represented by feature vectors u and v, respectively), we compute the relatedness τ (v, u) of the feature v to the feature u as follows, P w∈{x|f (v,x)>0} f (u, w) w∈{x|f (u,x)>0} f (u, w) τ (v, u) = P . (2) 1. Note that relatedness is an asymmetric measure by the definition given in Equation 2, and the relatedness τ (v, u) of an element v to another element u is not necessarily equal to τ (u, v), the relatedness of u to v. We use the relatedness measure defined in Equation 2 to construct a sentiment sensitive thesaurus in which,"
P11-1014,P02-1053,0,0.0546106,"e source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products. 1 Introduction Users express opinions about products or services they consume in blog posts, shopping sites, or review sites. It is useful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment 132 expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used to build sen"
P14-1058,J10-4006,0,0.21112,"., 2011) groups together words that express similar sentiments in • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies 614 sentence unigrams (surface) unigrams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic"
P14-1058,W06-1615,0,0.768872,"ve baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved fr"
P14-1058,D12-1120,0,0.0191454,"words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words. • Given the distribution wS of a word w in a source domain S, we propose a method for learning its distribution wT in a target domain T . The sentiment of a word can vary from one domain to another. In Structural Correspondence Learning (SCL) (Blitzer et al., 2006; Blitzer et al., 2007), a set of pivots are chosen using pointwise mutual infor"
P14-1058,P07-1056,0,0.942627,"the same word is often associated with negative sentimentbearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably between the two domains. In this paper, given the distribution wS of a word w in the source domain S, we propose an unsupervised method for predicting its distribution wT in a different target domain T . The ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks. For example, unsupervised cross-domain sentiment classification (Blitzer et al., 2007; Aue and Gamon, 2005) involves using sentiment-labeled user reviews from the source domain, and unlabeled reviews from both the source and the target domains to learn a sentiment classifier for the target domain. Domain adaptation (DA) of sentiment classification becomes extremely challenging when the distributions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not appear in the target domain reviews that must be classified. By predicting the distribution of a word across different domains, we can find"
P14-1058,P11-1014,1,0.95001,"ntwise mutual information. Linear predictors are then learnt to predict the occurrence of those pivots, and SVD is used to construct a lower dimensional representation in which a binary classifier is trained. Spectral Feature Alignment (SFA) (Pan et al., 2010) also uses pivots to compute an alignment between domain specific and domain independent features. Spectral clustering is performed on a bipartite graph representing domain specific and domain independent features to find a lowerdimensional projection between the two sets of features. The cross-domain sentiment-sensitive thesaurus (SST) (Bollegala et al., 2011) groups together words that express similar sentiments in • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni a"
P14-1058,P98-2127,0,0.0360269,"lving a unigram and a bigram. Consequently, in matrix A, we consider co-occurrences only between unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as representing the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr , and columns, nc , as the raw co-occurrence matrix A. Distribution Prediction In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows. Given a document H, such as a user-review of a product, we split"
P14-1058,P06-4020,1,0.711747,"ied to A. Matrix F has the same number of rows, nr , and columns, nc , as the raw co-occurrence matrix A. Distribution Prediction In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows. Given a document H, such as a user-review of a product, we split H into sentences, and lemmatize each word in a sentence using the RASP system (Briscoe et al., 2006). Using a standard stop word list, we filter out frequent non-content unigrams and select the remainder as unigram features to represent a sentence. Next, we generate bigrams of word lemmas and remove any bigrams that consists only of stop words. Bigram features capture negations more accurately than unigrams, and have been found to be useful for sentiment classification tasks. Table 1 shows the unigram and bigram features we extract for a sentence using this procedure. Using data from a single doNote that in addition to the above-mentioned representation, there are many other ways to represen"
P14-1058,N10-1004,0,0.066055,"o steps. Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain. Without requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a P"
P14-1058,P12-2071,0,0.0242231,"ised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter"
P14-1058,J07-2002,0,0.0299297,"d remove any bigrams that consists only of stop words. Bigram features capture negations more accurately than unigrams, and have been found to be useful for sentiment classification tasks. Table 1 shows the unigram and bigram features we extract for a sentence using this procedure. Using data from a single doNote that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the window of co-occurrence to the entire document (Baroni and Lenci, 2010). Since the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular 615 Algorithm 1 Learning a prediction model. feature representation method, any of these alternative methods could be used. To reduce the dimensionality of the feature space, and create dense representations for words, we perform SVD on F. We use the left singular vectors corresponding to the k largest singular ˆ of values to compute a rank k approximation F, F. We perform trunc"
P14-1058,J90-1003,0,0.411652,"cooccurrences of bigrams are rare compared to cooccurrences of unigrams, and co-occurrences involving a unigram and a bigram. Consequently, in matrix A, we consider co-occurrences only between unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as representing the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr , and columns, nc , as the raw co-occurrence matrix A. Distribution Prediction In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a se"
P14-1058,P05-1004,0,0.0329206,"niversity of Sussex Falmer, Brighton, BN1 9QJ, UK Introduction The Distributional Hypothesis, summarised by the memorable line of Firth (1957) – You shall know a word by the company it keeps – has inspired a diverse range of research in natural language processing. In such work, a word is represented by the distribution of other words that co-occur with it. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al., 2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc¸on et al., 1999), and lexical inference (Kotlerman et al., 2012). 1 In this paper, we use the term domain to refer to a collection of documents about a particular topic, for example reviews of a particular kind of product. 613 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 613–623, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics across different domains, and such variations are not captured by models that only learn a single semantic representation for a w"
P14-1058,P07-1033,0,0.288919,"Missing"
P14-1058,D12-1060,0,0.0154934,"ams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon. Ponomareva and Thelwall (2012) represent source and target domain reviews as nodes in a graph and apply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to our task of distribution prediction across domains. The unsupervised DA setting that we consider does not assume the availability of labeled data for the target"
P14-1058,N09-1032,0,0.085558,"uire any labeled data in either of the two steps. Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain. Without requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in bo"
P14-1058,I13-1023,0,0.0944857,"Missing"
P14-1058,P11-1013,0,0.0501642,"10; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies 614 sentence unigrams (surface) unigrams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon. Ponomareva and Thelwall (2012) represent source and target domain reviews as nodes in a graph and apply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to"
P14-1058,D13-1170,0,0.00551686,"SST) (Bollegala et al., 2011) groups together words that express similar sentiments in • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies 614 sentence unigrams (surface) unigrams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propos"
P14-1058,D13-1016,0,0.020529,"ain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to our task of distribution prediction across domains. The unsupervised DA setting that we consider does not assume the availability of labeled data for the target domain. However, if a small amount of labeled data is available for the target domain, it can be used to further improve the performance of DA tasks (Xiao et al., 2013; Daum´e III, 2007). 3 3.1 This is an interesting and well researched book this, is, an, interesting, and, well, researched, book this, be, an, interest, and, well, research, book interest, well, research, book this+be, be+an, an+interest, interest+and, and+well, well+research, research+book an+interest, interest+and, and+well, well+research, research+book Table 1: Extracting unigram and bigram features. main, we construct a feature co-occurrence matrix A in which columns correspond to unigram features and rows correspond to either unigram or bigram features. The value of the element aij in th"
P14-1058,D07-1118,0,\N,Missing
P14-1058,P09-1056,0,\N,Missing
P14-1058,C98-2122,0,\N,Missing
P14-1058,D09-1098,0,\N,Missing
P15-1071,J10-4006,0,0.0526489,"many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is represented by a vector w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010). Despite the numerous successful applications of co-occurrence countingbased distributional word representations, their high dimensionality and sparsity are often problematic in practice. Consequently, further postprocessing steps such as dimensionality reduction, and feature selection are often required when using counting-based word representations. On th"
P15-1071,P14-1023,0,0.0320841,"lity of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences in the entire corpus to learn word representations have shown to outperform methods that use only local cooccurrences (Huang et al., 2012; Pennington et al., 2014). Overall, prediction-based methods have shown to outperform counting-based methods (Baroni et al., 2014). Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for c"
P15-1071,P08-1017,0,0.0221832,"orrespondence Learnular similarity measures for f and found cosine ing (SCL) (Blitzer et al., 2007) are the state-ofsimilarity to perform consistently well. We can inthe-art methods for cross-domain sentiment clasterpret Eq. 13 as a method for expanding a test tarsification. However, those methods do not learn get document using nearest neighbor features from word representations. the source domain labeled data. It is analogous to We use Global Vector Prediction (GloVe) (Penquery expansion used in information retrieval to nington et al., 2014), the current state-of-theimprove document recall (Fang, 2008). Alternaart word representation learning method, to learn tively, Eq. 13 can be seen as a linearly-weighted word representations separately from the source additive kernel function over two feature spaces. and target domain unlabeled data, and use the 4 Experiments and Results learnt representations in Eq. 13 for sentiment classification. In contrast to the joint word representaFor train and evaluation purposes, we use the tions learnt by the proposed method, GloVe simAmazon product reviews collected by Blitzer et ulates the level of performance we would obtain al. (2007) for the four product"
P15-1071,D12-1129,0,0.0627319,"Missing"
P15-1071,W06-1615,0,0.923946,"ure domainspecific semantic variations of w. In this paper, we use the term domain to represent a collection of documents related to a particular topic such as user-reviews in Amazon for a product category (e.g. books, dvds, movies, etc.). However, a domain in general can be a field of study (e.g. biology, computer science, law, etc.) or even an entire source of information (e.g. twitter, blogs, news articles, etc.). In particular, we do not assume the availability of any labeled data for learning word representations. This problem setting is closely related to unsupervised domain adaptation (Blitzer et al., 2006), which has found numerous useful applications such as, sentiment classification and POS tagging. For example, in unsupervised cross-domain sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007), we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained classifier to predict sentiment of the target domain’s user reviews. Although the distinction between the source and the target domains is not important for the word representation learning step, it is important for the domain adaptation tasks in which we s"
P15-1071,P07-1056,0,0.935932,"g. books, dvds, movies, etc.). However, a domain in general can be a field of study (e.g. biology, computer science, law, etc.) or even an entire source of information (e.g. twitter, blogs, news articles, etc.). In particular, we do not assume the availability of any labeled data for learning word representations. This problem setting is closely related to unsupervised domain adaptation (Blitzer et al., 2006), which has found numerous useful applications such as, sentiment classification and POS tagging. For example, in unsupervised cross-domain sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007), we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained classifier to predict sentiment of the target domain’s user reviews. Although the distinction between the source and the target domains is not important for the word representation learning step, it is important for the domain adaptation tasks in which we subsequently evaluate the learnt word representations. Following prior work on domain adaptation (Blitzer et al., 2006), high-frequent features (unigrams/bigrams) common to both domains are referred to as domai"
P15-1071,P12-1092,0,0.231064,"f words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bol"
P15-1071,P11-1014,1,0.954458,"l., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represenMeaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learnin"
P15-1071,P07-1034,0,0.641511,"2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represenMeaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domainspecific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as pivots. Next, we optimize an objective function that enforces two constrai"
P15-1071,C12-1089,0,0.0276954,". In particular, the number of senses per word type is automatically estimated. However, their method is limited to a single domain, and does not consider how the representations vary across domains. On the other hand, our proposed method learns a single representation for a particular word for each domain in which it occurs. Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015). Such representations are particularly useful for cross-lingual information retrieval (Duc et al., 2010). It will be an interesting future research direction to extend our proposed method to learn such cross-lingual word representations. 3 3.1 Problem Definition Let us assume that we are given two sets of documents DS and DT respectively for a source (S) and a target (T ) domain. We do not consider the problem of retrieving documents for a domain, and assume such a collection of documents to be given. Then, given a particular word w, we define cross-domain representation"
P15-1071,P14-1058,1,0.872398,"erpool.ac.uk shizuoka.ac.jp Ken-ichi Kawarabayashi k keniti@ nii.ac.jp University of Liverpool Shizuoka University National Institute of Informatics JST, ERATO, Kawarabayashi Large Graph Project. Abstract ple, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a singl"
P15-1071,N10-1004,0,0.182835,"er and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represenMeaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domainspecific aspects of word semantics. First"
P15-1071,N13-1090,0,0.175438,"e deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegal"
P15-1071,D14-1113,0,0.113695,"a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang"
P15-1071,D14-1162,0,0.11211,"the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as c"
P15-1071,N10-1013,0,0.0371491,"tion learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 201"
P15-1071,D14-1045,0,0.026822,"in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. We propose an unsupervised cross-domain word representation learning method that jointly optimizes two criteria: (a) given a document d from the source or the target domain, we must accurately predict the non-pivots that occur in d using the pivots that occur in d, and (b) the source and target domain representations we learn for pivots must be similar. The main challenge in domain adaptation is feature mismatch, where the features that we use for training a classifier"
P15-1071,I13-1023,0,0.082818,"Missing"
P15-1071,D11-1014,0,0.0556015,"ingle domain. • We propose a distributed word representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. We propose an unsupervised cross-domain word representation learning method that jointly optimizes two criteria: (a) given a document d from the source or the target domain, we must accurately predict the non-pivots that occur in d using the pivots that occur in d, and (b) the source and target domain representations we learn f"
P15-1071,D13-1141,0,0.0303516,"d representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. We propose an unsupervised cross-domain word representation learning method that jointly optimizes two criteria: (a) given a document d from the source or the target domain, we must accurately predict the non-pivots that occur in d using the pivots that occur in d, and (b) the source and target domain representations we learn for pivots must be similar. The main challe"
P15-1071,P08-1028,0,\N,Missing
P19-1160,P12-1015,0,0.0344687,"y Measurement The correlation between the human ratings and similarity scores computed using word embeddings for pairs of words has been used as a measure of the quality of the word embeddings (Mikolov et al., 2013d). We compute cosine similarity between word embeddings and measure Spearman correlation against human ratings for the word-pairs in the following benchmark datasets: Word Similarity 353 dataset (WS) (Finkelstein et al., 2001), RubensteinGoodenough dataset (RG) (Rubenstein and Goodenough, 1965), MTurk (Halawi et al., 2012), rare words dataset (RW) (Luong et al., 2013), MEN dataset (Bruni et al., 2012) and SimLex dataset (Hill et al., 2015). Unfortunately, existing benchmark datasets for semantic similarity were not created considering gender-biases and contain many stereotypical examples. For example, in MEN, the word sexy has high human similarity ratings with lady and girl compared to man and guy. Furthermore, masculine words and soldier are included in multiple datasets with high human similarity ratings, whereas it is not compared with feminine words in any of the datasets. Although prior work studying gender bias have used these datasets for evaluation purposes (Bolukbasi et al., 2016"
P19-1160,D18-1002,0,0.331708,"hods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information. 1 Introduction Despite the impressive success stories behind word representation learning (Devlin et al., 2018; Peters et al., 2018; Pennington et al., 2014; Mikolov et al., 2013c,a), further investigations into the learnt representations have revealed several worrying issues. The semantic representations learnt, in particular from social media, have shown to encode significant levels of racist, offensive and discriminative language usage (Bolukbasi et al., 2016; Zhao et al., 2018b; Elazar and Goldberg, 2018; Rudinger et al., 2018; Zhao et al., 2018a). For example, Bolukbasi et al. (2016) showed that word representations learnt from a large (300GB) news corpus to amplify unfair gender biases. Microsoft’s AI chat bot Tay learnt abusive language from Twitter within the first 24 hours of its release, which forced Microsoft to shutdown the bot (The Telegraph, 2016). Caliskan et al. (2017) conducted an implicit association test (IAT) (Greenwald et al., 1998) using the cosine similarity measured from word representations, and showed that word representations computed from a large Web crawl contain huma"
P19-1160,J15-4004,0,0.0207864,"e human ratings and similarity scores computed using word embeddings for pairs of words has been used as a measure of the quality of the word embeddings (Mikolov et al., 2013d). We compute cosine similarity between word embeddings and measure Spearman correlation against human ratings for the word-pairs in the following benchmark datasets: Word Similarity 353 dataset (WS) (Finkelstein et al., 2001), RubensteinGoodenough dataset (RG) (Rubenstein and Goodenough, 1965), MTurk (Halawi et al., 2012), rare words dataset (RW) (Luong et al., 2013), MEN dataset (Bruni et al., 2012) and SimLex dataset (Hill et al., 2015). Unfortunately, existing benchmark datasets for semantic similarity were not created considering gender-biases and contain many stereotypical examples. For example, in MEN, the word sexy has high human similarity ratings with lady and girl compared to man and guy. Furthermore, masculine words and soldier are included in multiple datasets with high human similarity ratings, whereas it is not compared with feminine words in any of the datasets. Although prior work studying gender bias have used these datasets for evaluation purposes (Bolukbasi et al., 2016; Zhao et al., 2018a), we note that hig"
P19-1160,W14-1618,0,0.112917,"38.8 39.1 39.1 AE (GloVe) AE (GN-GloVe) 81.0 78.6 61.9 61.3 70.5 69.2 52.6 51.2 38.9 39.1 GP (GloVe) GP (GN-GloVe) 80.5 78.3 61.0 61.3 69.9 69.0 51.3 51.0 38.5 39.6 Table 2: Accuracy for solving word analogies. Datasets #Orig #Bal WS RG MTurk RW MEN SimLex 353 65 771 2,034 3,000 999 366 77 784 2,042 3,122 1,043 Table 3: Number of word-pairs in the original (Orig) and balanced (Bal) similarity benchmarks. original word embeddings. 4.4.1 Analogy Detection Given three words a, b, c in analogy detection, we must predict a word d that completes the analogy “a is b as c is to d”. We use the CosAdd (Levy and Goldberg, 2014) that finds d that has the maximum cosine similarity with (b−a+c). We use the semantic (sem) and syntactic (syn) analogies in the Google analogy dataset (Mikolov et al., 2013b) (in total contains 19,556 questions), MSR dataset (7,999 syntactic questions) (Mikolov et al., 2013d) and SemEval dataset (SE, 79 paradigms) (Jurgens et al., 2012) as benchmark datasets. The percentage of correctly solved analogy questions is reported in Table 2. We see that there is no significant degradation of performance due to debiasing using the proposed method. 4.4.2 Semantic Similarity Measurement The correlatio"
P19-1160,P18-2005,0,0.0603754,"b-vectors are maximised, while simultaneously minimising the GloVe objective. GNGloVe learns gender-debiased word embeddings from scratch from a given corpus, and cannot be used to debias pre-trained word embeddings. Moreover, similar to hard and soft debiasing methods described above, GN-GloVe uses pre-defined lists of feminine, masculine and gender-neutral words and does not debias words in these lists. Debiasing can be seen as a problem of hiding information related to a protected attribute such as gender, for which adversarial learning methods (Xie et al., 2017; Elazar and Goldberg, 2018; Li et al., 2018) have been proposed in the fairnessaware machine learning community (Kamiran and Calders, 2009). In these approaches, inputs are first encoded, and then two classifiers are trained – a target task predictor that uses the encoded input to predict the target NLP task, and a protectedattribute predictor that uses the encoded input to predict the protected attribute. The two classifiers and the encoder is learnt jointly such that the accuracy of the target task predictor is maximised, while minimising the accuracy of the protectedattribute predictor. However, Elazar and Goldberg (2018) showed that"
P19-1160,W13-3512,0,0.0656316,"d method. 4.4.2 Semantic Similarity Measurement The correlation between the human ratings and similarity scores computed using word embeddings for pairs of words has been used as a measure of the quality of the word embeddings (Mikolov et al., 2013d). We compute cosine similarity between word embeddings and measure Spearman correlation against human ratings for the word-pairs in the following benchmark datasets: Word Similarity 353 dataset (WS) (Finkelstein et al., 2001), RubensteinGoodenough dataset (RG) (Rubenstein and Goodenough, 1965), MTurk (Halawi et al., 2012), rare words dataset (RW) (Luong et al., 2013), MEN dataset (Bruni et al., 2012) and SimLex dataset (Hill et al., 2015). Unfortunately, existing benchmark datasets for semantic similarity were not created considering gender-biases and contain many stereotypical examples. For example, in MEN, the word sexy has high human similarity ratings with lady and girl compared to man and guy. Furthermore, masculine words and soldier are included in multiple datasets with high human similarity ratings, whereas it is not compared with feminine words in any of the datasets. Although prior work studying gender bias have used these datasets for evaluatio"
P19-1160,N13-1090,0,0.839919,"errelated information in feminine and masculine words, (b) preserves the neutrality in genderneutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information. 1 Introduction Despite the impressive success stories behind word representation learning (Devlin et al., 2018; Peters et al., 2018; Pennington et al., 2014; Mikolov et al., 2013c,a), further investigations into the learnt representations have revealed several worrying issues. The semantic representations learnt, in particular from social media, have shown to encode significant levels of racist, offensive and discriminative language usage (Bolukbasi et al., 2016; Zhao et al., 2018b; Elazar and Goldberg, 2018; Rudinger et al., 2018; Zhao et al., 2018a). For example, Bolukbasi et al. (2016) showed that word representations learnt from a large (300GB) news corpus to amplify unfair gender biases. Microsoft’s AI chat bot Tay learnt abusive language from Twitter within the"
P19-1160,D14-1162,0,0.100735,"at (a) preserves the genderrelated information in feminine and masculine words, (b) preserves the neutrality in genderneutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information. 1 Introduction Despite the impressive success stories behind word representation learning (Devlin et al., 2018; Peters et al., 2018; Pennington et al., 2014; Mikolov et al., 2013c,a), further investigations into the learnt representations have revealed several worrying issues. The semantic representations learnt, in particular from social media, have shown to encode significant levels of racist, offensive and discriminative language usage (Bolukbasi et al., 2016; Zhao et al., 2018b; Elazar and Goldberg, 2018; Rudinger et al., 2018; Zhao et al., 2018a). For example, Bolukbasi et al. (2016) showed that word representations learnt from a large (300GB) news corpus to amplify unfair gender biases. Microsoft’s AI chat bot Tay learnt abusive language fr"
P19-1160,N18-1202,0,0.0607584,"a debiasing method that (a) preserves the genderrelated information in feminine and masculine words, (b) preserves the neutrality in genderneutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information. 1 Introduction Despite the impressive success stories behind word representation learning (Devlin et al., 2018; Peters et al., 2018; Pennington et al., 2014; Mikolov et al., 2013c,a), further investigations into the learnt representations have revealed several worrying issues. The semantic representations learnt, in particular from social media, have shown to encode significant levels of racist, offensive and discriminative language usage (Bolukbasi et al., 2016; Zhao et al., 2018b; Elazar and Goldberg, 2018; Rudinger et al., 2018; Zhao et al., 2018a). For example, Bolukbasi et al. (2016) showed that word representations learnt from a large (300GB) news corpus to amplify unfair gender biases. Microsoft’s AI chat bot Tay l"
P19-1160,N18-2002,0,0.101205,"Missing"
P19-1160,P18-1232,0,0.0277196,"busive language from Twitter within the first 24 hours of its release, which forced Microsoft to shutdown the bot (The Telegraph, 2016). Caliskan et al. (2017) conducted an implicit association test (IAT) (Greenwald et al., 1998) using the cosine similarity measured from word representations, and showed that word representations computed from a large Web crawl contain human-like biases with respect to gender, profession and ethnicity. Given the broad applications of pre-trained word embeddings in various down-stream NLP tasks such as machine translation (Zou et al., 2013), sentiment analysis (Shi et al., 2018), dialogue generation (Zhang et al., 2018) etc., it is important to debias word embeddings before they are applied in NLP systems that interact with and/or make decisions that affect humans. We believe that no human should be discriminated on the basis of demographic attributes by an NLP system, and there exist clear legal (European Union, 1997), business and ethical obligations to make NLP systems unbiased (Holstein et al., 2018). Despite the growing need for unbiased word embeddings, debiasing pre-trained word embeddings is a challenging task that requires a fine balance between removing inf"
P19-1160,P18-1205,0,0.0215547,"first 24 hours of its release, which forced Microsoft to shutdown the bot (The Telegraph, 2016). Caliskan et al. (2017) conducted an implicit association test (IAT) (Greenwald et al., 1998) using the cosine similarity measured from word representations, and showed that word representations computed from a large Web crawl contain human-like biases with respect to gender, profession and ethnicity. Given the broad applications of pre-trained word embeddings in various down-stream NLP tasks such as machine translation (Zou et al., 2013), sentiment analysis (Shi et al., 2018), dialogue generation (Zhang et al., 2018) etc., it is important to debias word embeddings before they are applied in NLP systems that interact with and/or make decisions that affect humans. We believe that no human should be discriminated on the basis of demographic attributes by an NLP system, and there exist clear legal (European Union, 1997), business and ethical obligations to make NLP systems unbiased (Holstein et al., 2018). Despite the growing need for unbiased word embeddings, debiasing pre-trained word embeddings is a challenging task that requires a fine balance between removing information related to discriminative biases,"
P19-1160,N18-2003,0,0.0619428,"an existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information. 1 Introduction Despite the impressive success stories behind word representation learning (Devlin et al., 2018; Peters et al., 2018; Pennington et al., 2014; Mikolov et al., 2013c,a), further investigations into the learnt representations have revealed several worrying issues. The semantic representations learnt, in particular from social media, have shown to encode significant levels of racist, offensive and discriminative language usage (Bolukbasi et al., 2016; Zhao et al., 2018b; Elazar and Goldberg, 2018; Rudinger et al., 2018; Zhao et al., 2018a). For example, Bolukbasi et al. (2016) showed that word representations learnt from a large (300GB) news corpus to amplify unfair gender biases. Microsoft’s AI chat bot Tay learnt abusive language from Twitter within the first 24 hours of its release, which forced Microsoft to shutdown the bot (The Telegraph, 2016). Caliskan et al. (2017) conducted an implicit association test (IAT) (Greenwald et al., 1998) using the cosine similarity measured from word representations, and showed that word representations computed from a"
P19-1160,D18-1521,0,0.0688381,"an existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information. 1 Introduction Despite the impressive success stories behind word representation learning (Devlin et al., 2018; Peters et al., 2018; Pennington et al., 2014; Mikolov et al., 2013c,a), further investigations into the learnt representations have revealed several worrying issues. The semantic representations learnt, in particular from social media, have shown to encode significant levels of racist, offensive and discriminative language usage (Bolukbasi et al., 2016; Zhao et al., 2018b; Elazar and Goldberg, 2018; Rudinger et al., 2018; Zhao et al., 2018a). For example, Bolukbasi et al. (2016) showed that word representations learnt from a large (300GB) news corpus to amplify unfair gender biases. Microsoft’s AI chat bot Tay learnt abusive language from Twitter within the first 24 hours of its release, which forced Microsoft to shutdown the bot (The Telegraph, 2016). Caliskan et al. (2017) conducted an implicit association test (IAT) (Greenwald et al., 1998) using the cosine similarity measured from word representations, and showed that word representations computed from a"
P19-1160,D13-1141,0,0.034017,"s. Microsoft’s AI chat bot Tay learnt abusive language from Twitter within the first 24 hours of its release, which forced Microsoft to shutdown the bot (The Telegraph, 2016). Caliskan et al. (2017) conducted an implicit association test (IAT) (Greenwald et al., 1998) using the cosine similarity measured from word representations, and showed that word representations computed from a large Web crawl contain human-like biases with respect to gender, profession and ethnicity. Given the broad applications of pre-trained word embeddings in various down-stream NLP tasks such as machine translation (Zou et al., 2013), sentiment analysis (Shi et al., 2018), dialogue generation (Zhang et al., 2018) etc., it is important to debias word embeddings before they are applied in NLP systems that interact with and/or make decisions that affect humans. We believe that no human should be discriminated on the basis of demographic attributes by an NLP system, and there exist clear legal (European Union, 1997), business and ethical obligations to make NLP systems unbiased (Holstein et al., 2018). Despite the growing need for unbiased word embeddings, debiasing pre-trained word embeddings is a challenging task that requi"
P19-1160,S12-1047,0,\N,Missing
P19-1160,D17-1242,0,\N,Missing
R19-1025,P07-1056,0,0.699331,"cular, recent work on UDA (Morerio et al., 2018) has shown that minimising the entropy of a classifier on its predictions in the source and target domains is equivalent to learning a projection space that maximises the correlation between source and target instances. Motivated by these developments, we propose Self-Adapt, a method that combines the complementary strengths of projection-based methods and self-training methods for UDA. Our proposed method consists of three steps. As an evaluation task, we perform cross-domain sentiment classification on the Amazon multidomain sentiment dataset (Blitzer et al., 2007). Although most prior work on UDA have used this dataset as a standard evaluation benchmark, the evaluations have been limited to the four domains books, dvds, electronic appliances and kitchen appliances. We too report performances on those four domains for the ease of comparison against prior work. However, to reliably estimate the generalisability of the proposed method, we perform an additional extensive evaluation using 16 other domains included in the original version of the Amazon multi-domain sentiment dataset. Results from the cross-domain sentiment classification reveal several inter"
R19-1025,W06-1615,0,0.247015,"ar, in Unsupervised Domain Adaptation (UDA) (Blitzer et al., 2006, 2007; Pan et al., 2010) we do not assume the availability of any labelled instances from the target domain but a set of labelled instances from the source domain and unlabelled instances from both source and the target domains. Two main approaches for UDA can be identified from prior work: projection-based and selftraining. Projection1 -based methods for UDA learn an embedding space where the distribution of features in the source and the target domains become closer to each other than they were in the original feature spaces (Blitzer et al., 2006). For this purpose, the union of the source and target feature spaces is split into domain-independent (often referred to as pivots) and domain-specific features using heuristics such as mutual information or frequency of a feature in a domain. A projection is then learnt between those two feature spaces and used to adapt a classifier trained from the source domain labelled data. For example, methods based on different approaches such as graphdecomposition spectral feature alignment (Pan et al., 2010) or autoencoders (Louizos et al., 2015) have been proposed for this purpose. Self-training (Ya"
R19-1025,P06-1043,0,0.163417,"Missing"
R19-1025,P11-1014,1,0.802745,"proaches for UDA learn a (possibly lower-dimensional) projection where the difference between the source and target feature spaces is reduced. For example, Structural Correspondence Learning (SCL) (Blitzer et al., 2006, 2007) learns a projection using a set of domain invariant common features called pivots. Different strategies have been proposed in the literature for finding pivots for different tasks such as the frequency of a feature in a domain for crossdomain POS tagging (Blitzer et al., 2006; Cui et al., 2017a), mutual information (Blitzer et al., 2007) and pointwise mutual information (Bollegala et al., 2011, 2015) for cross-domain sentiment classification. Cui et al. (2017b) proposed a method for learning the appropriateness of a feature as a pivot (pivothood) from the data during training, without requiring any heuristics. Although we use projections in the proposed method, unlike prior work on projection-based UDA, we do not require splitting the feature space into domain independent and domain specific features. Moreover, we learn two separate projections for each of the source and target domain, which gives us more flexibility to address the domain-specific constrains in the learnt projectio"
R19-1025,D14-1162,0,0.0807949,"Missing"
R19-1025,P07-1078,0,0.0534814,"ms project and embed as synonymous in this paper 213 Proceedings of Recent Advances in Natural Language Processing, pages 213–222, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_025 learn a projected feature space in the target domain where the margin between the opposite pseudo-labelled nearest neighbours is maximised. We project labelled instances in the source domain and pseudo-labelled instances in the target domain respectively using Sprj and Tprj , and use those projected instances to learn a classifier for the target task. main in UDA (McClosky et al., 2006; Reichart and Rappoport, 2007; Drury et al., 2011). Specifically, the source domain’s labelled instances are used to initialise the self-training process and during subsequent iterations labels are inferred for the target domain’s unlabelled instances, which can be used to train a classifier for the task of interest. So far in UDA projection-learning and selftrained approaches have been explored separately. An interesting research question we ask and answer positively in this paper is whether can we improve the performance of projection-based methods in UDA using self-training? In particular, recent work on UDA (Morerio e"
R19-1025,P18-1096,0,0.308539,"t of the three classifiers agree upon a label for an unlabelled instance, that label is then assigned to the unlabelled instance. Søgaard (2010) proposed a variation of tri-training (i.e. tri-training with diversification) that diversifies the sampling process and reduces the number of additional instances, where they require exactly two out of the three classifiers to agree upon a label and the third classifier to disagree. It has been shown that the classic tri-training algorithm when applied to UDA acts as a strong baseline that outperforms even more complex SoTA neural adaptation methods (Ruder and Plank, 2018). As later shown in our experiments, the proposed Self-Adapt method consistently outperforms selftraining, tri-training and tri-training with diversification across most of the domain pairs considered. tion methods (Louizos et al., 2015; Ganin et al., 2016; Saito et al., 2017; Ruder and Plank, 2018). 2 Related Work Self-training (Yarowsky, 1995) has been adapted to various cross-domain NLP tasks such as document classification (Raina et al., 2007), POS tagging (McClosky et al., 2006; Reichart and Rappoport, 2007) and sentiment classification (Drury et al., 2011). Although different variants of"
R19-1025,P95-1026,0,0.798337,"6). For this purpose, the union of the source and target feature spaces is split into domain-independent (often referred to as pivots) and domain-specific features using heuristics such as mutual information or frequency of a feature in a domain. A projection is then learnt between those two feature spaces and used to adapt a classifier trained from the source domain labelled data. For example, methods based on different approaches such as graphdecomposition spectral feature alignment (Pan et al., 2010) or autoencoders (Louizos et al., 2015) have been proposed for this purpose. Self-training (Yarowsky, 1995; Abney, 2007) is a technique to iteratively increase a set of labelled instances by training a classifier using current labelled instances and applying the trained classifier to predict pseudo-labels for unlabelled instances. High confident predictions are then appended to the current labelled dataset, thereby increasing the number of labelled instances. The process is iterated until no additional pseudo-labelled instances can be found. Self-training provides a direct solution to the lack of labelled data in the target doLack of labelled data in the target domain for training is a common prob"
R19-1025,W11-0323,0,0.055785,"Missing"
S18-2004,S14-2010,0,0.0151665,"STS and the similarity scores computed following the above-mentioned procedure. If there exists a high degree of correlation between the sentence similarity scores computed using the NWS scores and human ratings, then it can be considered as empirical support for the accuracy of the NWS scores. Note that we have not trained the word salience model on the SemEval datasets, but are only using them to test the effectiveness of the computed NWS scores. As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015). Note that the tasks with the same name in different years actually represent different tasks. We use Pearson correlation coefficient as the evaluation measure. For a list of n ordered pairs of ratings {(xi , yi )}ni=1 , the Pearson correlation coefficient between the two ratings, r(x, y), is computed as follows: (5) From which we have, ∂h(si , sj ) ∂g(si , sj ) = h(si , sj ) . ∂q(w) ∂q(w) We can then compute ∂g ∂q(w) (6) as follows: I[w ∈ Si ]w&gt; sj + I[w ∈ Sj ]w&gt; si   X − log( exp si &gt; sj I[w ∈ Si ]w&gt; sk + (7) (8) k I[w ∈ Sk ]w&gt; si )) Measuring Semantic Text"
S18-2004,S12-1051,0,0.203755,"choice differentiates our work from previously proposed sentence embedding learning methods that jointly learn word embeddings as well as sentence embeddings (Hill et al., 2016; Kiros et al., 2015; Kenter et al., 2016). Moreover, it decouples the word salience score learning problem from word or sentence embedding learning problem, thereby simplifying the optimisation task and speeding up the learning process. We use the NWS scores to compute sentence embeddings and measure the similarity between two sentences using 18 benchmark datasets for semantic textual similarity in past SemEval tasks (Agirre et al., 2012). Experimental results show that the sentence similarity scores computed using the NWS scores and pre-trained word embeddings show a high degree of correlation with human similarity ratings in those benchmark datasets. Moreover, we compare the NWS scores against the human ratings for psycholinguistic properties of words such as arousal, valence, dominance, imageability, and concreteness. Our analysis shows that NWS scores demonstrate a moderate level of correlation with concreteness and imageability ratings, despite not being specifically trained to predict such psycholinguistic properties of"
S18-2004,S13-1004,0,0.014723,"rs in benchmark datasets for STS and the similarity scores computed following the above-mentioned procedure. If there exists a high degree of correlation between the sentence similarity scores computed using the NWS scores and human ratings, then it can be considered as empirical support for the accuracy of the NWS scores. Note that we have not trained the word salience model on the SemEval datasets, but are only using them to test the effectiveness of the computed NWS scores. As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015). Note that the tasks with the same name in different years actually represent different tasks. We use Pearson correlation coefficient as the evaluation measure. For a list of n ordered pairs of ratings {(xi , yi )}ni=1 , the Pearson correlation coefficient between the two ratings, r(x, y), is computed as follows: (5) From which we have, ∂h(si , sj ) ∂g(si , sj ) = h(si , sj ) . ∂q(w) ∂q(w) We can then compute ∂g ∂q(w) (6) as follows: I[w ∈ Si ]w&gt; sj + I[w ∈ Sj ]w&gt; si   X − log( exp si &gt; sj I[w ∈ Si ]w&gt; sk + (7) (8) k I[w ∈ Sk ]w&gt; s"
S18-2004,N15-1027,0,0.0188731,"ing the word salience scores we learn. Moreover, our work differs from Siamese CBOW (Kenter et al., 2016) in that we do not learn word embeddings but take pre-trained word embeddings as the input for learning word salience scores. NWS scores we learn in this paper are also different from the salience scores learnt by (Arora et al., 2017) because they do not constrain their word salience scores such that they can be used to predict the words that occur in adjacent sentences. Ideally, the normalisation term in the denominator in the softmax must be taken over all the sentences Sk in the corpus (Andreas and Klein, 2015). However, this is computationally expensive in most cases except for extremely small corpora. Therefore, following noise-contrastive estimation (Gutmann and Hyvärinen, 2012), we approximate the normalisation term using a randomly sampled set of K sentences, where K is typically less than 10. Because the similarity between two randomly sampled sentences is likely to be smaller than, for example, two adjacent sentences, we can see this sampling process as randomly sampling negative training instances from the corpus. For two sentences Si and Sj we consider them to be similar (positive training"
S18-2004,D12-1050,0,0.0313635,"rity between the two sentences using their embeddings si , sj . Difference between predicted similarity and actual label is considered as the error and its gradient is backpropagated through the network to update q(w). Sentences have a syntactic structure and the ordering of words affects the meaning expressed in the sentence. Consequently, compositional approaches for computing sentence-level semantic representations from word-level semantic representations have used numerous linear algebraic operators such as vector addition, element-wise multiplication, multiplying by a matrix or a tensor (Blacoe and Lapata, 2012; Mitchell and Lapata, 2008). ble performances to sentence embeddings that are learnt using more sophisticated word-order sensitive methods. For example, (Arora et al., 2017) proposed a method to find the optimal weights for combining word embeddings when creating sentence embeddings using unigram probabilities, by maximising the likelihood of the occurrences of words in a corpus. Siamese CBOW (Kenter et al., 2016) learns word embeddings such that we can accurately compute sentence embeddings by averaging the word embeddings. Although averaging is an order insensitive operator, (Adi et al., 20"
S18-2004,P08-1028,0,0.0738826,"ences using their embeddings si , sj . Difference between predicted similarity and actual label is considered as the error and its gradient is backpropagated through the network to update q(w). Sentences have a syntactic structure and the ordering of words affects the meaning expressed in the sentence. Consequently, compositional approaches for computing sentence-level semantic representations from word-level semantic representations have used numerous linear algebraic operators such as vector addition, element-wise multiplication, multiplying by a matrix or a tensor (Blacoe and Lapata, 2012; Mitchell and Lapata, 2008). ble performances to sentence embeddings that are learnt using more sophisticated word-order sensitive methods. For example, (Arora et al., 2017) proposed a method to find the optimal weights for combining word embeddings when creating sentence embeddings using unigram probabilities, by maximising the likelihood of the occurrences of words in a corpus. Siamese CBOW (Kenter et al., 2016) learns word embeddings such that we can accurately compute sentence embeddings by averaging the word embeddings. Although averaging is an order insensitive operator, (Adi et al., 2016) empirically showed that"
S18-2004,N16-1050,0,0.0247119,"ol exerted by the stimulus) contribute to how the meanings of words affect human psychology, and often referred to as the affective meanings of words. (Mandera et al., 2015) show that by using SGNS embeddings as features in a k-Nearest Neighbour classifier, it is possible to accurately extrapolate the affective meanings of words. Moreover, perceived psycholinguistic properties of words such as concreteness (how “palpable” the object the word refers to) and imageability (the intensity with which a word arouses images) have been successfully predicted using word embeddings (Turney et al., 2011; Paetzold and Specia, 2016). For example, (Turney et al., 2011) used the cosine similarity between word embeddings obtained via La39 Table 2: Effect of word embeddings. Dataset 2012 MSRpar OnWN SMTeuroparl SMTnews 2013 FNWN OnWN headlines 2014 OnWN deft-forum deft-news headlines images tweet-news 2015 answers-forums answers-students belief headlines images Overall Average Table 3: Pearson correlation coefficients against Psycholinguistic ratings of words in the ANEW and MRC databases. NWS with pre-trained SGNS CBOW GloVe 14.27 59.76 41.04 43.42 24.15 61.25 45.51 46.94 28.47 65.50 50.12 44.73 21.47 67.37 57.05 29.31 70.0"
S18-2004,D11-1014,0,0.158129,"Missing"
S18-2004,D16-1009,0,0.0733209,"in sentences. This can be understood intuitively by recalling that words that appear between two words are often different in contexts where those two words are swapped. For example, in the two sentences “Ostrich is a large bird that lives in Africa” and “Large birds such as Ostriches live in Africa”, the words that appear in between ostrich and bird are different, giving rise to different sentence embeddings even when sentence embeddings are computed by averaging the individual word embeddings. Instead of considering all words equally for sentence embedding purposes, attention-based models (Hahn and Keller, 2016; Yin et al., 2016; Wang et al., 2016) learn the amount of weight (attention) we must assign to each word in a given context. Alternatively to applying nonparametric operators on word embeddings to create sentence embeddings, recurrent neural networks can learn the optimal weight matrix that can produce an accurate sentence embedding when repeatedly applied to the constituent word embeddings. For example, skip-thought vectors (Kiros et al., 2015) use bi-directional LSTMs to predict the words in the order they appear in the previous and next sentences given the current sentence. Although skip-t"
S18-2004,N16-1108,0,0.0202711,"ng compared to weighted addition of the input word embeddings. FastSent (Hill et al., 2016) was proposed as an alternative lightweight approach for sentence embedding where a softmax objective is optimised to predict the occurrences of words in the next and the previous sentences, ignoring the ordering of the words in the sentence. Our proposed method for learning NWS scores is based on the prior observation that averaging is an effective heuristic for creating sentence embeddings from word embeddings. However, unlike sentence embedding learning methods that do not learn word salience scores (He and Lin, 2016; Yin Surprisingly, averaging word embeddings to create sentence embeddings has shown compara35 used as h. As a concrete example, here we use softmax of the inner-products as follows:  exp si &gt; sj h(si , sj ) = P (2) &gt; Sk ∈C exp (si sk ) et al., 2016) , our goal in this paper is to learn word salience scores and not sentence embeddings. We compute sentence embeddings only for the purpose of evaluating the word salience scores we learn. Moreover, our work differs from Siamese CBOW (Kenter et al., 2016) in that we do not learn word embeddings but take pre-trained word embeddings as the input fo"
S18-2004,D11-1063,0,0.0237654,"(the degree of control exerted by the stimulus) contribute to how the meanings of words affect human psychology, and often referred to as the affective meanings of words. (Mandera et al., 2015) show that by using SGNS embeddings as features in a k-Nearest Neighbour classifier, it is possible to accurately extrapolate the affective meanings of words. Moreover, perceived psycholinguistic properties of words such as concreteness (how “palpable” the object the word refers to) and imageability (the intensity with which a word arouses images) have been successfully predicted using word embeddings (Turney et al., 2011; Paetzold and Specia, 2016). For example, (Turney et al., 2011) used the cosine similarity between word embeddings obtained via La39 Table 2: Effect of word embeddings. Dataset 2012 MSRpar OnWN SMTeuroparl SMTnews 2013 FNWN OnWN headlines 2014 OnWN deft-forum deft-news headlines images tweet-news 2015 answers-forums answers-students belief headlines images Overall Average Table 3: Pearson correlation coefficients against Psycholinguistic ratings of words in the ANEW and MRC databases. NWS with pre-trained SGNS CBOW GloVe 14.27 59.76 41.04 43.42 24.15 61.25 45.51 46.94 28.47 65.50 50.12 44.73"
S18-2004,N16-1162,0,0.096004,"NWS scores. First, we do not require labelled data for learning NWS scores. Although we require semantically similar (positive) and semantically dissimilar (negative) pairs of sentences for learning the NWS scores, both positive and negative examples are automatically extracted from the given corpus. Second, we use pre-trained word embeddings as the input, and do not learn the word embeddings as part of the learning process. This design choice differentiates our work from previously proposed sentence embedding learning methods that jointly learn word embeddings as well as sentence embeddings (Hill et al., 2016; Kiros et al., 2015; Kenter et al., 2016). Moreover, it decouples the word salience score learning problem from word or sentence embedding learning problem, thereby simplifying the optimisation task and speeding up the learning process. We use the NWS scores to compute sentence embeddings and measure the similarity between two sentences using 18 benchmark datasets for semantic textual similarity in past SemEval tasks (Agirre et al., 2012). Experimental results show that the sentence similarity scores computed using the NWS scores and pre-trained word embeddings show a high degree of correlati"
S18-2004,P16-1048,0,0.0285915,"uitively by recalling that words that appear between two words are often different in contexts where those two words are swapped. For example, in the two sentences “Ostrich is a large bird that lives in Africa” and “Large birds such as Ostriches live in Africa”, the words that appear in between ostrich and bird are different, giving rise to different sentence embeddings even when sentence embeddings are computed by averaging the individual word embeddings. Instead of considering all words equally for sentence embedding purposes, attention-based models (Hahn and Keller, 2016; Yin et al., 2016; Wang et al., 2016) learn the amount of weight (attention) we must assign to each word in a given context. Alternatively to applying nonparametric operators on word embeddings to create sentence embeddings, recurrent neural networks can learn the optimal weight matrix that can produce an accurate sentence embedding when repeatedly applied to the constituent word embeddings. For example, skip-thought vectors (Kiros et al., 2015) use bi-directional LSTMs to predict the words in the order they appear in the previous and next sentences given the current sentence. Although skip-thought vectors have shown superior per"
S18-2004,P16-1089,0,0.0315963,"Missing"
S18-2004,Q16-1019,0,0.0334239,"be understood intuitively by recalling that words that appear between two words are often different in contexts where those two words are swapped. For example, in the two sentences “Ostrich is a large bird that lives in Africa” and “Large birds such as Ostriches live in Africa”, the words that appear in between ostrich and bird are different, giving rise to different sentence embeddings even when sentence embeddings are computed by averaging the individual word embeddings. Instead of considering all words equally for sentence embedding purposes, attention-based models (Hahn and Keller, 2016; Yin et al., 2016; Wang et al., 2016) learn the amount of weight (attention) we must assign to each word in a given context. Alternatively to applying nonparametric operators on word embeddings to create sentence embeddings, recurrent neural networks can learn the optimal weight matrix that can produce an accurate sentence embedding when repeatedly applied to the constituent word embeddings. For example, skip-thought vectors (Kiros et al., 2015) use bi-directional LSTMs to predict the words in the order they appear in the previous and next sentences given the current sentence. Although skip-thought vectors hav"
S18-2030,P07-1056,0,0.877762,"ly predict those pivots using the other (non-pivot) features. For example, in spectral feature alignment (SFA) (Pan et al., 2010), a bipartite graph is created between non-pivots (domain-specific) and pivots (domain-independent) then spectral methods are used to learn a projection from domain-specific to domain-independent feature spaces. Blitzer et al. (2006) proposed the frequency (FREQ) of a feature in the source and the target domain as the criterion for selecting pivots for structural correspondence learning (SCL) when performing crossdomain named entity recognition. However, they found (Blitzer et al., 2007) that mutual information (MI) to be a better pivot selection criterion for cross-domain sentiment classification tasks. Bollegala et al. (2015) proposed a feature expansionbased domain adaptation method, where a sentiment sensitive thesaurus (SST) is built using the pointwise mutual information (PMI) between a feature and the source/target domains. The cores identified by CP-decomposition can be seen as playing the role of pivots in cross-domain text classification tasks because cores get expanded by their corresponding peripheries during the feature expansion step. However, one notable charac"
S18-2030,D14-1181,0,0.00945614,"Missing"
S18-2030,W06-1615,0,0.635896,"be created relatively easily, handling of shorter texts poses several challenges. The number of features that are present in a given short-text will be a small fraction of the set of all features that exist in all of the train instances. Moreover, frequency of a feature in a short-text will be small, which makes it difficult to reliably estimate the salience of a feature using term frequency-based methods. This is known as the feature sparseness problem in text classification. Feature sparseness is not unique to shorttext classification but also encountered in crossdomain text classification (Blitzer et al., 2006, 2007; Bollegala et al., 2014), where the training and test data are selected from different domains with small intersection of feature spaces. In the domain adaptation (DA) setting, a classifier trained on one domain (source) might be agnostic to the features that are unique to a different domain (target), which results in a feature mismatch problem similar to the feature-sparseness problem discussed above. To address the feature sparseness problem encountered in short-text and cross-domain classification tasks, we propose a novel method that computes related features that can be appended to"
S18-2030,P15-1071,1,0.926696,"e graph is created between non-pivots (domain-specific) and pivots (domain-independent) then spectral methods are used to learn a projection from domain-specific to domain-independent feature spaces. Blitzer et al. (2006) proposed the frequency (FREQ) of a feature in the source and the target domain as the criterion for selecting pivots for structural correspondence learning (SCL) when performing crossdomain named entity recognition. However, they found (Blitzer et al., 2007) that mutual information (MI) to be a better pivot selection criterion for cross-domain sentiment classification tasks. Bollegala et al. (2015) proposed a feature expansionbased domain adaptation method, where a sentiment sensitive thesaurus (SST) is built using the pointwise mutual information (PMI) between a feature and the source/target domains. The cores identified by CP-decomposition can be seen as playing the role of pivots in cross-domain text classification tasks because cores get expanded by their corresponding peripheries during the feature expansion step. However, one notable characteristic in the proposed method is that we induce cores via CP-decomposition instead of applying heuristic measures such as MI or PMI. As we la"
S18-2030,P14-1058,1,0.861984,", handling of shorter texts poses several challenges. The number of features that are present in a given short-text will be a small fraction of the set of all features that exist in all of the train instances. Moreover, frequency of a feature in a short-text will be small, which makes it difficult to reliably estimate the salience of a feature using term frequency-based methods. This is known as the feature sparseness problem in text classification. Feature sparseness is not unique to shorttext classification but also encountered in crossdomain text classification (Blitzer et al., 2006, 2007; Bollegala et al., 2014), where the training and test data are selected from different domains with small intersection of feature spaces. In the domain adaptation (DA) setting, a classifier trained on one domain (source) might be agnostic to the features that are unique to a different domain (target), which results in a feature mismatch problem similar to the feature-sparseness problem discussed above. To address the feature sparseness problem encountered in short-text and cross-domain classification tasks, we propose a novel method that computes related features that can be appended to the feature vectors to reduce"
S18-2030,C14-1008,0,0.109389,"Missing"
S18-2030,P08-1017,0,0.0392848,"ith other peripheral vertices. To the best of our knowledge, we are the first to apply CP-decomposition to any NLP task, let alone short-text classification. Moreover, our formulation of the CP-decomposition is customised to the needs in the NLP domain such as prioritising linguistically appropriate cores and allows a single periphery to link to multiple cores. We hope that our work will inspire NLP practitioners to use CP-decomposition in related NLP tasks such as information retrieval (Mihalcea and Radev, 2011) (measuring similarity between short-text documents), query suggestion/expansion (Fang, 2008) (suggesting related peripheral terms to a query corresponding to a core). 3 3.2 Given a feature-relatedness graph G created using the process described in Section 3.1, we propose a method that decomposes G into a set of overlapping core-periphery structures. A core-periphery structure assumed in this study consists of one core vertex and an arbitrarily number of peripheral vertices that are adjacent (i.e., directly connected) to the core vertex.2 Therefore, a coreperiphery structure forms a star graph. We further assume that a core belongs only to one coreperiphery structure, but a periphery"
S18-2030,N16-1162,0,0.0236899,"Missing"
S18-2030,P04-1035,0,0.0540928,"7.44 78.52 85.65 64.26 73.97 63.21 75.50 82.75 59.10 70.14 78.86 80.87 88.05 73.55 80.33 80.34 83.89 89.75 75.23 82.30 80.56 83.89 90.15 74.95 82.39 80.86 84.40 90.48 75.66 82.85 Table 1: Results for the short-text classification task. For each dataset, the best results are shown in bold. 4 Experiments We evaluate the proposed method on two tasks: short-text classification (a non-DA task) and crossdomain sentiment classification (a DA task). For short-text classification we use the Stanford sentiment treebank (TR)3 , customer reviews dataset (CR) (Hu and Liu, 2004), subjective dataset (SUBJ) (Pang and Lee, 2004) and movie reviews (MR) (Pang and Lee, 2005). For DA we use Amazon multi-domain sentiment dataset (Blitzer et al., 2007) containing product reviews from four categories: Books (B), DVDs (D), Electronics (E) and Kitchen Appliances (K). Each category is regarded as a domain and has 1000 positive and 1000 negative reviews, and a large number of unlabelled reviews.4 We train a classifier on 12 domain pairs adapting from source to target (S-T): B-D, BE, B-K, D-B, D-E, D-K, E-B, E-D, E-K, K-B, KD, K-E. For the short-text classification datasets, we use the official train/test split. We represent eac"
S18-2030,P05-1015,0,0.225494,"75 59.10 70.14 78.86 80.87 88.05 73.55 80.33 80.34 83.89 89.75 75.23 82.30 80.56 83.89 90.15 74.95 82.39 80.86 84.40 90.48 75.66 82.85 Table 1: Results for the short-text classification task. For each dataset, the best results are shown in bold. 4 Experiments We evaluate the proposed method on two tasks: short-text classification (a non-DA task) and crossdomain sentiment classification (a DA task). For short-text classification we use the Stanford sentiment treebank (TR)3 , customer reviews dataset (CR) (Hu and Liu, 2004), subjective dataset (SUBJ) (Pang and Lee, 2004) and movie reviews (MR) (Pang and Lee, 2005). For DA we use Amazon multi-domain sentiment dataset (Blitzer et al., 2007) containing product reviews from four categories: Books (B), DVDs (D), Electronics (E) and Kitchen Appliances (K). Each category is regarded as a domain and has 1000 positive and 1000 negative reviews, and a large number of unlabelled reviews.4 We train a classifier on 12 domain pairs adapting from source to target (S-T): B-D, BE, B-K, D-B, D-E, D-K, E-B, E-D, E-K, K-B, KD, K-E. For the short-text classification datasets, we use the official train/test split. We represent each instance (document) using a bag-of-feature"
W06-0803,W03-0405,0,0.744464,"instances with the same name and from different documents refer to the same individual. Bagga and Baldwin (1998) first perform withindocument coreference resolution to form coreference chains for each entity in each document. They then use the text surrounding each reference chain to create summaries about each entity in each document. These summaries are then converted to a bag of words feature vector and are clustered using standard vector space model often employed in IR. The use of simplistic bag of words clustering is an inherently limiting aspect of their methodology. On the other hand, Mann and Yarowsky (2003) proposes a richer document representation involving automatically extracted features. However, their clustering technique can be basically used only for separating two people with the same name. Fleischman and Hovy (2004) constructs a maximum entropy classifier to learn distances between documents that are then clustered. Their method requires a large training set. to register and share personal information among friends and communities. There have been recent attempts to extract social networks using the information available on the Web 2 (Mika, 2004; Matsuo et al., 2006). In both Matsuo’s ("
W06-0803,P04-1036,0,0.21887,"s represented by the length of the edge between the corresponding two nodes. As a measure of the strength of the relationship between two people A and B, these algorithms use the number of hits obtained for the query A AND B. However, this approach fails when A or B has namesakes because the number of hits in these cases includes the hits for the namesakes. To overcome this problem, we could include phrases in the query that uniquely identify A and B from their namesakes. 3 Related Work Person name disambiguation can be seen as a special case of word sense disambiguation (WSD) (Schutze, 1998; McCarthy et al., 2004) problem which has been studied extensively in Natural Language Understanding. However, there are several fundamental differences between WSD and person name disambiguation. WSD typically concentrates on disambiguating between 2-4 possible meanings of the word, all of which are a priori known. However, in person name disambiguation in Web, the number of different namesakes can be much larger and unknown. From a resource point of view, WSD utilizes sense tagged dictionaries such as WordNet, whereas no dictionary can provide information regarding different namesakes for a particular name. The pr"
W06-0803,P98-1012,0,0.558459,"l name queries, we may receive web pages for other people with the same name (namesakes). For example, if we search Google 1 for Jim Clark, even among the top 100 results we find at least eight different Jim Clarks. The two popular namesakes; ∗ National Institute of Advanced Industrial Science and Technology 1 www.google.com 17 Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17–24, c Sydney, July 2006. 2006 Association for Computational Linguistics two instances with the same name and from different documents refer to the same individual. Bagga and Baldwin (1998) first perform withindocument coreference resolution to form coreference chains for each entity in each document. They then use the text surrounding each reference chain to create summaries about each entity in each document. These summaries are then converted to a bag of words feature vector and are clustered using standard vector space model often employed in IR. The use of simplistic bag of words clustering is an inherently limiting aspect of their methodology. On the other hand, Mann and Yarowsky (2003) proposes a richer document representation involving automatically extracted features. H"
W06-0803,J98-1004,0,0.0324093,"en two people is represented by the length of the edge between the corresponding two nodes. As a measure of the strength of the relationship between two people A and B, these algorithms use the number of hits obtained for the query A AND B. However, this approach fails when A or B has namesakes because the number of hits in these cases includes the hits for the namesakes. To overcome this problem, we could include phrases in the query that uniquely identify A and B from their namesakes. 3 Related Work Person name disambiguation can be seen as a special case of word sense disambiguation (WSD) (Schutze, 1998; McCarthy et al., 2004) problem which has been studied extensively in Natural Language Understanding. However, there are several fundamental differences between WSD and person name disambiguation. WSD typically concentrates on disambiguating between 2-4 possible meanings of the word, all of which are a priori known. However, in person name disambiguation in Web, the number of different namesakes can be much larger and unknown. From a resource point of view, WSD utilizes sense tagged dictionaries such as WordNet, whereas no dictionary can provide information regarding different namesakes for a"
W06-0803,W04-0701,0,0.0565327,"ment. They then use the text surrounding each reference chain to create summaries about each entity in each document. These summaries are then converted to a bag of words feature vector and are clustered using standard vector space model often employed in IR. The use of simplistic bag of words clustering is an inherently limiting aspect of their methodology. On the other hand, Mann and Yarowsky (2003) proposes a richer document representation involving automatically extracted features. However, their clustering technique can be basically used only for separating two people with the same name. Fleischman and Hovy (2004) constructs a maximum entropy classifier to learn distances between documents that are then clustered. Their method requires a large training set. to register and share personal information among friends and communities. There have been recent attempts to extract social networks using the information available on the Web 2 (Mika, 2004; Matsuo et al., 2006). In both Matsuo’s (2006) and Mika’s (2004) algorithms, each person is represented by a node in the social network and the strength of the relationship between two people is represented by the length of the edge between the corresponding two"
W06-0803,C96-1009,0,\N,Missing
W06-0803,C98-1012,0,\N,Missing
W10-4309,D09-1036,0,0.0138905,"s, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. The unsupervised method of Marcu and Echihabi (2002) was the first to try to detect ‘implicit’ relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Lin et al. (2009) studied the problem of detecting implicit relations in PDTB. Their relational classifier is trained using features extracted from dependency paths, contextual information, word pairs and production rules in parse trees. For the same task, Pitler et al. (2009) also use word pairs, as well as several other types of features such as verb classes, modality, context, and lexical features. In this paper, we are not aiming at defining novel features for improving performance in RST or PDTB relation classification. Instead we incorporate features that have already shown to be useful for discourse rel"
W10-4309,P02-1047,0,0.0373559,"ember 24-25, 2010. 2010 Association for Computational Linguistics 55 a feature correlation matrix, using unlabeled data. In a second section, we show how to extend feature vectors in order to include co-occurrence information. Finally, we describe the features used in the discourse relation classifiers. been built in the RST framework. In duVerle and Prendinger (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. Shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. The unsupervised method of Marcu and Echihabi (2002) was the first to try to detect ‘implicit’ relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Lin et al. (2009) studied the problem of detecting implicit relations in PDTB. Their relational classifier is trained"
W10-4309,C08-2022,0,0.0130644,"duVerle and Prendinger (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. Shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. The unsupervised method of Marcu and Echihabi (2002) was the first to try to detect ‘implicit’ relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Lin et al. (2009) studied the problem of detecting implicit relations in PDTB. Their relational classifier is trained using features extracted from dependency paths, contextual information, word pairs and production rules in parse trees. For the same task, Pitler et al. (2009) also use word pairs, as well as several other types of features such as verb classes, modality, context, and lexical features. In this paper, we are not aiming at defining nove"
W10-4309,P09-1077,0,0.0159968,"pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Lin et al. (2009) studied the problem of detecting implicit relations in PDTB. Their relational classifier is trained using features extracted from dependency paths, contextual information, word pairs and production rules in parse trees. For the same task, Pitler et al. (2009) also use word pairs, as well as several other types of features such as verb classes, modality, context, and lexical features. In this paper, we are not aiming at defining novel features for improving performance in RST or PDTB relation classification. Instead we incorporate features that have already shown to be useful for discourse relation learning and explore the possibilities of using unlabeled data for this task. 3 3.1 Feature Correlation Matrix A training/test instance is represented using a ddimensional feature vector f = [f1 , . . . , fd ]T , where fi ∈ {0, 1}. We define a feature co"
W10-4309,prasad-etal-2008-penn,0,0.101497,"ture cooccurrences in unlabeled data. This information is then used as a basis to extend the feature vectors during training. The proposed method is evaluated on both RST-DT and PDTB, where it significantly outperformed baseline classifiers. We believe that the proposed method is a first step towards improving classification performance, particularly for discourse relations lacking annotated data. 1 Introduction The RST Discourse Treebank (RST-DT) (Carlson et al., 2001), based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) framework, and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), are two of the most widely-used corpora for training discourse relation classifiers. They are both based on the Wall Street Journal (WSJ) corpus, although there are substantial differences in the relation taxonomy used to annotate the corpus. These corpora have been used in most of the recent work employing discourse relation classifiers, which are based 2 Related Work Since the release in 2002 of the RST-DT corpus, several fully-supervised discourse parsers have 1 We use the notation [N] and [S] respectively to denote the nucleus and satellite in a RST discourse relation. Proceedings of SIG"
W10-4309,N03-1030,0,0.15609,"rse and Dialogue, pages 55–58, c The University of Tokyo, September 24-25, 2010. 2010 Association for Computational Linguistics 55 a feature correlation matrix, using unlabeled data. In a second section, we show how to extend feature vectors in order to include co-occurrence information. Finally, we describe the features used in the discourse relation classifiers. been built in the RST framework. In duVerle and Prendinger (2009), a discourse parser based on Support Vector Machines (SVM) (Vapnik, 1995) is proposed. Shallow lexical, syntactic and structural features, including ‘dominance sets’ (Soricut and Marcu, 2003) are used. The unsupervised method of Marcu and Echihabi (2002) was the first to try to detect ‘implicit’ relations (i.e. relations not accompanied by a cue phrase, such as ‘however’, ‘but’), using word pairs extracted from two spans of text. Their method attempts to capture the difference of polarity in words. Discourse relation classifiers have also been trained using PDTB. Pitler et al. (2008) performed a corpus study of the PDTB, and found that ‘explicit’ relations can be most of the times distinguished by their discourse connectives. Lin et al. (2009) studied the problem of detecting impl"
W10-4309,W01-1605,0,0.0593859,"ackle this problem by employing a semi-supervised method for discourse relation classification. The proposed method is based on the analysis of feature cooccurrences in unlabeled data. This information is then used as a basis to extend the feature vectors during training. The proposed method is evaluated on both RST-DT and PDTB, where it significantly outperformed baseline classifiers. We believe that the proposed method is a first step towards improving classification performance, particularly for discourse relations lacking annotated data. 1 Introduction The RST Discourse Treebank (RST-DT) (Carlson et al., 2001), based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) framework, and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), are two of the most widely-used corpora for training discourse relation classifiers. They are both based on the Wall Street Journal (WSJ) corpus, although there are substantial differences in the relation taxonomy used to annotate the corpus. These corpora have been used in most of the recent work employing discourse relation classifiers, which are based 2 Related Work Since the release in 2002 of the RST-DT corpus, several fully-supervised discour"
W10-4309,P95-1037,0,\N,Missing
W10-4309,P09-1075,0,\N,Missing
W16-2804,llewellyn-etal-2014-using,0,0.0305258,"who have made a survey of the various works carried out in argument mining so far with an emphasis on the different machine learning approaches used, the two main approaches in argument mining relate to the extraction of abstract arguments (Cabrio and Villata, 2012; Yaglikci and Torroni, 2014) and structured arguments. Much recent work in extracting structured arguments has concentrated on extracting arguments pertaining to a specific domain such as ˇ online debates (Boltuˇzi´c and Snajder, 2014), user comments on blogs and forums (Ghosh et al., 2014; Park and Cardie, 2014), Twitter datasets (Llewellyn et al., 2014) and online product reviews (Wyner et al., 2012; Garcia-Villalba and Saint-Dizier, 2012). Each of these work target on identifying the kind of arguments that can be detected from a specific domain. Ghosh et al. (2014) analyse target-callout pairs among user comments, which are further annotated as stance/rationale callouts. Boltuzic and Snaider (2014) identify argument structures that they propose can help in stance classification. Our focus is not to identify the stance but to use the stance and the context of the relevant opinion to help in detecting and reconstructing enthymemes present in"
W16-2804,baccianella-etal-2010-sentiwordnet,0,0.0427903,"le-sized set of implicit opinions. For each such training set, we ran a 5 fold cross-validation and also tested it against the test set that we had created. We use the linear SVM classifier to train and test the data with the basic features (unigrams and bigrams respectively). The mean F1-scores for the crossvalidation on different train sets and the F1-scores on the test set for both explicit and implicit opinions are shown in Figure 2. The plot also contains the false positive rate for the test set with respect to different training sets. SentiWordNet score (senti) We used the SentiWordNet (Baccianella et al., 2010) lexical resource to assign scores for each word based on three sentiments i.e positive, negative and objective respectively. The positive, negative and objective scores sum up to 1. We use the individual lemmatized words in a statement as an input and obtain the scores for each of them. For each lemmatized word, we obtain the difference between their positive and negative score. We add up the computed scores for all the words present in a statement and average it which gives the overall statement score as a feature. Noun-Adjective patterns Both the statements in general expression cues and sp"
W16-2804,W14-2105,0,0.032113,"c. According to Lippi and Torroni (2015a) who have made a survey of the various works carried out in argument mining so far with an emphasis on the different machine learning approaches used, the two main approaches in argument mining relate to the extraction of abstract arguments (Cabrio and Villata, 2012; Yaglikci and Torroni, 2014) and structured arguments. Much recent work in extracting structured arguments has concentrated on extracting arguments pertaining to a specific domain such as ˇ online debates (Boltuˇzi´c and Snajder, 2014), user comments on blogs and forums (Ghosh et al., 2014; Park and Cardie, 2014), Twitter datasets (Llewellyn et al., 2014) and online product reviews (Wyner et al., 2012; Garcia-Villalba and Saint-Dizier, 2012). Each of these work target on identifying the kind of arguments that can be detected from a specific domain. Ghosh et al. (2014) analyse target-callout pairs among user comments, which are further annotated as stance/rationale callouts. Boltuzic and Snaider (2014) identify argument structures that they propose can help in stance classification. Our focus is not to identify the stance but to use the stance and the context of the relevant opinion to help in detectin"
W16-2804,W14-2107,0,0.090202,"Missing"
W16-2804,P12-2041,0,0.0444473,"with the help of local sentiment (positive or negative) and discard the rest of the statements. 32 unstructured texts found online. It is very difficult to extract properly formed arguments in online discussions and the absence of proper annotated corpora for automatic identification of these arguments is problematic. According to Lippi and Torroni (2015a) who have made a survey of the various works carried out in argument mining so far with an emphasis on the different machine learning approaches used, the two main approaches in argument mining relate to the extraction of abstract arguments (Cabrio and Villata, 2012; Yaglikci and Torroni, 2014) and structured arguments. Much recent work in extracting structured arguments has concentrated on extracting arguments pertaining to a specific domain such as ˇ online debates (Boltuˇzi´c and Snajder, 2014), user comments on blogs and forums (Ghosh et al., 2014; Park and Cardie, 2014), Twitter datasets (Llewellyn et al., 2014) and online product reviews (Wyner et al., 2012; Garcia-Villalba and Saint-Dizier, 2012). Each of these work target on identifying the kind of arguments that can be detected from a specific domain. Ghosh et al. (2014) analyse target-callout p"
W16-2804,P11-1099,0,0.0681509,"Missing"
W16-2804,W14-2106,0,0.0455792,"uments is problematic. According to Lippi and Torroni (2015a) who have made a survey of the various works carried out in argument mining so far with an emphasis on the different machine learning approaches used, the two main approaches in argument mining relate to the extraction of abstract arguments (Cabrio and Villata, 2012; Yaglikci and Torroni, 2014) and structured arguments. Much recent work in extracting structured arguments has concentrated on extracting arguments pertaining to a specific domain such as ˇ online debates (Boltuˇzi´c and Snajder, 2014), user comments on blogs and forums (Ghosh et al., 2014; Park and Cardie, 2014), Twitter datasets (Llewellyn et al., 2014) and online product reviews (Wyner et al., 2012; Garcia-Villalba and Saint-Dizier, 2012). Each of these work target on identifying the kind of arguments that can be detected from a specific domain. Ghosh et al. (2014) analyse target-callout pairs among user comments, which are further annotated as stance/rationale callouts. Boltuzic and Snaider (2014) identify argument structures that they propose can help in stance classification. Our focus is not to identify the stance but to use the stance and the context of the relevant opi"
W16-2804,C14-1053,0,0.0701155,"resent in health product advertisements. Amgoud et al. (2015) propose a formal language approach to construct arguments from natural language texts that are mostly enthymemes. Their work is related to mined arguments from texts that can be represented using a logical language and our work could be useful for evaluating (Amgoud et al., 2015) on a real dataset. Unlike the above, our approach classifies stances which can identify enthymemes and implicit premises that are present in online reviews. Research in opinion mining has started to understand the argumentative nature of opinionated texts (Wachsmuth et al., 2014a; Vincent and Winterstein, 2014). This growing interest to summarise what people write in online reviews and not just to identify the opinions is much of the motivation for our paper. 3 3.1 ble (predefined) conclusions for the hotel reviews, and these were: Conclusion 1 The reviewer is in favor of an aspect of the hotel or the hotel itself. Conclusion 2 The reviewer is against an aspect of the hotel or the hotel itself. We then annotated each of the 784 opinions with one of these conclusions. This was done to make the annotation procedure easier, since each opinion related to the conclusion f"
W16-2804,P13-1160,0,0.0299373,"Argument Major premise I am extremely disappointed with the room. Minor premise the rooms are small and dirty Conclusion The reviewer is not in favour of the hotel. While developing this approach is our longterm goal, this paper has a much more limited focus. In particular we consider Step 1(c), and study the classification of opinions into those with explicit stance and those with implicit stance. We focus on user reviews such as product reviews on Amazon.com, or hotel reviews on TripAdvisor.com. Such data has been extensively researched for sentiment classification tasks (Hu and Liu, 2004; Lazaridou et al., 2013). We build on this work, in particular, aspect-based approaches. In these approaches, sentiment classification is based around the detection of terms that denote aspects of the item being reviewed — the battery in the case of reviews of portable electronics, the room and the pool in the case of hotel reviews — and whether the sentiment expressed about these aspects is positive or negative. Our contributions in this paper are as follows: • As described above, we propose a two-step framework that identifies opinion structures in aspect-based statements which help in detecting enthymemes and reco"
