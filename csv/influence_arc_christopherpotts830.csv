2020.blackboxnlp-1.16,Q19-1004,0,0.114789,"on. These tests probe the generalization capabilities of state-of-the-art models with respect to the tasks they have been trained on, by focusing on difficult or underrepresented examples in a model’s training set (Jia and Liang, 2017; Naik et al., 2018; Glockner et al., 2018; Richardson et al., 2019; Talmor et al., 2019). 2 Probing Probes are supervised learning models trained to extract information from representations created by another model. They are a primary tool in the analysis of neural network models (Peters et al. 2018; Tenney et al. 2019; Clark et al. 2019; for a full review, see Belinkov and Glass 2019). In aggregate, this work has provided nuanced insights into the internal representations of these models, as well as their capacity to directly support learning diverse NLP tasks via fine-tuning (Hewitt and Liang, 2019). However, probes are only able to reveal how representations correlate with information. They cannot determine if that information plays a causal role in model predictions (Belinkov and Glass, 2019; Vig et al., 2020). Systematic Generalization Tasks Fodor and Pylyshyn (1988) offer systematicity as a hallmark of human cognition. Systematicity says that certain behaviors are int"
2020.blackboxnlp-1.16,D15-1075,1,0.809239,"models. In this paper, our learning target concerns the role of monotonicity in NLI (MacCartney, 2009; Icard and Moss, 2013). Specifically, we would like to determine whether models can learn to represent lexical relations and accurately model that negation reverses entailment relations (e.g., dance entails move, but not move entails not dance). This property of negation is downward monotonicity. In service of pursuing this question, we present Monotonicity NLI (MoNLI), a new naturalistic NLI dataset for training and assessing systems on these semantic notions (Section 3). MoNLI extends SNLI (Bowman et al., 2015) to provide comprehensive coverage of examples that depend on lexical reasoning with and without negation. Using MoNLI, we conduct both behavioral and structural evaluations, seeking to provide a detailed picture of the solutions that top-performing models learn. We evaluate Enhanced Sequential Inference Models (Chen et al., 2016) and BERT-based models (Devlin et al., 2019), along with standard baselines. Previous work evaluating the ability of neural models to learn monotonicity has focused on challenge test sets and systematic generalization tasks (Yanaka et al., 2019b,a; Geiger et al., 2019"
2020.blackboxnlp-1.16,W19-4828,0,0.0200529,"rained, developed, and (standardly) tested on. These tests probe the generalization capabilities of state-of-the-art models with respect to the tasks they have been trained on, by focusing on difficult or underrepresented examples in a model’s training set (Jia and Liang, 2017; Naik et al., 2018; Glockner et al., 2018; Richardson et al., 2019; Talmor et al., 2019). 2 Probing Probes are supervised learning models trained to extract information from representations created by another model. They are a primary tool in the analysis of neural network models (Peters et al. 2018; Tenney et al. 2019; Clark et al. 2019; for a full review, see Belinkov and Glass 2019). In aggregate, this work has provided nuanced insights into the internal representations of these models, as well as their capacity to directly support learning diverse NLP tasks via fine-tuning (Hewitt and Liang, 2019). However, probes are only able to reveal how representations correlate with information. They cannot determine if that information plays a causal role in model predictions (Belinkov and Glass, 2019; Vig et al., 2020). Systematic Generalization Tasks Fodor and Pylyshyn (1988) offer systematicity as a hallmark of human cognition."
2020.blackboxnlp-1.16,N19-1423,0,0.232809,"downward monotonicity. In service of pursuing this question, we present Monotonicity NLI (MoNLI), a new naturalistic NLI dataset for training and assessing systems on these semantic notions (Section 3). MoNLI extends SNLI (Bowman et al., 2015) to provide comprehensive coverage of examples that depend on lexical reasoning with and without negation. Using MoNLI, we conduct both behavioral and structural evaluations, seeking to provide a detailed picture of the solutions that top-performing models learn. We evaluate Enhanced Sequential Inference Models (Chen et al., 2016) and BERT-based models (Devlin et al., 2019), along with standard baselines. Previous work evaluating the ability of neural models to learn monotonicity has focused on challenge test sets and systematic generalization tasks (Yanaka et al., 2019b,a; Geiger et al., 2019; Richardson et al., 2019). These behavioral evaluations ask whether models achieve a desired input– output behavior. We employ these methods as well, but we also ask whether models achieve an algorithmic-level learning target, in the terms of Marr (1982). Monotonicity reasoning can be cast as an algorithm that solves MoNLI perfectly. Do neural models implement this algorit"
2020.blackboxnlp-1.16,D19-1456,1,0.890002,"owman et al., 2015) to provide comprehensive coverage of examples that depend on lexical reasoning with and without negation. Using MoNLI, we conduct both behavioral and structural evaluations, seeking to provide a detailed picture of the solutions that top-performing models learn. We evaluate Enhanced Sequential Inference Models (Chen et al., 2016) and BERT-based models (Devlin et al., 2019), along with standard baselines. Previous work evaluating the ability of neural models to learn monotonicity has focused on challenge test sets and systematic generalization tasks (Yanaka et al., 2019b,a; Geiger et al., 2019; Richardson et al., 2019). These behavioral evaluations ask whether models achieve a desired input– output behavior. We employ these methods as well, but we also ask whether models achieve an algorithmic-level learning target, in the terms of Marr (1982). Monotonicity reasoning can be cast as an algorithm that solves MoNLI perfectly. Do neural models implement this algorithm? We first report on two behavioral evaluations (Section 5). When MoNLI is used as a challenge test set, we find that models trained on SNLI and/or MNLI (Williams et al., 2018) fail to reason with lex163 Proceedings of the"
2020.blackboxnlp-1.16,W18-5426,0,0.0385052,"Test Sets Challenge1 test sets are supplementary evaluation resources that test the ability of a model to generalize to examples outside the dis1 Though adversarial and challenge are sometimes used synonymously, we opt for the term challenge, because our dataset was designed with the intention of evaluating whether a model learned a particular phenomenon, as opposed to breaking any particular model (cf. Nie et al. 2019b). Interventions Intervention studies go beyond probing to make changes to the internal states of a network, with the goal of observing how those changes affect system outputs. Giulianelli et al. (2018) use probing results to make informed interventions during LSTM language model predictions to preserve information about the grammatical subject’s number, and this led to improved performance in subject–verb agreement. Vig et al. (2020) use interventions to characterize how gender bias is represented in the internal causal structure 164 of a model, and find that a small number of synergistic neurons mediate gender bias. They also find that the effect of these neurons is roughly linearly separable from the effect of the remainder of the model, a remarkable finding considering the highly non-lin"
2020.blackboxnlp-1.16,P18-2103,0,0.40224,"s, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level. 1 Introduction Natural Language Inference (NLI) keys into fundamental aspects of how people reason with language. Although NLI is generally cast in informal terms that embrace the indeterminacy of such reasoning, the task nonetheless manifests a number of very predictable reasoning patterns. For example, systematic manipulations of the lexical meanings (Glockner et al., 2018), syntactic constructions (Nie et al., 2019a), and contextual assumptions (Pavlick and Callison-Burch, 2016) have systematic effects on the correct labels. These patterns present crisp, motivated learning targets that we can leverage to Christopher Potts Stanford University cgpotts@stanford.edu not only evaluate the ability of NLI models to learn robust solutions, but also to analyze the internal dynamics of successful models. In this paper, our learning target concerns the role of monotonicity in NLI (MacCartney, 2009; Icard and Moss, 2013). Specifically, we would like to determine whether mo"
2020.blackboxnlp-1.16,2020.acl-main.177,0,0.148312,"Missing"
2020.blackboxnlp-1.16,N18-2017,0,0.0943006,"Missing"
2020.blackboxnlp-1.16,D19-1275,0,0.136717,"nd Liang, 2017; Naik et al., 2018; Glockner et al., 2018; Richardson et al., 2019; Talmor et al., 2019). 2 Probing Probes are supervised learning models trained to extract information from representations created by another model. They are a primary tool in the analysis of neural network models (Peters et al. 2018; Tenney et al. 2019; Clark et al. 2019; for a full review, see Belinkov and Glass 2019). In aggregate, this work has provided nuanced insights into the internal representations of these models, as well as their capacity to directly support learning diverse NLP tasks via fine-tuning (Hewitt and Liang, 2019). However, probes are only able to reveal how representations correlate with information. They cannot determine if that information plays a causal role in model predictions (Belinkov and Glass, 2019; Vig et al., 2020). Systematic Generalization Tasks Fodor and Pylyshyn (1988) offer systematicity as a hallmark of human cognition. Systematicity says that certain behaviors are intrinsically connected to others by compositional structures. For example, understanding the puppy loves Sandy is intrinsically connected to understanding Sandy loves the puppy. For Fodor and Pylyshyn, these observations t"
2020.blackboxnlp-1.16,W19-0502,0,0.0164018,"et al., 2019; Yanaka et al., 2020; Bahdanau et al., 2018; Geiger et al., 2019; Goodwin et al., 2020). Related work Monotonicity Our empirical focus is entailment and negation. This is one (highly prevalent) aspect of monotonicity reasoning, which governs many aspects of lexical and constructional meaning in natural language (S´anchez-Valencia, 1991; van Benthem, 2008). There is an extensive literature on monotonicity logics (Moss, 2009; Icard, 2012; Icard and Moss, 2013; Icard et al., 2017). Within NLP, MacCartney and Manning (2008, 2009) apply very rich monotonicity algebras to NLI problems, Hu et al. (2019a,b) create NLI models that use polarity-marked parse trees, and Yanaka et al. (2019a,b) and Geiger et al. (2019) investigate the ability of neural models to understand natural logic reasoning. While we consider only a small fragment of these approaches, the methods we develop should apply to more complex systems as well. Challenge Test Sets Challenge1 test sets are supplementary evaluation resources that test the ability of a model to generalize to examples outside the dis1 Though adversarial and challenge are sometimes used synonymously, we opt for the term challenge, because our dataset was"
2020.blackboxnlp-1.16,2020.scil-1.40,1,0.818482,"Missing"
2020.blackboxnlp-1.16,W18-5419,0,0.0449605,"Missing"
2020.blackboxnlp-1.16,D17-1215,0,0.0299349,"ence that BERT does mirror the causal dynamics of the monotonicity algorithm, at least on large subsets of MoNLI. We conclude that this model at least partially embeds a theory of lexical entailment and negation at an algorithmic level, in addition to fully achieving the correct input–output behavior on MoNLI. tribution of the data it was trained, developed, and (standardly) tested on. These tests probe the generalization capabilities of state-of-the-art models with respect to the tasks they have been trained on, by focusing on difficult or underrepresented examples in a model’s training set (Jia and Liang, 2017; Naik et al., 2018; Glockner et al., 2018; Richardson et al., 2019; Talmor et al., 2019). 2 Probing Probes are supervised learning models trained to extract information from representations created by another model. They are a primary tool in the analysis of neural network models (Peters et al. 2018; Tenney et al. 2019; Clark et al. 2019; for a full review, see Belinkov and Glass 2019). In aggregate, this work has provided nuanced insights into the internal representations of these models, as well as their capacity to directly support learning diverse NLP tasks via fine-tuning (Hewitt and Lia"
2020.blackboxnlp-1.16,N19-1225,0,0.163456,"Missing"
2020.blackboxnlp-1.16,C08-1066,0,0.0581597,"o as systematic generalization tasks (Lake and Baroni, 2018; Hupkes et al., 2019; Yanaka et al., 2020; Bahdanau et al., 2018; Geiger et al., 2019; Goodwin et al., 2020). Related work Monotonicity Our empirical focus is entailment and negation. This is one (highly prevalent) aspect of monotonicity reasoning, which governs many aspects of lexical and constructional meaning in natural language (S´anchez-Valencia, 1991; van Benthem, 2008). There is an extensive literature on monotonicity logics (Moss, 2009; Icard, 2012; Icard and Moss, 2013; Icard et al., 2017). Within NLP, MacCartney and Manning (2008, 2009) apply very rich monotonicity algebras to NLI problems, Hu et al. (2019a,b) create NLI models that use polarity-marked parse trees, and Yanaka et al. (2019a,b) and Geiger et al. (2019) investigate the ability of neural models to understand natural logic reasoning. While we consider only a small fragment of these approaches, the methods we develop should apply to more complex systems as well. Challenge Test Sets Challenge1 test sets are supplementary evaluation resources that test the ability of a model to generalize to examples outside the dis1 Though adversarial and challenge are sometimes us"
2020.blackboxnlp-1.16,W09-3714,0,0.116811,"Missing"
2020.blackboxnlp-1.16,C18-1198,0,0.0459232,"irror the causal dynamics of the monotonicity algorithm, at least on large subsets of MoNLI. We conclude that this model at least partially embeds a theory of lexical entailment and negation at an algorithmic level, in addition to fully achieving the correct input–output behavior on MoNLI. tribution of the data it was trained, developed, and (standardly) tested on. These tests probe the generalization capabilities of state-of-the-art models with respect to the tasks they have been trained on, by focusing on difficult or underrepresented examples in a model’s training set (Jia and Liang, 2017; Naik et al., 2018; Glockner et al., 2018; Richardson et al., 2019; Talmor et al., 2019). 2 Probing Probes are supervised learning models trained to extract information from representations created by another model. They are a primary tool in the analysis of neural network models (Peters et al. 2018; Tenney et al. 2019; Clark et al. 2019; for a full review, see Belinkov and Glass 2019). In aggregate, this work has provided nuanced insights into the internal representations of these models, as well as their capacity to directly support learning diverse NLP tasks via fine-tuning (Hewitt and Liang, 2019). However,"
2020.blackboxnlp-1.16,P16-1204,0,0.0226465,"subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level. 1 Introduction Natural Language Inference (NLI) keys into fundamental aspects of how people reason with language. Although NLI is generally cast in informal terms that embrace the indeterminacy of such reasoning, the task nonetheless manifests a number of very predictable reasoning patterns. For example, systematic manipulations of the lexical meanings (Glockner et al., 2018), syntactic constructions (Nie et al., 2019a), and contextual assumptions (Pavlick and Callison-Burch, 2016) have systematic effects on the correct labels. These patterns present crisp, motivated learning targets that we can leverage to Christopher Potts Stanford University cgpotts@stanford.edu not only evaluate the ability of NLI models to learn robust solutions, but also to analyze the internal dynamics of successful models. In this paper, our learning target concerns the role of monotonicity in NLI (MacCartney, 2009; Icard and Moss, 2013). Specifically, we would like to determine whether models can learn to represent lexical relations and accurately model that negation reverses entailment relatio"
2020.blackboxnlp-1.16,D18-1179,0,0.0266781,"on MoNLI. tribution of the data it was trained, developed, and (standardly) tested on. These tests probe the generalization capabilities of state-of-the-art models with respect to the tasks they have been trained on, by focusing on difficult or underrepresented examples in a model’s training set (Jia and Liang, 2017; Naik et al., 2018; Glockner et al., 2018; Richardson et al., 2019; Talmor et al., 2019). 2 Probing Probes are supervised learning models trained to extract information from representations created by another model. They are a primary tool in the analysis of neural network models (Peters et al. 2018; Tenney et al. 2019; Clark et al. 2019; for a full review, see Belinkov and Glass 2019). In aggregate, this work has provided nuanced insights into the internal representations of these models, as well as their capacity to directly support learning diverse NLP tasks via fine-tuning (Hewitt and Liang, 2019). However, probes are only able to reveal how representations correlate with information. They cannot determine if that information plays a causal role in model predictions (Belinkov and Glass, 2019; Vig et al., 2020). Systematic Generalization Tasks Fodor and Pylyshyn (1988) offer systemati"
2020.blackboxnlp-1.16,P19-1452,0,0.0140435,"of the data it was trained, developed, and (standardly) tested on. These tests probe the generalization capabilities of state-of-the-art models with respect to the tasks they have been trained on, by focusing on difficult or underrepresented examples in a model’s training set (Jia and Liang, 2017; Naik et al., 2018; Glockner et al., 2018; Richardson et al., 2019; Talmor et al., 2019). 2 Probing Probes are supervised learning models trained to extract information from representations created by another model. They are a primary tool in the analysis of neural network models (Peters et al. 2018; Tenney et al. 2019; Clark et al. 2019; for a full review, see Belinkov and Glass 2019). In aggregate, this work has provided nuanced insights into the internal representations of these models, as well as their capacity to directly support learning diverse NLP tasks via fine-tuning (Hewitt and Liang, 2019). However, probes are only able to reveal how representations correlate with information. They cannot determine if that information plays a causal role in model predictions (Belinkov and Glass, 2019; Vig et al., 2020). Systematic Generalization Tasks Fodor and Pylyshyn (1988) offer systematicity as a hallmark o"
2020.blackboxnlp-1.16,N18-1101,0,0.21251,"ic generalization tasks (Yanaka et al., 2019b,a; Geiger et al., 2019; Richardson et al., 2019). These behavioral evaluations ask whether models achieve a desired input– output behavior. We employ these methods as well, but we also ask whether models achieve an algorithmic-level learning target, in the terms of Marr (1982). Monotonicity reasoning can be cast as an algorithm that solves MoNLI perfectly. Do neural models implement this algorithm? We first report on two behavioral evaluations (Section 5). When MoNLI is used as a challenge test set, we find that models trained on SNLI and/or MNLI (Williams et al., 2018) fail to reason with lex163 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 163–173 c Online, November 20, 2020. 2020 Association for Computational Linguistics ical entailments when negation is involved. However, we trace these failures to gaps in the training data. In response, we pose a systematic generalization task in which we expose models to MoNLI examples through fine-tuning while still requiring them to generalize to entirely new pairs of lexical items in negated linguistic contexts at test time. All our models solve the task,"
2020.blackboxnlp-1.16,2020.acl-main.543,0,0.148173,"ity as a hallmark of human cognition. Systematicity says that certain behaviors are intrinsically connected to others by compositional structures. For example, understanding the puppy loves Sandy is intrinsically connected to understanding Sandy loves the puppy. For Fodor and Pylyshyn, these observations trace to the mind’s ability to recombine known parts and rules. There are often strong intuitions that certain generalization tasks are only solved by models with systematic structures. These tasks are referred to as systematic generalization tasks (Lake and Baroni, 2018; Hupkes et al., 2019; Yanaka et al., 2020; Bahdanau et al., 2018; Geiger et al., 2019; Goodwin et al., 2020). Related work Monotonicity Our empirical focus is entailment and negation. This is one (highly prevalent) aspect of monotonicity reasoning, which governs many aspects of lexical and constructional meaning in natural language (S´anchez-Valencia, 1991; van Benthem, 2008). There is an extensive literature on monotonicity logics (Moss, 2009; Icard, 2012; Icard and Moss, 2013; Icard et al., 2017). Within NLP, MacCartney and Manning (2008, 2009) apply very rich monotonicity algebras to NLI problems, Hu et al. (2019a,b) create NLI mo"
2020.blackboxnlp-1.16,S19-1027,0,0.0704612,"). MoNLI extends SNLI (Bowman et al., 2015) to provide comprehensive coverage of examples that depend on lexical reasoning with and without negation. Using MoNLI, we conduct both behavioral and structural evaluations, seeking to provide a detailed picture of the solutions that top-performing models learn. We evaluate Enhanced Sequential Inference Models (Chen et al., 2016) and BERT-based models (Devlin et al., 2019), along with standard baselines. Previous work evaluating the ability of neural models to learn monotonicity has focused on challenge test sets and systematic generalization tasks (Yanaka et al., 2019b,a; Geiger et al., 2019; Richardson et al., 2019). These behavioral evaluations ask whether models achieve a desired input– output behavior. We employ these methods as well, but we also ask whether models achieve an algorithmic-level learning target, in the terms of Marr (1982). Monotonicity reasoning can be cast as an algorithm that solves MoNLI perfectly. Do neural models implement this algorithm? We first report on two behavioral evaluations (Section 5). When MoNLI is used as a challenge test set, we find that models trained on SNLI and/or MNLI (Williams et al., 2018) fail to reason with l"
2020.conll-1.5,J12-2003,1,0.835726,"Missing"
2020.conll-1.5,D18-1501,0,0.0469758,"Missing"
2020.emnlp-main.662,N18-2017,0,0.0929175,"Missing"
2020.emnlp-main.662,2020.eamt-1.50,0,0.0202548,"lish will generalize to other languages. A natural response to these gaps in our dataset coverage might be to launch new annotation efforts for multiple languages. However, this would likely be prohibitively expensive. For example, based on the costs of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018a), we estimate that each large dataset for NLI would cost upwards of US $50,000 if created completely from scratch. At the same time, commercial MT systems have improved dramatically in recent years (Wu et al., 2016; Johnson et al., 2017; Hieber et al., 2017, 2018; Tomasello, 2019; Hieber et al., 2020). They now offer high-quality translations between hundreds of language pairs. This raises the question: can we use these MT systems to translate Englishlanguage datasets and use the translated versions to drive more genuinely multilingual development in NLP? In this paper, we offer evidence that the answer is “yes”. Using Amazon Translate, we translated SNLI and MultiNLI from English into Turkish to create the first large Turkish NLI data sets, NLI-TR, at a tiny fraction of the cost of creating them from scratch. Turkish is an interesting challenge in this context since it is very different f"
2020.emnlp-main.662,N10-1045,0,0.082617,"Missing"
2020.emnlp-main.662,E17-3017,0,0.0228096,"ky to assume that models and results for English will generalize to other languages. A natural response to these gaps in our dataset coverage might be to launch new annotation efforts for multiple languages. However, this would likely be prohibitively expensive. For example, based on the costs of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018a), we estimate that each large dataset for NLI would cost upwards of US $50,000 if created completely from scratch. At the same time, commercial MT systems have improved dramatically in recent years (Wu et al., 2016; Johnson et al., 2017; Hieber et al., 2017, 2018; Tomasello, 2019; Hieber et al., 2020). They now offer high-quality translations between hundreds of language pairs. This raises the question: can we use these MT systems to translate Englishlanguage datasets and use the translated versions to drive more genuinely multilingual development in NLP? In this paper, we offer evidence that the answer is “yes”. Using Amazon Translate, we translated SNLI and MultiNLI from English into Turkish to create the first large Turkish NLI data sets, NLI-TR, at a tiny fraction of the cost of creating them from scratch. Turkish is an interesting challenge"
2020.emnlp-main.662,W18-1820,0,0.0285336,"Missing"
2020.emnlp-main.662,P18-1007,0,0.0127325,"st one epoch. with an interval of 1,000 training steps to observe the effect of morphological parsing as the dataset grew. Figure 1 reports the accuracy of all models with respect to fine-tuning steps on NLI-TR development sets, and Table 5 shows the final accuracies. 4.2.2 Results Figure 1 suggests that morphological parsing is beneficial where the training set is small, but its importance largely disappears for large training sets. This is reflected also in the final results in Table 5. We relate this to the fact that BERT models create contextual embeddings of both word and subword tokens (Kudo, 2018; Kudo and Richardson, 2018; Sennrich et al., 2016). Given a sufficiently large dataset, BERT models can approximate the effects of morphological parsing even for Turkish, a morphologically-rich language. The trends are not uniform for SNLI-TR and MultiNLI-TR. For SNLI-TR, all three models display a similar learning curve, with a slight edge for Zemberek early on. For MultiNLI-TR, models with morphological parsers are more differentiated. However, all three converge to similar performance at the end of training on both datasets (Table 5). In light of these findings, we suggest avoiding the use"
2020.emnlp-main.662,D18-2012,0,0.0155293,". with an interval of 1,000 training steps to observe the effect of morphological parsing as the dataset grew. Figure 1 reports the accuracy of all models with respect to fine-tuning steps on NLI-TR development sets, and Table 5 shows the final accuracies. 4.2.2 Results Figure 1 suggests that morphological parsing is beneficial where the training set is small, but its importance largely disappears for large training sets. This is reflected also in the final results in Table 5. We relate this to the fact that BERT models create contextual embeddings of both word and subword tokens (Kudo, 2018; Kudo and Richardson, 2018; Sennrich et al., 2016). Given a sufficiently large dataset, BERT models can approximate the effects of morphological parsing even for Turkish, a morphologically-rich language. The trends are not uniform for SNLI-TR and MultiNLI-TR. For SNLI-TR, all three models display a similar learning curve, with a slight edge for Zemberek early on. For MultiNLI-TR, models with morphological parsers are more differentiated. However, all three converge to similar performance at the end of training on both datasets (Table 5). In light of these findings, we suggest avoiding the use of morphological parsers f"
2020.emnlp-main.662,P11-1134,0,0.0510815,"Missing"
2020.emnlp-main.662,D11-1062,0,0.0418982,"Missing"
2020.emnlp-main.662,S12-1053,0,0.0605637,"Missing"
2020.emnlp-main.662,S13-2005,0,0.0425257,"Missing"
2020.emnlp-main.662,W10-0734,0,0.0874028,"Missing"
2020.emnlp-main.662,L16-1262,0,0.0290802,"Missing"
2020.emnlp-main.662,W19-3110,0,0.0239805,"Missing"
2020.emnlp-main.662,D16-1264,0,0.0311661,"dress core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machinetranslated datasets are successful on humantranslated evaluation sets. We share all code, models, and data publicly. 1 Introduction Many tasks in natural language processing have been transformed by the introduction of very large annotated datasets. Prominent examples include paraphrase (Ganitkevitch et al., 2013), parsing (Nivre et al., 2016), question answering (Rajpurkar et al., 2016), machine translation (MT; Bojar et al., 2014), and natural language inference (NLI; Bowman et al., 2015; Williams et al., 2018a). Unfortunately, outside of parsing and MT, these datasets tend to be in English. This is not only an obstacle to progress on other languages, but it also limits the field of NLP itself: English is generally not a representative example of the world’s languages when it comes to morphology, syntax, or spelling conventions and other kinds of standardization (Munro, 2012), so it’s risky to assume that models and results for English will generalize to other languages. A"
2020.emnlp-main.662,P09-2069,1,0.744066,"Missing"
2020.emnlp-main.662,P16-1162,0,0.00574257,"training steps to observe the effect of morphological parsing as the dataset grew. Figure 1 reports the accuracy of all models with respect to fine-tuning steps on NLI-TR development sets, and Table 5 shows the final accuracies. 4.2.2 Results Figure 1 suggests that morphological parsing is beneficial where the training set is small, but its importance largely disappears for large training sets. This is reflected also in the final results in Table 5. We relate this to the fact that BERT models create contextual embeddings of both word and subword tokens (Kudo, 2018; Kudo and Richardson, 2018; Sennrich et al., 2016). Given a sufficiently large dataset, BERT models can approximate the effects of morphological parsing even for Turkish, a morphologically-rich language. The trends are not uniform for SNLI-TR and MultiNLI-TR. For SNLI-TR, all three models display a similar learning curve, with a slight edge for Zemberek early on. For MultiNLI-TR, models with morphological parsers are more differentiated. However, all three converge to similar performance at the end of training on both datasets (Table 5). In light of these findings, we suggest avoiding the use of morphological parsers for Turkish NLI where the"
2020.emnlp-main.662,N18-1101,0,0.169554,"sing can be avoided where the training set is large. Finally, we show that models trained on our machinetranslated datasets are successful on humantranslated evaluation sets. We share all code, models, and data publicly. 1 Introduction Many tasks in natural language processing have been transformed by the introduction of very large annotated datasets. Prominent examples include paraphrase (Ganitkevitch et al., 2013), parsing (Nivre et al., 2016), question answering (Rajpurkar et al., 2016), machine translation (MT; Bojar et al., 2014), and natural language inference (NLI; Bowman et al., 2015; Williams et al., 2018a). Unfortunately, outside of parsing and MT, these datasets tend to be in English. This is not only an obstacle to progress on other languages, but it also limits the field of NLP itself: English is generally not a representative example of the world’s languages when it comes to morphology, syntax, or spelling conventions and other kinds of standardization (Munro, 2012), so it’s risky to assume that models and results for English will generalize to other languages. A natural response to these gaps in our dataset coverage might be to launch new annotation efforts for multiple languages. Howeve"
2020.emnlp-main.662,2020.emnlp-demos.6,0,0.0629347,"Missing"
2020.emnlp-main.662,Q14-1006,0,0.0284165,"ee of these parsers in this work to evaluate the role of morphology in NLI systems (Section 4.2). 8254 3 3.1 Creating and Validating NLI-TR English NLI Datasets We translated the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre Natural Language Inference Corpus (MultiNLI; Williams et al., 2018b) to create labeled NLI datasets for Turkish, NLI-TR. SNLI contains ≈570K semantically related English sentence pairs. The semantic relations are entailment, contradiction, and neutral. The premise sentences for SNLI are image captions from the Flickr30K corpus (Young et al., 2014), and the hypothesis sentences were written by crowdworkers. SNLI texts are mostly short and structurally simple. We translated SNLI while respecting the train, development (dev), and test splits. MultiNLI comprises ≈433K sentence pairs in English, and the pairs have the same semantic relations as SNLI. However, MultiNLI spans a broader range of genres, including travel guides, fiction, dialogue, and journalism. As a result, the texts are generally more complex than SNLI. In addition, MultiNLI contains matched and mismatched dev and test sets, where the sentences in the former set are from the"
2020.findings-emnlp.173,D16-1125,0,0.330921,"al. (2010). RSA defines nested probabilistic speaker and listener agents that reason about each other in communication to enrich the basic semantics of their language. The model has been applied to a wide variety of diverse linguistic phenomena. Since RSA is a probabilistic model of communication, it is amenable for incorporation into many modern NLP architectures. A growing body of literature shows that adding RSA components to NLP architectures can help them to capture important aspects of context dependence in language, including referential description generation (Monroe and Potts, 2015; Andreas and Klein, 2016; Monroe et al., 2017), instruction following (Fried et al., 2018), collaborative problem solving (Tellex et al., 2014), and translation (CohnGordon and Goodman, 2019). Broadly speaking, there are two kinds of approaches to incorporating RSA into NLP systems. One class performs end-to-end learning of the RSA agents (Monroe and Potts, 2015; Mao et al., 2016; White et al., 2020). The other uses a pretrained system and applies RSA at the decoding stage (Andreas and Klein, 2016; Vedantam et al., 2017; Monroe et al., 2017; Fried et al., 2018). We adopt this second approach, as it highlights the way"
2020.findings-emnlp.173,N19-1042,1,0.884005,"Missing"
2020.findings-emnlp.173,P82-1020,0,0.539935,"Missing"
2020.findings-emnlp.173,Q17-1023,1,0.936093,"nested probabilistic speaker and listener agents that reason about each other in communication to enrich the basic semantics of their language. The model has been applied to a wide variety of diverse linguistic phenomena. Since RSA is a probabilistic model of communication, it is amenable for incorporation into many modern NLP architectures. A growing body of literature shows that adding RSA components to NLP architectures can help them to capture important aspects of context dependence in language, including referential description generation (Monroe and Potts, 2015; Andreas and Klein, 2016; Monroe et al., 2017), instruction following (Fried et al., 2018), collaborative problem solving (Tellex et al., 2014), and translation (CohnGordon and Goodman, 2019). Broadly speaking, there are two kinds of approaches to incorporating RSA into NLP systems. One class performs end-to-end learning of the RSA agents (Monroe and Potts, 2015; Mao et al., 2016; White et al., 2020). The other uses a pretrained system and applies RSA at the decoding stage (Andreas and Klein, 2016; Vedantam et al., 2017; Monroe et al., 2017; Fried et al., 2018). We adopt this second approach, as it highlights the ways in which one can imb"
2020.findings-emnlp.173,D10-1040,0,\N,Missing
2020.findings-emnlp.173,N18-1177,0,\N,Missing
2020.findings-emnlp.173,N18-2070,1,\N,Missing
2020.scil-1.16,P17-1022,0,0.0160658,"al. (2017) use conversational context to predict how human annotators would score dialogue agents, and the importance of context in assessment of this domain is noted by Liu et al. (2016). Our work incorporates contextual information by modeling the task a hypothetical listener will perform with the language produced. 2.2 Task-based Language Evaluation Our work is particularly relevant for evaluation of utterances in task-specific scenarios. Overwhelmingly, work in this area uses humans performing some task with model-generated utterances to evaluate these utterances (Andreas and Klein, 2016; Andreas et al., 2017; Golland et al., 2010; Mao et al., 2016; Vedantam et al., 2017). Additionally, automatic evaluation metrics have been proposed. Monroe et al. (2017) and Cohn-Gordon et al. (2018) use a combination of language models conditioned on the context and Bayes’ rule, while Mao et al. (2016) use their joint image and text classifier to evaluate potential object descriptions. We compare these two approaches as well. Additionally, referring expressions tend not to be evaluated using n-gram overlap metrics; Vedantam et al. (2017)’s use of CIDEr is an exception. As far as we know, these communication-base"
2020.scil-1.16,D16-1125,0,0.0138674,"016). Similarly, Lowe et al. (2017) use conversational context to predict how human annotators would score dialogue agents, and the importance of context in assessment of this domain is noted by Liu et al. (2016). Our work incorporates contextual information by modeling the task a hypothetical listener will perform with the language produced. 2.2 Task-based Language Evaluation Our work is particularly relevant for evaluation of utterances in task-specific scenarios. Overwhelmingly, work in this area uses humans performing some task with model-generated utterances to evaluate these utterances (Andreas and Klein, 2016; Andreas et al., 2017; Golland et al., 2010; Mao et al., 2016; Vedantam et al., 2017). Additionally, automatic evaluation metrics have been proposed. Monroe et al. (2017) and Cohn-Gordon et al. (2018) use a combination of language models conditioned on the context and Bayes’ rule, while Mao et al. (2016) use their joint image and text classifier to evaluate potential object descriptions. We compare these two approaches as well. Additionally, referring expressions tend not to be evaluated using n-gram overlap metrics; Vedantam et al. (2017)’s use of CIDEr is an exception. As far as we know, th"
2020.scil-1.16,W05-0909,0,0.254717,"ions for n-grams of different sizes are then geometrically averaged together. Conventionally, n-gram overlaps for n = 1, 2, 3, and 4 are calculated. The second component of the BLEU score, the brevity penalty, acts as a recall constraint. Long candidate utterances could achieve a high modified ngram precision by containing many n-grams, but the brevity penalty negatively impacts the score of candidates longer than the reference. METEOR METEOR (Metric for Evaluation of Translation with Explicit ORdering), like BLEU, is designed for assessing utterances generated by machine translation systems (Banerjee and Lavie, 2005). METEOR searches for an alignment between the candidate and reference sentence using a form of beam search. Stemmed words, synonyms, and even paraphrases are considered in seeking the optimal alignment. This alignment is used to a calculate an F-score, usually favoring recall over precision. METEOR also has a “fragmentation score” that penalizes noncontiguous alignments and addresses issues related to word order. High METEOR scores mean large overlap between the tokens in the reference and candidate (including synonymy) as well as the CIDEr CIDEr (Consensus-based Image Description Evaluation)"
2020.scil-1.16,K16-1002,0,0.0364952,"elated Work NLG Evaluation Existing NLG evaluation methods make use of n-gram overlap scores, human evaluations, and model-based evaluations. Our own method blends human evaluation and model-based evaluation, as we advocate using humans or building models to act on generated language Other model-based evaluations take a variety of forms. Some involve training models to estimate human judgments of utterance quality (Lowe et al., 2017; Duˇsek et al., 2017; Kann et al., 2018). Others require training models to distinguish between language generated by humans and models—an adversarial evaluation (Bowman et al., 2016; Liu et al., 2016; Kannan and Vinyals, 2016; Bruni and Fern´andez, 2017). These methods focus on the utterance in a vacuum and tend to not to consider how language will actually interact with other conversational participants. They treat humans as assessors of quality or adversarial listeners, whereas our proposal takes the perspective that listeners are cooperative interlocutors who use the language they hear to inform their beliefs about the world. Our approach can also be seen as part of a larger effort to incorporate context into NLG evaluation. Prior work in this area includes the image"
2020.scil-1.16,W17-5534,0,0.0450111,"Missing"
2020.scil-1.16,P18-1060,0,0.0629902,"ans find them equally good, and negated candidates receive high scores even where the negation leads to dramatic deviation from the reference texts. Such metrics are particularly ineffective in scenarios where there are many potentially appropriate utterances (Liu et al., 2016; Novikova et al., 2017). To avoid this problem, one might turn to human judgments to assess the quality of modelgenerated language. In this setting, humans rate language according to grammaticality, typicality, informativeness, interestingness, and other qualitative dimensions (Lowe et al., 2017; Hashimoto et al., 2019; Chaganty et al., 2018). This addresses the problems with n-gram overlap methods, but it is expensive, and the human task does not reflect natural language use, which can lead to unreliable data. One shortcoming of these methods is that they fail to take into account the communicative function of language; a speaker’s goal is not only to produce well-formed expressions, but also to convey relevant information to a listener. Likewise, a listener is not only an assessor of quality, but also an agent that forms beliefs based on speakers’ utterances. Thus, our NLG systems should be expected to use language to communicat"
2020.scil-1.16,N18-2070,1,0.881391,"et al. (2016). Our work incorporates contextual information by modeling the task a hypothetical listener will perform with the language produced. 2.2 Task-based Language Evaluation Our work is particularly relevant for evaluation of utterances in task-specific scenarios. Overwhelmingly, work in this area uses humans performing some task with model-generated utterances to evaluate these utterances (Andreas and Klein, 2016; Andreas et al., 2017; Golland et al., 2010; Mao et al., 2016; Vedantam et al., 2017). Additionally, automatic evaluation metrics have been proposed. Monroe et al. (2017) and Cohn-Gordon et al. (2018) use a combination of language models conditioned on the context and Bayes’ rule, while Mao et al. (2016) use their joint image and text classifier to evaluate potential object descriptions. We compare these two approaches as well. Additionally, referring expressions tend not to be evaluated using n-gram overlap metrics; Vedantam et al. (2017)’s use of CIDEr is an exception. As far as we know, these communication-based and n-gram overlap evaluation approaches have not previously been compared. 2.3 Communicative Informativity Our communication-based evaluation method is closely related to the R"
2020.scil-1.16,finch-etal-2004-automatic,0,0.119069,"fferent n-grams sizes from the reference and candidate captions and calculating a weighted average of the cosine similarities between vectors for different n-gram sizes. Inverse document frequency is calculated over all of the reference sentences in the dataset. High CIDEr scores indicate that a candidate caption uses the same infrequent, and likely informative, n-grams as a number of the references. While the metrics described above (other than CIDEr) are defined for a single candidate and a single reference, the intention is that they be used with multiple reference texts per candidate, and Finch et al. (2004) found that using more reference sentences increases the reliability of these metrics. Gains start at 4 and continue up until 50 reference sentences in some cases (Vedantam et al., 2015). This is because a greater number of references provides more opportunities for the candidate to get a higher score. Because of this, when comparing these metrics to our communication-based evaluation we use multiple references. 4 Communication-based Evaluation We now define our communication-based evaluation method in general terms, leaving its specific application to the color reference game to Section 5. Fo"
2020.scil-1.16,D10-1040,0,0.0188477,"ational context to predict how human annotators would score dialogue agents, and the importance of context in assessment of this domain is noted by Liu et al. (2016). Our work incorporates contextual information by modeling the task a hypothetical listener will perform with the language produced. 2.2 Task-based Language Evaluation Our work is particularly relevant for evaluation of utterances in task-specific scenarios. Overwhelmingly, work in this area uses humans performing some task with model-generated utterances to evaluate these utterances (Andreas and Klein, 2016; Andreas et al., 2017; Golland et al., 2010; Mao et al., 2016; Vedantam et al., 2017). Additionally, automatic evaluation metrics have been proposed. Monroe et al. (2017) and Cohn-Gordon et al. (2018) use a combination of language models conditioned on the context and Bayes’ rule, while Mao et al. (2016) use their joint image and text classifier to evaluate potential object descriptions. We compare these two approaches as well. Additionally, referring expressions tend not to be evaluated using n-gram overlap metrics; Vedantam et al. (2017)’s use of CIDEr is an exception. As far as we know, these communication-based and n-gram overlap e"
2020.scil-1.16,N19-1169,0,0.0125025,"metrics even though humans find them equally good, and negated candidates receive high scores even where the negation leads to dramatic deviation from the reference texts. Such metrics are particularly ineffective in scenarios where there are many potentially appropriate utterances (Liu et al., 2016; Novikova et al., 2017). To avoid this problem, one might turn to human judgments to assess the quality of modelgenerated language. In this setting, humans rate language according to grammaticality, typicality, informativeness, interestingness, and other qualitative dimensions (Lowe et al., 2017; Hashimoto et al., 2019; Chaganty et al., 2018). This addresses the problems with n-gram overlap methods, but it is expensive, and the human task does not reflect natural language use, which can lead to unreliable data. One shortcoming of these methods is that they fail to take into account the communicative function of language; a speaker’s goal is not only to produce well-formed expressions, but also to convey relevant information to a listener. Likewise, a listener is not only an assessor of quality, but also an agent that forms beliefs based on speakers’ utterances. Thus, our NLG systems should be expected to us"
2020.scil-1.16,K18-1031,0,0.0167765,"uggest that, when evaluating NLG models grounded in a task, it is more effective to use task performance than n-gram overlap metrics. 2 2.1 Related Work NLG Evaluation Existing NLG evaluation methods make use of n-gram overlap scores, human evaluations, and model-based evaluations. Our own method blends human evaluation and model-based evaluation, as we advocate using humans or building models to act on generated language Other model-based evaluations take a variety of forms. Some involve training models to estimate human judgments of utterance quality (Lowe et al., 2017; Duˇsek et al., 2017; Kann et al., 2018). Others require training models to distinguish between language generated by humans and models—an adversarial evaluation (Bowman et al., 2016; Liu et al., 2016; Kannan and Vinyals, 2016; Bruni and Fern´andez, 2017). These methods focus on the utterance in a vacuum and tend to not to consider how language will actually interact with other conversational participants. They treat humans as assessors of quality or adversarial listeners, whereas our proposal takes the perspective that listeners are cooperative interlocutors who use the language they hear to inform their beliefs about the world. Ou"
2020.scil-1.16,W04-1013,0,0.0636114,"which in the framework involves choosing the utterance most helpful to the listener (Goodman and Frank, 2016). This idea has been used to model a wide range of linguistic phenomena. This utility function is very similar to our proposed method’s scoring function—differing only in a cost term. To our knowledge, this is the first case where this rational speaker utility function is used to evaluate language rather than model human utterance selection. correct word order. 3 ROUGE ROUGE (Recall Oriented Understudy of Gisting Evaluation) is a class of n-gram overlap metrics for assessing summaries (Lin, 2004). Like BLEU, many ROUGE metrics operate on the n-gram level, but unlike BLEU, their main component is an n-gram recall score that gives the proportion of n-grams in a reference that are in the candidate rather than a precision score that gives the proportion of n-grams in the candidate that are in the reference. The version of ROUGE we use here is called ROUGE-L. It uses the longest common subsequence between the candidate summary and reference summary to calculate an F-score heavily favoring recall. As such, a high ROUGE-L score indicates that a large proportion of tokens from the reference o"
2020.scil-1.16,D16-1230,0,0.057632,"Missing"
2020.scil-1.16,P17-1103,0,0.0901945,"with most of these metrics even though humans find them equally good, and negated candidates receive high scores even where the negation leads to dramatic deviation from the reference texts. Such metrics are particularly ineffective in scenarios where there are many potentially appropriate utterances (Liu et al., 2016; Novikova et al., 2017). To avoid this problem, one might turn to human judgments to assess the quality of modelgenerated language. In this setting, humans rate language according to grammaticality, typicality, informativeness, interestingness, and other qualitative dimensions (Lowe et al., 2017; Hashimoto et al., 2019; Chaganty et al., 2018). This addresses the problems with n-gram overlap methods, but it is expensive, and the human task does not reflect natural language use, which can lead to unreliable data. One shortcoming of these methods is that they fail to take into account the communicative function of language; a speaker’s goal is not only to produce well-formed expressions, but also to convey relevant information to a listener. Likewise, a listener is not only an assessor of quality, but also an agent that forms beliefs based on speakers’ utterances. Thus, our NLG systems"
2020.scil-1.16,Q17-1023,1,0.642439,"municationbased evaluations. In language use, the speaker intends to communicate information to the listener using an utterance, and the listener infers some information from that utterance. This provides the basis for evaluation: if the listener’s inference aligns with the speaker’s intentions, the utterance was successful. If these intentions are not aligned, the utterance was less successful. We formalize communication-based NLG evaluations using the Rational Speech Acts model of pragmatic language use (Frank and Goodman, 2012). To motivate this approach, we rely on a color reference game (Monroe et al., 2017, 2018). In this game, a speaker and a listener see a set of three colors. The speaker is told one color is the target and tries to communicate the target to the listener using a natural language utterance. A good utterance is more likely to lead the listener to select the target, while a bad utterance is less likely to do so. In turn, effective metrics should assign high scores to good utterances and low scores to bad ones. To test our evaluation proposal, we asked crowdworkers to write color descriptions falling into three separate quality categories: those that describe only the target colo"
2020.scil-1.16,N18-1196,1,0.88053,"Missing"
2020.scil-1.16,D17-1238,0,0.142487,"Missing"
2020.scil-1.16,P02-1040,0,0.111933,"the n-grams shared between the candidate utterances and human-generated reference utterances. They are commonly used for evaluation in a variety of domains and are consistently compared when evaluating the effectiveness of different metrics for various tasks (summarization, image captioning, dialogue; Novikova et al. 2017; Kilickaya et al. 2017; Sharma et al. 2017). BLEU BLEU (BiLingual Evaluation Understudy) was conceived as a method for automatically evaluating machine translation systems by comparing the tokens in the system outputs to reference sentences constructed by expert translators (Papineni et al., 2002). BLEU consists of two components—a modified n-gram precision and a brevity penalty. The modified n-gram precision rewards candidate translations that contain the same n-grams as the references. Calculated precisions for n-grams of different sizes are then geometrically averaged together. Conventionally, n-gram overlaps for n = 1, 2, 3, and 4 are calculated. The second component of the BLEU score, the brevity penalty, acts as a recall constraint. Long candidate utterances could achieve a high modified ngram precision by containing many n-grams, but the brevity penalty negatively impacts the sc"
2020.scil-1.16,D14-1162,0,0.0869881,"Missing"
2020.scil-1.16,N19-1410,0,0.0112343,"ate captions that would help a listener choose that input from among a set of distractor images (Vedantam et al., 2017; Mao et al., 2016; Cohn-Gordon et al., 2018). Similarly, a summarization tool should produce summaries that capture exactly the information that makes the source text stand out with respect to related inputs (Zhang et al., 2018), and a pure text generation tool (a language model) might be refashioned to produce texts conditional on specific pieces of metadata (e.g. genre, author) so that we can assess it based on a listener’s ability to recover that metadata from distractors (Shen et al., 2019). In general, we feel that these are healthy impositions on these tasks, as they encourage the systems to be grounded in specific contexts and to produce utterances that are not just true but also informative. 5 Evaluating the Evaluation Approaches We assess the effectiveness of our evaluation method using the color reference game described by Monroe et al. (2017), in which a speaker and a listener each see the same set of three color swatches (though perhaps in different orders) and the speaker’s task is to convey the identity of their (hidden) target color to the listener. This scenario is i"
2020.scil-1.16,W18-5623,0,0.0359485,"Missing"
2021.acl-long.186,S18-2005,0,0.16288,"ained for each sentence. On Dynabench, we explore conditions with and without prompt sentences that workers can edit to achieve their goal. Introduction Sentiment analysis is an early success story for NLP, in both a technical and an industrial sense. It has, however, entered into a more challenging phase for research and technology development: while present-day models achieve outstanding results on all available benchmark tasks, they still fall short when deployed as part of real-world systems (Burn-Murdoch, 2013; Grimes, 2014, 2017; Gossett, 2020) and display a range of clear shortcomings (Kiritchenko and Mohammad, 2018; Hanwen Shen et al., 2018; Wallace et al., 2019; Tsai et al., 2019; Jin et al., 2019; Zhang et al., 2020). In this paper, we seek to address the gap between benchmark results and actual utility by introduc∗ Model 0 RoBERTa fine-tuned on sentiment benchmarks ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.1 DynaSent is intended to be a dynamic benchmark that expands in response to new models, new modeling goals, and new adversarial attacks. We present the first two rounds here and motivate some specific data collection and modeli"
2021.acl-long.186,2020.acl-main.441,1,0.910072,"roduc∗ Model 0 RoBERTa fine-tuned on sentiment benchmarks ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.1 DynaSent is intended to be a dynamic benchmark that expands in response to new models, new modeling goals, and new adversarial attacks. We present the first two rounds here and motivate some specific data collection and modeling choices, and we propose that, when future models solve these rounds, we use those models to create additional DynaSent rounds. This is an instance of “the ‘moving post’ dynamic target” for NLP that Nie et al. (2020) envision. Figure 1 summarizes our method, which incorporates both naturally occurring sentences and sentences created by crowdworkers with the goal of fooling a top-performing model. The starting point is Model 0, which is trained on standard sentiment 1 Equal contribution. https://github.com/cgpotts/dynasent 2388 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2388–2404 August 1–6, 2021. ©2021 Association for Computational Linguistics benchmarks and used to find challengi"
2021.acl-long.186,N18-2002,0,0.0380916,"Missing"
2021.acl-long.186,P19-1163,0,0.013625,"grad, 1972; Levesque, 2013) have risen to prominence in response to the sense that benchmark results are over-stating the quality of the models we are developing (Linzen, 2020). These efforts seek to determine whether models have met specific learning targets (Alzantot et al., 2018; Glockner et al., 2018; Naik et al., 2018; Nie et al., 2019), exploit relatively superficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al., 2019; Schuster et al., 2019). For the most part, challenge and adversarial datasets are meant to be used primarily for evaluation (though Liu et al. (2019a) show that even small amounts of training on them can be fruitful in some scenarios). However, there are existing adversarial datasets that are large enough to support full-scale training efforts (Zellers et al., 2018, 2019; Chen et al., 2019; Dua et al., 2019; Bartolo et al., 2020). DynaSent falls into this class; it has large train sets that can support from-scratch training as well as fine-tuning. Our approach is closest to, and directly ins"
2021.acl-long.186,P04-1035,0,0.0586021,"ion 5.3), which favors our strategy of training models from scratch for each round. 2 Related Work Sentiment analysis was one of the first natural language understanding tasks to be revolutionized by data-driven methods. Rather than trying to survey the field (see Pang and Lee 2008; Liu 2012; Grimes 2014), we focus on the benchmark tasks that have emerged in this space, and then seek to situate these benchmarks with respect to challenge (adversarial) datasets and crowdsourcing methods. 2.1 Sentiment Benchmarks Many sentiment datasets are derived from customer reviews of products and services (Pang and Lee, 2004, 2005; Socher et al., 2013; Maas et al., 2011; Jindal and Liu, 2008; Ni et al., 2019; McAuley et al., 2012; Zhang et al., 2015). This is an appealing source of data, since such texts are accessible and abundant in many languages and regions of the world, and they tend to come with their own authorprovided labels (star ratings). On the other hand, over-reliance on such texts is likely also limiting progress; DynaSent begins moving away from such texts, though it remains rooted in this domain. Not all sentiment benchmarks are based in review texts. The MPQA Opinion Corpus of Wiebe et al. (2005)"
2021.acl-long.186,D19-1341,0,0.0210931,"ue, 2013) have risen to prominence in response to the sense that benchmark results are over-stating the quality of the models we are developing (Linzen, 2020). These efforts seek to determine whether models have met specific learning targets (Alzantot et al., 2018; Glockner et al., 2018; Naik et al., 2018; Nie et al., 2019), exploit relatively superficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al., 2019; Schuster et al., 2019). For the most part, challenge and adversarial datasets are meant to be used primarily for evaluation (though Liu et al. (2019a) show that even small amounts of training on them can be fruitful in some scenarios). However, there are existing adversarial datasets that are large enough to support full-scale training efforts (Zellers et al., 2018, 2019; Chen et al., 2019; Dua et al., 2019; Bartolo et al., 2020). DynaSent falls into this class; it has large train sets that can support from-scratch training as well as fine-tuning. Our approach is closest to, and directly inspired by, the Adversaria"
2021.acl-long.186,P05-1015,0,0.40513,"Missing"
2021.acl-long.186,P16-1162,0,0.0110857,"Missing"
2021.acl-long.186,D08-1027,0,0.197074,"Missing"
2021.acl-long.186,Q19-1043,0,0.0242673,"e highest-rated workers. This led to a substantial increase in dataset quality by removing a lot of labels that seemed to us to be randomly assigned. Appendix B describes the process in more detail, and our Datasheet enumerates the known unwanted biases that this process can introduce. 3.4 Round 1 Dataset The Round 1 dataset is summarized in Table 5, and Table 4 gives randomly selected short examples. Because each sentence has five ratings, there are two perspectives we can take on the dataset: Distributional Labels We can repeat each example with each of its labels (de Marneffe et al., 2012; Pavlick and Kwiatkowski, 2019). For instance, the first sentence in Table 4 would be repeated three times with ‘Mixed’ as the label and twice with ‘Negative’. For many classifier models, this reduces to labeling each example with its probability distribution over the labels. This is an appealing approach to creating training data, since it allows us to make use of all the examples,4 even those that do not have a majority label, and it allows us to make maximal use of the labeling information. In our experiments, we found that training on the distributional labels consistently led to slightly better 4 For ‘Mixed’ labels, we"
2021.acl-long.186,S18-2023,0,0.0519742,"Missing"
2021.acl-long.186,D13-1170,1,0.0189406,"s the labels Positive, Negative, and Neutral. This is a minimal expansion of the usual binary (Positive/Negative) sentiment task, but a crucial one, as it avoids the false presupposition that all texts convey binary sentiment. We chose this version of the problem to show that even basic sentiment analysis poses substantial challenges for our field. 2 https://dynabench.org/ We find that the Neutral category is especially difficult. While it is common to synthesize such a category from middle-scale product and service reviews, we use an independent validation of the Stanford Sentiment Treebank (Socher et al., 2013) dev set to argue that this tends to blur neutrality together with mixed sentiment and uncertain sentiment (Section 5.2). DynaSent can help tease these phenomena apart, since it already has a large number of Neutral examples and a large number of examples displaying substantial variation in validation. Finally, we argue that the variable nature of the Neutral category is an obstacle to fine-tuning (Section 5.3), which favors our strategy of training models from scratch for each round. 2 Related Work Sentiment analysis was one of the first natural language understanding tasks to be revolutioniz"
2021.acl-long.186,S17-2088,0,0.0621535,"Missing"
2021.acl-long.186,W19-4824,0,0.0476531,"Missing"
2021.acl-long.186,W17-1609,0,0.065098,"Missing"
2021.acl-long.186,L18-1239,0,0.0564547,"Missing"
2021.acl-long.186,D19-1221,0,0.0131273,"ns with and without prompt sentences that workers can edit to achieve their goal. Introduction Sentiment analysis is an early success story for NLP, in both a technical and an industrial sense. It has, however, entered into a more challenging phase for research and technology development: while present-day models achieve outstanding results on all available benchmark tasks, they still fall short when deployed as part of real-world systems (Burn-Murdoch, 2013; Grimes, 2014, 2017; Gossett, 2020) and display a range of clear shortcomings (Kiritchenko and Mohammad, 2018; Hanwen Shen et al., 2018; Wallace et al., 2019; Tsai et al., 2019; Jin et al., 2019; Zhang et al., 2020). In this paper, we seek to address the gap between benchmark results and actual utility by introduc∗ Model 0 RoBERTa fine-tuned on sentiment benchmarks ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.1 DynaSent is intended to be a dynamic benchmark that expands in response to new models, new modeling goals, and new adversarial attacks. We present the first two rounds here and motivate some specific data collection and modeling choices, and we propose that, when future mod"
2021.acl-long.186,P19-1472,0,0.0433071,"Missing"
2021.acl-long.186,H89-1033,0,0.771875,"MPQA Opinion Corpus of Wiebe et al. (2005) contains news articles labeled at the phrase-level for a variety of subjective states; it presents an exciting vision for how sentiment analysis might become more multidimensional. SemEval 2016 and 2017 (Nakov et al., 2016; Rosenthal et al., 2017) offered Twitter-based sentiment datasets. And of course there are numerous additional datasets for specific languages, domains, and emotional dimensions; Google’s Dataset Search currently reports over 100 datasets for sentiment. 2389 2.2 Challenge and Adversarial Datasets Challenge and adversarial datasets (Winograd, 1972; Levesque, 2013) have risen to prominence in response to the sense that benchmark results are over-stating the quality of the models we are developing (Linzen, 2020). These efforts seek to determine whether models have met specific learning targets (Alzantot et al., 2018; Glockner et al., 2018; Naik et al., 2018; Nie et al., 2019), exploit relatively superficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al."
2021.acl-long.186,D18-1009,0,0.0205107,"uperficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al., 2019; Schuster et al., 2019). For the most part, challenge and adversarial datasets are meant to be used primarily for evaluation (though Liu et al. (2019a) show that even small amounts of training on them can be fruitful in some scenarios). However, there are existing adversarial datasets that are large enough to support full-scale training efforts (Zellers et al., 2018, 2019; Chen et al., 2019; Dua et al., 2019; Bartolo et al., 2020). DynaSent falls into this class; it has large train sets that can support from-scratch training as well as fine-tuning. Our approach is closest to, and directly inspired by, the Adversarial NLI (ANLI) project, which is reported on by Nie et al. (2020) and which continues on Dynabench. In ANLI, human annotators construct new examples that fool a top-performing model but make sense to other human annotators. This is an iterative process that allows the annotation project itself to organically find phenomena that fool current mode"
2021.naacl-main.324,2020.emnlp-main.393,0,0.0658177,"hoice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likeli"
2021.naacl-main.324,2020.tacl-1.3,0,0.0240418,"med “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,W17-5401,0,0.025796,", 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah a"
2021.naacl-main.324,2020.acl-main.465,0,0.0806274,"solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al.,"
2021.naacl-main.324,2020.insights-1.13,0,0.0263233,"er, e.g., the multiple iterations of SemEval or WMT datasets over the years, we’ve already been handling this quite well—we accept that a model’s BLEU score on WMT16 is not comparable to WMT14. That is, it is perfectly natural for benchmark datasets to evolve as the community makes progress. The only thing Dynabench does differently is that it anticipates dataset saturation and embraces the loop so that we can make faster and more sustained progress. ever, it has also been found that model-in-the-loop counterfactually-augmented training data does not necessarily lead to better generalization (Huang et al., 2020). Given the distributional shift induced by adversarial settings, it would probably be wisest to combine adversarially collected data with nonadversarial data during training (ANLI takes this approach), and to also test models in both scenarios. To get the most useful training and testing data, it seems the focus should be on collecting adversarial data with the best available model(s), preferably with a wide range of expertise, as that will likely be beneficial to future models also. That said, we expect this to be both task and model dependent. Much more research is required, and we encourag"
2021.naacl-main.324,2020.acl-main.768,1,0.846016,"e the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle t"
2021.naacl-main.324,N19-1225,0,0.0738848,"bstantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and that encour- introduce Dynabench, an open-source, web-based age apples-to-apples model comparisons. Bench- research platform for dynamic data collection and marks provide a north star goal for researchers, and model benchmarking. The guiding hypothesis be4110 Proceedings of the 2021 Con"
2021.naacl-main.324,D17-1215,1,0.795486,"ard et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et a"
2021.naacl-main.324,2021.ccl-1.108,0,0.0774911,"Missing"
2021.naacl-main.324,D15-1166,0,0.00793057,"rk tasks, that milestone is now rou- cording to the narrow criteria used to define human performance) nonetheless fail on simple chaltinely reached within just a few years for newer lenge examples and falter in real-world scenarios. datasets (see Figure 1). As with the rest of AI, NLP A substantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and"
2021.naacl-main.324,2020.emnlp-main.154,0,0.0295127,"uperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring"
2021.naacl-main.324,J93-2004,0,0.0749322,"humans? This reveals the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang"
2021.naacl-main.324,C10-1091,0,0.0606954,"Missing"
2021.naacl-main.324,N19-1063,0,0.0241688,"d its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not s"
2021.naacl-main.324,P19-1334,0,0.0218097,"ang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al.,"
2021.naacl-main.324,K18-1007,1,0.843089,"for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing tar"
2021.naacl-main.324,W10-0719,1,0.726488,"collaborative effort, the platform is meant to be a platform technology for humanand-model-in-the-loop evaluation that belongs to the entire community. In the current iteration, the platform is set up for dynamic adversarial data collection, where humans can attempt to find modelfooling examples. This design choice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics."
2021.naacl-main.324,C18-1198,0,0.0239103,"ard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Mo"
2021.naacl-main.324,2020.acl-main.441,1,0.878791,"our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of resource collection and architectural improvements. Similar to Dynabench, recent work seeks to embrace this phenomenon, addressing many of the previously mentioned issues through an iterative human-and-model-in-the-loop annotation process (Yang et al., 2017; Dinan et al., 2019; Chen et al., 2019; Bartolo et al., 2020; Nie et al., 2020), to find “unknown unknowns” (Attenberg et al., 2015) or in a never-ending or life-long learning setting (Silver et al., 2013; Mitchell et al., 2018). The Adversarial NLI (ANLI) dataset (Nie et al., 2020), for example, was collected with an adversarial setting over multiple rounds to yield “a ‘moving post’ dynamic target for NLU systems, rather than a static benchmark that will eventually saturate”. In its few-shot learning mode, GPT-3 barely shows “signs of life” (Brown et al., 2020) (i.e., it is barely above random) on ANLI, which is evidence that we are still far away from human performance"
2021.naacl-main.324,S18-2023,0,0.0556136,"Missing"
2021.naacl-main.324,W12-4501,0,0.0439542,"the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However"
2021.naacl-main.324,P18-2124,1,0.869954,"Missing"
2021.naacl-main.324,D16-1264,0,0.230428,"n use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its lead"
2021.naacl-main.324,2020.acl-main.442,0,0.230761,"performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuri"
2021.naacl-main.324,K19-1019,0,0.019609,"al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models"
2021.naacl-main.324,N18-2002,0,0.0355102,"Missing"
2021.naacl-main.324,2020.emnlp-main.661,1,0.822242,"Missing"
2021.naacl-main.324,P19-1004,0,0.0188742,"put character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally"
2021.naacl-main.324,2020.acl-main.479,0,0.0170779,"ead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Etting"
2021.naacl-main.324,2020.acl-main.222,0,0.0281863,"19) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likelihood ilarly, the paradigm is perfectly compatible with training on i.i.d. train/test splits and human lan- collaborative settings that utilize human feedback, guage (Linzen, 2020; Stiennon et al., 2020). or even negotiation. The crucial aspect of this proposal is the fact that models and humans interact We think there is widespread agreement that something has to change about our standard eval- live “in the loop” for evaluation and data collection. uation paradigm and that we nee"
2021.naacl-main.324,D08-1027,0,0.352191,"Missing"
2021.naacl-main.324,D13-1170,1,0.0131607,"t data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather tha"
2021.naacl-main.324,2020.emnlp-main.746,0,0.0673253,"Missing"
2021.naacl-main.324,N18-1074,0,0.0416359,"Missing"
2021.naacl-main.324,L18-1239,0,0.0544033,"Missing"
2021.naacl-main.324,W19-3509,1,0.836306,"Missing"
2021.naacl-main.324,D19-1221,0,0.0180989,"2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of r"
2021.naacl-main.324,W18-5446,1,0.794698,"Missing"
2021.naacl-main.324,D19-1286,0,0.0188342,"ive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressiv"
2021.naacl-main.324,2020.tacl-1.25,0,0.0303378,"enging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzi"
2021.naacl-main.324,W17-3012,1,0.867269,"Missing"
2021.naacl-main.324,D18-1501,0,0.0349494,"Missing"
2021.naacl-main.324,N18-1101,1,0.774917,"he background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive bod"
2021.naacl-main.324,D18-1259,0,0.0459996,"Missing"
2021.naacl-main.324,2020.emnlp-main.397,0,0.0238252,"was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,2020.emnlp-main.659,1,0.819658,"Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by sim"
D13-1170,J10-4006,0,0.113389,"junction ‘but’ dominates. The complete training and testing code, a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks"
D13-1170,D08-1094,0,0.140304,"Missing"
D13-1170,D11-1129,0,0.143203,"ions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Coll"
D13-1170,W13-0112,0,0.126107,"data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dat"
D13-1170,P12-1092,1,0.167577,"used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-c"
D13-1170,P03-1054,1,0.146647,"ll and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 1631 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics this dataset will enable the community to train compositional models that a"
D13-1170,N10-1120,0,0.13649,"Missing"
D13-1170,J07-2002,0,0.140631,"/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus."
D13-1170,P05-1015,0,0.37586,"he meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 1631 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, c Seattle, Washington, USA, 18-21 O"
D13-1170,rentoumi-etal-2010-united,0,0.131845,"Missing"
D13-1170,P10-1093,0,0.139526,"her et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector R"
D13-1170,N07-1038,0,0.134233,"by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary posit"
D13-1170,D11-1014,1,0.293141,"this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Gies"
D13-1170,D12-1110,1,0.209782,"na presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by"
D13-1170,P12-3020,0,0.239464,"Missing"
D13-1170,D11-1016,0,0.210149,"capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse tree"
D13-1170,C10-1142,0,0.126738,". models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique"
D13-1170,R09-1048,0,\N,Missing
D15-1075,H05-1079,0,0.0281669,"rease in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 ‡ Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground 632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. A man inspects the uniform of a figure in some East As"
D15-1075,W15-4002,1,0.42771,"all three neural network models, suggesting that research into significantly higher capacity versions of these models would be productive. Figure 3: The neural network classification architecture: for each sentence embedding model evaluated in Tables 6 and 7, two identical copies of the model are run with the two sentences as input, and their outputs are used as the two 100d inputs shown here. sideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level. Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the mode"
D15-1075,S14-2001,0,0.0154894,"ent, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b). For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems. If we opt not to assume that events are coreferent, then"
D15-1075,W04-3205,0,0.0156665,"Missing"
D15-1075,W03-0906,0,0.156506,"Missing"
D15-1075,marelli-etal-2014-sick,0,0.145481,"ent, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b). For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems. If we opt not to assume that events are coreferent, then"
D15-1075,P08-1118,1,0.178349,"Missing"
D15-1075,P15-2070,0,0.23443,"Missing"
D15-1075,D14-1162,1,0.134204,"e concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the strength coefficient λ for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sen3.4 Analysis and dis"
D15-1075,W07-1401,0,0.11712,"ts such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007). We report results in Table 4. Each of the models Partition We distribute the corpus with a prespecified train/test/development split. The test and development sets contain 10k examples each. Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated. Parses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of"
D15-1075,D13-1170,1,0.072644,"Missing"
D15-1075,P03-1054,1,0.0911655,"Missing"
D15-1075,S14-2055,0,0.0386092,"Missing"
D15-1075,W14-1610,0,0.0526904,"ifiers, and neural network-based models. We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997). We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art. 2 plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b)."
D15-1075,P12-2018,1,0.254878,", including models lacking crossbigram features (Feature 6), and lacking all lexical features (Features 4–6). We report results both on the test set and the training set to judge overfitting. for removing all lexicalized features. On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and another from using the very sparse cross-bigram features. The latter result suggests that there is value in letting the classifier automatically learn to recognize structures like explicit negations and adjective modification. A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis. It is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations. Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model. Lexicalized Classifier Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these"
D15-1075,W09-3714,1,0.152106,"rs to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 ‡ Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground 632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. A man inspects the uniform of a figure in some East Asian country. contradiction The man is sleeping An o"
D15-1075,W07-1406,0,0.068274,"Missing"
D15-1075,P14-5008,0,0.0402786,"eate the sentence pair. A gold label reflects a consensus of three votes from among the author and the four annotators. fluent, correctly spelled English, with a mix of full sentences and caption-style noun phrase fragments, though punctuation and capitalization are often omitted. The corpus is available under a CreativeCommons Attribution-ShareAlike license, the same license used for the Flickr30k source captions. It can be downloaded at: nlp.stanford.edu/projects/snli/ 3.1 Excitement Open Platform models The first class of models is from the Excitement Open Platform (EOP, Pad´o et al. 2014; Magnini et al. 2014)—an open source platform for RTE research. EOP is a tool for quickly developing NLI systems while sharing components such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by run"
D15-1075,H89-1033,0,0.539141,"and the lexicalized model show similar performance when trained on the current full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets. We were struck by the speed with which the lexicalized classifier outperforms its unlexicalized 638 73.95 76.78 78.22 Unlexicalized Lexicalized this kind of inference through lexical cues can lead them astray. Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013). For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand. Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner"
D15-1075,Q14-1006,0,0.347432,"ese are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup1 http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool 633 York and A tourist visited the city. Assuming coreference between New York and the city justifies labeling the pair as an entailment, but without that assumption the city could be taken to refer to a specific unknown city, leaving the pair neutral. This kind of indeterminacy of label can"
D16-1243,D10-1040,0,0.247724,"Missing"
D16-1243,D16-1202,0,0.122746,"se descriptions. When queried for its single most likely prediction, arg maxd S(d |c), the result is nearly always an acceptable, “safe” description— manual inspection of 200 such top-1 predictions did not identify any errors. 5 Conclusion and future work We presented a model for generating compositional color descriptions that is capable of producing novel descriptions not seen in training and significantly outperforms prior work at conditional language modeling.3 One natural extension is the use of character-level sequence modeling to capture complex morphology (e.g., “-ish” in “greenish”). Kawakami et al. (2016) build character-level models for predicting colors given descriptions in addition to describing colors. Their model uses a Labspace color representation and uses the color to initialize the LSTM instead of feeding it in at each time step; they also focus on visualizing point predictions of their description-to-color model, whereas we examine the full distributions implied by our color-todescription model. Another extension we plan to investigate is modeling of context, to capture how people describe colors differently to contrast them with other colors via 3 We release our code at https://git"
D16-1243,J12-1006,0,0.0479869,"Missing"
D16-1243,Q15-1008,0,0.262029,"uction of color language is essential for referring expression generation (Krahmer and Van Deemter, 2012) and image captioning (Kulkarni et al., 2011; Mitchell et al., 2012), among other grounded language generation problems. We consider color description generation as a grounded language modeling problem. We present an effective new model for this task that uses a long short-term memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997; Graves, 2013) and a Fourier-basis color representation inspired by feature representations in computer vision. We compare our model with LUX (McMahan and Stone, 2015), a Bayesian generative model of color semantics. Our model improves on their approach in several respects, which we demonstrate by examining the meanings it assigns to various unusual descriptions: (1) it can generate compositional color descriptions not observed in training (Fig. 3); (2) it learns correct denotations for underspecified modifiers, which name a variety of colors (“dark”, “dull”; Fig. 2); and (3) it can model non-convex denotations, such as that of “greenish”, which includes both greenish yellows and blues (Fig. 4). As a result, our model also produces significant improvements"
D16-1243,E12-1076,0,0.0393602,"st-probability prediction. Introduction Color descriptions represent a microcosm of grounded language semantics. Basic color terms like “red” and “blue” provide a rich set of semantic building blocks in a continuous meaning space; in addition, people employ compositional color descriptions to express meanings not covered by basic terms, such as “greenish blue” or “the color of the rust on my aunt’s old Chevrolet” (Berlin and Kay, 1991). The production of color language is essential for referring expression generation (Krahmer and Van Deemter, 2012) and image captioning (Kulkarni et al., 2011; Mitchell et al., 2012), among other grounded language generation problems. We consider color description generation as a grounded language modeling problem. We present an effective new model for this task that uses a long short-term memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997; Graves, 2013) and a Fourier-basis color representation inspired by feature representations in computer vision. We compare our model with LUX (McMahan and Stone, 2015), a Bayesian generative model of color semantics. Our model improves on their approach in several respects, which we demonstrate by examining the mea"
D18-1140,K16-1017,0,0.0819753,"Missing"
D18-1140,D14-1179,0,0.038552,"Missing"
D18-1140,W07-0101,0,0.0823569,"pproach suffices in homogeneous contexts, whereas the added power of the dense embeddings proves valuable in more diverse ones. 1 Hidden layer(s) Sarcastic! x3f BiGRU layer Embeddings ‘WiseGuy33’ Introduction Irony and sarcasm1 are extreme examples of context-dependence in language. Given only the text Great idea! or What a hardship!, we cannot resolve the speaker’s intentions unless we have insight into the circumstances of utterance – who is speaking, and to whom, and how the content relates to the preceding discourse (Clark, 1996). While certain texts are biased in favor of sarcastic uses (Kreuz and Caucci, 2007; Wallace et al., 2014), the non-literal nature of this phenomenon ensures that there is an important role for pragmatic inference (Clark and Gerrig, 1984). The current paper is an in-depth study of one important aspect of the context dependence of sarcasm: the author. Our guiding hypotheses are that authors vary in their propensity for using sarcasm, that this propensity is influenced by more general facts about the context, and that authors have their own particular ways of indicating sarcasm. These hypotheses are well supported by psycholinguistic research (Colston and Lee, 2004; Gibbs, 200"
D18-1140,L18-1008,0,0.0609659,"Missing"
D18-1140,C16-1151,0,0.234741,"Missing"
D18-1140,D17-1169,0,0.0711275,"Missing"
D18-1140,D17-1035,0,0.0141796,"where the highdimensional embeddings outperform the Bayesian priors. This likely reflects two interacting factors. First, with smaller, more focused forums, it is harder to learn good author embeddings, so the simple prior is more reliable. Second, on the full dataset, there are more examples, and also more complex interactions between authors and their texts, so the added representational power of the embeddings proves justified. 6.2 Results and Discussion Quantitive assessment Table 2 reports the means of 10 runs to control for variation deriving from randomness in the optimization process (Reimers and Gurevych, 2017). Where there is overlap between our experiments and those of Hazarika et al. (2018) (CASCADE), our model is highly competitive. We slightly Qualitative comparisons Table 3 provides example predictions from the different models. Each example is taken from the holdout set of a run in which all three models were trained on the same training set and evaluation was conducted on the same holdout set. For both sarcastic and non-sarcastic comments, author modeling can be helpful for disambiguation. For instance, in examples 1 and 2, omitting 1118 Full balanced r/politics balanced unbalanced r/AskRedd"
D18-1140,D17-1050,0,0.023773,"Missing"
D18-1140,P11-2102,0,0.194664,"Missing"
D18-1140,P18-1093,0,0.0225951,"Missing"
D18-1140,P15-1100,0,0.0912006,"Missing"
D18-1140,P14-2084,0,0.11524,"geneous contexts, whereas the added power of the dense embeddings proves valuable in more diverse ones. 1 Hidden layer(s) Sarcastic! x3f BiGRU layer Embeddings ‘WiseGuy33’ Introduction Irony and sarcasm1 are extreme examples of context-dependence in language. Given only the text Great idea! or What a hardship!, we cannot resolve the speaker’s intentions unless we have insight into the circumstances of utterance – who is speaking, and to whom, and how the content relates to the preceding discourse (Clark, 1996). While certain texts are biased in favor of sarcastic uses (Kreuz and Caucci, 2007; Wallace et al., 2014), the non-literal nature of this phenomenon ensures that there is an important role for pragmatic inference (Clark and Gerrig, 1984). The current paper is an in-depth study of one important aspect of the context dependence of sarcasm: the author. Our guiding hypotheses are that authors vary in their propensity for using sarcasm, that this propensity is influenced by more general facts about the context, and that authors have their own particular ways of indicating sarcasm. These hypotheses are well supported by psycholinguistic research (Colston and Lee, 2004; Gibbs, 2000; Dress et al., 2008),"
D18-1140,Q14-1024,1,0.918614,"Missing"
D18-1140,C16-1231,0,0.0244458,"Missing"
D19-1385,D18-1004,1,0.820505,"its, arguing that our estimated rates of condescension are related to different community norms. 2 The TALK D OWN Corpus We chose Reddit as the basis for our corpus for a few key reasons. First, it is a large, publicly available dataset from an active set of more than one million user-created online communities (subreddits).1 Second, it varies in both content and tone. Third, users can develop strong identities on the site, which could facilitate user-level modeling, but these identities are generally pseudonymous, which is useful when studying charged social phenomena (Hamilton et al., 2017; Wang and Jurgens, 2018). Fourth, the subreddit structure of the site creates opportunities to study the impact of condescension on community structure and norms (Buntain and Golbeck, 2014; Lin et al., 2017; Zhang et al., 2017; Chandrasekharan et al., 2018). The basis for our work is the Reddit data dump 2006–2018.2 We first extracted C OM MENT /R EPLY pairs in which the R EPLY contains a condescension-related word. After further filtering out self-replies and moderator posts, and normalizing links and references, we obtain 2.62M C OMMENT/R EPLY pairs. Not all of these examples truly involve the R E PLY saying that t"
D19-1456,D15-1075,1,0.951357,"al., 2014). However, in the case of question answering, Jia and Liang (2017) show that training on one perturbation does not result in generalization to similar perturbations, revealing a 4485 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4485–4495, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics need for models with stronger generalization capabilities. Similarly, adversarial testing has shown that strong models for the SNLI dataset (Bowman et al., 2015a) have significant holes in their knowledge of lexical and compositional semantics (Glockner et al., 2018; Naik et al., 2018; Nie et al., 2018; Yanaka et al., 2019; Dasgupta et al., 2018). In addition, a number of recent papers suggest that even top models exploit dataset artifacts to achieve good quantitative results (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018), which further emphasizes the need to go beyond naturalistic evaluations. Artificially generated datasets have also been used extensively to gain analytic insights into what models are learning. These methods have th"
D19-1456,W15-4002,1,0.908086,"al., 2014). However, in the case of question answering, Jia and Liang (2017) show that training on one perturbation does not result in generalization to similar perturbations, revealing a 4485 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4485–4495, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics need for models with stronger generalization capabilities. Similarly, adversarial testing has shown that strong models for the SNLI dataset (Bowman et al., 2015a) have significant holes in their knowledge of lexical and compositional semantics (Glockner et al., 2018; Naik et al., 2018; Nie et al., 2018; Yanaka et al., 2019; Dasgupta et al., 2018). In addition, a number of recent papers suggest that even top models exploit dataset artifacts to achieve good quantitative results (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018), which further emphasizes the need to go beyond naturalistic evaluations. Artificially generated datasets have also been used extensively to gain analytic insights into what models are learning. These methods have th"
D19-1456,P18-2103,0,0.0807866,"erturbation does not result in generalization to similar perturbations, revealing a 4485 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4485–4495, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics need for models with stronger generalization capabilities. Similarly, adversarial testing has shown that strong models for the SNLI dataset (Bowman et al., 2015a) have significant holes in their knowledge of lexical and compositional semantics (Glockner et al., 2018; Naik et al., 2018; Nie et al., 2018; Yanaka et al., 2019; Dasgupta et al., 2018). In addition, a number of recent papers suggest that even top models exploit dataset artifacts to achieve good quantitative results (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018), which further emphasizes the need to go beyond naturalistic evaluations. Artificially generated datasets have also been used extensively to gain analytic insights into what models are learning. These methods have the advantage that the complexity of individual examples can be precisely characterized without reference to"
D19-1456,N18-2017,0,0.0739342,"Missing"
D19-1456,D17-1215,0,0.0329738,"the standard models for semantics. 2 Related Work There is a growing literature that uses targeted generalization tasks to probe the capacity of learning models. We seek to build on this work by developing a formal framework in which one can ask whether one of these tasks is even possible. In adversarial testing, training examples are systematically perturbed and then used for testing. In computer vision, it is common to adversarially train on artificially noisy examples to create a more robust model (Goodfellow et al., 2015; Szegedy et al., 2014). However, in the case of question answering, Jia and Liang (2017) show that training on one perturbation does not result in generalization to similar perturbations, revealing a 4485 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4485–4495, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics need for models with stronger generalization capabilities. Similarly, adversarial testing has shown that strong models for the SNLI dataset (Bowman et al., 2015a) have significant holes in their knowledge of lexical a"
D19-1456,W07-1431,0,0.0406892,"ments. Because there are two ways to realize T and F at the intermediate node, we can efficiently use only half of our sentences to satisfy our fairness property. 4 Fair Artificial NLI Datasets Our central empirical question is whether current neural models can learn to do robust natural language inference if given fair datasets. We now present a method for addressing this question. To do this, we need to move beyond the simple propositional logic example explored above, to come closer to the true complexity of natural language. To do this, we adopt a variant of the natural logic developed by MacCartney and Manning (2007, 2009) (see also S´anchez-Valencia 1991; van Benthem 2008; Icard and Moss 2013). Natural logic 4488 is a flexible approach to doing logical inference directly on natural language expressions. Thus, in this setting, we can work directly with natural language sentences while retaining complete control over all aspects of the generated dataset. 4.1 Natural Logic We define natural logic reasoning over aligned semantic parse trees that represent both the premise and hypothesis as a single structure and allow us to calculate semantic relations for all phrases compositionally. The core components ar"
D19-1456,W09-3714,0,0.66724,"{T, F} {⇒} symbol x≡y x@y xAy x∧y x|y x^y x#y C1 → {T, F} {¬, ε} {T, F} Figure 1: A composition tree that realizes a function evaluating propositional sentences. We define the functions C1 (U, V1 ) = U (V1 ) and C2 (V1 , ⇒, V2 ) = V1 ⇒ V2 where V1 , V2 ∈ {T, F} and U ∈ {¬, ε}. Train T T F F ⇒ ⇒ ⇒ ⇒ Test εF ¬F ¬T εT T T F F ⇒ ⇒ ⇒ ⇒ set theoretic definition couch ≡ sofa x=y crow @ bird x⊂y bird A crow x⊃y human ∧ nonhuman x ∩ y = ∅ ∧ x ∪ y = U cat |dog x ∩ y = ∅ ∧ x ∪ y 6= U animal ^ nonhuman x ∩ y 6= ∅ ∧ x ∪ y = U hungry # hippo (all other cases) Table 2: The seven basic semantic relations of MacCartney and Manning (2009): B = {# = independence, @ = entailment, A = reverse entailment, |= alternation, ^ = cover, ∧ = negation, ≡ = equivalence}. ¬T εT ¬F εF Pevery/some (A) = @ Pevery/some Table 1: A fair train/test split for the evaluation problem defined by the composition tree in Figure 1. We give just the terminal nodes; the examples are full trees. fine a challenging task, it is guaranteed to be possible. We noted in Section 2 that some challenging problems in the literature fail to meet this minimal requirement. 3.4 example Fair Tasks for Propositional Evaluation As a simple illustration of the above concept"
D19-1456,C18-1198,0,0.0307744,"sult in generalization to similar perturbations, revealing a 4485 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4485–4495, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics need for models with stronger generalization capabilities. Similarly, adversarial testing has shown that strong models for the SNLI dataset (Bowman et al., 2015a) have significant holes in their knowledge of lexical and compositional semantics (Glockner et al., 2018; Naik et al., 2018; Nie et al., 2018; Yanaka et al., 2019; Dasgupta et al., 2018). In addition, a number of recent papers suggest that even top models exploit dataset artifacts to achieve good quantitative results (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018), which further emphasizes the need to go beyond naturalistic evaluations. Artificially generated datasets have also been used extensively to gain analytic insights into what models are learning. These methods have the advantage that the complexity of individual examples can be precisely characterized without reference to the models being e"
D19-1456,S18-2023,0,0.0698188,"Missing"
D19-1456,D11-1014,0,0.0212991,"n phrase  rock A large rock Models We consider six different model architectures: CBoW Premise and hypothesis are represented by the average of their respective word embeddings (continuous bag of words). LSTM Encoder Premise and hypothesis are processed as sequences of words using a recurrent neural network (RNN) with LSTM cells, and the final hidden state of each serves as its representation (Hochreiter and Schmidhuber, 1997; Elman, 1990; Bowman et al., 2015a). TreeNN Premise and hypothesis are processed as trees, and the semantic composition function is a single-layer feed-forward network (Socher et al., 2011b,a). The value of the root node is the semantic representation in each case. Attention LSTM An LSTM RNN with word-byword attention (Rockt¨aschel et al., 2015). CompTreeNN Premise and hypothesis are processed as a single aligned tree, following the structure of the composition tree in Figure 3. The semantic composition function is a single-layer feed-forward network (Socher et al., 2011b,a). The value of the root node is the semantic representation of the premise and hypothesis together. CompTreeNTN Identical to the CompTreeNN, but with a neural tensor network as the composition function (Soch"
D19-1456,D13-1170,1,0.0396232,"2011b,a). The value of the root node is the semantic representation in each case. Attention LSTM An LSTM RNN with word-byword attention (Rockt¨aschel et al., 2015). CompTreeNN Premise and hypothesis are processed as a single aligned tree, following the structure of the composition tree in Figure 3. The semantic composition function is a single-layer feed-forward network (Socher et al., 2011b,a). The value of the root node is the semantic representation of the premise and hypothesis together. CompTreeNTN Identical to the CompTreeNN, but with a neural tensor network as the composition function (Socher et al., 2013). For the first three models, the premise and hypothesis representations are concatenated. For the CompTreeNN, CompTreeNTN, and Attention LSTM, there is just a single representation of the pair. In all cases, the premise–hypothesis representation is fed through two hidden layers and a softmax layer. All models are initialized with random 100dimensional word vectors and optimized using Adam (Kingma and Ba, 2014). It would not be possible to use pretrained word vectors, due to the artificial nature of our dataset. A grid hyperparameter search was run over dropout values of {0, 0.1, 0.2, 0.3} on"
D19-1456,L18-1239,0,0.0684307,"Missing"
J12-2003,J96-1002,0,0.0251094,"Missing"
J12-2003,de-marneffe-etal-2006-generating,1,0.142503,"Missing"
J12-2003,W09-3012,0,0.274288,"Missing"
J12-2003,W09-1401,0,0.0233968,"e new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the vast majority of information extraction systems work at roughly the c"
J12-2003,P03-1054,1,0.0286283,"on the training set. Predicate classes. Saur´ı (2008) deﬁnes classes of predicates (nouns and verbs) that project the same veridicality value onto the events they introduce. The classes also deﬁne the grammatical relations that need to hold between the predicate and the event it introduces, because grammatical contexts matter for veridicality. Different veridicality values will indeed be assigned to X in He doesn’t know that X and in He doesn’t know if X. The classes have names like ANNOUNCE, CONFIRM, CONJECTURE, and SAY. Like Saur´ı, we used dependency graphs produced by the Stanford parser (Klein and Manning 2003; de Marneffe, MacCartney, and Manning 2006) to follow the path from the target event to the root of the sentence. If a predicate in the path was contained in one of the classes and the grammatical relation matched, we added both the lemma of the predicate as a feature and a feature marking the predicate class. 6 The maximum entropy formulation differs from the standard multi-class logistic regression model by having a parameter value for each class giving logit terms for how a feature’s value affects the outcome probability relative to a zero feature, whereas in the standard multi-class logis"
J12-2003,N03-5008,1,0.65703,"o give the log-likelihood for the exact distribution from the Turkers (which thus gives an upper-bound) as well as a log-likelihood for a baseline model which uses only the overall distribution of classes in the training data. A maximum entropy classiﬁer is an instance of a generalized linear model with a logit link function. It is almost exactly equivalent to the standard multi-class (also called polytomous or multinomial) logistic regression model from statistics, and readers more familiar with this presentation can think of it as such. In all our experiments, we use the Stanford Classiﬁer (Manning and Klein 2003) with a Gaussian prior (also known as L2 regularization) set to N (0, 1).6 4.1 Features The features were selected through 10-fold cross-validation on the training set. Predicate classes. Saur´ı (2008) deﬁnes classes of predicates (nouns and verbs) that project the same veridicality value onto the events they introduce. The classes also deﬁne the grammatical relations that need to hold between the predicate and the event it introduces, because grammatical contexts matter for veridicality. Different veridicality values will indeed be assigned to X in He doesn’t know that X and in He doesn’t kno"
J12-2003,W09-1105,0,0.0316776,"ghlights the sort of vagueness and ambiguity that can affect veridicality. These new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the v"
J12-2003,C10-2117,0,0.178904,"Missing"
J12-2003,N07-2036,0,0.011388,"ilding 460, Stanford CA 94305, USA. E-mail: cgpotts@stanford.edu. Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication: 30 November 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 2 call this event veridicality, building on logical, linguistic, and computational insights about the relationship between language and reader commitment (Montague 1969; Barwise 1981; Giannakidou 1994, 1995, 1999, 2001; Zwarts 1995; Asher and Lascarides 2003; Karttunen and Zaenen 2005; Rubin, Liddy, and Kando 2005; Rubin 2007; Saur´ı 2008). The central goal of this article is to begin to identify the linguistic and contextual factors that shape readers’ veridicality judgments.1 There is a long tradition of tracing veridicality to ﬁxed properties of lexical items (Kiparsky and Kiparsky 1970; Karttunen 1973). On this view, a lexical item L is veridical if the meaning of L applied to argument p entails the truth of p. For example, because both true and false things can be believed, one should not infer directly from A believes that S that S is true, making believe non-veridical. Conversely, realize appears to be veri"
J12-2003,D08-1027,0,0.120747,"Missing"
J12-2003,W08-0606,0,0.0142166,"ct veridicality. These new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the vast majority of information extraction systems wor"
J12-2003,W05-1206,0,0.155297,"Missing"
J12-2003,W10-3001,0,\N,Missing
J12-2003,W07-1401,0,\N,Missing
J12-2003,W10-3100,0,\N,Missing
N13-1071,W10-4305,0,0.132984,"77* 66.40 73.14* 69.61 58.31 58.83* R CEAF-φ4 P F1 45.49 47.55* 46.50 47.77* 46.38 47.07* CoNLL F1 60.65 61.13* Table 6: Performance on the test set according to the official CoNLL-2012 scorer. Scores are on automatically predicted mentions. Stars indicate a statistically significant difference (paired Mann-Whitney U-test, p < 0.05). System Baseline w/ Lifespan R B3 P F1 58.53* 71.58 64.40 58.14 73.14* 64.78* R CEAF-φ3 P F1 63.71* 58.31 60.89 63.38 58.83* 61.02 CoNLL F1 58.86 59.52* Table 7: B3 , CEAF-φ3 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that follows Cai and Strube (2010). Scores are on automatically predicted mentions. 4 Application to coreference resolution To assess the usefulness of the lifespan model in an NLP application, we incorporate it into the Stanford coreference resolution system (Lee et al., 2011), which we take as our baseline. This was the highestscoring system in the CoNLL-2011 Shared Task, and was also part of the highest-scoring system in the CoNLL-2012 Shared Task (Fernandes et al., 2012). It is a rule-based system that includes a total of ten rules (or “sieves”) for entity coreference, such as exact string match and pronominal resolution."
N13-1071,de-marneffe-etal-2006-generating,1,0.0313242,"Missing"
N13-1071,J12-2003,1,0.227058,"Missing"
N13-1071,W12-4502,0,0.115124,"Missing"
N13-1071,J95-2003,0,0.424826,"Missing"
N13-1071,N06-2015,0,0.141562,"effective at predicting whether a given mention is singleton or coreferent. We then provide an initial assessment of the engineering value of making the singleton/coreferent distinction by incorporating our lifespan model into the Stanford coreference resolution system (Lee et al., 2011). This addition results in a significant improvement on the CoNLL-2012 Shared Task data, across the MUC, B3 , CEAF, and CoNLL scoring algorithms. 2 Data All the data used throughout the paper come from the CoNLL-2012 Shared Task (Pradhan et al., 2012), which included the 1.6M English words from OntoNotes v5.0 (Hovy et al., 2006) that have been annotated with different layers of annotation (coreference, parse trees, etc.). We used the training, development (dev), and test splits as defined in the shared task (Table 1). Since the OntoNotes coreference annotations do not contain singleton mentions, we automatically marked as singletons all the NPs 627 Proceedings of NAACL-HLT 2013, pages 627–633, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Dataset Docs Tokens Training Dev Test 2,802 343 348 1.3M 160K 170K M ENTIONS Coreferent Singletons 152,828 18,815 19,392 192,248 24,170 24,921 T"
N13-1071,W11-1902,0,0.277586,"Missing"
N13-1071,H05-1004,0,0.174329,"the S TANDARD model. Results and discussion To evaluate the coreference system with and without the lifespan model, we used the English dev and test sets from the CoNLL2012 Shared Task, presented in Section 2. Although the CoNLL shared task evaluated systems on only multi-mention (i.e., non-singleton) entities, by stopping singletons from being linked to multi-mention entities, we expected the lifespan model to increase the system’s precision. Our evaluation uses five of the measures given by the CoNLL-2012 scorer: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF-φ3 and CEAF-φ4 (Luo, 2005), and the CoNLL official score (Denis and Baldridge, 2009). We do not include BLANC (Recasens and Hovy, 2011) because it assumes gold mentions and so is not suited for the scenario considered in this paper, which uses automatically predicted mentions. Table 6 summarizes the test set performance. All the scores are on automatically predicted mentions. We use gold POS, parse trees, and NEs. The baseline is the Stanford system, and ‘w/ Lifespan’ is the same system extended with our lifespan model to discard singletons, as explained above. As expected, the lifespan model increases precision but de"
N13-1071,W12-4501,0,0.11096,"s the existing literature leads us to expect, and that the model itself is highly effective at predicting whether a given mention is singleton or coreferent. We then provide an initial assessment of the engineering value of making the singleton/coreferent distinction by incorporating our lifespan model into the Stanford coreference resolution system (Lee et al., 2011). This addition results in a significant improvement on the CoNLL-2012 Shared Task data, across the MUC, B3 , CEAF, and CoNLL scoring algorithms. 2 Data All the data used throughout the paper come from the CoNLL-2012 Shared Task (Pradhan et al., 2012), which included the 1.6M English words from OntoNotes v5.0 (Hovy et al., 2006) that have been annotated with different layers of annotation (coreference, parse trees, etc.). We used the training, development (dev), and test splits as defined in the shared task (Table 1). Since the OntoNotes coreference annotations do not contain singleton mentions, we automatically marked as singletons all the NPs 627 Proceedings of NAACL-HLT 2013, pages 627–633, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Dataset Docs Tokens Training Dev Test 2,802 343 348 1.3M 160K 170"
N13-1071,M95-1005,0,0.825852,"reference (on the dev set) was higher with the C ONFIDENT model than with the S TANDARD model. Results and discussion To evaluate the coreference system with and without the lifespan model, we used the English dev and test sets from the CoNLL2012 Shared Task, presented in Section 2. Although the CoNLL shared task evaluated systems on only multi-mention (i.e., non-singleton) entities, by stopping singletons from being linked to multi-mention entities, we expected the lifespan model to increase the system’s precision. Our evaluation uses five of the measures given by the CoNLL-2012 scorer: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF-φ3 and CEAF-φ4 (Luo, 2005), and the CoNLL official score (Denis and Baldridge, 2009). We do not include BLANC (Recasens and Hovy, 2011) because it assumes gold mentions and so is not suited for the scenario considered in this paper, which uses automatically predicted mentions. Table 6 summarizes the test set performance. All the scores are on automatically predicted mentions. We use gold POS, parse trees, and NEs. The baseline is the Stanford system, and ‘w/ Lifespan’ is the same system extended with our lifespan model to discard singletons, as explained ab"
N13-1071,C69-7001,0,\N,Missing
N13-1071,C69-6902,0,\N,Missing
N13-1127,P05-3001,0,0.0338163,"rive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation. With the current paper, we seek to show"
N13-1127,D10-1040,0,0.268756,"l, Max Bodoia, Christopher Potts, and Dan Jurafsky Stanford University Stanford, CA, USA {acvogel,mbodoia,cgpotts,jurafsky}@stanford.edu Abstract traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). Grice characterized communication in terms of the cooperative principle, which enjoins speakers to make only contributions that serve the evolving conversational goals. We show that the cooperative principle and the associated maxims of relevance, quality, and quantity emerge from multi-agent decision theory. We utilize the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) model of multi-agent decision making which relies only on basic definitions of"
N13-1127,J86-3001,0,0.426842,", relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models"
N13-1127,J12-1006,0,0.0492721,"Missing"
N13-1127,J80-3003,0,\N,Missing
N18-1196,D10-1040,0,0.195189,"Rational Observer model (McMahan and Stone, 2015). An important difference between our speaker model and those in the work cited above is that our speaker model is a neural network that makes a combined judgment of applicability (semantic appropriateness) and availability (utterance prior), instead of modeling the two components separately. However, we stop short of directly predicting the referent of an expression discriminatively, as is done by e.g. Kennington and Schlangen (2015), so as to require a model that is usable as a speaker. A related metric is communicative success as defined by Golland et al. (2010), which judges the speaker by the accuracy of a human listener when given model-produced utterances. Our pragmatic informativeness metric instead gives a modelderived listener human utterances and assesses its accuracy at identifying colors. Pragmatic informativeness has the advantage of not requiring additional expensive human labeling in response to model outputs; it can be assessed on an existing collection of human utterances, and can therefore be considered an automatic metric. 4.2 A note on perplexity Perplexity is a common intrinsic evaluation metric for generation models.3 However, for"
N18-1196,P14-1006,0,0.0281883,"ould make this equivalent to a simple form of multitask learning (Caruana, 1997; Collobert and Weston, 2008). However, allowing the model to use tokens from either language at any time is simpler and results in better modeling of mixedlanguage data, which is more common in nonEnglish environments. In fact, our model occasionally ignores the flag and “code-switches” be2162 tween the two languages within a single output, which is not possible in typical multitask architectures. Using shared parameters for cross-lingual representation transfer has a large literature. Klementiev et al. (2012) and Hermann and Blunsom (2014) use multitask learning with multilingual document classification to build cross-lingual word vectors, and observe accurate lexical translations from linear vector analogy operations. They include predicting translations for words in parallel data as one of their tasks. Our translations from vector relationships (Section 5.1) derive their cross-lingual relationships from the non-linguistic input of our grounded task, without parallel data. Huang et al. (2013) note gains in speech recognition from cross-lingual learning with shared parameters. In machine translation, Johnson et al. (2016) add t"
N18-1196,Y08-1019,0,0.0259058,"most frequently in the split condition and second most frequently in the close condition, while in Chinese, they occur at around the same rate in the split and close conditions. The literature is not conclusive about the source of these differences. Xia (2014) argues that complex attributives are rarely used and sound “syntactically deviant or Europeanized” (Zhu, 1982; Xie, 2001) in Chinese, citing the leftbranching nature of the language as restricting attributives in length and complexity. There are also conflicting theories on the markedness of gradable adjectives in Chinese (Grano, 2012; Ito, 2008); such markedness may contribute to the frequency at which comparative forms are used. We also see that both languages follow the same general trend of using negation more frequently as the condition becomes more difficult. Comparatives, superlatives, and negation To detect comparative and superlative adjectives in English, we use NLTK POS-tagging, which outputs JJR and RBR for comparatives, and JJS and RBS for superlatives. In Chinese, we look for the tokens 更 g`eng ‘more’ and 比 bˇı ‘comparatively’ to detect comparatives and 最 zu`ı ‘most’ to detect superlatives. We detect negation by tokenizi"
N18-1196,P15-1029,0,0.0293672,"nal Speech Acts model of pragmatic language understanding (Frank and Goodman, 2012; Goodman and Frank, 2016), or the pragmatic posterior of the Rational Observer model (McMahan and Stone, 2015). An important difference between our speaker model and those in the work cited above is that our speaker model is a neural network that makes a combined judgment of applicability (semantic appropriateness) and availability (utterance prior), instead of modeling the two components separately. However, we stop short of directly predicting the referent of an expression discriminatively, as is done by e.g. Kennington and Schlangen (2015), so as to require a model that is usable as a speaker. A related metric is communicative success as defined by Golland et al. (2010), which judges the speaker by the accuracy of a human listener when given model-produced utterances. Our pragmatic informativeness metric instead gives a modelderived listener human utterances and assesses its accuracy at identifying colors. Pragmatic informativeness has the advantage of not requiring additional expensive human labeling in response to model outputs; it can be assessed on an existing collection of human utterances, and can therefore be considered"
N18-1196,C12-1089,0,0.0244377,"nt on the output vocabulary would make this equivalent to a simple form of multitask learning (Caruana, 1997; Collobert and Weston, 2008). However, allowing the model to use tokens from either language at any time is simpler and results in better modeling of mixedlanguage data, which is more common in nonEnglish environments. In fact, our model occasionally ignores the flag and “code-switches” be2162 tween the two languages within a single output, which is not possible in typical multitask architectures. Using shared parameters for cross-lingual representation transfer has a large literature. Klementiev et al. (2012) and Hermann and Blunsom (2014) use multitask learning with multilingual document classification to build cross-lingual word vectors, and observe accurate lexical translations from linear vector analogy operations. They include predicting translations for words in parallel data as one of their tasks. Our translations from vector relationships (Section 5.1) derive their cross-lingual relationships from the non-linguistic input of our grounded task, without parallel data. Huang et al. (2013) note gains in speech recognition from cross-lingual learning with shared parameters. In machine translati"
N18-1196,J12-1006,0,0.0217512,"Missing"
N18-1196,D16-1230,0,0.030178,"l model helps compared to a Chinese monolingual one; however, the benefit is asymmetrical, as training on monolingual English data is superior for English data to training on a mix of Chinese and English. All differences in Table 1 are significant at p < 0.001 3 Two other intrinsic metrics, word error rate (WER) and BLEU (Papineni et al., 2002), were at or worse than chance despite qualitatively adequate speaker outputs, due to high diversity in valid outputs for similar contexts. This problem is common in dialogue tasks, for which BLEU is known to be an ineffective speaker evaluation metric (Liu et al., 2016). 4 The rare words that make this difference are primarily the small number of English words that were used by the Chinese-language participants; no Chinese words were observed in the English data from Monroe et al. (2017) 2159 test acc en en en+zh 80.51 79.73 83.06 81.43 zh zh en+zh 67.16 71.81 67.75 72.89 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0 monolingual and bilingual speakers. (approximate permutation test, 10,000 samples; Pad´o, 2006), except for the decrease on the English dev set, which is significant at p < 0.05. An important difference between our corpora is that the English dataset is an orde"
N18-1196,Q15-1008,0,0.0219847,"tive conditional likelihoods of human utterances for different target colors. In practice, since the agents are trained to minimize log likelihood, we do not observe our agents frequently producing wildly unhumanlike utterances; however, this is a caveat to keep in mind for evaluating agents that do not naturally approximate a language model. The understanding model implied in this metric is equivalent to a version of the Rational Speech Acts model of pragmatic language understanding (Frank and Goodman, 2012; Goodman and Frank, 2016), or the pragmatic posterior of the Rational Observer model (McMahan and Stone, 2015). An important difference between our speaker model and those in the work cited above is that our speaker model is a neural network that makes a combined judgment of applicability (semantic appropriateness) and availability (utterance prior), instead of modeling the two components separately. However, we stop short of directly predicting the referent of an expression discriminatively, as is done by e.g. Kennington and Schlangen (2015), so as to require a model that is usable as a speaker. A related metric is communicative success as defined by Golland et al. (2010), which judges the speaker by"
N18-1196,Q17-1023,1,0.780181,"tly to the speaker as the “target”. Both players share the same goal: that the listener correctly guesses the target color. The speaker may communicate with the listener in free-form naturallanguage dialogue to achieve this goal. Thus, a model of the speaker must process representations of the colors in the context and produce an utterance to distinguish the target color from the others. We evaluate a sequence-to-sequence speaker 2155 Proceedings of NAACL-HLT 2018, pages 2155–2165 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics agent based on that of Monroe et al. (2017), who also collected the English data we use; our Chinese data are new and were collected according to the same protocols. While English and Chinese both use fairly similar syntax for color descriptions, our reference game is designed to elicit constructions that make reference to the context, and these constructions— particularly comparatives and negation—differ morpho-syntactically and pragmatically between the two languages. Additionally, Chinese is considered to have a smaller number of basic color terms (Berlin and Kay, 1969), which predicts markedness of more specific descriptions. Our p"
N18-1196,P02-1040,0,0.100943,"ase, with no training data, a model can get a perfect perplexity of 1 by predicting hunki for every token.) 5 Experimental results and analysis Pragmatic informativeness of the models on English and Chinese data is shown in Table 1. The main result is that training a bilingual model helps compared to a Chinese monolingual one; however, the benefit is asymmetrical, as training on monolingual English data is superior for English data to training on a mix of Chinese and English. All differences in Table 1 are significant at p < 0.001 3 Two other intrinsic metrics, word error rate (WER) and BLEU (Papineni et al., 2002), were at or worse than chance despite qualitatively adequate speaker outputs, due to high diversity in valid outputs for similar contexts. This problem is common in dialogue tasks, for which BLEU is known to be an ineffective speaker evaluation metric (Liu et al., 2016). 4 The rare words that make this difference are primarily the small number of English words that were used by the Chinese-language participants; no Chinese words were observed in the English data from Monroe et al. (2017) 2159 test acc en en en+zh 80.51 79.73 83.06 81.43 zh zh en+zh 67.16 71.81 67.75 72.89 1.0 0.9 0.8 0.7 0.6"
N18-1196,Q17-1024,0,\N,Missing
N18-2034,P11-1015,1,0.340217,"all but ‘External GloVE’, we report means (with bootstrapped confidence intervals) over five runs of creating the embeddings and cross-validating the classifier’s hyperparameters, mainly to help verify that the differences do not derive from variation in the representation learning phase. 5.0 0.0 Representations 0.4 Mean Pearson ρ (a) Correlations between the dot product of pairs of learned vectors and their log probabilities. pretrained initialization random initialization For our sentiment experiments, we train our representations on the unlabeled part of the IMDB review dataset released by Maas et al. (2011). This simulates a common use-case: Mittens should enable us to achieve specialized representations for these reviews while benefiting from the large datasets used to train External GloVe. the left or right of j, with the counts weighted by 1/d where d is the distance in words from j. Only words with at least 300 tokens are included in the matrix, yielding a vocabulary of 3,133 words. For regular GloVe representations derived from the IMDB data – ‘IMDB GloVE’ – we train 50-dimensional representations and use the default parameters from Pennington et al. 2014: α = 0.75, xmax = 100, and a learni"
N18-2034,N16-1018,0,0.0384839,"Missing"
N18-2034,D14-1162,0,0.105906,"experiments support our hypothesis about Mittens representations and help identify where they are most useful. We present a simple extension of the GloVe representation learning model that begins with general-purpose representations and updates them based on data from a specialized domain. We show that the resulting representations can lead to faster learning and better results on a variety of tasks. 1 Introduction 2 Many NLP tasks have benefitted from the public availability of general-purpose vector representations of words trained on enormous datasets, such as those released by the GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2016) teams. These representations, when used as model inputs, have been shown to lead to faster learning and better results in a wide variety of settings (Erhan et al., 2009, 2010; Cases et al., 2017). However, many domains require more specialized representations but lack sufficient data to train them from scratch. We address this problem with a simple extension of the GloVe model (Pennington et al., 2014) that synthesizes general-purpose representations with specialized data sets. The guiding idea comes from the retrofitting work of Faruqui et al. (2015), w"
N18-2034,W16-2902,0,0.0142411,"et al., 2009, 2010; Cases et al., 2017). However, many domains require more specialized representations but lack sufficient data to train them from scratch. We address this problem with a simple extension of the GloVe model (Pennington et al., 2014) that synthesizes general-purpose representations with specialized data sets. The guiding idea comes from the retrofitting work of Faruqui et al. (2015), which updates a space of existing representations with new information from a knowledge graph while also staying faithful to the original space (see also Yu and Dredze 2014; Mrkˇsi´c et al. 2016; Pilehvar and Collier 2016). We show that the GloVe objective is amenable to a similar retrofitting extension. We call the resulting model ‘Mittens’, evoking the idea that it is ‘GloVe with a warm start’ or a ‘warmer GloVe’. Our hypothesis is that Mittens representations synthesize the specialized data and the generalpurpose pretrained representations in a way that gives us the best of both. To test this, we conducted a diverse set of experiments. In the first, we Mittens This section defines the Mittens objective. We first vectorize GloVe to help reveal why it can be extended into a retrofitting model. 2.1 Vectorizing"
N18-2034,D17-1035,0,0.0132488,"reporting (Subjective, Objective, Assessment, Plan). The sample has 1.3 million such segments, and these segments provide our notion of ‘document’. The rationale behind this experimental set-up is that it fairly directly evaluates the vectors themselves; whereas the neural networks we evaluate next can update the representations, this model relies heavily on their initial values. Via cross-validation on the training data, we optimize the number of trees, the number of features at each split, and the maximum depth of each tree. To help factor out variation in the representation learning step (Reimers and Gurevych, 2017), we report the average accuracies over five separate complete experimental runs. 4.1 The count matrix is created from the clinical text using the specifications described in sec. 3.1, but with the count threshold set to 500 to speed up optimization. The final matrix has a 6,519-word vocabulary. We train Mittens and GloVe as in sec. 3.1. The correlations in the sense of fig. 1a are ρ ≈ 0.51 for both GloVe and Mittens. Our results are given in tab. 2. Mittens outperforms External GloVe and IMDB GloVe, indicating that it effectively combines complementary information from both. 4 Word Representa"
N18-2034,N15-1184,0,0.142403,"Missing"
N18-2034,P14-2089,0,0.0254807,"sults in a wide variety of settings (Erhan et al., 2009, 2010; Cases et al., 2017). However, many domains require more specialized representations but lack sufficient data to train them from scratch. We address this problem with a simple extension of the GloVe model (Pennington et al., 2014) that synthesizes general-purpose representations with specialized data sets. The guiding idea comes from the retrofitting work of Faruqui et al. (2015), which updates a space of existing representations with new information from a knowledge graph while also staying faithful to the original space (see also Yu and Dredze 2014; Mrkˇsi´c et al. 2016; Pilehvar and Collier 2016). We show that the GloVe objective is amenable to a similar retrofitting extension. We call the resulting model ‘Mittens’, evoking the idea that it is ‘GloVe with a warm start’ or a ‘warmer GloVe’. Our hypothesis is that Mittens representations synthesize the specialized data and the generalpurpose pretrained representations in a way that gives us the best of both. To test this, we conducted a diverse set of experiments. In the first, we Mittens This section defines the Mittens objective. We first vectorize GloVe to help reveal why it can be ex"
N18-2034,P82-1020,0,0.76822,"Missing"
N18-2070,E12-1076,0,0.0420026,"about all possible utterances, which is intractable. We fully address this problem by implementing RSA at the level of characters rather than the level of utterances or words: the neural language model emits individual characters, choosing them to balance pragmatic informativeness with overall well-formedness. Thus, the agents reason not about full utterances, but rather only about all possible character choices, a very small space. The result is that the information encoded recurrently in the neural model allows us Introduction The success of automatic image captioning (Farhadi et al., 2010; Mitchell et al., 2012; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015) demonstrates compellingly that end-to-end statistical models can align visual information with language. However, high-quality captions are not merely true, but also pragmatically informative in the sense that they highlight salient properties and help distinguish their inputs from similar images. Captioning systems trained on single images struggle to be pragmatic in this sense, producing either very general or hyper-specific descriptions. In this paper, we present a neural image captioning system1 that is a pragmatic speaker as defined by t"
N18-2070,D16-1125,0,0.193963,"target, its objective is to generate a natural language expression which identifies the target in this context. For instance, the literal caption in Figure 1 could describe both the target and the top two distractors, whereas the pragmatic caption mentions something that is most salient of the target. Intuitively, the RSA speaker achieves this by reasoning not only about what is true but also about what it’s like to be a listener in this context trying to identify the target. This core idea underlies much work in referring expression generation (Dale and Reiter, 1995; Monroe and Potts, 2015; Andreas and Klein, 2016; Monroe et al., 2017) and image captioning (Mao et al., 2016a; Vedantam et al., 2017), but these models do not fully confront the fact that the agents must reason about all possible utterances, which is intractable. We fully address this problem by implementing RSA at the level of characters rather than the level of utterances or words: the neural language model emits individual characters, choosing them to balance pragmatic informativeness with overall well-formedness. Thus, the agents reason not about full utterances, but rather only about all possible character choices, a very small space."
N18-2070,Q17-1023,1,0.785434,"to generate a natural language expression which identifies the target in this context. For instance, the literal caption in Figure 1 could describe both the target and the top two distractors, whereas the pragmatic caption mentions something that is most salient of the target. Intuitively, the RSA speaker achieves this by reasoning not only about what is true but also about what it’s like to be a listener in this context trying to identify the target. This core idea underlies much work in referring expression generation (Dale and Reiter, 1995; Monroe and Potts, 2015; Andreas and Klein, 2016; Monroe et al., 2017) and image captioning (Mao et al., 2016a; Vedantam et al., 2017), but these models do not fully confront the fact that the agents must reason about all possible utterances, which is intractable. We fully address this problem by implementing RSA at the level of characters rather than the level of utterances or words: the neural language model emits individual characters, choosing them to balance pragmatic informativeness with overall well-formedness. Thus, the agents reason not about full utterances, but rather only about all possible character choices, a very small space. The result is that th"
N19-1365,W06-3907,1,0.738151,"Missing"
N19-1365,D14-1162,0,0.0807548,"Missing"
N19-1365,N18-1202,0,0.10909,"Missing"
N19-1365,S12-1020,1,0.887325,"Missing"
N19-1365,N18-1101,0,0.568933,"Routing Networks with an EM-like training approach. We show how to incorporate RRNs into different neural components (word representation layers, recurrent network hidden layers, classifier layers), and we study their application to natural language inference (NLI), in which premise–hypothesis pairs are labeled for whether the premise entails, contradicts, or is neutral with respect to the hypothesis. We chose this task because reasoning in natural language involves context-sensitive interpretation of words and sentences as well as compositional structure. We make use of the MULTINLI corpus (Williams et al., 2018), which includes text from multiple genres that we expect to condition linguistic senses in complex ways. Our experiments show that RRNs learn policies and components that reflect this genre structure, which leads to superior performance. We also introduce a new corpus of NLI examples involving implicative constructions like manage to, be able to, and fail to (Karttunen, 1971, 2012). This corpus follows the design of many recent NLI corpora, but with the added challenges of reasoning about implicatives, which have logical signatures that interact compositionally with each other and with surrou"
N19-1365,P18-2104,0,0.018209,"wo examples are learned using different weights, there is low potential for transfer and interference. This is beneficial for unrelated examples, but limits the potential for learning about commonalities between related ones. RRNs extend vanilla neural networks by granting them the leverage to navigate this trade-off by making global functional decisions at the module level. As a result, RRNs explicitly make decisions to compress or orthogonalize knowledge between examples by deciding whether to share specific weights. Thus far, hard-selection routing has not been applied to language domains. Zaremoodi et al. (2018) introduced a soft version of routing that falls within a larger class of Mixtures of Experts (MoE) models (Jacobs et al., 1991). However, M o E models differ from RRN s in two crucial ways. First, MoE models generally do not consider the recursive application of functions. The promise is that we can compose functions to reflect the compositional aspects of a problem. Imagine we have a sentence encoding and we want to answer a particular question. We can now condition the router on the question, so that it applies exactly the functions required that translate the encoding to extract the answer"
P10-1018,de-marneffe-etal-2006-generating,1,0.0316939,"Missing"
P10-1018,W09-3920,1,0.905684,"Missing"
P10-1018,W10-0719,1,0.812257,"Missing"
P10-1018,P94-1009,0,0.125376,"no’ but conveys this only partially to the hearer. c. The speaker is uncertain of ‘yes’ or ‘no’ and conveys this uncertainty to the hearer. d. The speaker is uncertain of ‘yes’ or ‘no’, but the hearer infers one of those with confidence. Related work Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. In the computational literature, Green and Carberry (1994, 1999) provide an extensive model that interprets and generates indirect answers to polar questions. They propose a logical inference model which makes use of discourse plans and coherence relations to infer categorical answers. However, to adequately interpret indirect answers, the uncertainty inherent in some answers needs to be captured (de Marneffe et al., 2009). While a straightforward ‘yes’ or ‘no’ response is clear in some indirect answers, such as in (1), the intended answer is less certain in other cases (2):1 (1) A: Do you think that’s a good idea, that we just begin to ignore these"
P10-1018,J99-3004,0,0.191727,"Missing"
P10-1018,J80-3003,0,0.513184,"to learn meanings that can drive pragmatic inference in dialogue. This paper demonstrates to some extent that meaning can be grounded from text in this way. 2 a. The speaker is certain of ‘yes’ or ‘no’ and conveys that directly and successfully to the hearer. b. The speaker is certain of ‘yes’ or ‘no’ but conveys this only partially to the hearer. c. The speaker is uncertain of ‘yes’ or ‘no’ and conveys this uncertainty to the hearer. d. The speaker is uncertain of ‘yes’ or ‘no’, but the hearer infers one of those with confidence. Related work Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. In the computational literature, Green and Carberry (1994, 1999) provide an extensive model that interprets and generates indirect answers to polar questions. They propose a logical inference model which makes use of discourse plans and coherence relations to infer categorical answers. However, to adequately interpret indirect answers, the uncertainty inher"
P10-1018,D08-1027,0,0.0518347,"Missing"
P10-1018,P03-1054,1,0.00969769,"Missing"
P10-1018,D08-1103,0,\N,Missing
P11-1015,H05-1073,0,0.0492431,"c and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of"
P11-1015,E06-1027,0,0.0693006,"search arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews for movies, pr"
P11-1015,D10-1005,0,0.0421157,"Missing"
P11-1015,N09-1037,0,0.0837585,"ization 144 weights (λ and ν), and the word vector dimensionality β. 3.2 Capturing Word Sentiment The model presented so far does not explicitly capture sentiment information. Applying this algorithm to documents will produce representations where words that occur together in documents have similar representations. However, this unsupervised approach has no explicit way of capturing which words are predictive of sentiment as opposed to content-related. Much previous work in natural language processing achieves better representations by learning from multiple tasks (Collobert and Weston, 2008; Finkel and Manning, 2009). Following this theme we introduce a second task to utilize labeled documents to improve our model’s word representations. Sentiment is a complex, multi-dimensional concept. Depending on which aspects of sentiment we wish to capture, we can give some body of text a sentiment label s which can be categorical, continuous, or multi-dimensional. To leverage such labels, we introduce an objective that the word vectors of our model should predict the sentiment label using some appropriate predictor, sˆ = f (φw ). (8) Using an appropriate predictor function f (x) we map a word vector φw to a predict"
P11-1015,W06-3808,0,0.0590443,"ve semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews for movies, products, etc. The technique is suffiProceedin"
P11-1015,P10-1141,0,0.0488633,"(weighting, normalization, dimensionality reduction algorithm) with little theoretical guidance to suggest which to prefer. Using term frequency (tf) and inverse document frequency (idf) weighting to transform the values in a VSM often increases the performance of retrieval and categorization systems. Delta idf weighting (Martineau and Finin, 2009) is a supervised variant of idf weighting in which the idf calculation is done for each document class and then one value is subtracted from the other. Martineau and Finin present evidence that this weighting helps with sentiment classification, and Paltoglou and Thelwall (2010) systematically explore a number of weighting schemes in the context of sentiment analysis. The success of delta idf weighting in previous work suggests that incorporating sentiment information into VSM values via supervised methods is helpful for sentiment analysis. We adopt this insight, but we are able to incorporate it directly into our model’s objective function. (Section 4 compares our approach with a representative sample of such weighting schemes.) The model we present in the next section draws inspiration from prior work on both probabilistic topic modeling and vector-spaced models fo"
P11-1015,P04-1035,0,0.105834,"on for Computational Linguistics, pages 142–150, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ciently general to work also with continuous and multi-dimensional notions of sentiment as well as non-sentiment annotations (e.g., political affiliation, speaker commitment). After presenting the model in detail, we provide illustrative examples of the vectors it learns, and then we systematically evaluate the approach on document-level and sentence-level classification tasks. Our experiments involve the small, widely used sentiment and subjectivity corpora of Pang and Lee (2004), which permits us to make comparisons with a number of related approaches and published results. We also show that this dataset contains many correlations between examples in the training and testing sets. This leads us to evaluate on, and make publicly available, a large dataset of informal movie reviews from the Internet Movie Database (IMDB). 2 Related work ing sentiment-imbued topics rather than embedding words in a vector space. Vector space models (VSMs) seek to model words directly (Turney and Pantel, 2010). Latent Semantic Analysis (LSA), perhaps the best known VSM, explicitly learns"
P11-1015,P05-1015,0,0.242865,"ontent and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews for movies, products, etc. The tec"
P11-1015,W02-1011,0,0.101706,"ction 2 for inducing word representations from the topic matrix. To train the 50-topic LDA model we use code released by Blei et al. (2003). We use the same 5,000 term vocabulary for LDA as is used for training word vector models. We leave the LDA hyperparameters at their default values, though some work suggests optimizing over priors for LDA is important (Wallach et al., 2009). Weighting Variants We evaluate both binary (b) term frequency weighting with smoothed delta idf (∆t’) and no idf (n) because these variants worked well in previous experiments in sentiment (Martineau and Finin, 2009; Pang et al., 2002). In all cases, we use cosine normalization (c). Paltoglou and Thelwall (2010) perform an extensive analysis 147 of such weighting variants for sentiment tasks. 4.3 Document Polarity Classification Our first evaluation task is document-level sentiment polarity classification. A classifier must predict whether a given review is positive or negative given the review text. Given a document’s bag of words vector v, we obtain features from our model using a matrixvector product Rv, where v can have arbitrary tf.idf weighting. We do not cosine normalize v, instead applying cosine normalization to th"
P11-1015,N07-1038,0,0.0724171,"distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews for movies, products, etc. The technique is suffiProceedings of the 49th Annual Meetin"
P11-1015,P10-1040,0,0.0781167,"esentations are a critical component of many natural language processing systems. It is common to represent words as indices in a vocabulary, but this fails to capture the rich relational structure of the lexicon. Vector-based models do much better in this regard. They encode continuous similarities between words as distance or angle between word vectors in a high-dimensional space. The general approach has proven useful in tasks such as word sense disambiguation, named entity 142 recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong posi"
P13-1025,abdul-mageed-diab-2012-awatif,0,0.018015,"lone we obtained the same qualitative results, with statistical significance always lower than 0.01.12 Ide, 1989; Blum-Kulka and Kasper, 1990; BlumKulka, 2003; Watts, 2003; Byon, 2006). The starting point for most research is the theory of Brown and Levinson (1987). Aspects of this theory have been explored from game-theoretic perspectives (Van Rooy, 2003) and implemented in language generation systems for interactive narratives (Walker et al., 1997), cooking instructions, (Gupta et al., 2007), translation (Faruqui and Pado, 2012), spoken dialog (Wang et al., 2012), and subjectivity analysis (Abdul-Mageed and Diab, 2012), among others. In recent years, politeness has been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and status, which has been studied in the context of workplace d"
P13-1025,de-marneffe-etal-2006-generating,0,0.0364066,"Missing"
P13-1025,E12-1064,0,0.0158263,"the newly designed experiments. When we re-ran our experiments on human-labeled data alone we obtained the same qualitative results, with statistical significance always lower than 0.01.12 Ide, 1989; Blum-Kulka and Kasper, 1990; BlumKulka, 2003; Watts, 2003; Byon, 2006). The starting point for most research is the theory of Brown and Levinson (1987). Aspects of this theory have been explored from game-theoretic perspectives (Van Rooy, 2003) and implemented in language generation systems for interactive narratives (Walker et al., 1997), cooking instructions, (Gupta et al., 2007), translation (Faruqui and Pado, 2012), spoken dialog (Wang et al., 2012), and subjectivity analysis (Abdul-Mageed and Diab, 2012), among others. In recent years, politeness has been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of ho"
P13-1025,W10-0719,1,0.284783,"Missing"
P13-1025,W11-0711,0,0.19197,"been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and status, which has been studied in the context of workplace discourse (Bramsen et al., ; Diehl et al., 2007; Peterson et al., 2011; Prabhakaran et al., 2012; Gilbert, 2012; McCallum et al., 2007) and social networking (Scholand et al., 2010). However, this research focusses on domain-specific textual cues, whereas the present work seeks to leverage domain-independent politeness cues, building on the literature on how politeness affects worksplace social dynamics and power structures (Gyasi Obeng, 1997; Chilton, 1990; Andersson and Pearson, 1999; Rogers and Lee-Wong, 2003; Holmes and Stubbe, 2005). Burke and Kraut (2008b) study the question of how and why specific individuals rise to administrative positions on Wikipedia,"
P13-1025,N12-1057,0,0.0212787,"settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and status, which has been studied in the context of workplace discourse (Bramsen et al., ; Diehl et al., 2007; Peterson et al., 2011; Prabhakaran et al., 2012; Gilbert, 2012; McCallum et al., 2007) and social networking (Scholand et al., 2010). However, this research focusses on domain-specific textual cues, whereas the present work seeks to leverage domain-independent politeness cues, building on the literature on how politeness affects worksplace social dynamics and power structures (Gyasi Obeng, 1997; Chilton, 1990; Andersson and Pearson, 1999; Rogers and Lee-Wong, 2003; Holmes and Stubbe, 2005). Burke and Kraut (2008b) study the question of how and why specific individuals rise to administrative positions on Wikipedia, and Danescu-Niculescu-Miz"
P13-1025,W12-1603,0,0.0397809,"re-ran our experiments on human-labeled data alone we obtained the same qualitative results, with statistical significance always lower than 0.01.12 Ide, 1989; Blum-Kulka and Kasper, 1990; BlumKulka, 2003; Watts, 2003; Byon, 2006). The starting point for most research is the theory of Brown and Levinson (1987). Aspects of this theory have been explored from game-theoretic perspectives (Van Rooy, 2003) and implemented in language generation systems for interactive narratives (Walker et al., 1997), cooking instructions, (Gupta et al., 2007), translation (Faruqui and Pado, 2012), spoken dialog (Wang et al., 2012), and subjectivity analysis (Abdul-Mageed and Diab, 2012), among others. In recent years, politeness has been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and sta"
P13-2014,D10-1040,0,0.484734,"Missing"
P13-2014,J12-1006,0,0.0376672,"Missing"
P13-2014,N13-1127,1,0.921432,"ut implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structures required for implicature calculation. Optimal decision making in Dec-POMDPs is NEXP complete, so we employ the single-agent POMDP approximation of Vogel et al. (2013). We show that agents in the Dec-POMDP reach implicature-rich interpretations simply as a byproduct of the way they reason about each other to maximize joint utility. Our simulations involve a reference game and a dynamic, interactional scenario involving implemented artificial agents. Conversational implicatures involve reasoning about multiply nested belief structures. This complexity poses significant challenges for computational models of conversation and cognition. We show that agents in the multi-agent DecentralizedPOMDP reach implicature-rich interpretations simply as a by-product of th"
P15-1006,D14-1217,1,0.803431,"Missing"
P15-1006,C12-1042,0,0.0459437,"Missing"
P15-1006,D13-1197,0,0.0217911,"rom seed description sentences (top). Additional descriptions provided by other participants from the created scene (bottom). Our dataset contains around 19 scenes per seed sentence, for a total of 1129 scenes. Scenes exhibit variation in the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More r"
P15-1006,D10-1040,0,0.0330872,"ions serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguisti"
P15-1006,D14-1086,0,0.0275189,"ences (top). Additional descriptions provided by other participants from the created scene (bottom). Our dataset contains around 19 scenes per seed sentence, for a total of 1129 scenes. Scenes exhibit variation in the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to"
P15-1006,H89-1033,0,0.0608002,"d Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes. However, the discourse domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data. These systems use manually defined mappings between language and"
P15-1006,Q13-1016,0,0.0443334,"approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. Pioneering work on the SHRDLU system (Winograd, 1972)"
P15-1006,P14-5010,1,0.0427666,"Missing"
P15-1006,P02-1040,0,0.0925837,"Section 5.3, we describe how we use our learned model to augment this model. This rule-based approach is a three-stage process using established NLP systems: 1) The input text is split into multiple sentences and parsed using the Stanford CoreNLP pipeline (Manning et Model To create a model for generating scene templates from text, we train a classifier to learn lexical 1 Available at http://nlp.stanford.edu/data/ text2scene.shtml. 2 Mean 26.2 words, SD 17.4; versus mean 16.6, SD 7.2 for the seed sentences. If one considers seed sentences to be the “reference,” the macro-averaged BLEU score (Papineni et al., 2002) of the Turker descriptions is 12.0. 56 red cup round yellow table green room black top tan love seat black bed open window Figure 4: Some examples extracted from the top 20 highest-weight features in our learned model: lexical terms from the descriptions in our scene corpus are grounded to 3D models within the scene corpus. al., 2014). Head words of noun phrases are identified as candidate object categories, filtered using WordNet (Miller, 1995) to only include physical objects. 2) References to the same object are collapsed using the Stanford coreference system. 3) Properties are attached to"
P15-1006,W11-0147,0,0.0266703,"te a concrete 3D scene visualizing the input description (right). The 3D scene is constructed by retrieving and arranging appropriate 3D models. 2 Task Description the structure of environments is rarely mentioned in natural language (e.g., that most tables are supported on the floor and in an upright orientation). Unfortunately, common 3D representations of objects and scenes used in computer graphics specify only geometry and appearance, and rarely include such information. Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP. In the text to 3D scene generation task, the input is a natural language description, and the output is a 3D representation of a plausible scene that fits the description and can be viewed and rendered from multiple perspectives. More precisely, given an utterance x as input, the output is a scene y: an arrangement of 3D models representing objects at specified positions and orientations in space. In this paper, we focus"
P15-1006,W14-3102,1,\N,Missing
P16-1139,D15-1075,1,0.173704,"volutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1466–1477, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics stack the cat the cat composition composition reduce shift transition tracking transition tracking sat down buffer the cat sat tracking down sat down (a) The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down. ‘Tracking’, ‘transition’, and ‘composition’ are neural network layers. Gray arrows indicate connections which are blocked by a gating fu"
P16-1139,P15-2142,0,0.0138497,"tecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time."
P16-1139,D14-1082,1,0.425506,"s upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive searc"
P16-1139,D16-1053,0,0.0398047,"Missing"
P16-1139,P15-1033,0,0.0232099,"t test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents"
P16-1139,N16-1024,0,0.0202595,"and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural lan"
P16-1139,P04-1013,0,0.021347,"produces the required parse structure on the fly. This design improves upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of opera"
P16-1139,W03-3017,0,0.0499586,"lated work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003). A shift-reduce parser accepts a sequence of input tokens x = (x 0, . . . , x N −1 ) and consumes transitions a = (a0, . . . , aT −1 ), where each at ∈ {shift, reduce} specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It proceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binary-branching tree structure over N words, this requires T = 2N − 1 transitions through a total of T + 1 states. The parser uses two auxiliary data structures: a s"
P16-1139,P14-1062,0,0.0561508,". This component, the sentence encoder, is generally formulated as a learned parametric function from a sequence of word vectors to a sentence vector, and this function can take a range of different forms. Common sentence encoders include sequencebased recurrent neural network models (RNNs, see Figure 1a) with Long Short-Term Memory (LSTM, first two authors contributed equally. cat cat (a) A conventional sequence-based RNN for two sentences. Introduction ∗ The old the Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of t"
P16-1139,Q16-1032,0,0.0243001,"Missing"
P16-1139,D15-1278,0,0.0436106,"sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1466–1477, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics stack the cat the cat composition composition reduce shift transition tracking transition tracking sat down buffer the cat sat tracking down sat down (a) The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down. ‘Tracking’, ‘transition’, and ‘composition’ are neural network layers. Gray arrows indicate connections which are b"
P16-1139,P16-2022,0,0.197438,"Missing"
P16-1139,P83-1017,0,0.426606,"rsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003). A shift-reduce parser accepts a sequence of input tokens x = (x 0, . . . , x N −1 ) and consumes transitions a = (a0, . . . , aT −1 ), where each at ∈ {shift, reduce} specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It proceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binary-branching tree structure over N words, this requires T = 2N − 1 transitions through a total of T + 1 states. The parser uses two auxiliary data s"
P16-1139,D11-1014,1,0.416432,"Missing"
P16-1139,P15-1150,1,0.752904,"Missing"
P16-1139,N16-1170,0,0.0699057,"Missing"
P16-1139,N16-1035,0,0.0142436,"ion for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence"
P16-1139,D14-1162,1,\N,Missing
P16-1139,W07-2218,0,\N,Missing
Q14-1024,W11-1701,0,0.0141253,"0; Feldman Barrett and Russell, 1998; Rubin and Talerico, 2009), and the dominant application is predicting the valence of product, company, and service reviews. We adopt the conceptual assumptions of this work for our basic sentiment model, but our focus is on person-to-person evaluations and their social consequences. This involves elements of work on modeling political affiliation (Agrawal et al., 2003; Malouf and Mullen, 2008; Yu et al., 2008), bias (Yano et al., 2010; Recasens et al., 2013), and stance on debate topics (Thomas et al., 2006; Somasundaran and Wiebe, 2010; Lin et al., 2006; Anand et al., 2011), but these aspects of belief and social identity are not our primary concern. Rather, we expect them to be predictive of the sentiment classifications we aim to make—e.g., if two people share political views, they will tend to evaluate each other positively. Recent work in sentiment analysis has brought in topical, contextual, and social information to make more nuanced predictions about language (Jurafsky et al., 2014; Wilson et al., 2005; Blitzer et al., 2007). We build on these insights with our model, which seeks to modulate sentiment predictions based on network information (and vice ver"
Q14-1024,P07-1056,0,0.0323592,"t al., 2010; Recasens et al., 2013), and stance on debate topics (Thomas et al., 2006; Somasundaran and Wiebe, 2010; Lin et al., 2006; Anand et al., 2011), but these aspects of belief and social identity are not our primary concern. Rather, we expect them to be predictive of the sentiment classifications we aim to make—e.g., if two people share political views, they will tend to evaluate each other positively. Recent work in sentiment analysis has brought in topical, contextual, and social information to make more nuanced predictions about language (Jurafsky et al., 2014; Wilson et al., 2005; Blitzer et al., 2007). We build on these insights with our model, which seeks to modulate sentiment predictions based on network information (and vice versa). 2.2 Signed-network analysis Many social networks encode person-to-person sentiment information via signed edges between users 298 summarizing their opinions of each other. In this setting, one can leverage sociological theories of pairwise relationships and group-level organization to identify and understand patterns in these relationships (Heider, 1946; Cartwright and Harary, 1956). Balance theory is based on simple intuitions like ‘a friend of my friend is"
Q14-1024,W06-2915,0,0.00917804,"sity (Russell, 1980; Feldman Barrett and Russell, 1998; Rubin and Talerico, 2009), and the dominant application is predicting the valence of product, company, and service reviews. We adopt the conceptual assumptions of this work for our basic sentiment model, but our focus is on person-to-person evaluations and their social consequences. This involves elements of work on modeling political affiliation (Agrawal et al., 2003; Malouf and Mullen, 2008; Yu et al., 2008), bias (Yano et al., 2010; Recasens et al., 2013), and stance on debate topics (Thomas et al., 2006; Somasundaran and Wiebe, 2010; Lin et al., 2006; Anand et al., 2011), but these aspects of belief and social identity are not our primary concern. Rather, we expect them to be predictive of the sentiment classifications we aim to make—e.g., if two people share political views, they will tend to evaluate each other positively. Recent work in sentiment analysis has brought in topical, contextual, and social information to make more nuanced predictions about language (Jurafsky et al., 2014; Wilson et al., 2005; Blitzer et al., 2007). We build on these insights with our model, which seeks to modulate sentiment predictions based on network info"
Q14-1024,P04-1035,0,0.00969619,"redictions of social balance and status theories for the bold black edge, given the thin gray edges. Balance theory reasons about undirected, status theory about directed, triangles. In the status diagrams, node size signifies social status. A positive edge may be replaced by a negative edge in the opposite direction, and vice versa, without changing the prediction. Status theory makes no prediction in the rightmost case. (b) Situations of the sort we aim to capture. At left, the network resolves textual ambiguity. At right, the text compensates for edge-label sparsity. graph-cuts approach of Pang and Lee (2004). They are guided by an assumption of homophily, i.e., that certain social relationships correlate with agreement on certain topics: Thomas et al. (2006) use party affiliation and mentions in speeches to predict voting patterns, and Tan et al. (2011) use Twitter follows and mentions to predict attitudes about political and social events. Related ideas are pursued by Ma et al. (2011) and Hu et al. (2013), who add terms to their models enforcing homophily between friends with regard to their preferences. We adopt some of the assumptions of the above authors, but our task is fundamentally differe"
Q14-1024,P13-1162,0,0.0377102,"ssumes a dimensional model in which emotions are defined primarily by valence/polarity and arousal/intensity (Russell, 1980; Feldman Barrett and Russell, 1998; Rubin and Talerico, 2009), and the dominant application is predicting the valence of product, company, and service reviews. We adopt the conceptual assumptions of this work for our basic sentiment model, but our focus is on person-to-person evaluations and their social consequences. This involves elements of work on modeling political affiliation (Agrawal et al., 2003; Malouf and Mullen, 2008; Yu et al., 2008), bias (Yano et al., 2010; Recasens et al., 2013), and stance on debate topics (Thomas et al., 2006; Somasundaran and Wiebe, 2010; Lin et al., 2006; Anand et al., 2011), but these aspects of belief and social identity are not our primary concern. Rather, we expect them to be predictive of the sentiment classifications we aim to make—e.g., if two people share political views, they will tend to evaluate each other positively. Recent work in sentiment analysis has brought in topical, contextual, and social information to make more nuanced predictions about language (Jurafsky et al., 2014; Wilson et al., 2005; Blitzer et al., 2007). We build on"
Q14-1024,W10-0214,0,0.0857772,"nce/polarity and arousal/intensity (Russell, 1980; Feldman Barrett and Russell, 1998; Rubin and Talerico, 2009), and the dominant application is predicting the valence of product, company, and service reviews. We adopt the conceptual assumptions of this work for our basic sentiment model, but our focus is on person-to-person evaluations and their social consequences. This involves elements of work on modeling political affiliation (Agrawal et al., 2003; Malouf and Mullen, 2008; Yu et al., 2008), bias (Yano et al., 2010; Recasens et al., 2013), and stance on debate topics (Thomas et al., 2006; Somasundaran and Wiebe, 2010; Lin et al., 2006; Anand et al., 2011), but these aspects of belief and social identity are not our primary concern. Rather, we expect them to be predictive of the sentiment classifications we aim to make—e.g., if two people share political views, they will tend to evaluate each other positively. Recent work in sentiment analysis has brought in topical, contextual, and social information to make more nuanced predictions about language (Jurafsky et al., 2014; Wilson et al., 2005; Blitzer et al., 2007). We build on these insights with our model, which seeks to modulate sentiment predictions bas"
Q14-1024,W06-1639,0,0.744912,"er et al., 2010), and we show that this implementation outperforms text-only and networkonly versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus, in which Wikipedia editors discuss and vote on who should be 297 Transactions of the Association for Computational Linguistics, 2 (2014) 297–310. Action Editor: Hal Daume III. c Submitted 5/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. promoted within the Wikipedia hierarchy (Sec. 4), and the Convote U.S. Congressional speech corpus (Thomas et al., 2006), in which elected officials discuss political topics (Sec. 5). These corpora differ dramatically in size, in the style and quality of their textual data, and in the structure and observability of their networks. Together, they provide a clear picture of how joint models of text and network structure can excel where their component parts cannot. 2 Background and related work 2.1 Sentiment analysis In NLP, the label sentiment analysis covers diverse phenomena concerning how information about emotions, attitudes, perspectives, and social identities is conveyed in language (Pang and Lee, 2008). M"
Q14-1024,H05-1044,0,0.0699621,", 2008), bias (Yano et al., 2010; Recasens et al., 2013), and stance on debate topics (Thomas et al., 2006; Somasundaran and Wiebe, 2010; Lin et al., 2006; Anand et al., 2011), but these aspects of belief and social identity are not our primary concern. Rather, we expect them to be predictive of the sentiment classifications we aim to make—e.g., if two people share political views, they will tend to evaluate each other positively. Recent work in sentiment analysis has brought in topical, contextual, and social information to make more nuanced predictions about language (Jurafsky et al., 2014; Wilson et al., 2005; Blitzer et al., 2007). We build on these insights with our model, which seeks to modulate sentiment predictions based on network information (and vice versa). 2.2 Signed-network analysis Many social networks encode person-to-person sentiment information via signed edges between users 298 summarizing their opinions of each other. In this setting, one can leverage sociological theories of pairwise relationships and group-level organization to identify and understand patterns in these relationships (Heider, 1946; Cartwright and Harary, 1956). Balance theory is based on simple intuitions like ‘a"
Q14-1024,W10-0723,0,0.0157307,"2008). Most work assumes a dimensional model in which emotions are defined primarily by valence/polarity and arousal/intensity (Russell, 1980; Feldman Barrett and Russell, 1998; Rubin and Talerico, 2009), and the dominant application is predicting the valence of product, company, and service reviews. We adopt the conceptual assumptions of this work for our basic sentiment model, but our focus is on person-to-person evaluations and their social consequences. This involves elements of work on modeling political affiliation (Agrawal et al., 2003; Malouf and Mullen, 2008; Yu et al., 2008), bias (Yano et al., 2010; Recasens et al., 2013), and stance on debate topics (Thomas et al., 2006; Somasundaran and Wiebe, 2010; Lin et al., 2006; Anand et al., 2011), but these aspects of belief and social identity are not our primary concern. Rather, we expect them to be predictive of the sentiment classifications we aim to make—e.g., if two people share political views, they will tend to evaluate each other positively. Recent work in sentiment analysis has brought in topical, contextual, and social information to make more nuanced predictions about language (Jurafsky et al., 2014; Wilson et al., 2005; Blitzer et"
Q17-1023,D16-1125,0,0.0450467,"e effectively than literal semantic agents could. In most work on RSA, the literal semantic agents use fixed message sets and stipulated grammars, which is a barrier to experiments in linguistically complex domains. In our formulation, the literal semantic agents are recurrent neural networks (RNNs) that produce and interpret color descriptions in context. These models are learned from data and scale easily to large datasets containing diverse utterances. The RSA recursion is then defined in terms of these base agents: the pragmatic speaker produces utterances based on a literal RNN listener (Andreas and Klein, 2016), and the pragmatic listener interprets utterances based on the pragmatic speaker’s behavior. We focus on accuracy in a listener task (i.e., at language understanding). However, our most successful model integrates speaker and listener perspectives, combining predictions made by a system trained to understand color descriptions and one trained to produce them. We evaluate this model with a new, psycholinguistically motivated corpus of real-time, dyadic reference games in which the referents are patches of color. Our task is fundamentally the same as that of Baumgaertner et al. (2012), but the"
Q17-1023,S12-1013,0,0.468381,"N listener (Andreas and Klein, 2016), and the pragmatic listener interprets utterances based on the pragmatic speaker’s behavior. We focus on accuracy in a listener task (i.e., at language understanding). However, our most successful model integrates speaker and listener perspectives, combining predictions made by a system trained to understand color descriptions and one trained to produce them. We evaluate this model with a new, psycholinguistically motivated corpus of real-time, dyadic reference games in which the referents are patches of color. Our task is fundamentally the same as that of Baumgaertner et al. (2012), but the corpus we release is larger by several orders of magnitude, consisting of 948 complete games with 53,365 utterances produced by human participants paired into dyads on the web. The linguistic behavior of the players exhibits many of the intricacies of language in general, including not just the context dependence and cognitive complexity discussed above, but also compositionality, vagueness, and ambiguity. While many previous data sets feature descriptions of individual colors (Cook et al., 2005; Munroe, 2010; Kawakami et al., 2016), situating colors in a communicative context elicit"
Q17-1023,D10-1040,0,0.0110224,"utterances conditioned on the context; it has successfully learned that drab would be more likely as a description of the grayish green than as a description of the yellowish one in this context. The speaker-based listener L1 therefore predicts the true target, with greater confidence than L0 or L2 . This prediction results in the blends La and Le preferring the true target, allowing the speaker’s perspective to override the listener’s. 7 Related work Prior work combining machine learning with probabilistic pragmatic reasoning models has largely focused on the speaker side, i.e., generation. Golland et al. (2010) develop a pragmatic speaker model, S(L0 ), that reasons about log-linear listeners trained on human utterances containing spatial references in virtual-world environments. Tellex et al. (2014) apply a similar technique, under the name inverse semantics, to create a robot that can informatively ask humans for assistance in accomplishing tasks. Meo et al. (2014) evaluate a model of color description generation (McMahan and Stone, 2015) on the color reference data of Baumgaertner et al. (2012) by creating an L(S0 ) listener. Monroe and Potts (2015) implement an end-to-end trained S(L(S0 )) model"
Q17-1023,D16-1202,0,0.0114903,". Our task is fundamentally the same as that of Baumgaertner et al. (2012), but the corpus we release is larger by several orders of magnitude, consisting of 948 complete games with 53,365 utterances produced by human participants paired into dyads on the web. The linguistic behavior of the players exhibits many of the intricacies of language in general, including not just the context dependence and cognitive complexity discussed above, but also compositionality, vagueness, and ambiguity. While many previous data sets feature descriptions of individual colors (Cook et al., 2005; Munroe, 2010; Kawakami et al., 2016), situating colors in a communicative context elicits greater variety in language use, including negations, comparatives, superlatives, metaphor, and shared associations. Experiments on the data in our corpus show that this combined pragmatic model improves accuracy in interpreting human-produced descriptions over the basic RNN listener alone. We find that the largest improvement over the single RNN comes from blending it with an RNN trained to perform the speaker task, despite the fact that a model based 326 Figure 1: Example trial in corpus collection task, from speaker’s perspective. The ta"
Q17-1023,Q15-1008,0,0.0203063,"ner’s. 7 Related work Prior work combining machine learning with probabilistic pragmatic reasoning models has largely focused on the speaker side, i.e., generation. Golland et al. (2010) develop a pragmatic speaker model, S(L0 ), that reasons about log-linear listeners trained on human utterances containing spatial references in virtual-world environments. Tellex et al. (2014) apply a similar technique, under the name inverse semantics, to create a robot that can informatively ask humans for assistance in accomplishing tasks. Meo et al. (2014) evaluate a model of color description generation (McMahan and Stone, 2015) on the color reference data of Baumgaertner et al. (2012) by creating an L(S0 ) listener. Monroe and Potts (2015) implement an end-to-end trained S(L(S0 )) model for referring expression generation in a reference game task. Many of these models require enumerating the set of possible utterances for each context, which is infeasible when utterances are as varied as those in our dataset. The closest work to ours that we are aware of is that of Andreas and Klein (2016), who also combine neural speaker and listener models in a reference game setting. They propose a pragmatic speaker, S(L0 ), samp"
Q17-1023,D16-1243,1,0.90868,"Missing"
Q17-1023,paetzel-etal-2014-multimodal,0,0.0324306,"Missing"
Q17-1023,J81-4005,0,0.744938,"Missing"
Q17-1023,N03-1033,0,0.0296469,"ge use and context type. Comparatives and superlatives As noted in Section 1, comparative morphology implicitly encodes a dependence on the context; a speaker who refers to the target color as the darker blue is presupposing that there is another (lighter) blue in the context. Similarly, superlatives like the bluest one or the lightest one presuppose that all the colors can be compared along a specific semantic dimension. We thus expect to see this morphology more often where two or more of the colors are comparable in this way. To test this, we used the Stanford CoreNLP part-ofspeech tagger (Toutanova et al., 2003) to mark the presence or absence of comparatives (JJR or RBR) and superlatives (JJS or RBS) for each message. We found two related patterns across conditions. First, participants were significantly more likely to use both comparatives (z = 37.39) and superlatives (z = 31.32) when one or more distractors were close to the target. Second, we found evidence of an asymmetry in the use of these constructions across the split and close contexts. Comparatives were used significantly more often in the split context (z = 4.4), where only one distractor was close to the target, while superlatives were m"
Q17-1023,P13-2014,1,0.343317,"eaker who reasons about a listener who reasons about L directly. The back-and-forth nature of this interpretive process mirrors that of conversational implicature (Grice, 1975) and reflects more general ideas from Bayesian cognitive modeling (Tenenbaum et al., 2011). The model and its variants have been shown to capture a wide range of pragmatic phenomena in a cognitively realistic manner (Goodman and Stuhlm¨uller, 2013; Smith et al., 2013; Kao et al., 2014; Bergen et al., 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al., 2014; Vogel et al., 2013). c3 Softmax c1 (µ, Σ) c2 h/si h; hsi h; u1 h; u2 Fully connected c3 LSTM LSTM h Embedding u2 u2 Softmax • • • u1 u1 c1 u3 (a) The L0 agent processes tokens ui of a color description u sequentially. The final representation is transformed into a Gaussian distribution in color space, which is used to score the context colors c1 . . . c3 . c2 ct (b) The S0 agent processes the target color ct in context and produces tokens ui of a color description sequentially. Each step in production is conditioned by the context representation h and the previous word produced. Figure 3: The neural base speaker"
W09-3920,J80-3003,0,0.886264,"y will be based on some combination of this disparate, partially conflicting, uncertain evidence. The plan and logical inference model of Green and Carberry falters in the face of such collections of uncertain evidence. However, natural dialogues are often interpreted in the midst of uncertain and conflicting signals. We therefore propose to enrich a logical inference model with probabilistic methods to deal with such cases. This study addresses the phenomenon of indirect question–answer pairs (IQAP), such as in (1), from both empirical and engineering perspectives. Introduction Clark (1979), Perrault and Allen (1980), and Allen and Perrault (1980) study indirect speech acts, identifying a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages. Prior discourse conditions, the relationship between the literal meaning and the common ground, and specific lexical, constructional, and intonational cues Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 136–143, c Queen Mary University of London, September 2009. 2009 Association for Computational Linguistics 136 First, we underta"
W09-3920,P92-1009,0,0.852988,"Missing"
W09-3920,P94-1009,0,0.383451,"Missing"
W10-0719,D09-1030,0,0.0337069,"Missing"
W10-0719,W09-1904,0,0.0736523,"that are utilizing crowdsourcing technologies, all of them somewhat novel to the NLP community but with potential for future research in computational linguistics. For each, we also discuss methods for evaluating quality, finding the crowdsourced results to often be indistinguishable from controlled laboratory experiments. Introduction Crowdsourcing’s greatest contribution to language studies might be the ability to generate new kinds of data, especially within experimental paradigms. The speed and cost benefits for annotation are certainly impressive (Snow et al., 2008; CallisonBurch, 2009; Hsueh et al., 2009) but we hope to show that some of the greatest gains are in the very nature of the phenomena that we can now study. For psycholinguistic experiments in particular, we are not so much utilizing ‘artificial artificial’ intelligence as the plain intelligence and linguistic intuitions of each crowdsourced worker – the ‘voices in the crowd’, so to speak. In many experiments we are studying gradient phenomena where there are no right answers. Even when there is binary response we are often interested in the distribution of responses over many speakers rather than specific data points. This different"
W10-0719,meyers-etal-2004-annotating,0,0.0136638,"26 Figure 4: Odds ratio of a Nominal Agent being embedded within a Sentential Agent or non-Agent, relative to random chance. (ρ &lt; 0.001 for all) that competence grammar includes access to probability distributions. Meanwhile, the strong correlations across populations offer encouraging evidence in support of using the latter in psycholinguistic judgment research. 6 Confirming corpus trends Crowdsourcing can also be used to establish the validity of corpus trends found in otherwise skewed data. The experiments in this section were motivated by the NomBank corpus of nominal predicate/arguments (Meyers et al., 2004) where we found that an Agent semantic role was much more likely to be embedded within a sentential Agent. For example, (1) is more likely than (2) to receive the Agent interpretation for the ‘the police’, but both have same potential range of meanings: (1) “The investigation of the police took 3 weeks to complete” (2) “It took 3 weeks to complete the investigation of the police” While the trend is significant (ρ &lt; 0.001), the corpus is not representative speech. First, there are no minimal pairs of sentences in NomBank like (1) and (2) that have the same potential range of meanings. Second, t"
W10-0719,D08-1027,0,0.0650986,"Missing"
W15-4002,W11-0114,0,0.0253397,"ilment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model these phenomena in their full generality on complex linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN archite"
W15-4002,S13-1001,0,0.0608655,"Missing"
W15-4002,W13-3209,0,0.0242407,"x=y x ∩ y = ∅ ∧ x ∪ y 6= D x∩y =∅∧x∪y =D x ∩ y 6= ∅ ∧ x ∪ y = D (else) turtle, reptile reptile, turtle couch, sofa turtle, warthog able, unable animal, non-turtle turtle, pet Table 1: The seven relations of MacCartney and Manning (2009)’s logic are defined abstractly on pairs of sets drawing from the universe D, but can be straightforwardly applied to any pair of natural language words, phrases, or sentences. The relations are defined so as to be mutually exclusive. standard NN layer function (1) and those with the more powerful neural tensor network layer function (2) proposed in Chen et al. (2013). The nonlinearity f (x) = tanh(x) is applied elementwise to the output of either layer function.  (l)  ~x (1) ~yTreeRNN = f (M (r) + ~b ) ~x the tree and into the classifier. For an objective function, we use the negative log likelihood of the correct label with tuned L2 regularization. We initialize parameters uniformly, using the range (−0.05, 0.05) for layer parameters and (−0.01, 0.01) for embeddings, and train the model using stochastic gradient descent (SGD) with learning rates computed using AdaDelta (Zeiler, 2012). The classifier feature vector is fixed at 75 dimensions and the dime"
W15-4002,Q14-1006,0,0.029844,"Missing"
W15-4002,P03-1054,1,0.0236939,"Missing"
W15-4002,J13-2005,0,0.00819476,"edu {∗ Dept. of Linguistics, † NLP Group, ‡ Dept. of Computer Science} Stanford University Stanford, CA 94305, USA Abstract the kind of high-fidelity distributed representations proposed in recent algebraic work on vector space modeling (Coecke et al., 2011; Grefenstette, 2013; Hermann et al., 2013; Rockt¨aschel et al., 2014), and whether any such model can match the performance of grammars based in logical forms in their ability to model core semantic phenomena like quantification, entailment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model t"
W15-4002,E12-1004,0,0.0204642,"Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model these phenomena in their full generality on complex linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN architecture for natural lang"
W15-4002,W09-3714,1,0.792478,"2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference (and some successful implemented models; MacCartney and Manning 2009; Watanabe et al. 2012) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language. In our first three experiments, we test our models’ ability to learn the foundations of natural language inference by training them to reproduce the behavior of the natural logic of MacCartney and Manning (2009) on artificial data. This logic defines seven mutually-exclusive relations of synonymy, entailment, contradiction, and mutual consistency, as sum"
W15-4002,S14-2001,0,0.0219712,"el restarts, suggesting that there is no fundamental obstacle to learning a perfect model for this problem. 6 The SICK textual entailment challenge The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, our experiments so far do not guarantee that it viable model for handling inference over real natural language data. To investigate our models’ ability to handle the noisy labels and the diverse range of linguistic structures seen in typical natural language data, we use the SICK textual entailment challenge corpus (Marelli et al., 2014b). The corpus consists of about 10k natural language sentence pairs, labeled with entailment, contradiction, or neutral. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, and our results nonetheless show that tree-structured NN models can learn to approximate natural logic-style inference in the real world. Adapting to this task requires us to make a few 4 We t"
W15-4002,marelli-etal-2014-sick,0,0.0210269,"el restarts, suggesting that there is no fundamental obstacle to learning a perfect model for this problem. 6 The SICK textual entailment challenge The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, our experiments so far do not guarantee that it viable model for handling inference over real natural language data. To investigate our models’ ability to handle the noisy labels and the diverse range of linguistic structures seen in typical natural language data, we use the SICK textual entailment challenge corpus (Marelli et al., 2014b). The corpus consists of about 10k natural language sentence pairs, labeled with entailment, contradiction, or neutral. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, and our results nonetheless show that tree-structured NN models can learn to approximate natural logic-style inference in the real world. Adapting to this task requires us to make a few 4 We t"
W15-4002,D14-1162,1,0.108463,"Missing"
W15-4002,W14-2409,0,0.126108,"Missing"
W15-4002,D11-1014,1,0.0289584,". In our first set of experiments, we generate artificial data from a logical grammar and use it to evaluate the models’ ability to learn to handle basic relational reasoning, recursive structures, and quantification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. 1 Introduction Tree-structured recursive neural network models (TreeRNNs; Goller and Kuchler 1996; Socher et al. 2011b) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis (Socher et al., 2011b; Irsoy and Cardie, 2014), image description (Socher et al., 2014), and paraphrase detection (Socher et al., 2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c B"
W15-4002,D12-1110,1,0.0397056,"scard training examples with more than 4 logical operators, yielding 60k short training examples, and 21k test examples across all 12 bins. In addition to the two tree models, we also train a summing NN baseline which is largely identical to the TreeRNN, except that instead of using a learned composition function, it simply sums the term vectors in each expression to compose them before passing them to the comparison layer. Unlike the two tree models, this baseline does not use word order, and is as such guaranteed to ignore some information that it would need in order to succeed perfectly. 3 Socher et al. (2012) show that a matrix-vector TreeRNN model somewhat similar to our TreeRNTN can learn boolean logic, a logic where the atomic symbols are simply the values T and F. While learning the operators of that logic is not trivial, the outputs of each operator can be represented accurately by a single bit. In the much more demanding task presented here, the atomic symbols are variables over these values, and 6 the sentence vectors must thus be able to distinguish up to 22 distinct conditions on valuations. Results Fig. 3 shows the relationship between test accuracy and statement size. While the summing"
W15-4002,D13-1170,1,0.00982266,"x linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN architecture for natural language inference which independently computes vector representations for each of two sentences using standard TreeRNN or TreeRNTN (Socher et al., 2013) models, and produces a judgment for the pair using only those representations. This allows us to gauge the abilities of these two models to represent all of the necessary semantic Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models— plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)—can correctly learn to ide"
W15-4002,Q14-1017,1,0.0553835,"antification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. 1 Introduction Tree-structured recursive neural network models (TreeRNNs; Goller and Kuchler 1996; Socher et al. 2011b) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis (Socher et al., 2011b; Irsoy and Cardie, 2014), image description (Socher et al., 2014), and paraphrase detection (Socher et al., 2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference"
W15-4002,P15-1150,1,0.520584,"s a learned full rank third-order tensor T, of dimension N × N × N , modeling multiplicative interactions between the child vectors. The comparison layer uses the same layer function as the composition layers (either an NN layer or an NTN layer) with independently learned parameters and a separate nonlinearity function. Rather than use a tanh nonlinearity here, we found better results with the leaky rectified linear function (Maas et al., 2013): f (x) = max(x, 0) + 0.01 min(x, 0). Other strong tree-structured models have been proposed in past work (Socher et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015), but we believe that these two provide a valuable case study, and that positive results on here are likely to generalize well to stronger models. To run the model forward, we assemble the two tree-structured networks so as to match the structures provided for each phrase, which are either included in the source data or given by a parser. The word vectors are then looked up from the vocabulary embedding matrix V (one of the learned model parameters), and the composition and comparison functions are used to pass information up 3 Reasoning about semantic relations The simplest kinds of deduction"
W15-4002,J82-3002,0,0.724179,"stopher D. Manning∗†‡ sbowman@stanford.edu cgpotts@stanford.edu manning@stanford.edu {∗ Dept. of Linguistics, † NLP Group, ‡ Dept. of Computer Science} Stanford University Stanford, CA 94305, USA Abstract the kind of high-fidelity distributed representations proposed in recent algebraic work on vector space modeling (Coecke et al., 2011; Grefenstette, 2013; Hermann et al., 2013; Rockt¨aschel et al., 2014), and whether any such model can match the performance of grammars based in logical forms in their ability to model core semantic phenomena like quantification, entailment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). Ho"
W15-4002,C12-1171,0,0.0263234,"couraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference (and some successful implemented models; MacCartney and Manning 2009; Watanabe et al. 2012) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language. In our first three experiments, we test our models’ ability to learn the foundations of natural language inference by training them to reproduce the behavior of the natural logic of MacCartney and Manning (2009) on artificial data. This logic defines seven mutually-exclusive relations of synonymy, entailment, contradiction, and mutual consistency, as summarized in Table 1, and"
W15-4002,P11-1060,0,\N,Missing
W19-0109,D16-1125,0,0.418675,"the Society for Computation in Linguistics (SCiL) 2019, pages 81-90. New York City, New York, January 3-6, 2019 RSA and its extensions have been applied to a wide range of pragmatic phenomena, including scalar implicatures (Frank et al., 2016; Potts et al., 2016), manner implicatures (Bergen et al., 2016), hyperbole (Kao et al., 2014), metaphor, and politeness (Yoon et al., 2016). In addition, RSA can be cast as a machine learning model, thereby allowing us to study pragmatic reasoning in large corpora and complex environments (Vogel et al., 2013; Monroe and Potts, 2015; Monroe et al., 2017; Andreas and Klein, 2016). Standard RSA models are global in the sense that the pragmatic reasoning is defined over complete utterances. Speakers are conditional distributions of the form P (u|w), while listeners are of the form P (w|u), for an utterance u and state w. We first present this global formulation (section 2.1), and then we show how to reformulate it so that utterances are sequences of linguistic units u = [u1 , . . . , un ] and the core RSA reasoning is applied to each step ui given [u1 , . . . ui 1 ]. Figure 1 presents a running illustrative example. We imagine there are three referents, a red dress (R1)"
W19-0109,N18-2070,1,0.871652,"ng, in that the calculations are done in terms of complete utterances. However, there is substantial evidence that pragmatic processing is incremental: listeners venture pragmatic inferences over the time-course of the utterances they hear, which influences the choices that speakers make. To address this, we develop an IR model that is incremental in the sense that pragmatic reasoning takes place word-by-word (though the process could be defined in terms of different linguistic units, like morphemes or phrases). A variant of this model was applied successfully to pragmatic image captioning by Cohn-Gordon et al. (2018); here we concentrate on its qualitative behavior and linguistic predictions. We present computational experiments which demonstrate that incremental and global pragmatics make different predictions, and we show that a speaker that incrementally makes pragmatically informative choices arrives at an utterance which is globally informative. We then argue that an incremental model can account for two empirical observations out of reach of a global model: (i) the asymmetry between adjective–noun and noun– adjective languages in over-informative referential behavior (Rubio-Fern´andez, 2016), and (i"
W19-0109,W06-1420,0,0.0837444,"Missing"
W19-0109,Q17-1023,1,0.930692,"3). 81 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 81-90. New York City, New York, January 3-6, 2019 RSA and its extensions have been applied to a wide range of pragmatic phenomena, including scalar implicatures (Frank et al., 2016; Potts et al., 2016), manner implicatures (Bergen et al., 2016), hyperbole (Kao et al., 2014), metaphor, and politeness (Yoon et al., 2016). In addition, RSA can be cast as a machine learning model, thereby allowing us to study pragmatic reasoning in large corpora and complex environments (Vogel et al., 2013; Monroe and Potts, 2015; Monroe et al., 2017; Andreas and Klein, 2016). Standard RSA models are global in the sense that the pragmatic reasoning is defined over complete utterances. Speakers are conditional distributions of the form P (u|w), while listeners are of the form P (w|u), for an utterance u and state w. We first present this global formulation (section 2.1), and then we show how to reformulate it so that utterances are sequences of linguistic units u = [u1 , . . . , un ] and the core RSA reasoning is applied to each step ui given [u1 , . . . ui 1 ]. Figure 1 presents a running illustrative example. We imagine there are three r"
W19-0109,W09-0629,0,0.0270177,"r two empirical observations out of reach of a global model: (i) the asymmetry between adjective–noun and noun– adjective languages in over-informative referential behavior (Rubio-Fern´andez, 2016), and (ii) the anticipatory implicatures arising from contrastive modifiers (Sedivy, 2007). The first of these observations requires a model of language production, while the second requires a model of language interpretation, and as such these case studies serve to demonstrate both aspects of incremental pragmatics. Finally, we apply the model to the TUNA corpus for referring expression generation (Gatt et al., 2009), showing that it makes more realistic predictions about attributive modifiers than does its global counterpart. 2 Iterated Response Models We construct our model within the Rational Speech Acts (RSA) paradigm (Frank and Goodman, 2012; Goodman and Stuhlm¨uller, 2013). 81 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 81-90. New York City, New York, January 3-6, 2019 RSA and its extensions have been applied to a wide range of pragmatic phenomena, including scalar implicatures (Frank et al., 2016; Potts et al., 2016), manner implicatures (Bergen et al., 2016), hyper"
W19-0109,N13-1127,1,0.783714,"Goodman, 2012; Goodman and Stuhlm¨uller, 2013). 81 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 81-90. New York City, New York, January 3-6, 2019 RSA and its extensions have been applied to a wide range of pragmatic phenomena, including scalar implicatures (Frank et al., 2016; Potts et al., 2016), manner implicatures (Bergen et al., 2016), hyperbole (Kao et al., 2014), metaphor, and politeness (Yoon et al., 2016). In addition, RSA can be cast as a machine learning model, thereby allowing us to study pragmatic reasoning in large corpora and complex environments (Vogel et al., 2013; Monroe and Potts, 2015; Monroe et al., 2017; Andreas and Klein, 2016). Standard RSA models are global in the sense that the pragmatic reasoning is defined over complete utterances. Speakers are conditional distributions of the form P (u|w), while listeners are of the form P (w|u), for an utterance u and state w. We first present this global formulation (section 2.1), and then we show how to reformulate it so that utterances are sequences of linguistic units u = [u1 , . . . , un ] and the core RSA reasoning is applied to each step ui given [u1 , . . . ui 1 ]. Figure 1 presents a running illus"
W19-1901,N18-1202,0,0.0171569,"hat pick out diverse healthcare concepts: the Chemical– Disease Relation dataset (CDR; Wei et al. 2015); the Penn Adverse Drug Reaction Twitter dataset (ADR; Nikfarjam et al. 2015); a new disease diagnosis dataset; a new prescription reasons dataset that involves identifying complex R EASON spans for drug–prescription actions; and a new dataset of 10K drug–disease treatment descriptions, which we release with this paper. 2 We explore two ways of initializing the dense representations: random initialization according to the method of Glorot and Bengio (2010) and the ELMo embeddings released by Peters et al. (2018). The ELMo embeddings were trained on the 1 billion word benchmark of Chelba et al. (2013) – general newswire text not specialized to the healthcare space. What is special about ELMo embeddings, as compared to more standard word representation learning, is that they are obtained from the parameters of a full language model, so that each word’s representation varies by, and is sensitive to, its linguistic context; see also McCann et al. 2017; Radford et al. 2018. The nature of the hand-built feature representations varies by task, so we leave most of the details to section 3. All the models fea"
W19-1901,N18-1080,0,0.0305154,"33 biomedical datasets than both available NER tools and entity-agnostic CRF methods, though they do not incorporate hand-built features. There are also competitions related to labeling tasks in the context of clinical text. The i2b2 Challenge (Sun et al., 2013) includes event detection as one of the task tracks, which is basically a labeling task. The best results on this task came from a team using a simple CRF. The Biocreative V Chemical–Disease relation (CDR) competition (Wei et al., 2015) released a widely used dataset for researchers to evaluate their NER tools for biomedical text, and Verga et al. (2018) report state-of-the-art results for a self-attention encoder, using a dataset that extends CDR. fectiveness of the ELMo-LSTM, we always have higher average potential scores from those features. This is reflected in the mean scores at left and in the comparatively large amount of white (high scores) in the panels. However, the handbuilt features always make substantial contributions, especially in Diagnosis Detection, Prescription Reasons, and CDR. We note also that, where the performance of the two base models is very similar (table 2), the potential scores in the combined model are also more"
