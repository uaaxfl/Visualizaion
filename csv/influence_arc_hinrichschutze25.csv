2009.mtsummit-posters.11,J93-2003,0,0.0297587,"tter than symmetrized IBM Model 1 in terms of alignment quality and that it supports trading off precision against recall. 1 Introduction Word alignment, the task of establishing correspondences between words in a bitext (i.e., a sentencealigned parallel corpus), is an important problem with applications in statistical machine translation, the automatic generation of bilingual dictionaries and cross-language information retrieval. According to Och and Ney (2003), there are two general approaches to computing word alignments: statistical and heuristic methods. In statistical alignment methods (Brown et al., 1993), P r (T |S) is written in terms of the conditional probability P r (T, a|S) as: X P r (T |S) = P r (T, a|S) (1) a Here, the alignment a describes a mapping from the positions of the words in the source text S to the positions in the target text T . The heuristic methods are considerably simpler. Generally speaking, they try to align each word according to the associative information of sourcetarget word pairs. This information can be provided using different methods. As our baseline method, we define a simple heuristic model and call it maximum linking. For a given target word tj , maximum li"
2009.mtsummit-posters.11,D07-1006,1,0.87953,"ng and as accurate or more accurate than IBM Model 1, the current method of choice for initializing training of statistical machine translation models. • 2D-Linking can align m-to-n units, unlike the IBM models which can only align 1-to-n units. • 2D-Linking can easily trade off precision vs. recall in word alignment. This is important for many applications of word alignment, in particular for cross-language information retrieval (which in many scenarios requires high precision of word alignments, (Kraaij, 2004)) and machine translation (which usually requires high recall in word alignments, (Fraser and Marcu, 2007b)). This paper is organized as follows. Related work is discussed in Section 2. Section 3 discusses the three association scores we investigate in this paper: the Dice coefficient, expected mutual information and pointwise mutual information. In Section 4, we describe the 2D-Linking algorithm. Sections 5 and 6 present our evaluation results and conclusions. 2 Related Work Statistical alignment models depend on a set of unknown parameters that must be learned from training data. IBM Model 1 is a particularly simple instance of the framework presented in Equation 1. This model assumes a uniform"
2009.mtsummit-posters.11,J07-3002,1,0.908704,"ng and as accurate or more accurate than IBM Model 1, the current method of choice for initializing training of statistical machine translation models. • 2D-Linking can align m-to-n units, unlike the IBM models which can only align 1-to-n units. • 2D-Linking can easily trade off precision vs. recall in word alignment. This is important for many applications of word alignment, in particular for cross-language information retrieval (which in many scenarios requires high precision of word alignments, (Kraaij, 2004)) and machine translation (which usually requires high recall in word alignments, (Fraser and Marcu, 2007b)). This paper is organized as follows. Related work is discussed in Section 2. Section 3 discusses the three association scores we investigate in this paper: the Dice coefficient, expected mutual information and pointwise mutual information. In Section 4, we describe the 2D-Linking algorithm. Sections 5 and 6 present our evaluation results and conclusions. 2 Related Work Statistical alignment models depend on a set of unknown parameters that must be learned from training data. IBM Model 1 is a particularly simple instance of the framework presented in Equation 1. This model assumes a uniform"
2009.mtsummit-posters.11,P04-1064,0,0.018845,"not have m-to-n links. However it is possible to improve the model. Moore (2004) introduced an improvement of IBM Model 1 that tries to control the trade-off between precision and recall by adding additional null words to the source sentence. Like Model 1, Moore’s model requires several passes through the data (whereas 2D-Linking only needs two) and cannot model m-to-n links. There are also some more advanced unsupervised models such as the HMM word alignment model (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), the Joint (phrase) model (Marcu and Wong, 2002) and Matrix Factorisation (Goutte et al., 2004). But they are much more expensive to calculate. In addition, models like HMM and Model 4 suffer from 1-to-n structure, while the Joint model only allows units consisting of consecutive words. LEAF (Fraser and Marcu, 2007a) directly models m-to-n structure, but is expensive to compute. Alignment by agreement (Liang et al., 2006) is another approach, in which two asymmetric models are trained jointly to maximize a combination of the likelihood of the data. This approach is more expensive than training the two assymetric models independently. It is also biased towards high precision alignment st"
2009.mtsummit-posters.11,N03-1017,0,0.0067491,"ffectively handle indirect associations. But, importantly, it can find m-to-n links. 2D-Linking is most similar to the approach by Tiedemann (2003). He defines the word alignment clue as a score which indicates an association between source-target words by considering various features derived from co-occurrence statistics. But the search algorithm is more similar to competitive linking though it handles 1-to-n and m-to-1 alignments as well. 2D-Linking could be extended to use Tiedemann’s scores in a straightforward manner since it can operate on any association measure. Och and Ney (2003) and Koehn et al. (2003) defined a heuristic procedure that produces an m-ton alignment. Start the procedure by generating the predicted 1-to-n alignment in the direction source to target. In this alignment one source word aligns to zero or more target words. Call the resulting alignment A1. Generate the predicted m-to-1 alignment in the direction target to source. In this alignment one target word aligns to zero or more source words. Call the resulting alignment A2. Combine A1 and A2 into an m-to-n alignment using a symmetrization heuristic. We consider the following three symmetrization heuristics in this paper: (1"
2009.mtsummit-posters.11,N06-1014,0,0.024293,"only needs two) and cannot model m-to-n links. There are also some more advanced unsupervised models such as the HMM word alignment model (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), the Joint (phrase) model (Marcu and Wong, 2002) and Matrix Factorisation (Goutte et al., 2004). But they are much more expensive to calculate. In addition, models like HMM and Model 4 suffer from 1-to-n structure, while the Joint model only allows units consisting of consecutive words. LEAF (Fraser and Marcu, 2007a) directly models m-to-n structure, but is expensive to compute. Alignment by agreement (Liang et al., 2006) is another approach, in which two asymmetric models are trained jointly to maximize a combination of the likelihood of the data. This approach is more expensive than training the two assymetric models independently. It is also biased towards high precision alignment structure as the posterior of both 1-to-n models must be high for a link to be selected. One of the best known heuristic models is competitive linking (Melamed, 1997). In competitive linking at first the word pair with the highest association score is aligned (this is similar to maximum linking, Eq. 2). Then the corresponding row"
2009.mtsummit-posters.11,W02-1018,0,0.0181157,"on of the other IBM models. Structurally, it cannot have m-to-n links. However it is possible to improve the model. Moore (2004) introduced an improvement of IBM Model 1 that tries to control the trade-off between precision and recall by adding additional null words to the source sentence. Like Model 1, Moore’s model requires several passes through the data (whereas 2D-Linking only needs two) and cannot model m-to-n links. There are also some more advanced unsupervised models such as the HMM word alignment model (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), the Joint (phrase) model (Marcu and Wong, 2002) and Matrix Factorisation (Goutte et al., 2004). But they are much more expensive to calculate. In addition, models like HMM and Model 4 suffer from 1-to-n structure, while the Joint model only allows units consisting of consecutive words. LEAF (Fraser and Marcu, 2007a) directly models m-to-n structure, but is expensive to compute. Alignment by agreement (Liang et al., 2006) is another approach, in which two asymmetric models are trained jointly to maximize a combination of the likelihood of the data. This approach is more expensive than training the two assymetric models independently. It is"
2009.mtsummit-posters.11,P97-1063,0,0.459608,"ows units consisting of consecutive words. LEAF (Fraser and Marcu, 2007a) directly models m-to-n structure, but is expensive to compute. Alignment by agreement (Liang et al., 2006) is another approach, in which two asymmetric models are trained jointly to maximize a combination of the likelihood of the data. This approach is more expensive than training the two assymetric models independently. It is also biased towards high precision alignment structure as the posterior of both 1-to-n models must be high for a link to be selected. One of the best known heuristic models is competitive linking (Melamed, 1997). In competitive linking at first the word pair with the highest association score is aligned (this is similar to maximum linking, Eq. 2). Then the corresponding row and column are deleted from the alignment matrix. This process is iterated until either all rows have been deleted or all columns have been deleted. The advantage of this method is its ability to filter out most indirect associations – words that have high association scores but are not translations of each other. The main problem with competitive linking is its inability to produce m-to-n links. Like competitive linking, we will"
2009.mtsummit-posters.11,P06-1065,0,0.0379284,"Missing"
2009.mtsummit-posters.11,J03-1002,0,0.0462934,"quires only two passes over the data and less memory than other methods. We show that 2D-Linking is superior to competitive linking and as good as or better than symmetrized IBM Model 1 in terms of alignment quality and that it supports trading off precision against recall. 1 Introduction Word alignment, the task of establishing correspondences between words in a bitext (i.e., a sentencealigned parallel corpus), is an important problem with applications in statistical machine translation, the automatic generation of bilingual dictionaries and cross-language information retrieval. According to Och and Ney (2003), there are two general approaches to computing word alignments: statistical and heuristic methods. In statistical alignment methods (Brown et al., 1993), P r (T |S) is written in terms of the conditional probability P r (T, a|S) as: X P r (T |S) = P r (T, a|S) (1) a Here, the alignment a describes a mapping from the positions of the words in the source text S to the positions in the target text T . The heuristic methods are considerably simpler. Generally speaking, they try to align each word according to the associative information of sourcetarget word pairs. This information can be provided"
2009.mtsummit-posters.11,E03-1026,0,0.268037,"column are deleted from the alignment matrix. This process is iterated until either all rows have been deleted or all columns have been deleted. The advantage of this method is its ability to filter out most indirect associations – words that have high association scores but are not translations of each other. The main problem with competitive linking is its inability to produce m-to-n links. Like competitive linking, we will show that 2D-Linking is also able to effectively handle indirect associations. But, importantly, it can find m-to-n links. 2D-Linking is most similar to the approach by Tiedemann (2003). He defines the word alignment clue as a score which indicates an association between source-target words by considering various features derived from co-occurrence statistics. But the search algorithm is more similar to competitive linking though it handles 1-to-n and m-to-1 alignments as well. 2D-Linking could be extended to use Tiedemann’s scores in a straightforward manner since it can operate on any association measure. Och and Ney (2003) and Koehn et al. (2003) defined a heuristic procedure that produces an m-ton alignment. Start the procedure by generating the predicted 1-to-n alignmen"
2009.mtsummit-posters.11,C96-2141,0,0.189041,"ore details see (Brown et al., 1993). IBM Model 1 is mostly used for initialization of the other IBM models. Structurally, it cannot have m-to-n links. However it is possible to improve the model. Moore (2004) introduced an improvement of IBM Model 1 that tries to control the trade-off between precision and recall by adding additional null words to the source sentence. Like Model 1, Moore’s model requires several passes through the data (whereas 2D-Linking only needs two) and cannot model m-to-n links. There are also some more advanced unsupervised models such as the HMM word alignment model (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), the Joint (phrase) model (Marcu and Wong, 2002) and Matrix Factorisation (Goutte et al., 2004). But they are much more expensive to calculate. In addition, models like HMM and Model 4 suffer from 1-to-n structure, while the Joint model only allows units consisting of consecutive words. LEAF (Fraser and Marcu, 2007a) directly models m-to-n structure, but is expensive to compute. Alignment by agreement (Liang et al., 2006) is another approach, in which two asymmetric models are trained jointly to maximize a combination of the likelihood of the data. This appro"
2020.acl-main.106,D17-1074,0,0.46324,"into perspective, we compare against four baselines, which we present in decreasing order of sophistication. 4.2 Baseline 1: Character-based Model (CM) We model P (W |d, Cr ) as P (W |fs , fa , Cr ) using a character-based model (CM), i.e., as opposed to the DGA, fs and fa are modeled directly by means of their orthographic form. This provides the CM with phonological information, a central predictor of MWF (see Section 2.2). CM might also learn semantic information during training, but it is not directly provided with it. Character-based models show competitive results on derivational tasks (Cotterell et al., 2017; Vylomova et al., 2017; Deutsch et al., 2018), a good reason to test their performance on MWF prediction. We use two one-layer bidirectional LSTMs to encode the stem and affix group into a vector o by concatenating the last hidden states from both LSTM directions ~hs , h~s , ~ha , and h~a , o = [~hs ⊕ h~s ⊕ ~ha ⊕ h~a ], (7) 4 Besides the simple dot-product decoder, we also implemented a bilinear decoder with h(zs , za ) = σ(z&gt; s Qza ), where Q is a trainable weight matrix. However, the model performed significantly worse. Baseline 2: Neural Classifier (NC) We model P (W |d, Cr ) as P (W |ms ,"
2020.acl-main.106,C18-1135,0,0.117256,"Missing"
2020.acl-main.106,P18-1180,0,0.285998,"ines, which we present in decreasing order of sophistication. 4.2 Baseline 1: Character-based Model (CM) We model P (W |d, Cr ) as P (W |fs , fa , Cr ) using a character-based model (CM), i.e., as opposed to the DGA, fs and fa are modeled directly by means of their orthographic form. This provides the CM with phonological information, a central predictor of MWF (see Section 2.2). CM might also learn semantic information during training, but it is not directly provided with it. Character-based models show competitive results on derivational tasks (Cotterell et al., 2017; Vylomova et al., 2017; Deutsch et al., 2018), a good reason to test their performance on MWF prediction. We use two one-layer bidirectional LSTMs to encode the stem and affix group into a vector o by concatenating the last hidden states from both LSTM directions ~hs , h~s , ~ha , and h~a , o = [~hs ⊕ h~s ⊕ ~ha ⊕ h~a ], (7) 4 Besides the simple dot-product decoder, we also implemented a bilinear decoder with h(zs , za ) = σ(z&gt; s Qza ), where Q is a trainable weight matrix. However, the model performed significantly worse. Baseline 2: Neural Classifier (NC) We model P (W |d, Cr ) as P (W |ms , ma , Cr ) using a neural classifier (NC) whos"
2020.acl-main.106,P11-1038,0,0.0292053,"Missing"
2020.acl-main.106,2020.acl-main.649,1,0.800191,"Missing"
2020.acl-main.106,P13-1149,0,0.180161,"of topical relatedness on embedding similarities. Related Work Much recent computational research on derivational morphology in NLP has focused on two related problems: predicting the meaning of a derivative given its form, and predicting the form of a derivative given its meaning. The first group of studies models the meaning of derivatives as a function of their morphological structure by training embeddings directly on text segmented into morphemes (Luong et al., 2013; Qiu et al., 2014) or by inferring morpheme embeddings from whole-word vector spaces, e.g., using the vector offset method (Lazaridou et al., 2013; Pad´o et al., 2016). Formally, given a derived form fd , this line of research tries to find the meaning md that maximizes P (md |fd ). The second group of studies models the form 8 One SR standing out in Figure 6 is lol, a multiplayer online video game, in which many common stems such as fame and range have highly idiosyncratic meanings. Conclusion We have introduced a derivational graph autoencoder (DGA) that combines syntactic and semantic information with associative information from the mental lexicon to predict morphological well-formedness (MWF), a task that has not been addressed bef"
2020.acl-main.106,L18-1413,0,0.0200619,"ften serves communicative needs (Bauer, 2019). As a result, the degree of MWF of a non-existing derivative is influenced by a multitude of factors and judged to be hard to predict (Bauer, 2001). In NLP, the lack of reliable ways to estimate the MWF of derivatives poses a bottleneck for generative models, particularly in languages exhibiting a rich derivational morphology; e.g., while inflected forms can be translated by generating morphologically corresponding forms in the target language (Minkov et al., 2007), generating derivatives is still a major challenge for machine translation systems (Sreelekha and Bhattacharyya, 2018). Similar problems exist in the area of automatic language generation (Gatt and Krahmer, 2018). This study takes a first step towards computationally modeling the MWF of English derivatives. We present a derivational graph auto-encoder (DGA) that combines semantic and syntactic information with associative information from the mental lexicon, achieving very good results on MWF prediction and performing on par with a character-based LSTM at a fraction of the number of trainable parameters. The model produces embeddings that capture information about the compatibility of affixes and stems in der"
2020.acl-main.106,W17-0239,0,0.0214265,"f the individual SRs with a window size of 10. These can be seen as GloVe variants of traditional morpheme embeddings as proposed, e.g., by Qiu et al. (2014), with the sole difference that we use affix groups instead of individual affixes. For training the embeddings, derivatives are segmented into prefix group, stem, and suffix group. In the case of both prefix and suffix groups, we add prefix and suffix group embeddings. Since the window size impacts the information represented by the embeddings, with larger windows tending to capture topical and smaller windows morphosyntactic information (Lison and Kutuzov, 2017), we also train the DGA with 200dimensional embeddings consisting of concatenated 100-dimensional embeddings trained with window sizes of 10 and 1, respectively (DGA+).7 Since DGA already receives associative informa6 The number of epochs until convergence lies within the typical range of values for graph convolutional networks. 7 We experimented with using vectors trained on isolated pairs of stems and affix groups instead of window-1 vectors trained on the full text, but the performance was comparable. We also implemented the DGA using only window-1 vectors (without concatenating them with w"
2020.acl-main.106,W13-3512,0,0.0504408,"r-coded values of ∆(S (i) , S (j) ) and ∆(A(i) , A(j) ) for all pairs of SRs, respectively. The block-diagonal structure highlights the impact of topical relatedness on embedding similarities. Related Work Much recent computational research on derivational morphology in NLP has focused on two related problems: predicting the meaning of a derivative given its form, and predicting the form of a derivative given its meaning. The first group of studies models the meaning of derivatives as a function of their morphological structure by training embeddings directly on text segmented into morphemes (Luong et al., 2013; Qiu et al., 2014) or by inferring morpheme embeddings from whole-word vector spaces, e.g., using the vector offset method (Lazaridou et al., 2013; Pad´o et al., 2016). Formally, given a derived form fd , this line of research tries to find the meaning md that maximizes P (md |fd ). The second group of studies models the form 8 One SR standing out in Figure 6 is lol, a multiplayer online video game, in which many common stems such as fame and range have highly idiosyncratic meanings. Conclusion We have introduced a derivational graph autoencoder (DGA) that combines syntactic and semantic info"
2020.acl-main.106,P07-1017,0,0.0191581,"t, 2012). This is particularly true in the case of derivational morphology, which is not obligatory and often serves communicative needs (Bauer, 2019). As a result, the degree of MWF of a non-existing derivative is influenced by a multitude of factors and judged to be hard to predict (Bauer, 2001). In NLP, the lack of reliable ways to estimate the MWF of derivatives poses a bottleneck for generative models, particularly in languages exhibiting a rich derivational morphology; e.g., while inflected forms can be translated by generating morphologically corresponding forms in the target language (Minkov et al., 2007), generating derivatives is still a major challenge for machine translation systems (Sreelekha and Bhattacharyya, 2018). Similar problems exist in the area of automatic language generation (Gatt and Krahmer, 2018). This study takes a first step towards computationally modeling the MWF of English derivatives. We present a derivational graph auto-encoder (DGA) that combines semantic and syntactic information with associative information from the mental lexicon, achieving very good results on MWF prediction and performing on par with a character-based LSTM at a fraction of the number of trainable"
2020.acl-main.106,C16-1122,0,0.524555,"Missing"
2020.acl-main.106,D14-1162,0,0.0887801,"Missing"
2020.acl-main.106,W18-5814,1,0.248694,"trict the ways in which affixes can be attached to stems and other affixes (Hay and Plag, 2004); e.g., the suffix $less can be combined with $ness (atom$less$ness) but not with $ity (atom$less$ity). The semantic and formal complexity of derivation makes predicting the MWF of derivatives more challenging than the MWF of inflectional forms (Anshen and Aronoff, 1999; Bauer, 2019). Here, we model the MWF of derivatives as the likelihood of their existence in the mental lexicon. 2.2 Derivatives in the Mental Lexicon How likely a derivative is to exist is influenced by various factors (Bauer, 2001; Pierrehumbert and Granell, 2018). In this study, we concentrate on the role of the structure of the mental lexicon. The mental lexicon can be thought of as a set of associations between meaning m and form f , i.e., words, organized in a network, where links correspond to shared semantic and phonological properties (see Pierrehumbert (2012) for a review). Since we base our study on textual data, we will treat the form of words orthographically rather than phonologically. We will refer to the type of information conveyed by the cognitive structure of the mental lexicon as associative information. Sets of words with similar sem"
2020.acl-main.106,C14-1015,0,0.164546,"ross entropy as loss function. Hyperparameter tuning is performed on the validation set. We train the DGA for 600 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 0.01.6 We use L = 2 hidden layers in the DGA with a dimension of 100. For regularization, we apply dropout of 0.1 after the input layer and 0.7 after the hidden layers. For xs and xa , we use 100-dimensional GloVe embeddings (Pennington et al., 2014) trained on the segmented text of the individual SRs with a window size of 10. These can be seen as GloVe variants of traditional morpheme embeddings as proposed, e.g., by Qiu et al. (2014), with the sole difference that we use affix groups instead of individual affixes. For training the embeddings, derivatives are segmented into prefix group, stem, and suffix group. In the case of both prefix and suffix groups, we add prefix and suffix group embeddings. Since the window size impacts the information represented by the embeddings, with larger windows tending to capture topical and smaller windows morphosyntactic information (Lison and Kutuzov, 2017), we also train the DGA with 200dimensional embeddings consisting of concatenated 100-dimensional embeddings trained with window size"
2020.acl-main.106,E17-2019,0,0.495004,"pare against four baselines, which we present in decreasing order of sophistication. 4.2 Baseline 1: Character-based Model (CM) We model P (W |d, Cr ) as P (W |fs , fa , Cr ) using a character-based model (CM), i.e., as opposed to the DGA, fs and fa are modeled directly by means of their orthographic form. This provides the CM with phonological information, a central predictor of MWF (see Section 2.2). CM might also learn semantic information during training, but it is not directly provided with it. Character-based models show competitive results on derivational tasks (Cotterell et al., 2017; Vylomova et al., 2017; Deutsch et al., 2018), a good reason to test their performance on MWF prediction. We use two one-layer bidirectional LSTMs to encode the stem and affix group into a vector o by concatenating the last hidden states from both LSTM directions ~hs , h~s , ~ha , and h~a , o = [~hs ⊕ h~s ⊕ ~ha ⊕ h~a ], (7) 4 Besides the simple dot-product decoder, we also implemented a bilinear decoder with h(zs , za ) = σ(z&gt; s Qza ), where Q is a trainable weight matrix. However, the model performed significantly worse. Baseline 2: Neural Classifier (NC) We model P (W |d, Cr ) as P (W |ms , ma , Cr ) using a neur"
2020.acl-main.368,D19-1539,0,0.0163001,"n the area of contextualized representations, many architectures employ subword segmentation methods (e.g. Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019b). Others use BERT riding a un ##ic ##y ##cle is hard BERT B ERTRAM riding a unicycle is hard Figure 1: Top: Standard use of BERT. Bottom: Our proposal; first B ERTRAM learns an embedding for “unicycle” that replaces the WordPiece sequence. BERT is then run on this improved input representation. convolutional neural networks to directly access character-level information (Kim et al., 2016; Peters et al., 2018; Baevski et al., 2019). Complementary to surface form, another useful source of information for understanding rare words are the contexts in which they occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018). Schick and Sch¨utze (2019a,b) show that combining form and context leads to significantly better results than using just one of the two. While all of these methods are bag-of-words models, Liu et al. (2019a) recently proposed an architecture based on context2vec (Melamud et al., 2016). However, in contrast to our work, they (i) do not incorporate surface-form information and (ii) do not"
2020.acl-main.368,Q17-1010,0,0.0708476,"contexts, it uses a simple bagof-words model, making poor use of the available information. • It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. Introduction As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations have been proposed. These approaches exploit either the contexts in which rare words occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), their surfaceform (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly 1 Our implementation of B ERTRAM is publicly available at https://github.com/timoschick/bertram. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leve"
2020.acl-main.368,N19-1423,0,0.178359,"hniques for improving their representations have been proposed. These approaches exploit either the contexts in which rare words occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), their surfaceform (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly 1 Our implementation of B ERTRAM is publicly available at https://github.com/timoschick/bertram. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bagof-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To"
2020.acl-main.368,P18-2006,0,0.0335128,"n context2vec (Melamud et al., 2016). However, in contrast to our work, they (i) do not incorporate surface-form information and (ii) do not directly access the hidden states of context2vec, but instead simply use its output distribution. Several datasets focus on rare words, e.g., Stanford Rare Word (Luong et al., 2013), Definitional Nonce (Herbelot and Baroni, 2017), and Contextual Rare Word (Khodak et al., 2018). However, unlike our rarified datasets, they are only suitable for evaluating uncontextualized word representations. Rarification is related to adversarial example generation (e.g. Ebrahimi et al., 2018), which manipulates the input to change a model’s prediction. We use a similar mechanism to determine which words in a given sentence are most important and replace them with rare synonyms. 3 3.1 Model Form-Context Model We first review the basis for our new model, the form-context model (FCM) (Schick and Sch¨utze, 2019b). Given a set of d-dimensional high-quality embeddings for frequent words, FCM induces embeddings for rare words that are appropriate for 3997 the given embedding space. This is done as follows: Given a word w and a context C in which form ∈ Rd it occurs, a surface-form embedd"
2020.acl-main.368,D19-6104,0,0.0248599,"Missing"
2020.acl-main.368,D17-1030,0,0.345553,"nd medium frequency words on both a rare word probing task and three downstream tasks.1 1 • For processing contexts, it uses a simple bagof-words model, making poor use of the available information. • It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. Introduction As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations have been proposed. These approaches exploit either the contexts in which rare words occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), their surfaceform (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly 1 Our implementation of B ERTRAM is publicly available at https://github.com/timoschick/bertram. These limit"
2020.acl-main.368,P18-1002,0,0.354413,"n both a rare word probing task and three downstream tasks.1 1 • For processing contexts, it uses a simple bagof-words model, making poor use of the available information. • It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. Introduction As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations have been proposed. These approaches exploit either the contexts in which rare words occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), their surfaceform (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly 1 Our implementation of B ERTRAM is publicly available at https://github.com/timoschick/bertram. These limitations apply not only"
2020.acl-main.368,W13-3512,0,0.564424,"1 • For processing contexts, it uses a simple bagof-words model, making poor use of the available information. • It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. Introduction As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations have been proposed. These approaches exploit either the contexts in which rare words occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), their surfaceform (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly 1 Our implementation of B ERTRAM is publicly available at https://github.com/timoschick/bertram. These limitations apply not only to AM, but to all previous work on obtaining representatio"
2020.acl-main.368,K16-1006,0,0.0332872,"l neural networks to directly access character-level information (Kim et al., 2016; Peters et al., 2018; Baevski et al., 2019). Complementary to surface form, another useful source of information for understanding rare words are the contexts in which they occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018). Schick and Sch¨utze (2019a,b) show that combining form and context leads to significantly better results than using just one of the two. While all of these methods are bag-of-words models, Liu et al. (2019a) recently proposed an architecture based on context2vec (Melamud et al., 2016). However, in contrast to our work, they (i) do not incorporate surface-form information and (ii) do not directly access the hidden states of context2vec, but instead simply use its output distribution. Several datasets focus on rare words, e.g., Stanford Rare Word (Luong et al., 2013), Definitional Nonce (Herbelot and Baroni, 2017), and Contextual Rare Word (Khodak et al., 2018). However, unlike our rarified datasets, they are only suitable for evaluating uncontextualized word representations. Rarification is related to adversarial example generation (e.g. Ebrahimi et al., 2018), which manipu"
2020.acl-main.368,N18-1202,0,0.326891,"wn to struggle with rare words, several techniques for improving their representations have been proposed. These approaches exploit either the contexts in which rare words occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), their surfaceform (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly 1 Our implementation of B ERTRAM is publicly available at https://github.com/timoschick/bertram. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bagof-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations fo"
2020.acl-main.368,N19-1326,0,0.0183095,"te this hypothesis, we perform another rarification of MNLI that differs from the 4003 Accuracy improvement BERT+B SL BERT+B SL+ID RoBERTa+B SL RoBERTa+B SL+ID 10 8 6 4 2 0 [0,125) [125,250) [250,500) [500,∞) Word counts Figure 4: Improvements for BERT (base) and RoBERTa (large) when adding B ERTRAM - SLASH (+B SL) or B ERTRAM - SLASH + INDOMAIN (+B SL+ID) on MNLI-1000 previous rarification in two respects. First, we increase the threshold for a word to count as rare from 100 to 1000. Second, as this means that we have more WordNet synonyms available, we do not use the misspelling dictionary (Piktus et al., 2019) for substitution. We refer to the resulting datasets for BERTbase and RoBERTalarge as MNLI-1000. Figure 4 shows results on MNLI-1000 for various rare word frequency ranges. For each value [c0 , c1 ) on the x-axis, the y-axis shows improvement in accuracy compared to standalone BERT or RoBERTa when only dataset entries are considered for which each rarified word occurs between c0 (inclusively) and c1 (exclusively) times in WWC+BooksCorpus. We see that for words with frequency less than 125, the improvement in accuracy remains similar even without using misspellings as another source of substit"
2020.acl-main.368,S19-1007,0,0.65542,"obing task and three downstream tasks.1 1 • For processing contexts, it uses a simple bagof-words model, making poor use of the available information. • It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. Introduction As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations have been proposed. These approaches exploit either the contexts in which rare words occur (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), their surfaceform (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly 1 Our implementation of B ERTRAM is publicly available at https://github.com/timoschick/bertram. These limitations apply not only to AM, but to all"
2020.acl-main.368,D17-1010,0,0.0587667,"Missing"
2020.acl-main.368,W18-1209,0,0.0375481,"Missing"
2020.acl-main.368,N19-1048,1,0.840035,"Missing"
2020.acl-main.368,P16-1162,0,0.13501,"Missing"
2020.acl-main.368,N18-1101,0,0.164041,"2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). B ERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: • We introduce B ERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. • We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. • We show t"
2020.acl-main.628,D14-1162,0,0.0880803,"Missing"
2020.acl-main.649,N19-1210,0,0.0245582,"Missing"
2020.acl-main.649,C00-1027,0,0.267801,"de at least one instance of w, and T˜(fw ) is the expected 7276 number of threads under a Poisson model. T˜(fw ) can again be calculated as T˜(fw ) = NT X T˜j ≈ j=1 NT  X T 1 − e−fw mj  , j=1 where NT , T˜j , and mTj are defined analogously to ˜j , and mU . The approximation is again valid NU , U j P T T since the data satisfy mTj / N j mj  1. Topicality Qw . Because SRs are communities centered around interests, words that are characteristic of a SR’s topic are more frequent in that SR than in the others. Topicality has been shown to have an impact on lexical dynamics at long time scales (Church, 2000; Montemurro and Zanette, 2016). It could also influence the productivity of morphological families: higher topical dissemination, i.e., lower topicality, could facilitate growth. To capture this effect, we introduce a metric of topical distinctiveness, Qw , which we define as Qw = fw , f˜w where fw is the relative frequency of the word w in a SR in i(c) , and f˜w is the expected relative frequency of w based on a random sample of posts from all SRs in i(c) . U and D T , The polarity of Qw is reversed to Dw w i.e., a word that is very clumped in SR space will have a high value of Qw , but a wo"
2020.acl-main.649,P18-1180,0,0.288019,"pness”, “trumpology”, “trumpster”, “trumpy” 07/2015 “trump”, “trumpening”, “trumper”, “trumpic”, “trumpification”, “trumpiness”, “trumpish”, “trumpism”, “trumpistan”, “trumpness”, “trumpster”, “trumpy” Table 1: Derivations of “trump” in four subsequent months of the r/politics Subreddit. of lexical change in social media, making them a promising tool for studies in the social sciences that draw on NLP techniques. At the same time, our work adds to the growing body of computational research on derivational morphology (Cotterell et al., 2017; Vylomova et al., 2017; Cotterell and Sch¨utze, 2018; Deutsch et al., 2018; Pierrehumbert and Granell, 2018; Hofmann et al., 2020) by introducing a temporal perspective. Contributions. We introduce the novel task of Morphological Family Expansion Prediction (MFEP), which aims at predicting whether a morphological family will increase in size or not. We publish a benchmark for MFEP and show that the growth of morphological families can be successfully modeled using social and linguistic factors relating to the morphological parent. Furthermore, 7273 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7273–7283 c July 5 - 10,"
2020.acl-main.649,Q18-1003,1,0.902122,"Missing"
2020.acl-main.649,N16-1080,1,0.893663,"Missing"
2020.acl-main.649,D17-1074,0,0.439008,"mpster”, “trumpy” 06/2015 “trump”, “trumpening”, “trumper”, “trumpish”, “trumpness”, “trumpology”, “trumpster”, “trumpy” 07/2015 “trump”, “trumpening”, “trumper”, “trumpic”, “trumpification”, “trumpiness”, “trumpish”, “trumpism”, “trumpistan”, “trumpness”, “trumpster”, “trumpy” Table 1: Derivations of “trump” in four subsequent months of the r/politics Subreddit. of lexical change in social media, making them a promising tool for studies in the social sciences that draw on NLP techniques. At the same time, our work adds to the growing body of computational research on derivational morphology (Cotterell et al., 2017; Vylomova et al., 2017; Cotterell and Sch¨utze, 2018; Deutsch et al., 2018; Pierrehumbert and Granell, 2018; Hofmann et al., 2020) by introducing a temporal perspective. Contributions. We introduce the novel task of Morphological Family Expansion Prediction (MFEP), which aims at predicting whether a morphological family will increase in size or not. We publish a benchmark for MFEP and show that the growth of morphological families can be successfully modeled using social and linguistic factors relating to the morphological parent. Furthermore, 7273 Proceedings of the 58th Annual Meeting of th"
2020.acl-main.649,P12-2027,0,0.0837016,"Missing"
2020.acl-main.649,P16-1141,0,0.032189,"nn et al., 2011; Stewart and Eisenstein, 2018). Another main factor is the dissemination of a word, i.e., how widely a word is spread across different social and linguistic contexts. Generally, the more disseminated a word is, the more likely it is to grow. This holds for social dissemination across users and threads (Altmann et al., 2011) as well as linguistic dissemination across different lexical collocations (Stewart and Eisenstein, 2018). The studies mentioned so far focus on token frequency. An exciting new approach looks instead at the meaning of words using diachronic word embeddings (Hamilton et al., 2016). del Tredici et al. (2019), e.g., explore short-term meaning shifts on Reddit and identify considerable changes even within a period of eight years. A main goal of this study is to add a third approach to studies on lexical change in social media besides word frequency and word embeddings: word families. From a linguistic point of view, these three approaches can be viewed to be complementary: whereas word frequency is contextindependent, both word embeddings and word families reflect context-sensitive measures. However, while word embeddings reflect proximity on the utterance level (which wo"
2020.acl-main.649,P11-1038,0,0.0836102,"5 486,746 2,020,843 1,859,228 613,158 721,641 19,000 19,000 17,000 13,000 4.64 4.00 3.44 3.20 4.05 3.48 3.09 2.93 Table 2: SR summary statistics. Start: first month included; end: last month included; NI : number of intervals; NW : number of word tokens; NU : number of users; NT : number of threads; |F|: number of family examples (1, 000 × NI ); µ|F |: mean family size per month; σ|F |: standard deviation of family size per month. full and shortened hyperlinks. We convert British English spelling variants to American English and lemmatize all words to remove inflectional morphology. We follow Han and Baldwin (2011) in reducing repetitions of more than three letters (“niiiiice”) to three letters. Except for stopwords, we do not employ a frequency threshold; in particular, we include words that occur only once. Computing morphological families. Given a collection of texts S, we define the morphological families as follows. Let VS be the vocabulary of S, i.e., all words occurring in it. We define the set of parents OS ⊂ VS as the 1,000 most frequent words in S, regardless of whether the word is decomposable or not. This means that parents are not necessarily morphological roots (Haspelmath and Sims, 2010)."
2020.acl-main.649,2020.acl-main.106,1,0.755808,"Missing"
2020.acl-main.649,C18-1135,0,0.217455,"Missing"
2020.acl-main.649,W15-0108,0,0.0621552,"Missing"
2020.acl-main.649,P13-1149,0,0.153834,"3) in general, is heavily influenced by social variation. Social groups differ in the morphological patterns they use and in the extent to which they extend these patterns to new words. This makes morphological productivity an exciting new area for future research in computational social science, and it further underscores the relevance of MFEP for that field. Derivational morphology in NLP. Derivational morphology has received increasing attention in NLP recently. Key challenges include segmenting (Cotterell et al., 2016; Luo et al., 2017; Cotterell and Sch¨utze, 2018), modeling the meaning (Lazaridou et al., 2013; Kisselew et al., 2015; Pad´o et al., 2016; Cotterell and Sch¨utze, 2018), and predicting the form (Vylomova et al., 2017; Cotterell et al., 2017; Deutsch et al., 2018) as well as morphological well-formedness (Hofmann et al., 2020) of derivatives. Whereas all these studies approach derivational morphology from a synchronic standpoint, MFEP is to the best of our knowledge the first computational task that addresses diachronic aspects of derivation. Lexical change in social media. Language change (Croft, 2000; Bybee, 2015) is most visible on the lexical level. New words like “detrumpify” attra"
2020.acl-main.649,Q17-1025,0,0.0234039,"ology (R´acz et al., 2015) and other components of language (Labov, 1963) in general, is heavily influenced by social variation. Social groups differ in the morphological patterns they use and in the extent to which they extend these patterns to new words. This makes morphological productivity an exciting new area for future research in computational social science, and it further underscores the relevance of MFEP for that field. Derivational morphology in NLP. Derivational morphology has received increasing attention in NLP recently. Key challenges include segmenting (Cotterell et al., 2016; Luo et al., 2017; Cotterell and Sch¨utze, 2018), modeling the meaning (Lazaridou et al., 2013; Kisselew et al., 2015; Pad´o et al., 2016; Cotterell and Sch¨utze, 2018), and predicting the form (Vylomova et al., 2017; Cotterell et al., 2017; Deutsch et al., 2018) as well as morphological well-formedness (Hofmann et al., 2020) of derivatives. Whereas all these studies approach derivational morphology from a synchronic standpoint, MFEP is to the best of our knowledge the first computational task that addresses diachronic aspects of derivation. Lexical change in social media. Language change (Croft, 2000; Bybee,"
2020.acl-main.649,W16-4002,0,0.0308141,"that addresses diachronic aspects of derivation. Lexical change in social media. Language change (Croft, 2000; Bybee, 2015) is most visible on the lexical level. New words like “detrumpify” attract attention, often becoming the subject of public discourse (Metcalf, 2002). Since innovations are taking place at a much faster rate on internet media (Crystal, 2004), social media have become a central resource for studies on lexical change over the last decade (Altmann et al., 2011; Garley 7280 and Hockenmaier, 2012; Danescu-Niculescu-Mizil et al., 2013; Grieve et al., 2016; Kershaw et al., 2016; Sang, 2016; Stewart and Eisenstein, 2018; del Tredici and Fern´andez, 2018). One central question in this field is: what factors determine whether a word will survive in the lexicon of an online community? Usage frequency is a well-known factor that influences the evolution of a word at historical time scales (Pagel et al., 2007). Studies on lexical change in online groups have shown that this is also true for shorter time scales (Altmann et al., 2011; Stewart and Eisenstein, 2018). Another main factor is the dissemination of a word, i.e., how widely a word is spread across different social and linguist"
2020.acl-main.649,D18-1467,0,0.265504,"t al., 2011), but not for changes in morphological type frequency. 3 Experimental Data We develop MFEP using Reddit, a social media platform hosting discussions about a variety of topics. Reddit is divided into smaller communities centered around a shared interest, so-called subreddits (SRs), which are highly conducive to linguistic innovation (del Tredici and Fern´andez, 2018). Concretely, we draw upon the Baumgartner Reddit Corpus, a collection of (almost) all publicly available comments posted on Reddit since 2005.4 A three-year slice of this corpus was used in a study on lexical change by Stewart and Eisenstein (2018). Gaffney and Matias (2018) show that the corpus’s coverage of Reddit is not complete, but we do not expect this to affect our analysis. Our study examines data from 2007 to 2018 in the four SRs r/gaming, r/movies, r/nba, and r/politics. These SRs were chosen because they are of comparable size, belong to the largest SRs of Reddit, and at the same time all reflect distinct areas of interest (Table 2). For each month, we also draw a random sample of comments from all SRs that will be used for computing word topicality (Section 4). The size of the sample equals the average size of the four selec"
2020.acl-main.649,P06-1124,0,0.137063,"in i(c) and i(p) ) are measured in i(c) . Family size |F |. The family size is a prime example of an endogenous (language-internal) factor, i.e., one that depends on the linguistic system. A morpheme with a large family might combine more readily with new affixes than a morpheme that occurs only with a small number of affixes. This idea bears a theoretical connection to smoothing techniques such as Witten-Bell and Kneser-Ney smoothing, which model the probability of previously unseen n-grams containing a given word (≈ |F˜n |) by assuming a rich-get-richer process (Manning and Sch¨utze, 1999; Teh, 2006). It is also in line with lexical growth models based on preferential attachment (Steyvers and Tenenbaum, 2005). Intuitively, the fact that morphological children themselves can become the basis for new derivations also suggests a rich-get-richer process. Notice that |F |is equivalent to the type fre7275 quency of w∗ . In linguistics, type frequency is known to be a good predictor of the productivity of inflectional patterns (Bybee, 1995). Furthermore, it has been shown that the morphological family size facilitates lexical processing (Schreuder and Baayen, 1997). To probe whether type frequen"
2020.acl-main.649,E17-2019,0,0.459897,"5 “trump”, “trumpening”, “trumper”, “trumpish”, “trumpness”, “trumpology”, “trumpster”, “trumpy” 07/2015 “trump”, “trumpening”, “trumper”, “trumpic”, “trumpification”, “trumpiness”, “trumpish”, “trumpism”, “trumpistan”, “trumpness”, “trumpster”, “trumpy” Table 1: Derivations of “trump” in four subsequent months of the r/politics Subreddit. of lexical change in social media, making them a promising tool for studies in the social sciences that draw on NLP techniques. At the same time, our work adds to the growing body of computational research on derivational morphology (Cotterell et al., 2017; Vylomova et al., 2017; Cotterell and Sch¨utze, 2018; Deutsch et al., 2018; Pierrehumbert and Granell, 2018; Hofmann et al., 2020) by introducing a temporal perspective. Contributions. We introduce the novel task of Morphological Family Expansion Prediction (MFEP), which aims at predicting whether a morphological family will increase in size or not. We publish a benchmark for MFEP and show that the growth of morphological families can be successfully modeled using social and linguistic factors relating to the morphological parent. Furthermore, 7273 Proceedings of the 58th Annual Meeting of the Association for Compu"
2020.acl-main.649,C16-1122,0,0.156489,"Missing"
2020.acl-main.649,W18-5814,1,0.692721,"“trumpster”, “trumpy” 07/2015 “trump”, “trumpening”, “trumper”, “trumpic”, “trumpification”, “trumpiness”, “trumpish”, “trumpism”, “trumpistan”, “trumpness”, “trumpster”, “trumpy” Table 1: Derivations of “trump” in four subsequent months of the r/politics Subreddit. of lexical change in social media, making them a promising tool for studies in the social sciences that draw on NLP techniques. At the same time, our work adds to the growing body of computational research on derivational morphology (Cotterell et al., 2017; Vylomova et al., 2017; Cotterell and Sch¨utze, 2018; Deutsch et al., 2018; Pierrehumbert and Granell, 2018; Hofmann et al., 2020) by introducing a temporal perspective. Contributions. We introduce the novel task of Morphological Family Expansion Prediction (MFEP), which aims at predicting whether a morphological family will increase in size or not. We publish a benchmark for MFEP and show that the growth of morphological families can be successfully modeled using social and linguistic factors relating to the morphological parent. Furthermore, 7273 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7273–7283 c July 5 - 10, 2020. 2020 Association for Compu"
2020.acl-main.698,P19-1470,0,0.0397579,"/or recent context on the other hand. Poerner et al. (2019) present a similar analysis. Our work is a new way of analyzing differences between PLMs and human-level natural language understanding. We should aspire to develop PLMs that – like humans – can handle negation and are not easily distracted by misprimes. 5 Related Work PLMs are top performers for many tasks, including QA (Kwiatkowski et al., 2019; Alberti et al., 2019). PLMs are usually finetuned (Liu et al., 2019; Devlin et al., 2019), but recent work has applied models without finetuning (Radford et al., 2019; Petroni et al., 2019). Bosselut et al. (2019) investigate PLMs’ common sense knowledge, but do not consider negation explicitly or priming. A wide range of literature analyzes linguistic knowledge stored in pretrained embeddings (Jumelet and Hupkes, 2018; Gulordava et al., 2018; Giulianelli et al., 2018; McCoy et al., 2019; Dasgupta et al., 2018; Marvin and Linzen, 2018; Warstadt and Bowman, 2019; Kann et al., 2019). Our work analyzes factual knowledge. McCoy et al. (2019) show that BERT finetuned to perform natural language inference heavily relies on syntactic heuristics, also suggesting that it is not able to adequately acquire common"
2020.acl-main.698,P19-1285,0,0.0529151,"Missing"
2020.acl-main.698,N19-1423,0,0.238995,"factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (“Birds cannot [MASK]”) and non-negated (“Birds can [MASK]”) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (“Talk? Birds can [MASK]”). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge. 1 Introduction PLMs like Transformer-XL (Dai et al., 2019), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have emerged as universal tools that capture a diverse range of linguistic and factual knowledge. Recently, Petroni et al. (2019) introduced LAMA (LAnguage Model Analysis) to investigate whether PLMs can recall factual knowledge that is part of their training corpus. Since the PLM training objective is to predict masked tokens, question answering (QA) tasks can be reformulated as cloze questions. For example, “Who wrote ‘Dubliners’?” is reformulated as “[MASK] wrote ‘Dubliners’.” In this setup, Petroni et al. (2019) show that PLMs outperform automatically extracted knowledge bases on QA. In t"
2020.acl-main.698,L18-1544,0,0.0238133,"r Computational Linguistics Version A B C D published.1 2 Data and Models LAMA’s cloze questions are generated from subject-relation-object triples from knowledge bases (KBs) and question-answer pairs. For KB triples, cloze questions are generated, for each relation, by a templatic statement that contains variables X and Y for subject and object (e.g, “X was born in Y”). We then substitute the subject for X and MASK for Y. In a question-answer pair, we MASK the answer. LAMA is based on several sources: (i) GoogleRE. 3 relations: “place of birth”, “date of birth”, “place of death”. (ii) T-REx (Elsahar et al., 2018). Subset of Wikidata triples. 41 relations. (iii) ConceptNet (Li et al., 2016). 16 commonsense relations. The underlying corpus provides matching statements to query PLMs. (iv) SQuAD (Rajpurkar et al., 2016). Subset of 305 context-insensitive questions, reworded as cloze questions. We use the source code provided by Petroni et al. (2019) and Wolf et al. (2019) to evaluate Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERTlarge (Bl). Negated LAMA. We created negated LAMA by manually inserting a negation element in each template or question. For ConceptNet w"
2020.acl-main.698,W18-5426,0,0.0222316,"dle negation and are not easily distracted by misprimes. 5 Related Work PLMs are top performers for many tasks, including QA (Kwiatkowski et al., 2019; Alberti et al., 2019). PLMs are usually finetuned (Liu et al., 2019; Devlin et al., 2019), but recent work has applied models without finetuning (Radford et al., 2019; Petroni et al., 2019). Bosselut et al. (2019) investigate PLMs’ common sense knowledge, but do not consider negation explicitly or priming. A wide range of literature analyzes linguistic knowledge stored in pretrained embeddings (Jumelet and Hupkes, 2018; Gulordava et al., 2018; Giulianelli et al., 2018; McCoy et al., 2019; Dasgupta et al., 2018; Marvin and Linzen, 2018; Warstadt and Bowman, 2019; Kann et al., 2019). Our work analyzes factual knowledge. McCoy et al. (2019) show that BERT finetuned to perform natural language inference heavily relies on syntactic heuristics, also suggesting that it is not able to adequately acquire common sense. Warstadt et al. (2019) investigate BERT’s understanding of how negative polarity items are licensed. Our work, focusing on factual knowledge stored in negated sentences, is complementary since grammaticality and factuality are mostly orthogonal proper"
2020.acl-main.698,N18-1108,0,0.0366255,"– like humans – can handle negation and are not easily distracted by misprimes. 5 Related Work PLMs are top performers for many tasks, including QA (Kwiatkowski et al., 2019; Alberti et al., 2019). PLMs are usually finetuned (Liu et al., 2019; Devlin et al., 2019), but recent work has applied models without finetuning (Radford et al., 2019; Petroni et al., 2019). Bosselut et al. (2019) investigate PLMs’ common sense knowledge, but do not consider negation explicitly or priming. A wide range of literature analyzes linguistic knowledge stored in pretrained embeddings (Jumelet and Hupkes, 2018; Gulordava et al., 2018; Giulianelli et al., 2018; McCoy et al., 2019; Dasgupta et al., 2018; Marvin and Linzen, 2018; Warstadt and Bowman, 2019; Kann et al., 2019). Our work analyzes factual knowledge. McCoy et al. (2019) show that BERT finetuned to perform natural language inference heavily relies on syntactic heuristics, also suggesting that it is not able to adequately acquire common sense. Warstadt et al. (2019) investigate BERT’s understanding of how negative polarity items are licensed. Our work, focusing on factual knowledge stored in negated sentences, is complementary since grammaticality and factuality ar"
2020.acl-main.698,W19-0129,0,0.0202296,"QA (Kwiatkowski et al., 2019; Alberti et al., 2019). PLMs are usually finetuned (Liu et al., 2019; Devlin et al., 2019), but recent work has applied models without finetuning (Radford et al., 2019; Petroni et al., 2019). Bosselut et al. (2019) investigate PLMs’ common sense knowledge, but do not consider negation explicitly or priming. A wide range of literature analyzes linguistic knowledge stored in pretrained embeddings (Jumelet and Hupkes, 2018; Gulordava et al., 2018; Giulianelli et al., 2018; McCoy et al., 2019; Dasgupta et al., 2018; Marvin and Linzen, 2018; Warstadt and Bowman, 2019; Kann et al., 2019). Our work analyzes factual knowledge. McCoy et al. (2019) show that BERT finetuned to perform natural language inference heavily relies on syntactic heuristics, also suggesting that it is not able to adequately acquire common sense. Warstadt et al. (2019) investigate BERT’s understanding of how negative polarity items are licensed. Our work, focusing on factual knowledge stored in negated sentences, is complementary since grammaticality and factuality are mostly orthogonal properties. Kim et al. (2019) investigate understanding of negation particles when PLMs are finetuned. In contrast, our f"
2020.acl-main.698,S19-1026,0,0.0872605,"Missing"
2020.acl-main.698,Q19-1026,0,0.0177121,". This may suggest that it is not knowledge that is learned by BERT, but that its performance is mainly based on similarity matching between the current context on the one hand and sentences in its training corpus and/or recent context on the other hand. Poerner et al. (2019) present a similar analysis. Our work is a new way of analyzing differences between PLMs and human-level natural language understanding. We should aspire to develop PLMs that – like humans – can handle negation and are not easily distracted by misprimes. 5 Related Work PLMs are top performers for many tasks, including QA (Kwiatkowski et al., 2019; Alberti et al., 2019). PLMs are usually finetuned (Liu et al., 2019; Devlin et al., 2019), but recent work has applied models without finetuning (Radford et al., 2019; Petroni et al., 2019). Bosselut et al. (2019) investigate PLMs’ common sense knowledge, but do not consider negation explicitly or priming. A wide range of literature analyzes linguistic knowledge stored in pretrained embeddings (Jumelet and Hupkes, 2018; Gulordava et al., 2018; Giulianelli et al., 2018; McCoy et al., 2019; Dasgupta et al., 2018; Marvin and Linzen, 2018; Warstadt and Bowman, 2019; Kann et al., 2019). Our work"
2020.acl-main.698,P16-1137,0,0.0426616,"oze questions are generated from subject-relation-object triples from knowledge bases (KBs) and question-answer pairs. For KB triples, cloze questions are generated, for each relation, by a templatic statement that contains variables X and Y for subject and object (e.g, “X was born in Y”). We then substitute the subject for X and MASK for Y. In a question-answer pair, we MASK the answer. LAMA is based on several sources: (i) GoogleRE. 3 relations: “place of birth”, “date of birth”, “place of death”. (ii) T-REx (Elsahar et al., 2018). Subset of Wikidata triples. 41 relations. (iii) ConceptNet (Li et al., 2016). 16 commonsense relations. The underlying corpus provides matching statements to query PLMs. (iv) SQuAD (Rajpurkar et al., 2016). Subset of 305 context-insensitive questions, reworded as cloze questions. We use the source code provided by Petroni et al. (2019) and Wolf et al. (2019) to evaluate Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERTlarge (Bl). Negated LAMA. We created negated LAMA by manually inserting a negation element in each template or question. For ConceptNet we only consider an easy-to-negate subset (see appendix). Misprimed LAMA. We mi"
2020.acl-main.698,2021.ccl-1.108,0,0.171542,"Missing"
2020.acl-main.698,D18-1151,0,0.0262595,"rk PLMs are top performers for many tasks, including QA (Kwiatkowski et al., 2019; Alberti et al., 2019). PLMs are usually finetuned (Liu et al., 2019; Devlin et al., 2019), but recent work has applied models without finetuning (Radford et al., 2019; Petroni et al., 2019). Bosselut et al. (2019) investigate PLMs’ common sense knowledge, but do not consider negation explicitly or priming. A wide range of literature analyzes linguistic knowledge stored in pretrained embeddings (Jumelet and Hupkes, 2018; Gulordava et al., 2018; Giulianelli et al., 2018; McCoy et al., 2019; Dasgupta et al., 2018; Marvin and Linzen, 2018; Warstadt and Bowman, 2019; Kann et al., 2019). Our work analyzes factual knowledge. McCoy et al. (2019) show that BERT finetuned to perform natural language inference heavily relies on syntactic heuristics, also suggesting that it is not able to adequately acquire common sense. Warstadt et al. (2019) investigate BERT’s understanding of how negative polarity items are licensed. Our work, focusing on factual knowledge stored in negated sentences, is complementary since grammaticality and factuality are mostly orthogonal properties. Kim et al. (2019) investigate understanding of negation partic"
2020.acl-main.698,N18-1202,0,0.0661465,"wo new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (“Birds cannot [MASK]”) and non-negated (“Birds can [MASK]”) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (“Talk? Birds can [MASK]”). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge. 1 Introduction PLMs like Transformer-XL (Dai et al., 2019), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have emerged as universal tools that capture a diverse range of linguistic and factual knowledge. Recently, Petroni et al. (2019) introduced LAMA (LAnguage Model Analysis) to investigate whether PLMs can recall factual knowledge that is part of their training corpus. Since the PLM training objective is to predict masked tokens, question answering (QA) tasks can be reformulated as cloze questions. For example, “Who wrote ‘Dubliners’?” is reformulated as “[MASK] wrote ‘Dubliners’.” In this setup, Petroni et al. (2019) show that PLMs outperform automatically extrac"
2020.acl-main.698,D19-1250,0,0.139165,"Missing"
2020.acl-main.698,D16-1264,0,0.0593916,"KB triples, cloze questions are generated, for each relation, by a templatic statement that contains variables X and Y for subject and object (e.g, “X was born in Y”). We then substitute the subject for X and MASK for Y. In a question-answer pair, we MASK the answer. LAMA is based on several sources: (i) GoogleRE. 3 relations: “place of birth”, “date of birth”, “place of death”. (ii) T-REx (Elsahar et al., 2018). Subset of Wikidata triples. 41 relations. (iii) ConceptNet (Li et al., 2016). 16 commonsense relations. The underlying corpus provides matching statements to query PLMs. (iv) SQuAD (Rajpurkar et al., 2016). Subset of 305 context-insensitive questions, reworded as cloze questions. We use the source code provided by Petroni et al. (2019) and Wolf et al. (2019) to evaluate Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERTlarge (Bl). Negated LAMA. We created negated LAMA by manually inserting a negation element in each template or question. For ConceptNet we only consider an easy-to-negate subset (see appendix). Misprimed LAMA. We misprime LAMA by inserting an incorrect word and a question mark at the beginning of a statement; e.g., “Talk?” in “Talk? Birds can"
2020.acl-main.698,P18-1079,0,0.0306016,", focusing on factual knowledge stored in negated sentences, is complementary since grammaticality and factuality are mostly orthogonal properties. Kim et al. (2019) investigate understanding of negation particles when PLMs are finetuned. In contrast, our focus is on the interaction of negation and factual knowledge learned in pretraining. Ettinger (2019) defines and applies psycho-linguistic diagnostics for PLMs. Our use of priming is complementary. Their data consists of two sets of 72 and 16 sentences whereas we create 42,867 negated sentences covering a wide range of topics and relations. Ribeiro et al. (2018) test for comprehension of minimally modified sentences in an adversarial setup while trying to keep the overall semantics the same. In contrast, we investigate large changes of meaning (negation) and context (mispriming). In contrast to adversarial work (e.g., (Wallace et al., 2019)), we do not focus on adversarial examples for a specific task, but on pretrained models’ ability to robustly store factual knowledge. 6 Conclusion Our results suggest that pretrained language models address open domain QA in datasets like LAMA by mechanisms that are more akin to relatively shallow pattern matching"
2020.acl-main.698,D19-1221,0,0.0167479,"of negation and factual knowledge learned in pretraining. Ettinger (2019) defines and applies psycho-linguistic diagnostics for PLMs. Our use of priming is complementary. Their data consists of two sets of 72 and 16 sentences whereas we create 42,867 negated sentences covering a wide range of topics and relations. Ribeiro et al. (2018) test for comprehension of minimally modified sentences in an adversarial setup while trying to keep the overall semantics the same. In contrast, we investigate large changes of meaning (negation) and context (mispriming). In contrast to adversarial work (e.g., (Wallace et al., 2019)), we do not focus on adversarial examples for a specific task, but on pretrained models’ ability to robustly store factual knowledge. 6 Conclusion Our results suggest that pretrained language models address open domain QA in datasets like LAMA by mechanisms that are more akin to relatively shallow pattern matching than the recall of learned factual knowledge and inference. Implications for future work on pretrained language models. (i) Both factual knowledge and logic are discrete phenomena in the sense that sentences with similar representations in current pretrained language models differ s"
2020.acl-main.698,D19-1286,0,0.0353476,"gate PLMs’ common sense knowledge, but do not consider negation explicitly or priming. A wide range of literature analyzes linguistic knowledge stored in pretrained embeddings (Jumelet and Hupkes, 2018; Gulordava et al., 2018; Giulianelli et al., 2018; McCoy et al., 2019; Dasgupta et al., 2018; Marvin and Linzen, 2018; Warstadt and Bowman, 2019; Kann et al., 2019). Our work analyzes factual knowledge. McCoy et al. (2019) show that BERT finetuned to perform natural language inference heavily relies on syntactic heuristics, also suggesting that it is not able to adequately acquire common sense. Warstadt et al. (2019) investigate BERT’s understanding of how negative polarity items are licensed. Our work, focusing on factual knowledge stored in negated sentences, is complementary since grammaticality and factuality are mostly orthogonal properties. Kim et al. (2019) investigate understanding of negation particles when PLMs are finetuned. In contrast, our focus is on the interaction of negation and factual knowledge learned in pretraining. Ettinger (2019) defines and applies psycho-linguistic diagnostics for PLMs. Our use of priming is complementary. Their data consists of two sets of 72 and 16 sentences whe"
2020.acl-main.698,2020.tacl-1.3,0,\N,Missing
2020.bucc-1.8,P19-1309,0,0.0951046,"Missing"
2020.bucc-1.8,P18-1073,0,0.144567,"e monolingual corpora, and training and testing dictionaries for high, middle and low frequency words. In this paper, we present our approach to the shared task and show results on English-German and English-Russian. BWEs are popular for solving BDI by calculating cosine similarity of word pairs and taking the n most similar candidates as translations for a given source word. They were shown to be very effective for the task using a small seed lexicon only (e.g., (Mikolov et al., 2013b)) as opposed to MT based approaches where parallel data is necessary. In addition, Conneau et al. (2018) and Artetxe et al. (2018) were able to learn BWEs without any seed dictionaries using a self-learning method that starts from an initial weak solution and improves the mapping iteratively. Due to this, BDI is one of the building blocks of unsupervised MT and are particularly relevant in low-resource settings (Artetxe et ∗ The authors contributed equally to this manuscript. 49 signal. We test our system on the English-German pairs (En-De, De-En) and English-Russian pairs (En-Ru, Ru-En) provided in the BUCC 2020 Shared Task (Rapp et al., 2020). We participate in both the open and closed tracks of the shared tasks, using"
2020.bucc-1.8,P19-1019,0,0.112307,"Missing"
2020.bucc-1.8,Q17-1010,0,0.532253,"results. 2. and German. On the other hand, for English and Russian the approach is not applicable due to the different character sets of the two languages, thus we employ an unsupervised transliteration model. 3.1. To build BWEs we follow the mapping approach of (Mikolov et al., 2013b), i.e., we build monolingual word embeddings (MWEs) which we then align to a share space using a seed dictionary. We create 4 types of MWE models for each language, since it was shown that combining them is beneficial for BDI (Braune et al., 2018): {word2vec, f asttext} × {cbow, skipgram} (Mikolov et al., 2013a; Bojanowski et al., 2017). We perform the mapping using VecMap (Artetxe et al., 2018) which learns an orthogonal projection of the source MWE to the target space. Although the approach supports unsupervised mapping, we use it in a supervised setup. As the seed lexicon, we use part of the provided high frequency dictionary. Although the dictionary contains multiple translations for some source words, we only use the first translation of each word in order to reduce noise. Finally, we generate a similarity dictionary based on each BWE type containing translation candidates, i.e., the 100 most similar target language wor"
2020.bucc-1.8,N18-2030,1,0.821236,"ource word. We participate in both the open and closed tracks of the shared task and we show improved results of our method compared to simple vector similarity based approaches. Our system was ranked in the top-3 teams and achieved the best results for English-Russian. Keywords: BDI, BWE, Orthography, Transliteration 1. Introduction al., 2019; Lample et al., 2018). Although BWE based methods work well for translating high frequency words, it was shown that they tend to have low performance when translating low-frequency words or named entities due to poor vector representation of such words (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). By using character n-gram representations and Levenshtein similarity of words, Braune et al. (2018) showed improved results on rare and domain specific words. Similarly, Riley and Gildea (2018) improves the translation of such words by integrating orthographic information into the vector representation of words and in the mapping procedure of BWEs. On the other hand, these techniques are only applicable in the case of language pairs having the same scripts. Recently, Riley and Gildea (2020) proposed an unsupervised system based on expectation"
2020.bucc-1.8,D14-1179,0,0.00580683,"Missing"
2020.bucc-1.8,D19-1090,0,0.158141,"nd closed tracks of the shared task and we show improved results of our method compared to simple vector similarity based approaches. Our system was ranked in the top-3 teams and achieved the best results for English-Russian. Keywords: BDI, BWE, Orthography, Transliteration 1. Introduction al., 2019; Lample et al., 2018). Although BWE based methods work well for translating high frequency words, it was shown that they tend to have low performance when translating low-frequency words or named entities due to poor vector representation of such words (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). By using character n-gram representations and Levenshtein similarity of words, Braune et al. (2018) showed improved results on rare and domain specific words. Similarly, Riley and Gildea (2018) improves the translation of such words by integrating orthographic information into the vector representation of words and in the mapping procedure of BWEs. On the other hand, these techniques are only applicable in the case of language pairs having the same scripts. Recently, Riley and Gildea (2020) proposed an unsupervised system based on expectation maximization and character-level RNN models to le"
2020.bucc-1.8,P19-1581,1,0.822411,"pervised transliteration model. In contrast to (Riley and Gildea, 2020), we propose a cleaning method for filtering non-transliteration pairs from the used dictionary before training the model to ensure a less noisy training Bilingual Dictionary Induction is the task of inducing word translations from monolingual corpora in different languages. It has been studied extensively as it is one of the main tasks used for evaluating the quality of BWE models (Mikolov et al., 2013b; Vulic and Korhonen, 2016). It is also important for downstream tasks such as translating out-of-vocabulary words in MT (Huck et al., 2019). Although there is a large amount of work for BDI, there is no standard way to measure the performance of the systems, the published results are not comparable and the pros and cons of the various approaches are not clear. The aim of the BUCC 2020 – Bilingual Dictionary Induction from Comparable Corpora – shared task (Rapp et al., 2020) is to solve this problem and compare various systems on a standard test set. It involves multiple language pairs including Chinese, English, French, German, Russian and Spanish and supports comparable monolingual corpora, and training and testing dictionaries"
2020.bucc-1.8,D18-1549,0,0.0299977,"similarity with word surface similarity methods, such as orthography and transliteration information. In addition to the often used top-n translation method, we experiment with a margin based approach aiming for dynamic number of translations for each source word. We participate in both the open and closed tracks of the shared task and we show improved results of our method compared to simple vector similarity based approaches. Our system was ranked in the top-3 teams and achieved the best results for English-Russian. Keywords: BDI, BWE, Orthography, Transliteration 1. Introduction al., 2019; Lample et al., 2018). Although BWE based methods work well for translating high frequency words, it was shown that they tend to have low performance when translating low-frequency words or named entities due to poor vector representation of such words (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). By using character n-gram representations and Levenshtein similarity of words, Braune et al. (2018) showed improved results on rare and domain specific words. Similarly, Riley and Gildea (2018) improves the translation of such words by integrating orthographic information into the vector represe"
2020.bucc-1.8,D15-1166,0,0.246365,"Missing"
2020.bucc-1.8,P18-2062,0,0.499995,"ipate in both the open and closed tracks of the shared task and we show improved results of our method compared to simple vector similarity based approaches. Our system was ranked in the top-3 teams and achieved the best results for English-Russian. Keywords: BDI, BWE, Orthography, Transliteration 1. Introduction al., 2019; Lample et al., 2018). Although BWE based methods work well for translating high frequency words, it was shown that they tend to have low performance when translating low-frequency words or named entities due to poor vector representation of such words (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). By using character n-gram representations and Levenshtein similarity of words, Braune et al. (2018) showed improved results on rare and domain specific words. Similarly, Riley and Gildea (2018) improves the translation of such words by integrating orthographic information into the vector representation of words and in the mapping procedure of BWEs. On the other hand, these techniques are only applicable in the case of language pairs having the same scripts. Recently, Riley and Gildea (2020) proposed an unsupervised system based on expectation maximization and charac"
2020.bucc-1.8,P16-1024,0,0.38169,"th different scripts where an orthographic comparison would not be possible and it is obtained from our novel fully unsupervised transliteration model. In contrast to (Riley and Gildea, 2020), we propose a cleaning method for filtering non-transliteration pairs from the used dictionary before training the model to ensure a less noisy training Bilingual Dictionary Induction is the task of inducing word translations from monolingual corpora in different languages. It has been studied extensively as it is one of the main tasks used for evaluating the quality of BWE models (Mikolov et al., 2013b; Vulic and Korhonen, 2016). It is also important for downstream tasks such as translating out-of-vocabulary words in MT (Huck et al., 2019). Although there is a large amount of work for BDI, there is no standard way to measure the performance of the systems, the published results are not comparable and the pros and cons of the various approaches are not clear. The aim of the BUCC 2020 – Bilingual Dictionary Induction from Comparable Corpora – shared task (Rapp et al., 2020) is to solve this problem and compare various systems on a standard test set. It involves multiple language pairs including Chinese, English, French"
2020.coling-main.324,P18-1246,0,0.0513826,"Missing"
2020.coling-main.324,Q17-1010,0,0.0115684,"d (bottom) convolution on the attention matrix σ(A) to get a postprocessed matrix A0 . For 1d convolution we use one filter per row. PTB. We work on the WSJ section of the Penn-Treebank (PTB) (Marcus et al., 1993) with the usual data split (train: 0-18, dev: 19-21, tst: 22-24). We report accuracy across all words, out of vocabulary (OOV), and ambiguous words. We consider a word ambiguous if it has more than one unique PoS tag in the training data. We report mean and standard deviation (in subscript) across three random seeds. For pretrained word embeddings we use fastttext subword embeddings (Bojanowski et al., 2017).1 UD. We use version 2.2 of the Universal Dependencies as used in the CoNLL 2018 shared task (Zeman et al., 2018). We consider treebanks that have train, development, and test data and where results are reported in (Smith et al., 2018). This results in 47 treebanks. 1 https://fasttext.cc/docs/en/english-vectors.html 3632 3 Results Dev Accuracy 0.98 Baseline 0.96 UD Treebank 0.94 SAN+PE[add] SAN+P+R SAN+PE[add]+Temp SAN+PE[add]+Conv SAN+PE[add]+Conv2d 0.92 0.90 0 5 10 15 # Epochs 20 25 Methods Baselines (a) Development accuracy on PTB during training. All OOV Ambig. SAN SAN+PE[con] SAN+PE[add]"
2020.coling-main.324,W19-4828,0,0.111062,"ue that learning position interactions can be modeled more directly than learning separate position embeddings and propose to replace embeddings with a direct position interaction matrix. (2) We hypothesize that spiky distributions generated by a softmax function within the attention head hinders the network from considering the broader sentence context effectively. Thus we introduce additional scalar parameters, a learnable temperature, that can support the network in using the context more effectively. (3) Convoluted Attention: attention matrices have been found to exhibit regular patterns (Clark et al., 2019; Kovaleva et al., 2019). A convolution which post-processes the attention matrix allows the network to detect attention patterns, and subsequently to reinforce or weaken attention scores. We perform experiments on Part-of-Speech (PoS) tagging. We argue that a PoS model can only be successful for ambiguous and out-of-vocabulary tokens if it carefully considers and processes the context. Thus we consider PoS a suitable task to probe whether our modifications on the attention matrix enable more efficient learning. We perform experiments on the Penn Treebank (PTB) and on 47 languages of Universal"
2020.coling-main.324,J81-4005,0,0.704703,"Missing"
2020.coling-main.324,N19-1423,0,0.0258475,"Missing"
2020.coling-main.324,2020.findings-emnlp.379,0,0.0427323,"hich is similar to SAN+P. They also find it to be a feasible alternative to position embeddings and report slight performance increases. In contrast to weight normalization (Salimans and Kingma, 2016), a related method to learnable temperature, we do not normalize the weight matrices. Instead we only add a learnable scalar parameter and observed that normalizing the weights actually harms performance. Lin et al. (2018) introduced a self-adaptive temperature. However, they focused on parametrizing the temperature of timestep t using the activations from timestep t−1. Contemporary to this work, Henry et al. (2020) proposed query-key normalization in Transformers. There is range of work trying to combine attention with convolution (Yin and Sch¨utze, 2018; Yu et al., 2018). We are not aware of any work that applies convolution directly to attention weights. 5 Conclusion We conclude that position embeddings can be replaced with direct position interactions.2 Learnable temperature has almost no effect. Convoluted attention speeds up learning on PTB and yields better results on UD. We are aware that this paper is a small study with limited validity as it considers only one task. Given that convoluted attent"
2020.coling-main.324,D19-1445,0,0.149278,"ition interactions can be modeled more directly than learning separate position embeddings and propose to replace embeddings with a direct position interaction matrix. (2) We hypothesize that spiky distributions generated by a softmax function within the attention head hinders the network from considering the broader sentence context effectively. Thus we introduce additional scalar parameters, a learnable temperature, that can support the network in using the context more effectively. (3) Convoluted Attention: attention matrices have been found to exhibit regular patterns (Clark et al., 2019; Kovaleva et al., 2019). A convolution which post-processes the attention matrix allows the network to detect attention patterns, and subsequently to reinforce or weaken attention scores. We perform experiments on Part-of-Speech (PoS) tagging. We argue that a PoS model can only be successful for ambiguous and out-of-vocabulary tokens if it carefully considers and processes the context. Thus we consider PoS a suitable task to probe whether our modifications on the attention matrix enable more efficient learning. We perform experiments on the Penn Treebank (PTB) and on 47 languages of Universal Dependencies (UD). In s"
2020.coling-main.324,D18-1331,0,0.0274446,"elative positions with scalar values, an idea also investigated by Schmitt et al. (2020). This approach is similar to our SAN+R. Contemporary to this submission, Ke et al. (2020) proposed TUPE, which is similar to SAN+P. They also find it to be a feasible alternative to position embeddings and report slight performance increases. In contrast to weight normalization (Salimans and Kingma, 2016), a related method to learnable temperature, we do not normalize the weight matrices. Instead we only add a learnable scalar parameter and observed that normalizing the weights actually harms performance. Lin et al. (2018) introduced a self-adaptive temperature. However, they focused on parametrizing the temperature of timestep t using the activations from timestep t−1. Contemporary to this work, Henry et al. (2020) proposed query-key normalization in Transformers. There is range of work trying to combine attention with convolution (Yin and Sch¨utze, 2018; Yu et al., 2018). We are not aware of any work that applies convolution directly to attention weights. 5 Conclusion We conclude that position embeddings can be replaced with direct position interactions.2 Learnable temperature has almost no effect. Convoluted"
2020.coling-main.324,J93-2004,0,0.0689117,"tern and increase the attention weight for w2 if this is beneficial for performance. For 1d convolution we use t convolutional filters per attention head to preserve the shape of the matrix. For 2d convolution we have one filter per attention head. This can be interpreted as a some sort of smoothing over the attention matrix. We use filter-width 3 in both cases. 2.6 Data Figure 1: Applying 1d (top) and 2d (bottom) convolution on the attention matrix σ(A) to get a postprocessed matrix A0 . For 1d convolution we use one filter per row. PTB. We work on the WSJ section of the Penn-Treebank (PTB) (Marcus et al., 1993) with the usual data split (train: 0-18, dev: 19-21, tst: 22-24). We report accuracy across all words, out of vocabulary (OOV), and ambiguous words. We consider a word ambiguous if it has more than one unique PoS tag in the training data. We report mean and standard deviation (in subscript) across three random seeds. For pretrained word embeddings we use fastttext subword embeddings (Bojanowski et al., 2017).1 UD. We use version 2.2 of the Universal Dependencies as used in the CoNLL 2018 shared task (Zeman et al., 2018). We consider treebanks that have train, development, and test data and whe"
2020.coling-main.324,N18-2074,0,0.135002,"mbeddings to word embeddings and subsequently concatenate the character level word representation. We use a residual connection from the beginning to the end of the network and around each attention layer. See Table 1a and Table 1b for more details on the overall architecture and hyperparameters, and Table 1c for the number of parameters. We used common hyperparameters and did not tune them for higher performance. 2.2 Self-Attention In this section we describe Self-Attention (Vaswani et al., 2017), for which we propose modifications in the following sections. We loosely follow the notation of Shaw et al. (2018) and define self attention as a function att : Rt×d → Rt×dh where t is the sequence length, d the input dimension and dh the output dimension. Consider an input X ∈ Rt×d and weights Wk , Wv , Wq ∈ Rd×dh . We denote the softmax function as σ. A scaled dot-product attention head is Z := att(X) = σ(A)XWv where √ −1 A = dh XWq (XWk ) |is the attention matrix and σ is applied along the horizontal axis. One self-attention layer consists of the concatenation of multiple attention heads. We call the model that adds (resp. concatenates) position embeddings to word embeddings SAN+PE[add] (resp. SAN+PE[c"
2020.coling-main.324,K18-2011,0,0.0209155,"Missing"
2020.coling-main.324,Q18-1047,1,0.863661,"Missing"
2020.coling-main.324,W17-4118,0,0.0147054,"ers to the model. SAN+P has less parameters than SAN+PE[ADD] when t2 × nheads &lt; t × d. Table 1: Model summary. 2 2.1 Methods Model Architecture To study our proposed modifications to self-attention we use a simple architectural setup; see Table 1a. Following embedding lookups for words and positions we deploy multiple layers of self-attention blocks and subsequently a softmax layer to get final PoS predictions. Our objective function is categorical cross-entropy. Character information is essential for PoS tagging (dos Santos and Zadrozny, 2014). To incorporate character information we follow (Yu et al., 2017) and use convolutional neural networks together with max-pooling to obtain a character level representation for words. We add/concatenate position embeddings to word embeddings and subsequently concatenate the character level word representation. We use a residual connection from the beginning to the end of the network and around each attention layer. See Table 1a and Table 1b for more details on the overall architecture and hyperparameters, and Table 1c for the number of parameters. We used common hyperparameters and did not tune them for higher performance. 2.2 Self-Attention In this section"
2020.coling-main.324,K18-2001,0,0.0423472,"Missing"
2020.coling-main.446,W19-3820,0,0.0154547,"(2020) proposed CAT (Context Association Test) to measure language modeling ability as well as the stereotypical bias of pretrained language models at sentence and discourse level. An alternative way to measure gender bias is to evaluate on downstream tasks. For coreference resolution, Zhao et al. (2018) designed Winobias and Rudinger et al. (2018) designed Winogender schemas. Webster et al. (2018) released GAP, a balanced corpus of Gendered Ambiguous Pronouns, which measures gender bias as the ratio of F1 score on masculine to F1 score on feminine. However, a ratio close to 1.0 (Chada, 2019; Attree, 2019) makes it hard to compare debiasing systems. For sentiment analysis, Equity Evaluation Corpus (EEC) (Kiritchenko and Mohammad, 2018) was designed to measure gender bias by the difference in emotional intensity predictions between gender-swapped sentences. Since the measures of gender bias in these datasets are not intuitive, we use association tests in this work. 5.2 Debiasing Methods Many methods to remove gender bias have been proposed. Based on the hypothesis that gender bias in stored in a linear subspace in the word embeddings, the most common way is to define a gender direction (or, more"
2020.coling-main.446,W19-3819,0,0.0117623,"adeem et al. (2020) proposed CAT (Context Association Test) to measure language modeling ability as well as the stereotypical bias of pretrained language models at sentence and discourse level. An alternative way to measure gender bias is to evaluate on downstream tasks. For coreference resolution, Zhao et al. (2018) designed Winobias and Rudinger et al. (2018) designed Winogender schemas. Webster et al. (2018) released GAP, a balanced corpus of Gendered Ambiguous Pronouns, which measures gender bias as the ratio of F1 score on masculine to F1 score on feminine. However, a ratio close to 1.0 (Chada, 2019; Attree, 2019) makes it hard to compare debiasing systems. For sentiment analysis, Equity Evaluation Corpus (EEC) (Kiritchenko and Mohammad, 2018) was designed to measure gender bias by the difference in emotional intensity predictions between gender-swapped sentences. Since the measures of gender bias in these datasets are not intuitive, we use association tests in this work. 5.2 Debiasing Methods Many methods to remove gender bias have been proposed. Based on the hypothesis that gender bias in stored in a linear subspace in the word embeddings, the most common way is to define a gender dire"
2020.coling-main.446,N19-1423,0,0.0295564,"nglish training data. 1 Introduction Word embeddings, which represent the semantic meaning of [MASK] kissed [MASK] . text data as vectors, are used as input in natural language proThe professor asked the nurse . cessing tasks. It has been found that word embeddings exhibit The child played with the car . biases such as gender bias, which are present in their training The child played with the doll . corpora (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg Table 1: Token level bias scores et al., 2018). Contextual word embedding models, such as (male/female bias depicted by blue/red) BERT (Devlin et al., 2019), have become increasingly comidentified using DensRay. See Table 6 mon and achieved new state-of-the-art results in many NLP tasks. Researchers have also found gender bias in contextu- for more details. alized embeddings (Zhao et al., 2019; May et al., 2019). It has been found that these biases have a big impact on downstream tasks (Vanmassenhove et al., 2018; Moryossef et al., 2019; Rudinger et al., 2018; Zhao et al., 2018). A common approach for removing gender information in static embeddings is to identify a linear gender subspace (e.g., a gender direction) and subsequently setting all va"
2020.coling-main.446,D19-1111,1,0.875521,"Missing"
2020.coling-main.446,W19-3621,0,0.0219866,"gender directions using a gendered word list mixed with male and female words. In contrast, Bolukbasi et al. (2016)’s hard debiasing used two groups of gendered words for definition and another two groups for alignment, to identify the gender direction by male-female pairs. The method we use, DensRay, is similar to Bolukbasi et al. (2016)’s hard debiasing in this aspect. However, DensRay uses only one male and one female word list, and it can be solved efficiently in a closed form. So it would be more stable to be applied to contextualized models than Bolukbasi et al. (2016)’s hard debiasing. Gonen and Goldberg (2019) showed that beyond the gender direction, biases on static embeddings also come from the association to other implicitly gendered terms. We assume that this phenomenon will be weakened in contextualized embedding, so we only focus on the gender direction; this may be a limitation.They also proposed some experiments to evaluate the remaining bias after debiasing, among which the gender classifier shared the same idea with OCCTMP. 5090 6 Conclusion We introduced DensRay debiasing on BERT. Our experiments showed that this method can effectively mitigate gender bias on OCCTMP and the Association T"
2020.coling-main.446,W19-3806,0,0.246005,"ics, pages 5082–5093 Barcelona, Spain (Online), December 8-13, 2020 iii) We analyze how gender information is processed in BERT by applying DensRay to attention heads and layers: we conclude that there is no single attention head responsible for processing gender information. In addition, we show in a qualitative analysis that token-level gender scores can be obtained. iv) We apply our debiasing method to the multilingual BERT (mBERT) model: we show that English training data can be used to debias Chinese. The source code of our experiments is available.1 2 Methodology 2.1 Debiasing Conceptor Karve et al. (2019) introduced conceptor debiasing. Given a set of gendered words V := {v1 , v2 , . . . , vn } and their embeddings E ∈ Rn×d , gender bias can be mitigated by multiplying the debiasing conceptor matrix ¬C = I − C. C is the conceptor matrix that minimizes the objective ||E − CE||2F + α−2 ||C||2F , (1) where α is a parameter and ||· ||F is the Frobenius norm. C has an analytical solution given by C= 1 1 EE T ( EE T + α−2 I)−1 . d d (2) Intuitively, C is a soft projection matrix on the linear subspace where embeddings have the maximum bias, thus the boolean-liked operation ¬C can debias on a combina"
2020.coling-main.446,S18-2005,0,0.0238134,"ias of pretrained language models at sentence and discourse level. An alternative way to measure gender bias is to evaluate on downstream tasks. For coreference resolution, Zhao et al. (2018) designed Winobias and Rudinger et al. (2018) designed Winogender schemas. Webster et al. (2018) released GAP, a balanced corpus of Gendered Ambiguous Pronouns, which measures gender bias as the ratio of F1 score on masculine to F1 score on feminine. However, a ratio close to 1.0 (Chada, 2019; Attree, 2019) makes it hard to compare debiasing systems. For sentiment analysis, Equity Evaluation Corpus (EEC) (Kiritchenko and Mohammad, 2018) was designed to measure gender bias by the difference in emotional intensity predictions between gender-swapped sentences. Since the measures of gender bias in these datasets are not intuitive, we use association tests in this work. 5.2 Debiasing Methods Many methods to remove gender bias have been proposed. Based on the hypothesis that gender bias in stored in a linear subspace in the word embeddings, the most common way is to define a gender direction (or, more generally, a subspace) by a set of gendered words and debias the word embeddings in a post-processing projection. Bolukbasi et al."
2020.coling-main.446,W19-3823,0,0.0742256,"n contrast to the attention heads we observe a different debiasing effect across different layers. We see that the debiasing effect is stronger in layers 7–10 than in the other layers in bert-base. This indicates that gender information is processed mostly in the upper layers of BERT. Quantifying Gender Bias with DensRay DensRay can be used to quantify gender bias for sentences and tokens. We use the distance to the origin in gender subspace as the measure. In BERT, we use the average bias score of tokens to quantify the whole sentence. Table 6 compares DensRay with the log probability score (Kurita et al., 2019), which quantifies gender bias based on the template “[TARGET] is a [ATTRIBUTE]”. We regard zero as a balance point without bias. Contrary to the log probability score, a positive DensRay score represents the level of female bias. These examples show that DensRay is more versatile, it can quantify bias both on the token and on the sentence level4 in contrast to the log-probability score. In the sentence “The professor asked the nurse .” one can immediately see that the model has a male bias on “professor” and a female bias on “nurse”. 4 Although token-level bias scores seem to work quite well"
2020.coling-main.446,N19-1063,0,0.243495,"ngs exhibit The child played with the car . biases such as gender bias, which are present in their training The child played with the doll . corpora (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg Table 1: Token level bias scores et al., 2018). Contextual word embedding models, such as (male/female bias depicted by blue/red) BERT (Devlin et al., 2019), have become increasingly comidentified using DensRay. See Table 6 mon and achieved new state-of-the-art results in many NLP tasks. Researchers have also found gender bias in contextu- for more details. alized embeddings (Zhao et al., 2019; May et al., 2019). It has been found that these biases have a big impact on downstream tasks (Vanmassenhove et al., 2018; Moryossef et al., 2019; Rudinger et al., 2018; Zhao et al., 2018). A common approach for removing gender information in static embeddings is to identify a linear gender subspace (e.g., a gender direction) and subsequently setting all values on the gender direction to zero. Successful approaches rely on simple principal component analysis (Bolukbasi et al., 2016; Mu and Viswanath, 2018). Bolukbasi et al. (2016) require pairs of gendered words to compute a direction (e.g., “man”-“woman”) and"
2020.coling-main.446,W19-3807,0,0.0255918,"ed with the doll . corpora (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg Table 1: Token level bias scores et al., 2018). Contextual word embedding models, such as (male/female bias depicted by blue/red) BERT (Devlin et al., 2019), have become increasingly comidentified using DensRay. See Table 6 mon and achieved new state-of-the-art results in many NLP tasks. Researchers have also found gender bias in contextu- for more details. alized embeddings (Zhao et al., 2019; May et al., 2019). It has been found that these biases have a big impact on downstream tasks (Vanmassenhove et al., 2018; Moryossef et al., 2019; Rudinger et al., 2018; Zhao et al., 2018). A common approach for removing gender information in static embeddings is to identify a linear gender subspace (e.g., a gender direction) and subsequently setting all values on the gender direction to zero. Successful approaches rely on simple principal component analysis (Bolukbasi et al., 2016; Mu and Viswanath, 2018). Bolukbasi et al. (2016) require pairs of gendered words to compute a direction (e.g., “man”-“woman”) and Mu and Viswanath (2018) rely on computing a PCA of a set of gender words hoping that the main variation direction is the gender"
2020.coling-main.446,N18-1202,0,0.0289288,"g. Prost et al. (2019) proposed a variant of the hard debiasing algorithm by simply removing the gender subspace component of all words in the vocabulary, we also applied this strategy in our approach. Dev and Phillips (2019) explored partial projection and some simple tricks to improve the hard debiasing method. Wang et al. (2020) introduced Double-Hard Debias to ameliorate hard debiasing by mitigating the negative impact of the word frequency in training corpora. Zhao et al. (2019) applied the data augmentation and debiasing method of Bolukbasi et al. (2016) to mitigate gender bias on ELMo (Peters et al., 2018). Karve et al. (2019) proposed the debiasing conceptor, which shrinks each principal component of the covariance matrix of the embeddings to achieve a soft debiasing. They also introduced a simple and intuitive hard debiasing method proposed by (Mu and Viswanath, 2018), which identified the gender subspace by PCA and set the first principal component to zero. Recently research (Vargas and Cotterell, 2020) showed that gender bias is well captured by a linear subspace, justifying the basic hypothesis of the above methods. The debiasing conceptor and Mu and Viswanath (2018)’s hard debiasing produ"
2020.coling-main.446,W19-3810,0,0.0329539,"ace) by a set of gendered words and debias the word embeddings in a post-processing projection. Bolukbasi et al. (2016) propose (i) hard debiasing: use the gendered words to compute the difference embedding vector as the gender direction, and remove the gender subspace component of the neutral words while preserving it for the gendered words; and (ii) soft debiasing, a machine learning based method that combines the inner-products objective of word embedding and an objective to project the word embedding into an orthogonal gender subspace. It has been found to work better than soft debiasing. Prost et al. (2019) proposed a variant of the hard debiasing algorithm by simply removing the gender subspace component of all words in the vocabulary, we also applied this strategy in our approach. Dev and Phillips (2019) explored partial projection and some simple tricks to improve the hard debiasing method. Wang et al. (2020) introduced Double-Hard Debias to ameliorate hard debiasing by mitigating the negative impact of the word frequency in training corpora. Zhao et al. (2019) applied the data augmentation and debiasing method of Bolukbasi et al. (2016) to mitigate gender bias on ELMo (Peters et al., 2018)."
2020.coling-main.446,N18-2002,0,0.0507864,"Missing"
2020.coling-main.446,D18-1334,0,0.0286883,"heir training The child played with the doll . corpora (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg Table 1: Token level bias scores et al., 2018). Contextual word embedding models, such as (male/female bias depicted by blue/red) BERT (Devlin et al., 2019), have become increasingly comidentified using DensRay. See Table 6 mon and achieved new state-of-the-art results in many NLP tasks. Researchers have also found gender bias in contextu- for more details. alized embeddings (Zhao et al., 2019; May et al., 2019). It has been found that these biases have a big impact on downstream tasks (Vanmassenhove et al., 2018; Moryossef et al., 2019; Rudinger et al., 2018; Zhao et al., 2018). A common approach for removing gender information in static embeddings is to identify a linear gender subspace (e.g., a gender direction) and subsequently setting all values on the gender direction to zero. Successful approaches rely on simple principal component analysis (Bolukbasi et al., 2016; Mu and Viswanath, 2018). Bolukbasi et al. (2016) require pairs of gendered words to compute a direction (e.g., “man”-“woman”) and Mu and Viswanath (2018) rely on computing a PCA of a set of gender words hoping that the main variation"
2020.coling-main.446,2020.emnlp-main.232,0,0.0293433,"ating the negative impact of the word frequency in training corpora. Zhao et al. (2019) applied the data augmentation and debiasing method of Bolukbasi et al. (2016) to mitigate gender bias on ELMo (Peters et al., 2018). Karve et al. (2019) proposed the debiasing conceptor, which shrinks each principal component of the covariance matrix of the embeddings to achieve a soft debiasing. They also introduced a simple and intuitive hard debiasing method proposed by (Mu and Viswanath, 2018), which identified the gender subspace by PCA and set the first principal component to zero. Recently research (Vargas and Cotterell, 2020) showed that gender bias is well captured by a linear subspace, justifying the basic hypothesis of the above methods. The debiasing conceptor and Mu and Viswanath (2018)’s hard debiasing produce gender directions using a gendered word list mixed with male and female words. In contrast, Bolukbasi et al. (2016)’s hard debiasing used two groups of gendered words for definition and another two groups for alignment, to identify the gender direction by male-female pairs. The method we use, DensRay, is similar to Bolukbasi et al. (2016)’s hard debiasing in this aspect. However, DensRay uses only one"
2020.coling-main.446,W18-5446,0,0.028238,"indicates less gender bias. We also prefer a high p-value (at least 0.05) that aims to not reject the null hypothesis, i.e., we do not reject that there is no gender bias. We use the three categories C6: career/family, C7: math/arts, C8: science/arts, following Karve et al. (2019)’s WEAT setup and May et al. (2019)’s SEAT setup. 3.2.3 Model Performance It is crucial that debiasing methods do not harm the downstream performance of BERT models. Thus we test the perplexity of language modeling on Wikitext-2 (Merity et al., 2016), a subset of Wikipedia with 2 million words. We also test on GLUE (Wang et al., 2018), using the same setup as Wolf et al. (2019).3 Since the bias already exists in the data of these tasks (Babaeianjelodar et al., 2020), the overall performance will decrease after debiasing. Our expectation here is to cause less damage to the model performance after debiasing. 4 Results 4.1 Debiasing Results Table 2 gives results on OCCTMP. We show that DensRay can mitigate gender bias in BERT as measured by diff : bias between predicting he/she drops by a large margin (e.g., for bert-base from 1.98 to 0.36). The table indicates that DensRay outperforms the other two methods on OCCTMP. Note th"
2020.coling-main.446,2020.acl-main.484,0,0.0300202,"rving it for the gendered words; and (ii) soft debiasing, a machine learning based method that combines the inner-products objective of word embedding and an objective to project the word embedding into an orthogonal gender subspace. It has been found to work better than soft debiasing. Prost et al. (2019) proposed a variant of the hard debiasing algorithm by simply removing the gender subspace component of all words in the vocabulary, we also applied this strategy in our approach. Dev and Phillips (2019) explored partial projection and some simple tricks to improve the hard debiasing method. Wang et al. (2020) introduced Double-Hard Debias to ameliorate hard debiasing by mitigating the negative impact of the word frequency in training corpora. Zhao et al. (2019) applied the data augmentation and debiasing method of Bolukbasi et al. (2016) to mitigate gender bias on ELMo (Peters et al., 2018). Karve et al. (2019) proposed the debiasing conceptor, which shrinks each principal component of the covariance matrix of the embeddings to achieve a soft debiasing. They also introduced a simple and intuitive hard debiasing method proposed by (Mu and Viswanath, 2018), which identified the gender subspace by PC"
2020.coling-main.446,Q18-1042,0,0.0170685,"ty bias score to measure the association between targets and attributes in BERT. Since it can only be applied on specific templates, we compare this method with DensRay as a measure of gender bias in §4.4. Nadeem et al. (2020) proposed CAT (Context Association Test) to measure language modeling ability as well as the stereotypical bias of pretrained language models at sentence and discourse level. An alternative way to measure gender bias is to evaluate on downstream tasks. For coreference resolution, Zhao et al. (2018) designed Winobias and Rudinger et al. (2018) designed Winogender schemas. Webster et al. (2018) released GAP, a balanced corpus of Gendered Ambiguous Pronouns, which measures gender bias as the ratio of F1 score on masculine to F1 score on feminine. However, a ratio close to 1.0 (Chada, 2019; Attree, 2019) makes it hard to compare debiasing systems. For sentiment analysis, Equity Evaluation Corpus (EEC) (Kiritchenko and Mohammad, 2018) was designed to measure gender bias by the difference in emotional intensity predictions between gender-swapped sentences. Since the measures of gender bias in these datasets are not intuitive, we use association tests in this work. 5.2 Debiasing Methods"
2020.coling-main.446,N18-2003,0,0.134666,"016; Caliskan et al., 2017; Garg Table 1: Token level bias scores et al., 2018). Contextual word embedding models, such as (male/female bias depicted by blue/red) BERT (Devlin et al., 2019), have become increasingly comidentified using DensRay. See Table 6 mon and achieved new state-of-the-art results in many NLP tasks. Researchers have also found gender bias in contextu- for more details. alized embeddings (Zhao et al., 2019; May et al., 2019). It has been found that these biases have a big impact on downstream tasks (Vanmassenhove et al., 2018; Moryossef et al., 2019; Rudinger et al., 2018; Zhao et al., 2018). A common approach for removing gender information in static embeddings is to identify a linear gender subspace (e.g., a gender direction) and subsequently setting all values on the gender direction to zero. Successful approaches rely on simple principal component analysis (Bolukbasi et al., 2016; Mu and Viswanath, 2018). Bolukbasi et al. (2016) require pairs of gendered words to compute a direction (e.g., “man”-“woman”) and Mu and Viswanath (2018) rely on computing a PCA of a set of gender words hoping that the main variation direction is the gender direction. We propose to use DensRay (Duft"
2020.coling-main.446,N19-1064,0,0.0792617,"d that word embeddings exhibit The child played with the car . biases such as gender bias, which are present in their training The child played with the doll . corpora (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg Table 1: Token level bias scores et al., 2018). Contextual word embedding models, such as (male/female bias depicted by blue/red) BERT (Devlin et al., 2019), have become increasingly comidentified using DensRay. See Table 6 mon and achieved new state-of-the-art results in many NLP tasks. Researchers have also found gender bias in contextu- for more details. alized embeddings (Zhao et al., 2019; May et al., 2019). It has been found that these biases have a big impact on downstream tasks (Vanmassenhove et al., 2018; Moryossef et al., 2019; Rudinger et al., 2018; Zhao et al., 2018). A common approach for removing gender information in static embeddings is to identify a linear gender subspace (e.g., a gender direction) and subsequently setting all values on the gender direction to zero. Successful approaches rely on simple principal component analysis (Bolukbasi et al., 2016; Mu and Viswanath, 2018). Bolukbasi et al. (2016) require pairs of gendered words to compute a direction (e.g.,"
2020.coling-main.488,D15-1056,0,0.0126585,"ually simpler and faster, requires fewer hyperparameters – which can be crucial in a data-scarce scenario – and performs much better, especially for difficult tasks. For P ET, expert knowledge is mostly encoded in the mapping from a language model’s prediction to labels, which is why we focus on automating this part. The complementary problem of automatically transforming inputs before processing them with a language model has been studied by Jiang et al. (2019). This is also closely related to approaches for extracting patterns in relation extraction (Brin, 1999; Agichtein and Gravano, 2000; Batista et al., 2015; Bouraoui et al., 2020). 3 Pattern-Exploiting Training We review Pattern-Exploiting Training (P ET) as proposed by Schick and Sch¨utze (2020a). Let M be a pretrained masked language model (MLM), T its vocabulary and [MASK] ∈ T the mask token. We consider the task of mapping textual inputs x ∈ X to some label y ∈ Y where we assume w.l.o.g. that Y = {1, . . . , k} for some k ∈ N. In addition to training data T = {(x1 , y1 ), . . . , (xn , yn )}, P ET requires a set of pattern-verbalizer pairs (PVPs). As exemplified in Figure 1, each PVP p = (P, v) consists of • a pattern P that is used to conve"
2020.coling-main.488,D19-1109,0,0.0226759,"proach for identifying words that can serve as proxies for labels given small amounts of training data. At its core, our approach breaks the intractable problem of finding the mapping that maximizes the likelihood of the training data into several manageable subproblems. Integrating our approach into P ET significantly outperforms regular supervised training and almost matches the performance of P ET with a manually defined mapping. 2 Related Work Reformulating problems as language modeling tasks has been explored in fully unsupervised settings (Radford et al., 2019; Puri and Catanzaro, 2019; Davison et al., 2019), in few-shot scenarios with limited amounts of training data (Opitz, 2019; Shwartz et al., 2020; Brown et al., 2020), and even in high-resource settings (Raffel et al., 2019). The same idea is also commonly used for probing the knowledge contained within pretrained language models (Petroni et al., 2019; Talmor et al., 2019; Schick and Sch¨utze, 2020b; Ettinger, 2020, inter alia). 1 Our implementation is publicly available at https://github.com/timoschick/pet. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License det"
2020.coling-main.488,N19-1423,0,0.0371033,"em with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language model’s abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand-crafted label-to-word mappings.1 1 Introduction Pretraining language models on large corpora has led to improvements on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia), but learning to solve tasks from only a few examples remains a challenging problem. As small datasets are common for many realworld applications of NLP, solving this challenge is crucial to enable broad applicability. A promising direction for many tasks is to reformulate them (e.g., by appending an instruction such as “translate into French”) so that they can directly be solved by a pretrained language model (Radford et al., 2019; Schick and Sch¨utze, 2020a; Brown et al., 2020). The key idea of P ET (Schick and Sch¨utze, 2020a), one such approach aimed at text"
2020.coling-main.488,2020.tacl-1.3,0,0.022886,"hes the performance of P ET with a manually defined mapping. 2 Related Work Reformulating problems as language modeling tasks has been explored in fully unsupervised settings (Radford et al., 2019; Puri and Catanzaro, 2019; Davison et al., 2019), in few-shot scenarios with limited amounts of training data (Opitz, 2019; Shwartz et al., 2020; Brown et al., 2020), and even in high-resource settings (Raffel et al., 2019). The same idea is also commonly used for probing the knowledge contained within pretrained language models (Petroni et al., 2019; Talmor et al., 2019; Schick and Sch¨utze, 2020b; Ettinger, 2020, inter alia). 1 Our implementation is publicly available at https://github.com/timoschick/pet. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5569 Proceedings of the 28th International Conference on Computational Linguistics, pages 5569–5578 Barcelona, Spain (Online), December 8-13, 2020 qp (y |x) 1 Business [MASK] News: American Duo Wins Opening Beach Volleyball Match 2 World P (x) 3 Sports y v(y) x Figure 1: Exemplary application of a pattern-verbalizer pair p = (P, v): An input x is conver"
2020.coling-main.531,P19-1309,0,0.146474,"NEWS transliteration shared task has been a continuous effort to promote research on this task since 2009 (Li et al., 2009; Chen et al., 2018). A fully unsupervised transliteration model was proposed by Sajjad et al. (2017); it consists of interpolated statistical sub-models for transliteration and non-transliteration detection. In this work we follow a similar idea and propose an unsupervised neural network based system to look for transliteration pairs of source words as possible translation candidates. We also use this system to build BOEs of words. In a parallel sentence mining approach, Artetxe and Schwenk (2019) use a shared encoder and decoder for all languages to build a language agnostic sentence encoder. They use the encoder representations as sentence embeddings to efficiently mine parallel sentences. Similarly, our BOEs are extracted from a single language agnostic encoder, for both English and Russian. In an ablation study, we check the quality of our BOEs on the NEWS 2010 shared task (Kumaran et al., 2010); see below. 3 Approach To tackle the BDI task we exploit BWEs, character-level information (in the form of BOEs) and manually engineered features – such as word frequency and length – and i"
2020.coling-main.531,P17-1042,0,0.0246018,"1 Introduction The task of Bilingual Dictionary Induction is defined as finding target language translations of source language words. It is an important building block in the area of Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016). Recent work shows that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017). In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018), making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019). Standard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space. Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities for which orthographic information"
2020.coling-main.531,P18-1073,0,0.338793,"f source language words. It is an important building block in the area of Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016). Recent work shows that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017). In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018), making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019). Standard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space. Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities for which orthographic information should be used instead of semantic information. Several approaches have integrated orthographic information in"
2020.coling-main.531,P19-1019,0,0.0860492,"(MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016). Recent work shows that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017). In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018), making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019). Standard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space. Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities for which orthographic information should be used instead of semantic information. Several approaches have integrated orthographic information into the BDI system. Heyman et al. (2017) relied on character-level information in their classif"
2020.coling-main.531,Q17-1010,0,0.232504,"ed relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017). In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018), making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019). Standard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space. Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities for which orthographic information should be used instead of semantic information. Several approaches have integrated orthographic information into the BDI system. Heyman et al. (2017) relied on character-level information in their classification based BDI system by using an RNN architecture. Braune et al. (2018) combined orthographic information with BWE-based word similarity information using an ensembling method. Bot"
2020.coling-main.531,N18-2030,1,0.893548,"roaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space. Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities for which orthographic information should be used instead of semantic information. Several approaches have integrated orthographic information into the BDI system. Heyman et al. (2017) relied on character-level information in their classification based BDI system by using an RNN architecture. Braune et al. (2018) combined orthographic information with BWE-based word similarity information using an ensembling method. Both of these approaches showed improved results, but they relied on Levenshtein distance to get translation candidates for a source word during prediction, which is not applicable for language pairs with different scripts. To bridge the gap between languages with different scripts, a transliteration system was employed by Severini et al. (2020). They followed the approach of Braune et al. (2018) but used the transliteration system instead of Levenshtein distance to get candidates used in"
2020.coling-main.531,W18-2409,0,0.020488,"rvine and Callison-Burch (2017) applied Levenshtein distance to language pairs with different 6045 alphabets by first transliterating from non-Latin to Latin scripts. In contrast, we use a novel transliteration model that encodes the relevant information directly into BOEs without requiring a separate transliteration step. 2.2 Transliteration As motivated above, transliteration mining is an important task that bridges the gap between languages with different scripts. The NEWS transliteration shared task has been a continuous effort to promote research on this task since 2009 (Li et al., 2009; Chen et al., 2018). A fully unsupervised transliteration model was proposed by Sajjad et al. (2017); it consists of interpolated statistical sub-models for transliteration and non-transliteration detection. In this work we follow a similar idea and propose an unsupervised neural network based system to look for transliteration pairs of source words as possible translation candidates. We also use this system to build BOEs of words. In a parallel sentence mining approach, Artetxe and Schwenk (2019) use a shared encoder and decoder for all languages to build a language agnostic sentence encoder. They use the encod"
2020.coling-main.531,D19-1090,0,0.0114955,"ective when only a small seed lexicon is provided (e.g., (Mikolov et al., 2013b)). Conneau et al. (2018) and Artetxe et al. (2018) dispense with seed dictionaries and iteratively improve the mapping from an initial weak solution in a self-learning approach. This setting provides a building block for unsupervised MT and is particularly effective in the low-resource setting where less parallel seed data is available (Artetxe et al., 2019; Lample et al., 2018). BWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). Koehn and Knight (2002) and Haghighi et al. (2008) show that orthographic features help in the translation process. Languages with a common alphabet (e.g., English/German) often have word pairs with similar orthography (e.g., Concepts/Konzepte, Philosophies/Philosophien), especially in the case of low frequency words. Riley and Gildea (2018) integrate orthographic information into the vector representation of such words and into the mapping procedure of BWEs to improve their quality. Braune et al. (2018) use character n-gram representations and Levenshtein distance to improve BDI while Heyma"
2020.coling-main.531,P08-1088,0,0.0750546,"., (Mikolov et al., 2013b)). Conneau et al. (2018) and Artetxe et al. (2018) dispense with seed dictionaries and iteratively improve the mapping from an initial weak solution in a self-learning approach. This setting provides a building block for unsupervised MT and is particularly effective in the low-resource setting where less parallel seed data is available (Artetxe et al., 2019; Lample et al., 2018). BWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). Koehn and Knight (2002) and Haghighi et al. (2008) show that orthographic features help in the translation process. Languages with a common alphabet (e.g., English/German) often have word pairs with similar orthography (e.g., Concepts/Konzepte, Philosophies/Philosophien), especially in the case of low frequency words. Riley and Gildea (2018) integrate orthographic information into the vector representation of such words and into the mapping procedure of BWEs to improve their quality. Braune et al. (2018) use character n-gram representations and Levenshtein distance to improve BDI while Heyman et al. (2017) extract this feature automatically f"
2020.coling-main.531,E17-1102,0,0.0376756,"Missing"
2020.coling-main.531,J17-2001,0,0.0196008,"hographic information into the vector representation of such words and into the mapping procedure of BWEs to improve their quality. Braune et al. (2018) use character n-gram representations and Levenshtein distance to improve BDI while Heyman et al. (2017) extract this feature automatically from training data. In languages with different scripts (e.g., English/Russian), the source word is often written with the closest corresponding letters of the target alphabet, i.e., it is transliterated. Richard/Ричард and integrator/интегратор are examples of transliterations between English and Russian. Irvine and Callison-Burch (2017) applied Levenshtein distance to language pairs with different 6045 alphabets by first transliterating from non-Latin to Latin scripts. In contrast, we use a novel transliteration model that encodes the relevant information directly into BOEs without requiring a separate transliteration step. 2.2 Transliteration As motivated above, transliteration mining is an important task that bridges the gap between languages with different scripts. The NEWS transliteration shared task has been a continuous effort to promote research on this task since 2009 (Li et al., 2009; Chen et al., 2018). A fully uns"
2020.coling-main.531,W10-2405,0,0.0951717,"Missing"
2020.coling-main.531,2020.acl-main.618,0,0.0951141,"Missing"
2020.coling-main.531,W02-0902,0,0.282156,"eed lexicon is provided (e.g., (Mikolov et al., 2013b)). Conneau et al. (2018) and Artetxe et al. (2018) dispense with seed dictionaries and iteratively improve the mapping from an initial weak solution in a self-learning approach. This setting provides a building block for unsupervised MT and is particularly effective in the low-resource setting where less parallel seed data is available (Artetxe et al., 2019; Lample et al., 2018). BWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). Koehn and Knight (2002) and Haghighi et al. (2008) show that orthographic features help in the translation process. Languages with a common alphabet (e.g., English/German) often have word pairs with similar orthography (e.g., Concepts/Konzepte, Philosophies/Philosophien), especially in the case of low frequency words. Riley and Gildea (2018) integrate orthographic information into the vector representation of such words and into the mapping procedure of BWEs to improve their quality. Braune et al. (2018) use character n-gram representations and Levenshtein distance to improve BDI while Heyman et al. (2017) extract t"
2020.coling-main.531,W10-2404,0,0.281312,"0 shared task (Rapp et al., 2020). Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches. Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set. Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words. We show good performance on the task indicating the usefulness of BOEs for other downstream tasks. 2 2.1 Related Work Bilingual Dictionary Induction BWEs are often used for solving BDI tasks by calculating cosine similarity of word pairs and taking the n most similar target candidates as translations for a source word. As opposed to general MT based approaches that rely on parallel sentences, BWEs are also effective when only a small seed lexicon is provided (e.g., (Mikolov et al., 2013b)). Conneau et al. (2018) and"
2020.coling-main.531,D18-1549,0,0.100146,"Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016). Recent work shows that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017). In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018), making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019). Standard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space. Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities for which orthographic information should be used instead of semantic information. Several approaches have integrated orthographic information into the BDI system. Heyman et al. (2017) relied on character-level infor"
2020.coling-main.531,W09-3501,0,0.0498586,"sh and Russian. Irvine and Callison-Burch (2017) applied Levenshtein distance to language pairs with different 6045 alphabets by first transliterating from non-Latin to Latin scripts. In contrast, we use a novel transliteration model that encodes the relevant information directly into BOEs without requiring a separate transliteration step. 2.2 Transliteration As motivated above, transliteration mining is an important task that bridges the gap between languages with different scripts. The NEWS transliteration shared task has been a continuous effort to promote research on this task since 2009 (Li et al., 2009; Chen et al., 2018). A fully unsupervised transliteration model was proposed by Sajjad et al. (2017); it consists of interpolated statistical sub-models for transliteration and non-transliteration detection. In this work we follow a similar idea and propose an unsupervised neural network based system to look for transliteration pairs of source words as possible translation candidates. We also use this system to build BOEs of words. In a parallel sentence mining approach, Artetxe and Schwenk (2019) use a shared encoder and decoder for all languages to build a language agnostic sentence encoder"
2020.coling-main.531,D15-1166,0,0.016458,"requent words such as named entities, we propose seq2seqTr, a novel transliteration system. seq2seqTr is trained on a list of word pairs that are translations of one another – both transliterations and non-transliterations. It is unsupervised because we do not rely on labels to distinguish between transliteration and non-transliteration training pairs. seq2seqTr is a character-level sequence-to-sequence model (Sutskever et al., 2014) with a single-layer encoder and a single-layer decoder. The encoder is a bidirectional GRU (Cho et al., 2014) while the decoder is unidirectional with attention (Luong et al., 2015). The input characters are represented as vectors. Figure 1 (bottom) depicts the model. We use this model to calculate the probability of each word in the target language vocabulary with respect to each source test word. The probability corresponds to the average negative log likelihood of the characters in the target word with respect to the source word. We select n target transliteration candidates for each source word. To train seq2seqTr, we start with the same training dictionary as for building BWEs. Since it contains many non-transliteration pairs, we reduce their number with an iterativ"
2020.coling-main.531,2020.bucc-1.2,0,0.0251298,"sliterated (which means we shoud primarily trust the BOEs) and which should be semantically translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017), exploiting our pretrained encoder from seq2seqTr. In contrast to their approach, we use additional features, such as frequency, length, similarity scores, and the ranks assigned by the semantic and character-level submodels, and show that they are necessary to make the right decision. We test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020). Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches. Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set. Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using th"
2020.coling-main.531,P18-2062,0,0.0170745,"ences, BWEs are also effective when only a small seed lexicon is provided (e.g., (Mikolov et al., 2013b)). Conneau et al. (2018) and Artetxe et al. (2018) dispense with seed dictionaries and iteratively improve the mapping from an initial weak solution in a self-learning approach. This setting provides a building block for unsupervised MT and is particularly effective in the low-resource setting where less parallel seed data is available (Artetxe et al., 2019; Lample et al., 2018). BWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). Koehn and Knight (2002) and Haghighi et al. (2008) show that orthographic features help in the translation process. Languages with a common alphabet (e.g., English/German) often have word pairs with similar orthography (e.g., Concepts/Konzepte, Philosophies/Philosophien), especially in the case of low frequency words. Riley and Gildea (2018) integrate orthographic information into the vector representation of such words and into the mapping procedure of BWEs to improve their quality. Braune et al. (2018) use character n-gram representations and Levenshtein distance"
2020.coling-main.531,J17-2003,1,0.900897,"Missing"
2020.coling-main.531,2020.bucc-1.8,1,0.745542,"Missing"
2020.coling-main.531,P16-1024,0,0.0858517,"sed BDI system that uses BWEs, BOEs and a number of other features to make this decision. We test our system on EnglishRussian BDI and show improved performance. In addition, we show the effectiveness of our BOEs by successfully using them for transliteration mining based on cosine similarity. 1 Introduction The task of Bilingual Dictionary Induction is defined as finding target language translations of source language words. It is an important building block in the area of Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016). Recent work shows that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017). In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018), making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019). Standard BDI learns word representations based on approaches that exploit solely word-level information"
2020.conll-1.45,N19-1423,0,0.0288891,"but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success. 1 Introduction Pretrained language models (PLMs) like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) and RoBERTa (Liu et al., 2019) have emerged as universal tools that capture a diverse range of linguistic and – as more and more evidence suggests – factual knowledge (Petroni et al., 2019; Radford et al., 2019). Recent work on knowledge captured by PLMs is focused on probing, a methodology that identifies the set of facts a PLM has command of. But little is understood about how this knowledge is acquired during pretraining and why. We analyze the ability of PLMs to acquire factual knowledge focusing on two mechanisms: reasoning and memorization. We pose the foll"
2020.conll-1.45,N19-1246,0,0.0182896,"ting logic and monotonicity reasoning. They show that BERT performs poorly on these new datasets, but can be quickly finetuned to good performance. The difference to our work again is that they make the premise available to the model at inference time. For complex reasoning QA benchmarks (Yang 559 et al., 2018; Sinha et al., 2019), PLMs are finetuned to the downstream tasks. Their performance is difficult to analyze: it is not clear whether any reasoning capability is learned by the PLM or by the task specific component. Another line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018; Dua et al., 2019; McCoy et al., 2019) shows that much of PLMs’ performance on reasoning tasks is due to statistical artifacts in datasets and does not exhibit true reasoning and generalization capabilities. With the help of synthetic corpora, we can cleanly investigate PLMs’ reasoning capabilities. Hupkes et al. (2020) study the ability of neural models to capture compositionality. They do not investigate our six rules, nor do they consider the effects of fact frequency and schema conformity. Our work confirms their finding that transformers have the ability to capture both rules and exceptions. A large body"
2020.conll-1.45,2020.tacl-1.3,0,0.0683302,"Missing"
2020.conll-1.45,2020.acl-main.177,0,0.093943,"Missing"
2020.conll-1.45,N18-2017,0,0.0534802,"Missing"
2020.conll-1.45,2021.ccl-1.108,0,0.109323,"Missing"
2020.conll-1.45,P19-1334,0,0.0239679,"otonicity reasoning. They show that BERT performs poorly on these new datasets, but can be quickly finetuned to good performance. The difference to our work again is that they make the premise available to the model at inference time. For complex reasoning QA benchmarks (Yang 559 et al., 2018; Sinha et al., 2019), PLMs are finetuned to the downstream tasks. Their performance is difficult to analyze: it is not clear whether any reasoning capability is learned by the PLM or by the task specific component. Another line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018; Dua et al., 2019; McCoy et al., 2019) shows that much of PLMs’ performance on reasoning tasks is due to statistical artifacts in datasets and does not exhibit true reasoning and generalization capabilities. With the help of synthetic corpora, we can cleanly investigate PLMs’ reasoning capabilities. Hupkes et al. (2020) study the ability of neural models to capture compositionality. They do not investigate our six rules, nor do they consider the effects of fact frequency and schema conformity. Our work confirms their finding that transformers have the ability to capture both rules and exceptions. A large body of research in psycho"
2020.conll-1.45,D19-1250,0,0.0422936,"Missing"
2020.conll-1.45,D19-1458,0,0.0912209,"019; Zhang et al., 2020) has shown that models that are able to learn symbolic rules are superior to ones that are not. Talmor et al. (2019) also investigate symbolic reasoning in BERT using cloze-style queries. However, in their setup, there are two possible reasons for BERT having answered a cloze-style query correctly: (i) the underlying fact was correctly inferred or (ii) it was seen during training. In contrast, since we pretrain BERT from scratch, we have full control over the training setup and can distinguish cases (i) and (ii). A unique feature of our approach compared to prior work (Sinha et al., 2019; Richardson et al., 2020; Weston et al., 2016; Clark et al., 2020) is that we do not gather all relevant facts and present them to the model at inference time. This is a crucial 552 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 552–564 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Rule EQUI SYM INV NEG IMP COMP Equivalence Symmetry Inversion Negation Implication Composition Definition (e, r, a) ⇐⇒ (e, s, a) (e, r, f ) ⇐⇒ (f, r, e) (e, r, f ) ⇐⇒ (f, s, e) (e, r, a) ⇐⇒ (e, not r, b) (e, r, a)"
2020.conll-1.45,2020.acl-main.543,0,0.031288,". Hupkes et al. (2020) study the ability of neural models to capture compositionality. They do not investigate our six rules, nor do they consider the effects of fact frequency and schema conformity. Our work confirms their finding that transformers have the ability to capture both rules and exceptions. A large body of research in psychology and cognitive science has investigated how some of our rules are processed in humans, e.g., Sloman (1996) for implication. There is also a lively debate in cognitive science as to how important rule-based reasoning is for human cognition (Politzer, 2007). Yanaka et al. (2020); Goodwin et al. (2020) are concurrent studies of systematicity in PLMs. The first shows that monotonicity inference is feasible for syntactic structures close to the ones observed during training. The latter shows that PLMs can exhibit high over-all performance on natural language inference despite being non-systematic. Roberts et al. (2020) show that the amount of knowledge captured by PLMs increases with model size. Our memorization experiments investigate the factors that determine successful acquisition of knowledge. Guu et al. (2020) modify the PLM objective to incentivize knowledge acqu"
2020.conll-1.45,D18-1259,0,0.0801742,"Missing"
2020.emnlp-main.174,W02-1001,0,0.141927,"has effectively encoded context-independent general meanings of words in the embedding layer (Zhao et al., 2020). Hence, learning a selective mask for this layer is unnecessary. Also, we do not learn masks for biases and layer normalization parameters as we did not observe a positive effect on performance. 4 Datasets and Setup Datasets. We present results for masking BERT, RoBERTa, and DistilBERT in part-of-speech tagging, named-entity recognition, sequence classification, and reading comprehension. We experiment with part-of-speech tagging (POS) on Penn Treebank (Marcus et al., 1993), using Collins (2002)’s train/dev/test split. For named-entity recognition (NER), we conduct experiments on the CoNLL-2003 NER shared task (Tjong Kim Sang and De Meulder, 2003). For sequence classification, the following GLUE tasks (Wang et al., 2018) are evaluated: Stanford Sentiment Treebank (SST2) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005), Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Recognizing Textual Entailment (RTE) (Dagan et al., 2005), and Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016). In addition, we experimen"
2020.emnlp-main.174,D19-1445,0,0.0136199,"layers and downstream tasks. For the initial and trained binary masks Mt,init and Mt,trained of a layer trained on bin bin , 1 P Pn where kWk1 = m i=1 j=1 |wi,j |. Note that for the same random seed, Mt1,init and Mt2,init are bin bin the same. The dissimilarity s measures the difference between two masks as a fraction of all changes brought about by training. Figure 5 shows that, after training, the dissimilarities of masks of higher BERT layers are larger than those of lower BERT layers. Similar observations are made for finetuning: top layer weights in finetuned BERT are more task-specific (Kovaleva et al., 2019). The figure also shows that the learned masks for downstream tasks tend to be dissimilar to each other, even for similar tasks. For a given task, there exist different sets of masks (initialized with different random seeds) yielding similar performance. This observation is similar to the results of evaluating the lottery ticket hypothesis on BERT (Prasanna et al., 2020; Chen et al., 2020): a number of subnetworks exist in BERT achieving similar task performance. 6.3 Loss landscape Training complex neural networks can be viewed as searching for good minima in the highly nonconvex landscape def"
2020.emnlp-main.174,N19-1112,0,0.3696,"yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning. 1 Introduction Finetuning a large pretrained language model like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), and XLNet (Yang et al., 2019) often yields competitive or even state-of-the-art results on NLP benchmarks (Wang et al., 2018, 2019). Given an NLP task, standard finetuning stacks a linear layer on top of the pretrained language model and then updates all parameters using mini-batch SGD. Various aspects like brittleness (Dodge et al., 2020) and adaptiveness (Peters et al., 2019) of this two-stage transfer learning NLP paradigm (Dai and Le, 2015; Howard and Ruder, 2018) have been studied. Despite the simplicity and impressive performance of finetuning, the prohibitively large number of param"
2020.emnlp-main.174,S16-1001,0,0.182249,"icrosoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005), Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Recognizing Textual Entailment (RTE) (Dagan et al., 2005), and Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016). In addition, we experiment on sequence classification datasets that have publicly available test sets: the 6-class question classification dataset TREC (Voorhees and Tice, 2000), the 4-class news classification dataset AG News (AG) (Zhang et al., 2015), and the binary Twitter sentiment classification task SemEval-2016 4B (SEM) (Nakov et al., 2016). We experiment with reading comprehension on SWAG (Zellers et al., 2018) using the official data splits. We report Matthew’s correlation coefficient (MCC) for CoLA, micro-F1 for NER, and accuracy for the other tasks. Setup. Due to resource limitations and in the spirit of environmental responsibility (Strubell et al., 2019; Schwartz et al., 2019), we conduct our experiments on the base models: BERT-baseuncased, RoBERTa-base, and DistilBERT-baseuncased. Thus, the BERT/RoBERTa models we use have 12 transformer blocks (0–11 indexed) producing 768-dimension vectors; the DistilBERT model we use ha"
2020.emnlp-main.174,P19-1459,0,0.0679391,"Missing"
2020.emnlp-main.174,S16-1023,0,0.0615922,"Missing"
2020.emnlp-main.174,voorhees-tice-2000-trec,0,0.306788,"Sang and De Meulder, 2003). For sequence classification, the following GLUE tasks (Wang et al., 2018) are evaluated: Stanford Sentiment Treebank (SST2) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005), Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Recognizing Textual Entailment (RTE) (Dagan et al., 2005), and Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016). In addition, we experiment on sequence classification datasets that have publicly available test sets: the 6-class question classification dataset TREC (Voorhees and Tice, 2000), the 4-class news classification dataset AG News (AG) (Zhang et al., 2015), and the binary Twitter sentiment classification task SemEval-2016 4B (SEM) (Nakov et al., 2016). We experiment with reading comprehension on SWAG (Zellers et al., 2018) using the official data splits. We report Matthew’s correlation coefficient (MCC) for CoLA, micro-F1 for NER, and accuracy for the other tasks. Setup. Due to resource limitations and in the spirit of environmental responsibility (Strubell et al., 2019; Schwartz et al., 2019), we conduct our experiments on the base models: BERT-baseuncased, RoBERTa-base"
2020.emnlp-main.174,W18-5446,0,0.0545341,"Missing"
2020.emnlp-main.174,Q19-1040,0,0.0381948,"Missing"
2020.emnlp-main.174,D18-1009,0,0.026559,"Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Recognizing Textual Entailment (RTE) (Dagan et al., 2005), and Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016). In addition, we experiment on sequence classification datasets that have publicly available test sets: the 6-class question classification dataset TREC (Voorhees and Tice, 2000), the 4-class news classification dataset AG News (AG) (Zhang et al., 2015), and the binary Twitter sentiment classification task SemEval-2016 4B (SEM) (Nakov et al., 2016). We experiment with reading comprehension on SWAG (Zellers et al., 2018) using the official data splits. We report Matthew’s correlation coefficient (MCC) for CoLA, micro-F1 for NER, and accuracy for the other tasks. Setup. Due to resource limitations and in the spirit of environmental responsibility (Strubell et al., 2019; Schwartz et al., 2019), we conduct our experiments on the base models: BERT-baseuncased, RoBERTa-base, and DistilBERT-baseuncased. Thus, the BERT/RoBERTa models we use have 12 transformer blocks (0–11 indexed) producing 768-dimension vectors; the DistilBERT model we use has the same dimension but contains 6 transformer blocks (0–5 indexed). We"
2020.emnlp-main.174,I05-5002,0,\N,Missing
2020.emnlp-main.174,J93-2004,0,\N,Missing
2020.emnlp-main.174,D13-1170,0,\N,Missing
2020.emnlp-main.174,E17-1005,0,\N,Missing
2020.emnlp-main.174,E17-2026,0,\N,Missing
2020.emnlp-main.174,P19-1452,0,\N,Missing
2020.emnlp-main.174,N19-1423,0,\N,Missing
2020.emnlp-main.174,P19-1595,0,\N,Missing
2020.emnlp-main.174,P19-1441,0,\N,Missing
2020.emnlp-main.174,D19-1424,0,\N,Missing
2020.emnlp-main.316,W19-4825,0,0.0152114,"ilar representations. Taken together, these findings provide further evidence for the crucial importance of morphologically valid segmentation strategies in language model pretraining (Bostrom and Durrett, 2020). 6 Related Work PLMs such as ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019), and BERT (Devlin et al., 2019) have been the focus of much recent work in NLP. Several studies have been devoted to the linguistic knowledge encoded by the parameters of PLMs (see Rogers et al. (2020) for a review), particularly syntax (Goldberg, 2019; Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019) and semantics (Ethayarajh, 2019; Wiedemann et al., 2019; Ettinger, 2020). There is also a recent study examining morphosyntactic information in a PLM, specifically BERT (Edmiston, 2020). There has been relatively little recent work on derivational morphology in NLP. Both Cotterell et al. (2017) and Deutsch et al. (2018) propose neural architectures that represent derivational meanings as tags. More closely related to our study, Vylomova et al. (2017) develop an encoder-decoder model that uses the context sentence for predicting deverbal nouns. Hofmann et al. (2020b) propose a graph auto-encod"
2020.emnlp-main.316,W17-5405,0,0.0220122,"Missing"
2020.emnlp-main.316,E17-2019,0,0.149101,"auto, contra, extra, hyper, mega, mini, multi, non, proto, pseudo ##able, ##an, ##ate, ##ee, ##ess, ##ful, ##ify, ##ize, ##ment, ##ness, ##ster Table 1: Examples of derivational affixes in the BERT WordPiece vocabulary. Word-internal WordPiece tokens are marked with ## throughout the paper. these subword units are derivational affixes suggests that PLMs might acquire knowledge about derivational morphology (Table 1), but this has not been tested. On the other hand, we are interested in derivation generation (DG) per se, a task that has been only addressed using LSTMs (Cotterell et al., 2017; Vylomova et al., 2017; Deutsch et al., 2018), not models based on Transformers like BERT. Contributions. We develop the first framework for generating derivationally complex English words with a PLM, specifically BERT, and analyze BERT’s performance in different settings. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms an LSTM-based model, the previous state of the art. 3848 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3848–3861, c November 16–20, 2020. 2020 Association for Computational Linguistics We find that DagoBER"
2020.emnlp-main.316,D14-1162,0,0.0861315,"Missing"
2020.emnlp-main.316,N18-1202,0,0.246851,"thermore, our experiments show that the input segmentation crucially impacts BERT’s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used. this jacket is [MASK] wear [MASK] . Figure 1: Basic experimental setup. We input sentences such as this jacket is unwearable . to BERT, mask out derivational affixes, and recover them using a derivational classification layer (DCL). Type 1 ##able Introduction Prefixes What kind of linguistic knowledge is encoded by pretrained language models (PLMs) such as ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019), and BERT (Devlin et al., 2019)? This question has attracted a lot of attention in NLP recently, with a focus on syntax (e.g., Goldberg, 2019) and semantics (e.g., Ethayarajh, 2019). It is much less clear what PLMs learn about other aspects of language. Here, we present the first study on the knowledge of PLMs about derivational morphology, taking BERT as the example PLM. Given an English cloze sentence such as this jacket is . and a base such as wear, we ask: can BERT generate correct derivatives such as unwearable? The motivation for this study is twofold. On t"
2020.emnlp-main.316,W18-5814,1,0.835312,"Sch¨utze, 2020). Pretrained BERT shows similar confusion patterns overall but overgenerates several affixes much more strongly than DagoBERT, in particular re, non, y, ly, and er, which are among the most productive affixes in English (Plag, 1999, 2003). To probe the impact of productivity more quantitatively, we measure the cardinality of the set of hapaxes formed by means of a particular affix a in the entire dataset, |Ha |, and calculate a linear regression to predict the MRR values of affixes based on |Ha |. |Ha |is a common measure of morphological productivity (Baayen and Lieber, 1991; Pierrehumbert and Granell, 2018). This analysis shows a significant positive correlation for both prefixes (R2 = .566, F (1, 43) = 56.05, p &lt; .001) and suffixes (R2 = .410, F (1, 41) = 28.49, p &lt; .001): the more productive an affix, the higher its MRR value. This also holds for DagoBERT’s predictions of prefixes (R2 = .423, F (1, 43) = 31.52, p &lt; .001) and suffixes (R2 = .169, F (1, 41) = 8.34, p &lt; .01), but the correlation is weaker, particularly in the case of suffixes (Figure 4). 5.3 Impact of Input Segmentation We have shown that BERT can generate derivatives if it is provided with the morphologically correct segmentatio"
2020.emnlp-main.358,2020.acl-main.421,0,0.32818,"del is multilingual BERT1 (mBERT). It is a BERT-Base model (Devlin et al., 2019) trained on the 104 largest Wikipedias with a shared subword vocabulary. There is no additional crosslingual signal. Still, mBERT yields highquality multilingual representations (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). The exact reason for mBERT’s multilinguality is – to the best of our knowledge – still debated. K et al. (2020) provide an extensive study and conclude that a shared vocabulary is not necessary, but that the model needs to be deep and languages need to share a similar “structure”. Artetxe et al. (2020) show that neither a shared vocabulary nor joint pretraining is required for BERT to be multilingual. Conneau et al. (2020b) find that BERT models across languages can be easily aligned and 1 https://github.com/google-research/ bert/blob/master/multilingual.md 4423 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4423–4437, c November 16–20, 2020. 2020 Association for Computational Linguistics that a necessary requirement for achieving multilinguality are shared parameters in the top layers. This work continues this line of research. We find indicat"
2020.emnlp-main.358,Q19-1038,0,0.0291648,"sional wordpiece embeddings from the l-th layer of BERT to obtain embeddings E(s(k) ) ∈ Rn×d for k ∈ {eng, fake}. The similarity matrix S ∈ [0, 1]n×n is computed by  Sij := cosine-sim E(s(eng) )i , E(s(fake) )j . Two wordpieces i and j are aligned if (i = arg max Sl,j ) ∧ (j = arg max Si,l ). l l The alignments are evaluated using precision, recall and F1 as follows: p= |P ∩ G| |P ∩ G| 2pr , r= , F1 = , |P | |G| p+r where P is the set of predicted alignments and G the set of true alignment edges. We report F1 . Sentence Retrieval is popular for evaluating crosslingual representations (e.g., (Artetxe and Schwenk, 2019; Libovick`y et al., 2019)). We obtain the embeddings E(s(k) ) as before and compute (k) a sentence embedding es simply by averaging vectors across all tokens in a sentence (ignoring CLS and SEP tokens). Computing cosine similarities between English and Fake-English sentences yields the similarity matrix R ∈ Rm×m where (eng) (fake) Rij = cosine-sim(ei , ej ) for m sentences. (eng) Given an English query sentence si , we obtain the retrieved sentences in Fake-English by ranking them according to similarity. Since we can do the same with Fake-English as query language, we report the mean precisi"
2020.emnlp-main.358,P11-1103,0,0.0223195,"Missing"
2020.emnlp-main.358,Q17-1010,0,0.105952,"Missing"
2020.emnlp-main.358,2021.naacl-main.280,0,0.591265,"Missing"
2020.emnlp-main.358,2020.acl-main.747,0,0.119615,"Missing"
2020.emnlp-main.358,D18-1269,0,0.124933,"Missing"
2020.emnlp-main.358,N19-1392,0,0.0195033,"lar structure and thus conclude that mBERT somehow aligns those isomorphic spaces. They investigate having separate embedding look-ups per language (including position embeddings and special tokens) and a variant of avoiding cross-language replacements. Their method “extra anchors” yields a higher degree of multilinguality. In contrast to this prior work, we investigate multilinguality in a clean laboratory setting, investigate the interaction of architectural aspects and research new aspects such as overparameterization or inv-order. Other work focuses on creating better multilingual models. Mulcaire et al. (2019) proposed a method to learn multilingual contextual representations. Conneau and Lample (2019) introduce the translation modeling objective. Conneau et al. (2020a) propose XLM-R. They introduce the term “curse of multilinguality” and show that multilingual model quality degrades with an increased number of languages given a fixed number of parameters. This can be interpreted as the minimum number of parameters required whereas we find indications that models that are too large can be harmful for multilinguality as well. Cao et al. (2020) improve the multilinguality of mBERT by introducing a re"
2020.emnlp-main.358,N18-1202,0,0.0439533,"models need to be maintained to serve many languages, resource requirements are reduced, and low- and mid-resource languages can benefit from crosslingual transfer. Further, multilingual models are useful in machine translation, zero-shot task transfer and typological research. There is a clear need for multilingual models for the world’s 7000+ languages. With the rise of static word embeddings, many multilingual embedding algorithms have been proposed (Mikolov et al., 2013; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014); for a survey see (Ruder et al., 2019). Pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) have high performance across tasks, outperforming static word embeddings. A simple multilingual model is multilingual BERT1 (mBERT). It is a BERT-Base model (Devlin et al., 2019) trained on the 104 largest Wikipedias with a shared subword vocabulary. There is no additional crosslingual signal. Still, mBERT yields highquality multilingual representations (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). The exact reason for mBERT’s multilinguality is – to the best of our knowledge – still debated. K et al. (2020) provide an extensive stud"
2020.emnlp-main.358,2020.emnlp-main.617,0,0.061309,"Missing"
2020.emnlp-main.358,P19-1493,0,0.0798002,"ding algorithms have been proposed (Mikolov et al., 2013; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014); for a survey see (Ruder et al., 2019). Pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) have high performance across tasks, outperforming static word embeddings. A simple multilingual model is multilingual BERT1 (mBERT). It is a BERT-Base model (Devlin et al., 2019) trained on the 104 largest Wikipedias with a shared subword vocabulary. There is no additional crosslingual signal. Still, mBERT yields highquality multilingual representations (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). The exact reason for mBERT’s multilinguality is – to the best of our knowledge – still debated. K et al. (2020) provide an extensive study and conclude that a shared vocabulary is not necessary, but that the model needs to be deep and languages need to share a similar “structure”. Artetxe et al. (2020) show that neither a shared vocabulary nor joint pretraining is required for BERT to be multilingual. Conneau et al. (2020b) find that BERT models across languages can be easily aligned and 1 https://github.com/google-research/ bert/blob/master/multilingua"
2020.emnlp-main.358,W19-6204,0,0.0338594,"Missing"
2020.emnlp-main.358,2020.findings-emnlp.147,1,0.88524,"Missing"
2020.emnlp-main.358,D19-1077,0,0.140607,"been proposed (Mikolov et al., 2013; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014); for a survey see (Ruder et al., 2019). Pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) have high performance across tasks, outperforming static word embeddings. A simple multilingual model is multilingual BERT1 (mBERT). It is a BERT-Base model (Devlin et al., 2019) trained on the 104 largest Wikipedias with a shared subword vocabulary. There is no additional crosslingual signal. Still, mBERT yields highquality multilingual representations (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). The exact reason for mBERT’s multilinguality is – to the best of our knowledge – still debated. K et al. (2020) provide an extensive study and conclude that a shared vocabulary is not necessary, but that the model needs to be deep and languages need to share a similar “structure”. Artetxe et al. (2020) show that neither a shared vocabulary nor joint pretraining is required for BERT to be multilingual. Conneau et al. (2020b) find that BERT models across languages can be easily aligned and 1 https://github.com/google-research/ bert/blob/master/multilingual.md 4423 Proceedings"
2020.emnlp-main.358,2020.repl4nlp-1.16,0,0.0397779,"(2020) (adapters) and Chi et al. (2020) (parallel data). We propose a simple extension to make mBERT more multilingual; it does not require additional supervision, parallel data or a more complex loss function – in contrast to this prior work. Finally, many papers find that mBERT yields competitive zero-shot performance across a range of languages and tasks such as parsing and NER (Pires et al., 2019; Wu and Dredze, 2019), word alignment and sentence retrieval (Libovick`y et al., 2019) and language generation (R¨onnqvist et al., 2019); Hu et al. (2020) show this for 40 languages and 9 tasks. Wu and Dredze (2020) consider the performance on up to 99 languages for NER. In contrast, Lauscher et al. (2020) show limitations of the zero-shot setting and Zhao et al. (2020) observe poor performance of mBERT in reference-free machine translation evaluation. Prior work here focuses on investigating the degree of multilinguality, not the reasons for it. 7 Conclusion We investigated which architectural and linguistic properties are essential for BERT to yield crosslingual representations. The main takeaways are: i) Shared position embeddings, shared special tokens, replacing masked tokens with random tokens and"
2020.emnlp-main.358,2020.acl-main.151,0,0.0200064,"Missing"
2020.emnlp-main.577,van-assem-etal-2006-conversion,0,0.030748,"Missing"
2020.emnlp-main.577,W05-0909,0,0.0807041,"Missing"
2020.emnlp-main.577,P18-1026,0,0.0287917,"bout the entity. We, however, generate a description of the whole KG, which involves multiple entities and their relations. Koncel-Kedziorski et al. (2019) generate texts from whole KGs. They, however, do not evaluate on human-generated KGs but automatically generated ones from the scientific information extraction tool SciIE (Luan et al., 2018). Their supervised model is based on message passing through the topology of the incidence graph of the KG input. Such graph neural networks (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) have been widely adopted in supervised graph-to-text tasks (Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019, 2020). Even though Marcheggiani and PerezBeltrachini (2018) report that graph neural networks can make better use of graph input than RNNs for supervised learning, for our unsupervised approach we follow the line of research that uses RNN-based sequence-to-sequence models (Cho et al., 2014; Sutskever et al., 2014) operating on serialized triple sets (Gardent et al., 2017b; Trisedya et al., 2018; Gehrmann et al., 2018; Castro Ferreira et al., 2019; Fan et al., 2019). We make this choice because learning a common semantic space for both texts and"
2020.emnlp-main.577,P18-1081,0,0.223991,"Missing"
2020.emnlp-main.577,P17-1017,0,0.158119,"y of the incidence graph of the KG input. Such graph neural networks (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) have been widely adopted in supervised graph-to-text tasks (Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019, 2020). Even though Marcheggiani and PerezBeltrachini (2018) report that graph neural networks can make better use of graph input than RNNs for supervised learning, for our unsupervised approach we follow the line of research that uses RNN-based sequence-to-sequence models (Cho et al., 2014; Sutskever et al., 2014) operating on serialized triple sets (Gardent et al., 2017b; Trisedya et al., 2018; Gehrmann et al., 2018; Castro Ferreira et al., 2019; Fan et al., 2019). We make this choice because learning a common semantic space for both texts and graphs by means of a shared encoder and decoder is a central component of our model. It is a nontrivial, separate research question whether and how encoder-decoder parameters can effectively be shared for models working on both sequential and non-sequential data. We thus leave the adaptation of our approach to graph neural networks for future work. text → graph. Converting a text into a KG representation, our method is"
2020.emnlp-main.577,W17-3518,0,0.363743,"y of the incidence graph of the KG input. Such graph neural networks (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) have been widely adopted in supervised graph-to-text tasks (Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019, 2020). Even though Marcheggiani and PerezBeltrachini (2018) report that graph neural networks can make better use of graph input than RNNs for supervised learning, for our unsupervised approach we follow the line of research that uses RNN-based sequence-to-sequence models (Cho et al., 2014; Sutskever et al., 2014) operating on serialized triple sets (Gardent et al., 2017b; Trisedya et al., 2018; Gehrmann et al., 2018; Castro Ferreira et al., 2019; Fan et al., 2019). We make this choice because learning a common semantic space for both texts and graphs by means of a shared encoder and decoder is a central component of our model. It is a nontrivial, separate research question whether and how encoder-decoder parameters can effectively be shared for models working on both sequential and non-sequential data. We thus leave the adaptation of our approach to graph neural networks for future work. text → graph. Converting a text into a KG representation, our method is"
2020.emnlp-main.577,W18-6505,0,0.026828,"graph neural networks (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) have been widely adopted in supervised graph-to-text tasks (Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019, 2020). Even though Marcheggiani and PerezBeltrachini (2018) report that graph neural networks can make better use of graph input than RNNs for supervised learning, for our unsupervised approach we follow the line of research that uses RNN-based sequence-to-sequence models (Cho et al., 2014; Sutskever et al., 2014) operating on serialized triple sets (Gardent et al., 2017b; Trisedya et al., 2018; Gehrmann et al., 2018; Castro Ferreira et al., 2019; Fan et al., 2019). We make this choice because learning a common semantic space for both texts and graphs by means of a shared encoder and decoder is a central component of our model. It is a nontrivial, separate research question whether and how encoder-decoder parameters can effectively be shared for models working on both sequential and non-sequential data. We thus leave the adaptation of our approach to graph neural networks for future work. text → graph. Converting a text into a KG representation, our method is an alternative to prior work on open informati"
2020.emnlp-main.577,P11-1149,0,0.0356819,"into a KG with potentially multiple facts. Our text→graph task is therefore most closely related to semantic parsing (Kamath and Das, 2019), but we convert statements into KG facts whereas semantic parsing typically converts a question into a KG or database query. Poon and Domingos (2009) proposed the first unsupervised approach. They, however, still need an additional KG alignment step, i.e., are not able to directly adjust to the target KG. Other approaches overcome this limitation but only in exchange for the inflexibility of manually created domain-specific lexicons (Popescu et al., 2004; Goldwasser et al., 2011). Poon (2013)’s approach is more flexible but still relies on preprocessing by a dependency parser, which generally means that language-specific annotations to train such a parser are needed. Our approach is endto-end, i.e., does not need any language-specific preprocessing during inference and only depends on a POS tagger used in the rule-based text→graph system to bootstrap training. Unsupervised sequence generation. Our unsu7118 pervised training regime for both text↔graph tasks is inspired by (Lample et al., 2018b). They used self-supervised pretraining and backtranslation for unsupervised"
2020.emnlp-main.577,P16-1154,0,0.0306545,"o simple heuristics to extract facts: (1) Each verb becomes a predicate; is creates facts with predicate attr. The content words directly before and after such a predicate word become subject and object. (2) Adjectives a form attributes, i.e., build facts of the form (X, attr, a) where X is filled with the first noun after a. These heuristics are similar in nature to a rudimentary parser. See Table 8 for an example. 4.2 Neural seq2seq systems Our main system is a neural seq2seq architecture. We equip the standard encoder-decoder model with attention (Bahdanau et al., 2014) and copy mechanism (Gu et al., 2016). Allowing the model to directly copy from the source to the target side is beneficial in data to text generation (Wiseman et al., 2017; Puduppully et al., 2019). The encoder (resp. decoder) is a bidirectional (resp. unidirectional) LSTM (Hochreiter and Schmidhuber, 1997). Dropout (Hinton et al., 2012) is applied at the input of both encoder and decoder (Britz et al., 2017). We combine this model with the following concepts: Multi-task model. In unsupervised machine translation, systems are trained for both translation directions (Lample et al., 2018b). In the same way, we train our system for"
2020.emnlp-main.577,P13-1092,0,0.175609,"multiple facts. Our text→graph task is therefore most closely related to semantic parsing (Kamath and Das, 2019), but we convert statements into KG facts whereas semantic parsing typically converts a question into a KG or database query. Poon and Domingos (2009) proposed the first unsupervised approach. They, however, still need an additional KG alignment step, i.e., are not able to directly adjust to the target KG. Other approaches overcome this limitation but only in exchange for the inflexibility of manually created domain-specific lexicons (Popescu et al., 2004; Goldwasser et al., 2011). Poon (2013)’s approach is more flexible but still relies on preprocessing by a dependency parser, which generally means that language-specific annotations to train such a parser are needed. Our approach is endto-end, i.e., does not need any language-specific preprocessing during inference and only depends on a POS tagger used in the rule-based text→graph system to bootstrap training. Unsupervised sequence generation. Our unsu7118 pervised training regime for both text↔graph tasks is inspired by (Lample et al., 2018b). They used self-supervised pretraining and backtranslation for unsupervised translation"
2020.emnlp-main.577,D09-1001,0,0.130374,"related to relation extraction in the unsupervised (Yao et al., 2011; Marcheggiani and Titov, 2016; Simon et al., 2019) and distantly supervised setting (Riedel et al., 2010; Parikh et al., 2015). However, these systems merely predict a single relation between two given entities in a single sentence, while we translate a whole text into a KG with potentially multiple facts. Our text→graph task is therefore most closely related to semantic parsing (Kamath and Das, 2019), but we convert statements into KG facts whereas semantic parsing typically converts a question into a KG or database query. Poon and Domingos (2009) proposed the first unsupervised approach. They, however, still need an additional KG alignment step, i.e., are not able to directly adjust to the target KG. Other approaches overcome this limitation but only in exchange for the inflexibility of manually created domain-specific lexicons (Popescu et al., 2004; Goldwasser et al., 2011). Poon (2013)’s approach is more flexible but still relies on preprocessing by a dependency parser, which generally means that language-specific annotations to train such a parser are needed. Our approach is endto-end, i.e., does not need any language-specific prep"
2020.emnlp-main.577,C04-1021,0,0.0123595,"ranslate a whole text into a KG with potentially multiple facts. Our text→graph task is therefore most closely related to semantic parsing (Kamath and Das, 2019), but we convert statements into KG facts whereas semantic parsing typically converts a question into a KG or database query. Poon and Domingos (2009) proposed the first unsupervised approach. They, however, still need an additional KG alignment step, i.e., are not able to directly adjust to the target KG. Other approaches overcome this limitation but only in exchange for the inflexibility of manually created domain-specific lexicons (Popescu et al., 2004; Goldwasser et al., 2011). Poon (2013)’s approach is more flexible but still relies on preprocessing by a dependency parser, which generally means that language-specific annotations to train such a parser are needed. Our approach is endto-end, i.e., does not need any language-specific preprocessing during inference and only depends on a POS tagger used in the rule-based text→graph system to bootstrap training. Unsupervised sequence generation. Our unsu7118 pervised training regime for both text↔graph tasks is inspired by (Lample et al., 2018b). They used self-supervised pretraining and backtr"
2020.emnlp-main.577,W17-4770,0,0.0596289,"Missing"
2020.emnlp-main.577,D19-1314,0,0.0349988,"cription of the whole KG, which involves multiple entities and their relations. Koncel-Kedziorski et al. (2019) generate texts from whole KGs. They, however, do not evaluate on human-generated KGs but automatically generated ones from the scientific information extraction tool SciIE (Luan et al., 2018). Their supervised model is based on message passing through the topology of the incidence graph of the KG input. Such graph neural networks (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) have been widely adopted in supervised graph-to-text tasks (Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019, 2020). Even though Marcheggiani and PerezBeltrachini (2018) report that graph neural networks can make better use of graph input than RNNs for supervised learning, for our unsupervised approach we follow the line of research that uses RNN-based sequence-to-sequence models (Cho et al., 2014; Sutskever et al., 2014) operating on serialized triple sets (Gardent et al., 2017b; Trisedya et al., 2018; Gehrmann et al., 2018; Castro Ferreira et al., 2019; Fan et al., 2019). We make this choice because learning a common semantic space for both texts and graphs by means of a shared encoder and decoder"
2020.emnlp-main.577,2020.tacl-1.38,0,0.20749,"Missing"
2020.emnlp-main.577,P98-2209,0,0.0821132,". 2 Related Work graph → text. Our work is the first attempt at fully unsupervised text generation from KGs. In this respect it is only comparable to traditional rule- or template-based approaches (Kukich, 1983; McRoy et al., 2000). However, in contrast to these approaches, which need to be manually adapted to new domains and KG schemas, our method is generally applicable to all kinds of data without modification. There is a large body of literature about supervised text generation from structured data, notably about the creation of sports game summaries from statistical records (Robin, 1995; Tanaka-Ishii et al., 1998). Recent efforts make use of neural encoderdecoder mechanisms (Wiseman et al., 2017; Puduppully et al., 2019). Although text creation from relational databases is related and our unsupervised method is, in principle, also applicable to this domain, in our work we specifically address text creation from graph-like structures such as KGs. One recent work on supervised text creation from KGs is (Bhowmik and de Melo, 2018). They generate a short description of an entity, i.e., a single KG node, based on a set of facts about the entity. We, however, generate a description of the whole KG, which inv"
2020.emnlp-main.577,P18-1151,0,0.0390433,"h of the KG input. Such graph neural networks (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) have been widely adopted in supervised graph-to-text tasks (Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019, 2020). Even though Marcheggiani and PerezBeltrachini (2018) report that graph neural networks can make better use of graph input than RNNs for supervised learning, for our unsupervised approach we follow the line of research that uses RNN-based sequence-to-sequence models (Cho et al., 2014; Sutskever et al., 2014) operating on serialized triple sets (Gardent et al., 2017b; Trisedya et al., 2018; Gehrmann et al., 2018; Castro Ferreira et al., 2019; Fan et al., 2019). We make this choice because learning a common semantic space for both texts and graphs by means of a shared encoder and decoder is a central component of our model. It is a nontrivial, separate research question whether and how encoder-decoder parameters can effectively be shared for models working on both sequential and non-sequential data. We thus leave the adaptation of our approach to graph neural networks for future work. text → graph. Converting a text into a KG representation, our method is an alternative to prior"
2020.emnlp-main.577,D11-1135,0,0.038544,"arate research question whether and how encoder-decoder parameters can effectively be shared for models working on both sequential and non-sequential data. We thus leave the adaptation of our approach to graph neural networks for future work. text → graph. Converting a text into a KG representation, our method is an alternative to prior work on open information extraction (Niklaus et al., 2018) with the advantage that the extractions, though trained without labeled data, automatically adjust to the KGs used for training. It is therefore also related to relation extraction in the unsupervised (Yao et al., 2011; Marcheggiani and Titov, 2016; Simon et al., 2019) and distantly supervised setting (Riedel et al., 2010; Parikh et al., 2015). However, these systems merely predict a single relation between two given entities in a single sentence, while we translate a whole text into a KG with potentially multiple facts. Our text→graph task is therefore most closely related to semantic parsing (Kamath and Das, 2019), but we convert statements into KG facts whereas semantic parsing typically converts a question into a KG or database query. Poon and Domingos (2009) proposed the first unsupervised approach. Th"
2020.emnlp-main.577,W00-1437,0,\N,Missing
2020.emnlp-main.577,P02-1040,0,\N,Missing
2020.emnlp-main.577,P06-4018,0,\N,Missing
2020.emnlp-main.577,P83-1022,0,\N,Missing
2020.emnlp-main.577,W15-2812,0,\N,Missing
2020.emnlp-main.577,N15-1077,0,\N,Missing
2020.emnlp-main.577,D17-1151,0,\N,Missing
2020.emnlp-main.577,D17-1239,0,\N,Missing
2020.emnlp-main.577,W18-6543,0,\N,Missing
2020.emnlp-main.577,N19-1366,0,\N,Missing
2020.emnlp-main.577,N19-1238,0,\N,Missing
2020.emnlp-main.577,W18-6501,0,\N,Missing
2020.emnlp-main.577,C18-1326,0,\N,Missing
2020.emnlp-main.577,D19-1428,0,\N,Missing
2020.emnlp-main.577,C98-2204,0,\N,Missing
2020.emnlp-main.577,P19-1133,0,\N,Missing
2020.findings-emnlp.109,W03-1022,0,0.110603,"the contextualization process, one possibility is to use word senses and to tap resources like the WordNet (WN) (Fellbaum, 1998) based word sense disambiguation benchmarks of the Senseval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Raganato et al., 2017). However, the abstraction level in WN sense inventories has been criticized as too fine-grained (Izquierdo et al., 2009), providing limited information to applications requiring higher level abstraction. Various levels of granularity of abstraction have been explored such as WN domains (Magnini and Cavaglià, 2000), supersenses (Ciaramita and Johnson, 2003; Levine et al., 2019) and basic level concepts (Beviá et al., 2007). In this paper, we use semantic classes (s-classes) (Yarowsky, 1992; Resnik, 1993; Kohomban and Lee, 2005; Yaghoobzadeh et al., 2019) as the proxy for the meaning contents of words to study the contextualization capability of BERT. Specifically, we use the Wikipedia-based resource for Probing Semantics in Word Embeddings (Wiki-PSE) (Yaghoobzadeh et al., 2019) which is detailed in §3.1. 3 3.1 Probing Dataset and Task Probing dataset For s-class probing, we use the s-class labeled corpus Wiki-PSE (Yaghoobzadeh et al., 2019). It"
2020.findings-emnlp.109,E09-1045,0,0.0844233,"Missing"
2020.findings-emnlp.109,D19-1006,0,0.031513,"representations. Liu et al. (2019) and Lin et al. (2019) investigate the linguistic knowledge encoded in BERT. Adi et al. (2016), Conneau et al. (2018), and Wieting and Kiela (2019) study sentence embedding properties via probing. Peters et al. (2018b) probe how the network architecture affects the learned vectors. In all of these studies, probing serves to analyze representations and reveal their properties. We employ probing to investigate the contextualization of words in pretrained language models quantitatively. In addition, we exploit how finetuning affects word contextualization. 1226 Ethayarajh (2019) quantitatively investigates contextualized embeddings, using unsupervised cosine-similarity-based evaluation. Inferring sclasses, we address a complementary set of questions because we can quantify contextualization with a uniform set of semantic classes. Brunner et al. (2020) employ token identifiability to compute the deviation of a contextualized embedding from the uncontextualized embedding. Voita et al. (2019) address this from the mutual information perspective, e.g., low mutual information between an uncontextualized embedding and its contextualized embedding can be viewed as a reflect"
2020.findings-emnlp.109,N19-1112,0,0.0283456,"ed in pretrained language model representations. Shi et al. (2016) show that string-based RNNs encode syntactic information. Belinkov et al. (2017) investigate word representations at different layers in NMT. Linzen et al. (2016) assess the syntactic ability of LSTM (Hochreiter and Schmidhuber, 1997) encoders and Goldberg (2019) of BERT. Tenney et al. (2019a) find that information on POS tagging, parsing, NER, semantic roles, and coreference is represented on increasingly higher layers of BERT. Yaghoobzadeh et al. (2019) assess the disambiguation properties of type-level word representations. Liu et al. (2019) and Lin et al. (2019) investigate the linguistic knowledge encoded in BERT. Adi et al. (2016), Conneau et al. (2018), and Wieting and Kiela (2019) study sentence embedding properties via probing. Peters et al. (2018b) probe how the network architecture affects the learned vectors. In all of these studies, probing serves to analyze representations and reveal their properties. We employ probing to investigate the contextualization of words in pretrained language models quantitatively. In addition, we exploit how finetuning affects word contextualization. 1226 Ethayarajh (2019) quantitatively in"
2020.findings-emnlp.109,magnini-cavaglia-2000-integrating,0,0.237798,"analyze in this paper. For investigating the contextualization process, one possibility is to use word senses and to tap resources like the WordNet (WN) (Fellbaum, 1998) based word sense disambiguation benchmarks of the Senseval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Raganato et al., 2017). However, the abstraction level in WN sense inventories has been criticized as too fine-grained (Izquierdo et al., 2009), providing limited information to applications requiring higher level abstraction. Various levels of granularity of abstraction have been explored such as WN domains (Magnini and Cavaglià, 2000), supersenses (Ciaramita and Johnson, 2003; Levine et al., 2019) and basic level concepts (Beviá et al., 2007). In this paper, we use semantic classes (s-classes) (Yarowsky, 1992; Resnik, 1993; Kohomban and Lee, 2005; Yaghoobzadeh et al., 2019) as the proxy for the meaning contents of words to study the contextualization capability of BERT. Specifically, we use the Wikipedia-based resource for Probing Semantics in Word Embeddings (Wiki-PSE) (Yaghoobzadeh et al., 2019) which is detailed in §3.1. 3 3.1 Probing Dataset and Task Probing dataset For s-class probing, we use the s-class labeled corpu"
2020.findings-emnlp.109,D14-1162,0,0.0911367,"d word representations from pretrained language models are widely utilized; however, the fact that pretrained language models implement a process of contextualization – starting with a completely uncontextualized layer of wordpieces at the bottom – is not well studied. Table 1 gives an example: BERT’s wordpiece embedding of “suit” is not contextualized: it contains several meanings of the word, including “to suit” (“be convenient”), lawsuit, and garment (“slacks”). Thus, there is no difference in this respect between BERT’s wordpiece embeddings and uncontextualized word embeddings like GloVe (Pennington et al., 2014). Pretrained language models start out with an uncontextualized representation at the lowest layer, then gradually contextualize it. This is the process we analyze in this paper. For investigating the contextualization process, one possibility is to use word senses and to tap resources like the WordNet (WN) (Fellbaum, 1998) based word sense disambiguation benchmarks of the Senseval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Raganato et al., 2017). However, the abstraction level in WN sense inventories has been criticized as too fine-grained (Izquierdo et al., 2009), providing l"
2020.findings-emnlp.109,N18-1202,0,0.743348,"s in understanding and utilizing pretrained language models. We show that the top layer representations support highly accurate inference of semantic classes; that the strongest contextualization effects occur in the lower layers; that local context is mostly sufficient for contextualizing words; and that top layer representations are more task-specific after finetuning while lower layer representations are more transferable. Finetuning uncovers task-related features, but pretrained knowledge about contextualization is still well preserved. 1 Introduction Pretrained language models like ELMo (Peters et al., 2018a), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019) are top performers in NLP because they learn contextualized representations, i.e., representations that reflect the interpretation of a word in context as opposed to its general meaning, which is less helpful in solving NLP tasks. As stated, pretrained language models contextualize words, is clear qualitatively; there has been little work on investigating contextualization, i.e., to which extent a word can be interpreted in context, quantitatively. We use BERT (Devlin et al., 2019) as our pretrained language model and quantify conte"
2020.findings-emnlp.109,D18-1179,0,0.276205,"s in understanding and utilizing pretrained language models. We show that the top layer representations support highly accurate inference of semantic classes; that the strongest contextualization effects occur in the lower layers; that local context is mostly sufficient for contextualizing words; and that top layer representations are more task-specific after finetuning while lower layer representations are more transferable. Finetuning uncovers task-related features, but pretrained knowledge about contextualization is still well preserved. 1 Introduction Pretrained language models like ELMo (Peters et al., 2018a), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019) are top performers in NLP because they learn contextualized representations, i.e., representations that reflect the interpretation of a word in context as opposed to its general meaning, which is less helpful in solving NLP tasks. As stated, pretrained language models contextualize words, is clear qualitatively; there has been little work on investigating contextualization, i.e., to which extent a word can be interpreted in context, quantitatively. We use BERT (Devlin et al., 2019) as our pretrained language model and quantify conte"
2020.findings-emnlp.109,W19-4302,0,0.0321073,"Missing"
2020.findings-emnlp.109,J93-2004,0,0.0732855,"ss “language”. But the BERT MLP for context size 2 predicts “language” correctly since it includes the strong signal “speak”. This context is a case of selectional restrictions (Resnik, 1993; Jurafsky and Martin, 2009), in this case possible objects of “speak”. As small context sizes already contain noticeable information contextualizing the words, we hypothesize that it may not be necessary to exploit the full context in cases where the quadratic complexity of full-sentence self attention is problematic, e.g., on edge devices. Initial results on part-of-speech tagging with the Penn Treebank (Marcus et al., 1993) in Appendix §C confirm our hypothesis. We leave more experiments to future work. P-BERT shows a similar pattern when varying the context sizes. However, large context sizes such as 16 and 32 hurt contextualization, meaning that averaging too many embeddings results in a bag of words not specific to a particular token. Comparing BERT layers. Higher layers of BERT yield better contextualized word embeddings. This phenomenon is more noticeable for large context sizes such as 8, 16 and 32. However for small context sizes, e.g., 0, embeddings from all layers perform similarly and badly. This means"
2020.findings-emnlp.109,D17-1010,0,0.03136,"pretraining corpora. As a result, BERT may not be able to derive the correct composed meaning. Then the MLPs cannot identify the correct s-class from the noisy input. Consider the tokenized results of “@glutamate@-biology” and one of its contexts: “three ne ##uro ##tra ##ns ##mit ##ters that play important roles in adolescent brain development are g ##lu ##tama ##te . . . ” Though “brain development” hints at a context related to “biology”, this signal could be swamped by the noise in embeddings of other – especially short – wordpieces. Schick and Schütze (2020) propose a mimicking approach (Pinter et al., 2017) to help BERT understand rare words. (ii) Uninformative contexts. Some contexts do not provide sufficient information related to the sclass. For example, according to probing results on BERTw, the wordpiece embedding of “goodfellas” does not encode the meaning of s-class “art” (i.e., movies); the context “Chase also said he wanted Imperioli because he had been in Goodfellas” of 1223 probing results (F1) 0.8 BERT layer 0 BERT layer 1 BERT layer 2 BERT layer 3 BERT layer 4 BERT layer 5 BERT layer 6 BERT layer 7 BERT layer 8 BERT layer 9 BERT layer 10 BERT layer 11 P-BERT 0.7 0.6 0.5 0.4 0 2 4 8"
2020.findings-emnlp.109,D19-1448,0,0.0204744,"employ probing to investigate the contextualization of words in pretrained language models quantitatively. In addition, we exploit how finetuning affects word contextualization. 1226 Ethayarajh (2019) quantitatively investigates contextualized embeddings, using unsupervised cosine-similarity-based evaluation. Inferring sclasses, we address a complementary set of questions because we can quantify contextualization with a uniform set of semantic classes. Brunner et al. (2020) employ token identifiability to compute the deviation of a contextualized embedding from the uncontextualized embedding. Voita et al. (2019) address this from the mutual information perspective, e.g., low mutual information between an uncontextualized embedding and its contextualized embedding can be viewed as a reflection of more contextualization. Similar observations are made: higher layer embeddings are more contextualized while lower layer embeddings are less contextualized. In contrast, we draw the observations from the perspective of s-class inference. The higher layer embeddings perform better when evaluating the semantic classes – they are better contextualized and have higher fitness to the context than the lower layer e"
2020.findings-emnlp.109,I05-5002,0,\N,Missing
2020.findings-emnlp.109,W04-0811,0,\N,Missing
2020.findings-emnlp.109,W02-1001,0,\N,Missing
2020.findings-emnlp.109,C92-2070,0,\N,Missing
2020.findings-emnlp.109,H93-1054,0,\N,Missing
2020.findings-emnlp.109,P05-1005,0,\N,Missing
2020.findings-emnlp.109,H92-1045,0,\N,Missing
2020.findings-emnlp.109,D13-1170,0,\N,Missing
2020.findings-emnlp.109,Q17-1010,0,\N,Missing
2020.findings-emnlp.109,Q16-1037,0,\N,Missing
2020.findings-emnlp.109,E17-1010,0,\N,Missing
2020.findings-emnlp.109,P18-1031,0,\N,Missing
2020.findings-emnlp.109,W18-5446,0,\N,Missing
2020.findings-emnlp.109,Q19-1004,0,\N,Missing
2020.findings-emnlp.109,P19-1452,0,\N,Missing
2020.findings-emnlp.109,W19-4825,0,\N,Missing
2020.findings-emnlp.109,I17-1001,0,\N,Missing
2020.findings-emnlp.109,N19-1423,0,\N,Missing
2020.findings-emnlp.109,D19-1445,0,\N,Missing
2020.findings-emnlp.109,D19-1424,0,\N,Missing
2020.findings-emnlp.134,D16-1264,0,0.0398498,"he random seed, we report mean and standard error over eight runs. 4.3 Results and discussion Table 4 shows entity-level precision, recall and F1, as measured by the CoNLL NER scorer. For ease of visualization, Figure 1 shows test set F1 shifted and scaled as f (x) = x − BERT(ref) BioBERTv1.0(ref) − BERT(ref) Experiment 2: Covid-19 QA In this section, we use the proposed method to quickly adapt an existing general-domain QA model to an emerging target domain: the Covid-19 pandemic. Our baseline model is SQuADBERT,3 an existing BERT model that was finetuned on the general-domain SQuAD dataset (Rajpurkar et al., 2016). We evaluate on Deepset-AI Covid-QA (M¨oller et al., 2020), a SQuAD-style dataset with 2019 annotated span-selection questions about 147 papers from CORD-19 (Covid-19 Open Research Dataset).4 We assume that there is no labeled targetdomain data for finetuning on the task, and instead use the entire Covid-QA dataset as a test set. This is a realistic setup for an emerging domain without annotated training data. 3 www.huggingface.co/bert-large-uncasedwhole-word-masking-finetuned-squad 4 https://pages.semanticscholar.org/ coronavirus-research 1485 domain adaptation corpus SQuADBERT ——– size EM F"
2020.findings-emnlp.134,P19-1355,0,0.0394009,"Missing"
2020.findings-emnlp.134,W04-1213,0,\N,Missing
2020.findings-emnlp.134,W19-1909,0,\N,Missing
2020.findings-emnlp.134,N19-1423,0,\N,Missing
2020.findings-emnlp.134,D19-1371,0,\N,Missing
2020.findings-emnlp.134,D19-1433,0,\N,Missing
2020.findings-emnlp.147,W18-6318,0,0.082199,"stical aligner, trained on 100k parallel sentences. 1 Sir Nils Olav III. です ペンギン knighted by el rey noruego Nils Olav der Dritte is a penguin nominato cavaliere par un roi norvégien Figure 1: Our method does not rely on parallel training data and can align distant language pairs (GermanUzbek, top) and even mixed sentences (bottom). Example sentence is manually created. Algorithm: Itermax. Introduction Word alignments are essential for statistical machine translation and useful in NMT, e.g., for imposing priors on attention matrices (Liu et al., 2016; Chen et al., 2016; Alkhouli and Ney, 2017; Alkhouli et al., 2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and their implementations Giza++ (Och and Ney, 2003), fast-align (Dyer"
2020.findings-emnlp.147,W16-2206,0,0.0299395,"sentences. 1 Sir Nils Olav III. です ペンギン knighted by el rey noruego Nils Olav der Dritte is a penguin nominato cavaliere par un roi norvégien Figure 1: Our method does not rely on parallel training data and can align distant language pairs (GermanUzbek, top) and even mixed sentences (bottom). Example sentence is manually created. Algorithm: Itermax. Introduction Word alignments are essential for statistical machine translation and useful in NMT, e.g., for imposing priors on attention matrices (Liu et al., 2016; Chen et al., 2016; Alkhouli and Ney, 2017; Alkhouli et al., 2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and their implementations Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013), as well as newer models"
2020.findings-emnlp.147,N13-1073,0,0.807961,"2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and their implementations Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013), as well as newer models such as eflo¨ mal (Ostling and Tiedemann, 2016) are widely used for alignment. With the rise of NMT (Bahdanau et al., 2014), attempts have been made to interpret attention matrices as soft word alignments (Cohn et al., 2016; Koehn and Knowles, 2017; Ghader and Monz, 2017). Several methods create alignments from attention matrices (Peter et al., 2017; Zenkel et al., 2019) or pursue a multitask approach for alignment and translation (Garg et al., 2019). However, most systems require parallel data (in sufficient amount to train high quality NMT systems) and their perform"
2020.findings-emnlp.147,C16-1302,1,0.748781,"13), Giza++ (Och and Ney, ¨ 2003) and eflomal (Ostling and Tiedemann, 2016). ¨ (Ostling, 2015a) showed that Bayesian Alignment Models perform well. Neural network based extensions of these models have been considered (Ayan et al., 2005; Ho and Yvon, 2019). All of these models are trained on parallel text. Our method instead aligns based on embeddings that are induced from monolingual data only. We compare with prior methods and observe comparable performance. Prior work on using learned representations for alignment includes (Smadja et al., 1996; Och and Ney, 2003) (Dice coefficient), (Jalili Sabet et al., 2016) (incorporation of embeddings into IBM models), (Legrand et al., 2016) (neural network alignment model) and (Pourdamghani et al., 2018) (embeddings are used to encourage words to align to similar words). Tamura et al. (2014) use recurrent neural networks to learn alignments. They use noise contrastive estimation to avoid supervision. Yang et al. (2013) train a neural network that uses pretrained word embeddings in the initial layer. All of this work requires parallel data. mBERT is used for word alignments in concurrent work: Libovick´y et al. (2019) use the high quality of mBERT alignments as"
2020.findings-emnlp.147,W19-6721,0,0.0640835,"Missing"
2020.findings-emnlp.147,2005.mtsummit-papers.11,0,0.432713,"Missing"
2020.findings-emnlp.147,P00-1056,0,0.572969,"Missing"
2020.findings-emnlp.147,P19-1124,0,0.0189498,"at standard attention does not have access to the target word. To address this, Peter et al. (2017) tailor attention matrices to obtain higher quality alignments. Li et al. (2018)’s and Zenkel et al. (2019)’s models perform similarly to and Zenkel et al. (2020) outperform Giza++. Ding et al. (2019) propose better decoding algorithms to deduce word alignments from NMT predictions. Chen et al. (2016), Mi et al. (2016) and Garg et al. (2019) obtain alignments and translations in a multitask setup. Garg et al. (2019) find that operating at the subword level can be beneficial for alignment models. Li et al. (2019) propose two methods to extract alignments from NMT 1634 models, however they do not outperform fast-align. Stengel-Eskin et al. (2019) compute similarity matrices of encoder-decoder representations that are leveraged for word alignments, together with supervised learning, which requires manually annotated alignment. We find our proposed methods to be competitive with these approaches. In contrast to our work, they all require parallel data. 6 Conclusion We presented word aligners based on contextualized embeddings that outperform in four and match the performance of state-of-the-art aligners"
2020.findings-emnlp.147,N18-1125,0,0.0232238,"vick´y et al. (2019) use the high quality of mBERT alignments as evidence for the “language-neutrality” of mBERT. Nagata et al. (2020) phrase word alignment as crosslingual span prediction and finetune mBERT using gold alignments. Attention in NMT (Bahdanau et al., 2014) is related to a notion of soft alignment, but often deviates from conventional word alignments (Ghader and Monz, 2017; Koehn and Knowles, 2017). One difference is that standard attention does not have access to the target word. To address this, Peter et al. (2017) tailor attention matrices to obtain higher quality alignments. Li et al. (2018)’s and Zenkel et al. (2019)’s models perform similarly to and Zenkel et al. (2020) outperform Giza++. Ding et al. (2019) propose better decoding algorithms to deduce word alignments from NMT predictions. Chen et al. (2016), Mi et al. (2016) and Garg et al. (2019) obtain alignments and translations in a multitask setup. Garg et al. (2019) find that operating at the subword level can be beneficial for alignment models. Li et al. (2019) propose two methods to extract alignments from NMT 1634 models, however they do not outperform fast-align. Stengel-Eskin et al. (2019) compute similarity matrices"
2020.findings-emnlp.147,P19-1493,0,0.0788748,"Missing"
2020.findings-emnlp.147,C16-1291,0,0.0318143,"5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences. 1 Sir Nils Olav III. です ペンギン knighted by el rey noruego Nils Olav der Dritte is a penguin nominato cavaliere par un roi norvégien Figure 1: Our method does not rely on parallel training data and can align distant language pairs (GermanUzbek, top) and even mixed sentences (bottom). Example sentence is manually created. Algorithm: Itermax. Introduction Word alignments are essential for statistical machine translation and useful in NMT, e.g., for imposing priors on attention matrices (Liu et al., 2016; Chen et al., 2016; Alkhouli and Ney, 2017; Alkhouli et al., 2018) or for decoding (Alkhouli et al., 2016; Press and Smith, 2018). Further, word alignments have been successfully used in a range of tasks such as typological analysis (Lewis and ¨ Xia, 2008; Ostling, 2015b), annotation projection (Yarowsky et al., 2001; Pad´o and Lapata, 2009; Asgari and Sch¨utze, 2017; Huck et al., 2019) and creating multilingual embeddings (Guo et al., 2016; Ammar et al., 2016; Dufter et al., 2018). ∗ Equal contribution - random order. Statistical word aligners such as the IBM models (Brown et al., 1993) and"
2020.findings-emnlp.147,2000.bcs-1.11,0,0.662232,"Missing"
2020.findings-emnlp.147,2020.acl-demos.14,0,0.0236288,"erheblich zu dieser Entwicklung beigetragen . The Commission , for its part , will continue to play an active part in the intergovernmental conference . Die Kommission wird bei der Regierungskonferenz auch weiterhin eine aktive Rolle spielen . Figure 7: Example alignment of auxiliary verbs. Same setting as in Table 6. Solid lines: mBERT’s alignment, identical to the gold standard. Dashed lines: eflomal’s incorrect alignment. 4.5 Part-Of-Speech Analysis To analyze the performance with respect to different part-of-speech (POS) tags, the ENG-DEU gold standard was tagged with the Stanza toolkit (Qi et al., 2020). We evaluate the alignment performance for each POS tag by only considering the alignment edges where at least one of their member words has this tag. Table 6 shows results for frequent POS tags. Compared to eflomal, mBERT aligns auxiliaries, pronouns and verbs better. The relative position of auxiliaries and verbs in German can diverge strongly from that in English because they occur at the end of the sentence (verb-end position) in many clause types. Positions of pronouns can also diverge due to a more flexible word order in German. It is difficult for an HMM-based aligner like eflomal to m"
2020.findings-emnlp.147,J96-1001,0,0.726218,"gners, often based on IBM models, include fastalign (Dyer et al., 2013), Giza++ (Och and Ney, ¨ 2003) and eflomal (Ostling and Tiedemann, 2016). ¨ (Ostling, 2015a) showed that Bayesian Alignment Models perform well. Neural network based extensions of these models have been considered (Ayan et al., 2005; Ho and Yvon, 2019). All of these models are trained on parallel text. Our method instead aligns based on embeddings that are induced from monolingual data only. We compare with prior methods and observe comparable performance. Prior work on using learned representations for alignment includes (Smadja et al., 1996; Och and Ney, 2003) (Dice coefficient), (Jalili Sabet et al., 2016) (incorporation of embeddings into IBM models), (Legrand et al., 2016) (neural network alignment model) and (Pourdamghani et al., 2018) (embeddings are used to encourage words to align to similar words). Tamura et al. (2014) use recurrent neural networks to learn alignments. They use noise contrastive estimation to avoid supervision. Yang et al. (2013) train a neural network that uses pretrained word embeddings in the initial layer. All of this work requires parallel data. mBERT is used for word alignments in concurrent work:"
2020.findings-emnlp.147,J93-2003,0,\N,Missing
2020.findings-emnlp.147,H01-1035,0,\N,Missing
2020.findings-emnlp.147,P11-1042,0,\N,Missing
2020.findings-emnlp.147,P02-1050,0,\N,Missing
2020.findings-emnlp.147,W03-0301,0,\N,Missing
2020.findings-emnlp.147,P15-2034,0,\N,Missing
2020.findings-emnlp.147,J03-1002,0,\N,Missing
2020.findings-emnlp.147,2016.amta-researchers.10,0,\N,Missing
2020.findings-emnlp.147,Q17-1010,0,\N,Missing
2020.findings-emnlp.147,P16-2028,0,\N,Missing
2020.findings-emnlp.147,bojar-prokopova-2006-czech,0,\N,Missing
2020.findings-emnlp.147,P18-1141,1,\N,Missing
2020.findings-emnlp.147,N18-2083,0,\N,Missing
2020.findings-emnlp.147,P19-1452,0,\N,Missing
2020.findings-emnlp.147,P16-1162,0,\N,Missing
2020.findings-emnlp.147,N19-1423,0,\N,Missing
2020.findings-emnlp.147,D19-1453,0,\N,Missing
2020.findings-emnlp.147,D19-1084,0,\N,Missing
2020.findings-emnlp.147,D19-1448,0,\N,Missing
2020.findings-emnlp.147,I08-2093,0,\N,Missing
2020.findings-emnlp.147,H05-1009,0,\N,Missing
2020.findings-emnlp.147,W08-0303,0,\N,Missing
2020.findings-emnlp.147,P14-1138,0,\N,Missing
2020.findings-emnlp.147,W17-4711,0,\N,Missing
2020.findings-emnlp.152,N19-1423,0,0.187219,"formance over 5 datasets. 1 Figure 1: Computational cost vs sequence length BERT pre-training (NAS) (Strubell et al., 2019) BERT fine-training (n=512)* Table 1: Similar to Strubell et al. (2019) who estimate the carbon footprint of BERT during pretraining, we estimate the carbon footprint (lbs of CO2 equivalent) during finetuning BERT for document classification. *: see supplementary material for details. Introduction Natural Language Processing (NLP) has recently witnessed a series of breakthroughs by the evolution of large-scale language models (LM) such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019) etc. due to improved capabilities for language understanding (Bengio et al., 2003; Mikolov et al., 2013). However this massive increase in model size comes at the expense of very high computational costs: longer training time, high GPU/TPU memory constraints, adversely high carbon footprints, and unaffordable invoices for small-scale enterprises. Figure 1 shows the computational cost (training time: millisecond/batch; CO2 emission, and GPU memory usage) of BERT all of which grow quadratically with sequence length (N). We note that this *E"
2020.findings-emnlp.152,D19-1588,0,0.037762,"Missing"
2020.findings-emnlp.152,D14-1181,0,0.0147478,"Missing"
2020.findings-emnlp.152,2021.ccl-1.108,0,0.0846966,"Missing"
2020.findings-emnlp.152,N18-1202,0,0.0549253,"Missing"
2020.findings-emnlp.152,P19-1355,0,0.0628367,"Missing"
2020.findings-emnlp.307,P19-1470,0,0.0132992,"uring pretraining better than BERT, see Table 2. BERT was not trained on 2020 events, so it must resort to guessing. Generally, we see that BERT’s knowledge is mainly based on guessing as it has seen Wikipedia during training but is not able to recall the knowledge recovered by kNN. Table 4 gives examples for BERT and BERTkNN predictions. We see that BERT predicts the answer category correctly, but it often needs help from kNN to recover the correct entity within that category. 5 Related work PLMs are top performers for many tasks, including QA (Kwiatkowski et al., 2019; Alberti et al., 2019; Bosselut et al., 2019). Petroni et al. (2019) introduced the LAMA QA task to probe PLMs’ knowledge of facts typically modeled by KBs. The basic idea of BERT-kNN is similar to Khandelwal et al. (2020)’s interpolation of a PLM and kNN for language modeling. In contrast, we address QA. We introduce an IR step into the model that is essential for good performance. Also, our context representations differ as we use embeddings of the masked token. Grave et al. (2016) and Merity et al. (2017), inter alia, also make use of memory to store hidden states. They focus on recent history, making it easier to copy rare vocabulary"
2020.findings-emnlp.307,Q19-1026,0,0.0154305,"ion per dataset. BERT-kNN answers facts unseen during pretraining better than BERT, see Table 2. BERT was not trained on 2020 events, so it must resort to guessing. Generally, we see that BERT’s knowledge is mainly based on guessing as it has seen Wikipedia during training but is not able to recall the knowledge recovered by kNN. Table 4 gives examples for BERT and BERTkNN predictions. We see that BERT predicts the answer category correctly, but it often needs help from kNN to recover the correct entity within that category. 5 Related work PLMs are top performers for many tasks, including QA (Kwiatkowski et al., 2019; Alberti et al., 2019; Bosselut et al., 2019). Petroni et al. (2019) introduced the LAMA QA task to probe PLMs’ knowledge of facts typically modeled by KBs. The basic idea of BERT-kNN is similar to Khandelwal et al. (2020)’s interpolation of a PLM and kNN for language modeling. In contrast, we address QA. We introduce an IR step into the model that is essential for good performance. Also, our context representations differ as we use embeddings of the masked token. Grave et al. (2016) and Merity et al. (2017), inter alia, also make use of memory to store hidden states. They focus on recent his"
2020.findings-emnlp.307,P16-1137,0,0.0310734,"er: https://github.com/norakassner/ BERT-kNN. 2 Data to KB queries. A cloze question is generated using a subject-relation-object triple from a KB and a templatic statement for the relation that contains variables X and Y for subject and object; e.g, “X was born in Y”. The subject is substituted for X and [MASK] for Y. In all LAMA triples, Y is a single-token answer. LAMA covers different sources: The GoogleRE1 set covers the relations “place of birth”, “date of birth” and “place of death”. TREx (ElSahar et al., 2018) consists of a subset of Wikidata triples covering 41 relations. ConceptNet (Li et al., 2016) combines 16 commonsense relations among words and phrases. The underlying Open Mind Common Sense corpus provides matching statements to query the language model. SQuAD (Rajpurkar et al., 2016) is a standard question answering dataset. LAMA contains a subset of 305 context-insensitive questions. Unlike KB queries, SQuAD uses manually reformulated cloze-style questions which are not based on a template. We use SQuAD and an additional 305 ConceptNet queries for hyperparamter search. Poerner et al. (2019) introduce LAMA-UHN, a subset of LAMA’s TREx and GoogleRE questions from which easy-to-guess"
2020.findings-emnlp.307,2021.ccl-1.108,0,0.101185,"Missing"
2020.findings-emnlp.307,K16-1025,0,0.0276792,"s answers from text. It does not use PLMs nor a kNN module. Most importantly, BERT-kNN is fully unsupervised and does not require any extra training. Some work on knowledge in PLMs focuses on injecting knowledge into BERT’s encoder. ERNIE (Zhang et al., 2019) and KnowBert (Peters et al., 2019) are entity-enhanced versions of BERT. They introduce additional encoder layers that are integrated into BERT’s original encoder by expensive additional pretraining. Poerner et al. (2019) injects factual entity knowledge into BERT’s embeddings without pretraining by aligning Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT’s wordpiece vocabulary. This approach is also limited to labeled entities. Our approach on the other hand is not limited to labeled entities nor does it require any pretraining. Our approach is conceptually different from entity-enhanced versions of BERT and could potentially be combined with them for 3427 even better performance. Also, these models address language modeling, not QA. The combination of PLMs with an IR step/kNN search has attracted a lot of recent research interest. The following paragraph lists concurrent work: Petroni et al. (2020) also combine BERT with an IR step"
2020.findings-emnlp.307,P19-1139,0,0.337474,"training set or any other suitable text corpus. Due to its kNN component and its resulting ability to directly access facts stated in the searched text, BERT-kNN outperforms BERT on cloze-style 3424 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3424–3430 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Dataset LAMA LAMA-UHN BERT-base 27.7 20.6 BERT-large 30.6 23.0 ERNIE 30.4 24.7 Know-BERT 31.7 24.6 E-BERT 36.2 31.1 BERT-kNN 39.4 34.8 Table 1: Mean P@1 on LAMA and LAMA-UHN on the TREx and GoogleRE subsets for BERT-base, BERTlarge, ERNIE (Zhang et al., 2019), KnowBert (Peters et al., 2019), E-BERT (Poerner et al., 2019) and BERT-kNN. BERT-kNN performs best. QA by large margins. A schematic depiction of the model is shown in Figure 1. Specifically, we use BERT to embed each token’s masked context s in the text collection (BERT (s)). Each pair of context embedding and token is stored as a key-value pair in a datastore. Testing for a cloze question q, the embedding of q (BERT (q)) serves as query to find the k contexttarget pairs in the subset of the datastore that are closest. The final prediction is an interpolation of the kNN search and the PLM p"
2020.findings-emnlp.307,D19-1005,0,0.044075,"Missing"
2020.findings-emnlp.307,D19-1250,0,0.0502706,"Missing"
2020.findings-emnlp.307,D16-1264,0,0.0473862,"ion that contains variables X and Y for subject and object; e.g, “X was born in Y”. The subject is substituted for X and [MASK] for Y. In all LAMA triples, Y is a single-token answer. LAMA covers different sources: The GoogleRE1 set covers the relations “place of birth”, “date of birth” and “place of death”. TREx (ElSahar et al., 2018) consists of a subset of Wikidata triples covering 41 relations. ConceptNet (Li et al., 2016) combines 16 commonsense relations among words and phrases. The underlying Open Mind Common Sense corpus provides matching statements to query the language model. SQuAD (Rajpurkar et al., 2016) is a standard question answering dataset. LAMA contains a subset of 305 context-insensitive questions. Unlike KB queries, SQuAD uses manually reformulated cloze-style questions which are not based on a template. We use SQuAD and an additional 305 ConceptNet queries for hyperparamter search. Poerner et al. (2019) introduce LAMA-UHN, a subset of LAMA’s TREx and GoogleRE questions from which easy-to-guess facts have been removed. To test BERT-kNN’s performance on unseen facts, we collect Wikidata triples containing TREx relations from Wikipedia pages created January– May 2020 and add them to the"
2020.findings-emnlp.307,L18-1544,0,\N,Missing
2020.findings-emnlp.307,N19-1423,0,\N,Missing
2020.findings-emnlp.71,D18-1455,0,0.0287681,"e base completion. 2.3 vertices are LEnt and (3) a version of Word2Vec where words are predicted from entities. Loss (3) ensures that entities and words are embedded into the same space. 2.4 Vector space alignment Our vector space alignment strategy is inspired by cross-lingual word vector alignment (e.g., Mikolov et al. (2013b); Smith et al. (2017)). A related method was recently applied by Wang et al. (2019a) to map cross-lingual word vectors into the multilingual BERT wordpiece vector space. 2.5 Unsupervised QA QA has typically been tackled as a supervised problem (e.g., Das et al. (2017); Sun et al. (2018)). Recently, there has been interest in using unsupervised LMs such as GPT-2 or BERT for this task (Radford et al., 2019; Petroni et al., 2019). Davison et al. (2019) mine unsupervised commonsense knowledge from BERT, and Jiang et al. (2019) show the importance of using good prompts for unsupervised QA. None of this prior work differentiates quantitatively between factual knowledge of LMs and their ability to reason about the surface form of entity names. 3 E-BERT 3.1 Aligning entity and wordpiece vectors Conceptually, we want to transform the vectors of the entity vector space EWikipedia [LEn"
2020.findings-emnlp.71,K19-1030,0,0.120976,"expensive further pretraining of the BERT encoder. We evaluate E-BERT on unsupervised question answering (QA), supervised relation classification (RC) and entity linking (EL). On all three tasks, EBERT outperforms BERT and other baselines. We also show quantitatively that the original BERT model is overly reliant on the surface form of entity names (e.g., guessing that someone with an Italian-sounding name speaks Italian), and that E-BERT mitigates this problem. Summary of contributions. 1 Introduction BERT (Devlin et al., 2019) and its successors (e.g., Yang et al. (2019); Liu et al. (2019); Wang et al. (2019b)) continue to achieve state of the art performance on various NLP tasks. Recently, there has been interest in enhancing BERT with factual knowledge about entities (Zhang et al., 2019; Peters et al., 2019). To this end, we introduce EBERT: We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT’s wordpiece vector space (Section 3.1) and feed the aligned vectors into BERT as if they were wordpiece vectors (Section 3.2). Importantly, we do not make any changes to the BERT encoder itself, and we do no additional pretraining. This stands in contrast to previous entity-enhanced versi"
2020.findings-emnlp.71,D11-1072,0,\N,Missing
2020.findings-emnlp.71,L18-1544,0,\N,Missing
2020.findings-emnlp.71,K18-1050,0,\N,Missing
2020.findings-emnlp.71,N19-1423,0,\N,Missing
2020.findings-emnlp.71,D19-1109,0,\N,Missing
2020.findings-emnlp.71,D19-1224,0,\N,Missing
2020.findings-emnlp.71,D19-1005,0,\N,Missing
2020.findings-emnlp.71,2020.acl-main.368,1,\N,Missing
2020.findings-emnlp.71,K19-1063,0,\N,Missing
2020.lrec-1.296,P17-1042,0,0.0203898,"loser to each other than the other two in terms of adaptation loss and classification accuracy. Another proposed similarity measure is the perplexity of a language model when trained on one domain and tested on another (as for example reported by Hu et al. (2019) Appendix A.2). This measure, however, is not symmetric, which makes it not straightforward to compare to. As symmetry is not necessarily required, it would be interesting to adapt the CCA measure by using different mapping algorithms, for example based on the Orthogonal Procrustes problem (Mikolov et al., 2013a; Artetxe et al., 2016; Artetxe et al., 2017; Lample et al., 2018; Lubin et al., 2019). Examining this will be left for future work. 6. Conclusion In this work, we presented the CCA measure, a new measure to capture domain similarity based on the dimensionwise correlation of embeddings spaces. In contrast to other approaches of domain similarity, our CCA measure does not rely on To investigate the relations between embedding spaces from different domains and languages, we map PPMI+SVD embeddings into a maximally correlated space by applying CCA to a selected set of pairwise combinations. We define the CCA measure as the mean of the dime"
2020.lrec-1.296,C18-1070,0,0.337001,"the need for task specific, manual feature extraction by experts. This is achieved by so called word embeddings, which are high dimensional vector representations that can be induced from unlabeled data. Despite the success of embeddings, there are still open research questions about what exactly these representations capture. In this paper, we address the question as how differences in domains and languages are reflected by embedding spaces. Previous work has shown that it is possible to align embedding spaces across domains and languages (e.g. Mikolov et al. (2013a), Artetxe et al. (2016), Barnes et al. (2018)), but to the best of our knowledge, no task independent, multi-lingual analysis of the underlying structural similarities has been carried out so far. A systematic notion of similarity is also interesting from an application point of view. A widely used technique is to augment a model’s capabilities by using pre-trained word embeddings, based on larger or more diverse corpora. This can be applied across different text domains or languages. In both cases, it can be useful to know how similar available pre-trained embeddings are to the target corpus in order to pick the most similar or to estim"
2020.lrec-1.296,P07-1056,0,0.148073,"ing approach can close the gap between the original and the simulated datapoints in the cross-lingual comparison, or whether this is due to inherent language differences (words without exact 1-1 correspondences, ambiguity pat2436 (a) Two Wikipedia corpora (b) Wikipedia and subtitle corpus (c) Wikipedia and legislative corpus (d) Wikipedia and medical corpus (e) Wikipedia and Europarl corpus Figure 3: The blue curve shows the distribution of CCA measures for 100 random corpus splits (after permutation). The red x is the CCA measure for the original (unpermuted) corpus pair. the CCA measure to (Blitzer et al., 2007)’s results as well as to the Jensen-Shannon Divergence results reported by Barnes et al. (2018). The Amazon dataset contains product reviews for four different categories, namely books, DVDs, electronics and kitchen, annotated for sentiment. As Barnes et al. (2018), we use the unlabeled parts of the dataset to extract word embeddings and compare the domains using the CCA measure. Table 7 compares CCA measure and Jensen-Shannon Divergence, as reported by Barnes et al. (2018).14 book DVD electronics kitchen Figure 4: Resampling results for comparison of English and German parallel Europarl corpo"
2020.lrec-1.296,N19-1423,0,0.011004,"information. These can be learned task-specifically by the first layer in a neural network, but it has been shown to be more effective (in terms of computational resources and overall quality) to use word embeddings that have been pretrained on large datasets in advance. To this end, there exist several approaches, either based on co-occurrence counts (Sch¨utze, 1993; Bullinaria and Levy, 2007; Turney and Pantel, 2010) or learned implicitly by neural network architectures solving a language modeling or word prediction task (Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). In order to avoid any source of additional uncertainty in the embedding spaces, we focus on the former approach in this work. Different approaches to represent these underlying cooccurrences have been proposed (see Turney and Pantel (2010) for an overview), the most adopted of which is the Positive Pointwise Mutual Information (PPMI, cf. Levy et al. (2015)), which reflects the strength of association between word pairs (i.e., if they co-occur more often with each other than with other words, they will receive a higher score). ! Pˆ (w, c) ,0 P P M I(w, c) = max Pˆ (w)Pˆ (c) where Pˆ (x) is th"
2020.lrec-1.296,E14-1049,0,0.0377032,"improves the performance. Another hyperparameter that is shown to impact the performance is to use eigenvalue weighting, i.e., using the top d components of U Σp , which yields better results for p = 0.5 and p = 0 (i.e., ignoring Σ) than the original approach. They also report that applying L2 vector normalization leads to better performance. To compare embedding spaces, they have to be mapped into a shared space. Different supervised, semi-supervised and unsupervised approaches for mapping embedding spaces have been proposed (Mikolov et al., 2013a; Artetxe et al., 2016; Lample et al., 2018; Faruqui and Dyer, 2014; Rastogi et al., 2015; Lu et al., 2015). In this work, we apply Canonical Correlation Analysis (CCA, Hotelling (1936)), which maps two multi-dimensional spaces into a shared space in which they are maximally correlated. It was first proposed for mapping embedding spaces by Faruqui and Dyer (2014). Given two count-based embedding spaces Σ ∈ Rn1 ×d1 and Ω ∈ Rn2 ×d2 where n1 and n2 denote the vocabulary sizes and d1 and d2 are the embedding dimensions, they first extract the sub-spaces Σ0 and Ω0 with shared vocabulary n. For every corresponding vector pair x ∈ Σ0 and y ∈ Ω0 CCA then finds two tr"
2020.lrec-1.296,P19-1286,0,0.0272941,"ings, it is not suited for corpora as small as 1 or 2.5 MB. 2437 gence. The CCA measure is also consistent with the domain adaptation findings by Blitzer et al. (2007) and the ones reported in Barnes et al. (2018), who find that the book and DVD categories are closer to each other than to any of the other two and likewise kitchen and electronics are closer to each other than the other two in terms of adaptation loss and classification accuracy. Another proposed similarity measure is the perplexity of a language model when trained on one domain and tested on another (as for example reported by Hu et al. (2019) Appendix A.2). This measure, however, is not symmetric, which makes it not straightforward to compare to. As symmetry is not necessarily required, it would be interesting to adapt the CCA measure by using different mapping algorithms, for example based on the Orthogonal Procrustes problem (Mikolov et al., 2013a; Artetxe et al., 2016; Artetxe et al., 2017; Lample et al., 2018; Lubin et al., 2019). Examining this will be left for future work. 6. Conclusion In this work, we presented the CCA measure, a new measure to capture domain similarity based on the dimensionwise correlation of embeddings"
2020.lrec-1.296,2005.mtsummit-papers.11,0,0.0246531,"9. The raw article texts are extracted using the wikiextractor tool3 and split into sentences using the NLTK Tokenizer Package.4 Due to its encyclopedic nature it is treated as a general-domain corpus here, even though it covers a wide range of topics in itself. As pre-trained embeddings are generally available based on the whole corpus,5 a more thorough examination of semantic sub-parts will be left for future work. Europarl? (euro) The Europarl corpus is a parallel multilingual collection of the transcriptions of the proceedings in the European Parliament, originally compiled and aligned by Koehn (2005) as a resource for machine translation. As our approach does not necessarily require parallel corpora, we downloaded the monolingual untokenized raw texts to make use of the largest amount of data available per language. For the comparison in Section 3.4. we also use the parallel corpora for English and our selected languages. OpenSubtitles? (sub) The OpenSubtitles corpus (Lison and Tiedemann, 2016) consists of pre-processed movie subtitles.6 We use the 2018 version and again downloaded the monolingual untokenized raw texts. Acquis Communautaire? (dgt) This corpus consists of the publicly acce"
2020.lrec-1.296,L16-1147,0,0.0113146,"ts will be left for future work. Europarl? (euro) The Europarl corpus is a parallel multilingual collection of the transcriptions of the proceedings in the European Parliament, originally compiled and aligned by Koehn (2005) as a resource for machine translation. As our approach does not necessarily require parallel corpora, we downloaded the monolingual untokenized raw texts to make use of the largest amount of data available per language. For the comparison in Section 3.4. we also use the parallel corpora for English and our selected languages. OpenSubtitles? (sub) The OpenSubtitles corpus (Lison and Tiedemann, 2016) consists of pre-processed movie subtitles.6 We use the 2018 version and again downloaded the monolingual untokenized raw texts. Acquis Communautaire? (dgt) This corpus consists of the publicly accessible Translation Memory provided by the Directorate-General for Translation of the European Commission, which contains translations of the European Unions legislative documents (Acquis Communautaire).7 It is similar to the Europarl corpus, but differs in the form of language (written documents vs. transcribed speeches). Medical documents (med) The UFAL Medical Corpus is a pairwise bilingual (one o"
2020.lrec-1.296,N15-1028,0,0.0279266,"ter that is shown to impact the performance is to use eigenvalue weighting, i.e., using the top d components of U Σp , which yields better results for p = 0.5 and p = 0 (i.e., ignoring Σ) than the original approach. They also report that applying L2 vector normalization leads to better performance. To compare embedding spaces, they have to be mapped into a shared space. Different supervised, semi-supervised and unsupervised approaches for mapping embedding spaces have been proposed (Mikolov et al., 2013a; Artetxe et al., 2016; Lample et al., 2018; Faruqui and Dyer, 2014; Rastogi et al., 2015; Lu et al., 2015). In this work, we apply Canonical Correlation Analysis (CCA, Hotelling (1936)), which maps two multi-dimensional spaces into a shared space in which they are maximally correlated. It was first proposed for mapping embedding spaces by Faruqui and Dyer (2014). Given two count-based embedding spaces Σ ∈ Rn1 ×d1 and Ω ∈ Rn2 ×d2 where n1 and n2 denote the vocabulary sizes and d1 and d2 are the embedding dimensions, they first extract the sub-spaces Σ0 and Ω0 with shared vocabulary n. For every corresponding vector pair x ∈ Σ0 and y ∈ Ω0 CCA then finds two transformations v and w such that xv and y"
2020.lrec-1.296,N19-1045,0,0.0414142,"s corpus. en–es en–de en–cs parallel cleaned 0.57 0.53 0.50 0.71 0.71 0.64 Table 6: CCA measure scores for cross-lingual comparison. For parallel portion of the Europarl corpus and additionally using a cleaned dictionary. Cleaned Dictionary In the cross-lingual setting, our approach has another influencing factor: As we apply a supervised mapping algorithm, we rely on the use of a dictionary. The MUSE dictionaries are created using a translation tool and can therefore also contain erroneous translations (a short inspection of the English-German dictionary revealed also English-English pairs). Lubin et al. (2019) propose a noise-aware mapping approach based on the EM algorithm that combines the Orthogonal Procrustes mapping with a dictionary cleaning task and thereby determines the “useful” portion of the dictionary.12 We exploit their mapping approach to create cleaned dictionaries from the overlap of the MUSE dictionaries with the shared vocabulary for the parallel Europarl corpora. We then use these cleaned vocabularies for our CCA mapping on the original embeddings. The CCA measure scores with these cleaned vocabularies are displayed in the second column Table 6. 3.5. Discussion we abstract away f"
2020.lrec-1.296,D14-1162,0,0.0862663,"Missing"
2020.lrec-1.296,N18-1202,0,0.0187727,"that encode semantic information. These can be learned task-specifically by the first layer in a neural network, but it has been shown to be more effective (in terms of computational resources and overall quality) to use word embeddings that have been pretrained on large datasets in advance. To this end, there exist several approaches, either based on co-occurrence counts (Sch¨utze, 1993; Bullinaria and Levy, 2007; Turney and Pantel, 2010) or learned implicitly by neural network architectures solving a language modeling or word prediction task (Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). In order to avoid any source of additional uncertainty in the embedding spaces, we focus on the former approach in this work. Different approaches to represent these underlying cooccurrences have been proposed (see Turney and Pantel (2010) for an overview), the most adopted of which is the Positive Pointwise Mutual Information (PPMI, cf. Levy et al. (2015)), which reflects the strength of association between word pairs (i.e., if they co-occur more often with each other than with other words, they will receive a higher score). ! Pˆ (w, c) ,0 P P M I(w, c) = max Pˆ (w)Pˆ"
2020.lrec-1.296,N15-1058,0,0.0520571,"Missing"
2020.lrec-1.296,tiedemann-2012-parallel,0,0.0123825,"ng spaces, we will focus on more recently proposed mapping approaches and leave a comparison with this method for future work. 3. The CCA measure This section describes our approach to measuring domain similarity in terms of embedding space correlation. 3.1. Data In the following, we compare several corpora from different text domains. The corpora were selected following an intuitive notion of domain to cover a wide range of different text types. All corpora are available in four languages: English, German, Spanish and Czech. The corpora marked with ? are retrieved from the OPUS project page (Tiedemann, 2012).1 The names in parentheses will be used to refer to the corpora throughout this study. Wikipedia (wiki) One of the largest corpora available in many languages is the Wikipedia corpus.2 We use the dumps from February 22, 2019. The raw article texts are extracted using the wikiextractor tool3 and split into sentences using the NLTK Tokenizer Package.4 Due to its encyclopedic nature it is treated as a general-domain corpus here, even though it covers a wide range of topics in itself. As pre-trained embeddings are generally available based on the whole corpus,5 a more thorough examination of sema"
2020.lrec-1.858,D14-1179,0,0.0567471,"Missing"
2020.lrec-1.858,Q18-1032,0,0.0242354,"esentations from recurrent neural LMs have gained increased interest from the research community 6949 due to their ability to improve the performance of various downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). A recurrent neural LM estimates the sequence’s probability distribution by predicting the next word for each word in a sequence. While word-level LMs can capture syntactic and semantic features of words, character-level LMs are used for extracting sub-word information and improving word level representations (Kim et al., 2016; Bojanowski et al., 2015; Gerz et al., 2018; Verwimp et al., 2017; Peters et al., 2018). The work from Hahn and Baroni (2019) revealed that the hidden states of a recurrent neural character LM that has been trained on unsegmented English corpora encode information that can help identify word boundaries. Our approach is motivated by the idea that integrating such information into a word segmentation system could increase its performance. 2.4. Bi-directional LSTM Due to the ability to capture information in long sequences from both forward and backward directions, BiLSTMs have been applied and achieved great success in various sequence l"
2020.lrec-1.858,P19-1590,0,0.0341571,"Missing"
2020.lrec-1.858,Q19-1033,0,0.112702,"research community 6949 due to their ability to improve the performance of various downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). A recurrent neural LM estimates the sequence’s probability distribution by predicting the next word for each word in a sequence. While word-level LMs can capture syntactic and semantic features of words, character-level LMs are used for extracting sub-word information and improving word level representations (Kim et al., 2016; Bojanowski et al., 2015; Gerz et al., 2018; Verwimp et al., 2017; Peters et al., 2018). The work from Hahn and Baroni (2019) revealed that the hidden states of a recurrent neural character LM that has been trained on unsegmented English corpora encode information that can help identify word boundaries. Our approach is motivated by the idea that integrating such information into a word segmentation system could increase its performance. 2.4. Bi-directional LSTM Due to the ability to capture information in long sequences from both forward and backward directions, BiLSTMs have been applied and achieved great success in various sequence labeling tasks including POS tagging, chunking, NER (Huang et al., 2015; Alzboun et"
2020.lrec-1.858,W13-4702,0,0.0598,"Missing"
2020.lrec-1.858,P18-1031,0,0.288229,"ra using methods like Continuous Bag of Words (CBOW), Skip-Gram (Mikolov et al., 2013a), co-occurrence counts (Pennington et al., 2014), and by training a neural language model (Bengio et al., 2003). A drawback of traditional word embeddings is that each word in the vocabulary is typically assigned one explicit representation, while in fact, many words are ambiguous and can have more than one meaning depending on the context. Recently, many studies have focused on developing word representations which are more context sensitive, for instance embeddings from BERT (Devlin et al., 2019), ULMfit (Howard and Ruder, 2018), and ELMo (Peters et al., 2018). These representations are even richer than the traditional word embeddings, since the models also consider the context in which the word appears before assigning the representation. Instead of applying the learned knowledge to only the first layer of the model like in the traditional word embeddings approach, ULMfit transfers both weights of the embedding layer and the recurrent layer of the pretrained LM to the downstream model. This method has shown to be a great performance boost for text classification. The authors also suggested a few fine-tuning methods"
2020.lrec-1.858,W97-0126,0,0.523057,"to choose the best segmentation are Longest Matching (Poowarawan, 1986) and Maximal Matching (Sornlertlamvanich, 1993). Since dictionary based approaches segment ambiguities according to static predefined rules without considering the context of the word, they cannot handle unknown words and ambiguities efficiently. Later, many statistical models using supervised machine learning were developed to overcome the drawbacks of dictionary based approaches. Such statistical approaches include: Decision Trees, Naive Bayes, Support Vector Machines (Haruechaiyasak et al., 2008), Trigram Markov Models (Kawtrakul and Thumkanon, 1997), and feature based models using feature extraction algorithms (Meknavin et 2 https://github.com/meanna/ThaiLMCUT al., 1997). Kruengkrai et al. (2009) also proposed a model based on word and character clustering. Regarding methods using machine learning, models based on Conditional Random Fields (CRFs) have proven to be among the most popular and suitable models for this task (Kruengkrai et al., 2006; Haruechaiyasak and Kongyoung, 2009; Kongyoung et al., 2015; Nararatwong et al., 2018). Haruechaiyasak and Kongyoung (2009) have shown that the lexical property of Thai characters provides effecti"
2020.lrec-1.858,N06-2001,0,0.166256,"Missing"
2020.lrec-1.858,kruengkrai-etal-2006-conditional,0,0.129712,"Missing"
2020.lrec-1.858,P09-1012,0,0.180732,"Missing"
2020.lrec-1.858,D14-1162,0,0.095845,"Missing"
2020.lrec-1.858,P17-1161,0,0.0254032,"gmentation outperformed other more complex models on various benchmarks. The model applies pretrained character and bigram embeddings to the first layer of the network. For Thai word segmentation, models based on Bi-LSTM have also reported highly accurate results (Jousimo et al., 2017; Phuriphatwatthana, 2017). Bi-directional information is also an important component of modern contextual pretrained word representation models including BERT, ELMo, and ULMfit. Forward and backward information helps the model learn contextsensitive representation by taking the whole sequence into consideration. Peters et al. (2017) proposed a pretrained bi-directional LM for sequence tagging. The model uses the concatenation of separate forward and backward unidirectional LSTMs. Both LSTMs are trained separately with no shared parameters unlike in the traditional architecture proposed by Schuster and Paliwal (1997). ELMo learns deep contextualized word representations from a bi-directional LM and uses all its layers in prediction. It uses a similar structure of Bi-LSTM as the one outlined by Peters et al. (2017), but shares some weights between directions instead of using completely independent parameters. Howard and Ru"
2020.lrec-1.858,N18-1202,0,0.219159,"g of Words (CBOW), Skip-Gram (Mikolov et al., 2013a), co-occurrence counts (Pennington et al., 2014), and by training a neural language model (Bengio et al., 2003). A drawback of traditional word embeddings is that each word in the vocabulary is typically assigned one explicit representation, while in fact, many words are ambiguous and can have more than one meaning depending on the context. Recently, many studies have focused on developing word representations which are more context sensitive, for instance embeddings from BERT (Devlin et al., 2019), ULMfit (Howard and Ruder, 2018), and ELMo (Peters et al., 2018). These representations are even richer than the traditional word embeddings, since the models also consider the context in which the word appears before assigning the representation. Instead of applying the learned knowledge to only the first layer of the model like in the traditional word embeddings approach, ULMfit transfers both weights of the embedding layer and the recurrent layer of the pretrained LM to the downstream model. This method has shown to be a great performance boost for text classification. The authors also suggested a few fine-tuning methods to adapt the pretrained LM to do"
2020.lrec-1.858,H93-1035,0,0.666895,"We provide an implementation of ThaiLMCut as a publicly available word segmentation library2 2. 2.1. Related Work Thai Word Segmentation For over 30 years, researchers have been actively working on solving the word segmentation problem for Thai. Early works used dictionary based methods where a given text is segmented according to words that are defined in dictionaries. In the presence of multiple segmentation choices, a method for selecting the best one needs to be applied. Two classical algorithms to choose the best segmentation are Longest Matching (Poowarawan, 1986) and Maximal Matching (Sornlertlamvanich, 1993). Since dictionary based approaches segment ambiguities according to static predefined rules without considering the context of the word, they cannot handle unknown words and ambiguities efficiently. Later, many statistical models using supervised machine learning were developed to overcome the drawbacks of dictionary based approaches. Such statistical approaches include: Decision Trees, Naive Bayes, Support Vector Machines (Haruechaiyasak et al., 2008), Trigram Markov Models (Kawtrakul and Thumkanon, 1997), and feature based models using feature extraction algorithms (Meknavin et 2 https://gi"
2020.lrec-1.858,E17-1040,0,0.0350261,"Missing"
2020.semeval-1.24,2020.lrec-1.859,0,0.309987,"Missing"
2020.semeval-1.24,W13-1404,1,0.747475,"native speakers or scholars of the respective languages (Schlechtweg et al., 2020). Human languages constantly change due to cultural, technological, and social drift. Lexical semantic changes of human languages can materialize in the form of introducing/borrowing new words, or for the existing words can involve acquiring/losing some word senses (Koch, 2016; Traugott, 2017). Computational methods for automated detection of semantic changes can be extremely helpful in the study of historical texts or corpora spanning a very long period of time, e.g., semantic analysis of 1000 years of poetry (Asgari and Chappelier, 2013; Asgari et al., 2013), or in the design of the OCR algorithm for text digitization, or in designing an information retrieval system incorporating the semantic changes (Tahmasebi et al., 2018). Applications in the study of historical texts aside, the proposed methods detect lexical-semantic drift also in the same time period for different domains. This can be useful for compiling glossaries and specific training material in certain industries where new senses are introduced for words as compared to their standard usage e.g. to facilitate a more efficient training for new employees. In the past"
2020.semeval-1.24,W16-1208,1,0.832475,"saries and specific training material in certain industries where new senses are introduced for words as compared to their standard usage e.g. to facilitate a more efficient training for new employees. In the past decade, a variety of methods were introduced in the literature for automatic detection of lexical-semantic changes (Tahmasebi et al., 2018), where we only can refer to a subset of work, including but not limited to (i) co-occurrence-based methods (Sagi et al., 2009; Basile et al., 2016), (ii) embeddingbased approaches (Bamman and Crane, 2011; Kim et al., 2014; Kulkarni et al., 2015; Asgari and Mofrad, 2016; Hamilton et al., 2016a; Asgari, 2019; Asgari et al., 2020), and (iii) topic-models-based (Frermann 201 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 201–207 Barcelona, Spain (Online), December 12, 2020. and Lapata, 2016) approaches. In this paper, we extend our recently introduced DomDrift embeddingbased approach for the detection of semantic changes (Asgari et al., 2020) introduced for the extension of the computational analyses on 1000+ languages (Asgari and Sch¨utze, 2017). Similar our earlier work on WELD(Asgari and Mofrad, 2016) and similar to (Hamilton et"
2020.semeval-1.24,D17-1011,1,0.845675,"Missing"
2020.semeval-1.24,2020.lrec-1.506,1,0.820278,"Missing"
2020.semeval-1.24,Q17-1010,0,0.229648,"edding spaces: The training of word embeddings using a language modeling objective (e.g., skip-gram) has shown to preserve the syntactic and the semantic regularities in the vector space (Mikolov et al., 2013; Pennington et al., 2014). Semantic changes impact 202 the neighborhoods in the embedding space (H3). Thus, the first step to investigate change is to train embeddings separately for the text corpora in time periods t1 and t2 (steps 1.1 and 1.2 in Figure 1). In order to generate the embedding space Ωt , the only necessary resource is the raw text. For embedding creation, we use fasttext (Bojanowski et al., 2017) which leverages subword information within the skip-gram architecture. Using sub-word information minimizes the Out-OF-Vocabulary problem for query terms (Bojanowski et al., 2017). The result of this step are separate embedding spaces Ωt1 and Ωt2 for the time periods t1 and t2 , Ωtx : Vtx → Rhtx , tx ∈ {t1 , t2 } mapping their vocabulary Vltx to continuous vector representations in Rhtx . 2. Selection of fixed words and prepare pivot sets: To measure the degree of semantic change for the given query words in Ωt1 and Ωt2 , we need some fixed points, called pivot set VP comprising words with th"
2020.semeval-1.24,D17-1118,0,0.059722,"ies. (ii) We create multiple word profiles by resampling from a set of pivot words. We show that the EmbLexChange can reliably detect the lexical-semantic changes in English, German, Swedish, and Latin achieving an average accuracy of 0.686 as second best system of the competition where the first place system achieved an accuracy of 0.687. 2 System overview Here we detail the steps of the EmbLexChange system, where the overview is depicted in Figure 1. The EmbLexChange framework is developed based on the following assumptions: H1: frequent words change at slower rates (Hamilton et al., 2016b; Dubossarsky et al., 2017). H2: the relative frequency of unchanged words is not dramatically different in different time periods/domains. H3: changes of the word sense are reflected in the context, which are captured by the embedding model resulting in changes of the neighbors in the embedding space. Thus, the relative drift of a query word (a word which we target to investigate its lexical semantic change) with respect to unchanged words in the embedding space can characterize the lexical-semantic change. The relative location of query in Ωt1 1 w2 1 w1 query word 1 w3 3. Query proﬁles creation Ωt1 Input 1.1 Train emb"
2020.semeval-1.24,Q16-1003,0,0.0559205,"Missing"
2020.semeval-1.24,D16-1229,0,0.0970921,"ing material in certain industries where new senses are introduced for words as compared to their standard usage e.g. to facilitate a more efficient training for new employees. In the past decade, a variety of methods were introduced in the literature for automatic detection of lexical-semantic changes (Tahmasebi et al., 2018), where we only can refer to a subset of work, including but not limited to (i) co-occurrence-based methods (Sagi et al., 2009; Basile et al., 2016), (ii) embeddingbased approaches (Bamman and Crane, 2011; Kim et al., 2014; Kulkarni et al., 2015; Asgari and Mofrad, 2016; Hamilton et al., 2016a; Asgari, 2019; Asgari et al., 2020), and (iii) topic-models-based (Frermann 201 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 201–207 Barcelona, Spain (Online), December 12, 2020. and Lapata, 2016) approaches. In this paper, we extend our recently introduced DomDrift embeddingbased approach for the detection of semantic changes (Asgari et al., 2020) introduced for the extension of the computational analyses on 1000+ languages (Asgari and Sch¨utze, 2017). Similar our earlier work on WELD(Asgari and Mofrad, 2016) and similar to (Hamilton et al., 2016a), DomDrift"
2020.semeval-1.24,P16-1141,0,0.129029,"ing material in certain industries where new senses are introduced for words as compared to their standard usage e.g. to facilitate a more efficient training for new employees. In the past decade, a variety of methods were introduced in the literature for automatic detection of lexical-semantic changes (Tahmasebi et al., 2018), where we only can refer to a subset of work, including but not limited to (i) co-occurrence-based methods (Sagi et al., 2009; Basile et al., 2016), (ii) embeddingbased approaches (Bamman and Crane, 2011; Kim et al., 2014; Kulkarni et al., 2015; Asgari and Mofrad, 2016; Hamilton et al., 2016a; Asgari, 2019; Asgari et al., 2020), and (iii) topic-models-based (Frermann 201 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 201–207 Barcelona, Spain (Online), December 12, 2020. and Lapata, 2016) approaches. In this paper, we extend our recently introduced DomDrift embeddingbased approach for the detection of semantic changes (Asgari et al., 2020) introduced for the extension of the computational analyses on 1000+ languages (Asgari and Sch¨utze, 2017). Similar our earlier work on WELD(Asgari and Mofrad, 2016) and similar to (Hamilton et al., 2016a), DomDrift"
2020.semeval-1.24,W14-2517,0,0.0768758,"ns. This can be useful for compiling glossaries and specific training material in certain industries where new senses are introduced for words as compared to their standard usage e.g. to facilitate a more efficient training for new employees. In the past decade, a variety of methods were introduced in the literature for automatic detection of lexical-semantic changes (Tahmasebi et al., 2018), where we only can refer to a subset of work, including but not limited to (i) co-occurrence-based methods (Sagi et al., 2009; Basile et al., 2016), (ii) embeddingbased approaches (Bamman and Crane, 2011; Kim et al., 2014; Kulkarni et al., 2015; Asgari and Mofrad, 2016; Hamilton et al., 2016a; Asgari, 2019; Asgari et al., 2020), and (iii) topic-models-based (Frermann 201 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 201–207 Barcelona, Spain (Online), December 12, 2020. and Lapata, 2016) approaches. In this paper, we extend our recently introduced DomDrift embeddingbased approach for the detection of semantic changes (Asgari et al., 2020) introduced for the extension of the computational analyses on 1000+ languages (Asgari and Sch¨utze, 2017). Similar our earlier work on WELD(Asga"
2020.semeval-1.24,W17-0239,0,0.0165267,"he English, German, Swedish, and Latin datasets used in the SemEval shared task. 1. Language-model-based embedding setup: We train fasttext (Bojanowski et al., 2017) embeddings using the skip-gram architecture for each pair of language and time period separately. In the training of fasttext, we set the window size to c = 7 and the embedding size of d = 100. In the presence of a validation set, both c and d can be optimized as the hyper-parameters for each setting. It is known that a larger c is favorable for semantics representation of words and a smaller c for a syntaxrelated representation (Lison and Kutuzov, 2017). 2. Pivot resamples creation: We firstly prepare a set of frequent words existing in both t1 and t2 for each language considering the α (relative freq.) as a way to select the top frequent words. Next, we filter this set to keep the words with the property that the ratio of their normalized frequencies is not 3 t1 (w) substantially changed in t1 and t2 , 32 < ff req reqt2 (w) < 2 resulting in our VP set. Subsequently, we draw N = 10 resamples from VP with the size of M = 5000 for each language. 3. Query profiles creation: In the next step, as presented in §2 step 3, for each query word we cre"
2020.semeval-1.24,D14-1162,0,0.0914815,"y proﬁle based on VP1 ﬁxed points Query proﬁle creation based on embedding similarity to the ﬁxed pivots using the Ωt2 embedding space The relative location of query in Ωt2 1 1 w1 w2 query word 1 w3 Figure 1: Overview of the EmbLexChange system for unsupervised detection of lexical-semantic changes. The steps are detailed in the §2.1. 2.1 EmbLexChange 1. Training language-model-based embedding spaces: The training of word embeddings using a language modeling objective (e.g., skip-gram) has shown to preserve the syntactic and the semantic regularities in the vector space (Mikolov et al., 2013; Pennington et al., 2014). Semantic changes impact 202 the neighborhoods in the embedding space (H3). Thus, the first step to investigate change is to train embeddings separately for the text corpora in time periods t1 and t2 (steps 1.1 and 1.2 in Figure 1). In order to generate the embedding space Ωt , the only necessary resource is the raw text. For embedding creation, we use fasttext (Bojanowski et al., 2017) which leverages subword information within the skip-gram architecture. Using sub-word information minimizes the Out-OF-Vocabulary problem for query terms (Bojanowski et al., 2017). The result of this step are"
2020.semeval-1.24,W09-0214,0,0.0182411,"e proposed methods detect lexical-semantic drift also in the same time period for different domains. This can be useful for compiling glossaries and specific training material in certain industries where new senses are introduced for words as compared to their standard usage e.g. to facilitate a more efficient training for new employees. In the past decade, a variety of methods were introduced in the literature for automatic detection of lexical-semantic changes (Tahmasebi et al., 2018), where we only can refer to a subset of work, including but not limited to (i) co-occurrence-based methods (Sagi et al., 2009; Basile et al., 2016), (ii) embeddingbased approaches (Bamman and Crane, 2011; Kim et al., 2014; Kulkarni et al., 2015; Asgari and Mofrad, 2016; Hamilton et al., 2016a; Asgari, 2019; Asgari et al., 2020), and (iii) topic-models-based (Frermann 201 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 201–207 Barcelona, Spain (Online), December 12, 2020. and Lapata, 2016) approaches. In this paper, we extend our recently introduced DomDrift embeddingbased approach for the detection of semantic changes (Asgari et al., 2020) introduced for the extension of the computationa"
2020.semeval-1.24,2020.semeval-1.1,0,0.10815,"Missing"
2021.acl-demo.8,P18-1141,1,0.904085,"Missing"
2021.acl-demo.8,D17-2008,0,0.0224968,"inspected and flagged for correction. 2 ments to extract names from the PBC. One of the first attempts to index the Bible and align words in multiple languages were Strong’s numbers (Strong, 2009[1890]); they tag words with similar meanings with the same ID. Mayer and Cysouw (2014) created an inverted index of word ¨ forms. Ostling (2014) align massively parallel corpora simultaneously. We use the Eflomal word aligner by the same authorsostling2016efficient. Finally, we review work on Word Alignment Browsers. Gilmanov et al. (2014)’s tool supports visualization and editing of word alignments. Akbik and Vollgraf (2017) use co-occurrence weights for word alignment and provide a tool for the inspection of annotation projection. Aulamo et al. (2020)’s filtering tool increases the quality of (mined) parallel corpora. Gra¨en et al. (2017) rely on linguistic preprocessing, target corpus and word alignment exploration, do not show the graph of alignment edges and do not provide a dictionary view. While there is commonality with this prior work, ParCourE is distinguished by both its functionality and its motivating use cases: an important use case for us are typological searches; linguistic preprocessing is not ava"
2021.acl-demo.8,N13-1073,0,0.197055,"omplementary functionality in prior work would be useful additions to ParCourE. Another source of useful additional functionality would be work on embedding learning ¨ (Dufter et al., 2018; Kurfal and Ostling, 2018) and machine translation (Tiedemann, 2018; Santy et al., 2019; Mueller et al., 2020) for PBC. Related Work Word Alignment is an important tool for typological analysis (Lewis and Xia, 2008) and annotation ¨ projection (Yarowsky et al., 2001; Ostling, 2015; Asgari and Sch¨utze, 2017). Statistical models such as IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013) ¨ and Eflomal (Ostling and Tiedemann, 2016b) are widely used. Recently, neural models were proposed, such as SimAlign (Jalili Sabet et al., 2020), Awesome-align (Dou and Neubig, 2021), and methods that are based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). We use Eflomal and SimAlign for generating alignments. Resources. There are many online resources that enable typological research. WALS (Dryer and Haspelmath, 2013) provides manually created features for more than 2000 languages. We prepare a multiparallel corpus for investigating these features on real data. htt"
2021.acl-demo.8,D17-1011,1,0.85998,"Missing"
2021.acl-demo.8,D16-1026,0,0.0375752,"Missing"
2021.acl-demo.8,2020.acl-demos.20,0,0.0245321,"ds in multiple languages were Strong’s numbers (Strong, 2009[1890]); they tag words with similar meanings with the same ID. Mayer and Cysouw (2014) created an inverted index of word ¨ forms. Ostling (2014) align massively parallel corpora simultaneously. We use the Eflomal word aligner by the same authorsostling2016efficient. Finally, we review work on Word Alignment Browsers. Gilmanov et al. (2014)’s tool supports visualization and editing of word alignments. Akbik and Vollgraf (2017) use co-occurrence weights for word alignment and provide a tool for the inspection of annotation projection. Aulamo et al. (2020)’s filtering tool increases the quality of (mined) parallel corpora. Gra¨en et al. (2017) rely on linguistic preprocessing, target corpus and word alignment exploration, do not show the graph of alignment edges and do not provide a dictionary view. While there is commonality with this prior work, ParCourE is distinguished by both its functionality and its motivating use cases: an important use case for us are typological searches; linguistic preprocessing is not available for many PBC languages; ParCourE can be used as an interactive explorer (but is not a fully-automated pipeline for a specif"
2021.acl-demo.8,J93-2003,0,0.192166,"Missing"
2021.acl-demo.8,D19-1453,0,0.0125647,"y et al., 2019; Mueller et al., 2020) for PBC. Related Work Word Alignment is an important tool for typological analysis (Lewis and Xia, 2008) and annotation ¨ projection (Yarowsky et al., 2001; Ostling, 2015; Asgari and Sch¨utze, 2017). Statistical models such as IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013) ¨ and Eflomal (Ostling and Tiedemann, 2016b) are widely used. Recently, neural models were proposed, such as SimAlign (Jalili Sabet et al., 2020), Awesome-align (Dou and Neubig, 2021), and methods that are based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). We use Eflomal and SimAlign for generating alignments. Resources. There are many online resources that enable typological research. WALS (Dryer and Haspelmath, 2013) provides manually created features for more than 2000 languages. We prepare a multiparallel corpus for investigating these features on real data. http://panlex.org is an online dictionary project with 2500 dictionaries covering 5700 languages and BabelNet (Navigli and Ponzetto, 2012) is a large semantic network covering 500 languages, but their information is generally on the type level, without access to e"
2021.acl-demo.8,C10-1027,0,0.0220073,"lexical error in a particular context c by mistranslating source word ws with target word wt . The L EXICON view can be consulted for ws and the user can then click on the erroneous target word wt to get back to a M ULTA LIGN view of aligned sentence pairs containing ws and wt . She can then analyze why the MT system mismatched c with these contexts. Examples of the desired translation are easy to find and inspect to support the formation of hypotheses as to the source of the error. iii) For multi-source approaches to MT (Zoph and Knight, 2016; Firat et al., 2016; Libovick´y and Helcl, 2017; Crego et al., 2010), ParCourE supports the inspection of all input sentences together. The MT system output can also be loaded into ParCourE for a view that contains all input sentences and the output sentence. Since any of the input sentences can be responsible for an error in multi-source MT, this facilitates analysis and hypothesis formation as to what caused a specific error. Figure 6: Use case 2, grammatical differentiation. English “who” has three different translations in this Spanish example: relative pronoun (“que”), and singular (“qui´en)” and plural (“qui´enes”) interrogative pronoun. matical case (Fi"
2021.acl-demo.8,gilmanov-etal-2014-swift,0,0.0729553,"Missing"
2021.acl-demo.8,N19-1423,0,0.00538545,"urce and target translation. Creating queries for typology research can take time. Thus, M ULTA LIGN allows the user to save and retrieve (e) queries. 65 # editions # languages 1758 1334 # verses # verses / # editions # tokens / # verses dex to support search-as-you-type capability and a standard index for normal queries. Alignment Generation. SimAlign (Jalili Sabet et al., 2020) is a recent word alignment method that uses representations from pretrained language models to align sentences. It has achieved better results than statistical word aligners. For the languages that multilingual BERT (Devlin et al., 2019) supports, we use SimAlign to generate word alignments. For the remaining languages, we use ¨ Eflomal (Ostling and Tiedemann, 2016a), an efficient word aligner using a Bayesian model with Markov Chain Monte Carlo (MCMC) inference. The alignments generated by SimAlign are symmetric. We use atools3 and the grow-diag-final-and heuristic to symmetrize Eflomal alignments. Lexicon Induction. We exploit the generated word alignments to induce lexicons for all 889,111 language pairs. To this end, we consider aligned words as translations of each other. For a given word from the source language, we cou"
2021.acl-demo.8,W17-0231,0,0.0468593,"Missing"
2021.acl-demo.8,2021.eacl-main.181,0,0.0283411,"., 2018; Kurfal and Ostling, 2018) and machine translation (Tiedemann, 2018; Santy et al., 2019; Mueller et al., 2020) for PBC. Related Work Word Alignment is an important tool for typological analysis (Lewis and Xia, 2008) and annotation ¨ projection (Yarowsky et al., 2001; Ostling, 2015; Asgari and Sch¨utze, 2017). Statistical models such as IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013) ¨ and Eflomal (Ostling and Tiedemann, 2016b) are widely used. Recently, neural models were proposed, such as SimAlign (Jalili Sabet et al., 2020), Awesome-align (Dou and Neubig, 2021), and methods that are based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). We use Eflomal and SimAlign for generating alignments. Resources. There are many online resources that enable typological research. WALS (Dryer and Haspelmath, 2013) provides manually created features for more than 2000 languages. We prepare a multiparallel corpus for investigating these features on real data. http://panlex.org is an online dictionary project with 2500 dictionaries covering 5700 languages and BabelNet (Navigli and Ponzetto, 2012) is a large semantic network covering 500 languag"
2021.acl-demo.8,J03-1002,0,0.0428544,"Missing"
2021.acl-demo.8,E14-4024,0,0.0187484,"nd L EXICON views) are interlinked, resulting in a richer user experience. iv) ParCourE has a generic design and can be set up for any parallel corpus. This is useful for analyzing and managing parallel corpora; e.g., errors in an automatically mined parallel corpus can be inspected and flagged for correction. 2 ments to extract names from the PBC. One of the first attempts to index the Bible and align words in multiple languages were Strong’s numbers (Strong, 2009[1890]); they tag words with similar meanings with the same ID. Mayer and Cysouw (2014) created an inverted index of word ¨ forms. Ostling (2014) align massively parallel corpora simultaneously. We use the Eflomal word aligner by the same authorsostling2016efficient. Finally, we review work on Word Alignment Browsers. Gilmanov et al. (2014)’s tool supports visualization and editing of word alignments. Akbik and Vollgraf (2017) use co-occurrence weights for word alignment and provide a tool for the inspection of annotation projection. Aulamo et al. (2020)’s filtering tool increases the quality of (mined) parallel corpora. Gra¨en et al. (2017) rely on linguistic preprocessing, target corpus and word alignment exploration, do not show the"
2021.acl-demo.8,P15-2034,0,0.0482263,"Missing"
2021.acl-demo.8,2020.findings-emnlp.147,1,0.882262,"Missing"
2021.acl-demo.8,I08-2093,0,0.098061,"Missing"
2021.acl-demo.8,D19-3018,0,0.0246576,"Missing"
2021.acl-demo.8,P17-2031,0,0.0402234,"Missing"
2021.acl-demo.8,W12-0209,0,0.012495,"om/license. 6 Bootstrap ParCourE Use Cases Languages differ in how they encode meanings/functions. There are various aspects that make such differences an interesting problem when dealing with a dataset that has good coverage of the entire variation of the world’s languages. (i) Many such differences between languages are not widely acknowledged in linguistic theory, so to document the extent of variation becomes a discovery of sorts. For example, the fact that interrogative words might distinguish between singular and plural (Figure 6) turns out to be a typologically salient differentiation (Mayer and Cysouw, 2012). (ii) The variation of linguistic marking is even stronger in the domain of grammatical function, like the differentiation between the interrogative and relative pronoun in Figure 6. (iii) In lexical semantics, ParCourE supports the investigation of how languages carve up the meaning space differently (cf. Figure 5), especially when it comes to the ≈1000 low-resource languages covered in PBC. Massively parallel texts are an ideal resource to investigate such variation (Haspelmath, 2003). Grammatical differences between languages, like differences in word order, have a long history in research"
2021.acl-demo.8,mayer-cysouw-2014-creating,0,0.357404,"res across languages and eases maintenance for globally operating companies, but also helps save languages from digital extinction and fosters more diversity in NLP techniques. There are extensive resources that can be used for massively multilingual typological research, such as WALS (Dryer and Haspelmath, 2013), Glottolog (Hammarstrm et al., 2020), BabelNet (Navigli and Ponzetto, 2012) or http://panlex.org. Many of them are manually created or crowdsourced, which guarantees high quality, but limits coverage, both in terms of content and languages. We work on the Parallel Bible Corpus (PBC) (Mayer and Cysouw, 2014), covering 1334 languages. More specifically, we provide a wordaligned version of PBC, created using state-of-theart word alignment tools. As word alignments themselves are only of limited use, we provide an interactive online tool1 that allows effective browsing of the alignments. The main contributions of this work are: i) We provide a word-aligned version of the Parallel Bible Corpus (PBC) spanning 1334 languages and a total of 20M sentences (‘verses’). For the alignment we use the state-of-the-art alignment methods SimA¨ lign (Jalili Sabet et al., 2020) and Eflomal (Ostling and Tiedemann,"
2021.acl-demo.8,L18-1263,0,0.016228,"uages and BabelNet (Navigli and Ponzetto, 2012) is a large semantic network covering 500 languages, but their information is generally on the type level, without access to example contexts. In contrast, ParCourE supports the exploration of word translations across 1334 languages in context. Another line of work uses the Parallel Bible Corpus (PBC) for analysis. Asgari and Sch¨utze (2017) investigate tense typology across PBC languages. Xia and Yarowsky (2017) created a multiway alignment based on fast-align (Dyer et al., 2013) and extracted resources such as paraphrases for 27 Bible editions. Wu et al. (2018) used align3 Features ParCourE’s user facing functionality can be divided into three main parts: M ULTA LIGN and L EXICON views and interconnections between the two. 3.1 Multiparallel Alignment Browser: M ULTA LIGN ParCourE allows the user to search through the parallel corpus and check word alignments in a multiparallel corpus. An overview of M ULTA LIGN is shown in Figure 2. In the search field (a(1)), the user can enter a text query and select (a(2)) multiple sentences for alignment. For narrowing the search scope, the 64 Figure 3: L EXICON view example: for the English word “confusion”, th"
2021.acl-demo.8,2020.lrec-1.458,0,0.0831824,"Missing"
2021.acl-demo.8,I17-2076,0,0.0194107,"multiparallel corpus for investigating these features on real data. http://panlex.org is an online dictionary project with 2500 dictionaries covering 5700 languages and BabelNet (Navigli and Ponzetto, 2012) is a large semantic network covering 500 languages, but their information is generally on the type level, without access to example contexts. In contrast, ParCourE supports the exploration of word translations across 1334 languages in context. Another line of work uses the Parallel Bible Corpus (PBC) for analysis. Asgari and Sch¨utze (2017) investigate tense typology across PBC languages. Xia and Yarowsky (2017) created a multiway alignment based on fast-align (Dyer et al., 2013) and extracted resources such as paraphrases for 27 Bible editions. Wu et al. (2018) used align3 Features ParCourE’s user facing functionality can be divided into three main parts: M ULTA LIGN and L EXICON views and interconnections between the two. 3.1 Multiparallel Alignment Browser: M ULTA LIGN ParCourE allows the user to search through the parallel corpus and check word alignments in a multiparallel corpus. An overview of M ULTA LIGN is shown in Figure 2. In the search field (a(1)), the user can enter a text query and sel"
2021.acl-demo.8,H01-1035,0,0.314318,"Missing"
2021.acl-demo.8,2020.acl-main.146,0,0.0112788,"ller et al., 2020) for PBC. Related Work Word Alignment is an important tool for typological analysis (Lewis and Xia, 2008) and annotation ¨ projection (Yarowsky et al., 2001; Ostling, 2015; Asgari and Sch¨utze, 2017). Statistical models such as IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003), fast-align (Dyer et al., 2013) ¨ and Eflomal (Ostling and Tiedemann, 2016b) are widely used. Recently, neural models were proposed, such as SimAlign (Jalili Sabet et al., 2020), Awesome-align (Dou and Neubig, 2021), and methods that are based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). We use Eflomal and SimAlign for generating alignments. Resources. There are many online resources that enable typological research. WALS (Dryer and Haspelmath, 2013) provides manually created features for more than 2000 languages. We prepare a multiparallel corpus for investigating these features on real data. http://panlex.org is an online dictionary project with 2500 dictionaries covering 5700 languages and BabelNet (Navigli and Ponzetto, 2012) is a large semantic network covering 500 languages, but their information is generally on the type level, without access to example contexts. In co"
2021.acl-demo.8,N16-1004,0,0.0125724,"pairs. ii) Suppose an MT system trained on the parallel corpus makes a lexical error in a particular context c by mistranslating source word ws with target word wt . The L EXICON view can be consulted for ws and the user can then click on the erroneous target word wt to get back to a M ULTA LIGN view of aligned sentence pairs containing ws and wt . She can then analyze why the MT system mismatched c with these contexts. Examples of the desired translation are easy to find and inspect to support the formation of hypotheses as to the source of the error. iii) For multi-source approaches to MT (Zoph and Knight, 2016; Firat et al., 2016; Libovick´y and Helcl, 2017; Crego et al., 2010), ParCourE supports the inspection of all input sentences together. The MT system output can also be loaded into ParCourE for a view that contains all input sentences and the output sentence. Since any of the input sentences can be responsible for an error in multi-source MT, this facilitates analysis and hypothesis formation as to what caused a specific error. Figure 6: Use case 2, grammatical differentiation. English “who” has three different translations in this Spanish example: relative pronoun (“que”), and singular (“qui"
2021.acl-long.279,2020.nlp4convai-1.5,0,0.0359913,"Missing"
2021.acl-long.279,Q18-1003,1,0.845267,"ecial tokens), we show that even when keeping the pretrained vocabulary fixed, employing it in a morphologically correct way leads to better performance.14 14 There are also studies that analyze morphological aspects of PLMs without a focus on questions surrounding segmentation (Edmiston, 2020; Klemen et al., 2020). Most NLP studies on derivational morphology have been devoted to the question of how semantic representations of derivationally complex words can be enhanced by including morphological information (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Cotterell and Schütze, 2018), and how affix embeddings can be computed (Lazaridou et al., 2013; Kisselew et al., 2015; Padó et al., 2016). Cotterell et al. (2017), Vylomova et al. (2017), and Deutsch et al. (2018) propose sequence-to-sequence models for the generation of derivationally complex words. Hofmann et al. (2020a) address the same task using BERT. In contrast, we analyze how different input segmentations affect the semantic representations of derivationally complex words in PLMs, a question that has not been addressed before. 5 Conclusion We have examined how the input segmentation of PLMs, specifically BERT, af"
2021.acl-long.279,W13-3512,0,0.372845,"esentations that include their internal structure. Complex-word processing is driven by analogical processes over the mental lexicon (Rácz et al., 2020). 2.2 Complex Words in NLP and PLMs Most models of word meaning proposed in NLP can be roughly assigned to either the single-route or dual-route approach. Word embeddings that represent complex words as whole-word vectors (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014) can be seen as single-route storage models. Word embeddings that represent complex words as a function of subword or morpheme vectors (Schütze, 1992; Luong et al., 2013) can be seen as single-route computation models. Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches. Where are PLMs to be located in this taxonomy? PLMs represent many complex words as wholeword vectors (which are fully stored). Similarly to how character-based models represent word meaning (Kim et al., 2016; Adel"
2021.acl-long.279,2020.coling-main.4,0,0.0324455,"meaning of complex words from the subwords when the wholeword meaning is not stored in the model weights and the subwords are not morphological. 4 Related Work Several recent studies have examined how the performance of PLMs is affected by their input segmentation. Tan et al. (2020) show that tokenizing inflected words into stems and inflection symbols allows BERT to generalize better on non-standard inflections. Bostrom and Durrett (2020) pretrain RoBERTa with different tokenization methods and find tokenizations that align more closely with morphology to perform better on a number of tasks. Ma et al. (2020) show that providing BERT with character-level information also leads to enhanced performance. Relatedly, studies from automatic speech recognition have demonstrated that morphological decomposition improves the perplexity of language models (Fang et al., 2015; Jain et al., 2020). Whereas these studies change the vocabulary of input tokens (e.g., by adding special tokens), we show that even when keeping the pretrained vocabulary fixed, employing it in a morphologically correct way leads to better performance.14 14 There are also studies that analyze morphological aspects of PLMs without a focu"
2021.acl-long.279,2020.acl-main.645,0,0.0302368,"Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3594–3608 August 1–6, 2021. ©2021 Association for Computational Linguistics The topic of this paper is related to the more fundamental question of how PLMs represent the meaning of complex words in the first place. So far, most studies have focused on methods of representation extraction, using ad-hoc heuristics such as averaging the subword embeddings (Pinter et al., 2020; Sia et al., 2020; Vuli´c et al., 2020) or taking the first subword embedding (Devlin et al., 2019; Heinzerling and Strube, 2019; Martin et al., 2020). While not resolving the issue, we lay the theoretical groundwork for more systematic analyses by showing that PLMs can be regarded as serial dual-route models (Caramazza et al., 1988), i.e., the meanings of complex words are either stored or else need to be computed from the subwords. Contributions. We present the first study examining how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex English words. We show that PLMs can be interpreted as serial dualroute models, which implies that maximally meaningful input tokens should allow for"
2021.acl-long.279,P09-1113,0,0.0205639,"Missing"
2021.acl-long.279,D19-1018,0,0.0131516,"es of semantic probing tasks on which derivational segmentation substantially outperforms BERT’s WordPiece segmentation. This suggests that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used. We also publish three large datasets of derivationally complex words with corresponding semantic properties.1 2 2.1 How Are Complex Words Processed? Complex Words in Psycholinguistics The question of how complex words are processed has been at the center of psycholinguistic research over the last decades (see Leminen et al. (2019) for a recent review). Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al., 2004; Taft, 2004; Rastle and Davis, 2008). 1 We make our code and data available at https:// github.com/valentinhofmann/superbiza"
2021.acl-long.279,C16-1122,0,0.024212,"t way leads to better performance.14 14 There are also studies that analyze morphological aspects of PLMs without a focus on questions surrounding segmentation (Edmiston, 2020; Klemen et al., 2020). Most NLP studies on derivational morphology have been devoted to the question of how semantic representations of derivationally complex words can be enhanced by including morphological information (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Cotterell and Schütze, 2018), and how affix embeddings can be computed (Lazaridou et al., 2013; Kisselew et al., 2015; Padó et al., 2016). Cotterell et al. (2017), Vylomova et al. (2017), and Deutsch et al. (2018) propose sequence-to-sequence models for the generation of derivationally complex words. Hofmann et al. (2020a) address the same task using BERT. In contrast, we analyze how different input segmentations affect the semantic representations of derivationally complex words in PLMs, a question that has not been addressed before. 5 Conclusion We have examined how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex words. Drawing upon insights from psycholinguistics, we"
2021.acl-long.279,D14-1162,0,0.0864135,"Missing"
2021.acl-long.279,2020.findings-emnlp.138,0,0.0317937,"ons are involved, e.g., in query processing (Kacprzak et al., 2017). 3594 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3594–3608 August 1–6, 2021. ©2021 Association for Computational Linguistics The topic of this paper is related to the more fundamental question of how PLMs represent the meaning of complex words in the first place. So far, most studies have focused on methods of representation extraction, using ad-hoc heuristics such as averaging the subword embeddings (Pinter et al., 2020; Sia et al., 2020; Vuli´c et al., 2020) or taking the first subword embedding (Devlin et al., 2019; Heinzerling and Strube, 2019; Martin et al., 2020). While not resolving the issue, we lay the theoretical groundwork for more systematic analyses by showing that PLMs can be regarded as serial dual-route models (Caramazza et al., 1988), i.e., the meanings of complex words are either stored or else need to be computed from the subwords. Contributions. We present the first study examining how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally comple"
2021.acl-long.279,C14-1015,0,0.104751,"d in NLP can be roughly assigned to either the single-route or dual-route approach. Word embeddings that represent complex words as whole-word vectors (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014) can be seen as single-route storage models. Word embeddings that represent complex words as a function of subword or morpheme vectors (Schütze, 1992; Luong et al., 2013) can be seen as single-route computation models. Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches. Where are PLMs to be located in this taxonomy? PLMs represent many complex words as wholeword vectors (which are fully stored). Similarly to how character-based models represent word meaning (Kim et al., 2016; Adel et al., 2017), they can also store the meaning of frequent complex words that are segmented into subwords, i.e., frequent sub3595 word collocations, in their model weights. When the complex-word meaning is neither store"
2021.acl-long.279,W18-1209,0,0.0183134,"beddings that represent complex words as whole-word vectors (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014) can be seen as single-route storage models. Word embeddings that represent complex words as a function of subword or morpheme vectors (Schütze, 1992; Luong et al., 2013) can be seen as single-route computation models. Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches. Where are PLMs to be located in this taxonomy? PLMs represent many complex words as wholeword vectors (which are fully stored). Similarly to how character-based models represent word meaning (Kim et al., 2016; Adel et al., 2017), they can also store the meaning of frequent complex words that are segmented into subwords, i.e., frequent sub3595 word collocations, in their model weights. When the complex-word meaning is neither stored as a whole-word vector nor in the model weights, PLMs compute the meaning as a compositional function o"
2021.acl-long.279,P16-1162,0,0.0598999,"XLNet (Yang et al., 2019), ELECTRA (Clark et al., 2020), and T5 (Raffel et al., 2020) have yielded substantial improvements on a range of NLP tasks. What linguistic properties do they have? Various studies have tried to illuminate this question, with a focus on syntax (Hewitt and Manning, 2019; Jawahar et al., 2019) and semantics (Ethayarajh, 2019; Ettinger, 2020; Vuli´c et al., 2020). One common characteristic of PLMs is their input segmentation: PLMs are based on fixed-size vocabularies of words and subwords that are generated by compression algorithms such as bytepair encoding (Gage, 1994; Sennrich et al., 2016) and WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016). The segmentations produced by these algorithms are linguistically questionable at times (Church, 2020), which has been shown to worsen performance on certain downstream tasks (Bostrom and Durrett, 2020; Hofmann et al., 2020a). However, the wider implications of these findings, particularly with regard to the generalization capabilities of PLMs, are still poorly understood. Here, we address a central aspect of this issue, namely how the input segmentation affects the semantic representations of PLMs, taking BERT as the example PLM."
2021.acl-long.279,2020.emnlp-main.135,0,0.0220699,"., in query processing (Kacprzak et al., 2017). 3594 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3594–3608 August 1–6, 2021. ©2021 Association for Computational Linguistics The topic of this paper is related to the more fundamental question of how PLMs represent the meaning of complex words in the first place. So far, most studies have focused on methods of representation extraction, using ad-hoc heuristics such as averaging the subword embeddings (Pinter et al., 2020; Sia et al., 2020; Vuli´c et al., 2020) or taking the first subword embedding (Devlin et al., 2019; Heinzerling and Strube, 2019; Martin et al., 2020). While not resolving the issue, we lay the theoretical groundwork for more systematic analyses by showing that PLMs can be regarded as serial dual-route models (Caramazza et al., 1988), i.e., the meanings of complex words are either stored or else need to be computed from the subwords. Contributions. We present the first study examining how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex English words. W"
2021.acl-long.279,2020.emnlp-main.455,0,0.024652,"(overseasoned, inkinetic, promosque). This case is particularly detrimental since it not only makes it difficult to recover the meaning of the stem but also creates associations with unrelated meanings, sometimes even opposite meanings as in the case of superbizarre. The three classes thus underscore the difficulty of inferring the meaning of complex words from the subwords when the wholeword meaning is not stored in the model weights and the subwords are not morphological. 4 Related Work Several recent studies have examined how the performance of PLMs is affected by their input segmentation. Tan et al. (2020) show that tokenizing inflected words into stems and inflection symbols allows BERT to generalize better on non-standard inflections. Bostrom and Durrett (2020) pretrain RoBERTa with different tokenization methods and find tokenizations that align more closely with morphology to perform better on a number of tasks. Ma et al. (2020) show that providing BERT with character-level information also leads to enhanced performance. Relatedly, studies from automatic speech recognition have demonstrated that morphological decomposition improves the perplexity of language models (Fang et al., 2015; Jain"
2021.acl-long.279,2020.emnlp-main.586,0,0.0547589,"Missing"
2021.acl-long.279,E17-2019,0,0.0209529,"are also studies that analyze morphological aspects of PLMs without a focus on questions surrounding segmentation (Edmiston, 2020; Klemen et al., 2020). Most NLP studies on derivational morphology have been devoted to the question of how semantic representations of derivationally complex words can be enhanced by including morphological information (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Cotterell and Schütze, 2018), and how affix embeddings can be computed (Lazaridou et al., 2013; Kisselew et al., 2015; Padó et al., 2016). Cotterell et al. (2017), Vylomova et al. (2017), and Deutsch et al. (2018) propose sequence-to-sequence models for the generation of derivationally complex words. Hofmann et al. (2020a) address the same task using BERT. In contrast, we analyze how different input segmentations affect the semantic representations of derivationally complex words in PLMs, a question that has not been addressed before. 5 Conclusion We have examined how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex words. Drawing upon insights from psycholinguistics, we have deduced a conceptual interpretation of PLMs"
2021.acl-long.279,Q17-1021,0,0.0304569,"Missing"
2021.acl-long.447,D19-1607,0,0.0452479,"Missing"
2021.acl-long.447,2020.emnlp-main.40,0,0.467333,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,2020.emnlp-main.369,0,0.514216,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,P19-1493,0,0.364814,"s, we make our sampled few shots publicly available.1 1 Introduction Multilingual pretrained encoders like multilingual BERT (mBERT; Devlin et al. (2019)) and XLMR (Conneau et al., 2020) are the top performers in crosslingual tasks such as natural language inference (Conneau et al., 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English f"
2021.acl-long.447,2020.acl-main.467,0,0.054412,"Missing"
2021.acl-long.447,P19-1459,0,0.0154384,"he clock didn’t even work one minute ... Visually, however, very nice.”) Pretrained multilingual encoders are shown to learn and store “language-agnostic” features (Pires et al., 2019; Zhao et al., 2020); §5.3 shows that source-training mBERT on EN substantially benefits other languages, even for difficult semantic tasks like PAWSX. Conditioning on such languageagnostic features, we expect that the buckets should lead to good understanding and reasoning capabilities for a target language. However, plain few-shot finetuning still relies heavily on unintended shallow lexical cues and shortcuts (Niven and Kao, 2019; Geirhos et al., 2020) that generalize poorly. Other open research questions for future work arise: How do we overcome this excessive reliance on lexical features? How can we leverage language-agnostic features with few shots? Our standardized buckets, baseline results, and analyses are the initial step towards researching and answering these questions. 5.5 Target-Adapting Methods SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning"
2021.acl-long.447,P19-1015,0,0.0280507,"in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for each target (i.e., non-English) language of a task to get a reliable estimation of model performance. CLS Tasks. For MLDoc and MARC, e"
2021.acl-long.447,N18-1101,0,0.0283863,"l selection in this stage. 4 Experimental Setup We consider three types of tasks requiring varying degrees of semantic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to a"
2021.acl-long.447,D19-1077,0,0.12812,"019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially im"
2021.acl-long.447,2020.repl4nlp-1.16,0,0.269393,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.emnlp-main.362,0,0.385788,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.findings-emnlp.29,0,0.094877,"Missing"
2021.acl-long.447,2020.emnlp-main.608,0,0.0774621,"Missing"
2021.acl-long.447,D19-1382,0,0.0227255,"tic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for"
2021.acl-long.447,2020.emnlp-main.660,0,0.0765103,"Missing"
2021.acl-long.447,N18-1109,0,0.0145892,"e task instead of between different tasks. Few-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images. Methods like prototypical networks (Snell et al., 2017) and modelagnostic meta-learning (MAML; Finn et al. (2017)) have also been applied to many monolingual (typically English) NLP tasks such as relation classification (Han et al., 2018; Gao et al., 2019), namedentity recognition (Hou et al., 2020a), word sense disambiguation (Holla et al., 2020), and text classification (Yu et al., 2018; Yin, 2020; Yin et al., 2020; Bansal et al., 2020; Gupta et al., 2020). However, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020). Inspired by this work, we compare various fewshot finetuning methods from computer vision in the context of FS-XLT. Task Performance Variance. Deep neural networks’ performance on NLP tasks is bound to exhibit large varian"
2021.acl-long.447,N19-1131,0,0.034028,"Missing"
2021.acl-long.542,Q17-1010,0,0.328053,"linguistic context. Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of NLP tasks involving semantic variability. We highlight potential application scenarios by means of qualitative and quantitative analyses on four English datasets. 1 eij ˜(k) e d (k) φij Introduction Over the last decade, word embeddings have revolutionized the field of NLP. Traditional methods such as LSA (Deerwester et al., 1990), word2vec (Mikolov et al., 2013a,b), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017) compute static word embeddings, i.e., they represent words as a single vector. From a theoretical standpoint, this way of modeling lexical semantics is problematic since it ignores the variability of word meaning in different linguistic contexts (e.g., polysemy) as well as different extralinguistic contexts (e.g., temporal and social variation). The first shortcoming was addressed by the introduction of contextualized word embeddings that represent words as vectors varying across linguistic contexts. This allows them to capture more complex characteristics of word meaning, including polysemy."
2021.acl-long.542,2020.semeval-1.14,0,0.01579,"than one dimension of variation. Recently, a few studies have taken first steps in this direction by using genre information within a Bayesian model of semantic change (Frermann and Lapata, 2016; Perrone et al., 2019) and including social variables in training diachronic word embeddings (Jawahar and Seddah, 2019). In addition, to capture the full range of lexical-semantic variability, dynamic word embeddings should also be contextualized. Crucially, while contextualized word embeddings have been used to investigate semantic change (Giulianelli, 2019; Hu et al., 2019; Giulianelli et al., 2020; Kutuzov and Giulianelli, 2020; Martinc et al., 2020a,b), the word embeddings employed in these studies are not dynamic, i.e., they represent a word in a specific linguistic context by the same contextualized word embedding independent of extralinguistic context or are fit to different time periods as separate models.2 3 Model 3.1 Model Overview   Given a sequence of words X = x(1) , . . . , x(K) and corresponding non-contextualized embeddings  (1)  (K) E = e ,...,e , contextualizing language models compute the contextualized embedding of a particular word x(k) , h(k) , as a function c of its non-contextualized embeddi"
2021.acl-long.542,C18-1117,0,0.0242688,"h et al., 2020a,b; Yao et al., 2020). Other studies have demonstrated that performance on a diverse set of tasks can be increased by including temporal (Jaidka et al., 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019). The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020). Besides dynamic word embeddings, many studies on lexical semantic change detection use methods based on static word embeddings (Kim et al., 2014; Kulkarni et al., 2015), e.g., the alignment of static word embedding spaces (Hamilton et al., 2016b). However, such approaches come at the cost of modeling disadvantages (Bamler and Mandt, 2017). Sociolinguistics"
2021.acl-long.542,P19-1247,0,0.020185,"er and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al., 2017, 2018; Oba et al., 2019; Welch et al., 2020a,b; Yao et al., 2020). Other studies have demonstrated that performance on a diverse set of tasks can be increased by including temporal (Jaidka et al., 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019). The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020). Besides dynamic word embeddings, many studies on lexical semantic change detection use methods based on static word embeddings (Kim et al., 2014;"
2021.acl-long.542,D14-1162,0,0.101752,"Missing"
2021.acl-long.542,W19-4707,0,0.0169673,"deling disadvantages (Bamler and Mandt, 2017). Sociolinguistics has shown that temporal and social variation in language are tightly interwoven: innovations such as a new word sense in the case of lexical semantics spread through the language community along social ties (Milroy, 1980, 1992; 6971 Labov, 2001; Pierrehumbert, 2012). However, most proposed dynamic word embedding types cannot capture more than one dimension of variation. Recently, a few studies have taken first steps in this direction by using genre information within a Bayesian model of semantic change (Frermann and Lapata, 2016; Perrone et al., 2019) and including social variables in training diachronic word embeddings (Jawahar and Seddah, 2019). In addition, to capture the full range of lexical-semantic variability, dynamic word embeddings should also be contextualized. Crucially, while contextualized word embeddings have been used to investigate semantic change (Giulianelli, 2019; Hu et al., 2019; Giulianelli et al., 2020; Kutuzov and Giulianelli, 2020; Martinc et al., 2020a,b), the word embeddings employed in these studies are not dynamic, i.e., they represent a word in a specific linguistic context by the same contextualized word embe"
2021.acl-long.542,N18-1202,0,0.35607,"mantics is problematic since it ignores the variability of word meaning in different linguistic contexts (e.g., polysemy) as well as different extralinguistic contexts (e.g., temporal and social variation). The first shortcoming was addressed by the introduction of contextualized word embeddings that represent words as vectors varying across linguistic contexts. This allows them to capture more complex characteristics of word meaning, including polysemy. Contextualized word embeddings are widely used in NLP, constituting the semantic backbone of pretrained language models (PLMs) such as ELMo (Peters et al., 2018a), BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), XLNet Figure 1: Dynamic contextualized word embeddings. ˜(k) ( ) is mapped to a dynamic A static embedding e (k) embedding eij ( ) by a function d that takes time and social space into account. The scattered points ( ) are (k) (k) contextualized versions of eij . Variability in φij indicates semantic dynamics across time and social space. The embeddings have 768 dimensions. (Yang et al., 2019), ELECTRA (Clark et al., 2020), and T5 (Raffel et al., 2020). A concurrent line of work focused on the second shortcoming of static word embed"
2021.acl-long.542,D18-1179,0,0.139446,"mantics is problematic since it ignores the variability of word meaning in different linguistic contexts (e.g., polysemy) as well as different extralinguistic contexts (e.g., temporal and social variation). The first shortcoming was addressed by the introduction of contextualized word embeddings that represent words as vectors varying across linguistic contexts. This allows them to capture more complex characteristics of word meaning, including polysemy. Contextualized word embeddings are widely used in NLP, constituting the semantic backbone of pretrained language models (PLMs) such as ELMo (Peters et al., 2018a), BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), XLNet Figure 1: Dynamic contextualized word embeddings. ˜(k) ( ) is mapped to a dynamic A static embedding e (k) embedding eij ( ) by a function d that takes time and social space into account. The scattered points ( ) are (k) (k) contextualized versions of eij . Variability in φij indicates semantic dynamics across time and social space. The embeddings have 768 dimensions. (Yang et al., 2019), ELECTRA (Clark et al., 2020), and T5 (Raffel et al., 2020). A concurrent line of work focused on the second shortcoming of static word embed"
2021.acl-long.542,2020.findings-emnlp.138,0,0.0253643,"l and tempo(k) ral contexts si and tj , eij , (k) (k) simij = cos φij , (k) (14) (k) ˜(k) and eij (Figwhere φij is the angle between e ure 1).14 To find words with a high degree of variability, we compute the standard deviation of (k) simij based on all si and tj in which a given word x(k) occurs in the data,   (k) (k) σsim = σ {simij |(x(k) , si , tj ) ∈ D} , (15) where we take the development set for D. Looking at the top-ranked words according to (k) σsim , we observe that they exhibit pronounced In cases where x(k) is split into several WordPiece tokens by BERT, we follow previous work (Pinter et al., 2020; Sia et al., 2020) and average the subword embeddings. 6975 14 (k) (k) (k) Context for simij > µsim Word (k) Context for simij &lt; µsim Extralinguistic Linguistic Extralinguistic Linguistic “isolating” r/SAHP 12/19 It’s really hard to explain to other people how isolating and exhausting being a SAHP can be. r/Asthma 03/20 I wish I knew if I’d had covid so that I could stop self isolating and instead volunteer in my community. “testing” r/VJoeShows 04/20 Testing a photocell light fixture during the day is easy when you know how. This is what this DIY video is about. r/vancouver 03/20 Testing is"
2021.acl-long.542,P19-1072,0,0.0615669,"Missing"
2021.acl-long.542,2020.semeval-1.1,0,0.149004,", 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019). The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020). Besides dynamic word embeddings, many studies on lexical semantic change detection use methods based on static word embeddings (Kim et al., 2014; Kulkarni et al., 2015), e.g., the alignment of static word embedding spaces (Hamilton et al., 2016b). However, such approaches come at the cost of modeling disadvantages (Bamler and Mandt, 2017). Sociolinguistics has shown that temporal and social variation in language are tightly interwoven: innovations such as a new word sense in the case of lexical semantics spread through the language community along social ties (Milroy, 1980, 1992; 6971 Labov,"
2021.acl-long.542,N18-2027,0,0.0265348,"et al., 2020). Other studies have demonstrated that performance on a diverse set of tasks can be increased by including temporal (Jaidka et al., 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019). The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020). Besides dynamic word embeddings, many studies on lexical semantic change detection use methods based on static word embeddings (Kim et al., 2014; Kulkarni et al., 2015), e.g., the alignment of static word embedding spaces (Hamilton et al., 2016b). However, such approaches come at the cost of modeling disadvantages (Bamler and Mandt, 2017). Sociolinguistics has shown that temporal an"
2021.acl-long.542,2020.emnlp-main.135,0,0.0109617,"ntexts si and tj , eij , (k) (k) simij = cos φij , (k) (14) (k) ˜(k) and eij (Figwhere φij is the angle between e ure 1).14 To find words with a high degree of variability, we compute the standard deviation of (k) simij based on all si and tj in which a given word x(k) occurs in the data,   (k) (k) σsim = σ {simij |(x(k) , si , tj ) ∈ D} , (15) where we take the development set for D. Looking at the top-ranked words according to (k) σsim , we observe that they exhibit pronounced In cases where x(k) is split into several WordPiece tokens by BERT, we follow previous work (Pinter et al., 2020; Sia et al., 2020) and average the subword embeddings. 6975 14 (k) (k) (k) Context for simij > µsim Word (k) Context for simij &lt; µsim Extralinguistic Linguistic Extralinguistic Linguistic “isolating” r/SAHP 12/19 It’s really hard to explain to other people how isolating and exhausting being a SAHP can be. r/Asthma 03/20 I wish I knew if I’d had covid so that I could stop self isolating and instead volunteer in my community. “testing” r/VJoeShows 04/20 Testing a photocell light fixture during the day is easy when you know how. This is what this DIY video is about. r/vancouver 03/20 Testing is not required if a p"
2021.acl-long.542,D16-1152,0,0.0643888,"Missing"
2021.acl-long.542,Q17-1021,0,0.384775,"word embeddings have been proposed: diachronic word embeddings for temporal semantic change (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al., 2017, 2018; Oba et al., 2019; Welch et al., 2020a,b; Yao et al., 2020). Other studies have demonstrated that performance on a diverse set of tasks can be increased by including temporal (Jaidka et al., 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019). The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020). Besides dynamic word embeddings, many studies on"
2021.acl-long.542,2020.emnlp-main.586,0,0.0618914,"Missing"
2021.acl-long.542,2020.emnlp-main.334,0,0.0771668,"Missing"
2021.acl-long.542,2020.coling-main.604,0,0.0714906,"Missing"
2021.adaptnlp-1.24,W16-6610,0,0.0190747,"h three levels of information from the interleaved text; posts, phrases, and words, rather than traditional two levels; post and word (Nallapati et al., 2017, 2016; Tan et al., 2017; Cheng and Lapata, 2016). The remaining paper is structured as follows. In Section 2, we discuss related work. In Section 3, we provide a detailed description of our hierarchical seq2seq model. In Section 4, we provide a detailed description on the synthetic data creation algorithm. In Section 5, we describe and discuss the experiments. And in Section 6, we present our conclusions. 2 Related Work Ma et al. (2012); Aker et al. (2016); Shang et al. (2018) each designed a system that summarizes posts in multi-party conversations in order to provide readers with overview on the discussed matters. They broadly follow the same two-step approach: cluster the posts and then extract a summary from each cluster. There are two kinds of summarization: abstractive and extractive. In abstractive summarization, the model utilizes a corpus level vocabulary and generates novel sentences as the summary, while extractive models extract or rearrange the source words as the summary. Abstractive models based on neural sequence-to-sequence (se"
2021.adaptnlp-1.24,W16-3605,0,0.114195,"Intelligence, Siemens Healthineers, Princeton 3 Toyota Research Institute, Los Altos, California 4 Machine Intelligence, Siemens AG, Munich skarn@cis.lmu.de francine@acm.org yan-ying.chen@tri.global ulli.waltinger@siemens.com inquiries@cislmu.org Abstract posts to single-sentence summaries. However, disentanglement e.g., Wang and Oard (2009), propagates error to the downstream summarization task. An end-to-end supervised summarization system that implicitly identifies the conversations would eliminate error propagation. However, labeling of interleaved texts is a difficult and expensive task (Barker et al., 2016; Aker et al., 2016; Verberne et al., 2018). Interleaved texts, where posts belonging to different threads occur in a sequence, commonly occur in online chat posts, so that it can be time-consuming to quickly obtain an overview of the discussions. Existing systems first disentangle the posts by threads and then extract summaries from those threads. A major issue with such systems is error propagation from the disentanglement component. While endto-end trainable summarization system could obviate explicit disentanglement, such systems require a large amount of labeled data. To address this, we"
2021.adaptnlp-1.24,P18-1063,0,0.0166421,"uires fine-tuning of the decoder and hierarchical attention, a highly-sophisticated semisupervised training of both the encoder and decoder of BART and larger model size (100x) yields better performance. However, for applications that have limited memory, as on some mobile devices, our model may be more desirable. Furthermore, despite a pre-trained encoder of BertSumExtAbs, a de novo training of a large size decoder with a tiny AMI data lead to over-fitting, and therefore, lower scores. Human Evaluation: We also performed a qualitative evaluation of our system using human judgments. Following Chen and Bansal (2018), we performed a comparative evaluation, where we provided six human judges (graduate students fluent in English) with meetings (≈ 6000 words) and summaries from three sources, i.e., human reference, two-step baseline and hier2hier t-learn (here after referred to as the ”our model”), and asked them to rate on a scale of 1 to 5 the two questions: 1) is the summary concise, fluent and grammatical (fluency) and 2) does the summary retain key information from the meeting (relevancy)? We sampled six meetings (each with three summaries corresponding to three sources), duplicated them, and then rando"
2021.adaptnlp-1.24,P16-1046,0,0.216702,"enhances performance. Thus, the summarization task acts as an auxiliary task for the disentanglement. Additionally, we show that fine-tuning of the encoder-decode system with the learned disentanglement representations on a real-world AMI dataset achieves a substantial increment in evaluation metrics despite a small number of labels. We also propose using a hierarchical attention in the encoder-decoder system with three levels of information from the interleaved text; posts, phrases, and words, rather than traditional two levels; post and word (Nallapati et al., 2017, 2016; Tan et al., 2017; Cheng and Lapata, 2016). The remaining paper is structured as follows. In Section 2, we discuss related work. In Section 3, we provide a detailed description of our hierarchical seq2seq model. In Section 4, we provide a detailed description on the synthetic data creation algorithm. In Section 5, we describe and discuss the experiments. And in Section 6, we present our conclusions. 2 Related Work Ma et al. (2012); Aker et al. (2016); Shang et al. (2018) each designed a system that summarizes posts in multi-party conversations in order to provide readers with overview on the discussed matters. They broadly follow the"
2021.adaptnlp-1.24,N16-1012,0,0.03149,"re than a vanilla sequential encoderdecoder model (seq2seq). Krause et al. (2017) and Jing et al. (2018) showed multi-sentence captioning of an image through a hierarchical Recurrent Neural Network (RNN), topic-to-topic followed by word-to-word, is better than seq2seq. These works suggest a hierarchical decoder, thread-tothread followed by word-to-word, may intrinsically disentangle the posts, and therefore, generate more appropriate summaries. Integration of attention into a seq2seq model (Bahdanau et al., 2014) led to further advancement of abstractive summarization (Nallapati et al., 2016; Chopra et al., 2016). Nallapati et al. (2016) devised a hierarchical attention mechanism for a seq2seq model, where two levels of attention distributions over the source, i.e., sentence and word, are computed at every step of the word decoding. Based on the sentence attentions, the word attentions are rescaled. Our hierarchical attention is more intuitive, computes post(sentence)-level and phraselevel attentions for every new summary sentence, and is trained end-to-end. Semi-supervised learning has recently gained popularity as it helps training parameters of large models without any training data. Researchers ha"
2021.adaptnlp-1.24,I17-2052,0,0.0183662,"words in a ground-truth summary and Dw2w generation respectively. 4 Synthetic Dataset Obtaining labeled training data for interleaved conversation summarization is challenging. The available ones are either extractive Verberne et al. (2018) or too small (Barker et al., 2016; Anguera et al., 2012) to train a neural model and thoroughly verify the architecture. To get around this issue, we synthesized a dataset by utilizing a corpus of conventional texts for which summaries are available. We created a corpus of interleaved texts from the abstracts and titles of articles from the PubMed corpus (Dernoncourt and Lee, 2017). We chose PubMed abstracts as it has, in contrast to other corpora such as news articles or StackOverflow posts, a single-sentence summary that can only be comprehended out of a whole abstract. Further, the number of sentences more closely resembles that of a conversationalist in a conversation. Interleave Algorithm: The Interleave Algorithm generates interleaved texts, each containing randomly interleaved sentences from a small number of abstracts, where the number is a random value within a specified range. The number of sentences used per abstract is also a random value within a specified"
2021.adaptnlp-1.24,P18-1240,0,0.0272577,"nd generates novel sentences as the summary, while extractive models extract or rearrange the source words as the summary. Abstractive models based on neural sequence-to-sequence (seq2seq) (Rush et al., 2015) proved to generate summaries with higher ROUGE scores than the feature-based abstractive models. Li et al. (2015) proposed an encoder-decoder (auto-encoder) model that utilizes a hierarchy of networks: word-to-word followed by sentence-tosentence. Their model is better at capturing the underlying structure than a vanilla sequential encoderdecoder model (seq2seq). Krause et al. (2017) and Jing et al. (2018) showed multi-sentence captioning of an image through a hierarchical Recurrent Neural Network (RNN), topic-to-topic followed by word-to-word, is better than seq2seq. These works suggest a hierarchical decoder, thread-tothread followed by word-to-word, may intrinsically disentangle the posts, and therefore, generate more appropriate summaries. Integration of attention into a seq2seq model (Bahdanau et al., 2014) led to further advancement of abstractive summarization (Nallapati et al., 2016; Chopra et al., 2016). Nallapati et al. (2016) devised a hierarchical attention mechanism for a seq2seq m"
2021.adaptnlp-1.24,K16-1028,0,0.10728,"ls of information from the interleaved text; posts, phrases, and words, rather than traditional two levels; post and word (Nallapati et al., 2017, 2016; Tan et al., 2017; Cheng and Lapata, 2016). The remaining paper is structured as follows. In Section 2, we discuss related work. In Section 3, we provide a detailed description of our hierarchical seq2seq model. In Section 4, we provide a detailed description on the synthetic data creation algorithm. In Section 5, we describe and discuss the experiments. And in Section 6, we present our conclusions. 2 Related Work Ma et al. (2012); Aker et al. (2016); Shang et al. (2018) each designed a system that summarizes posts in multi-party conversations in order to provide readers with overview on the discussed matters. They broadly follow the same two-step approach: cluster the posts and then extract a summary from each cluster. There are two kinds of summarization: abstractive and extractive. In abstractive summarization, the model utilizes a corpus level vocabulary and generates novel sentences as the summary, while extractive models extract or rearrange the source words as the summary. Abstractive models based on neural sequence-to-sequence (se"
2021.adaptnlp-1.24,P15-1107,0,0.0248956,"with overview on the discussed matters. They broadly follow the same two-step approach: cluster the posts and then extract a summary from each cluster. There are two kinds of summarization: abstractive and extractive. In abstractive summarization, the model utilizes a corpus level vocabulary and generates novel sentences as the summary, while extractive models extract or rearrange the source words as the summary. Abstractive models based on neural sequence-to-sequence (seq2seq) (Rush et al., 2015) proved to generate summaries with higher ROUGE scores than the feature-based abstractive models. Li et al. (2015) proposed an encoder-decoder (auto-encoder) model that utilizes a hierarchy of networks: word-to-word followed by sentence-tosentence. Their model is better at capturing the underlying structure than a vanilla sequential encoderdecoder model (seq2seq). Krause et al. (2017) and Jing et al. (2018) showed multi-sentence captioning of an image through a hierarchical Recurrent Neural Network (RNN), topic-to-topic followed by word-to-word, is better than seq2seq. These works suggest a hierarchical decoder, thread-tothread followed by word-to-word, may intrinsically disentangle the posts, and therefo"
2021.adaptnlp-1.24,P19-1210,0,0.0158469,"o those of the generations. The table is best viewed in color. Figure 2: ROUGE uni- and bi-gram precision (green) and recall (blue) of AMI fine-tuned hier2hier models with different numbers of pretraining iterations. Maximum words in a summary is 300. As a reference, solid horizontal lines show the scores of a model trained only on AMI. different domains, we use the byte pair encoding (BPE) (Sennrich et al., 2016) based subword dictionary. As shown in Table 6, hier2hier readily transfers its disentangling knowledge, and therefore, obtains a boost in recall while maintaining its precision. The Li et al. (2019) system has the best ROUGE-1 scores, however their model is not directly comparable as unlike Shang et al. (2018) and our text-based model, it uses audio and video in addition to text. Additionally, we also performed transfer learning experiments with models pre-trained for a dif250 ferent number of iterations, and as seen in Figure 2, hier2hier readily transfers its disentangling knowledge, and therefore, obtains a boost in recall while maintaining its precision. However, longer pretraining drives the model to generate shorter summaries similar to PubMed abstracts, and thereby, results in inc"
2021.adaptnlp-1.24,D19-1387,0,0.0884581,"ns over the source, i.e., sentence and word, are computed at every step of the word decoding. Based on the sentence attentions, the word attentions are rescaled. Our hierarchical attention is more intuitive, computes post(sentence)-level and phraselevel attentions for every new summary sentence, and is trained end-to-end. Semi-supervised learning has recently gained popularity as it helps training parameters of large models without any training data. Researchers have pre-trained masked language models, in which only an encoder is used to reconstruct the text, e.g., BERT (Devlin et al., 2018). Liu and Lapata (2019) used BERT as seq2seq encoder and showed improved performance on several abstractive summarization tasks. Similarly, researchers have published pre-trained seq2seq models using a different semi-supervised learning technique, where a seq2seq model is learned to reconstruct the original text, e.g., BART (Lewis et al., 2019) and MASS (Song et al., 2019). In this work, we rely on transfer learning and demonstrate that by pretraining with appropriate interleaved text data, a seq2seq model readily transfers to a new domain with just a few examples. 3 Model Our hierarchical encoder (see Figure 1 left"
2021.adaptnlp-1.24,D15-1044,0,0.141994,"Missing"
2021.adaptnlp-1.24,P16-1162,0,0.0157818,"athletes . Table 5: An example of hier2hier generated summary sentences of a three thread interleaved text. Summaries are coloured differently and colors of attended phrases (β) in the text are identical to those of the generations. The table is best viewed in color. Figure 2: ROUGE uni- and bi-gram precision (green) and recall (blue) of AMI fine-tuned hier2hier models with different numbers of pretraining iterations. Maximum words in a summary is 300. As a reference, solid horizontal lines show the scores of a model trained only on AMI. different domains, we use the byte pair encoding (BPE) (Sennrich et al., 2016) based subword dictionary. As shown in Table 6, hier2hier readily transfers its disentangling knowledge, and therefore, obtains a boost in recall while maintaining its precision. The Li et al. (2019) system has the best ROUGE-1 scores, however their model is not directly comparable as unlike Shang et al. (2018) and our text-based model, it uses audio and video in addition to text. Additionally, we also performed transfer learning experiments with models pre-trained for a dif250 ferent number of iterations, and as seen in Figure 2, hier2hier readily transfers its disentangling knowledge, and th"
2021.adaptnlp-1.24,P18-1062,0,0.0631994,"Missing"
2021.adaptnlp-1.24,N09-1023,0,0.08227,"Missing"
2021.eacl-main.108,D18-1523,0,0.0218638,"se surface forms during pretraining. Patterns and entailment. Pattern-based approaches have long been known for hypernymy detection (Hearst, 1992). Recent work combined them with vector space models (Mirkin et al., 2006; Roller and Erk, 2016; Roller et al., 2018). While there are effective patterns, such as X is a Y , that are indicative for entailment between nouns, there is little work on comparable patterns for verbs. Schwartz et al. (2015) mine symmetric patterns for lexical similarity and achieve good results for verbs. Entailment, however, is not symmetric. Patterns and language models. Amrami and Goldberg (2018) were the first to manipulate LM predictions with a simple pattern to enhance the quality of substitute words in a given context for word sense induction. Petroni et al. (2019) found that large pretrained LMs can be queried for factual knowledge, when presented with appropriate pattern-generated cloze-style sentences. This zeroshot factual knowledge has later been shown to be quite fragile (Kassner and Sch¨utze, 2020). So we rather focus on approaches that fine-tune an LM on at least a few samples. Forbes et al. (2019) train a binary classifier on top of a fine-tuned BERT (Devlin et al., 2019a"
2021.eacl-main.108,P11-1062,0,0.194101,"ly appearing together in a document. Related Work Lexical inference. There has been a lot of work on lexical inference for nouns, notably hypernymy detection, resulting in a variety of benchmarks (Kotlerman et al., 2010; Kiela et al., 2015) and methods (Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018). Although there has been work on predicate entailment before (Lin and Pantel, 2001; Lewis and Steedman, 2013), Levy and Dagan (2016) were the first to create a general benchmark for evaluating entailment between verbs. In their evaluation, neither resource-based approaches (Pavlick et al., 2015; Berant et al., 2011) nor vector space models (Levy and Goldberg, 2014) achieved satisfying results. Holt (2018) later published a re-annotated version, which was readily adopted by later work. Hosseini et al. (2018) put global constraints on top of directed local similarity scores (Weeds and Weir, 2003; Lin, 1998; Szpektor and Dagan, 2008) based on distributional features of the predicates. Hosseini et al. (2019) replaced these scores by transition probabilities in a bipartite graph where edge weights are computed by a link prediction model. When Schmitt and Sch¨utze (2019) created the SherLIiC benchmark, they al"
2021.eacl-main.108,W04-3205,0,0.0269882,"ed LIiC benchmarks, Levy/Holt’s dataset (Levy and Dagan, 2016; Holt, 2018) and SherLIiC (Schmitt and Sch¨utze, 2019), all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020), automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding. word2vec representations (Mikolov et al., 2013) with a vector representation of the arguments that co-occur with a predicate. 2 Chklovski and Pantel (2004) handcrafted 35 patterns to distinguish 6 semantic relations for pairs of distributionally similar verbs. Some of their classes like strength (taint :: poison) or antonymy (ban :: allow) can be indicators of entailment and nonentailment but are, in general, much more narrowly defined than the patterns we use in our approach. Another difference to our work is that verb pairs are scored based on co-occurrence counts on the web, while we employ an LM, which does not depend on a valid entailment pair actually appearing together in a document. Related Work Lexical inference. There has been a lot of"
2021.eacl-main.108,N19-1423,0,0.608304,"of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019). The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Sch¨utze, 2019). Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (Devlin et al., 2019a; Liu et al., 2019) form the basis of a lot of current stateof-the-art models. Besides zero- and few-shot capabilities (Radford et al., 2019; Brown et al., 2020), pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020). The entailment relation certainly stands out among previously explored semantic relations – such as the relation between a country and its capital – because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt an"
2021.eacl-main.108,D19-1224,0,0.0280014,"Missing"
2021.eacl-main.108,D10-1106,0,0.13684,"with different arguments. Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (“Bezos runs Amazon”) vs. run ⇒ execute in a COMPUTER / SOFTWARE context (“My mac runs macOS”). LIiC is thus also closely related to 1 Our code is publicly available: https://github. com/mnschmit/lm-lexical-inference the task of natural language inference (NLI) – also called recognizing textual entailment (Dagan et al., 2013) – and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010), event coreference (Shwartz et al., 2017; Meged et al., 2020), and link prediction in knowledge graphs (Hosseini et al., 2019). Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019). The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockn"
2021.eacl-main.108,K15-1026,0,0.0173534,"ly reason about the sentence surface in an end-to-end NLI task without access to previously observed argument pairs. This is possible because our models have learned about these surface forms during pretraining. Patterns and entailment. Pattern-based approaches have long been known for hypernymy detection (Hearst, 1992). Recent work combined them with vector space models (Mirkin et al., 2006; Roller and Erk, 2016; Roller et al., 2018). While there are effective patterns, such as X is a Y , that are indicative for entailment between nouns, there is little work on comparable patterns for verbs. Schwartz et al. (2015) mine symmetric patterns for lexical similarity and achieve good results for verbs. Entailment, however, is not symmetric. Patterns and language models. Amrami and Goldberg (2018) were the first to manipulate LM predictions with a simple pattern to enhance the quality of substitute words in a given context for word sense induction. Petroni et al. (2019) found that large pretrained LMs can be queried for factual knowledge, when presented with appropriate pattern-generated cloze-style sentences. This zeroshot factual knowledge has later been shown to be quite fragile (Kassner and Sch¨utze, 2020)"
2021.eacl-main.108,K15-1018,0,0.0232782,"antonymy (ban :: allow) can be indicators of entailment and nonentailment but are, in general, much more narrowly defined than the patterns we use in our approach. Another difference to our work is that verb pairs are scored based on co-occurrence counts on the web, while we employ an LM, which does not depend on a valid entailment pair actually appearing together in a document. Related Work Lexical inference. There has been a lot of work on lexical inference for nouns, notably hypernymy detection, resulting in a variety of benchmarks (Kotlerman et al., 2010; Kiela et al., 2015) and methods (Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018). Although there has been work on predicate entailment before (Lin and Pantel, 2001; Lewis and Steedman, 2013), Levy and Dagan (2016) were the first to create a general benchmark for evaluating entailment between verbs. In their evaluation, neither resource-based approaches (Pavlick et al., 2015; Berant et al., 2011) nor vector space models (Levy and Goldberg, 2014) achieved satisfying results. Holt (2018) later published a re-annotated version, which was readily adopted by later work. Hosseini et al. (2018) put global constraints on top of directed local similarity"
2021.eacl-main.108,S17-1019,0,0.0840317,"lead in a PERSON / COMPANY context (“Bezos runs Amazon”) vs. run ⇒ execute in a COMPUTER / SOFTWARE context (“My mac runs macOS”). LIiC is thus also closely related to 1 Our code is publicly available: https://github. com/mnschmit/lm-lexical-inference the task of natural language inference (NLI) – also called recognizing textual entailment (Dagan et al., 2013) – and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010), event coreference (Shwartz et al., 2017; Meged et al., 2020), and link prediction in knowledge graphs (Hosseini et al., 2019). Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019). The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Sch¨utze, 20"
2021.eacl-main.108,C08-1107,0,0.330923,"re has been work on predicate entailment before (Lin and Pantel, 2001; Lewis and Steedman, 2013), Levy and Dagan (2016) were the first to create a general benchmark for evaluating entailment between verbs. In their evaluation, neither resource-based approaches (Pavlick et al., 2015; Berant et al., 2011) nor vector space models (Levy and Goldberg, 2014) achieved satisfying results. Holt (2018) later published a re-annotated version, which was readily adopted by later work. Hosseini et al. (2018) put global constraints on top of directed local similarity scores (Weeds and Weir, 2003; Lin, 1998; Szpektor and Dagan, 2008) based on distributional features of the predicates. Hosseini et al. (2019) replaced these scores by transition probabilities in a bipartite graph where edge weights are computed by a link prediction model. When Schmitt and Sch¨utze (2019) created the SherLIiC benchmark, they also mainly focused on resource- and vector-based models for evaluation. Their best model combines general-purpose All these works (i) base the probability of entailment validity on the similarity of the verbs and (ii) compute this similarity via (expected) co-occurrence of verbs and their arguments. Our work differs in t"
2021.eacl-main.108,N18-1103,0,0.0523376,"Missing"
2021.eacl-main.108,W03-1011,0,0.238777,"and Mrkˇsi´c, 2018). Although there has been work on predicate entailment before (Lin and Pantel, 2001; Lewis and Steedman, 2013), Levy and Dagan (2016) were the first to create a general benchmark for evaluating entailment between verbs. In their evaluation, neither resource-based approaches (Pavlick et al., 2015; Berant et al., 2011) nor vector space models (Levy and Goldberg, 2014) achieved satisfying results. Holt (2018) later published a re-annotated version, which was readily adopted by later work. Hosseini et al. (2018) put global constraints on top of directed local similarity scores (Weeds and Weir, 2003; Lin, 1998; Szpektor and Dagan, 2008) based on distributional features of the predicates. Hosseini et al. (2019) replaced these scores by transition probabilities in a bipartite graph where edge weights are computed by a link prediction model. When Schmitt and Sch¨utze (2019) created the SherLIiC benchmark, they also mainly focused on resource- and vector-based models for evaluation. Their best model combines general-purpose All these works (i) base the probability of entailment validity on the similarity of the verbs and (ii) compute this similarity via (expected) co-occurrence of verbs and"
2021.eacl-main.108,N18-1101,0,0.039531,"e entailment has also been shown useful for question answering (Schoenmackers et al., 2010), event coreference (Shwartz et al., 2017; Meged et al., 2020), and link prediction in knowledge graphs (Hosseini et al., 2019). Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019). The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Sch¨utze, 2019). Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (Devlin et al., 2019a; Liu et al., 2019) form the basis of a lot of current stateof-the-art models. Besides zero- and few-shot capabilities (Radford et al., 2019; Brown et al., 2020), pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 202"
2021.eacl-main.20,D15-1056,0,0.0132415,"ual class descriptors typically assume that abundant examples are available for a subset of classes (e.g., Romera-Paredes and Torr, 2015; Veeranna et al., 2016; Ye et al., 2020). In contrast, our approach requires no additional labeled data and provides an intuitive interface to leverage task-specific human knowledge. The idea behind iP ET – training multiple generations of models on data labeled by previous generations – bears resemblance to self-training and bootstrapping approaches for word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000; Batista et al., 2015), parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Huang and Harper, 2009), machine translation (Hoang et al., 2018), and sequence generation (He et al., 2020). 3 Pattern-Exploiting Training Let M be a masked language model with vocabulary V and mask token ∈ V , and let L be a set of labels for our target classification task A. We write an input for task A as a sequence of phrases x = (s1 , . . . , sk ) with si ∈ V ∗ ; for example, k = 2 if A is textual inference (two input sentences). We define a pattern to be a function P that takes x as input and outputs a phrase or sentence P"
2021.eacl-main.20,2020.acl-main.194,0,0.298185,"inh and Le, 2018; Petroni et al., 2019; Wang et al., 2019; Sakaguchi et al., 2020), linguistic capabilities (Ettinger, 2020; Kassner and Sch¨utze, 2020), understanding of rare words (Schick and Sch¨utze, 2020), and ability to perform symbolic reasoning (Talmor et al., 2019). Jiang et al. (2020) consider the problem of finding the best pattern to express a given task. Other approaches for few-shot learning in NLP include exploiting examples from related tasks (Yu et al., 2018; Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Yin et al., 2019) and using data augmentation (Xie et al., 2020; Chen et al., 2020); the latter commonly relies on back-translation (Sennrich et al., 2016), requiring large amounts of parallel data. Approaches using textual class descriptors typically assume that abundant examples are available for a subset of classes (e.g., Romera-Paredes and Torr, 2015; Veeranna et al., 2016; Ye et al., 2020). In contrast, our approach requires no additional labeled data and provides an intuitive interface to leverage task-specific human knowledge. The idea behind iP ET – training multiple generations of models on data labeled by previous generations – bears resemblance to self-training an"
2021.eacl-main.20,2020.tacl-1.3,0,0.0273141,"ms as language modeling tasks, but their patterns only loosely resemble natural language and are unsuitable for few-shot learning.2 Another recent line of work uses cloze-style phrases to probe the knowledge that PLMs acquire during pretraining; this includes probing for factual 2 For example, they convert inputs (a, b) for recognizing textual entailment (RTE) to “rte sentence1: a sentence2: b”, and the PLM is asked to predict strings like “not entailment”. and commonsense knowledge (Trinh and Le, 2018; Petroni et al., 2019; Wang et al., 2019; Sakaguchi et al., 2020), linguistic capabilities (Ettinger, 2020; Kassner and Sch¨utze, 2020), understanding of rare words (Schick and Sch¨utze, 2020), and ability to perform symbolic reasoning (Talmor et al., 2019). Jiang et al. (2020) consider the problem of finding the best pattern to express a given task. Other approaches for few-shot learning in NLP include exploiting examples from related tasks (Yu et al., 2018; Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Yin et al., 2019) and using data augmentation (Xie et al., 2020; Chen et al., 2020); the latter commonly relies on back-translation (Sennrich et al., 2016), requiring large amounts of para"
2021.eacl-main.20,D18-1398,0,0.0273093,"entence1: a sentence2: b”, and the PLM is asked to predict strings like “not entailment”. and commonsense knowledge (Trinh and Le, 2018; Petroni et al., 2019; Wang et al., 2019; Sakaguchi et al., 2020), linguistic capabilities (Ettinger, 2020; Kassner and Sch¨utze, 2020), understanding of rare words (Schick and Sch¨utze, 2020), and ability to perform symbolic reasoning (Talmor et al., 2019). Jiang et al. (2020) consider the problem of finding the best pattern to express a given task. Other approaches for few-shot learning in NLP include exploiting examples from related tasks (Yu et al., 2018; Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Yin et al., 2019) and using data augmentation (Xie et al., 2020; Chen et al., 2020); the latter commonly relies on back-translation (Sennrich et al., 2016), requiring large amounts of parallel data. Approaches using textual class descriptors typically assume that abundant examples are available for a subset of classes (e.g., Romera-Paredes and Torr, 2015; Veeranna et al., 2016; Ye et al., 2020). In contrast, our approach requires no additional labeled data and provides an intuitive interface to leverage task-specific human knowledge. The idea behind iP ET"
2021.eacl-main.20,P19-1253,0,0.019428,"PLM is asked to predict strings like “not entailment”. and commonsense knowledge (Trinh and Le, 2018; Petroni et al., 2019; Wang et al., 2019; Sakaguchi et al., 2020), linguistic capabilities (Ettinger, 2020; Kassner and Sch¨utze, 2020), understanding of rare words (Schick and Sch¨utze, 2020), and ability to perform symbolic reasoning (Talmor et al., 2019). Jiang et al. (2020) consider the problem of finding the best pattern to express a given task. Other approaches for few-shot learning in NLP include exploiting examples from related tasks (Yu et al., 2018; Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Yin et al., 2019) and using data augmentation (Xie et al., 2020; Chen et al., 2020); the latter commonly relies on back-translation (Sennrich et al., 2016), requiring large amounts of parallel data. Approaches using textual class descriptors typically assume that abundant examples are available for a subset of classes (e.g., Romera-Paredes and Torr, 2015; Veeranna et al., 2016; Ye et al., 2020). In contrast, our approach requires no additional labeled data and provides an intuitive interface to leverage task-specific human knowledge. The idea behind iP ET – training multiple generations of m"
2021.eacl-main.20,P19-1393,0,0.0236424,"veral tasks as QA problems. Raffel et al. (2020) frame various problems as language modeling tasks, but their patterns only loosely resemble natural language and are unsuitable for few-shot learning.2 Another recent line of work uses cloze-style phrases to probe the knowledge that PLMs acquire during pretraining; this includes probing for factual 2 For example, they convert inputs (a, b) for recognizing textual entailment (RTE) to “rte sentence1: a sentence2: b”, and the PLM is asked to predict strings like “not entailment”. and commonsense knowledge (Trinh and Le, 2018; Petroni et al., 2019; Wang et al., 2019; Sakaguchi et al., 2020), linguistic capabilities (Ettinger, 2020; Kassner and Sch¨utze, 2020), understanding of rare words (Schick and Sch¨utze, 2020), and ability to perform symbolic reasoning (Talmor et al., 2019). Jiang et al. (2020) consider the problem of finding the best pattern to express a given task. Other approaches for few-shot learning in NLP include exploiting examples from related tasks (Yu et al., 2018; Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Yin et al., 2019) and using data augmentation (Xie et al., 2020; Chen et al., 2020); the latter commonly relies on back-tr"
2021.eacl-main.20,P95-1026,0,0.643717,"et al., 2016), requiring large amounts of parallel data. Approaches using textual class descriptors typically assume that abundant examples are available for a subset of classes (e.g., Romera-Paredes and Torr, 2015; Veeranna et al., 2016; Ye et al., 2020). In contrast, our approach requires no additional labeled data and provides an intuitive interface to leverage task-specific human knowledge. The idea behind iP ET – training multiple generations of models on data labeled by previous generations – bears resemblance to self-training and bootstrapping approaches for word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000; Batista et al., 2015), parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Huang and Harper, 2009), machine translation (Hoang et al., 2018), and sequence generation (He et al., 2020). 3 Pattern-Exploiting Training Let M be a masked language model with vocabulary V and mask token ∈ V , and let L be a set of labels for our target classification task A. We write an input for task A as a sequence of phrases x = (s1 , . . . , sk ) with si ∈ V ∗ ; for example, k = 2 if A is textual inference (two input sentences). We define a"
2021.eacl-main.284,P10-1023,0,0.149248,"Missing"
2021.eacl-main.284,P16-1023,1,0.871506,"Missing"
2021.eacl-main.284,N18-1202,0,0.041946,"anguage bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin. 1 Table 1: Language bias when querying (TyQ) mBERT. Top: For an Italian cloze question, Italy is favored as country of origin. Bottom: There is no overlap between the top-ranked predictions, demonstrating the influence of language – even though the facts are the same: the same set of triples is evaluated across languages. Table 3 shows that pooling predictions across languages addresses bias and improves performance. WW = “Wirtschaftswissenschaftler”. Introduction Pretrained language models (LMs) (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) can be finetuned to a variety of natural language processing (NLP) tasks and generally yield high performance. Increasingly, these models and their generative variants are used to solve tasks by simple text generation, without any finetuning (Brown et al., 2020). This motivated research on how much knowledge is contained in LMs: Petroni et al. (2019) used models pretrained with masked language to answer fill-in-the-blank templates such as “Paris is the capital of [MASK].” ∗ Equal contribution - random order. This research so far has been exclusive"
2021.eacl-main.284,D19-1250,0,0.101017,"Missing"
2021.eacl-main.284,2020.findings-emnlp.71,1,0.636948,"Missing"
2021.eacl-main.42,D14-1162,0,0.0991076,"understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future. 1 Introduction Natural language processing (NLP) has advanced drastically in the last decade with the design of larger and more sophisticated models, availability of larger corpora and increasing computational power. Pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) popularized the use of distributed word representations, which became a fundamental building block for NLP systems. Peters et al. (2018a) introduced LSTMbased deep contextual representations and obtained large performance gains by fine-tuning on tasks after unsupervised pretraining (Radford et al., 2018; Howard and Ruder, 2018). More recently, the attention based transformer architecture was shown to use context more effectively (Vaswani et al., 2017) and several subsequent models achieved state of the art results in many NLP tasks by combining the transformer architecture with unsupervised D"
2021.eacl-main.42,N19-1423,0,0.073695,"Missing"
2021.eacl-main.42,N16-2002,0,0.0146913,"ed pretraining (Radford et al., 2018; Howard and Ruder, 2018). More recently, the attention based transformer architecture was shown to use context more effectively (Vaswani et al., 2017) and several subsequent models achieved state of the art results in many NLP tasks by combining the transformer architecture with unsupervised Deep models improve performance. However, what they actually learn about language and word meaning is still to a large extent unclear due to their uninterpretable nature. For static word embeddings, researchers used word similarity (Hill et al., 2015) and word analogy (Gladkova et al., 2016) tests to shed light on what information is captured in these dense vector spaces. For language models, a great amount of linguistic knowledge is stored in the model parameters (Peters et al., 2018b). Several studies proposed using ‘fill in the blank’ type cloze statements to test knowledge learned by these models during unsupervised pretraining. Petroni et al. (2019) proposed the LAMA (LAnguage Model Analysis) probe to test the factual and common sense knowledge stored in language models. Similarly, Schick and Sch¨utze (2020) introduced WNLaMPro (WordNet Language Model Probing) to assess the"
2021.eacl-main.42,J15-4004,0,0.0446208,"fine-tuning on tasks after unsupervised pretraining (Radford et al., 2018; Howard and Ruder, 2018). More recently, the attention based transformer architecture was shown to use context more effectively (Vaswani et al., 2017) and several subsequent models achieved state of the art results in many NLP tasks by combining the transformer architecture with unsupervised Deep models improve performance. However, what they actually learn about language and word meaning is still to a large extent unclear due to their uninterpretable nature. For static word embeddings, researchers used word similarity (Hill et al., 2015) and word analogy (Gladkova et al., 2016) tests to shed light on what information is captured in these dense vector spaces. For language models, a great amount of linguistic knowledge is stored in the model parameters (Peters et al., 2018b). Several studies proposed using ‘fill in the blank’ type cloze statements to test knowledge learned by these models during unsupervised pretraining. Petroni et al. (2019) proposed the LAMA (LAnguage Model Analysis) probe to test the factual and common sense knowledge stored in language models. Similarly, Schick and Sch¨utze (2020) introduced WNLaMPro (WordN"
2021.eacl-main.42,P18-1031,0,0.0199156,". 1 Introduction Natural language processing (NLP) has advanced drastically in the last decade with the design of larger and more sophisticated models, availability of larger corpora and increasing computational power. Pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) popularized the use of distributed word representations, which became a fundamental building block for NLP systems. Peters et al. (2018a) introduced LSTMbased deep contextual representations and obtained large performance gains by fine-tuning on tasks after unsupervised pretraining (Radford et al., 2018; Howard and Ruder, 2018). More recently, the attention based transformer architecture was shown to use context more effectively (Vaswani et al., 2017) and several subsequent models achieved state of the art results in many NLP tasks by combining the transformer architecture with unsupervised Deep models improve performance. However, what they actually learn about language and word meaning is still to a large extent unclear due to their uninterpretable nature. For static word embeddings, researchers used word similarity (Hill et al., 2015) and word analogy (Gladkova et al., 2016) tests to shed light on what informatio"
2021.eacl-main.42,N18-1202,0,0.0482452,"words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future. 1 Introduction Natural language processing (NLP) has advanced drastically in the last decade with the design of larger and more sophisticated models, availability of larger corpora and increasing computational power. Pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) popularized the use of distributed word representations, which became a fundamental building block for NLP systems. Peters et al. (2018a) introduced LSTMbased deep contextual representations and obtained large performance gains by fine-tuning on tasks after unsupervised pretraining (Radford et al., 2018; Howard and Ruder, 2018). More recently, the attention based transformer architecture was shown to use context more effectively (Vaswani et al., 2017) and several subsequent models achieved state of the art results in many NLP tasks by combining the transformer architecture with unsupervised Deep models improve performance. However, what they actually learn about language and word meaning is still to a large extent unclear due"
2021.eacl-main.42,D18-1179,0,0.0149176,"words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future. 1 Introduction Natural language processing (NLP) has advanced drastically in the last decade with the design of larger and more sophisticated models, availability of larger corpora and increasing computational power. Pretrained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) popularized the use of distributed word representations, which became a fundamental building block for NLP systems. Peters et al. (2018a) introduced LSTMbased deep contextual representations and obtained large performance gains by fine-tuning on tasks after unsupervised pretraining (Radford et al., 2018; Howard and Ruder, 2018). More recently, the attention based transformer architecture was shown to use context more effectively (Vaswani et al., 2017) and several subsequent models achieved state of the art results in many NLP tasks by combining the transformer architecture with unsupervised Deep models improve performance. However, what they actually learn about language and word meaning is still to a large extent unclear due"
2021.eacl-main.42,D19-1250,0,0.0363712,"Missing"
2021.eacl-main.42,2021.ccl-1.108,0,0.0356369,"Missing"
2021.eacl-main.42,L18-1008,0,0.142284,"of a target word (with its definition) and its taxonomic sisters (with their definitions). Using taxonomic sisters, rather than random word groups, makes the task more challenging for statistical models that are based on the distributional hypothesis since these words have similar distributional characteristics (Lenci, 2008). We evaluate two masked language models, BERT and RoBERTa, and the auto-regressive model GPT2 on WDLMPro using two different probing tests: (i) match definition to word (D2W) (ii) match word to definition (W2D). We also provide a baseline using static fastText embeddings (Mikolov et al., 2018). We find that all three language models perform clearly better than the baseline. Nevertheless, they have great difficulty matching words and their definitions, implying a poor understanding of word meaning. This is an important result that could help guide research on LMs in the future. 2 that is, G contains all synsets that are “sister hyponyms” to t with respect to a hypernym of t. G(t), along with the definitions of the synsets in G(t), will be used to set up the WDLMPro tasks that require matching of words and definitions. We discard groups G(t) that have a size of less than 5. In this s"
2021.emnlp-main.32,2021.mrl-1.1,0,0.0663447,"Missing"
2021.emnlp-main.32,2020.acl-main.194,0,0.111668,"t classification objective and does not scale to long output sequences. Radford et al. (2019) consider task descriptions for text generation tasks, but do so only in a zero-shot setting. In a similar spirit, Brown et al. (2020) investigate the ability of pretrained language models to leverage task descriptions and examples without any gradientbased optimization. Other approaches to few-shot learning in NLP commonly require large sets of examples from related tasks (Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Ye et al., 2020), parallel data for consistency training (Xie et al., 2020; Chen et al., 2020), or highly specialized methods tailored towards a specific task (Laban et al., 2020). In contrast, GEN P ET requires no additional labeled data and provides an intuitive interface to leveraging task-specific human knowledge. Our work is also related to prefix-constrained decoding in interactive machine translation for making suggestions on how to complete a partial translation (Knowles and Koehn, 2016; Wuebker et al., 2016). Keskar et al. (2019) and He et al. (2020) similarly use prompts and keywords for controllable text generation, but require specific pretraining procedures and do so only"
2021.emnlp-main.32,2020.acl-main.703,0,0.410381,"ings, however, signifiments throughout NLP (Peters et al., 2018; Howard cant gains are possible by reversing what is adapted and Ruder, 2018; Radford et al., 2018; Devlin to what: Instead of making pretraining more simet al., 2019; Raffel et al., 2020; Brown et al., 2020, ilar to a downstream task, we can reformulate the i.a.). Further improvements are often possible by downstream task to make it more similar to the choosing a different pretraining objective that more pretraining objective. For masked language models closely matches the downstream task of interest. (e.g., Devlin et al., 2019; Lewis et al., 2020), one Examples include casing prediction for named en- such reformulation technique is to convert inputs tity recognition (Mayhew et al., 2020), gap sen- to cloze questions by adding a text snippet that tence generation for summarization (Zhang et al., contains some form of task description, often in the form of a short prompt (Radford et al., 2019; 1 Our implementation of GEN P ET and code to recreate our Schick and Schütze, 2021a). Besides making prefew-shot training datasets is publicly available at https: //github.com/timoschick/pet. training and finetuning more similar, this approach 390"
2021.emnlp-main.32,W04-1013,0,0.0109935,"ext generation with instructions: • a pattern P : X → V ∗ that maps each input 1. How should we provide an instruction to an encoder-decoder model so that the model can make the best possible use of it? (§5.1) 2. How can we ensure that the model understands the instructions provided sufficiently well, and how do we deal with the fact that even minor modifications to the patterns can have a big impact on performance (Jiang et al., 2020; Schick and Schütze, 2021a; Elazar et al., 2021)? (§5.2) 2 The most informative sentences are selected where informativeness is measured as the Rouge1 F1 score (Lin, 2004) between the sentence and the remaining document. 392 3. How do we prevent overfitting, a major issue in few-shot settings? (§5.3) Notation Let P be a pattern, x ∈ X and y ∈ Y input and output text sequences, and z = P (x) the result of applying P to x, i.e., a text sequence containing a single mask token. Furthermore, let y = y1 . . . yn , z = z1 . . . zm and let the mask token in z be at some position h ≤ m. We denote the subsequence yi . . . yj by yi:j . We consider an encoder-decoder model M pretrained by masked language modeling. That is, the model must be able to compute a probability pM"
2021.emnlp-main.32,D18-1206,0,0.0212506,"nstruction to choose during test time, and querying the model with all instructions would be inefficient. 6 Experiments Tasks We evaluate P EGASUS with and without GEN P ET on a subset of the tasks in Zhang et al. (2020). As our computing resources are limited, we only choose those tasks for which the maximum output length in Zhang et al. (2020) is at most 128 tokens. We include the following tasks: • AESLC (Zhang and Tetreault, 2019): Given an email body, predict the title of the email. • Gigaword (Rush et al., 2015): Given the first sentence of a news article, generate its headline. • XSum (Narayan et al., 2018): Summarize articles spanning a wide range of different topics. • Reddit TIFU (Kim et al., 2019): Generate summaries for posts from the TIFU community in Reddit. • NEWSROOM (Grusky et al., 2018): Generate summaries for articles from various major publications. • CNN/DailyMail (Hermann et al., 2015): For articles from CNN and the Daily Mail, generate a list of highlights. For each task, we use the entire test set for evaluation.4 We create two types of training sets containing either 10 or 100 training examples; in addition, we provide 1,000 unlabeled examples per Task Decoder Prefixes AESLC Gi"
2021.emnlp-main.32,N18-1202,0,0.0393945,"(bottom) even in zero-shot settings and enables much more data-efficient learning. 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of ex1 Introduction amples is available for the downstream task, which Pretraining large neural networks with a language is a common scenario for many real-word uses of modeling objective has led to significant improve- NLP. In such few-shot settings, however, signifiments throughout NLP (Peters et al., 2018; Howard cant gains are possible by reversing what is adapted and Ruder, 2018; Radford et al., 2018; Devlin to what: Instead of making pretraining more simet al., 2019; Raffel et al., 2020; Brown et al., 2020, ilar to a downstream task, we can reformulate the i.a.). Further improvements are often possible by downstream task to make it more similar to the choosing a different pretraining objective that more pretraining objective. For masked language models closely matches the downstream task of interest. (e.g., Devlin et al., 2019; Lewis et al., 2020), one Examples include casing prediction for"
2021.emnlp-main.32,D19-1250,0,0.0240036,"a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. • We show that training P EGASUS with GEN P ET outperforms standard finetuning across a broad set of tasks and training set sizes. • We analyze the factors contributing to GEN P ET’s strong performance and quantify the impact of all its components. and P EGASUS (Zhang et al., 2020), of which we make use in this work. The idea to rephrase tasks as cloze questions is commonly used to probe the knowledge contained within masked language models (e.g., Petroni et al., 2019; Wang et al., 2019; Talmor et al., 2020; Schick and Schütze, 2020; Ettinger, 2020; Kassner and Schütze, 2020; Sakaguchi et al., 2020). Schick and Schütze (2021a) propose P ET, which combines this idea with gradient-based learning for efficient few-shot text classification. Jiang et al. (2020) and Schick et al. (2020) consider the problem of finding the best way to rephrase a given task as a cloze question. Schick and Schütze (2021b)’s version of P ET can generate multiple tokens, but still requires a text classification objective and does not scale to long output sequences. Radford et al. (20"
2021.emnlp-main.32,P19-1253,0,0.0883898,"ck and Schütze (2021b)’s version of P ET can generate multiple tokens, but still requires a text classification objective and does not scale to long output sequences. Radford et al. (2019) consider task descriptions for text generation tasks, but do so only in a zero-shot setting. In a similar spirit, Brown et al. (2020) investigate the ability of pretrained language models to leverage task descriptions and examples without any gradientbased optimization. Other approaches to few-shot learning in NLP commonly require large sets of examples from related tasks (Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Ye et al., 2020), parallel data for consistency training (Xie et al., 2020; Chen et al., 2020), or highly specialized methods tailored towards a specific task (Laban et al., 2020). In contrast, GEN P ET requires no additional labeled data and provides an intuitive interface to leveraging task-specific human knowledge. Our work is also related to prefix-constrained decoding in interactive machine translation for making suggestions on how to complete a partial translation (Knowles and Koehn, 2016; Wuebker et al., 2016). Keskar et al. (2019) and He et al. (2020) similarly use prompts and keywor"
2021.emnlp-main.32,D15-1044,0,0.0491564,") because even given a single model understanding all instructions, it would be unclear which instruction to choose during test time, and querying the model with all instructions would be inefficient. 6 Experiments Tasks We evaluate P EGASUS with and without GEN P ET on a subset of the tasks in Zhang et al. (2020). As our computing resources are limited, we only choose those tasks for which the maximum output length in Zhang et al. (2020) is at most 128 tokens. We include the following tasks: • AESLC (Zhang and Tetreault, 2019): Given an email body, predict the title of the email. • Gigaword (Rush et al., 2015): Given the first sentence of a news article, generate its headline. • XSum (Narayan et al., 2018): Summarize articles spanning a wide range of different topics. • Reddit TIFU (Kim et al., 2019): Generate summaries for posts from the TIFU community in Reddit. • NEWSROOM (Grusky et al., 2018): Generate summaries for articles from various major publications. • CNN/DailyMail (Hermann et al., 2015): For articles from CNN and the Daily Mail, generate a list of highlights. For each task, we use the entire test set for evaluation.4 We create two types of training sets containing either 10 or 100 trai"
2021.emnlp-main.32,2020.coling-main.488,1,0.732437,"GEN P ET’s strong performance and quantify the impact of all its components. and P EGASUS (Zhang et al., 2020), of which we make use in this work. The idea to rephrase tasks as cloze questions is commonly used to probe the knowledge contained within masked language models (e.g., Petroni et al., 2019; Wang et al., 2019; Talmor et al., 2020; Schick and Schütze, 2020; Ettinger, 2020; Kassner and Schütze, 2020; Sakaguchi et al., 2020). Schick and Schütze (2021a) propose P ET, which combines this idea with gradient-based learning for efficient few-shot text classification. Jiang et al. (2020) and Schick et al. (2020) consider the problem of finding the best way to rephrase a given task as a cloze question. Schick and Schütze (2021b)’s version of P ET can generate multiple tokens, but still requires a text classification objective and does not scale to long output sequences. Radford et al. (2019) consider task descriptions for text generation tasks, but do so only in a zero-shot setting. In a similar spirit, Brown et al. (2020) investigate the ability of pretrained language models to leverage task descriptions and examples without any gradientbased optimization. Other approaches to few-shot learning in NLP"
2021.emnlp-main.32,2021.naacl-main.185,1,0.087624,"a different pretraining objective that more pretraining objective. For masked language models closely matches the downstream task of interest. (e.g., Devlin et al., 2019; Lewis et al., 2020), one Examples include casing prediction for named en- such reformulation technique is to convert inputs tity recognition (Mayhew et al., 2020), gap sen- to cloze questions by adding a text snippet that tence generation for summarization (Zhang et al., contains some form of task description, often in the form of a short prompt (Radford et al., 2019; 1 Our implementation of GEN P ET and code to recreate our Schick and Schütze, 2021a). Besides making prefew-shot training datasets is publicly available at https: //github.com/timoschick/pet. training and finetuning more similar, this approach 390 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 390–402 c November 7–11, 2021. 2021 Association for Computational Linguistics has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructio"
2021.emnlp-main.32,2020.tacl-1.48,0,0.0359258,"guage models that achieves great data efficiency by using both textual instructions and training examples. • We show that training P EGASUS with GEN P ET outperforms standard finetuning across a broad set of tasks and training set sizes. • We analyze the factors contributing to GEN P ET’s strong performance and quantify the impact of all its components. and P EGASUS (Zhang et al., 2020), of which we make use in this work. The idea to rephrase tasks as cloze questions is commonly used to probe the knowledge contained within masked language models (e.g., Petroni et al., 2019; Wang et al., 2019; Talmor et al., 2020; Schick and Schütze, 2020; Ettinger, 2020; Kassner and Schütze, 2020; Sakaguchi et al., 2020). Schick and Schütze (2021a) propose P ET, which combines this idea with gradient-based learning for efficient few-shot text classification. Jiang et al. (2020) and Schick et al. (2020) consider the problem of finding the best way to rephrase a given task as a cloze question. Schick and Schütze (2021b)’s version of P ET can generate multiple tokens, but still requires a text classification objective and does not scale to long output sequences. Radford et al. (2019) consider task descriptions for text"
2021.emnlp-main.32,P16-1007,0,0.0237535,"e large sets of examples from related tasks (Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Ye et al., 2020), parallel data for consistency training (Xie et al., 2020; Chen et al., 2020), or highly specialized methods tailored towards a specific task (Laban et al., 2020). In contrast, GEN P ET requires no additional labeled data and provides an intuitive interface to leveraging task-specific human knowledge. Our work is also related to prefix-constrained decoding in interactive machine translation for making suggestions on how to complete a partial translation (Knowles and Koehn, 2016; Wuebker et al., 2016). Keskar et al. (2019) and He et al. (2020) similarly use prompts and keywords for controllable text generation, but require specific pretraining procedures and do so only in high-resource settings. 3 P EGASUS Pretraining We briefly summarize the pretraining procedure of P EGASUS (Zhang et al., 2020), the model to 2 Related Work which we apply GEN P ET. P EGASUS is a stanMasked language modeling was proposed as a pre- dard Transformer encoder-decoder architecture training objective by Devlin et al. (2019). Several (Vaswani et al., 2017) that is pretrained using gapvariants of this objective th"
2021.emnlp-main.32,2020.acl-main.272,0,0.0420494,"1b)’s version of P ET can generate multiple tokens, but still requires a text classification objective and does not scale to long output sequences. Radford et al. (2019) consider task descriptions for text generation tasks, but do so only in a zero-shot setting. In a similar spirit, Brown et al. (2020) investigate the ability of pretrained language models to leverage task descriptions and examples without any gradientbased optimization. Other approaches to few-shot learning in NLP commonly require large sets of examples from related tasks (Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019; Ye et al., 2020), parallel data for consistency training (Xie et al., 2020; Chen et al., 2020), or highly specialized methods tailored towards a specific task (Laban et al., 2020). In contrast, GEN P ET requires no additional labeled data and provides an intuitive interface to leveraging task-specific human knowledge. Our work is also related to prefix-constrained decoding in interactive machine translation for making suggestions on how to complete a partial translation (Knowles and Koehn, 2016; Wuebker et al., 2016). Keskar et al. (2019) and He et al. (2020) similarly use prompts and keywords for controllabl"
2021.emnlp-main.32,P19-1043,0,0.0426866,"Missing"
2021.emnlp-main.32,P19-1393,0,0.0206991,"for generative language models that achieves great data efficiency by using both textual instructions and training examples. • We show that training P EGASUS with GEN P ET outperforms standard finetuning across a broad set of tasks and training set sizes. • We analyze the factors contributing to GEN P ET’s strong performance and quantify the impact of all its components. and P EGASUS (Zhang et al., 2020), of which we make use in this work. The idea to rephrase tasks as cloze questions is commonly used to probe the knowledge contained within masked language models (e.g., Petroni et al., 2019; Wang et al., 2019; Talmor et al., 2020; Schick and Schütze, 2020; Ettinger, 2020; Kassner and Schütze, 2020; Sakaguchi et al., 2020). Schick and Schütze (2021a) propose P ET, which combines this idea with gradient-based learning for efficient few-shot text classification. Jiang et al. (2020) and Schick et al. (2020) consider the problem of finding the best way to rephrase a given task as a cloze question. Schick and Schütze (2021b)’s version of P ET can generate multiple tokens, but still requires a text classification objective and does not scale to long output sequences. Radford et al. (2019) consider task d"
2021.emnlp-main.555,S14-2010,0,0.0866635,"Missing"
2021.emnlp-main.555,S16-1081,0,0.0151694,"y NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise unsupervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. Task: Write two sentences that mean the same thing. Sentence 1: “A man is playing a flute.” Sentence 2: “He’s playing a flute.” Task: Write two sentences that are somewhat similar. Sentence 1: “A man is playing a flute.” Sentence 2: “A woman has been playing the violin.” Task: Write two sentences that are on completely different topics. Sentence 1: “A man is playing a flute.” Sentence 2: “A woman is walking"
2021.emnlp-main.555,S13-1004,0,0.0385567,"Missing"
2021.emnlp-main.555,S12-1051,0,0.142306,"Missing"
2021.emnlp-main.555,Q17-1010,0,0.0464552,"ful text pairs and labels. D INO requires a set of instructions I = {Iy |y ∈ Y } where each Iy ∈ I is a function that, given an input x1 ∈ X1 , prompts its recipient to generate an appropriate second text x2 . We use the instruction template in Figure 2 and consider three levels of similarity (Y = {0, 0.5, 1}), where   if y = 1 mean the same thing iy = are somewhat similar if y = 0.5   are on completely different topics if y = 0 There are many unsupervised approaches to obtaining sentence embeddings, for example by averaging word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) or with carefully designed sentence-level objectives (Le and Mikolov, 2014; Kiros et al., 2015). Ensembling several methods improves results (Pörner and Schütze, 2019; Pörner et al., 2020). Recent work obtains sentence representations by supplementing BERT (Devlin et al., 2019) or other PLMs with additional unsupervised objectives (Zhang et al., 2020; Li et al., 2020; Wu et al., 2020; Giorgi et al., 2020). Often, labeled datasets such as paraphrase databases (Wieting and Gimpel, 2018) or natural language inference datasets (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019) a"
2021.emnlp-main.555,D15-1075,0,0.573943,"trong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise unsupervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. Task: Write two sentences that mean the same thing. Sentence 1: “A man is playing a flute.” Sentence 2: “He’s playing a flute.” Task: Write two sentences that are somewhat similar. Sentence 1: “A man is playing a flute.” Sentence 2: “A woman has been playing the violin.” Task: Write two sentences that are on completely different topics. Sentence 1: “A man is playing a flute.” Sentence 2"
2021.emnlp-main.555,2021.mrl-1.1,0,0.0709922,"Missing"
2021.emnlp-main.555,marelli-etal-2014-sick,0,0.0889068,"Missing"
2021.emnlp-main.555,P18-1152,0,0.0194212,"dataset of text pairs with semantic similarity labels. We generate two variants: • STS- -x2 , for which we make use of STSb to obtain a set of texts X1 ; • STS- -x1 x2 , where the set of sentences X1 is generated from scratch. We use GPT2-XL as PLM with a decay constant of λ = 100 and the set of counterlabels CL(y) = {y ′ ∈ Y |y ′ > y}. That is, we do not restrict the PLM when generating texts for y = 1, but for y = 0.5 (y = 0) we encourage it not to generate texts x2 that mean the same thing as (are somewhat similar to) x1 . We apply top-p (Holtzman et al., 2020) and top-k (Fan et al., 2018; Holtzman et al., 2018) sampling with p = 0.9, k = 5 and generate up to 40 output tokens. For each x1 ∈ X1 and y ∈ Y , we generate up to two corresponding x2 ’s.2 For STS- -x1 x2 , we obtain X1 by generating 15,000 sentences using only top-p sampling (again with p = 0.9) and no top-k sampling to ensure more diversity in the generated output. We remove all examples where x1 = x2 (as those provide no training signal to the model) and split the datasets 90/10 into training and validation. To assess the quality of the generated datasets, we use them to train Sentence-RoBERTa (Reimers and Gurevych, 2019), a biencoder arc"
2021.emnlp-main.555,2020.tacl-1.28,0,0.03772,"flute. ✗ Table 4: A selection of high-quality (✓) and low-quality (✗) examples in STS- -x2 . Many sentence pairs for y = 1 are not similar and have quite different meanings. Some sentence pairs for y = 0 are not on completely different topics. et al. (2021). With appropriate measures for handling noisy data, models trained on datasets generated with D INO achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with D INO can further be reduced, e.g., by using different sets of instructions (Jiang et al., 2020; Schick and Schütze, 2021a) or by supplementing our pipeline with some additional filtering steps. Acknowledgments This work was funded by the European Research Council (ERC #740516). We thank the anonymous reviewers for their helpful comments. Conclusion References We have introduced D INO, a method for using large PLMs to generate entire datasets of labeled sen- Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei tence pairs from scratch, requiring no labeled data Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada and no parameter updates. This is a"
2021.emnlp-main.555,2020.emnlp-main.733,0,0.306679,"e then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.1 1 Introduction While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise unsupervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. Task: Write two sentences that mean the same thing. Sentence 1: “A man is playing a flute.” Sentence 2: “He’s playing a flute.” Task: Write two s"
2021.emnlp-main.555,D14-1162,0,0.0868004,"ng and (for our task) useful text pairs and labels. D INO requires a set of instructions I = {Iy |y ∈ Y } where each Iy ∈ I is a function that, given an input x1 ∈ X1 , prompts its recipient to generate an appropriate second text x2 . We use the instruction template in Figure 2 and consider three levels of similarity (Y = {0, 0.5, 1}), where   if y = 1 mean the same thing iy = are somewhat similar if y = 0.5   are on completely different topics if y = 0 There are many unsupervised approaches to obtaining sentence embeddings, for example by averaging word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) or with carefully designed sentence-level objectives (Le and Mikolov, 2014; Kiros et al., 2015). Ensembling several methods improves results (Pörner and Schütze, 2019; Pörner et al., 2020). Recent work obtains sentence representations by supplementing BERT (Devlin et al., 2019) or other PLMs with additional unsupervised objectives (Zhang et al., 2020; Li et al., 2020; Wu et al., 2020; Giorgi et al., 2020). Often, labeled datasets such as paraphrase databases (Wieting and Gimpel, 2018) or natural language inference datasets (Conneau et al., 2017; Cer et al., 2018; Rei"
2021.emnlp-main.555,N18-1202,0,0.0233667,"n this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.1 1 Introduction While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise unsupervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermor"
2021.emnlp-main.555,D19-1173,1,0.852117,"o generate an appropriate second text x2 . We use the instruction template in Figure 2 and consider three levels of similarity (Y = {0, 0.5, 1}), where   if y = 1 mean the same thing iy = are somewhat similar if y = 0.5   are on completely different topics if y = 0 There are many unsupervised approaches to obtaining sentence embeddings, for example by averaging word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) or with carefully designed sentence-level objectives (Le and Mikolov, 2014; Kiros et al., 2015). Ensembling several methods improves results (Pörner and Schütze, 2019; Pörner et al., 2020). Recent work obtains sentence representations by supplementing BERT (Devlin et al., 2019) or other PLMs with additional unsupervised objectives (Zhang et al., 2020; Li et al., 2020; Wu et al., 2020; Giorgi et al., 2020). Often, labeled datasets such as paraphrase databases (Wieting and Gimpel, 2018) or natural language inference datasets (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019) are used for supervised learning. Some approaches augment existing datasets with automatically generated examples (Anaby-Tavor et al., 2020; Papanikolaou and Pierleoni,"
2021.emnlp-main.555,2020.acl-main.628,1,0.889585,"descriptions for zero- or fewsign that it is done. shot learning has been studied extensively (e.g., For a given x1 ∈ X1 and y ∈ Y , we could Radford et al., 2019; Puri and Catanzaro, 2019; directly use the instructions Iy to obtain x2 by conBrown et al., 2020; Schick and Schütze, 2020, tinuously sampling tokens 2021b,a; Weller et al., 2020; Gao et al., 2021; Tam et al., 2021). However, none of these approaches is xk ∼ pM (xk |Iy (x1 ), x1 , . . . , xk−1 ) suitable for generating sentence embeddings. Closely related to our work, Efrat and Levy starting from k = 1 until xk is a quotation mark (2020) examine the ability of PLMs to follow natu- and setting x2 = x1 , . . . , xk−1 . However, we may 6944 want the PLM to generate a text x2 that is not only a good fit for instruction Iy (x1 ), but also not a good fit for some other instruction Iy′ (x1 ). We refer to y ′ as a counterlabel for y and denote the set of y’s counterlabels as CL(y). For example, 1 ∈ CL(0.5) means that for y = 0.5, we want M to generate a sentence x2 that is similar to (y = 0.5), but at the same time does not have the same meaning as (y = 1) sentence x1 . We achieve this using Schick et al. (2021)’s self-debiasing algo"
2021.emnlp-main.555,D19-1410,0,0.285714,"finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.1 1 Introduction While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise unsupervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes"
2021.emnlp-main.555,2021.eacl-main.20,1,0.743157,"selection of high-quality (✓) and low-quality (✗) examples in STS- -x2 . Many sentence pairs for y = 1 are not similar and have quite different meanings. Some sentence pairs for y = 0 are not on completely different topics. et al. (2021). With appropriate measures for handling noisy data, models trained on datasets generated with D INO achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with D INO can further be reduced, e.g., by using different sets of instructions (Jiang et al., 2020; Schick and Schütze, 2021a) or by supplementing our pipeline with some additional filtering steps. Acknowledgments This work was funded by the European Research Council (ERC #740516). We thank the anonymous reviewers for their helpful comments. Conclusion References We have introduced D INO, a method for using large PLMs to generate entire datasets of labeled sen- Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei tence pairs from scratch, requiring no labeled data Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada and no parameter updates. This is achieved by Mihalcea, Germa"
2021.emnlp-main.555,2021.naacl-main.185,1,0.734701,"selection of high-quality (✓) and low-quality (✗) examples in STS- -x2 . Many sentence pairs for y = 1 are not similar and have quite different meanings. Some sentence pairs for y = 0 are not on completely different topics. et al. (2021). With appropriate measures for handling noisy data, models trained on datasets generated with D INO achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with D INO can further be reduced, e.g., by using different sets of instructions (Jiang et al., 2020; Schick and Schütze, 2021a) or by supplementing our pipeline with some additional filtering steps. Acknowledgments This work was funded by the European Research Council (ERC #740516). We thank the anonymous reviewers for their helpful comments. Conclusion References We have introduced D INO, a method for using large PLMs to generate entire datasets of labeled sen- Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei tence pairs from scratch, requiring no labeled data Guo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada and no parameter updates. This is achieved by Mihalcea, Germa"
2021.emnlp-main.555,2020.emnlp-main.105,0,0.032103,"ilarity scheme. Note that for all y, Iy ends with these approaches require that there already exists a an opening quotation mark, which allows us to treat labeled dataset for finetuning the generator. Providthe first quotation mark generated by the PLM as a ing PLMs with task descriptions for zero- or fewsign that it is done. shot learning has been studied extensively (e.g., For a given x1 ∈ X1 and y ∈ Y , we could Radford et al., 2019; Puri and Catanzaro, 2019; directly use the instructions Iy to obtain x2 by conBrown et al., 2020; Schick and Schütze, 2020, tinuously sampling tokens 2021b,a; Weller et al., 2020; Gao et al., 2021; Tam et al., 2021). However, none of these approaches is xk ∼ pM (xk |Iy (x1 ), x1 , . . . , xk−1 ) suitable for generating sentence embeddings. Closely related to our work, Efrat and Levy starting from k = 1 until xk is a quotation mark (2020) examine the ability of PLMs to follow natu- and setting x2 = x1 , . . . , xk−1 . However, we may 6944 want the PLM to generate a text x2 that is not only a good fit for instruction Iy (x1 ), but also not a good fit for some other instruction Iy′ (x1 ). We refer to y ′ as a counterlabel for y and denote the set of y’s counterlabels as"
2021.emnlp-main.555,P18-1042,0,0.0181327,"sentence embeddings, for example by averaging word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) or with carefully designed sentence-level objectives (Le and Mikolov, 2014; Kiros et al., 2015). Ensembling several methods improves results (Pörner and Schütze, 2019; Pörner et al., 2020). Recent work obtains sentence representations by supplementing BERT (Devlin et al., 2019) or other PLMs with additional unsupervised objectives (Zhang et al., 2020; Li et al., 2020; Wu et al., 2020; Giorgi et al., 2020). Often, labeled datasets such as paraphrase databases (Wieting and Gimpel, 2018) or natural language inference datasets (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019) are used for supervised learning. Some approaches augment existing datasets with automatically generated examples (Anaby-Tavor et al., 2020; Papanikolaou and Pierleoni, 2020; Yang et al., 2020; Mohapatra et al., 2020; Kumar is loosely based on Cer et al. (2017)’s five-level et al., 2021), but in contrast to our work, all of similarity scheme. Note that for all y, Iy ends with these approaches require that there already exists a an opening quotation mark, which allows us to treat labeled"
2021.emnlp-main.555,N18-1101,0,0.19688,"rated by GPT2-XL with D INO for three different task descriptions. We investigate two different unsupervised approaches to generating sentence-similarity datasets: (i) The input sentence is given and only the continuation is generated. This requires that an (unlabeled) set of sentences is available. (ii) Both input sentence and continuation are generated. This does not rely on the availability of any resources. To alleviate both problems, we explore a novel approach to obtaining high-quality sentence embeddings: We mimic the creation of NLI datasets by human crowdworkers (Bowman et al., 2015; Williams et al., 2018), but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models. Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like GPT-3 (Brown et al., 2020) without requiring any updates to their parameters. As illustrated in Figure 1, our approach is based on recent methods for providing instructions to PLMs (e.g., Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2020, 2021a). We use 1 the self-debiasing appro"
2021.emnlp-main.555,2020.findings-emnlp.90,0,0.0537475,"Missing"
2021.emnlp-main.555,2020.emnlp-main.124,0,0.330811,"rom scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.1 1 Introduction While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise unsupervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. Task: Write two sentences that mean the same thing. Sentence 1: “A man is playing a flute.” Sentence 2: “He’s playing a flute.”"
2021.emnlp-main.555,2021.emnlp-main.407,0,0.0146503,"ends with these approaches require that there already exists a an opening quotation mark, which allows us to treat labeled dataset for finetuning the generator. Providthe first quotation mark generated by the PLM as a ing PLMs with task descriptions for zero- or fewsign that it is done. shot learning has been studied extensively (e.g., For a given x1 ∈ X1 and y ∈ Y , we could Radford et al., 2019; Puri and Catanzaro, 2019; directly use the instructions Iy to obtain x2 by conBrown et al., 2020; Schick and Schütze, 2020, tinuously sampling tokens 2021b,a; Weller et al., 2020; Gao et al., 2021; Tam et al., 2021). However, none of these approaches is xk ∼ pM (xk |Iy (x1 ), x1 , . . . , xk−1 ) suitable for generating sentence embeddings. Closely related to our work, Efrat and Levy starting from k = 1 until xk is a quotation mark (2020) examine the ability of PLMs to follow natu- and setting x2 = x1 , . . . , xk−1 . However, we may 6944 want the PLM to generate a text x2 that is not only a good fit for instruction Iy (x1 ), but also not a good fit for some other instruction Iy′ (x1 ). We refer to y ′ as a counterlabel for y and denote the set of y’s counterlabels as CL(y). For example, 1 ∈ CL(0.5) means"
2021.emnlp-main.556,N19-1423,0,0.190615,"sseini et al., 2019). can be adapted more flexibly to a PLM’s idAlthough LIiC is an inherently directional task, iosyncrasies. Contrasting patterns where a “tosymmetric cosine similarity in a vector space, such ken” can be any continuous vector vs. those as word2vec (an, 2013), has long been the state of where a discrete choice between vocabulary elthe art for this task. Only recently transfer learnements has to be made, we call our method CONtinuous pAtterNs (CONAN). We evaluate ing with pretrained Transformer (Vaswani et al., CONAN on two established benchmarks for lex2017) language models (Devlin et al., 2019), has ical inference in context (LIiC) a.k.a. predicate led to large improvements for LIiC. Schmitt and entailment, a challenging natural language unSchütze (2021) combine natural language (NL) patderstanding task with relatively small training terns with a pretrained language model (PLM) and sets. In a direct comparison with discrete patnot only set a new state of the art but also beat terns, CONAN consistently leads to improved baselines without access to such patterns. performance, setting a new state of the art. Our experiments give valuable insights into the kind Empirical findings sugges"
2021.emnlp-main.556,2021.acl-long.295,0,0.0354261,"inuous patintuitive sense to humans and outperform standard terns. CONAN does not depend on any manual RoBERTa on LIiC. A large problem with all these approaches, how- pattern design and is efficient as the shortest possible patterns with good performance can be found ever, is to find well-functioning patterns, for which numerous solutions have been proposed (Shin et al., automatically. It provides an automatic way of sys2020; Haviv et al., 2021; Bouraoui et al., 2020; tematically testing structural properties of patterns, such as length, w.r.t. performance changes. In our Jiang et al., 2020; Gao et al., 2021; Reynolds and experiments on two established LIiC benchmarks, McDonell, 2021). We argue that it is not optimal to constrain pattern search to the space of NL se- CONAN outperforms previous work using NL patquences if the primary goal is better task perfor- terns and sets a new state of the art. mance, and therefore abandon this constraint. Acknowledgments PLMs and continuous patterns. Li and Liang (2021) and Hambardzumyan et al. (2021) contem- We gratefully acknowledge a Ph.D. scholarship poraneously introduced the idea of mixing the input awarded to the first author by the German Acatoken em"
2021.emnlp-main.556,P18-2103,0,0.0697567,"Missing"
2021.emnlp-main.556,2021.acl-long.381,0,0.0129075,"; Haviv et al., 2021; Bouraoui et al., 2020; tematically testing structural properties of patterns, such as length, w.r.t. performance changes. In our Jiang et al., 2020; Gao et al., 2021; Reynolds and experiments on two established LIiC benchmarks, McDonell, 2021). We argue that it is not optimal to constrain pattern search to the space of NL se- CONAN outperforms previous work using NL patquences if the primary goal is better task perfor- terns and sets a new state of the art. mance, and therefore abandon this constraint. Acknowledgments PLMs and continuous patterns. Li and Liang (2021) and Hambardzumyan et al. (2021) contem- We gratefully acknowledge a Ph.D. scholarship poraneously introduced the idea of mixing the input awarded to the first author by the German Acatoken embeddings of a PLM with other continuous demic Scholarship Foundation (Studienstiftung des vectors that do not correspond to vocabulary ele- deutschen Volkes). This work was supported by the ments. In the spirit of GPT-2 (see above), they keep BMBF as part of the project MLWin (01IS18050). 6956 References Tomas Mikolov an. 2013. Efficient estimation of word representations in vector space. ArXiv preprint, abs/1301.3781. Kurt Bollacker, C"
2021.emnlp-main.556,2021.eacl-main.316,0,0.111291,"ctions (Efrat and Levy, guage inference (NLI) or recognizing textual entail- 2020), that (ii) PLMs can behave drastically differment (Dagan et al., 2013) with focus on the lexical ent with paraphrases of the same pattern (Elazar semantics of verbs and verbal expressions (Levy et al., 2021), and that (iii) performance increases if and Dagan, 2016; Schmitt and Schütze, 2019). Its we train a second model to rewrite an input pattern goal is to detect entailment between two very sim- with the goal of making it more comprehensible for ilar sentences, i.e., sentences that share subject a target PLM (Haviv et al., 2021), strongly suggest and object and only differ in the predicate, e.g., that patterns do not make sense to PLMs in the PERSON (A) runs ORG (B) → PERSON (A) leads same way as they do to humans. ORG (B). NLI models that were not specifically Our work sheds light on the interaction of pat1 Our code is publicly available: https://github. terns and PLMs and proposes a new method of imcom/mnschmit/conan proving pattern-based models fully automatically. 6952 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6952–6959 c November 7–11, 2021. 2021 Association fo"
2021.emnlp-main.556,P19-1468,0,0.0161553,"general language both zero- and few-shot settings. For zero-shot understanding. Other use cases for this kind of performance, it makes sense to design patterns lexical entailment knowledge include question anthat closely resemble the text seen during selfswering (Schoenmackers et al., 2010; McKenna supervised pretraining because the model has et al., 2021), event coreference (Shwartz et al., never seen anything else. Supervised training allows for more flexibility. If we allow for to2017; Meged et al., 2020), and link prediction in kens outside the PLM’s vocabulary, patterns knowledge graphs (Hosseini et al., 2019). can be adapted more flexibly to a PLM’s idAlthough LIiC is an inherently directional task, iosyncrasies. Contrasting patterns where a “tosymmetric cosine similarity in a vector space, such ken” can be any continuous vector vs. those as word2vec (an, 2013), has long been the state of where a discrete choice between vocabulary elthe art for this task. Only recently transfer learnements has to be made, we call our method CONtinuous pAtterNs (CONAN). We evaluate ing with pretrained Transformer (Vaswani et al., CONAN on two established benchmarks for lex2017) language models (Devlin et al., 2019)"
2021.emnlp-main.556,2020.tacl-1.28,0,0.0391452,"e of a PLM with continuous patintuitive sense to humans and outperform standard terns. CONAN does not depend on any manual RoBERTa on LIiC. A large problem with all these approaches, how- pattern design and is efficient as the shortest possible patterns with good performance can be found ever, is to find well-functioning patterns, for which numerous solutions have been proposed (Shin et al., automatically. It provides an automatic way of sys2020; Haviv et al., 2021; Bouraoui et al., 2020; tematically testing structural properties of patterns, such as length, w.r.t. performance changes. In our Jiang et al., 2020; Gao et al., 2021; Reynolds and experiments on two established LIiC benchmarks, McDonell, 2021). We argue that it is not optimal to constrain pattern search to the space of NL se- CONAN outperforms previous work using NL patquences if the primary goal is better task perfor- terns and sets a new state of the art. mance, and therefore abandon this constraint. Acknowledgments PLMs and continuous patterns. Li and Liang (2021) and Hambardzumyan et al. (2021) contem- We gratefully acknowledge a Ph.D. scholarship poraneously introduced the idea of mixing the input awarded to the first author by the"
2021.emnlp-main.556,2021.naacl-main.208,0,0.0755497,"Missing"
2021.emnlp-main.556,2021.emnlp-main.243,0,0.0554267,"Missing"
2021.emnlp-main.556,P16-2041,0,0.0353286,"Missing"
2021.emnlp-main.556,2021.acl-long.353,0,0.0148356,"automatic way of sys2020; Haviv et al., 2021; Bouraoui et al., 2020; tematically testing structural properties of patterns, such as length, w.r.t. performance changes. In our Jiang et al., 2020; Gao et al., 2021; Reynolds and experiments on two established LIiC benchmarks, McDonell, 2021). We argue that it is not optimal to constrain pattern search to the space of NL se- CONAN outperforms previous work using NL patquences if the primary goal is better task perfor- terns and sets a new state of the art. mance, and therefore abandon this constraint. Acknowledgments PLMs and continuous patterns. Li and Liang (2021) and Hambardzumyan et al. (2021) contem- We gratefully acknowledge a Ph.D. scholarship poraneously introduced the idea of mixing the input awarded to the first author by the German Acatoken embeddings of a PLM with other continuous demic Scholarship Foundation (Studienstiftung des vectors that do not correspond to vocabulary ele- deutschen Volkes). This work was supported by the ments. In the spirit of GPT-2 (see above), they keep BMBF as part of the project MLWin (01IS18050). 6956 References Tomas Mikolov an. 2013. Efficient estimation of word representations in vector space. ArXiv preprint,"
2021.emnlp-main.556,2021.ccl-1.108,0,0.0464024,"Missing"
2021.emnlp-main.556,2021.emnlp-main.840,0,0.0642535,"Missing"
2021.emnlp-main.556,2020.findings-emnlp.440,0,0.0142059,"2019), making LIiC an imwith textual patterns has been shown to help in portant evaluation criterion for general language both zero- and few-shot settings. For zero-shot understanding. Other use cases for this kind of performance, it makes sense to design patterns lexical entailment knowledge include question anthat closely resemble the text seen during selfswering (Schoenmackers et al., 2010; McKenna supervised pretraining because the model has et al., 2021), event coreference (Shwartz et al., never seen anything else. Supervised training allows for more flexibility. If we allow for to2017; Meged et al., 2020), and link prediction in kens outside the PLM’s vocabulary, patterns knowledge graphs (Hosseini et al., 2019). can be adapted more flexibly to a PLM’s idAlthough LIiC is an inherently directional task, iosyncrasies. Contrasting patterns where a “tosymmetric cosine similarity in a vector space, such ken” can be any continuous vector vs. those as word2vec (an, 2013), has long been the state of where a discrete choice between vocabulary elthe art for this task. Only recently transfer learnements has to be made, we call our method CONtinuous pAtterNs (CONAN). We evaluate ing with pretrained Transf"
2021.emnlp-main.556,2021.eacl-main.20,1,0.717821,"The key difference is that, instead of making predictions with different patterns and taking the majority vote, we rather compare the scores for different patterns to make our prediction. PLMs and text patterns. GPT-2 (Radford et al., 2019) made the idea popular that a PLM can perform tasks without access to any training data when prompted with the right NL task instructions. With GPT-3, Brown et al. (2020) adapted this idea to fewshot settings where the task prompt is extended by a few training samples. While this kind of few-shot adaptation with a frozen PLM only works with very big models, Schick and Schütze (2021b) achieve similar performance with smaller models by finetuning the PLM on the available training data and putting them into NL templates. Recently, Schmitt and Schütze (2021) investigated the use of PLMs for LIiC. Compared to a standard sequence classification fine-tuning approach, they were able to improve the PLM RoBERTa’s performance by putting an entailment candidate into textual contexts that only make sense for either a valid or invalid exam- 7 Conclusion ple. Patterns like “y because x.” (valid) or “It does We presented CONAN, a method that improves finenot mean that y just because x."
2021.emnlp-main.556,2021.naacl-main.185,1,0.750902,"The key difference is that, instead of making predictions with different patterns and taking the majority vote, we rather compare the scores for different patterns to make our prediction. PLMs and text patterns. GPT-2 (Radford et al., 2019) made the idea popular that a PLM can perform tasks without access to any training data when prompted with the right NL task instructions. With GPT-3, Brown et al. (2020) adapted this idea to fewshot settings where the task prompt is extended by a few training samples. While this kind of few-shot adaptation with a frozen PLM only works with very big models, Schick and Schütze (2021b) achieve similar performance with smaller models by finetuning the PLM on the available training data and putting them into NL templates. Recently, Schmitt and Schütze (2021) investigated the use of PLMs for LIiC. Compared to a standard sequence classification fine-tuning approach, they were able to improve the PLM RoBERTa’s performance by putting an entailment candidate into textual contexts that only make sense for either a valid or invalid exam- 7 Conclusion ple. Patterns like “y because x.” (valid) or “It does We presented CONAN, a method that improves finenot mean that y just because x."
2021.emnlp-main.556,P19-1086,1,0.852179,"ütze, 1 Introduction 2021a), little is known about the reasons for their Lexical inference in context (LIiC) – also called success. Recent findings that (i) PLMs can fail to predicate entailment – is a variant of natural lan- follow even simple instructions (Efrat and Levy, guage inference (NLI) or recognizing textual entail- 2020), that (ii) PLMs can behave drastically differment (Dagan et al., 2013) with focus on the lexical ent with paraphrases of the same pattern (Elazar semantics of verbs and verbal expressions (Levy et al., 2021), and that (iii) performance increases if and Dagan, 2016; Schmitt and Schütze, 2019). Its we train a second model to rewrite an input pattern goal is to detect entailment between two very sim- with the goal of making it more comprehensible for ilar sentences, i.e., sentences that share subject a target PLM (Haviv et al., 2021), strongly suggest and object and only differ in the predicate, e.g., that patterns do not make sense to PLMs in the PERSON (A) runs ORG (B) → PERSON (A) leads same way as they do to humans. ORG (B). NLI models that were not specifically Our work sheds light on the interaction of pat1 Our code is publicly available: https://github. terns and PLMs and pro"
2021.emnlp-main.556,2021.eacl-main.108,1,0.928368,"b ∈ R2 are learnable parameters, σ is the softmax function, and applying M means encoding the whole input sequence in a single d-dimensional vector according to the specifics of the PLM. For BERT (Devlin et al., 2019) and its successor RoBERTa (Liu et al., 2019), this implies a dense pooler layer with tanh activation over the contextualized token embeddings and picking the first of these embeddings (i.e., [CLS] for BERT and hsi for RoBERTa).2 For training, we apply dropout with a probability of 0.1 to the output of M(·). Inference with multiple patterns. Previous work (Bouraoui et al., 2020; Schmitt and Schütze, 2021) combined multiple patterns with the intuition that different NL patterns can capture different aspects of the task. This intuition makes also sense for CONAN . We conjecture that an efficient use of the model parameters requires different continuous patterns to learn different representations, which can detect different types of entailment. Following the aforementioned work, we form our final score s by combining the probability estimates from different patterns Γ by comparing the maximum probability for the two classes 0, 1 over all patterns: m1 (p, h) = max P (ˆ y = 1 |γ(p, h)) γ∈Γ m0 (p, h"
2021.emnlp-main.556,D10-1106,0,0.0506872,"Processing (CIS) LMU Munich, Germany martin@cis.lmu.de Abstract trained with lexical knowledge have been reported to struggle with this task (Glockner et al., 2018; Combining a pretrained language model (PLM) Schmitt and Schütze, 2019), making LIiC an imwith textual patterns has been shown to help in portant evaluation criterion for general language both zero- and few-shot settings. For zero-shot understanding. Other use cases for this kind of performance, it makes sense to design patterns lexical entailment knowledge include question anthat closely resemble the text seen during selfswering (Schoenmackers et al., 2010; McKenna supervised pretraining because the model has et al., 2021), event coreference (Shwartz et al., never seen anything else. Supervised training allows for more flexibility. If we allow for to2017; Meged et al., 2020), and link prediction in kens outside the PLM’s vocabulary, patterns knowledge graphs (Hosseini et al., 2019). can be adapted more flexibly to a PLM’s idAlthough LIiC is an inherently directional task, iosyncrasies. Contrasting patterns where a “tosymmetric cosine similarity in a vector space, such ken” can be any continuous vector vs. those as word2vec (an, 2013), has long"
2021.emnlp-main.556,2020.emnlp-main.346,0,0.0746247,"Missing"
2021.emnlp-main.556,S17-1019,0,0.0193782,"Missing"
2021.emnlp-main.665,W16-2206,0,0.0269224,"ormation latent in a multiparallel corpus to achieve better word alignments than aligning pairs of languages in isolation. Starting from translations of a sentence in multiple languages in a multiparallel corpus, 1 Introduction MPWA generates bilingual word alignments for all Word alignment is a challenging NLP task that language pairs using any available bilingual word plays an essential role in statistical machine trans- aligner. MPWA then improves the quality of word lation and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially"
2021.emnlp-main.665,W17-4711,0,0.0253646,"ithms to exploit the information latent in a multiparallel corpus to achieve better word alignments than aligning pairs of languages in isolation. Starting from translations of a sentence in multiple languages in a multiparallel corpus, 1 Introduction MPWA generates bilingual word alignments for all Word alignment is a challenging NLP task that language pairs using any available bilingual word plays an essential role in statistical machine trans- aligner. MPWA then improves the quality of word lation and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in w"
2021.emnlp-main.665,D17-1011,1,0.827965,"alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and NLP tasks like word sense d"
2021.emnlp-main.665,2008.iwslt-papers.1,0,0.0357154,"sidered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and NLP tasks like word sense disambiguation (Schütze, 1998). In 2009, Netflix’s recommender system competition"
2021.emnlp-main.665,J93-2003,0,0.207669,"Missing"
2021.emnlp-main.665,N19-1423,0,0.00475273,"2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of monolingual corpora. Complementarily, MPWA assumes the existence of a very small (a few 10,000s of sentences in our case) parallel corpus and then takes advantage of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Ef"
2021.emnlp-main.665,2021.eacl-main.181,0,0.207864,"d typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively leverages them for high-performing word ∗ Equal contribution - random order. alignment. 8457 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8457–8469 c November 7–11, 2021. 2021 Association for Computational Linguistics Link prediction methods are used to predict whether there should be a link between two nodes in a graph. They have various applications like movie recommendations, knowledge graph completion, and metabolic network reconstructi"
2021.emnlp-main.665,P18-1141,1,0.851847,"ds are used to predict whether there should be a link between two nodes in a graph. They have various applications like movie recommendations, knowledge graph completion, and metabolic network reconstruction (Zhang and Chen, 2018). We use the Adamic-Adar index (Adamic and Adar, 2003); it is a second-order link prediction algorithm, i.e., it exploits the information of neighbors that are up to two hops aways from the starting target nodes (Zhou et al., 2009). We use a second-order algorithm since a set of aligned words in multiple languages (representing a concept) tends to establish a clique (Dufter et al., 2018). This means that exploring the influence of nodes at a distance of two in the graph provides informative signals while at the same time keeping runtime complexity low. lel corpus. This is an alternative approach that is especially important for low resource languages for which monolingual data often are not available. The PBC corpus does not contain a word alignment gold standard. To conduct the comparative evaluation of our new method, we port three existing word alignment gold standards of Bible translations to PBC, for the language pairs EnglishFrench, Finnish-Hebrew and Finnish-Greek. We"
2021.emnlp-main.665,N13-1073,0,0.273162,"to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin"
2021.emnlp-main.665,P07-1092,0,0.0633369,"of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Multiparallel corpus alignment. Most work on word alignment has focused on bilingual corpora. To the best of our knowledge, only one method specifically designed for multiparallel corpora was previously proposed: (Östling, 2014).2 However this method is outperformed by a “biparallel” method by the same author, Eflomal (Östling and Tiedemann, 2016). We compare with Eflomal in our experiments. Cohn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepag"
2021.emnlp-main.665,D19-1453,0,0.0196733,"our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of"
2021.emnlp-main.665,2020.acl-main.747,0,0.0314769,"stling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of monolingual corpora. Complementarily, MPWA assumes the existence of a very small (a few 10,000s of sentences in our case) parallel corpus and then takes advantage of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Mul"
2021.emnlp-main.665,2020.findings-emnlp.147,1,0.899592,"ilable and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant lan"
2021.emnlp-main.665,2020.acl-main.560,0,0.0185496,"e to fertility: words in the source language generally have only a few possible matches in the target language (Zhao and Gildea, 2010). A multiparallel corpus provides parallel sentences in more than two languages. This type of corpus facilitates the study of multiple languages together, which is especially important for research on low resource languages. As far as we know, out of all available multiparallel corpora, the Parallel Bible Corpus (Mayer and Cysouw, 2014) (PBC) provides the highest language coverage, supporting 1334 different languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and"
2021.emnlp-main.665,N03-1017,0,0.0405072,"Missing"
2021.emnlp-main.665,D07-1005,0,0.0791796,"llelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Multiparallel corpus alignment. Most work on word alignment has focused on bilingual corpora. To the best of our knowledge, only one method specifically designed for multiparallel corpora was previously proposed: (Östling, 2014).2 However this method is outperformed by a “biparallel” method by the same author, Eflomal (Östling and Tiedemann, 2016). We compare with Eflomal in our experiments. Cohn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and"
2021.emnlp-main.665,2008.amta-papers.11,0,0.0610193,"hn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and lin"
2021.emnlp-main.665,C08-2014,0,0.0615272,"hn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and lin"
2021.emnlp-main.665,R09-1040,0,0.0450543,"llel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent fac"
2021.emnlp-main.665,I08-2093,0,0.0597577,"ion and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively leverages them for high-performing word ∗ Equal contribution - random o"
2021.emnlp-main.665,W12-0209,0,0.0276456,"cedure. Since the goal of this research is to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 201"
2021.emnlp-main.665,mayer-cysouw-2014-creating,0,0.181368,"wo translations of a sentence with lengths M and N , among all possible alignment links (M × N ), only a few (O(M + N )) are correct. This is partly due to fertility: words in the source language generally have only a few possible matches in the target language (Zhao and Gildea, 2010). A multiparallel corpus provides parallel sentences in more than two languages. This type of corpus facilitates the study of multiple languages together, which is especially important for research on low resource languages. As far as we know, out of all available multiparallel corpora, the Parallel Bible Corpus (Mayer and Cysouw, 2014) (PBC) provides the highest language coverage, supporting 1334 different languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based"
2021.emnlp-main.665,W17-4804,0,0.0191402,"hine trans- aligner. MPWA then improves the quality of word lation and is useful for neural machine translation alignments for a target language pair by inspect(Alkhouli and Ney, 2017; Alkhouli et al., 2016; ing how they are aligned to other languages. The Koehn et al., 2003). Other applications of word central idea is to exploit the graph structure of an alignments include bilingual lexicon induction, an- initial multiparallel word alignment to improve the notation projection, and typological analysis (Shi alignment for a target language pair. To this end, et al., 2021; Rasooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively"
2021.emnlp-main.665,J03-1002,0,0.282036,"t languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresourced. 2. We port and publish three word alignment gold standards for the Parallel Bible Corpus. 3. We show that our method is also applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language"
2021.emnlp-main.665,E14-4024,0,0.0222088,"he availabiliy of monolingual corpora. Complementarily, MPWA assumes the existence of a very small (a few 10,000s of sentences in our case) parallel corpus and then takes advantage of information from the other languages in the paral8458 1 https://github.com/cisnlp/graph-align leverage multiparallelism on top of any bilingual word aligner; in this paper, we use Eflomal and SimAlign. Multiparallel corpus alignment. Most work on word alignment has focused on bilingual corpora. To the best of our knowledge, only one method specifically designed for multiparallel corpora was previously proposed: (Östling, 2014).2 However this method is outperformed by a “biparallel” method by the same author, Eflomal (Östling and Tiedemann, 2016). We compare with Eflomal in our experiments. Cohn and Lapata (2007) make use of multiparallel corpora to obtain more reliable translations from small datasets. Kumar et al. (2007) show that multiparallel corpora can be of benefit to reach better performance in phrase-based statistical machine translation (SMT). Filali and Bilmes (2005) present a multilingual SMT-based word alignment model, extending IBM models, based on HMM models and a two step alignment procedure. Since t"
2021.emnlp-main.665,P15-2034,0,0.0181217,"to tackle word alignment directly without considering machine translation, these works are not considered here. In another line of research, Lardilleux and Lepage (2008a) introduce a corpus splitting method to come up with a perfect alignment of multiwords. Lardilleux and Lepage (2008b), and Lardilleux and Lepage (2009) suggest to rely only on low frequency terms for a similar purpose: sub-sentential alignment. These methods solve a somewhat different problem than what is addressed by us. Other usages of multiparallel corpora are language comparison (Mayer and Cysouw, 2012), typology studies (Östling, 2015; Asgari and Schütze, 2017; ImaniGooghari et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and N"
2021.emnlp-main.665,P14-1131,1,0.796545,"onnecting links (Zhou et al., 2009). Link prediction algorithms compute the likelihood of links based on different heuristics. One can categorize available methods based on the maximum number of hops they consider in their computations for each node (Zhang and Chen, 2018). First order algorithms, such as common neighbors (CN), only consider one hop neighborhoods, e.g., (Barabási and Albert, 1999). Second order methods consider two hops, e.g., (Zhou et al., 2009). Finally, higher order methods take the whole network into account for making predictions (Brin and Page, 1998; Jeh and Widom, 2002; Rothe and Schütze, 2014). In this paper, we use a two-hop method since it offers a good tradeoff between effectiveness and efficiency. 3 3.1 Methods The MPWA framework While a bilingual aligner considers each language pair separately, MPWA utilizes the synergy between all language pairs to improve word alignment performance. In Figure 1, Eflomal alignments of a sentence from PBC in four different languages are depicted. Although Eflomal has failed to find the link between German “Schritt” and French “pas”, we can easily find this relation by observing that the four nodes “step”, “Schritt”, “paso”, and “pas” are fully"
2021.emnlp-main.665,J98-1004,1,0.16735,"et al., 2021) and SMT (Nakov and Ng, 2012; Bertoldi et al., 2008; Dyer et al., 2013) Matrix factorization and link prediction. Matrix factorization is a technique that factors, in the most typical case, a matrix into two lower-ranked matrices in which the latent factors of the original matrix are represented. Matrix factorization approaches have been widely used in document clustering (Xu et al., 2003; Shahnaz et al., 2006), topic modeling (Kuang et al., 2015; Choo et al., 2013) information retrieval (Zamani et al., 2016; Deerwester et al., 1990) and NLP tasks like word sense disambiguation (Schütze, 1998). In 2009, Netflix’s recommender system competition revealed that this 2 https://github.com/robertostling/ eflomal technique effectively works for collaborative filtering (Koren et al., 2009). Since then it has been a state of the art method in recommender systems. Link prediction algorithms are widely used in different areas of science since many social, biological, and information systems can be described as networks with nodes and connecting links (Zhou et al., 2009). Link prediction algorithms compute the likelihood of links based on different heuristics. One can categorize available metho"
2021.emnlp-main.665,2021.acl-long.67,0,0.034569,"Missing"
2021.emnlp-main.665,2020.lrec-1.522,0,0.124181,"multilingual word alignment matrix W for each sentence as shown in Figure 2. Each cell contains 0 or 1 for Adamic-Adar or the alignment score for Weighted Adamic-Adar. We again apply Argmax to extract new alignment edges and then add them to the original alignment. 4 4.1 Experimental setup PBC New Testament and Hebrew Bible while others contain only one. Table 2 gives corpus statistics. 4.2 Word alignment datasets PBC does not provide gold word alignments. To evaluate MPWA, we port two word alignment gold datasets of the Bible to PBC: Blinker (Melamed, 1998) and the recently published HELFI (Yli-Jyrä et al., 2020). We further experiment with bilingual datasets, using Machine Translation (MT) to create multiparallel corpora. Table 1 gives dataset statistics. The HELFI dataset consists of the Greek New Testament, the Hebrew Bible and translations of both into Finnish. In addition, morpheme alignments are provided for Finnish-Greek and FinnishHebrew. We reformatted this dataset to the format used by PBC. In more detail, we added three new editions for the three languages to PBC. We identified the PBC verse identifier for each verse of HELFI to ensure proper verse alignment of these three new editions. The"
2021.emnlp-main.665,2020.acl-main.146,0,0.0138534,"applicable, using machine translation, to scenarios where multiparallel data is not available. 4. We publish our code1 and data. 2 Related Work Bilingual Word Aligners take different approaches. Some are based on statistical analysis, like IBM models (Brown et al., 1993), Giza++ (Och and Ney, 2003a), fast-align (Dyer et al., 2013) and Eflomal (Östling and Tiedemann, 2016). Another more recent group, including SimAlign (Jalili Sabet et al., 2020) and Awesome-align (Dou and Neubig, 2021), utilizes neural language models. The last group is based on neural machine translation (Garg et al., 2019; Zenkel et al., 2020). While neural models outperform statistical models, for cases where only a small parallel dataset is available, statistical models are still superior. In this paper we use PBC, a corpus with 1334 languages, of which only about two hundred are supported by multilingual language models like Bert and XLM-R (Devlin et al., 2019; Conneau et al., 2020). MPWA can MPWA has especially strong word alignment improvements for distant language pairs for which existing bilingual word aligners perform poorly. Much work that addresses low resource languages relies on the availabiliy of monolingual corpora. C"
2021.emnlp-main.665,D10-1058,0,0.0395324,". Matrix factorization is a collaborative filtering algorithm that is most prominently used in recommender systems where it provides users with product recommendations based on their interactions with other products. This method is especially useful if the matrix is sparse (Koren et al., 2009). This is true for our application: Given two translations of a sentence with lengths M and N , among all possible alignment links (M × N ), only a few (O(M + N )) are correct. This is partly due to fertility: words in the source language generally have only a few possible matches in the target language (Zhao and Gildea, 2010). A multiparallel corpus provides parallel sentences in more than two languages. This type of corpus facilitates the study of multiple languages together, which is especially important for research on low resource languages. As far as we know, out of all available multiparallel corpora, the Parallel Bible Corpus (Mayer and Cysouw, 2014) (PBC) provides the highest language coverage, supporting 1334 different languages, many of which belong to categories 0 and 1 (Joshi et al., 2020) – that is, they are languages for which no language technologies are available and that are severely underresource"
2021.emnlp-main.665,2020.emnlp-main.362,0,0.0346647,"sooli et al., 2018; Müller, 2017; MPWA casts the multiparallel word alignment task Lewis and Xia, 2008). With the advent of deep as a link (or edge) prediction problem. We explore learning, interest in word alignment initially de- standard algorithms for this purpose: Adamic-Adar creased. However, recently a new wave of publica- and matrix factorization. While these two graphtions has again drawn attention to the task (Jalili Sa- based algorithms are quite different and are used in bet et al., 2020; Dou and Neubig, 2021; Marchisio different applications, we will show that MPWA efet al., 2021; Wu and Dredze, 2020). fectively leverages them for high-performing word ∗ Equal contribution - random order. alignment. 8457 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8457–8469 c November 7–11, 2021. 2021 Association for Computational Linguistics Link prediction methods are used to predict whether there should be a link between two nodes in a graph. They have various applications like movie recommendations, knowledge graph completion, and metabolic network reconstruction (Zhang and Chen, 2018). We use the Adamic-Adar index (Adamic and Adar, 2003); it is a second"
2021.emnlp-main.672,D15-1075,0,0.314495,"y outperforms finetuning (Devlin et al., 2019) when adapting pretrained language models (PLMs; Devlin et al. (2019); Conneau et al. (2020)) to downstream tasks in the low-resource regime (Brown et al., 2020; Schick and Schütze, 2021; Gao et al., 2020; Tam et al., 2021; Le Scao and Rush, 2021), i.e., few-shot learning, a more realistic scenario than having tens of thousands of annotations, even for English (Yu et al., 2018; Yin et al., 2020; Ram et al., 2021). In contrast to finetuning, which learns discriminative classifiers for tasks like natural language inference (NLI; Dagan et al. (2006); Bowman et al. (2015)), prompting reformulates the classification task to generative text-to-text (Raffel et al., 2020) or cloze-style (McCann et al., 2018; Brown et al., 2020) queries which are given to a PLM to answer. For example, the NLI task of assigning premise “They whinnied, eyes wide” and hypothesis “Their eyes were open wide” to class “entailment” can be reformulated as: They whinnied, eyes wide . were open wide ? Answer: Question: . The PLM is requested to fill in, for the blank ( ), the word “yes”, which is mapped to “entailment”. Prompting makes a human description of the task available in learning. A"
2021.emnlp-main.672,2020.acl-main.747,0,0.16735,"(2021) propose prefix-tuning to encourage PLMs to solve generation tasks with high parameter-efficiency (Houlsby et al., 2019; Zhao et al., 2020). Lester et al. (2021) demonstrate that soft prompting benefits from scaling up the number of PLM parameters. Liu et al. (2021) show that GPT (Radford et al., 2019) can solve NLU tasks (Wang et al., 2019) with soft prompting. All of this work focuses on English. We show that discrete and soft prompting perform better than finetuning in few-shot crosslingual natural language inference (XNLI; Conneau et al. (2018)) with multilingual PLMs (XLM-RoBERTa; Conneau et al. (2020)). We conduct experiments on NLI because it is one of the most representative and challenging NLU tasks (Dagan et al., 2006; Bowman et al., 2015), and has been commonly used in prior work on prompting. prompt. Specifically, we ask the PLM to fill in the blank ( ) in sentence: Premise . Question: Hypothesis ? Answer: Premise and Hypothesis are a pair of sentences from the NLI dataset. The gold labels are mapped to words in the PLM vocabulary. Concretely, we use following mapping (verbalizer; Schick and Schütze (2021)): “entailment”→ “yes”; “contradiction”→ “no”; “neutral”→ “maybe”. The optimiza"
2021.emnlp-main.672,D18-1269,0,0.122667,"o soft prompting but with the PLM being frozen, Li and Liang (2021) propose prefix-tuning to encourage PLMs to solve generation tasks with high parameter-efficiency (Houlsby et al., 2019; Zhao et al., 2020). Lester et al. (2021) demonstrate that soft prompting benefits from scaling up the number of PLM parameters. Liu et al. (2021) show that GPT (Radford et al., 2019) can solve NLU tasks (Wang et al., 2019) with soft prompting. All of this work focuses on English. We show that discrete and soft prompting perform better than finetuning in few-shot crosslingual natural language inference (XNLI; Conneau et al. (2018)) with multilingual PLMs (XLM-RoBERTa; Conneau et al. (2020)). We conduct experiments on NLI because it is one of the most representative and challenging NLU tasks (Dagan et al., 2006; Bowman et al., 2015), and has been commonly used in prior work on prompting. prompt. Specifically, we ask the PLM to fill in the blank ( ) in sentence: Premise . Question: Hypothesis ? Answer: Premise and Hypothesis are a pair of sentences from the NLI dataset. The gold labels are mapped to words in the PLM vocabulary. Concretely, we use following mapping (verbalizer; Schick and Schütze (2021)): “entailment”→ “y"
2021.emnlp-main.672,2020.acl-main.703,0,0.0930401,"Missing"
2021.emnlp-main.672,2021.acl-long.239,0,0.0801255,"Missing"
2021.emnlp-main.672,2021.naacl-main.185,1,0.928937,"prompt describing the NLU task is prepended to an input example; GPT3 is then caTheir eyes pable of making accurate predictions without updating its parameters. However, the number of 8547 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8547–8555 c November 7–11, 2021. 2021 Association for Computational Linguistics parameters in GPT3 is prohibitively large (175B). Integrating gradient descent into prompting, smaller (w.r.t. GPT3) PLMs also achieve good fewshot performance. Like GPT3, discrete prompting uses natural language to describe NLU tasks. Schick and Schütze (2021), Tam et al. (2021), Le Scao and Rush (2021) use human-designed prompts. Gao et al. (2020) leverage T5 (Raffel et al., 2020) to generate prompts. Shin et al. (2020) use extra training data to search tokens for constructing the prompts. Discrete prompting naturally inherits interpretability from the task descriptions. Soft prompting relaxes the constraint that a prompt needs to be composed of discrete tokens. Instead, it learns the prompt in the continuous space with SGD. Qin and Eisner (2021) and Zhong et al. (2021) learn soft prompts eliciting more knowledge (Petroni et al., 2019) from PLMs t"
2021.emnlp-main.672,2020.emnlp-main.346,0,0.0325082,"the number of 8547 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8547–8555 c November 7–11, 2021. 2021 Association for Computational Linguistics parameters in GPT3 is prohibitively large (175B). Integrating gradient descent into prompting, smaller (w.r.t. GPT3) PLMs also achieve good fewshot performance. Like GPT3, discrete prompting uses natural language to describe NLU tasks. Schick and Schütze (2021), Tam et al. (2021), Le Scao and Rush (2021) use human-designed prompts. Gao et al. (2020) leverage T5 (Raffel et al., 2020) to generate prompts. Shin et al. (2020) use extra training data to search tokens for constructing the prompts. Discrete prompting naturally inherits interpretability from the task descriptions. Soft prompting relaxes the constraint that a prompt needs to be composed of discrete tokens. Instead, it learns the prompt in the continuous space with SGD. Qin and Eisner (2021) and Zhong et al. (2021) learn soft prompts eliciting more knowledge (Petroni et al., 2019) from PLMs than discrete prompts. Similar to soft prompting but with the PLM being frozen, Li and Liang (2021) propose prefix-tuning to encourage PLMs to solve generation tasks"
2021.emnlp-main.697,2020.tacl-1.3,0,0.027822,"988), and uncertainty (Pearl, 1986). Similarly, work in cognitive science has promoted mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A close analogy to our task is in knowledge graph (KG) construction. Pujara et al. (2"
2021.emnlp-main.697,K18-1007,0,0.0878341,"d with an original PTLM. In contrast to prior work, this system does not rely on fine-tuning the PTLM. Fine-tuning requires expensive training data, and risks destabilizing the model’s performance on other tasks outside the scope of training. Instead, our system functions without training data, explicitly reasoning about beliefs using an external mechanism, thus allowing both controllability and interpretability. Most significantly, we find that improving consistency in this way improves accuracy, while earlier finetuning-based approaches report either no accuracy gains (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019) or only slight gains (Asai and Hajishirzi, 2020). Specifically, we show that two mechanisms – constraint reasoning and feedback – improve the overall system’s accuracy and consistency over time. 2. We contribute a targeted dataset to measure a system’s consistency against given constraints. 3. We provide an analysis of the failure modes and directions for future work. This work is significant as it is a first step towards PTLM-based architectures that have a systematic notion of belief, allowing them to construct a more coherent picture of the world, and improve over time wi"
2021.emnlp-main.697,D19-1250,0,0.153537,"w is not a fish”) as the query context. We find that both consistency and accuracy of the overall system improve. Example: The model M shown in the figure incorrectly answers “yes”, when asked “a swallow has gills?”. But (as shown above) if reminded of its previous answer “a swallow is not a fish”, M correctly answers ""no"". conditional on its other knowledge and reasoning abilities. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p ("
2021.emnlp-main.697,2020.starsem-1.10,0,0.0233858,"imilarly, work in cognitive science has promoted mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A close analogy to our task is in knowledge graph (KG) construction. Pujara et al. (2013) We make the following contributions: 1. We"
2021.emnlp-main.697,P19-1621,0,0.355107,"ge and reasoning abilities. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p (and its other beliefs). Pragmati- ble to inconsistent and/or irrational behavior. Our goal is a first step to avoid these problems by emcally, we expect the system to (a) give a consistent bedding a PTLM in a broader system with a clearer answer to different paraphrases of the question ""p?"" (""Are eagles birds?"", ""Is an eagle a type of bird?"", notion of be"
2021.emnlp-main.697,2020.emnlp-main.437,0,0.0223052,"nd Nilsson, 1987; Moore, 1983), belief revision (De Kleer, 1986; Dechter and Dechter, 1988), and uncertainty (Pearl, 1986). Similarly, work in cognitive science has promoted mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A clo"
2021.emnlp-main.697,2020.acl-main.698,1,0.935731,"xample: The model M shown in the figure incorrectly answers “yes”, when asked “a swallow has gills?”. But (as shown above) if reminded of its previous answer “a swallow is not a fish”, M correctly answers ""no"". conditional on its other knowledge and reasoning abilities. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p (and its other beliefs). Pragmati- ble to inconsistent and/or irrational behavior. Our goal is a first step to avoid thes"
2021.emnlp-main.697,D19-1405,0,0.371899,"ties. Maintaining a consistent set of beliefs (a “belief system”) is a key facet of intelligence, as it can help debug errors and encourage rational behavior. However, although PTLMs contain substantial world knowledge (Petroni et al., 2019), their 1 Introduction answers to probing questions can be inconsistent (Elazar et al., 2021; Kassner and Schütze, 2020), Intelligent agents are typically considered to have even after specialized training to reduce inconsisbeliefs about the world – propositions that they take as true (Genin and Huber, 2021). In general, a sys- tency (Ribeiro et al., 2019; Li et al., 2019). As a result, it is sometimes hard to pin down what a tem can be said to (appear to) believe a proposition PTLM actually “believes”, making them susceptip, e.g., “eagles are birds”, if it produces answers consistent with p (and its other beliefs). Pragmati- ble to inconsistent and/or irrational behavior. Our goal is a first step to avoid these problems by emcally, we expect the system to (a) give a consistent bedding a PTLM in a broader system with a clearer answer to different paraphrases of the question ""p?"" (""Are eagles birds?"", ""Is an eagle a type of bird?"", notion of belief (see Figure 1"
2021.emnlp-main.697,2020.emnlp-main.373,0,0.0543443,"Missing"
2021.emnlp-main.697,2020.acl-main.495,0,0.0267304,"d mental models – coherent, constructed representations of the way the world is believed to be – as central to understanding and communication (Johnson-Laird, 1983; Gentner and Stevens, 1983; Hilton, 1996). We draw on these ideas, proposing how they can be layered on top of PTLMs, here representing beliefs as NL statements rather than formal structures. Although PTLMs contain extensive world knowledge (Petroni et al., 2019; Roberts et al., 2020), they can be inconsistent in their answers to probing questions (Ettinger, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020), making their “world model” unclear. Although various approaches have improved answer consistency, mainly through modified model training, e.g., (Ribeiro et al., 2019; Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020), they have not solved the problem. Current PTLMs still behave as a source of noisy knowledge, rather than projecting a coherent picture of the world (Kassner and Schütze, 2020). A close analogy to our task is in knowledge graph (KG) construction. Pujara et al. (2013) We make the following contributions: 1. We show that a PTLM-based system can be given a con"
2021.findings-emnlp.205,2020.acl-main.658,0,0.17116,"tokenization in French it will be hard to train common cross-lingual representations. ii) Given a limited vocabulary budget, e.g., from memory constraints, a researcher is naturally interested in how to distribute the budget across multiple languages. iii) Currently, a balance of the tokenization is heuristically achieved through up- and downsampling (e.g., Conneau et al., 2020a). With a better tokenization strategy, one might be able to achieve performance improvements. Research in multilinguality has so far focused, among other directions, on analyzing and improving cross-lingual transfer (Artetxe et al., 2020a; K et al., 2020; Wu and Dredze, 2019; Conneau et al., 2020b) and modeling tasks (Conneau and Lam2382 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2382–2399 November 7–11, 2021. ©2021 Association for Computational Linguistics ple, 2019; Huang et al., 2019). Earlier work on tokenization focused on the development of algorithms (Schuster and Nakajima, 2012; Gage, 1994) while analysis of the effect of tokenization has been undertaken both in the general (Bostrom and Durrett, 2020) and in the multilingual domain (Wei et al., 2021). We propose to investigate the com"
2021.findings-emnlp.205,Q17-1010,0,0.0444986,"h is a concept examined in K et al. (2020), where English characters are converted to some separate, special characters such that the model processes English and Fake-English as two different languages. The “new” data that is created exhibits the same grammar and structure as English, but the script is different. This offers both a simple upper performance limit and a scrying glass into potential bugs or shortcomings of our approach. Note that to generate the Fake-English data, we used the same edition as the English Bible set, to ensure better comparability. static embeddings using fastText (Bojanowski et al., 2017) with default parameters. 4.2.2 Compatibility Evaluation Comparing static embedding spaces at different granularities is challenging. Tasks like word translation or sentence retrieval disqualify, as obtaining word or sentence representations through mean pooling is too crude to get discriminative evaluation results. Training a neural network on top of the embeddings to evaluate multilinguality requires additional input, might obliterate the effect we want to investigate (i.e., the compatibility of the embedding spaces) and is not feasible when processing many language pairs. Therefore we emplo"
2021.findings-emnlp.205,2020.findings-emnlp.414,0,0.0210749,"so far focused, among other directions, on analyzing and improving cross-lingual transfer (Artetxe et al., 2020a; K et al., 2020; Wu and Dredze, 2019; Conneau et al., 2020b) and modeling tasks (Conneau and Lam2382 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2382–2399 November 7–11, 2021. ©2021 Association for Computational Linguistics ple, 2019; Huang et al., 2019). Earlier work on tokenization focused on the development of algorithms (Schuster and Nakajima, 2012; Gage, 1994) while analysis of the effect of tokenization has been undertaken both in the general (Bostrom and Durrett, 2020) and in the multilingual domain (Wei et al., 2021). We propose to investigate the compatibility of tokenizations across languages systematically. To this end, we train both static and contextualized embeddings using different vocabulary sizes and WordPiece tokenization (Schuster and Nakajima, 2012). Subsequently we investigate the similarity of the embedding spaces and their degree of multilinguality. Figure 1 shows an example of our research question: here we trained static embedding spaces and investigated the similarity of the embedding spaces by computing the singular value gap (SVG) measu"
2021.findings-emnlp.205,2020.acl-main.747,0,0.247907,"Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) have become the de-facto standard in natural language processing (NLP). Due to memory constraints and to overcome sparsity, subword tokenization models such as byte pair encoding (Sennrich et al., 2016) or WordPiece tokenization (Schuster and Nakajima, 2012) are most commonly used. Multilingual pretrained language models (PLMs) such as mBERT (Devlin et al., 2019) are mostly trained in an unsupervised fashion without any cross-lingual supervision. They rely on the fact that the underlying distributions across languages can be well aligned (Conneau et al., 2020b). However, there is little work that investigates the compatibility of tokenizations across languages. We argue that this is an important research question for several reasons. i) Recent work suggests that the tokenization in multilingual models influences ∗ Equal contribution. 9 the multilinguality of these models (Rust et al., 2021). This corresponds to a strong intuition: in case of heavily diverging tokenizations, e.g., word tokenization in English and character tokenization in French it will be hard to train common cross-lingual representations. ii) Given a limited vocabulary budget, e."
2021.findings-emnlp.205,D18-1269,0,0.180791,"n recent years. Wu and Dredze (2019); Nooralahzadeh et al. (2020); Artetxe et al. (2020a); Conneau et al. (2020b) investigate cross-lingual transfer, with an extensive study of multilinguality at-scale presented in Conneau et al. (2020a). K et al. (2020) research how linguistic variation in language properties affects multilingual BERT models, with Dufter and Schütze (2020) further investigating the effect of model architecture and linguistic properties on BERT’s multilinguality. Conneau and Lample (2019) focus on variations of modeling tasks and their effect on downstream tasks such as XNLI (Conneau et al., 2018) and machine translation. A novel set of crosslingual pre-training tasks was introduced in Huang et al. (2019). Anastasopoulos and Neubig (2020) show that performance in bilingual experiments is not optimal when English is used as the “hub”, but it depends on the language pair. Work on the representation space of BERT has also been abundant. Singh et al. (2019) investigate the representation space of mBERT and its properties and Pires et al. (2019) initially researched the degree of multilinguality of mBERT. In our work, we further investigate the zeroshot capabilities of bilingual models and"
2021.findings-emnlp.205,2020.acl-main.536,0,0.166909,"Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) have become the de-facto standard in natural language processing (NLP). Due to memory constraints and to overcome sparsity, subword tokenization models such as byte pair encoding (Sennrich et al., 2016) or WordPiece tokenization (Schuster and Nakajima, 2012) are most commonly used. Multilingual pretrained language models (PLMs) such as mBERT (Devlin et al., 2019) are mostly trained in an unsupervised fashion without any cross-lingual supervision. They rely on the fact that the underlying distributions across languages can be well aligned (Conneau et al., 2020b). However, there is little work that investigates the compatibility of tokenizations across languages. We argue that this is an important research question for several reasons. i) Recent work suggests that the tokenization in multilingual models influences ∗ Equal contribution. 9 the multilinguality of these models (Rust et al., 2021). This corresponds to a strong intuition: in case of heavily diverging tokenizations, e.g., word tokenization in English and character tokenization in French it will be hard to train common cross-lingual representations. ii) Given a limited vocabulary budget, e."
2021.findings-emnlp.205,N19-1423,0,0.17912,"ates larger (smaller) compatibility. Black stars indicate vocabulary sizes where absolute compression rates of both languages are equal. The figure confirms our intuitions about vocabulary size compatibility: for English/Russian, similar vocabulary sizes are compatible, but for Chinese, we only find compatibility for larger English vocabulary sizes. This is because a Chinese logogram by itself is usually a meaning bearing unit, so that most subword (or sub-root) elements in English have no analog in Chinese. Introduction Pretrained language models (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) have become the de-facto standard in natural language processing (NLP). Due to memory constraints and to overcome sparsity, subword tokenization models such as byte pair encoding (Sennrich et al., 2016) or WordPiece tokenization (Schuster and Nakajima, 2012) are most commonly used. Multilingual pretrained language models (PLMs) such as mBERT (Devlin et al., 2019) are mostly trained in an unsupervised fashion without any cross-lingual supervision. They rely on the fact that the underlying distributions across languages can be well aligned (Conneau et al., 2020b). However, there is little work"
2021.findings-emnlp.205,2020.emnlp-main.186,0,0.0945557,"Missing"
2021.insights-1.3,D19-1453,0,0.0216952,"in mBERT. In Pires et al. (2019), an analysis of mBERT is presented, while in Conneau and Lample (2019); Wu and Dredze (2019); Artetxe et al. (2020) zero-shot cross-lingual transfer is analyzed. Dufter and Schütze (2020) further analyzes mBERT’s capabilities with BERT’s architecture and the structure of languages examined. The authors performed their experiments on a pairing of English with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based on Recurrent Neural Networks (Luong et al., 2015) for characterlevel tasks such as transliteration and graphemeto-phoneme conversion. Further work to develop character-level BERT-based models is conducted in El Boukkouri et al. (2020); Ma et al. (2020). Och and Ney (2003); Legrand et al. (2016) worked towards representation-based word alignment, with implementations of aligners proposed in Dyer et al. (2013); Östling and Tiedemann (2016). We also note recent efforts"
2021.insights-1.3,2020.acl-main.421,0,0.0339532,"contains an English word (split in characters), the second line a German word (split in characters), and so on. For the conversion of English to Fake-English, we employ a mapping of characters to integers. The integers are in the range [100, 151]. The same mapping takes place for the English → German setup, since the two languages share the same script. Data is given as input lineby-line to the model. Research has been conducted to uncover elements of multilinguality in mBERT. In Pires et al. (2019), an analysis of mBERT is presented, while in Conneau and Lample (2019); Wu and Dredze (2019); Artetxe et al. (2020) zero-shot cross-lingual transfer is analyzed. Dufter and Schütze (2020) further analyzes mBERT’s capabilities with BERT’s architecture and the structure of languages examined. The authors performed their experiments on a pairing of English with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based o"
2021.insights-1.3,Q17-1010,0,0.0232688,"alphabets were examined and we found that the closer two languages are, the better BERT does in aligning them. Fake-English is the easiest to align with English, whereas German is worse, with Greek trailing far behind. We conclude that BERT is not able to perform adequate character alignment. Ablation Studies We also performed some minor experiments with static embeddings and adjustments to the MLM task, as well as a variation to our EngF ake setup. Apart from the BERT embeddings, we also examined static embeddings, to introduce another baseline. Specifically, we used the FastText algorithm (Bojanowski et al., 2017). Data was the same as the previous experiments. Results were seemingly random, with FastText unable to capture any meaningful representations. Cosine similarities were generally very low and alignments incoherent. For the MLM task, we experimented with different probabilities for token masking. In the origiAcknowledgments This work was supported by ERCAdG #740516. We want to thank the anonymous reviewers for their insightful comments and suggestions. 3 To control for character positions, we also aligned characters without positional embeddings and results got worse. 4 While this is a more nua"
2021.insights-1.3,2020.findings-emnlp.147,1,0.917826,"ll Table 1: Max alignments for English → German and English → Greek, as extracted from a cosine similarity matrix between the two alphabets for each experiment. Correct alignments are in bold. We see low accuracy for both setups, but especially low for English → Greek. Introduction For the many sweeping successes BERT has had in the field of Natural Language Processing, the model’s alignment capabilities have been lacking and under-explored. Work in this area is picking up and it has been shown that BERT can operate with adequate efficiency in word alignment tasks (Zenkel et al., 2019; Jalili Sabet et al., 2020). The question whether or not BERT can perform character-level alignment, though, has not been answered yet. Even though characters on their own do not necessarily hold much semantic meaning, we investigate whether BERT is able to generate useful representation spaces for characters. Character-level alignment would be useful in tasks like transliteration (Li et al., 2009; Sajjad et al., 2017) or wordlevel alignment (Legrand et al., 2016). In a lot of occurrences of transliterations, grammatical inflections are added or dropped, which causes difficulties in an array of tasks (Czarnowska et al.,"
2021.insights-1.3,P19-1356,0,0.0207514,"k scoring highly across the board as well. Finally, in Table 2 (c), DeuEll is shown. There is very little that can be inferred from this matrix, since performance seems to be random. Results Similarity Matrix Comparisons The cosine similarity matrices are shown for our different setups. First, we give as input the characters of the two alphabets separately and extract their first-layer representations. We examined all layer representations, but since the characters are given without context, we decided to go with the first layer, which has been shown to contain contextindependent information (Jawahar et al., 2019). In Figure 1 (a) the cosine similarity matrix for EngF akebase is presented. The diagonal shows that the model correctly aligns English with the base Fake-English language. In Figure 1 (b) we see the similarity matrix of EngF akef1 f2 . The strong diagonal indicates that BERT indeed manages to align English with its Fake-English equivalent. The added perturbation (f1 f2 instead of ‘f’) is correctly captured by the model as well. The English ‘f’ has high similarity scores with both f1 and f2 , with f1 having a slightly higher score. We also compare against the combined f1 f2 bigram, by computi"
2021.insights-1.3,D19-1090,0,0.016769,"abet et al., 2020). The question whether or not BERT can perform character-level alignment, though, has not been answered yet. Even though characters on their own do not necessarily hold much semantic meaning, we investigate whether BERT is able to generate useful representation spaces for characters. Character-level alignment would be useful in tasks like transliteration (Li et al., 2009; Sajjad et al., 2017) or wordlevel alignment (Legrand et al., 2016). In a lot of occurrences of transliterations, grammatical inflections are added or dropped, which causes difficulties in an array of tasks (Czarnowska et al., 2019; Vania and Lopez, 2017). With characterlevel awareness, we can have better models for transliteration detection and extraction tasks. Word alignment could also benefit from character-level information in instances where words get split up within a sentence (eg., separable verbs in German or phrasal verbs in English). With our work we show that even though BERT is not able to align languages on the character level, the closer these languages are the better the alignment. In the trivial case of English to Fake-English alignment, the model successfully learns to align characters. For English to"
2021.insights-1.3,N19-1423,0,0.0341396,"and et al. (2016) worked towards representation-based word alignment, with implementations of aligners proposed in Dyer et al. (2013); Östling and Tiedemann (2016). We also note recent efforts towards unsupervised word alignment. In Zenkel et al. (2019), an extension to the usual Machine Translation encoderdecoder is proposed to jointly learn language translation and word alignment in an unsupervised manner. BERT has also been shown to be able to perform word alignment (Jalili Sabet et al., 2020) through embedding matrix similarities. 3 3.1 3.2 Model Setup We experimented with different BERT (Devlin et al., 2019) model sizes and parameters. In our hyperparameter search, we mainly examined the effect of hidden layer size, layer numbers, embedding space size and attention heads number. We found that when models have fewer than 3 layers or more than 9 layers, we see underfitting and overfitting respectively. In the end, we settled for a 6-layer model with a quarter of the original BERT-base parameters, trained for 50 epochs. An analysis of model size effect on performance is omitted. We are training from scratch on the usual Masked Language Modeling task as described in (Devlin et al., 2019), with the di"
2021.insights-1.3,2005.mtsummit-papers.11,0,0.102327,"‘101’, ‘A’ to ‘126’ etc. All other numbers were removed from both sets. We call this setup EngF akebase . Apart from the base setup, we tried minor alterations. Namely, we tried to break the one-to-one mapping from English to Fake-English. The letter ‘f’ was mapped not to a single, unique integer, but instead two new indices (denoted by f1 and f2 ). So, ‘f’ was mapped to ‘200 201’, which are the unique tokens for f1 and f2 respectively. In the same way, capital ‘F’ was replaced by the tokens for F1 and F2 . We call this setup EngF akef1 f2 . Experimental Setup Data Setup The EuroParl Corpus (Koehn, 2005) is a parallel corpus containing recorded proceedings of the European Parliament. Originally 21 languages were included, although here we examined three: English (ENG), German (DEU) and Greek (ELL).1 Each set is split by words and each word is further split in characters. Finally, special start and end tokens are added around each (split) word. This process results in a data file where each line contains a word split in characters. Finally, these language 1 We follow the ISO 639-3 standard for language codes: https://iso639-3.sil.org/ 17 After the successful EngF ake experiments, we experiment"
2021.insights-1.3,W16-2207,0,0.100605,"lored. Work in this area is picking up and it has been shown that BERT can operate with adequate efficiency in word alignment tasks (Zenkel et al., 2019; Jalili Sabet et al., 2020). The question whether or not BERT can perform character-level alignment, though, has not been answered yet. Even though characters on their own do not necessarily hold much semantic meaning, we investigate whether BERT is able to generate useful representation spaces for characters. Character-level alignment would be useful in tasks like transliteration (Li et al., 2009; Sajjad et al., 2017) or wordlevel alignment (Legrand et al., 2016). In a lot of occurrences of transliterations, grammatical inflections are added or dropped, which causes difficulties in an array of tasks (Czarnowska et al., 2019; Vania and Lopez, 2017). With characterlevel awareness, we can have better models for transliteration detection and extraction tasks. Word alignment could also benefit from character-level information in instances where words get split up within a sentence (eg., separable verbs in German or phrasal verbs in English). With our work we show that even though BERT is not able to align languages on the character level, the closer these"
2021.insights-1.3,2020.emnlp-main.358,1,0.811649,"erman word (split in characters), and so on. For the conversion of English to Fake-English, we employ a mapping of characters to integers. The integers are in the range [100, 151]. The same mapping takes place for the English → German setup, since the two languages share the same script. Data is given as input lineby-line to the model. Research has been conducted to uncover elements of multilinguality in mBERT. In Pires et al. (2019), an analysis of mBERT is presented, while in Conneau and Lample (2019); Wu and Dredze (2019); Artetxe et al. (2020) zero-shot cross-lingual transfer is analyzed. Dufter and Schütze (2020) further analyzes mBERT’s capabilities with BERT’s architecture and the structure of languages examined. The authors performed their experiments on a pairing of English with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based on Recurrent Neural Networks (Luong et al., 2015) for characterlevel task"
2021.insights-1.3,N13-1073,0,0.0183947,"learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based on Recurrent Neural Networks (Luong et al., 2015) for characterlevel tasks such as transliteration and graphemeto-phoneme conversion. Further work to develop character-level BERT-based models is conducted in El Boukkouri et al. (2020); Ma et al. (2020). Och and Ney (2003); Legrand et al. (2016) worked towards representation-based word alignment, with implementations of aligners proposed in Dyer et al. (2013); Östling and Tiedemann (2016). We also note recent efforts towards unsupervised word alignment. In Zenkel et al. (2019), an extension to the usual Machine Translation encoderdecoder is proposed to jointly learn language translation and word alignment in an unsupervised manner. BERT has also been shown to be able to perform word alignment (Jalili Sabet et al., 2020) through embedding matrix similarities. 3 3.1 3.2 Model Setup We experimented with different BERT (Devlin et al., 2019) model sizes and parameters. In our hyperparameter search, we mainly examined the effect of hidden layer size, la"
2021.insights-1.3,D15-1166,0,0.0155354,"sfer is analyzed. Dufter and Schütze (2020) further analyzes mBERT’s capabilities with BERT’s architecture and the structure of languages examined. The authors performed their experiments on a pairing of English with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based on Recurrent Neural Networks (Luong et al., 2015) for characterlevel tasks such as transliteration and graphemeto-phoneme conversion. Further work to develop character-level BERT-based models is conducted in El Boukkouri et al. (2020); Ma et al. (2020). Och and Ney (2003); Legrand et al. (2016) worked towards representation-based word alignment, with implementations of aligners proposed in Dyer et al. (2013); Östling and Tiedemann (2016). We also note recent efforts towards unsupervised word alignment. In Zenkel et al. (2019), an extension to the usual Machine Translation encoderdecoder is proposed to jointly learn language translation and w"
2021.insights-1.3,2020.coling-main.609,0,0.0854244,"Missing"
2021.insights-1.3,2020.coling-main.4,0,0.0383597,"glish with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based on Recurrent Neural Networks (Luong et al., 2015) for characterlevel tasks such as transliteration and graphemeto-phoneme conversion. Further work to develop character-level BERT-based models is conducted in El Boukkouri et al. (2020); Ma et al. (2020). Och and Ney (2003); Legrand et al. (2016) worked towards representation-based word alignment, with implementations of aligners proposed in Dyer et al. (2013); Östling and Tiedemann (2016). We also note recent efforts towards unsupervised word alignment. In Zenkel et al. (2019), an extension to the usual Machine Translation encoderdecoder is proposed to jointly learn language translation and word alignment in an unsupervised manner. BERT has also been shown to be able to perform word alignment (Jalili Sabet et al., 2020) through embedding matrix similarities. 3 3.1 3.2 Model Setup We experime"
2021.insights-1.3,J03-1002,0,0.019978,"glish, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based on Recurrent Neural Networks (Luong et al., 2015) for characterlevel tasks such as transliteration and graphemeto-phoneme conversion. Further work to develop character-level BERT-based models is conducted in El Boukkouri et al. (2020); Ma et al. (2020). Och and Ney (2003); Legrand et al. (2016) worked towards representation-based word alignment, with implementations of aligners proposed in Dyer et al. (2013); Östling and Tiedemann (2016). We also note recent efforts towards unsupervised word alignment. In Zenkel et al. (2019), an extension to the usual Machine Translation encoderdecoder is proposed to jointly learn language translation and word alignment in an unsupervised manner. BERT has also been shown to be able to perform word alignment (Jalili Sabet et al., 2020) through embedding matrix similarities. 3 3.1 3.2 Model Setup We experimented with different"
2021.insights-1.3,P19-1493,0,0.0178941,"rk sets are merged together, alternating between lines. For example, in the English → German setup, the first line contains an English word (split in characters), the second line a German word (split in characters), and so on. For the conversion of English to Fake-English, we employ a mapping of characters to integers. The integers are in the range [100, 151]. The same mapping takes place for the English → German setup, since the two languages share the same script. Data is given as input lineby-line to the model. Research has been conducted to uncover elements of multilinguality in mBERT. In Pires et al. (2019), an analysis of mBERT is presented, while in Conneau and Lample (2019); Wu and Dredze (2019); Artetxe et al. (2020) zero-shot cross-lingual transfer is analyzed. Dufter and Schütze (2020) further analyzes mBERT’s capabilities with BERT’s architecture and the structure of languages examined. The authors performed their experiments on a pairing of English with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown"
2021.insights-1.3,J17-2003,1,0.835565,"capabilities have been lacking and under-explored. Work in this area is picking up and it has been shown that BERT can operate with adequate efficiency in word alignment tasks (Zenkel et al., 2019; Jalili Sabet et al., 2020). The question whether or not BERT can perform character-level alignment, though, has not been answered yet. Even though characters on their own do not necessarily hold much semantic meaning, we investigate whether BERT is able to generate useful representation spaces for characters. Character-level alignment would be useful in tasks like transliteration (Li et al., 2009; Sajjad et al., 2017) or wordlevel alignment (Legrand et al., 2016). In a lot of occurrences of transliterations, grammatical inflections are added or dropped, which causes difficulties in an array of tasks (Czarnowska et al., 2019; Vania and Lopez, 2017). With characterlevel awareness, we can have better models for transliteration detection and extraction tasks. Word alignment could also benefit from character-level information in instances where words get split up within a sentence (eg., separable verbs in German or phrasal verbs in English). With our work we show that even though BERT is not able to align langu"
2021.insights-1.3,P17-1184,0,0.0263849,"uestion whether or not BERT can perform character-level alignment, though, has not been answered yet. Even though characters on their own do not necessarily hold much semantic meaning, we investigate whether BERT is able to generate useful representation spaces for characters. Character-level alignment would be useful in tasks like transliteration (Li et al., 2009; Sajjad et al., 2017) or wordlevel alignment (Legrand et al., 2016). In a lot of occurrences of transliterations, grammatical inflections are added or dropped, which causes difficulties in an array of tasks (Czarnowska et al., 2019; Vania and Lopez, 2017). With characterlevel awareness, we can have better models for transliteration detection and extraction tasks. Word alignment could also benefit from character-level information in instances where words get split up within a sentence (eg., separable verbs in German or phrasal verbs in English). With our work we show that even though BERT is not able to align languages on the character level, the closer these languages are the better the alignment. In the trivial case of English to Fake-English alignment, the model successfully learns to align characters. For English to German performance drops"
2021.insights-1.3,2021.eacl-main.163,0,0.0311673,"f multilinguality in mBERT. In Pires et al. (2019), an analysis of mBERT is presented, while in Conneau and Lample (2019); Wu and Dredze (2019); Artetxe et al. (2020) zero-shot cross-lingual transfer is analyzed. Dufter and Schütze (2020) further analyzes mBERT’s capabilities with BERT’s architecture and the structure of languages examined. The authors performed their experiments on a pairing of English with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequence approaches based on Recurrent Neural Networks (Luong et al., 2015) for characterlevel tasks such as transliteration and graphemeto-phoneme conversion. Further work to develop character-level BERT-based models is conducted in El Boukkouri et al. (2020); Ma et al. (2020). Och and Ney (2003); Legrand et al. (2016) worked towards representation-based word alignment, with implementations of aligners proposed in Dyer et al. (2013); Östling and Tiedemann (2016). We also"
2021.insights-1.3,D19-1077,0,0.018337,"setup, the first line contains an English word (split in characters), the second line a German word (split in characters), and so on. For the conversion of English to Fake-English, we employ a mapping of characters to integers. The integers are in the range [100, 151]. The same mapping takes place for the English → German setup, since the two languages share the same script. Data is given as input lineby-line to the model. Research has been conducted to uncover elements of multilinguality in mBERT. In Pires et al. (2019), an analysis of mBERT is presented, while in Conneau and Lample (2019); Wu and Dredze (2019); Artetxe et al. (2020) zero-shot cross-lingual transfer is analyzed. Dufter and Schütze (2020) further analyzes mBERT’s capabilities with BERT’s architecture and the structure of languages examined. The authors performed their experiments on a pairing of English with Fake-English, as proposed by K et al. (2020) in their rigorous empirical study of mBERT where linguistic properties of languages, architecture and learning objectives are investigated. In Wu et al. (2021); Garg et al. (2019), it is shown that transformers (Vaswani et al., 2017) can achieve similar performance with sequenceto-sequ"
2021.naacl-main.185,N19-1423,0,0.17846,"bined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.1 1 iPET PET GPT-3 70 60 50 102 103 104 105 106 Parameters (Millions) Figure 1: Performance on SuperGLUE with 32 training examples. ALBERT with P ET/iP ET outperforms GPT-3 although it is much “greener” in that it has three orders of magnitude fewer parameters. Introduction Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained"
2021.naacl-main.185,2020.tacl-1.3,0,0.024928,"ed to text classifi3 Pattern-Exploiting Training cation (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and ar- Let M be a masked language model (MLM), T its gumentative relation classification (Opitz, 2019). vocabulary and __ ∈ T the mask token; we denote It is also commonly used for probing the knowl- the set of all token sequences as T ∗ . For some edge contained within LMs (Trinh and Le, 2018; z ∈ T ∗ containing at least k masks and t ∈ T , Petroni et al., 2019; Talmor et al., 2020; Schick and k (t |z) the probability that M we denote with qM Schütze, 2020; Ettinger, 2020, i.a.). assigns to t at the kth masked position in z; the As finding ways to reformulate tasks as cloze model’s logits before applying softmax are denoted questions that are understood well by LMs is diffi- with sk (t |z). We consider the task of mapping M cult (Jiang et al., 2020), Schick and Schütze (2021) inputs x ∈ X to outputs y ∈ Y , for which P ET propose P ET, a method that uses knowledge distil- requires a set of pattern-verbalizer pairs (PVPs). lation (Hinton et al., 2015) and self-training (e.g., Each PVP p = (P, v) consists of Scudder, 1965; Yarowsky, 1995; Brin, 1999; Mc• a patte"
2021.naacl-main.185,D19-1633,0,0.0521812,"Missing"
2021.naacl-main.185,S12-1052,0,0.0148535,"thesis h, we use h? |__, p , “h”? |__, “p” , h? |__. p , “h”? |__. “p” turn requires some modifications during training and inference that are discussed in Appendix A. MultiRC (Khashabi et al., 2018) is a QA task. Given a passage p, a question q and an answer candidate a, the task is to decide whether a is a correct answer for q. We use the same verbalizer as for BoolQ and similar patterns: • p. Question: q ? Is it a? __. and a verbalizer that maps entailment to yes, disagreement to no and neutral to maybe. • p. Question: q ? Is the correct answer “a”? __. Given a premise p, the task in COPA (Gordon et al., 2012) is to determine the cause or effect of the premise given two options c1 and c2 . For determining the effect, we use the following patterns: • “c1 ” or “c2 ”? p, so __. , c1 or c2 ? p, so __. For determining the cause, we use the same patterns but replace so with because. The verbalizer for c1 and c2 is the identity function. For WiC (Pilehvar and Camacho-Collados, 2019), given a word w and two sentences s1 and s2 in which it occurs, the task is to decide if w is used with the same sense in both sentences. We use: • “s1 ” / “s2 ”. Similar sense of “w”? __. • s1 s2 Does w have the same meaning"
2021.naacl-main.185,2020.tacl-1.28,0,0.468749,"we denote It is also commonly used for probing the knowl- the set of all token sequences as T ∗ . For some edge contained within LMs (Trinh and Le, 2018; z ∈ T ∗ containing at least k masks and t ∈ T , Petroni et al., 2019; Talmor et al., 2020; Schick and k (t |z) the probability that M we denote with qM Schütze, 2020; Ettinger, 2020, i.a.). assigns to t at the kth masked position in z; the As finding ways to reformulate tasks as cloze model’s logits before applying softmax are denoted questions that are understood well by LMs is diffi- with sk (t |z). We consider the task of mapping M cult (Jiang et al., 2020), Schick and Schütze (2021) inputs x ∈ X to outputs y ∈ Y , for which P ET propose P ET, a method that uses knowledge distil- requires a set of pattern-verbalizer pairs (PVPs). lation (Hinton et al., 2015) and self-training (e.g., Each PVP p = (P, v) consists of Scudder, 1965; Yarowsky, 1995; Brin, 1999; Mc• a pattern P : X → T ∗ that maps inputs to Closky et al., 2006) to easily combine several recloze questions containing a single mask; formulations. Our modified version of P ET uses masked language models (Devlin et al., 2019) to • a verbalizer v : Y → T that maps each output assign probabi"
2021.naacl-main.185,2020.findings-emnlp.372,0,0.0626072,"entailment: An input x = (x1 , x2 ) is converted into a cloze question P (x); qp (y |x) for each y is derived from the probability of v(y) being a plausible choice for the masked position. and Brown et al. (2020) investigate priming, where examples are given as context but no parameter updates are performed. Finally, our focus on reducing the amount of compute required for few-shot learning is closely related to other efforts in Green AI (Schwartz et al., 2020a) that aim to improve model efficiency, including techniques for knowledge distillation (e.g., Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020; Mao et al., 2020; Anderson and GómezRodríguez, 2020), pruning (Han et al., 2015, 2016; Sanh et al., 2020) and quantization (Gong et al., 2014; Zafrir et al., 2019; Stock et al., 2021) as well as early exit strategies for inference (Liu et al., 2020; Schwartz et al., 2020b; Xin et al., 2020). Enabling LMs to perform zero-shot learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classifi3 Pattern-Exploiting Training cation (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and ar- Let M be a masked language mode"
2021.naacl-main.185,N18-1023,0,0.0158499,"t https://github. cross entropy is not an ideal training objective as it com/timoschick/fewglue. 2342 We define two verbalizers mapping questions containing a true statement to yes/true and others to no/false, respectively, for a total of 6 PVPs. CB (De Marneffe et al., 2019) and RTE (Dagan et al., 2006) are textual entailment tasks like MNLI, so we use PVPs similar to Schick and Schütze (2021). For a premise p and hypothesis h, we use h? |__, p , “h”? |__, “p” , h? |__. p , “h”? |__. “p” turn requires some modifications during training and inference that are discussed in Appendix A. MultiRC (Khashabi et al., 2018) is a QA task. Given a passage p, a question q and an answer candidate a, the task is to decide whether a is a correct answer for q. We use the same verbalizer as for BoolQ and similar patterns: • p. Question: q ? Is it a? __. and a verbalizer that maps entailment to yes, disagreement to no and neutral to maybe. • p. Question: q ? Is the correct answer “a”? __. Given a premise p, the task in COPA (Gordon et al., 2012) is to determine the cause or effect of the premise given two options c1 and c2 . For determining the effect, we use the following patterns: • “c1 ” or “c2 ”? p, so __. , c1 or c2"
2021.naacl-main.185,2020.acl-main.703,0,0.0452948,"re holding the entire ensemble of MLMs in memory at the same time as each model’s predictions can be computed sequentially; therefore, it is not more memory expensive than using a single model. To give MLMs trained on different patterns further opportunity to learn from one another, Schick and Schütze (2021) also propose iP ET, an iterative variant of P ET in which several generations of models are trained on datasets of increasing size that are labeled by previous generations. This is 3 While P ET can easily be adapted to generative MLMs achieved as follows: First, an ensemble of MLMs (e.g., Lewis et al., 2020; Raffel et al., 2020), we stick with is trained as in regular P ET. For each model Mi , a regular MLMs as they are more lightweight and performed better on simple cloze tasks in preliminary experiments. random subset of other models is used to generate 2341 Inference For x ∈ X, y ∈ Yx and |v(y) |= k, we redefine qp (y |x) in an autoregressive fashion: Starting from P k (x), we perform k consecutive predictions, where we always select the next token to predict based on the MLM’s confidence. That is, we set qp (y |x) = q(v(y) |P k (x)) where ( 1 if k = 0 q(t1 ... tk |z) = (3) j 0 0 qM (tj |z) ·"
2021.naacl-main.185,2020.acl-main.537,0,0.0186339,"e given as context but no parameter updates are performed. Finally, our focus on reducing the amount of compute required for few-shot learning is closely related to other efforts in Green AI (Schwartz et al., 2020a) that aim to improve model efficiency, including techniques for knowledge distillation (e.g., Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020; Mao et al., 2020; Anderson and GómezRodríguez, 2020), pruning (Han et al., 2015, 2016; Sanh et al., 2020) and quantization (Gong et al., 2014; Zafrir et al., 2019; Stock et al., 2021) as well as early exit strategies for inference (Liu et al., 2020; Schwartz et al., 2020b; Xin et al., 2020). Enabling LMs to perform zero-shot learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classifi3 Pattern-Exploiting Training cation (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and ar- Let M be a masked language model (MLM), T its gumentative relation classification (Opitz, 2019). vocabulary and __ ∈ T the mask token; we denote It is also commonly used for probing the knowl- the set of all token sequences as T ∗ . For some edge contained within LMs (Trinh and Le"
2021.naacl-main.185,2020.coling-main.287,0,0.042773,"t x = (x1 , x2 ) is converted into a cloze question P (x); qp (y |x) for each y is derived from the probability of v(y) being a plausible choice for the masked position. and Brown et al. (2020) investigate priming, where examples are given as context but no parameter updates are performed. Finally, our focus on reducing the amount of compute required for few-shot learning is closely related to other efforts in Green AI (Schwartz et al., 2020a) that aim to improve model efficiency, including techniques for knowledge distillation (e.g., Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020; Mao et al., 2020; Anderson and GómezRodríguez, 2020), pruning (Han et al., 2015, 2016; Sanh et al., 2020) and quantization (Gong et al., 2014; Zafrir et al., 2019; Stock et al., 2021) as well as early exit strategies for inference (Liu et al., 2020; Schwartz et al., 2020b; Xin et al., 2020). Enabling LMs to perform zero-shot learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classifi3 Pattern-Exploiting Training cation (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and ar- Let M be a masked language model (MLM), T its gum"
2021.naacl-main.185,N06-1020,0,0.0798349,"Missing"
2021.naacl-main.185,D19-1250,0,0.0348742,"s to perform zero-shot learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classifi3 Pattern-Exploiting Training cation (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and ar- Let M be a masked language model (MLM), T its gumentative relation classification (Opitz, 2019). vocabulary and __ ∈ T the mask token; we denote It is also commonly used for probing the knowl- the set of all token sequences as T ∗ . For some edge contained within LMs (Trinh and Le, 2018; z ∈ T ∗ containing at least k masks and t ∈ T , Petroni et al., 2019; Talmor et al., 2020; Schick and k (t |z) the probability that M we denote with qM Schütze, 2020; Ettinger, 2020, i.a.). assigns to t at the kth masked position in z; the As finding ways to reformulate tasks as cloze model’s logits before applying softmax are denoted questions that are understood well by LMs is diffi- with sk (t |z). We consider the task of mapping M cult (Jiang et al., 2020), Schick and Schütze (2021) inputs x ∈ X to outputs y ∈ Y , for which P ET propose P ET, a method that uses knowledge distil- requires a set of pattern-verbalizer pairs (PVPs). lation (Hinton et al., 2015"
2021.naacl-main.185,N19-1128,0,0.0246758,"Missing"
2021.naacl-main.185,2020.acl-main.240,0,0.020942,"aining (e.g., Each PVP p = (P, v) consists of Scudder, 1965; Yarowsky, 1995; Brin, 1999; Mc• a pattern P : X → T ∗ that maps inputs to Closky et al., 2006) to easily combine several recloze questions containing a single mask; formulations. Our modified version of P ET uses masked language models (Devlin et al., 2019) to • a verbalizer v : Y → T that maps each output assign probabilities to sequences of text; this is simto a single token representing its task-specific ilar to using them in a generative fashion (Wang meaning in the pattern. and Cho, 2019) and has previously been investigated by Salazar et al. (2020) and GhazvinineAs illustrated in Figure 2, the core idea of P ET jad et al. (2019). In contrast to P ET, which uses is to derive the probability of y being the correct gradient-based optimization, Radford et al. (2019) output for x from the probability of v(y) being 2340 the “correct” token at the masked position in P (x). Based on this intuition, a conditional probability distribution qp of y given x is defined as exp sp (y |x) qp (y |x) = P 0 y 0 ∈Y exp sp (y |x) (a) z = Awful pizza! It was __ __ . x P 2 (x) 1 (terri |z) &lt; q 2 (•ble |z) qM M (1) (b) z0 = s1M (v(y) where sp (y |x) = |P (x)) i"
2021.naacl-main.185,2020.acl-main.593,0,0.0170806,"ces rise ? __ , Oil prices fall back . x2 x1 P (x) entailment Yes not_entailment No y v(y) Figure 2: Application of a PVP p = (P, v) for recognizing textual entailment: An input x = (x1 , x2 ) is converted into a cloze question P (x); qp (y |x) for each y is derived from the probability of v(y) being a plausible choice for the masked position. and Brown et al. (2020) investigate priming, where examples are given as context but no parameter updates are performed. Finally, our focus on reducing the amount of compute required for few-shot learning is closely related to other efforts in Green AI (Schwartz et al., 2020a) that aim to improve model efficiency, including techniques for knowledge distillation (e.g., Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020; Mao et al., 2020; Anderson and GómezRodríguez, 2020), pruning (Han et al., 2015, 2016; Sanh et al., 2020) and quantization (Gong et al., 2014; Zafrir et al., 2019; Stock et al., 2021) as well as early exit strategies for inference (Liu et al., 2020; Schwartz et al., 2020b; Xin et al., 2020). Enabling LMs to perform zero-shot learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classifi3 P"
2021.naacl-main.185,P19-1355,0,0.0346333,"LM problems, 1 Our implementation is publicly available at https:// github.com/timoschick/pet. GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed. While being straightforward to use, this method has two major drawbacks: • It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). • It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens.2 An alternative to priming is pattern-exploiting training (P ET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While P ET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled 2 While GPT-3 can process up to 2,048 tokens, this is still not enough to fit ≥32 examples for some SuperGLUE tasks. 2339 Proceedings of the 2021 Conference of the North American"
2021.naacl-main.185,2020.tacl-1.48,0,0.0468707,"learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classifi3 Pattern-Exploiting Training cation (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and ar- Let M be a masked language model (MLM), T its gumentative relation classification (Opitz, 2019). vocabulary and __ ∈ T the mask token; we denote It is also commonly used for probing the knowl- the set of all token sequences as T ∗ . For some edge contained within LMs (Trinh and Le, 2018; z ∈ T ∗ containing at least k masks and t ∈ T , Petroni et al., 2019; Talmor et al., 2020; Schick and k (t |z) the probability that M we denote with qM Schütze, 2020; Ettinger, 2020, i.a.). assigns to t at the kth masked position in z; the As finding ways to reformulate tasks as cloze model’s logits before applying softmax are denoted questions that are understood well by LMs is diffi- with sk (t |z). We consider the task of mapping M cult (Jiang et al., 2020), Schick and Schütze (2021) inputs x ∈ X to outputs y ∈ Y , for which P ET propose P ET, a method that uses knowledge distil- requires a set of pattern-verbalizer pairs (PVPs). lation (Hinton et al., 2015) and self-training ("
2021.naacl-main.185,W19-2304,0,0.0611103,"Missing"
2021.naacl-main.185,2020.sustainlp-1.11,0,0.0809775,"s are performed. Finally, our focus on reducing the amount of compute required for few-shot learning is closely related to other efforts in Green AI (Schwartz et al., 2020a) that aim to improve model efficiency, including techniques for knowledge distillation (e.g., Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020; Mao et al., 2020; Anderson and GómezRodríguez, 2020), pruning (Han et al., 2015, 2016; Sanh et al., 2020) and quantization (Gong et al., 2014; Zafrir et al., 2019; Stock et al., 2021) as well as early exit strategies for inference (Liu et al., 2020; Schwartz et al., 2020b; Xin et al., 2020). Enabling LMs to perform zero-shot learning by providing task descriptions was proposed by Radford et al. (2019) and has been applied to text classifi3 Pattern-Exploiting Training cation (Puri and Catanzaro, 2019), commonsense knowledge mining (Davison et al., 2019) and ar- Let M be a masked language model (MLM), T its gumentative relation classification (Opitz, 2019). vocabulary and __ ∈ T the mask token; we denote It is also commonly used for probing the knowl- the set of all token sequences as T ∗ . For some edge contained within LMs (Trinh and Le, 2018; z ∈ T ∗ containing at least k masks"
2021.naacl-main.185,P95-1026,0,0.505691,"e with qM Schütze, 2020; Ettinger, 2020, i.a.). assigns to t at the kth masked position in z; the As finding ways to reformulate tasks as cloze model’s logits before applying softmax are denoted questions that are understood well by LMs is diffi- with sk (t |z). We consider the task of mapping M cult (Jiang et al., 2020), Schick and Schütze (2021) inputs x ∈ X to outputs y ∈ Y , for which P ET propose P ET, a method that uses knowledge distil- requires a set of pattern-verbalizer pairs (PVPs). lation (Hinton et al., 2015) and self-training (e.g., Each PVP p = (P, v) consists of Scudder, 1965; Yarowsky, 1995; Brin, 1999; Mc• a pattern P : X → T ∗ that maps inputs to Closky et al., 2006) to easily combine several recloze questions containing a single mask; formulations. Our modified version of P ET uses masked language models (Devlin et al., 2019) to • a verbalizer v : Y → T that maps each output assign probabilities to sequences of text; this is simto a single token representing its task-specific ilar to using them in a generative fashion (Wang meaning in the pattern. and Cho, 2019) and has previously been investigated by Salazar et al. (2020) and GhazvinineAs illustrated in Figure 2, the core id"
2021.naacl-main.186,Q17-1010,0,0.0504617,"by including multiple [MASK] tokens in the templates. We then compute an object’s score as the average of the log probabilities for its individual tokens. Note that we do not perform any finetuning. 3.2 Vocabulary cheap for static embeddings. We thus experiment with different vocabulary sizes for static embeddings. To this end, we train new vocabularies for each language on Wikipedia using the wordpiece tokenizer (Schuster and Nakajima, 2012). 3.3 Static Embeddings Using either newly trained vocabularies or existing BERT vocabularies, we tokenize Wikipedia. We then train fastText embeddings (Bojanowski et al., 2017) with default parameters (http://fasttext.cc). We consider the same candidate set C as for PLMs. Let c ∈ C be a candidate that gets split into tokens t1 , . . . , tk by the wordpiece tokenizer. We then assign to c the embedding vector k 1X e¯c = eti k i=1 where eti is the fastText vector for token ti . We compute the representations for a query q analogously. For a query q (the subject of a triple), we then compute the prediction as: arg max cosine-sim(¯ eq , e¯c ), c∈C i.e., we perform simple nearest neighbor matching. Note that the static embedding method does not get any signal about the re"
2021.naacl-main.186,P19-1470,0,0.0318622,"Missing"
2021.naacl-main.186,N18-1202,0,0.0604255,"Missing"
2021.naacl-main.186,D19-1005,0,0.0615374,"Missing"
2021.naacl-main.186,D19-1250,0,0.462406,"st, BERT exploits its more sophisticated, but expensive ability to compose meaningful representations from a much smaller subword vocabulary. 1 Oracle Pretrained language models (PLMs) (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) can be finetuned to a variety of natural language processing (NLP) tasks and then generally yield high performance. Increasingly, these models and their generative variants (e.g., GPT, Brown et al., 2020) are used to solve tasks by simple text generation, without any finetuning. This motivated research on how much knowledge is contained in PLMs: Petroni et al. (2019) used models pretrained with a masked language objective to answer clozestyle templates such as: (Ex1) Paris is the capital of [MASK]. Using this methodology, Petroni et al. (2019) showed that PLMs capture some knowledge implicitly. This has been interpreted as suggesting Equal contribution - random order. LAMA p1 LAMA-UHN 22.0 23.7 BERT mBERT 30k 110k 39.6 36.3 30.7 27.4 fastText BERT-30k mBERT-110k 30k 120k 250k 500k 1000k 26.9 27.5 16.4 34.3 37.7 39.9 41.2 16.8 17.8 5.8 25.0 29.0 31.8 33.4 Table 1: Results for majority oracle, BERT, mBERT and fastText. Static fastText embeddings are competi"
2021.naacl-main.186,P19-1355,0,0.0125038,".0 31.6 31.8 30.5 26.0 22.1 33.2 35.6 36.1 36.6 Table 3: p1 for mBERT and fastText on mLAMA. fastText clearly outperforms mBERT for large vocabularies. Numbers across languages are not comparable as the number of triples varies. Model Power (W) BERT fastText-en ratio-en 12,041 618 h kWh · PUE CO2 e 79 5 1,507 1,438 5 5 0.05 0.06 0.003 0.003 Table 4: Power consumption (Power), hours of computation (h), energy consumption (kWh · PUE) and carbon emissions (CO2 e) of BERT vs. fastText. Training embeddings for all languages takes around 4 times the resources as training English. BERT numbers from (Strubell et al., 2019). We use our server’s peak power consumption. See appendix for details. 2021) and use typed querying: for each relation, we create a candidate set C and then predict arg maxc∈C p(c|t). For most templates, there is only one valid entity type, e.g., country for (Ex1). We choose as C the set of objects across all triples for a single relation. The candidate set could also be obtained from an entity typing system (e.g., Yaghoobzadeh et al., 2018), but this is beyond the scope of this paper. Variants of typed prediction have been used before (Xiong et al., 2020). We accommodate multi-token objects,"
2021.naacl-main.186,P19-1139,0,0.0513463,"ner et al., ple of a general problem: “green” baselines are 2020), finding alternatives to a cloze-style approach often ignored, but should be considered when eval(Bouraoui et al., 2020; Heinzerling and Inui, 2020; uating resource-hungry deep learning models. A Jiang et al., 2020) or analyzing different model promising way forward would be to combine the sizes (Roberts et al., 2020). best of both worlds, e.g., by building on work that There is a recent surge of work that tries to im- incorporates large vocabularies into PLMs after prove PLMs’ ability to harvest factual knowledge: pretraining. Zhang et al. (2019), Peters et al. (2019) and Wang Acknowledgements. This work was supported et al. (2020) inject factual knowledge into PLMs. by the European Research Council (# 740516) and Guu et al. (2020), Lewis et al. (2020), Izacard and the German Federal Ministry of Education and ReGrave (2020), Kassner and Schütze (2020) and search (BMBF) under Grant No. 01IS18036A. The Petroni et al. (2020) combine PLMs with informa- authors of this work take full responsibility for its tion retrieval and Bosselut et al. (2019), Liu et al. content. The first author was supported by the (2020) and Yu et al. (2020) with k"
2021.naacl-main.186,2020.findings-emnlp.71,1,0.63844,"sahar et al., 2018). TREx consists of triples of the form (object, relation, subject). The underlying idea of LAMA is to query knowledge from PLMs using templates without any finetuning: the triple (Paris, capital-of, France) is queried with the template “Paris is the capital of [MASK].” TREx covers 41 relations. Templates for each relation were manually created by Petroni et al. (2019). LAMA has been found to contain many “easy-toguess” triples; e.g., it is easy to guess that a person with an Italian sounding name is Italian. LAMAUHN is a subset of triples that are “hard-to-guess” created by Poerner et al. (2020). Beyond English, we run experiments on nine additional languages using mLAMA, a multilingual version of TREx (Kassner et al., 2021). For an overview of languages and language families see Table 2. For training static embeddings, we use Wikipedia dumps from October 2020. 3 Methods We describe our proposed setup, which allows to compare PLMs with static embeddings. 3.1 PLMs We use the following two PLMs: (i) BERT for English (BERT-base-cased, Devlin et al. (2019)), (ii) mBERT for all ten languages (the multilingual version BERT-base-multilingual-cased). Petroni et al. (2019) use templates like"
2021.naacl-main.186,2020.emnlp-main.437,0,0.461893,"018). TREx consists of triples of the form (object, relation, subject). The underlying idea of LAMA is to query knowledge from PLMs using templates without any finetuning: the triple (Paris, capital-of, France) is queried with the template “Paris is the capital of [MASK].” TREx covers 41 relations. Templates for each relation were manually created by Petroni et al. (2019). LAMA has been found to contain many “easy-toguess” triples; e.g., it is easy to guess that a person with an Italian sounding name is Italian. LAMAUHN is a subset of triples that are “hard-to-guess” created by Poerner et al. (2020). Beyond English, we run experiments on nine additional languages using mLAMA, a multilingual version of TREx (Kassner et al., 2021). For an overview of languages and language families see Table 2. For training static embeddings, we use Wikipedia dumps from October 2020. 3 Methods We describe our proposed setup, which allows to compare PLMs with static embeddings. 3.1 PLMs We use the following two PLMs: (i) BERT for English (BERT-base-cased, Devlin et al. (2019)), (ii) mBERT for all ten languages (the multilingual version BERT-base-multilingual-cased). Petroni et al. (2019) use templates like"
2021.naacl-main.332,Q17-1010,0,0.109785,"Missing"
2021.naacl-main.332,P15-1077,0,0.0322207,"e, xp, windows, software apple, talk, computers, shares, disease, driver, electronics, profit, ios Z2 (S 2 ) Z3 (S 3 ) Z4 (T ) Topic Label Trading Product Line Operating System ? Table 1: Coherent (Z1 -Z3 ) vs Incoherent (Z4 ) topics from high-resource (S 1 -S 3 ) and low-resource (T ) texts modeling (TM) still remains challenging especially in the sparse-data setting, especially for the cases where word co-occurrence data is insufficient, e.g., on short-text or a corpus of few documents. It leads to a poor quality of topics and representations. To address data sparsity issues, several works (Das et al., 2015; Nguyen et al., 2015; Gupta et al., 2019a, 2020) have introduced external knowledge in traditional topic models, e.g., incorporating word embeddings obtained from Glove (Pennington et al., 2014) or word2vec (Mikolov et al., 2013a). However, no prior work in topic modeling has employed multi-view embedding spaces: (1) pretrained topics, i.e., topical embeddings obtained from large document collections, and (2) pretrained contextualized word embeddings from large-scale language models like BERT (Devlin et al., 2019). 1 Introduction Though topics and word embeddings are comProbabilistic topic mo"
2021.naacl-main.332,N19-1423,0,0.427922,"lity of topics and representations. To address data sparsity issues, several works (Das et al., 2015; Nguyen et al., 2015; Gupta et al., 2019a, 2020) have introduced external knowledge in traditional topic models, e.g., incorporating word embeddings obtained from Glove (Pennington et al., 2014) or word2vec (Mikolov et al., 2013a). However, no prior work in topic modeling has employed multi-view embedding spaces: (1) pretrained topics, i.e., topical embeddings obtained from large document collections, and (2) pretrained contextualized word embeddings from large-scale language models like BERT (Devlin et al., 2019). 1 Introduction Though topics and word embeddings are comProbabilistic topic models, such as LDA (Blei et al., plementary in how they represent the meaning, they 2003), Replicated Softmax (RSM) (Salakhutdiare distinctive in how they learn from word occurnov and Hinton, 2009) and Document Neural Aurences observed in text corpora. A topic model toregressive Distribution Estimator (DocNADE) (Blei et al., 2003) is a statistical tool to infers topic (Larochelle and Lauly, 2012) are often used to distributions across a collection of documents and extract topics from text collections and learn laass"
2021.naacl-main.332,2021.ccl-1.108,0,0.0323651,"Missing"
2021.naacl-main.332,Q15-1022,0,0.0234183,"ftware apple, talk, computers, shares, disease, driver, electronics, profit, ios Z2 (S 2 ) Z3 (S 3 ) Z4 (T ) Topic Label Trading Product Line Operating System ? Table 1: Coherent (Z1 -Z3 ) vs Incoherent (Z4 ) topics from high-resource (S 1 -S 3 ) and low-resource (T ) texts modeling (TM) still remains challenging especially in the sparse-data setting, especially for the cases where word co-occurrence data is insufficient, e.g., on short-text or a corpus of few documents. It leads to a poor quality of topics and representations. To address data sparsity issues, several works (Das et al., 2015; Nguyen et al., 2015; Gupta et al., 2019a, 2020) have introduced external knowledge in traditional topic models, e.g., incorporating word embeddings obtained from Glove (Pennington et al., 2014) or word2vec (Mikolov et al., 2013a). However, no prior work in topic modeling has employed multi-view embedding spaces: (1) pretrained topics, i.e., topical embeddings obtained from large document collections, and (2) pretrained contextualized word embeddings from large-scale language models like BERT (Devlin et al., 2019). 1 Introduction Though topics and word embeddings are comProbabilistic topic models, such as LDA (Bl"
2021.naacl-main.332,D14-1162,0,0.0979962,"Coherent (Z1 -Z3 ) vs Incoherent (Z4 ) topics from high-resource (S 1 -S 3 ) and low-resource (T ) texts modeling (TM) still remains challenging especially in the sparse-data setting, especially for the cases where word co-occurrence data is insufficient, e.g., on short-text or a corpus of few documents. It leads to a poor quality of topics and representations. To address data sparsity issues, several works (Das et al., 2015; Nguyen et al., 2015; Gupta et al., 2019a, 2020) have introduced external knowledge in traditional topic models, e.g., incorporating word embeddings obtained from Glove (Pennington et al., 2014) or word2vec (Mikolov et al., 2013a). However, no prior work in topic modeling has employed multi-view embedding spaces: (1) pretrained topics, i.e., topical embeddings obtained from large document collections, and (2) pretrained contextualized word embeddings from large-scale language models like BERT (Devlin et al., 2019). 1 Introduction Though topics and word embeddings are comProbabilistic topic models, such as LDA (Blei et al., plementary in how they represent the meaning, they 2003), Replicated Softmax (RSM) (Salakhutdiare distinctive in how they learn from word occurnov and Hinton, 2009"
2021.naacl-main.332,N18-1202,0,0.0276137,"g large text corpora, the topic hidden in document collection. On other hand, * : equal contribution word embeddings have primarily local view in the 4205 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4205–4217 June 6–11, 2021. ©2021 Association for Computational Linguistics sense that they are learned based on local collocation pattern in a text corpus, where the representation of each word often depends on a local context window (Mikolov et al., 2013b) or is a function of its sentence(s) (Peters et al., 2018). Consequently, they are not aware of the thematic structures underlying the document collection. Additionally, recent studies (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019) have shown a reasonable success in several NLP applications by employing pretrained contextualized word embeddings, where the representation of a word is different in different contexts (i.e., context-sensitive). In context of this work, the representations due to global and local (contextsensitive or context-insensitive) views together are referred as multi-view embeddings. For example in Table 1, consider f"
2021.nlp4convai-1.20,N18-3011,0,0.0526414,"Missing"
2021.nlp4convai-1.20,2021.acl-long.342,0,0.0150715,"tween entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge (Bonial et al., 2020; Bai et al., 2021) or can be the result of a database query for conversational QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures (Vaswani et al., 2017) have considerably outperformed prior state of the art in various downstream tasks (Devlin et al., 2019; Yang et al., 2019a; Liu et al., 2020; Radford et al., 2019). In this paper, we analyze the applicability of two recen"
2021.nlp4convai-1.20,W13-2322,0,0.0364976,"more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs’ success on graph-totext tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.1 1 Introduction Graphs are important data structures in NLP as they represent complex relations within a set of objects. For example, semantic and syntactic structures of sentences can be represented using different graph representations (e.g., AMRs, Banarescu et al., 2013; semantic-role labeling, Surdeanu et al., 2008; syntactic and semantic graphs, Belz et al., 2011) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017)."
2021.nlp4convai-1.20,P18-1026,0,0.0725876,"possible reasons for such a good performance. 2 Related Work Graph-to-text Learning. Various neural models have been proposed to generate sentences from graphs from different domains. Konstas et al. (2017) propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure, whereas other efforts (Ribeiro et al., 2020; Schmitt et al., 2020; Yao et al., 2020; Wang et al., 2020) inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation. Pretrained Language Models. Pretrained Transformer-based models, such as BERT (Devl"
2021.nlp4convai-1.20,W11-2832,0,0.0408327,"e PLMs’ success on graph-totext tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.1 1 Introduction Graphs are important data structures in NLP as they represent complex relations within a set of objects. For example, semantic and syntactic structures of sentences can be represented using different graph representations (e.g., AMRs, Banarescu et al., 2013; semantic-role labeling, Surdeanu et al., 2008; syntactic and semantic graphs, Belz et al., 2011) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store t"
2021.nlp4convai-1.20,2020.lrec-1.86,0,0.01309,"form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge (Bonial et al., 2020; Bai et al., 2021) or can be the result of a database query for conversational QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures (Vaswani et al., 2017) have considerably outperformed prior state of the art in various downstream tasks (Devlin et al., 2019; Yang et al., 2019a; Liu et al., 2020; Radford et al., 2019). In this paper, we analyze the applica"
2021.nlp4convai-1.20,P19-1470,0,0.041308,"Missing"
2021.nlp4convai-1.20,2020.acl-main.119,0,0.021947,"ditional Task-specific Data In order to evaluate the proposed task-adaptive pretraining strategies for graph-to-text generation, we collect task-specific data for two graph domains: meaning representations (like AMR) and scientific data (like AGENDA). We did not attempt collecting additional data like WebNLG because the texts in this benchmark do not stem from a corpus but were specifically written by annotators. AMR Silver Data. In order to generate additional data for AMR, we sample two sentence collections of size 200K and 2M from the Gigaword5 corpus and use a state-of-the-art AMR parser (Cai and Lam, 2020a) to parse them into AMR graphs.6 For supervised pretraining, we condition a model on the AMR silver graphs to generate the corresponding sentences before fine-tuning it on gold AMR graphs. For self-supervised pretraining, we only use the sentences.7 4 BLEU 27.87 31.82 32.46 33.90 34.10 Details of the preprocessing procedure of AMRs are provided in Appendix A. 5 https://catalog.ldc.upenn.edu/LDC2003T05 6 We filter out sentences that do not yield well-formed AMR graphs. 7 Gigaword and AMR datasets share similar data sources. Semantic Scholar AI Data. We collect titles and abstracts of around 1"
2021.nlp4convai-1.20,2020.acl-main.740,0,0.0449382,"Missing"
2021.nlp4convai-1.20,2020.coling-main.218,0,0.0564303,"Missing"
2021.nlp4convai-1.20,P17-1014,0,0.0962897,"ly better fluency than existing works and the human references. • We discover that PLMs perform well even when trained on a shuffled linearized graph representation without any information about connectivity (bag of node and edge labels), which is surprising since prior studies showed that explicitly encoding the graph structure improves models trained from scratch (e.g., Zhao et al., 2020a); and investigate the possible reasons for such a good performance. 2 Related Work Graph-to-text Learning. Various neural models have been proposed to generate sentences from graphs from different domains. Konstas et al. (2017) propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure,"
2021.nlp4convai-1.20,P02-1040,0,0.111203,"77.57 78.46 78.40 78.29 62.76 66.53 67.69 70.92 72.25 Table 2: Results on WebNLG. A, S and U stand for all, seen, and unseen partitions of the test set, respectively. models for AMR. Following Wolf et al. (2019), we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 3 · 10−5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and chrF++ (Popovi´c, 2015) metrics. We also use MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. §5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. 5.1 Results on AMR-to-Text other metrics follow similar trends. See Table 13 in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in"
2021.nlp4convai-1.20,2020.acl-main.703,0,0.21309,"onal QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures (Vaswani et al., 2017) have considerably outperformed prior state of the art in various downstream tasks (Devlin et al., 2019; Yang et al., 2019a; Liu et al., 2020; Radford et al., 2019). In this paper, we analyze the applicability of two recent text-to-text pretrained language models (PLMs), BART (Lewis et al., 2020) and T5 (Raffel et al., 2019), for graph-to-text generation. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin. While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation (Song et al."
2021.nlp4convai-1.20,D19-1250,0,0.0604649,"Missing"
2021.nlp4convai-1.20,W15-3049,0,0.0449003,"Missing"
2021.nlp4convai-1.20,D18-1360,0,0.0246621,"Appendix A. 5 https://catalog.ldc.upenn.edu/LDC2003T05 6 We filter out sentences that do not yield well-formed AMR graphs. 7 Gigaword and AMR datasets share similar data sources. Semantic Scholar AI Data. We collect titles and abstracts of around 190K scientific papers from the Semantic Scholar (Ammar et al., 2018) taken from the proceedings of 36 top Computer Science/AI conferences. We construct KGs from the paper abstracts employing DyGIE++ (Wadden et al., 2019), an information extraction system for scientific texts. Note that the AGENDA dataset was constructed using the older SciIE system (Luan et al., 2018), which also extracts KGs from AI scientific papers. A second difference is that in our new dataset, the domain is broader as we collected data from 36 conferences compared to 12 from AGENDA. Furthermore, to prevent data leakage, all AGENDA samples used for performance evaluation are removed from our dataset. We will call the new dataset KGAIA (KGs from AI Abstracts).8 Table 11 in Appendix shows relevant dataset statistics. 5 Experiments We modify the BART and T5 implementations released by Hugging Face (Wolf et al., 2019) in order to adapt them to graph-to-text generation. For the KG datasets"
2021.nlp4convai-1.20,W18-6501,0,0.0308433,"Missing"
2021.nlp4convai-1.20,P19-1081,0,0.026448,"erent graph representations (e.g., AMRs, Banarescu et al., 2013; semantic-role labeling, Surdeanu et al., 2008; syntactic and semantic graphs, Belz et al., 2011) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge (Bonial et al., 2020; Bai et al., 2021) or can be the result of a database query for conversational QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based arch"
2021.nlp4convai-1.20,2021.textgraphs-1.2,1,0.789375,"Various neural models have been proposed to generate sentences from graphs from different domains. Konstas et al. (2017) propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure, whereas other efforts (Ribeiro et al., 2020; Schmitt et al., 2020; Yao et al., 2020; Wang et al., 2020) inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation. Pretrained Language Models. Pretrained Transformer-based models, such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), or RoBERTa (Liu et al., 2020), have establi"
2021.nlp4convai-1.20,2020.acl-main.704,0,0.0228128,"llowing Wolf et al. (2019), we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 3 · 10−5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and chrF++ (Popovi´c, 2015) metrics. We also use MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. §5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. 5.1 Results on AMR-to-Text other metrics follow similar trends. See Table 13 in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in the AMR dataset suggests that PLMs can infer the AMR structure by a simple linear sequence of the graph, in contrast to GNN-based models that explicitly consider the graph structure"
2021.nlp4convai-1.20,P18-1150,0,0.0752798,"al., 2020) and T5 (Raffel et al., 2019), for graph-to-text generation. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin. While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation (Song et al., 2018; Ribeiro et al., 2019, 2020; Schmitt et al., 2020; Zhao et al., 2020a, to name a few), our approaches based on PLMs 1 consistently outperform these models, even though Our code is available at https://github.com/UKPLab/plmsgraph2text. PLMs – as sequence models – do not exhibit any 211 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 211–227 November 10, 2021. ©2021 Association for Computational Linguistics a) ARG0 ARG1 terrible-01 degree r he we u ck ba Alan ber Bean Mem de an ARG1 time crew Alfred Worden now tion upa occ Test Pilot m ARG2 Nasa fee"
2021.nlp4convai-1.20,2020.acl-main.640,0,0.0208796,"es a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure, whereas other efforts (Ribeiro et al., 2020; Schmitt et al., 2020; Yao et al., 2020; Wang et al., 2020) inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation. Pretrained Language Models. Pretrained Transformer-based models, such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), or RoBERTa (Liu et al., 2020), have established a qualitatively new level of baseline performance for many widely used natural language understanding (NLU) benchmarks. Generative pretrained 2 The model architecture does not exp"
2021.nlp4convai-1.20,2020.acl-main.224,0,0.364129,"n. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin. While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation (Song et al., 2018; Ribeiro et al., 2019, 2020; Schmitt et al., 2020; Zhao et al., 2020a, to name a few), our approaches based on PLMs 1 consistently outperform these models, even though Our code is available at https://github.com/UKPLab/plmsgraph2text. PLMs – as sequence models – do not exhibit any 211 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 211–227 November 10, 2021. ©2021 Association for Computational Linguistics a) ARG0 ARG1 terrible-01 degree r he we u ck ba Alan ber Bean Mem de an ARG1 time crew Alfred Worden now tion upa occ Test Pilot m ARG2 Nasa feel-01 ARG0 t lo i pP m ARG1 have-rel-role-91 child tor opera co ARG0 A"
2021.nlp4convai-1.20,D19-1053,0,0.0238115,"een partitions of the test set, respectively. models for AMR. Following Wolf et al. (2019), we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 3 · 10−5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and chrF++ (Popovi´c, 2015) metrics. We also use MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. §5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. 5.1 Results on AMR-to-Text other metrics follow similar trends. See Table 13 in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in the AMR dataset suggests that PLMs can infer the AMR structure by a simple linear sequence of the graph, in contrast"
2021.nlp4if-1.1,2020.acl-main.164,0,0.0801319,"Missing"
2021.nlp4if-1.1,2020.coling-main.208,0,0.0625523,"Missing"
2021.nlp4if-1.1,N19-1423,0,0.0612986,"Missing"
2021.nlp4if-1.1,P11-1032,0,0.0972169,"Missing"
2021.nlp4if-1.1,2020.emnlp-demos.25,0,0.0696521,"Missing"
2021.nlp4if-1.1,C18-1287,0,0.0479867,"Missing"
2021.nlp4if-1.1,P19-3019,0,0.0927491,"are indeed able to capture patterns in the original data. Even though the number of words in the generated headlines is bound by the maximum number of words learned in the corresponding language model, the distribution of words is similar across real and generated headlines. In Figures 1 and 2 we indicatively show the 15 most frequent words in the real and generated headlines respectively. POS tag frequencies are shown in Table 1 for the top tags in each set. In real headlines, nouns are used more often, whereas in generated headlines the distribution is smoother, consistent with findings in Gehrmann et al. (2019). Furthermore, in generated headlines verbs appear more often in their base (VB) and third-person singular (VBZ) form while in real headlines verb tags are more uniformly distributed. Overall, GPT-2 has accurately learned the real distribution, with similarities across the board. Dataset 3.1 Dataset Development The dataset was created using Australian Broadcasting Corporation headlines and headlines generated from a model. A pretrained5 GPT-2 model (Radford et al., 2019) was finetuned on the headlines data. Text was generated using sampling with tem5 Figure 1: Top 15 Words for real headlines L"
2021.textgraphs-1.2,P18-1081,0,0.046667,"Missing"
2021.textgraphs-1.2,2020.acl-main.640,0,0.203716,"edge Graphs Martin Schmitt1 Leonardo F. R. Ribeiro2 Philipp Dufter1 Iryna Gurevych2 Hinrich Schütze1 1 2 Center for Information and Language Processing (CIS), LMU Munich Research Training Group AIPHES and UKP Lab, Technische Universität Darmstadt martin@cis.lmu.de Abstract et al., 2018) or variants of Transformer (Vaswani et al., 2017) that apply self-attention on all nodes together, including those that are not directly connected. To avoid losing information, the latter approaches use edge or node labels from the shortest path when computing the attention between two nodes (Zhu et al., 2019; Cai and Lam, 2020). Assuming the existence of a path between any two nodes is particularly problematic for KGs: a set of KG facts often does not form a connected graph. We propose a flexible alternative that neither needs such an assumption nor uses label information to model graph structure: a Transformerbased encoder that interprets the lengths of shortest paths in a graph as relative position information and thus, by means of multi-head attention, dynamically learns different structural views of the input graph with differently weighted connection patterns. We call this new architecture Graformer. Following"
2021.textgraphs-1.2,D19-1052,0,0.0340134,"Missing"
2021.textgraphs-1.2,P18-1008,0,0.0181307,") is the one-hotencoding of the ith node’s label. To compute the node representation H (L) in the Lth layer, we follow Vaswani et al. (2017), i.e., we first normalize the input from the previous layer H (L−1) via layer normalization LN , followed by multi-head graph self-attention SelfAtt g (see § 3.3 for details), which – after dropout regularization Dr and a residual connection – yields the intermediate representation I (cf. Eq. (1)). A feedforward layer FF with one hidden layer and GeLU (Hendrycks and Gimpel, 2016) activation computes the final layer output (cf. Eq. (2)). As recommended by Chen et al. (2018), we apply an additional layer normalization step to the output H (LE ) of the last encoder layer LE . singular −1 −2 +1 value −1 comparison +2 +1 decomposition used-for word2vec embedding −1 +1 Graformer encoder used-for learning (c) Incidence graph with SAMEp edges (dashed green) Figure 1: Different representations of the same KG (types are omitted for clarity). I (L) = Dr (SelfAtt g (LN (H (L−1) ))) + H (L−1) (1) ality of node labels in the graph structure by splitting each node into as many nodes as there are tokens in its label. We thus obtain a directed hypergraph GT = (VT , A, sT , tT ,"
2021.textgraphs-1.2,N18-3011,0,0.196784,"Missing"
2021.textgraphs-1.2,W17-3518,0,0.507841,"(LD ) of the last decoder layer LD is used to compute the probability distribution Pi ∈ [0, 1]|Σ| over all words in the vocabulary Σ at time step i:   (L ) Pi = σ Zi D E &gt; (14) Note that E ∈ R|Σ|×d is the same matrix that is also used to embed node labels and text tokens. 3.5 Training We train Graformer by minimizing the standard negative log-likelihood loss based on the likelihood estimations described in the previous section. 4 4.1 Experiments Datasets We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA (Koncel-Kedziorski et al., 2019) and WebNLG (Gardent et al., 2017). While the latter contains crowd-sourced texts corresponding to subgraphs from various DBPedia categories, the former was automatically created by applying an information extraction tool (Luan et al., 2018) on a corpus of scientific abstracts (Ammar et al., 2018). As this process is noisy, we corrected 7 train instances where an entity name was erroneously split on a special character and, for the same reason, deleted 1 train instance entirely. Otherwise, we use the data as is, including the train/dev/test split. We list the number of instances per data split, as well as general statistics ab"
2021.textgraphs-1.2,Q19-1019,0,0.0329763,"xtreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN) based on message passing through the input graph’s topology as the encoder in their encoder-decoder architectures (Marcheggiani and Perez-Beltrachini, 2018; Koncel-Kedziorski et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). As one layer of these encoders only considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these approaches incorporate information about the graph topology with some varian"
2021.textgraphs-1.2,W05-0909,0,0.0857936,"LG, we follow previous work (Gardent et al., 2017) by replacing underscores in entity names with whitespace and breaking apart camelcased relations. We furthermore follow the evaluation protocol of the original challenge by converting all characters to lowercased ASCII and separating all punctuation from alphanumeric characters during tokenization. For both datasets, we train a BPE vocabulary using sentencepiece (Kudo and Richardson, 2018) on 5 5.1 Results and Discussion Overall performance Table 2 shows the results of our evaluation on AGENDA in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CHRF++ (Popovi´c, 2017). Like the models we compare with, we report the average and standard deviation of 4 runs with different random seeds. Our model outperforms previous Transformerbased models that only consider first-order neighborhoods per encoder layer (Koncel-Kedziorski et al., 2019; An et al., 2019). Compared to the very 15 BLEU Ours METEOR CHRF++ µc #P 17.80 ±0.31 22.07 ±0.23 45.43 ±0.39 36.3 GT 14.30 ±1.01 18.80 ±0.28 – – GT+RBS 15.1 ±0.97 19.5 ±0.29 – – CGE-LW 18.01 ±0.14 22.34 ±0.07 46.69 ±0.17 69.8 Table 2: Experimental results on AGENDA. GT (Graph Transformer) from (Koncel"
2021.textgraphs-1.2,N19-1119,0,0.0562463,"Missing"
2021.textgraphs-1.2,W17-4770,0,0.0247789,"Missing"
2021.textgraphs-1.2,D18-2012,0,0.051661,"Missing"
2021.textgraphs-1.2,D19-1314,1,0.858326,"ach can only see two extreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN) based on message passing through the input graph’s topology as the encoder in their encoder-decoder architectures (Marcheggiani and Perez-Beltrachini, 2018; Koncel-Kedziorski et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). As one layer of these encoders only considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these approaches incorporate information about the graph topolo"
2021.textgraphs-1.2,D18-1360,0,0.060737,"me matrix that is also used to embed node labels and text tokens. 3.5 Training We train Graformer by minimizing the standard negative log-likelihood loss based on the likelihood estimations described in the previous section. 4 4.1 Experiments Datasets We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA (Koncel-Kedziorski et al., 2019) and WebNLG (Gardent et al., 2017). While the latter contains crowd-sourced texts corresponding to subgraphs from various DBPedia categories, the former was automatically created by applying an information extraction tool (Luan et al., 2018) on a corpus of scientific abstracts (Ammar et al., 2018). As this process is noisy, we corrected 7 train instances where an entity name was erroneously split on a special character and, for the same reason, deleted 1 train instance entirely. Otherwise, we use the data as is, including the train/dev/test split. We list the number of instances per data split, as well as general statistics about the graphs in Table 1. Note that the graphs in WebNLG are humanauthored subgraphs of DBpedia while the graphs Graformer decoder Our decoder follows closely the standard Transformer decoder (Vaswani et al"
2021.textgraphs-1.2,2020.tacl-1.38,1,0.885365,"2. Our graph encoder efficiently handles dependencies between much more distant nodes. Pei et al. (2020) define an additional neighborhood based on Euclidean distance in a continuous node embedding space. Similar to our work, a node can thus receive information from distant nodes, given their embeddings are close enough. However, Pei et al. (2020) compute these embeddings only once before training whereas in our approach node similarity is based on the learned representation in each encoder layer. This allows Graformer to dynamically change node interaction patterns during training. Recently, Ribeiro et al. (2020) use two GNN encoders – one using the original topology and one with a fully connected version of the graph – and combine their output in various ways for graph-totext generation. This approach can only see two extreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN)"
2021.textgraphs-1.2,W18-6501,0,0.16164,"mbine their output in various ways for graph-totext generation. This approach can only see two extreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN) based on message passing through the input graph’s topology as the encoder in their encoder-decoder architectures (Marcheggiani and Perez-Beltrachini, 2018; Koncel-Kedziorski et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). As one layer of these encoders only considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these app"
2021.textgraphs-1.2,2020.emnlp-main.577,1,0.8342,"ion A knowledge graph (KG) is a flexible data structure commonly used to store both general world knowledge (Auer et al., 2008) and specialized information, e.g., in biomedicine (Wishart et al., 2018) and computer vision (Krishna et al., 2017). Generating a natural language description of such a graph (KG→text) makes the stored information accessible to a broader audience of end users. It is therefore important for KG-based question answering (Bhowmik and de Melo, 2018), datato-document generation (Moryossef et al., 2019; Koncel-Kedziorski et al., 2019) and interpretability of KGs in general (Schmitt et al., 2020). Recent approaches to KG→text employ encoderdecoder architectures: the encoder first computes vector representations of the graph’s nodes, the decoder then uses them to predict the text sequence. Typical encoder choices are graph neural networks based on message passing between direct neighbors in the graph (Kipf and Welling, 2017; Veliˇckovi´c 1 Our code is publicly available: https://github. com/mnschmit/graformer 10 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 10–21 June 11, 2021. ©2021 Association for Computational Lin"
2021.textgraphs-1.2,N19-1236,0,0.0399162,"Missing"
2021.textgraphs-1.2,N18-2074,0,0.460794,"ly considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these approaches incorporate information about the graph topology with some variant of relative position embeddings (Shaw et al., 2018). They, however, assume that there is always a path between any pair of nodes, i.e., there are no unreachable nodes or disconnected subgraphs. Thus they use an LSTM (Hochreiter and Schmidhuber, 1997) to compute a relation embedding from the labels along this path. However, in contrast to the AMR2 graphs used for their evaluation, KGs are frequently disconnected. Graformer is more flexible and makes no assumption about connectivity. Furthermore, its relative position embeddings only depend on the lengths of shortest paths i.e., purely structural information, not labels. It thus effectively lear"
2021.textgraphs-1.2,P02-1040,0,0.109913,"ts are also lowercased. For WebNLG, we follow previous work (Gardent et al., 2017) by replacing underscores in entity names with whitespace and breaking apart camelcased relations. We furthermore follow the evaluation protocol of the original challenge by converting all characters to lowercased ASCII and separating all punctuation from alphanumeric characters during tokenization. For both datasets, we train a BPE vocabulary using sentencepiece (Kudo and Richardson, 2018) on 5 5.1 Results and Discussion Overall performance Table 2 shows the results of our evaluation on AGENDA in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CHRF++ (Popovi´c, 2017). Like the models we compare with, we report the average and standard deviation of 4 runs with different random seeds. Our model outperforms previous Transformerbased models that only consider first-order neighborhoods per encoder layer (Koncel-Kedziorski et al., 2019; An et al., 2019). Compared to the very 15 BLEU Ours METEOR CHRF++ µc #P 17.80 ±0.31 22.07 ±0.23 45.43 ±0.39 36.3 GT 14.30 ±1.01 18.80 ±0.28 – – GT+RBS 15.1 ±0.97 19.5 ±0.29 – – CGE-LW 18.01 ±0.14 22.34 ±0.07 46.69 ±0.17 69.8 Table 2: Experimental results on AGENDA."
2021.textgraphs-1.2,D19-1548,0,0.0729804,"eration from Knowledge Graphs Martin Schmitt1 Leonardo F. R. Ribeiro2 Philipp Dufter1 Iryna Gurevych2 Hinrich Schütze1 1 2 Center for Information and Language Processing (CIS), LMU Munich Research Training Group AIPHES and UKP Lab, Technische Universität Darmstadt martin@cis.lmu.de Abstract et al., 2018) or variants of Transformer (Vaswani et al., 2017) that apply self-attention on all nodes together, including those that are not directly connected. To avoid losing information, the latter approaches use edge or node labels from the shortest path when computing the attention between two nodes (Zhu et al., 2019; Cai and Lam, 2020). Assuming the existence of a path between any two nodes is particularly problematic for KGs: a set of KG facts often does not form a connected graph. We propose a flexible alternative that neither needs such an assumption nor uses label information to model graph structure: a Transformerbased encoder that interprets the lengths of shortest paths in a graph as relative position information and thus, by means of multi-head attention, dynamically learns different structural views of the input graph with differently weighted connection patterns. We call this new architecture G"
atterer-schutze-2008-inverted,W06-2913,1,\N,Missing
atterer-schutze-2008-inverted,P06-4005,0,\N,Missing
atterer-schutze-2008-inverted,N04-1016,0,\N,Missing
atterer-schutze-2008-inverted,P06-1128,0,\N,Missing
blessing-schutze-2010-fine,E09-1070,0,\N,Missing
blessing-schutze-2010-fine,P03-1015,0,\N,Missing
blessing-schutze-2010-fine,zesch-etal-2008-extracting,0,\N,Missing
blessing-schutze-2010-fine,N06-1038,0,\N,Missing
blessing-schutze-2010-fine,W09-3301,0,\N,Missing
C08-1059,P07-1033,0,0.0686461,"Missing"
C08-1059,P04-1075,0,0.0759707,"arameter when other factors (e.g., the batch size) change. If lower noise allows it, as for the Lewis estimate, moving averages should be used in place of moving medians. 6 Related Work Sch¨utze et al. (2006) studied a Lewis-based performance estimation method in a binary text classification setting. They attribute difficulties in estimating recall to a ”missed cluster effect”, meaning that the active sampling procedure is failing to select some clusters of relevant training examples in the pool that are too dissimilar to the relevant examples already known. Diversity measures as proposed by (Shen et al., 2004) might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER. Since missed clusters of relevant examples in the training data would eventually be used as we exhaustively label the entire pool, we should see improvements in recall when the missed clusters get used. Instead, we observed in section 2.1, that there are no further performance gains after a certain portion of the pool is labeled. Thus, all examples that the classifier can make use of must have been taken into account, and there appear to be no missed clust"
C08-1059,W03-0419,0,0.0297194,"Missing"
C08-1059,D07-1051,0,0.0836453,"t help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER. Since missed clusters of relevant examples in the training data would eventually be used as we exhaustively label the entire pool, we should see improvements in recall when the missed clusters get used. Instead, we observed in section 2.1, that there are no further performance gains after a certain portion of the pool is labeled. Thus, all examples that the classifier can make use of must have been taken into account, and there appear to be no missed clusters. Tomanek et al. (2007) present a stopping criterion for query-by-committee-based AL that is based on the rate of disagreement of the classifiers in the committee. While our uncertainty convergence criterion can only be applied to uncertainty sampling, the performance convergence criterion can be used in a committee-based setting. Li and Sethi (2006) estimate the conditional error as a measure of uncertainty in selection (instead of using it for stopping as we do), using a variablebin histogram for improving the error estimates. They do not evaluate the quality of the probability estimates. As with our stopping crit"
C08-1059,D07-1082,0,0.102183,"with the newly gathered information. In this paper, we adopt the uncertainty sampling approach to AL (Lewis and Gale, 1994). Uncertainty sampling selects those examples in the pool as most inc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. formative for which the statistical classifier is least certain in its classification decision. While AL is an active area of research in NLP, the issue of determining when to stop the AL process has only recently come into focus (Zhu and Hovy, 2007; Vlachos, 2008). This is somewhat surprising because the main purpose of active learning is to save on annotation effort; deciding on the point when enough data is annotated is crucial to fulfilling this goal. We investigate three different stopping criteria in this paper. First, a user of a classification system may want to set a minimum absolute performance for the system to be deployed. The standard way of assessing classifier performance uses a held-out labeled test set. However, labeling a test set of sufficient size is contrary to the goal of minimizing annotation effort and impractical"
C10-1010,zesch-etal-2008-extracting,0,0.0378706,"Missing"
C10-1010,P09-1114,0,0.0220775,"just the text in the Wikipedia) are needed to increase coverage beyond the limited number of instances covered in Wikipedia. Nothman et al. (2009) also annotate Wikipedia’s unstructured text using structured data. The type of structured data they use is hyperlinking (as opposed to infoboxes) and they use it to derive a labeled named entity corpus. They show that the quality of the annotation is comparable to other manually labeled named entity recognition gold standards. We interpret their results as evidence that self-annotation can be used to create high quality gold standards. Related work Jiang (2009) also addresses the issue of supervised relation extraction when no large manually labeled data set is available. They use only a few seed instances of the target relation type to train a supervised relation extraction system. However, they use multi-task transfer learning including a large amount of labeled instances of other relation types for training their system. In contrast, our work eliminates manual labeling by using structured data to annotate the relations. Wu and Weld (2007) extract facts from infoboxes and link them with their corresponding representation in the text. They discuss"
C10-1010,W09-3301,0,0.0144336,"he extraction task is performed by a supervised classifier; this classifier is also implemented as a UIMA component and uses the ClearTK framework. We evaluate our approach on two types of fine-grained relations. 2 attributes are used to model different facts. So the semantics of the infoboxes changes slightly and introduces noise into the structured information. Their work differs from self-annotation in that they are not interested in the creation of selfannotated corpora that can be used as training data for other tasks. Their goal is to develop methods that make infoboxes more consistent. Zhang and Iria (2009) use a novel entity extraction method to automatically generate gazetteers from seed lists using Wikipedia as knowledge source. In contrast to our work they need structured data for the extraction while our system focuses on the extraction of information from unstructured text. Methods that are applicable to any unstructured text (not just the text in the Wikipedia) are needed to increase coverage beyond the limited number of instances covered in Wikipedia. Nothman et al. (2009) also annotate Wikipedia’s unstructured text using structured data. The type of structured data they use is hyperlink"
C10-1010,E09-1070,0,0.0253029,"can be used as training data for other tasks. Their goal is to develop methods that make infoboxes more consistent. Zhang and Iria (2009) use a novel entity extraction method to automatically generate gazetteers from seed lists using Wikipedia as knowledge source. In contrast to our work they need structured data for the extraction while our system focuses on the extraction of information from unstructured text. Methods that are applicable to any unstructured text (not just the text in the Wikipedia) are needed to increase coverage beyond the limited number of instances covered in Wikipedia. Nothman et al. (2009) also annotate Wikipedia’s unstructured text using structured data. The type of structured data they use is hyperlinking (as opposed to infoboxes) and they use it to derive a labeled named entity corpus. They show that the quality of the annotation is comparable to other manually labeled named entity recognition gold standards. We interpret their results as evidence that self-annotation can be used to create high quality gold standards. Related work Jiang (2009) also addresses the issue of supervised relation extraction when no large manually labeled data set is available. They use only a few"
C10-1010,P03-1015,0,0.0250717,"tax constructions. We therefore have enhanced the API for our extraction task to get high quality data for German Wikipedia articles. 5.2 5.3 UIMA The self-annotated corpora are processed by several components of the UIMA (M¨uller et al., 2008) pipeline. The advantage of exchangeable collection readers is that they seamlessly handle structured and unstructured data. Another advantage of using UIMA is the possibility to share components with other research groups. We can easily exchange different components, like the usage of the commonly known OpenNLP processing tools or the FSPar NLP engine (Schiehlen, 2003) (which includes the TreeTagger (Schmid, 1995)). This allows us to experiment with different approaches, e.g., shallow vs. deep analysis. The components we use provide linguistic analysis on different levels: tokens, morphology, part of speech (POS), chunking and partial dependency analysis. Figure 4 shows the results after the linguistic processing of our sample sentence. For this work only a few annotations are wrapped as UIMA types: token (incl. lemma, POS), multiword, sentence, NP, PP and dependency relations (labeled edges between tokens). We will introduce our machine learning component"
C10-2070,W09-0212,1,0.943508,"f-speech patterns based on a version of the corpus tagged with TreeTagger (Schmid, 1994). We use lemmas instead of surface forms. Because we perform the SimRank matrix multiplications in memory, we need to filter out rare words and relations; otherwise, running SimRank to convergence would not be feasible. For adjective-noun pairs, we apply a filter on de en n a v 34,545 10,067 2,828 22,257 12,878 4,866 ncrd amod dobj 65,299 417,151 143,906 288,889 686,073 510,351 Table 2: Node and edge statistics 4 SimRank Our work is based on the SimRank graph similarity algorithm (Jeh and Widom, 2002). In (Dorow et al., 2009), we proposed a formulation of SimRank in terms of matrix operations, which can be applied to (i) weighted graphs and (ii) bilingual problems. We now briefly review SimRank and its bilingual extension. For more details we refer to (Dorow et al., 2009). The basic idea of SimRank is to consider two nodes as similar if they have similar neighborhoods. Node similarity scores are recursively computed from the scores of neighboring nodes: the similarity Sij of two nodes i and j is computed 616 as the normalized sum of the pairwise similarities of their neighbors: X c Skl . Sij = |N (i) ||N (j)| k∈N"
C10-2070,J93-1003,0,0.0215787,"ject subcategorization a man sleeps poss n, n possessive the child’s toy acrd a, a adjective coordination red or blue car Table 1: Relations used in this paper (top) and possible extensions (bottom). dobj amod ncrd verb adjective noun interesting article idea like political book magazine pair frequency (≥ 3). We process noun pairs by applying a frequency threshold on words (≥ 100) and pairs (≥ 3). Verb-object pairs (the smallest data set) were not frequency-filtered. Based on the resulting frequency counts, we calculate association scores for all relationships using the loglikelihood measure (Dunning, 1993). For noun pairs, we discard all pairs with an association score &lt; 3.84 (significance at α = .05). For all three relations, we discard pairs whose observed frequency was smaller than their expected frequency (Evert, 2004, p. 76). As a last step, we further reduce noise by removing nodes of degree 1. Key statistics for the resulting graphs are given in Table 2. We have found that accuracy of extraction is poor if unweighted edges are used. Using the log-likelihood score directly as edge weight gives too much weight to “semantically weak” highfrequency words like put and take. We therefore use t"
C10-2070,C04-1024,0,0.0229047,"fore use the logarithms of the log-likelihood score as edge weights in all SimRank computations reported in this paper. nodes promote de en Figure 2: Graph snippet with typed edges edges promote. Based on amod and dobj, the four nouns are equally similar to each other. However, the greater similarity of article, book, and magazine to each other can be deduced from the fact that these three nouns also occur in the relation ncrd. We exploit this information in the MEE method. Data and Preprocessing. Our corpus in this paper is the Wikipedia. We parse all German and English articles with BitPar (Schmid, 2004) to extract verb-argument relations. We extract adjective-noun modification and noun coordinations with part-of-speech patterns based on a version of the corpus tagged with TreeTagger (Schmid, 1994). We use lemmas instead of surface forms. Because we perform the SimRank matrix multiplications in memory, we need to filter out rare words and relations; otherwise, running SimRank to convergence would not be feasible. For adjective-noun pairs, we apply a filter on de en n a v 34,545 10,067 2,828 22,257 12,878 4,866 ncrd amod dobj 65,299 417,151 143,906 288,889 686,073 510,351 Table 2: Node and edg"
C10-2070,P98-1069,0,0.323919,"hampered by the lack of a common benchmark; we therefore publish a benchmark and the performance of MEE as a baseline for future research. The paper discusses related work in Section 2. We then describe our translation model (Section 3) and multi-edge extraction (Section 4). The benchmark we publish as part of this paper is described in Section 5. Section 6 presents our experimental results and Section 7 analyzes and discusses them. Section 8 summarizes. 2 Related Work Rapp (1999) uses word cooccurrence in a vector space model for bilingual lexicon extraction. Details are given in Section 5. Fung and Yee (1998) also use a vector space approach, but use TF/IDF values in the vector components and experiment with different vector similarity measures for ranking the translation candidates. Koehn and Knight (2002) combine 614 Coling 2010: Poster Volume, pages 614–622, Beijing, August 2010 a vector-space approach with other clues such as orthographic similarity and frequency. They report an accuracy of .39 on the 1000 most frequent English-German noun translation pairs. Garera et al. (2009) use a vector space model with dependency links as dimensions instead of cooccurring words. They report outperforming"
C10-2070,W09-1117,0,0.456159,"999) uses word cooccurrence in a vector space model for bilingual lexicon extraction. Details are given in Section 5. Fung and Yee (1998) also use a vector space approach, but use TF/IDF values in the vector components and experiment with different vector similarity measures for ranking the translation candidates. Koehn and Knight (2002) combine 614 Coling 2010: Poster Volume, pages 614–622, Beijing, August 2010 a vector-space approach with other clues such as orthographic similarity and frequency. They report an accuracy of .39 on the 1000 most frequent English-German noun translation pairs. Garera et al. (2009) use a vector space model with dependency links as dimensions instead of cooccurring words. They report outperforming a cooccurrence vector model by 16 percentage points accuracy on English-Spanish. Haghighi et al. (2008) use a probabilistic model over word feature vectors containing cooccurrence and orthographic features. They then use canonical correlation analysis to find matchings between words in a common latent space. They evaluate on multiple languages and report high precision even without a seed lexicon. Most previous work has used vector spaces and (except for Garera et al. (2009)) c"
C10-2070,P08-1088,0,0.352091,"and experiment with different vector similarity measures for ranking the translation candidates. Koehn and Knight (2002) combine 614 Coling 2010: Poster Volume, pages 614–622, Beijing, August 2010 a vector-space approach with other clues such as orthographic similarity and frequency. They report an accuracy of .39 on the 1000 most frequent English-German noun translation pairs. Garera et al. (2009) use a vector space model with dependency links as dimensions instead of cooccurring words. They report outperforming a cooccurrence vector model by 16 percentage points accuracy on English-Spanish. Haghighi et al. (2008) use a probabilistic model over word feature vectors containing cooccurrence and orthographic features. They then use canonical correlation analysis to find matchings between words in a common latent space. They evaluate on multiple languages and report high precision even without a seed lexicon. Most previous work has used vector spaces and (except for Garera et al. (2009)) cooccurrence data. Our approach uses linguistic relations like subcategorization, modification and coordination in a graph-based model. Further, we evaluate our approach on different parts of speech, whereas some previous"
C10-2070,W02-0902,0,0.367552,"cribe our translation model (Section 3) and multi-edge extraction (Section 4). The benchmark we publish as part of this paper is described in Section 5. Section 6 presents our experimental results and Section 7 analyzes and discusses them. Section 8 summarizes. 2 Related Work Rapp (1999) uses word cooccurrence in a vector space model for bilingual lexicon extraction. Details are given in Section 5. Fung and Yee (1998) also use a vector space approach, but use TF/IDF values in the vector components and experiment with different vector similarity measures for ranking the translation candidates. Koehn and Knight (2002) combine 614 Coling 2010: Poster Volume, pages 614–622, Beijing, August 2010 a vector-space approach with other clues such as orthographic similarity and frequency. They report an accuracy of .39 on the 1000 most frequent English-German noun translation pairs. Garera et al. (2009) use a vector space model with dependency links as dimensions instead of cooccurring words. They report outperforming a cooccurrence vector model by 16 percentage points accuracy on English-Spanish. Haghighi et al. (2008) use a probabilistic model over word feature vectors containing cooccurrence and orthographic feat"
C10-2070,michelbacher-etal-2010-building,1,0.875682,"Missing"
C10-2070,P99-1067,0,0.579143,"vel method, multi-edge extraction, which is both modular and scalable; (iv) progress in bilingual lexicon extraction has been hampered by the lack of a common benchmark; we therefore publish a benchmark and the performance of MEE as a baseline for future research. The paper discusses related work in Section 2. We then describe our translation model (Section 3) and multi-edge extraction (Section 4). The benchmark we publish as part of this paper is described in Section 5. Section 6 presents our experimental results and Section 7 analyzes and discusses them. Section 8 summarizes. 2 Related Work Rapp (1999) uses word cooccurrence in a vector space model for bilingual lexicon extraction. Details are given in Section 5. Fung and Yee (1998) also use a vector space approach, but use TF/IDF values in the vector components and experiment with different vector similarity measures for ranking the translation candidates. Koehn and Knight (2002) combine 614 Coling 2010: Poster Volume, pages 614–622, Beijing, August 2010 a vector-space approach with other clues such as orthographic similarity and frequency. They report an accuracy of .39 on the 1000 most frequent English-German noun translation pairs. Gare"
C10-2070,C98-1066,0,\N,Missing
C10-2127,D08-1014,0,0.0271153,"ntiment transfer method which we apply in experiments in section 5. 2 Related Work Mihalcea et al. (2007) propose two methods for translating sentiment lexicons. The first method simply uses bilingual dictionaries to translate an English sentiment lexicon. A sentence-based classifier built with this list achieved high precision, but low recall on a small Romanian test set. The second method is based on parallel corpora. The source language in the corpus is annotated with sentiment information, and the information is then projected to the target language. Problems arise due to mistranslations. Banea et al. (2008) use machine translation for multilingual sentiment analysis. Given a corpus annotated with sentiment information in one language, machine translation is used to produce an annotated corpus in the target language, by preserving the annotations. The original annotations 1104 Coling 2010: Poster Volume, pages 1104–1112, Beijing, August 2010 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. In co-training, one classifier produces additional training data for a second classifier. In this case, an English classifier assists in train"
C10-2127,W09-0212,1,0.715264,"sentiment. In contrast to previous approaches, the work presented in this paper uses corpora that are not annotated with sentiment. Turney (2002) suggests a corpus-based extraction method based on his pointwise mutual information (PMI) synonymy measure. He assumes that the sentiment orientation of a phrase can be determined by comparing its pointwise mutual information with a positive (excellent) and a negative phrase (poor). An introduction to this method is given in Section 3.2. 3 3.1 Background We use the extension of the SimRank (Jeh and Widom, 2002) node similarity algorithm proposed by Dorow et al. (2009). Given two graphs A and B, the similarity between two nodes a in A and b in B is computed in each iteration as: c |NA (a)||NB (b)| Multi-Edge Extraction (MEE). MEE is an extension of SimRank that, in each iteration, computes the average node-node similarity of several different SimRank matrices. In our case, we use two different SimRank matrices, one for coordinations and one for adjective modification. See (Dorow et al., 2009) for details. We also used the node p degree normalization function h(n) = √ n × maxk (|N (k)|) (where n is the node degree, and N (k) the degree of node k) to decrease"
C10-2127,P97-1023,0,0.0310951,"timent information in one language, machine translation is used to produce an annotated corpus in the target language, by preserving the annotations. The original annotations 1104 Coling 2010: Poster Volume, pages 1104–1112, Beijing, August 2010 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. In co-training, one classifier produces additional training data for a second classifier. In this case, an English classifier assists in training a Chinese classifier. The induction of a sentiment lexicon is the subject of early work by Hatzivassiloglou and McKeown (1997). They construct graphs from coordination data from large corpora based on the intuition that adjectives with the same sentiment orientation are likely to be coordinated. For example, fresh and delicious is more likely than rotten and delicious. They then apply a graph clustering algorithm to find groups of adjectives with the same orientation. Finally, they assign the same label to all adjectives that belong to the same cluster. Corpus work and bilingual dictionaries are promising resources for translating sentiment. In contrast to previous approaches, the work presented in this paper uses co"
C10-2127,C02-1114,0,0.0121183,"nd-annotated resources for sentiment analysis exist in other languages, it seems plausible to use automatic translation of sentiment information to leverage these resources. In order to translate sentiment, we will use multiple sources of information that we represent in a MEE graph as given in Section 3.1. In our first experiments (Scheible, 2010), coordinated adjectives were used as the sole training source. Two adjectives are coordinated if they are linked with a conjunction like and or but. The intuition behind using coordinations – based on work by Hatzivassiloglou and McKeown (1997) and Widdows and Dorow (2002) – was that words which are coordinated share properties. In particular, coordinated adjectives usually express similar sentiments even though there are exceptions (e.g., “The movie was both good and bad”). In this paper, we focus on using multiple edge types for sentiment translation. In particular, the graph we will use contains two types of relations, coordinations and adjective-noun modification. In the sentence “The movie was enjoyable and fun”, enjoyable and fun are coordinated. In This is an enjoyable movie, the adjective enjoyable modifies the noun movie. We selected these two relation"
C10-2127,C10-2070,1,0.889348,"Missing"
C10-2127,H05-1044,0,0.0660827,"iments were made for English, so we had to construct our own set of seed words. For German, we chose gut (good), nett (nice), richtig (right), sch¨on (beautiful), ordentlich (neat), angenehm (pleasant), aufrichtig (honest), gewissenhaft (faithful), and hervorragend (excellent) as positive seed words, and schlecht (bad), teuer (expensive), falsch (wrong), b¨ose (evil), feindlich (hostile), verhasst (invidious), widerlich (disgusting), fehlerhaft (faulty), and mangelhaft (flawed) as negative ones. 5.2 Sentiment Lexicon For our experiments, we used two different polarity lexicons. The lexicon of Wilson et al. (2005) contains sentiment annotations for 8,221 words 2 3 http://www.dict.cc http://jbauman.com/aboutgsl.html annotation positive weakpos neutral weakneg negative value 1.0 0.5 0.0 −0.5 −1.0 Table 1: Assigned values for Wilson et al. set which are tagged as positive, neutral, or negative. A few words are tagged as weakneg, implying weak negativity. These categorial annotations are mapped to the range [-1,1] using the assignment scheme given in Table 1. 5.3 Human Ratings In order to manually annotate a test set, we chose 200 German adjectives that occurred in the Wikipedia corpus and that were part o"
C10-2127,michelbacher-etal-2010-building,1,0.873101,"Missing"
C10-2127,P07-1123,0,0.0506004,"graph-based translation. Our experiments are carried out using English as a source language and German as a target language. We evaluate our method using a hand-annotated set of German adjectives which we intend to publish. In the following section, related work is discussed. Section 3.1 gives an introduction to SimRank and its application to lexicon induction, while section 3.2 reviews SO-PMI (Turney, 2002), an unsupervised baseline method for the generation of sentiment lexicons. In section 4, we define our sentiment transfer method which we apply in experiments in section 5. 2 Related Work Mihalcea et al. (2007) propose two methods for translating sentiment lexicons. The first method simply uses bilingual dictionaries to translate an English sentiment lexicon. A sentence-based classifier built with this list achieved high precision, but low recall on a small Romanian test set. The second method is based on parallel corpora. The source language in the corpus is annotated with sentiment information, and the information is then projected to the target language. Problems arise due to mistranslations. Banea et al. (2008) use machine translation for multilingual sentiment analysis. Given a corpus annotated"
C10-2127,W02-1011,0,0.0185019,"Missing"
C10-2127,P10-3005,1,0.398159,"an operator of a search engine that returns documents in which the operands occur within a close range of each other. 1105 4 Sentiment Translation Unsupervised methods like SO-PMI are suitable to acquire basic sentiment information in a language. However, since hand-annotated resources for sentiment analysis exist in other languages, it seems plausible to use automatic translation of sentiment information to leverage these resources. In order to translate sentiment, we will use multiple sources of information that we represent in a MEE graph as given in Section 3.1. In our first experiments (Scheible, 2010), coordinated adjectives were used as the sole training source. Two adjectives are coordinated if they are linked with a conjunction like and or but. The intuition behind using coordinations – based on work by Hatzivassiloglou and McKeown (1997) and Widdows and Dorow (2002) – was that words which are coordinated share properties. In particular, coordinated adjectives usually express similar sentiments even though there are exceptions (e.g., “The movie was both good and bad”). In this paper, we focus on using multiple edge types for sentiment translation. In particular, the graph we will use co"
C10-2127,P02-1053,0,0.239245,"method is its ability to handle sparse data about the relations between the languages well (i.e., a small seed lexicon). Further, we experiment with combining multiple types of linguistic relations for graph-based translation. Our experiments are carried out using English as a source language and German as a target language. We evaluate our method using a hand-annotated set of German adjectives which we intend to publish. In the following section, related work is discussed. Section 3.1 gives an introduction to SimRank and its application to lexicon induction, while section 3.2 reviews SO-PMI (Turney, 2002), an unsupervised baseline method for the generation of sentiment lexicons. In section 4, we define our sentiment transfer method which we apply in experiments in section 5. 2 Related Work Mihalcea et al. (2007) propose two methods for translating sentiment lexicons. The first method simply uses bilingual dictionaries to translate an English sentiment lexicon. A sentence-based classifier built with this list achieved high precision, but low recall on a small Romanian test set. The second method is based on parallel corpora. The source language in the corpus is annotated with sentiment informat"
C10-2127,P09-1027,0,0.0388012,"llel corpora. The source language in the corpus is annotated with sentiment information, and the information is then projected to the target language. Problems arise due to mistranslations. Banea et al. (2008) use machine translation for multilingual sentiment analysis. Given a corpus annotated with sentiment information in one language, machine translation is used to produce an annotated corpus in the target language, by preserving the annotations. The original annotations 1104 Coling 2010: Poster Volume, pages 1104–1112, Beijing, August 2010 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. In co-training, one classifier produces additional training data for a second classifier. In this case, an English classifier assists in training a Chinese classifier. The induction of a sentiment lexicon is the subject of early work by Hatzivassiloglou and McKeown (1997). They construct graphs from coordination data from large corpora based on the intuition that adjectives with the same sentiment orientation are likely to be coordinated. For example, fresh and delicious is more likely than rotten and delicious. They then apply a graph c"
C12-1003,W11-0413,0,0.014833,"Question Answering (MPQA) corpus. This corpus has detailed manual annotations of a set of 535 news articles. The corpus separates subjective and objective expressions. It has some information about objective content (such as the source and the target of the objective speech), but only has sentiment information about the subjective content. Our annotated corpus is different because it is concerned with objective language. Our task requires different annotation guidelines and, in general, a different setup for the annotation process compared to work on sentiment analysis. Balahur et al. (2010), Abdul-Mageed and Diab (2011) and Balahur and Steinberger (2009) try to distinguish between positive and negative sentiment vs. good and bad news. Good and bad news are considered objective information and excluded from the classification process. In contrast, our method deals with good and bad objective information in the classification step. Some prior work has classified financial news according to polarity. Some papers limit their classification to the subjective content of the news (e.g., (Agic et al., 2010)); other papers have classified objective content as well (Ahmad, 2006; Devitt and Ahmad, 2007; Shtrimberg, 200"
C12-1003,agic-etal-2010-towards,0,0.0228033,"rent setup for the annotation process compared to work on sentiment analysis. Balahur et al. (2010), Abdul-Mageed and Diab (2011) and Balahur and Steinberger (2009) try to distinguish between positive and negative sentiment vs. good and bad news. Good and bad news are considered objective information and excluded from the classification process. In contrast, our method deals with good and bad objective information in the classification step. Some prior work has classified financial news according to polarity. Some papers limit their classification to the subjective content of the news (e.g., (Agic et al., 2010)); other papers have classified objective content as well (Ahmad, 2006; Devitt and Ahmad, 2007; Shtrimberg, 2004). For example, Shtrimberg (2004) proposed an approach to classify news stories about companies as positive or negative. His classifier learned from a corpus where every news story about a company is labeled based on its impact on the future price of its stock. However, impact on price is different from positive/negative. For example, bad economic news can have a positive impact on stock prices if investors think it will make the Federal Reserve more likely to launch another round of"
C12-1003,D10-1111,0,0.12628,"Missing"
C12-1003,balahur-etal-2010-sentiment,0,0.0765888,"Missing"
C12-1003,N12-1092,0,0.0222102,"ee different annotators. We map the five point rating scale to [−2, −1, 0, 1, 2]; e.g., “much more positive” is mapped to 2. The gold standard score g ∆POV for a pair of articles is then the average of the three scores given by the annotators (where the superscript g indicates “gold standard”). Intercoder agreement is α = .585 (Krippendorff, 2004). This agreement is not as good as we would like it to be, but it is sufficient to evaluate our method; several other studies have published evaluation results based on gold standards with similar agreement (Bhardwaj et al., 2010; Brusk et al., 2010; Becker et al., 2012; Chen et al., 2012).6 4 Method Absolute POV classification. For the task of determining the absolute POV – positive, neutral, negative – of a sentence, we adopt a statistical classification approach and use the Stanford MaxEnt classifier (Manning and Klein, 2003) with default parameters. We refer to the probability of the positive (resp. negative) class for a sentence s as PosScore (resp. NegScore): PosScore(s) = P(positive|s) NegScore(s) = P(negative|s) Our features are bag of words (BOW) and letter k-grams (n-grams) where 2 ≤ k ≤ 6. 6 The two gold standards are available at ✐❢♥❧♣✳♦r❣✴s❝❤✉❡"
C12-1003,W10-1806,0,0.0236885,". Each pair of articles is annotated by three different annotators. We map the five point rating scale to [−2, −1, 0, 1, 2]; e.g., “much more positive” is mapped to 2. The gold standard score g ∆POV for a pair of articles is then the average of the three scores given by the annotators (where the superscript g indicates “gold standard”). Intercoder agreement is α = .585 (Krippendorff, 2004). This agreement is not as good as we would like it to be, but it is sufficient to evaluate our method; several other studies have published evaluation results based on gold standards with similar agreement (Bhardwaj et al., 2010; Brusk et al., 2010; Becker et al., 2012; Chen et al., 2012).6 4 Method Absolute POV classification. For the task of determining the absolute POV – positive, neutral, negative – of a sentence, we adopt a statistical classification approach and use the Stanford MaxEnt classifier (Manning and Klein, 2003) with default parameters. We refer to the probability of the positive (resp. negative) class for a sentence s as PosScore (resp. NegScore): PosScore(s) = P(positive|s) NegScore(s) = P(negative|s) Our features are bag of words (BOW) and letter k-grams (n-grams) where 2 ≤ k ≤ 6. 6 The two gold st"
C12-1003,W10-4333,0,0.0263886,"is annotated by three different annotators. We map the five point rating scale to [−2, −1, 0, 1, 2]; e.g., “much more positive” is mapped to 2. The gold standard score g ∆POV for a pair of articles is then the average of the three scores given by the annotators (where the superscript g indicates “gold standard”). Intercoder agreement is α = .585 (Krippendorff, 2004). This agreement is not as good as we would like it to be, but it is sufficient to evaluate our method; several other studies have published evaluation results based on gold standards with similar agreement (Bhardwaj et al., 2010; Brusk et al., 2010; Becker et al., 2012; Chen et al., 2012).6 4 Method Absolute POV classification. For the task of determining the absolute POV – positive, neutral, negative – of a sentence, we adopt a statistical classification approach and use the Stanford MaxEnt classifier (Manning and Klein, 2003) with default parameters. We refer to the probability of the positive (resp. negative) class for a sentence s as PosScore (resp. NegScore): PosScore(s) = P(positive|s) NegScore(s) = P(negative|s) Our features are bag of words (BOW) and letter k-grams (n-grams) where 2 ≤ k ≤ 6. 6 The two gold standards are availabl"
C12-1003,P07-1124,0,0.0387515,"l. (2010), Abdul-Mageed and Diab (2011) and Balahur and Steinberger (2009) try to distinguish between positive and negative sentiment vs. good and bad news. Good and bad news are considered objective information and excluded from the classification process. In contrast, our method deals with good and bad objective information in the classification step. Some prior work has classified financial news according to polarity. Some papers limit their classification to the subjective content of the news (e.g., (Agic et al., 2010)); other papers have classified objective content as well (Ahmad, 2006; Devitt and Ahmad, 2007; Shtrimberg, 2004). For example, Shtrimberg (2004) proposed an approach to classify news stories about companies as positive or negative. His classifier learned from a corpus where every news story about a company is labeled based on its impact on the future price of its stock. However, impact on price is different from positive/negative. For example, bad economic news can have a positive impact on stock prices if investors think it will make the Federal Reserve more likely to launch another round of quantitative easing. Our approach generates a score that indicates the POV of the article tow"
C12-1003,N09-1057,0,0.0793997,"Missing"
C12-1003,D10-1028,0,0.0431017,"Missing"
C12-1003,W11-0406,0,0.0870322,"rice of its stock. However, impact on price is different from positive/negative. For example, bad economic news can have a positive impact on stock prices if investors think it will make the Federal Reserve more likely to launch another round of quantitative easing. Our approach generates a score that indicates the POV of the article toward the subject matter and that is not directly related to the impact such information might have on the financial markets. Another topic related to POV is media bias (Gentzkow and Shapiro, 2005, 2006). Some studies on this topic investigate bias in Wikipedia. Herzig et al. (2011) propose a novel annotation scheme as a basic step towards an automatic machine learning system to detect biased language in English Wikipedia. The scheme has multiple levels of bias tagging: the intra-sentential level, which includes polar-phrase, weasel, repetition, and personal-tone, and the sentence and entry level. The proposed scheme was applied to a set of articles from the service providers category in Wikipedia. Annotation categories distinguished between biased language and unbiased language. The authors conducted their annotation scheme based on the articles which explicitly violate"
C12-1003,C12-2056,1,0.828652,"escribe negative events that happened to the target, e.g., “Roosevelt contracted . . . polio which resulted in permanent paralysis.” Again, this will decrease the POVScore of the article even though the information reports something negative about the circumstances of the person’s life that will not affect a reader’s POV towards the person in a negative way. An even subtler problem occurs if positive or negative words occur in a sentence that is directly relevant for POV towards the target, but these positive or negative words are in the scope of another word that reverses their meaning (cf. (Kessler and Schütze, 2012)). For example, the statement: John started a war on violence against women supports a positive POV towards John even though most of the words in the statement are negative words. The immediate effect of the shortcomings of a BOW-based feature representation is an incorrect estimation of absolute POV. However, since these effects are somewhat random and will in most cases not affect Arabic and English to the same extent, the BOW problem can also give rise to incorrect POV differences. In our data set, this is the reason that our system does not correctly predict the POV difference for Mohamed"
C12-1003,P10-2047,0,0.183103,"Missing"
C12-1003,D11-1143,1,0.836821,"nt for the annotation task in most cases, sometimes it is difficult to determine the correct POV without reading some preceding or following sentences. For example, if the target sentence contains a pronoun, the annotator needs the context of the paragraph to resolve the reference. We select one word from the sentence to be annotated randomly and render it in green. In the figure, the word is agency. The worker has to type this word in the corresponding answer field instead of using radio buttons or check boxes. We have found that this simple copying operation improves AMT annotation quality (Laws et al., 2011). Workers are asked to label the sentence with one of three labels: positive, neutral and negative, based on the POV of the sentence toward the target. In Figure 1, the sentence to be annotated shows a negative POV towards the target (Mel Gibson), so we would expect the worker to annotate it as negative. Incomplete assignments where the worker submits the task without giving all the required information and suspicious assignments where the worker spends only a few seconds on the task are rejected and republished to a different worker. We use Fleiss’ κ (Fleiss, 1971) (instead of Cohen’s κ) to c"
C12-1003,P06-1133,0,0.0562598,"Missing"
C12-1003,W06-2915,0,0.294703,"Missing"
C12-1003,N03-5008,0,0.0271988,"script g indicates “gold standard”). Intercoder agreement is α = .585 (Krippendorff, 2004). This agreement is not as good as we would like it to be, but it is sufficient to evaluate our method; several other studies have published evaluation results based on gold standards with similar agreement (Bhardwaj et al., 2010; Brusk et al., 2010; Becker et al., 2012; Chen et al., 2012).6 4 Method Absolute POV classification. For the task of determining the absolute POV – positive, neutral, negative – of a sentence, we adopt a statistical classification approach and use the Stanford MaxEnt classifier (Manning and Klein, 2003) with default parameters. We refer to the probability of the positive (resp. negative) class for a sentence s as PosScore (resp. NegScore): PosScore(s) = P(positive|s) NegScore(s) = P(negative|s) Our features are bag of words (BOW) and letter k-grams (n-grams) where 2 ≤ k ≤ 6. 6 The two gold standards are available at ✐❢♥❧♣✳♦r❣✴s❝❤✉❡t❡✴♣♦✈. 40 For English, BOW and n-gram features are directly computed from text (as tokenized by the Stanford classifier) without any further linguistic preprocessing like lemmatization. For Arabic, we investigate a number of different options for linguistic prep"
C12-1003,D10-1007,0,0.143276,"Missing"
C12-1082,N12-1009,0,0.0526341,"earch databases (Nanba et al., 2004)[CJPF]. Bibliometric measures that quantify the impact of publications (e.g., Moed, 2005)[CJPF] are also based on citations. Most of this work does not differentiate between uses of citations, e.g., whether a citation is more or less important to the paper or whether the paper’s authors support or refute the claims made in the cited work. However, recently a number of research groups have attempted to classify citations with respect to dimensions like importance and relation to cited work (Teufel et al., 2006b; Dong and Schäfer, 2011; Sugiyama et al., 2010; Abu-Jbara and Radev, 2012)[CEPF]. By adding such fine-grained information to individual citations, the various applications of citation analysis can be better served; e.g., citations that are foundational to a paper may constitute better summary sentences for the cited paper. Thus, there are clear potential benefits to fine-grained citation analysis; and a number of case studies have been published that demonstrate this potential (Nanba et al., 2004; Teufel et al., 2006b)[CEPF]. However, fine-grained citation analysis is currently not widely used in applications that access and analyze the scientific literature. In thi"
C12-1082,P11-3015,0,0.0664418,"abel. It is also important to note that this classification has no undefined class. Several previous annotation schemes have a default label, neutral or other, that is assigned to a citation when no other classes can be. In the work we have seen that uses such annotation schemes, more than half of the citation instances are assigned this undefined label. In these cases, summarization or IR systems that want to make use of citation information obtain no useful information from the citation classifier for more than half of citations. 3 Corpus Our corpus, like corpora from some previous studies (Athar, 2011; Dong and Schäfer, 2011)[CEPF], is taken from NLP literature. Specifically, we have taken the 2004 ACL proceedings from the ACL Anthology Reference Corpus (ARC) (Bird et al., 2008)[OEPF]. NLP literature was chosen because our annotators (NLP students) are more familiar with this data and can make more informed decisions when annotating the citations. Some statistics on the number of documents and citations in the corpus can be found in Table 1. Each citation in the corpus has been independently annotated by at least two of six annotators. Gold labels are chosen by a simple majority vote and i"
C12-1082,N12-1073,0,0.336111,"of one sentence. It is not clear how much context is best for feature extraction, so in another set of experiments we fix the feature set and test the features extracted from different sized context windows. In previous work, different sized context windows were used by different studies, e.g., Athar (2011)[CEPF] used only the sentence containing the citation while Dong and Schäfer (2011)[CEPF] used up to three sentences. Kaplan et al. (2009)[CEPF]and Abu-Jbara and Radev (2012)[CEPF]have illustrated the difficulties in delineating the exact boundary for each individual citation context, while Athar and Teufel (2012)[CEPF] tried different fixed context sizes for citation classification. We follow this general idea and test context lengths of 1, 2, and 3 sentences. Feature class comparison. In addition to comparing our own feature set with those from previous work, we also want to investigate what feature classes assist most in the classification. We perform this analysis by examining the impact of the seven feature classes described in Section 4. More specifically, we compare the results of their individual performance using only features in the feature class (Table 4, top), and their ablation from the en"
C12-1082,bird-etal-2008-acl,0,0.0258398,"ed to a citation when no other classes can be. In the work we have seen that uses such annotation schemes, more than half of the citation instances are assigned this undefined label. In these cases, summarization or IR systems that want to make use of citation information obtain no useful information from the citation classifier for more than half of citations. 3 Corpus Our corpus, like corpora from some previous studies (Athar, 2011; Dong and Schäfer, 2011)[CEPF], is taken from NLP literature. Specifically, we have taken the 2004 ACL proceedings from the ACL Anthology Reference Corpus (ARC) (Bird et al., 2008)[OEPF]. NLP literature was chosen because our annotators (NLP students) are more familiar with this data and can make more informed decisions when annotating the citations. Some statistics on the number of documents and citations in the corpus can be found in Table 1. Each citation in the corpus has been independently annotated by at least two of six annotators. Gold labels are chosen by a simple majority vote and in the case of ties the votes of more experienced annotators are weighted higher. The annotators were given guidelines to help ensure consistent annotation. We built a browser-based"
C12-1082,C10-1011,0,0.0302195,". NO99=Nanba and Okumura (1999); TST06=Teufel et al. (2006b); Ath11=Athar (2011); DS11=Dong and Schäfer (2011). “unknown” = exact definition of the feature (e.g., Boolean or Real) is unknown. Examples of possible feature values are given in italics where appropriate. 1348 related only to the main verb were shown to be effective by Teufel et al. (2006b)[CEPF], e.g., tense (tense), voice (voice), and modality (modal). We also include modality in our feature set (has-modal) along with separate features for the main verb (main-verb) and the root (root) as determined by the MATE dependency parser (Bohnet, 2010)[OEPF]. We do not include POS as features per se, but some features are triggered by the occurrence of selected POS: 1st and 3rd person pronouns (has-1stPRP, has-3rdPRP); and comparatives and superlatives (comp/sup). Comparatives and superlatives can help distinguish CONF from NEG. Pronouns on the other hand may be useful in classifying EVOL-JUX, e.g., first person pronouns are used when clarifying the differences between proposed and cited approaches. We add two other features for the contrastive conjunction “but” (but) and the abbreviation “cf.” (has-cf). In our analysis of citations we look"
C12-1082,W06-1670,0,0.0103771,"take this a step further and train a named-entity recognition (NER) system to identify NLP named entities. We identify two types of NLP named entities: corpora and tools. First, we create a gazetteer of NLP tools and corpora from an online list of these resources.5 Next, we tag a portion of our corpus using the gazetteer list to label any occurrence of the words in the list and then manually check those labeled instances to be sure they are correctly labeled. In this way we can expediently create training data, with an emphasis on precision over recall. Finally, we train the SuperSenseTagger (Ciaramita and Altun, 2006)[OEPF] on this annotated portion, and tag the remaining part of the corpus. NER is not central to our task, so we did no direct evaluation of it; we looked only to see if it might lead to improvements in our classification. We include two features, has-resource and has-tool, for the two types of entities. The NER features we extract are related only to the NLP domain. However, this approach for acquiring named entities is not domain dependent and can be used to develop a reasonably efficient NER system using lists of tools or resources from any domain. 5 Experiments In this section we will out"
C12-1082,de-marneffe-etal-2006-generating,0,0.0434405,"Missing"
C12-1082,I11-1070,0,0.119091,"t al., 2006)[CJPF], and building integrated research databases (Nanba et al., 2004)[CJPF]. Bibliometric measures that quantify the impact of publications (e.g., Moed, 2005)[CJPF] are also based on citations. Most of this work does not differentiate between uses of citations, e.g., whether a citation is more or less important to the paper or whether the paper’s authors support or refute the claims made in the cited work. However, recently a number of research groups have attempted to classify citations with respect to dimensions like importance and relation to cited work (Teufel et al., 2006b; Dong and Schäfer, 2011; Sugiyama et al., 2010; Abu-Jbara and Radev, 2012)[CEPF]. By adding such fine-grained information to individual citations, the various applications of citation analysis can be better served; e.g., citations that are foundational to a paper may constitute better summary sentences for the cited paper. Thus, there are clear potential benefits to fine-grained citation analysis; and a number of case studies have been published that demonstrate this potential (Nanba et al., 2004; Teufel et al., 2006b)[CEPF]. However, fine-grained citation analysis is currently not widely used in applications that a"
C12-1082,W09-3611,0,0.0572019,"the types of features that seem to perform best on our dataset with our annotation scheme. Citation context size. The tests just described are run with a fixed context size of one sentence. It is not clear how much context is best for feature extraction, so in another set of experiments we fix the feature set and test the features extracted from different sized context windows. In previous work, different sized context windows were used by different studies, e.g., Athar (2011)[CEPF] used only the sentence containing the citation while Dong and Schäfer (2011)[CEPF] used up to three sentences. Kaplan et al. (2009)[CEPF]and Abu-Jbara and Radev (2012)[CEPF]have illustrated the difficulties in delineating the exact boundary for each individual citation context, while Athar and Teufel (2012)[CEPF] tried different fixed context sizes for citation classification. We follow this general idea and test context lengths of 1, 2, and 3 sentences. Feature class comparison. In addition to comparing our own feature set with those from previous work, we also want to investigate what feature classes assist most in the classification. We perform this analysis by examining the impact of the seven feature classes describe"
C12-1082,N03-5008,0,0.0303671,"Missing"
C12-1082,C08-1087,0,0.0237816,"ned for citation classification and compare them experimentally with previously proposed citation features, showing that these new features improve classification accuracy. KEYWORDS: citation classification, feature extraction. Proceedings of COLING 2012: Technical Papers, pages 1343–1358, COLING 2012, Mumbai, December 2012. 1343 1 Introduction Citations are a valuable resource for characterizing scientific publications and their links to each other. They have been exploited for a number of natural language processing (NLP) and information retrieval (IR) applications, including summarization (Qazvinian and Radev, 2008; Qazvinian et al., 2010)[CJPF]1 , improved indexing and retrieval (Ritchie et al., 2006)[CJPF], and building integrated research databases (Nanba et al., 2004)[CJPF]. Bibliometric measures that quantify the impact of publications (e.g., Moed, 2005)[CJPF] are also based on citations. Most of this work does not differentiate between uses of citations, e.g., whether a citation is more or less important to the paper or whether the paper’s authors support or refute the claims made in the cited work. However, recently a number of research groups have attempted to classify citations with respect to"
C12-1082,C10-1101,0,0.0111848,"tion and compare them experimentally with previously proposed citation features, showing that these new features improve classification accuracy. KEYWORDS: citation classification, feature extraction. Proceedings of COLING 2012: Technical Papers, pages 1343–1358, COLING 2012, Mumbai, December 2012. 1343 1 Introduction Citations are a valuable resource for characterizing scientific publications and their links to each other. They have been exploited for a number of natural language processing (NLP) and information retrieval (IR) applications, including summarization (Qazvinian and Radev, 2008; Qazvinian et al., 2010)[CJPF]1 , improved indexing and retrieval (Ritchie et al., 2006)[CJPF], and building integrated research databases (Nanba et al., 2004)[CJPF]. Bibliometric measures that quantify the impact of publications (e.g., Moed, 2005)[CJPF] are also based on citations. Most of this work does not differentiate between uses of citations, e.g., whether a citation is more or less important to the paper or whether the paper’s authors support or refute the claims made in the cited work. However, recently a number of research groups have attempted to classify citations with respect to dimensions like importanc"
C12-1082,W06-0804,0,0.0165627,"tion features, showing that these new features improve classification accuracy. KEYWORDS: citation classification, feature extraction. Proceedings of COLING 2012: Technical Papers, pages 1343–1358, COLING 2012, Mumbai, December 2012. 1343 1 Introduction Citations are a valuable resource for characterizing scientific publications and their links to each other. They have been exploited for a number of natural language processing (NLP) and information retrieval (IR) applications, including summarization (Qazvinian and Radev, 2008; Qazvinian et al., 2010)[CJPF]1 , improved indexing and retrieval (Ritchie et al., 2006)[CJPF], and building integrated research databases (Nanba et al., 2004)[CJPF]. Bibliometric measures that quantify the impact of publications (e.g., Moed, 2005)[CJPF] are also based on citations. Most of this work does not differentiate between uses of citations, e.g., whether a citation is more or less important to the paper or whether the paper’s authors support or refute the claims made in the cited work. However, recently a number of research groups have attempted to classify citations with respect to dimensions like importance and relation to cited work (Teufel et al., 2006b; Dong and Sch"
C12-1082,W06-1312,0,0.126244,"d retrieval (Ritchie et al., 2006)[CJPF], and building integrated research databases (Nanba et al., 2004)[CJPF]. Bibliometric measures that quantify the impact of publications (e.g., Moed, 2005)[CJPF] are also based on citations. Most of this work does not differentiate between uses of citations, e.g., whether a citation is more or less important to the paper or whether the paper’s authors support or refute the claims made in the cited work. However, recently a number of research groups have attempted to classify citations with respect to dimensions like importance and relation to cited work (Teufel et al., 2006b; Dong and Schäfer, 2011; Sugiyama et al., 2010; Abu-Jbara and Radev, 2012)[CEPF]. By adding such fine-grained information to individual citations, the various applications of citation analysis can be better served; e.g., citations that are foundational to a paper may constitute better summary sentences for the cited paper. Thus, there are clear potential benefits to fine-grained citation analysis; and a number of case studies have been published that demonstrate this potential (Nanba et al., 2004; Teufel et al., 2006b)[CEPF]. However, fine-grained citation analysis is currently not widely us"
C12-1082,W06-1613,0,0.12081,"d retrieval (Ritchie et al., 2006)[CJPF], and building integrated research databases (Nanba et al., 2004)[CJPF]. Bibliometric measures that quantify the impact of publications (e.g., Moed, 2005)[CJPF] are also based on citations. Most of this work does not differentiate between uses of citations, e.g., whether a citation is more or less important to the paper or whether the paper’s authors support or refute the claims made in the cited work. However, recently a number of research groups have attempted to classify citations with respect to dimensions like importance and relation to cited work (Teufel et al., 2006b; Dong and Schäfer, 2011; Sugiyama et al., 2010; Abu-Jbara and Radev, 2012)[CEPF]. By adding such fine-grained information to individual citations, the various applications of citation analysis can be better served; e.g., citations that are foundational to a paper may constitute better summary sentences for the cited paper. Thus, there are clear potential benefits to fine-grained citation analysis; and a number of case studies have been published that demonstrate this potential (Nanba et al., 2004; Teufel et al., 2006b)[CEPF]. However, fine-grained citation analysis is currently not widely us"
C12-1082,H05-1044,0,0.0215746,"is motivated by the fact that citations 1349 at the end of the sentence are predominantly PERF. Frequency features. Dong and Schäfer (2011)[CEPF] used the number of citations in a single sentence (popularity) and in the citation sentence plus its neighboring sentences (density) as features. They also included a third feature for the average density of neighboring sentences (avgDensity). Sentiment features. Athar (2011)[CEPF] included two different polarity lexicons. One is handcrafted and specific to the scientific domain (scilex). The other is the large general purpose polarity lexicon from Wilson et al. (2005)[OEPF] (cpol). He also tried features (neg) that account for negation. This was done by appending “_neg” to the end of the 15 lexical items that follow any negation term. We were not able to obtain the scientific polarity lexicon, but use the polarity lexicon from Wilson et al. (2005)[OEPF] to extract sentiment features. Our polarity features are represented as a bag of words (BOW) where the citation context words present in the polarity lexicon are added to the BOW features positive-words or negative-words according to their polarity. Although CONF-NEG is not strictly a matter of sentiment, w"
C12-2056,P08-1031,0,0.0440826,"Missing"
C12-2056,D08-1083,0,0.123906,"cy classification, the format of syntactic constructions and the extraction of training examples. We then present experimental results for polarity classification (Section 4). The second part of this paper describes our method for automatically extracting PRCs (Section 5), and evaluates their usefulness (Section 6). Finally, we conclude and outline future work. 2 Related Work Negations, or, more generally, polarity reversers, create inconsistent words which are a major source of errors for polarity classification. Polarity reversers are diverse and do not include only negation function words (Choi and Cardie, 2008). Thus, some treatment of inconsistent words in polarity classification is common; for a survey see Wiegand et al. (2010). Most approaches for polarity classification work on word-level and simply consider a word w as inconsistent if it is preceded by a word out of a fixed list of polarity reversers, this includes rule-based (Polanyi and Zaenen, 2004; Hu and Liu, 2004) as well as statistical approaches (Pang et al., 2002). Unlike these approaches, we use syntactic information. Some approaches go beyond word-level, e.g., Wilson et al. (2005) use special features to model the existence of polari"
C12-2056,W10-3110,0,0.0126527,"ntegrate polarity reversing words into a dependency tree based method. While these works include some syntactic information, they still use a manually defined list of polarity reversing words. In contrast, we use machine learning to identify polarity reversing constructions (PRCs). 1 Note that our terminology differs from that used by (Dragut et al., 2012) who use the term “inconsistent” to refer to a word that has conflicting polarity information in a sentiment dictionary or across dictionaries. 570 An important challenge that most approaches ignore is the detection of the scope of negation. Councill et al. (2010) use dependency parses to predict the scope of polarity reversing words. Our approach goes the opposite way: given a sentiment word, we determine if it is in the scope of any PRC. Our definition of syntactic constructions explicitly includes scope. The work most closely related to our approach is (Ikeda et al., 2008) who also address the task of inconsistency classification. Their inconsistency classifier uses the local context of three words to the left and right of the target sentiment word as features. Li et al. (2010) extend that method to document level by stacking two classifiers trained"
C12-2056,P12-1105,0,0.023917,"go beyond word-level, e.g., Wilson et al. (2005) use special features to model the existence of polarity modifiers in the syntactic context of a sentiment word, Choi and Cardie (2008) use syntactic patterns to treat content negators, and Nakagawa et al. (2010) integrate polarity reversing words into a dependency tree based method. While these works include some syntactic information, they still use a manually defined list of polarity reversing words. In contrast, we use machine learning to identify polarity reversing constructions (PRCs). 1 Note that our terminology differs from that used by (Dragut et al., 2012) who use the term “inconsistent” to refer to a word that has conflicting polarity information in a sentiment dictionary or across dictionaries. 570 An important challenge that most approaches ignore is the detection of the scope of negation. Councill et al. (2010) use dependency parses to predict the scope of polarity reversing words. Our approach goes the opposite way: given a sentiment word, we determine if it is in the scope of any PRC. Our definition of syntactic constructions explicitly includes scope. The work most closely related to our approach is (Ikeda et al., 2008) who also address"
C12-2056,J93-1003,0,0.0530056,"ision for a sentiment word. M I(x, C) between candidate x and the classes C = {consistent, inconsistent} is defined as M I(x, C) = X P(x, c) log2 c∈C P(x, c) P(x) · P(c) + X c∈C P(¯ x , c) log2 P(¯ x , c) P(¯ x ) · P(c) (2) where P(x) is the probability that x occurred, and P(¯ x ) the probability that x didn’t occur. The n candidates with the highest scores are taken as PRCs. MI extracts candidates that serve as a good indicator for one of the classes, but not necessarily for the class inconsistent. For the MI+ score, we remove candidates with negative association from the final set of PRCs (Dunning, 1993). 6 Experiments with PRCs 6.1 Results of PRC extraction For the robust extraction of PRCs we need more annotated sentences than the customer review corpus contains. As there is no such corpus in the domain and to avoid manual annotation effort, we use semistructured reviews in which users provide pros (product aspects the user evaluates as positive) and cons (product aspects the user evaluates as negative) in addition to the written text of the review. We automatically create a corpus annotated with polarity at the sentence level as follows: All pros (resp. cons) longer than 3 tokens are extra"
C12-2056,I08-1039,0,0.554079,"om that used by (Dragut et al., 2012) who use the term “inconsistent” to refer to a word that has conflicting polarity information in a sentiment dictionary or across dictionaries. 570 An important challenge that most approaches ignore is the detection of the scope of negation. Councill et al. (2010) use dependency parses to predict the scope of polarity reversing words. Our approach goes the opposite way: given a sentiment word, we determine if it is in the scope of any PRC. Our definition of syntactic constructions explicitly includes scope. The work most closely related to our approach is (Ikeda et al., 2008) who also address the task of inconsistency classification. Their inconsistency classifier uses the local context of three words to the left and right of the target sentiment word as features. Li et al. (2010) extend that method to document level by stacking two classifiers trained on reversed and nonreversed sentences. Both works use only word-level information in their classifiers. We go beyond word-level and use syntactic constructions. We also attempt to explicitly identify and extract the syntactic constructions that are responsible for making a sentiment word inconsistent. 3 Approach The"
C12-2056,C10-1072,0,0.020599,"most approaches ignore is the detection of the scope of negation. Councill et al. (2010) use dependency parses to predict the scope of polarity reversing words. Our approach goes the opposite way: given a sentiment word, we determine if it is in the scope of any PRC. Our definition of syntactic constructions explicitly includes scope. The work most closely related to our approach is (Ikeda et al., 2008) who also address the task of inconsistency classification. Their inconsistency classifier uses the local context of three words to the left and right of the target sentiment word as features. Li et al. (2010) extend that method to document level by stacking two classifiers trained on reversed and nonreversed sentences. Both works use only word-level information in their classifiers. We go beyond word-level and use syntactic constructions. We also attempt to explicitly identify and extract the syntactic constructions that are responsible for making a sentiment word inconsistent. 3 Approach The main component of our approach is the inconsistency classifier, that assigns a score sincons (w) to each sentiment word token w in context, and classifies w as being inconsistent (sincons (w) &gt; 0) or consiste"
C12-2056,N03-5008,0,0.042804,"Missing"
C12-2056,N10-1120,0,0.0271954,"es for polarity classification work on word-level and simply consider a word w as inconsistent if it is preceded by a word out of a fixed list of polarity reversers, this includes rule-based (Polanyi and Zaenen, 2004; Hu and Liu, 2004) as well as statistical approaches (Pang et al., 2002). Unlike these approaches, we use syntactic information. Some approaches go beyond word-level, e.g., Wilson et al. (2005) use special features to model the existence of polarity modifiers in the syntactic context of a sentiment word, Choi and Cardie (2008) use syntactic patterns to treat content negators, and Nakagawa et al. (2010) integrate polarity reversing words into a dependency tree based method. While these works include some syntactic information, they still use a manually defined list of polarity reversing words. In contrast, we use machine learning to identify polarity reversing constructions (PRCs). 1 Note that our terminology differs from that used by (Dragut et al., 2012) who use the term “inconsistent” to refer to a word that has conflicting polarity information in a sentiment dictionary or across dictionaries. 570 An important challenge that most approaches ignore is the detection of the scope of negation"
C12-2056,W02-1011,0,0.0121014,"create inconsistent words which are a major source of errors for polarity classification. Polarity reversers are diverse and do not include only negation function words (Choi and Cardie, 2008). Thus, some treatment of inconsistent words in polarity classification is common; for a survey see Wiegand et al. (2010). Most approaches for polarity classification work on word-level and simply consider a word w as inconsistent if it is preceded by a word out of a fixed list of polarity reversers, this includes rule-based (Polanyi and Zaenen, 2004; Hu and Liu, 2004) as well as statistical approaches (Pang et al., 2002). Unlike these approaches, we use syntactic information. Some approaches go beyond word-level, e.g., Wilson et al. (2005) use special features to model the existence of polarity modifiers in the syntactic context of a sentiment word, Choi and Cardie (2008) use syntactic patterns to treat content negators, and Nakagawa et al. (2010) integrate polarity reversing words into a dependency tree based method. While these works include some syntactic information, they still use a manually defined list of polarity reversing words. In contrast, we use machine learning to identify polarity reversing cons"
C12-2056,W10-3111,0,0.406315,"ntal results for polarity classification (Section 4). The second part of this paper describes our method for automatically extracting PRCs (Section 5), and evaluates their usefulness (Section 6). Finally, we conclude and outline future work. 2 Related Work Negations, or, more generally, polarity reversers, create inconsistent words which are a major source of errors for polarity classification. Polarity reversers are diverse and do not include only negation function words (Choi and Cardie, 2008). Thus, some treatment of inconsistent words in polarity classification is common; for a survey see Wiegand et al. (2010). Most approaches for polarity classification work on word-level and simply consider a word w as inconsistent if it is preceded by a word out of a fixed list of polarity reversers, this includes rule-based (Polanyi and Zaenen, 2004; Hu and Liu, 2004) as well as statistical approaches (Pang et al., 2002). Unlike these approaches, we use syntactic information. Some approaches go beyond word-level, e.g., Wilson et al. (2005) use special features to model the existence of polarity modifiers in the syntactic context of a sentiment word, Choi and Cardie (2008) use syntactic patterns to treat content"
C12-2056,H05-1044,0,0.301278,"se and do not include only negation function words (Choi and Cardie, 2008). Thus, some treatment of inconsistent words in polarity classification is common; for a survey see Wiegand et al. (2010). Most approaches for polarity classification work on word-level and simply consider a word w as inconsistent if it is preceded by a word out of a fixed list of polarity reversers, this includes rule-based (Polanyi and Zaenen, 2004; Hu and Liu, 2004) as well as statistical approaches (Pang et al., 2002). Unlike these approaches, we use syntactic information. Some approaches go beyond word-level, e.g., Wilson et al. (2005) use special features to model the existence of polarity modifiers in the syntactic context of a sentiment word, Choi and Cardie (2008) use syntactic patterns to treat content negators, and Nakagawa et al. (2010) integrate polarity reversing words into a dependency tree based method. While these works include some syntactic information, they still use a manually defined list of polarity reversing words. In contrast, we use machine learning to identify polarity reversing constructions (PRCs). 1 Note that our terminology differs from that used by (Dragut et al., 2012) who use the term “inconsist"
C14-1029,C94-2167,0,0.376005,"erature. We exclude comparatives and superlatives as modifying adjectives because they are rarely used attributively in patents and usually modify quantities or qualities of terms(e.g., “higher shunt currents”); in other words, only “positive” (base-form) adjectives are included in our definition. Note that the number of tokens per term is not restricted by the definition. Our approach aims to find terms of arbitrary length. Part (ii) of the definition restricts terms to be specific to a domain D. We can set D to a general domain like ‘electricity’ and be on a par with many prior definitions (Ananiadou, 1994; Georgantopoulos and Piperidis, 2000; Zhang and Fang, 2010), but we can also set D to a narrow domain like ‘emergency protective circuit arrangements’ (IPC code1 H02H). Here, we choose the most general technical domain possible: the domain of all technical subjects. This is a good setting for many downstream tasks, e.g., information retrieval should benefit from a broad coverage of D. It also makes annotation easier: Non-experts can carry it out with good agreement (Section 5.1) because they simply look for all technical expressions. The syntactic and semantic parts of our definition of term"
C14-1029,C10-1011,0,0.0379453,"computation and models dependence of decisions correctly; but it lacks the more ‘global’ view of ATAS-TC on entire candidates. We want to investigate which approach is more suited for ATA. In what follows we describe how we preprocess patents, the linguistic filters used to implement our syntactic definition of term, automatic labeling of training data (step (i) of ATAS), training of term candidate classifier and CRF (step (ii) of ATAS), features and feature selection. 292 4.1 Preprocessing The preprocessing pipeline consists of the ANNIE tokenizer, OpenNLP sentence splitter, Mate POS tagger (Bohnet (2010), retrained for patents) and Mate lemmatizer. Preprocessing has a big influence on computational terminology because special domain text poses problems for off-the-shelf components. For example, patents tend to use common language words in rare functions or meanings, e.g., “said” as a de facto determiner in contexts such as “the structure of said component”. Other problems are the use of special language words, e.g., substances like “triphenylphosphine” and acronyms like “AC”. Such properties pose serious problems to POS taggers. Patent citations, acronyms and even product names can include pu"
C14-1029,georgantopoulos-piperidis-2000-term,0,0.0574038,"de comparatives and superlatives as modifying adjectives because they are rarely used attributively in patents and usually modify quantities or qualities of terms(e.g., “higher shunt currents”); in other words, only “positive” (base-form) adjectives are included in our definition. Note that the number of tokens per term is not restricted by the definition. Our approach aims to find terms of arbitrary length. Part (ii) of the definition restricts terms to be specific to a domain D. We can set D to a general domain like ‘electricity’ and be on a par with many prior definitions (Ananiadou, 1994; Georgantopoulos and Piperidis, 2000; Zhang and Fang, 2010), but we can also set D to a narrow domain like ‘emergency protective circuit arrangements’ (IPC code1 H02H). Here, we choose the most general technical domain possible: the domain of all technical subjects. This is a good setting for many downstream tasks, e.g., information retrieval should benefit from a broad coverage of D. It also makes annotation easier: Non-experts can carry it out with good agreement (Section 5.1) because they simply look for all technical expressions. The syntactic and semantic parts of our definition of term correspond to the concepts of unithoo"
C14-1029,P09-1113,0,0.0142517,"nd proteins are often single-token abbreviations. 3.1 Training Data Collection One of our main contributions is unsupervised training data generation (Section 4.3). Prior work has used automatically recognized training data for computational terminology, specifically for ATR (Craven and Kumlien, 1999; Hatzivassiloglou et al., 2001; Morgan et al., 2003; Zhang et al., 2010) in the biomedical domain. Given large precompiled term lists they search for occurrences of list elements, e.g., genes, in texts and use the occurrences they find as training examples. This is similar to distant supervision (Mintz et al., 2009) which also uses pre-existing resources such as gazetteers for, e.g., relation extraction. In contrast, our method is applied to ATA for the technological domain and does not rely on precompiled resources – we make use of figure references, which are an inherent part of patents. Our method can be characterized as training data identification: we exploit given conditions in patents for our search of training data. In contrast, training data recognition methods need precompiled resources as input and search for instances of resource elements in texts. 3.2 Learning Algorithms and Features Differe"
C14-1029,W03-1301,0,0.044774,"biomedical and technological domains are different. In biomedicine, categories like DNA and protein dominate. For these terms, shape features are informative – in contrast to terms in patents. Another difference is that terms in patents tend to be long whereas DNA and proteins are often single-token abbreviations. 3.1 Training Data Collection One of our main contributions is unsupervised training data generation (Section 4.3). Prior work has used automatically recognized training data for computational terminology, specifically for ATR (Craven and Kumlien, 1999; Hatzivassiloglou et al., 2001; Morgan et al., 2003; Zhang et al., 2010) in the biomedical domain. Given large precompiled term lists they search for occurrences of list elements, e.g., genes, in texts and use the occurrences they find as training examples. This is similar to distant supervision (Mintz et al., 2009) which also uses pre-existing resources such as gazetteers for, e.g., relation extraction. In contrast, our method is applied to ATA for the technological domain and does not rely on precompiled resources – we make use of figure references, which are an inherent part of patents. Our method can be characterized as training data ident"
C14-1029,C00-2137,0,0.0271829,"stently and considerably better than all baselines in all settings. E.g., line 6 shows system F1 on T ltest of ATAS-TC (.765 for S-SEL, .689 for U-SEL) and ATAS-CRF (.783 for S-SEL, .654 for U-SEL) compared to Z-CRF (.631), FRTC (.430), and the C-value baseline (.350) . The better results mainly come from higher recall (except for C-value, which is also beaten in precision). In general, precision of the baselines is higher, but recall much smaller than for ATAS. This shows that (i) statistical classifiers can be successfully trained for ATA using our method 7 We use approximate randomization (Yeh, 2000) for all significance tests in this paper. 297 Figure 1: System F1 as a function of training set size (in percent) in setting G. for automatically generating training data and (ii) these classifiers beat a state-of-the-art system in both S-SEL and U-SEL settings. Comparing S-SEL and U-SEL shows that precision and recall for U-SEL are lower than for S-SEL. For instance, F1 of ATAS-TC on T ltest is .765 for S-SEL and .689 for U-SEL; F1 of ATAS-CRF is .783 for S-SEL and .654 for U-SEL (line 6). In general, we note a bigger drop in recall than in precision, indicating that U-SEL does not generaliz"
C14-1029,Y10-1067,0,0.148154,"logical domains are different. In biomedicine, categories like DNA and protein dominate. For these terms, shape features are informative – in contrast to terms in patents. Another difference is that terms in patents tend to be long whereas DNA and proteins are often single-token abbreviations. 3.1 Training Data Collection One of our main contributions is unsupervised training data generation (Section 4.3). Prior work has used automatically recognized training data for computational terminology, specifically for ATR (Craven and Kumlien, 1999; Hatzivassiloglou et al., 2001; Morgan et al., 2003; Zhang et al., 2010) in the biomedical domain. Given large precompiled term lists they search for occurrences of list elements, e.g., genes, in texts and use the occurrences they find as training examples. This is similar to distant supervision (Mintz et al., 2009) which also uses pre-existing resources such as gazetteers for, e.g., relation extraction. In contrast, our method is applied to ATA for the technological domain and does not rely on precompiled resources – we make use of figure references, which are an inherent part of patents. Our method can be characterized as training data identification: we exploit"
C14-1031,D13-1035,0,0.0318313,"(2008)) or dialog systems (e.g., Komatani et al. (2003)). Conversely, our work is concerned with predicting a ranking by expertise within a single task. Several publications have dealt with natural language processing related to games. Chen and Mooney (2008) investigate grounded language learning where commentary describing the speciﬁc course of a game is automatically generated. Commentator expertise is not taken into account in this study. Branavan et al. (2012) introduced a model for using game manuals to increase the strength of a computer playing the strategy video game Civilization II. Cadilhac et al. (2013) investigated the prediction of player actions in the strategy board game The Settlers of Catan. Our approach differs conceptually from theirs as their main focus lies on modeling concrete actions in the game (either predicting or learning them); our goal is to predict player strength, i.e., to learn to compare players among each other. Rather than explicitly modeling the game, commentary analysis aims to provide insight into speciﬁc thought processes. Work in psychology research by Pfau and Murphy (1988) showed the quality of chess players’ verbalization about positions is correlated signiﬁca"
C14-1031,W13-5011,0,0.0476001,"Missing"
C14-1031,P03-1033,0,0.0224526,"rs to be less familiar with such terms. However, it appears that they are frequently overused by weaker players. This also holds for metaphorical terms, such as fall or eat instead of capture. 6 Related Work The treatment of writer expertise in extralinguistic tasks in NLP has mostly focused on two problems: (i) retrieval of experts for speciﬁc areas – i.e., predicting the area of expertise of a writer (e.g., Tu et al. (2010; Kivim¨aki et al. (2013)); and (ii) using expert status in different downstream applications such as sentiment analysis (e.g., Liu et al. (2008)) or dialog systems (e.g., Komatani et al. (2003)). Conversely, our work is concerned with predicting a ranking by expertise within a single task. Several publications have dealt with natural language processing related to games. Chen and Mooney (2008) investigate grounded language learning where commentary describing the speciﬁc course of a game is automatically generated. Commentator expertise is not taken into account in this study. Branavan et al. (2012) introduced a model for using game manuals to increase the strength of a computer playing the strategy video game Civilization II. Cadilhac et al. (2013) investigated the prediction of pl"
C14-1031,C10-2145,0,0.0232826,"blunder – a grave mistake, tempo, checkmate – experts use mate, opening, castle) actually indicate a weaker player. This seems counterintuitive at ﬁrst, as we may expect lower-rated players to be less familiar with such terms. However, it appears that they are frequently overused by weaker players. This also holds for metaphorical terms, such as fall or eat instead of capture. 6 Related Work The treatment of writer expertise in extralinguistic tasks in NLP has mostly focused on two problems: (i) retrieval of experts for speciﬁc areas – i.e., predicting the area of expertise of a writer (e.g., Tu et al. (2010; Kivim¨aki et al. (2013)); and (ii) using expert status in different downstream applications such as sentiment analysis (e.g., Liu et al. (2008)) or dialog systems (e.g., Komatani et al. (2003)). Conversely, our work is concerned with predicting a ranking by expertise within a single task. Several publications have dealt with natural language processing related to games. Chen and Mooney (2008) investigate grounded language learning where commentary describing the speciﬁc course of a game is automatically generated. Commentator expertise is not taken into account in this study. Branavan et al."
C14-1031,H05-1044,0,0.0035202,"ratings. (iii) Many terms related to elementary tactics (e.g., pin, fork) indicate lowerrated players, whereas terms relating to tactical foresight (e.g., threat, danger, stop) as well as positional terms (e.g., weakness, light and dark squares, variation) indicate higher-rated players. Emotions. A popular and wide-spread claim is that weaker chess players often lose because they are too emotionally invested in the game and thus get carried away (e.g., Cleveland (1907), Silman (1999)). We experimented with a sentiment feature, counting polar terms in the annotations using a polarity lexicon (Wilson et al., 2005). However, this feature did not improve our results. Manual examination of features expressing sentiment reveals that both amateurs and experts use subjective terms. We note that the vocabulary of subjective expressions is very constrained for stronger 318 players while it is open for weaker ones. Expert players tend to assess positions as winning or losing for a side, whereas weaker players tend to use terms such as like and hate. Both terms are identiﬁed as indicators of the respective strength class in our models. Other subjective assessments (e.g., good and bad) are divided among the class"
C16-1164,D13-1160,0,0.766093,"entities as subject are then the fact search space for this question. CharCNN and word-CNN decompose each question-fact match into an entity-mention surface-form match and a predicate-pattern semantic match. Our approach has a simple architecture, but it outperforms the state-of-the-art, a system that has a much more complicated structure. 2 Related Work As mentioned in Section 1, factoid QA against Freebase can be categorized into single-relation QA and multi-relation QA. Much work has been done on multi-relation QA in the past decade, especially after the release of benchmark WebQuestions (Berant et al., 2013). Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing th"
C16-1164,D14-1067,0,0.48883,"work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in"
C16-1164,P16-1076,0,0.692821,"m by an embedding-based QA system developed under the framework of Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015). The setting of the SimpleQA corresponds to the elementary operation of performing a single lookup in the memory. They investigate the performance of training on the combination of SimpleQuestions, WebQuestions and Reverb training sets. Golub and He (2016) propose a character-level attention-based encoder-decoder framework to encode the question and subsequently decode into (subject, predicate) tuple. Our model in this work is much simpler than these prior systems. Dai et al. (2016) combine a unified conditional probabilistic framework with deep recurrent neural networks and neural embeddings to get state-of-the-art performance. Treating SimpleQA as fact selection is inspired by work on answer selection (e.g., Yu et al. (2014), Yin et al. (2016b), Santos et al. (2016)) that looks for the correct answer(s) from some candidates for a given question. The answer candidates in those tasks are raw text, not structured information as facts in Freebase are. We are also inspired by work that generates natural language questions given knowledge graph facts (Seyler et al., 2015; Se"
C16-1164,P15-1026,0,0.370013,"form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word"
C16-1164,D11-1142,0,0.0153623,"h the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word is first preprocessed into a count vector of character-trigram vocabulary, then forwarded into the CNN as input. We treat entities and mentions as character sequences. Our char-CNN for entity-mention match is more end-to-end without data preprocessing. (ii)"
C16-1164,P13-1158,0,0.0602297,"eebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a question, such as “what’s the hometown of Obama?”, asks a single and direct topic of an entity. In this example, the entity is “Obama” and the topic is hometown. So our task is reduced to finding one fact (subject, predicate, object) in Freebase that answers the question, which roughly means the subject and predicate are the best matches for the topical entity “Obama” and for the topic description “what’s the hometown of”, respectively. Thus, we aim to des"
C16-1164,D16-1166,0,0.655535,"system, given a question, is asked to choose the best answer from a list of candidates. In this work, we formulate the SimpleQA task as a fact selection problem and the key issue lies in the system design for how to match a fact candidate to the question. The first obstacle is that Freebase has an overwhelming number of facts. A common and effective way is to first conduct entity linking of a question over Freebase, so that only a small subset of facts remain as candidates. Prior work achieves entity linking by searching word n-grams of a question among all entity names (Bordes et al., 2015; Golub and He, 2016). Then, facts whose subject entities match those n-grams are kept. Our first contribution in this work is to present a simple while effective entity linker ∗ 1 This work was conducted during the first author’s internship at IBM Watson Group. We release our entity linking results at: https://github.com/Gorov/SimpleQuestions-EntityLinking This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1746 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers"
C16-1164,P14-1090,0,0.301761,"Missing"
C16-1164,W14-2416,0,0.0221327,"Missing"
C16-1164,N15-3014,0,0.0599456,"c parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashin"
C16-1164,P14-2105,0,0.095584,"e based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a"
C16-1164,P15-1128,0,0.592067,"y linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a que"
C16-1164,W16-0103,1,0.856224,"Missing"
C16-1164,Q16-1019,1,0.0271245,"Missing"
C16-1164,N03-1033,0,\N,Missing
C16-1164,Q14-1002,1,\N,Missing
C16-1239,P15-1166,0,0.0148295,"g systems (Roth and Yih, 2004; Kate and Mooney, 2010; Miwa and Sasaki, 2014) are complex feature-based models for joint entity and relation extraction. The most related work to our method is (Miwa and Sasaki, 2014); however they employ complex search heuristics (Goldberg and Elhadad, 2010; Stoyanov and Eisner, 2012) to fill the entity-relation table based on structured prediction method. They explicitly model the label dependencies and their joint approach is not based on neural networks. Multi-task learning (Caruana, 1998) via neural networks (Zhang and Yeung, 2012; Seltzer and Droppo, 2013; Dong et al., 2015; Li and J, 2014; Collobert and Weston, 2008) have been used to model relationships among the correlated tasks. Therefore, we present a unified neural network based multi-task framework to model the entity-relation table for end-to-end relation extraction. 6 Conclusion We proposed TF-MTRNN, a novel architecture that jointly models entity and relation extraction, and showed how an entity-relation table is mapped to a neural network framework that learns label interdependencies. We introduced word-level relation classification through composition of words; this is advantageous in modeling multip"
C16-1239,P15-1061,0,0.0274725,"1 w3 w4 w5 TF-MTRNN M1 for NER w5 U-* w1 L-* w1 w5 w1 R w2 w2 L-* Predicted NE entries on diagonal. from non-candidate pairs. ?: To be determined from RC. U-* w1 ... w4 w5 O, O, w4 B-* ? w3 U-*, w3 O w4 w5 w4 O w3 w2 w3 U-* w2 w1 w2 w5 TF-MTRNN M2 for RC M1 and M2 independent. No parameter sharing. B-*, R L-*, Entity-Relation Table Filled from word pair composition of entity types: (L-*,L-*), (L-*, U-*), (U-*,U-*). Figure 9: Pipeline Approach in End-to-End Relation Extraction. 2.5 Ranking Bi-directional Recurrent Neural Network (R-biRNN) Ranking loss has been used in neural architectures (dos Santos et al., 2015) and (Vu et al., 2016b) to handle artificial classes. In our experiments, for a given sentence x with class label y ` , the competitive class c´ is chosen the one with the highest score among all competitive classes during SGD step. The basic principle is to learn to maximize the distance between the true label y ` and the best competitive label c´ for a given data point x. We use the ranking loss to handle the two artificial classes i.e. ‘O’ and K in entity and relation types, respectively. The ranking objective function is defined asL “ logp1 ` exppγpm` ´ sθ pxqy` qqq ` logp1 ` exppγpm´ ` sθ"
C16-1239,N10-1115,0,0.0113703,"the interdependencies of entity and relation, do not handle multiple relation instances in a sentence and therefore, can not detect entity mention pairs for the sentence-level relations. Our approach is a joint entity and word-level relation extraction capable to model multiple relation instances, without knowing nominal pairs. Existing systems (Roth and Yih, 2004; Kate and Mooney, 2010; Miwa and Sasaki, 2014) are complex feature-based models for joint entity and relation extraction. The most related work to our method is (Miwa and Sasaki, 2014); however they employ complex search heuristics (Goldberg and Elhadad, 2010; Stoyanov and Eisner, 2012) to fill the entity-relation table based on structured prediction method. They explicitly model the label dependencies and their joint approach is not based on neural networks. Multi-task learning (Caruana, 1998) via neural networks (Zhang and Yeung, 2012; Seltzer and Droppo, 2013; Dong et al., 2015; Li and J, 2014; Collobert and Weston, 2008) have been used to model relationships among the correlated tasks. Therefore, we present a unified neural network based multi-task framework to model the entity-relation table for end-to-end relation extraction. 6 Conclusion We"
C16-1239,W10-2924,0,0.171306,"ased in between ORG and LOC, while Located In between LOC and LOC entities. Inversely, for a given word with associated relation(s), the candidate entity types can be detected. For example, in Figure 2, for a given relation, say Located in, the candidate entity pair is (LOC, LOC). Therefore, the two tasks are interdependent and optimising a single network for ER and RC to model the interdependencies in the candidate entity pairs and corresponding relations is achieved via the proposed joint modeling of subtasks and a simple piggybacking approach. Joint learning approaches (Roth and Yih, 2004; Kate and Mooney, 2010) built joint models upon complex multiple individual models for the subtasks. (Miwa and Sasaki, 2014) proposed a joint entity and relation extraction approach using a history-based structured learning with a table representation; however, they explicitly incorporate entity-relation label interdependencies, use complex features and search heuristics to fill table. In addition, their state-of-the-art method is structured prediction and not based on neural network frameworks. However, deep learning methods such as recurrent and convolutional neural networks (Zeng et al., 2014; Zhang and Wang, 201"
C16-1239,N16-1030,0,0.0164744,"Missing"
C16-1239,P14-1038,0,0.410263,"Missing"
C16-1239,D14-1200,0,0.720133,"rd with associated relation(s), the candidate entity types can be detected. For example, in Figure 2, for a given relation, say Located in, the candidate entity pair is (LOC, LOC). Therefore, the two tasks are interdependent and optimising a single network for ER and RC to model the interdependencies in the candidate entity pairs and corresponding relations is achieved via the proposed joint modeling of subtasks and a simple piggybacking approach. Joint learning approaches (Roth and Yih, 2004; Kate and Mooney, 2010) built joint models upon complex multiple individual models for the subtasks. (Miwa and Sasaki, 2014) proposed a joint entity and relation extraction approach using a history-based structured learning with a table representation; however, they explicitly incorporate entity-relation label interdependencies, use complex features and search heuristics to fill table. In addition, their state-of-the-art method is structured prediction and not based on neural network frameworks. However, deep learning methods such as recurrent and convolutional neural networks (Zeng et al., 2014; Zhang and Wang, 2015; Nguyen and Grishman, 2015) treat relation This work is licensed under a Creative Commons Attributi"
C16-1239,W15-1506,0,0.0600004,"ilt joint models upon complex multiple individual models for the subtasks. (Miwa and Sasaki, 2014) proposed a joint entity and relation extraction approach using a history-based structured learning with a table representation; however, they explicitly incorporate entity-relation label interdependencies, use complex features and search heuristics to fill table. In addition, their state-of-the-art method is structured prediction and not based on neural network frameworks. However, deep learning methods such as recurrent and convolutional neural networks (Zeng et al., 2014; Zhang and Wang, 2015; Nguyen and Grishman, 2015) treat relation This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 Entity Recognition (ER) = Entity Extraction (EE); Relation Classification (RC) = Relation Extraction (RE) 2537 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2537–2547, Osaka, Japan, December 11-17 2016. LOC PER PER ORGBased_In Located_In Work_For ORGBased_In ORG LOC LOC Figure 1: An entity and relation example (CoNLL04 data). PER: Person, ORG: Organization, LOC:"
C16-1239,W04-2401,0,0.761581,"d ORG entities, ORGBased in between ORG and LOC, while Located In between LOC and LOC entities. Inversely, for a given word with associated relation(s), the candidate entity types can be detected. For example, in Figure 2, for a given relation, say Located in, the candidate entity pair is (LOC, LOC). Therefore, the two tasks are interdependent and optimising a single network for ER and RC to model the interdependencies in the candidate entity pairs and corresponding relations is achieved via the proposed joint modeling of subtasks and a simple piggybacking approach. Joint learning approaches (Roth and Yih, 2004; Kate and Mooney, 2010) built joint models upon complex multiple individual models for the subtasks. (Miwa and Sasaki, 2014) proposed a joint entity and relation extraction approach using a history-based structured learning with a table representation; however, they explicitly incorporate entity-relation label interdependencies, use complex features and search heuristics to fill table. In addition, their state-of-the-art method is structured prediction and not based on neural network frameworks. However, deep learning methods such as recurrent and convolutional neural networks (Zeng et al., 2"
C16-1239,C12-1154,0,0.0139782,"ity and relation, do not handle multiple relation instances in a sentence and therefore, can not detect entity mention pairs for the sentence-level relations. Our approach is a joint entity and word-level relation extraction capable to model multiple relation instances, without knowing nominal pairs. Existing systems (Roth and Yih, 2004; Kate and Mooney, 2010; Miwa and Sasaki, 2014) are complex feature-based models for joint entity and relation extraction. The most related work to our method is (Miwa and Sasaki, 2014); however they employ complex search heuristics (Goldberg and Elhadad, 2010; Stoyanov and Eisner, 2012) to fill the entity-relation table based on structured prediction method. They explicitly model the label dependencies and their joint approach is not based on neural networks. Multi-task learning (Caruana, 1998) via neural networks (Zhang and Yeung, 2012; Seltzer and Droppo, 2013; Dong et al., 2015; Li and J, 2014; Collobert and Weston, 2008) have been used to model relationships among the correlated tasks. Therefore, we present a unified neural network based multi-task framework to model the entity-relation table for end-to-end relation extraction. 6 Conclusion We proposed TF-MTRNN, a novel"
C16-1239,P10-1040,0,0.0159761,"context awareness (Figure 5); p’backing: piggybacking predicted and candidate entities in RE and NER, respectively; ensemble: majority vote. 3.2 Figure 11: T-SNE view of the semantic entity-relation space for the combined hidden representations of each word pair composition. Relations: (0:LIVEIN, 1:ORGBASEDIN, 2:LOCATEDIN, 3:WORKFOR, 4:KILL, 5:NORELATION). Entity-pair and relation denoted by E1-RELATION-E2 and/or count in [0, 1, 2, 3, 4, 5]. 5: misclassified entity-pairs. Word Representation and Features Each word is represented by concatenation of pre-trained 50-dimensional word embeddings3 (Turian et al., 2010) with N-gram, part-of-speech (POS), capital feature (CF: all-capitalized; initial-capitalized) and piggybacked entity vectors (Section 2.4). The word embeddings are shared across entity and relation extraction tasks and are adapted by updating them during training. We use 7-gram (wt´3 wt´2 wt´1 wt wt`1 wt`2 wt`3 q obtained by concatenating corresponding word embeddings. 3.3 State Machine driven Multi-tasking Multi-task training is performed via switching across multiple tasks in a block of training steps. However, we perform switches between ER and RC subtasks based on the performance of each"
C16-1239,N16-1065,1,0.84114,"Missing"
C16-1239,C14-1220,0,0.0588232,"and Yih, 2004; Kate and Mooney, 2010) built joint models upon complex multiple individual models for the subtasks. (Miwa and Sasaki, 2014) proposed a joint entity and relation extraction approach using a history-based structured learning with a table representation; however, they explicitly incorporate entity-relation label interdependencies, use complex features and search heuristics to fill table. In addition, their state-of-the-art method is structured prediction and not based on neural network frameworks. However, deep learning methods such as recurrent and convolutional neural networks (Zeng et al., 2014; Zhang and Wang, 2015; Nguyen and Grishman, 2015) treat relation This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 Entity Recognition (ER) = Entity Extraction (EE); Relation Classification (RC) = Relation Extraction (RE) 2537 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2537–2547, Osaka, Japan, December 11-17 2016. LOC PER PER ORGBased_In Located_In Work_For ORGBased_In ORG LOC LOC Figure 1: An entity and relation example (C"
C18-1200,W14-4012,0,0.0859746,"Missing"
C18-1200,E17-1013,0,0.294514,"on 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// order when used in compositional training schemes. For example, path query q1 = (h, r1 →r2 →· · · →rk , ?) will be encoded into the same embedding as path query q2 = (h, r2 →r1 →· · · →rk , ?), resulting in (often incorrect) prediction of the same tail entity. Instead, the relation order should influence the prediction. This limitation in modeling multi-hop relation paths motivates the RNN approach: using recurrent neural networks (RNN (Elman, 1990)) to model relation paths (Neelakantan et al., 2015). Das et al. (2017) further extend this approach by incorporating entity information and apply it to multi-hop KBC. Intermediate entities should influence the reasoning decision. For example, given two paths with the same relation sequence: (Donald Trump, child, Ivanka Trump, mother, Ivana Trump) and (Donald Trump, child, Barron Trump, mother, Melania Trump), even though both paths have the relation sequence [child, mother], the relation between (Donald Trump, Melania Trump) is “spouse” while it does not hold between (Donald Trump, Ivana Trump) due to the intermediate entities: “Ivanka Trump” vs. “Barron Trump”."
C18-1200,D15-1038,0,0.0848204,"i-hop paths, e.g., the path query (U.S.A, president→spouse→born in, ?) for the question “Where was the first lady of the United States born?” (ii) How can we address different KG reasoning problems driven by multi-hop paths in a universal paradigm rather than via different systems? (iii) How can we combine specific multi-hop KG reasoning tasks with generic KG representation learning, so that KG representation learning can either stand alone or be incorporated into diverse multi-hop KG-related NLU problems. We get inspiration from following two types of work. First, prior work in KG reasoning. Guu et al. (2015) extend one-hop reasoning regimes such as TransE (Bordes et al., 2013) to multi-hop PQA. However, these basic one-hop models do not encode the relation 1 https://github.com/yinwenpeng/KBPath This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// order when used in compositional training schemes. For example, path query q1 = (h, r1 →r2 →· · · →rk , ?) will be encoded into the same embedding as path query q2 = (h, r2 →r1 →· · · →rk , ?), resulting in (often incorrect) prediction of the same tail entity."
C18-1200,N16-1030,0,0.0569168,"hild, mother], the relation between (Donald Trump, Melania Trump) is “spouse” while it does not hold between (Donald Trump, Ivana Trump) due to the intermediate entities: “Ivanka Trump” vs. “Barron Trump”. Similarly, paths (JFK, located in, NYC, located in, NY) and (Yankee Stadium, located in, NYC, located in, NY) would predict the same score for target relation “airport serves place” if we do not consider that Yankee Stadium is not an airport (Das et al., 2017). Second, sequence labeling tasks such as POS tagging, chunking and NER have been successfully addressed by RNNs (Huang et al., 2015; Lample et al., 2016). These approaches model the mechanism in a structure of form “input1 , tag1 , input2 , tag2 , input3 , tag3 , · · · , inputt , tagt ”, which resembles the structure of multi-hop KG paths. Inspired by this prior work, we propose ROP, Recurrent One-hop Predictor. Given a head entity, ROP encodes a multi-hop sequence of relations and predicts a sequence of entities using an RNN. More formally, given relation sequence “r1 , r2 , · · · , rt ” and the head entity eh , ROP predicts the sequence e1 , e2 , · · · , et , thus generating a complete KG path eh , r1 , e1 , r2 , e2 , · · · , rt , et . Intui"
C18-1200,D11-1049,0,0.709781,"Missing"
C18-1200,D15-1082,0,0.25426,"rents/live in. mh-PQA tries to find answers to the path queries and hence compositional questions. Unfortunately, KGs often have missing facts (edges), which makes mh-PQA a non-trivial problem. A path query qt consists of an initial anchor entity, eh , followed by a sequence of t relations to be traversed, p = (r1 , · · · , rt ). Following (Guu et al., 2015), the answer or denotation of the query is the set of all entities that can be reached from eh by traversing p. 3 Related Work Here we focus on the multi-hop path reasoning literature. Some work (Neelakantan et al., 2015; Guu et al., 2015; Lin et al., 2015; Lin et al., 2016; Shen et al., 2016) does some composition over relation paths. Given relation path p = (r1 , · · · , rt ), the composition operation is add (p = r1 +· · ·+rt ), multiplication (p = r1 · · · rt ) or an RNN step: pi = RNN(pi−1 , ri ), where pi is the accumulated relation information up to step i. Some work explores compositional encoding of long paths (Lin et al., 2016; Shen et al., 2016), but still performs reasoning in one-hop scenario. Neelakantan et al. (2015) use RNNs to model multi-hop paths. Das et al. (2017) extend the RNN approaches by leveraging within-path entities"
C18-1200,N13-1095,0,0.0154196,"-paths of arbitrary lengths while updating the entity and relation representations by the training signal at each step; (ii) handling different types of mh-KG reasoning in a unified framework. Our models show state-of-the-art for two important multi-hop KG reasoning tasks: Knowledge Base Completion and Path Query Answering.1 1 Introduction Natural language understanding (NLU) is impossible without knowledge about the world. Large scale knowledge graphs (KGs) such as Freebase (Bollacker et al., 2008) are structures that store world knowledge. Unfortunately, KGs suffer from incomplete coverage (Min et al., 2013); e.g., Freebase contains Brandon Lee, but not his ethnicity. The knowledge in KGs needs to be expanded to cover more facts; reasoning is one way to do so. For example, we could infer that (Microsoft, ?, United States) instantiates “CountryOfHQ” given the facts (Microsoft, IsBasedIn, Seattle) and (Seattle, LocatedIn, United States); or we could infer Brandon Lee’s ethnicity from his parents’ ethnicity, i.e., answering the query (Brandon Lee, Ethnicity, ?) by facts (Brandon Lee, Father, Bruce Lee) and (Bruce Lee, Ethnicity, Chinese). We refer to the two reasoning examples as “knowledge base com"
C18-1200,P15-1016,0,0.385414,"Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// order when used in compositional training schemes. For example, path query q1 = (h, r1 →r2 →· · · →rk , ?) will be encoded into the same embedding as path query q2 = (h, r2 →r1 →· · · →rk , ?), resulting in (often incorrect) prediction of the same tail entity. Instead, the relation order should influence the prediction. This limitation in modeling multi-hop relation paths motivates the RNN approach: using recurrent neural networks (RNN (Elman, 1990)) to model relation paths (Neelakantan et al., 2015). Das et al. (2017) further extend this approach by incorporating entity information and apply it to multi-hop KBC. Intermediate entities should influence the reasoning decision. For example, given two paths with the same relation sequence: (Donald Trump, child, Ivanka Trump, mother, Ivana Trump) and (Donald Trump, child, Barron Trump, mother, Melania Trump), even though both paths have the relation sequence [child, mother], the relation between (Donald Trump, Melania Trump) is “spouse” while it does not hold between (Donald Trump, Ivana Trump) due to the intermediate entities: “Ivanka Trump”"
C18-1200,P16-1136,0,0.0233579,"explores compositional encoding of long paths (Lin et al., 2016; Shen et al., 2016), but still performs reasoning in one-hop scenario. Neelakantan et al. (2015) use RNNs to model multi-hop paths. Das et al. (2017) extend the RNN approaches by leveraging within-path entities into the encoding of inputs along with relations. We also include within-path entities, but we do not give them as inputs; instead, we force our RNN to predict them as outputs and do updates at each step in the path. This supports representation learning for KG entities and relations even without task-specific annotations. Toutanova et al. (2016) propose a dynamic programming algorithm to model both relation types and intermediate entities in the compositional path representations and test on WordNet and a biomedical KG. These two works address mh-KBC; for mh-PQA, there is no prior work on using within-path entities2 , including Das et al. (2017), in which the system uses within-path entities as input, while those entities are not available for testing. So Das et al. (2017) use RNN for mh-PQA to encode the relation sequence, but it does not incorporate the intermediate entities involved. 4 Recurrent One-Hop Prediction We propose three"
C18-1200,D15-1083,1,0.904555,"Missing"
D08-1096,biemann-etal-2004-automatic,0,0.195658,"lated work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly better than previously proposed categorical representations. An initial evaluation of high-order representations showed little improvement over low-order representations. In future work, we intend to investigate the influence of noise"
D08-1096,P06-3002,0,0.0131482,"eir results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work the CHILDES corpus, the evaluation shows that the graded syntactic repr"
D08-1096,J92-4003,0,0.0988842,"Missing"
D08-1096,E03-1009,0,0.0220407,"ent merits further investigation. 7 Related work Data-oriented parsing (Bod et al., 2003) shares basic assumptions about linguistic inference with exemplar-based theory, but it does not model or use the similarity between input and stored exemplars. Previous work on exemplar theory in syntax (AbbotSmith and Tomasello, 2006; Bybee, 2006; Hay and Bresnan, 2006) has not been computational or formal. Previous work on non-categorical representations of words has viewed these representations as an intermediate step for arriving at categorical parts of speech (Redington et al., 1998; Sch¨utze, 1995; Clark, 2003). Consequently, all of these papers evaluate their results by comparing induced categories to gold-standard parts of speech. Redington et al. (1998) did not find a difference in categorization accuracy between simple syntagmatic representation and those using non-adjacent words. The BEAGLE model (Jones and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a uni"
D08-1096,P97-1067,0,0.312425,"and Mewhort, 2007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly better than previously proposed categorical representations. An initial evaluation of high-order representations showed little improvement over low-order representations. In future work, we intend to"
D08-1096,C02-1007,0,0.0273177,"007), and related work (Sahlgren et al., 2008), merges cooccurrence information and word order information into a single composite vector through a process of vector convolution. Our model differs in that it explicitly captures the recursive relationship between the orders in a unified framework. Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains (Sch¨utze and Pedersen, 1993; Lund and Burgess, 1996; Edmonds, 1997; Rapp, 2002; Biemann et al., 2004; Lemaire and Denhi`ere, 2006). Toutanova et al. (2004) found that their graph model of predicate argument structure deteriorated after a small number of iterations of the random walk, similar to our findings. 8 Conclusions and Future Work the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly better than previously proposed categorical representations. An initial evaluation of high-order representations showed little improvement over low-order representations. In future work, we intend to investigate"
D08-1096,W97-0309,0,0.0420253,"Missing"
D08-1096,E95-1020,1,0.857425,"Missing"
D11-1073,W10-3704,0,0.0349914,"Missing"
D11-1073,W03-1812,0,0.0119494,"questions that arise are (i) which vectors to compare, (ii) how to combine the vectors of the parts and (iii) from what point on a certain dissimilarity indicates non-compositionality. To our knowledge, there are no generally accepted answers to these questions. Regarding (i), Schone and Jurafsky (2001) compare the semantic vector of a phrase p and the vectors of its component words in two ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not. Regarding (ii), they suggest weighted or unweighted sums of the semantic vectors of the parts. Baldwin et al. (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. They address (i) by comparing the semantic vectors of phrases with the vectors of their parts individually to detect meaning changes; e.g., they compare vice president to vice and president. We propose a new method that compares phrases with their alternative phrases, in the spirit of Lin (1999)’s substitution approach (see Section 4.3). Our rationale is that context features should be based on contexts that are syntactically similar to the phrase in question. With respect to (iii), the above-mentioned studies"
D11-1073,W09-2901,0,0.0439333,"Missing"
D11-1073,W07-1106,0,0.0145201,"udies use ad hoc thresholds to separate compositional and non-compositional phrases but do not offer a principled decision criterion.2 In contrast, we train a statistical classifier to learn a decision criterion. There is a larger body of work concerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2 Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. 795 a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase MWUs are fixed and exh"
D11-1073,W09-2903,0,0.0172151,"ning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2 Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. 795 a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase MWUs are fixed and exhibit no syntactic variability. In contrast, verb constructions are often discontinuous. The motivation for most work on MWU detection is lexicography, terminology extraction or the creation of machine-readable dictionaries. Our motivation – tokenization i"
D11-1073,P01-1025,0,0.0113988,"prepositional phrases with the word of followed by a non-modified, indefinite, singular noun, e.g., speed of light or moment of inertia. Out of all phrases extracted with part-of-speech patterns, we keep only the ones that appear more often than 50 times because it is hard to compute reliable features for less frequent phrases. All experiments were carried out with lemmatized word forms. We refer to lemmas as words if not noted otherwise. 4.2 Association measures Statistical association measures are frequently used for MWU detection and collocation extraction (e.g. Schone and Jurafsky (2001), Evert and Krenn (2001), Pecina (2010)). We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase’s contingency table. These measures are Student’s t-score, z-score, χ2 , pointwise mutual information (MI), Dice coefficient, frequency, log-likelihood (G2 ) and symmetric conditional probability. We define the AMs in Table 3 based on the notation for the contingency table shown in Table 2 (cf. Evert (2004)). Oij is observed frequency and Ri Cj Eij = N expected frequency. log-likelihood (amG2 ) symmetric conditional probability (amscp ) O11 2 P Oij log i,j Oij Eij O11 2 R1 C1 Table 3: Ass"
D11-1073,W06-1203,0,0.0223239,"o (iii), the above-mentioned studies use ad hoc thresholds to separate compositional and non-compositional phrases but do not offer a principled decision criterion.2 In contrast, we train a statistical classifier to learn a decision criterion. There is a larger body of work concerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2 Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. 795 a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase"
D11-1073,C10-2078,0,0.0116819,"which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2 Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. 795 a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase MWUs are fixed and exhibit no syntactic variability. In contrast, verb constructions are often discontinuous. The motivation for most work on MWU detection is lexicography, terminology extraction or the creation of machine-readable dictionaries. Our motivation – tokenization in a preprocessing setting"
D11-1073,P99-1041,0,0.309897,"ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not. Regarding (ii), they suggest weighted or unweighted sums of the semantic vectors of the parts. Baldwin et al. (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. They address (i) by comparing the semantic vectors of phrases with the vectors of their parts individually to detect meaning changes; e.g., they compare vice president to vice and president. We propose a new method that compares phrases with their alternative phrases, in the spirit of Lin (1999)’s substitution approach (see Section 4.3). Our rationale is that context features should be based on contexts that are syntactically similar to the phrase in question. With respect to (iii), the above-mentioned studies use ad hoc thresholds to separate compositional and non-compositional phrases but do not offer a principled decision criterion.2 In contrast, we train a statistical classifier to learn a decision criterion. There is a larger body of work concerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idioma"
D11-1073,ramisch-etal-2010-mwetoolkit,0,0.026998,"Missing"
D11-1073,W01-0513,0,0.0335455,"Missing"
D11-1073,J93-1007,0,0.139914,"Missing"
D11-1073,W06-2405,0,\N,Missing
D11-1143,D09-1031,0,0.0106767,"pplied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. Combined, the two approaches could substantially lower the cost of creating training sets. Our main contribution in this paper is that we show for the first time that AL is significantly better than randomly selected annotation examples in a real crowdsourcing annotation scenario. Our experiments directly address two tasks, named entity recognition and sentiment detection, but our Proceedings of the 2011 Conference on Empiric"
D11-1143,D09-1030,0,0.013013,"Missing"
D11-1143,J93-3001,0,0.0270986,"Missing"
D11-1143,W10-0713,0,0.0561458,"–31, 2011. 2011 Association for Computational Linguistics evidence suggests that AL is of general benefit in crowdsourcing. We also show that the effectiveness of MTurk annotation with AL can be further enhanced by using two techniques that increase label quality: adaptive voting and fragment recovery. 2 2.1 Related Work Crowdsourcing Pioneered by Snow et al. (2008), Crowdsourcing, especially using MTurk, has become a widely used service in the NLP community. A number of studies have looked at crowdsourcing for NER. Voyer et al. (2010) use a combination of expert and crowdsourced annotations. Finin et al. (2010) annotate Twitter messages – short sequences of words – and this is reflected in their vertically oriented user interface. Lawson et al. (2010) choose an annotation interface where annotators have to drag the mouse to select entities. Carpenter and Poesio (2010) argue that dragging is less convenient for workers than marking tokens. These papers do not address AL in crowdsourcing. Another important difference is that previous studies on NER have used data sets for which no “linguistic” gold annotation is available. In contrast, we reannotate the CoNLL-2003 English NER dataset. This allows us t"
D11-1143,W05-0619,0,0.0185845,"face. Lawson et al. (2010) choose an annotation interface where annotators have to drag the mouse to select entities. Carpenter and Poesio (2010) argue that dragging is less convenient for workers than marking tokens. These papers do not address AL in crowdsourcing. Another important difference is that previous studies on NER have used data sets for which no “linguistic” gold annotation is available. In contrast, we reannotate the CoNLL-2003 English NER dataset. This allows us to conduct a detailed comparison of MTurk AL to conventional expert annotation. 2.2 Active Learning with Noisy Labels Hachey et al. (2005) were among the first to investigate the effect of actively sampled instances on agreement of labels and annotation time. They demonstrate applicability of AL when annotators are trained experts. This is an important result. However, AL depends on accurate assessments of uncertainty and informativeness and such an accurate assessment is made more difficult if labels are noisy as is the case in crowdsourcing. For this reason, the problem of AL performance with noisy labels has become a topic of interest in the AL community. Rehbein et al. (2010) investigate AL with human expert annotators for w"
D11-1143,W10-0105,0,0.0148823,"hus, we perform the retraining and uncertainty rescoring concurrently with the annotation user interface. The unlabeled pool is stored in a priority queue that is ordered according to the examples’ informativeness. The annotation user interface takes the most informative example from the pool and presents it to the annotator. The labeled example is then inserted into a second queue that feeds and updates retraining and rescoring processes. The pool queue then is resorted according to the new informativeness. In this way, annotation and example selection can run in parallel. This is similar to Haertel et al. (2010). 3.2 Adaptive voting and fragment recovery MTurk labels often have a high error rate. A common strategy for improving label quality is to acquire multiple labels by different workers for each example and then consolidate the annotations into a single label of higher quality. To trade off number of annotated examples against quality of annotations, we adopt adaptive voting. It uses majority Budget RS 1 2 3 4 AL 5 6 7 8 S 3-v 5/4-v 5-v+f S 3-v 5/4-v 5-v+f #train 5820 1624 1488 1996 5820 1808 1679 2165 F1 59.6 61.4† 63.0† 63.6† 67.0 70.0† 70.4† 70.5 NER 5820 cost/sent w.-accuracy 1.00 51.6 3.58"
D11-1143,W10-0712,0,0.014385,"the effectiveness of MTurk annotation with AL can be further enhanced by using two techniques that increase label quality: adaptive voting and fragment recovery. 2 2.1 Related Work Crowdsourcing Pioneered by Snow et al. (2008), Crowdsourcing, especially using MTurk, has become a widely used service in the NLP community. A number of studies have looked at crowdsourcing for NER. Voyer et al. (2010) use a combination of expert and crowdsourced annotations. Finin et al. (2010) annotate Twitter messages – short sequences of words – and this is reflected in their vertically oriented user interface. Lawson et al. (2010) choose an annotation interface where annotators have to drag the mouse to select entities. Carpenter and Poesio (2010) argue that dragging is less convenient for workers than marking tokens. These papers do not address AL in crowdsourcing. Another important difference is that previous studies on NER have used data sets for which no “linguistic” gold annotation is available. In contrast, we reannotate the CoNLL-2003 English NER dataset. This allows us to conduct a detailed comparison of MTurk AL to conventional expert annotation. 2.2 Active Learning with Noisy Labels Hachey et al. (2005) were"
D11-1143,N03-5008,0,0.0334724,"Missing"
D11-1143,N04-1012,0,0.0183121,"uisition bottleneck for supervised learning is active 1546 learning (AL). AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number"
D11-1143,W02-1011,0,0.010297,"Missing"
D11-1143,C10-1107,0,0.0645493,"sks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. Combined, the two approaches could substantially lower the cost of creating training sets. Our main contribution in this paper is that we show for the first time that AL is significantly better than randomly selected annotation examples in a real crowdsourcing annotation scenario. Our experiments directly address two tasks, named entity recognition and sentiment detection, but our Proceedings of the 2011 Conference on Empirical Methods in Natural L"
D11-1143,W07-1516,0,0.0285444,"omising approach to the data acquisition bottleneck for supervised learning is active 1546 learning (AL). AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complemen"
D11-1143,D08-1027,0,0.189424,"Missing"
D11-1143,W03-0419,0,0.00943238,"e is removed from the unlabeled pool and, together with its label(s), added to the set of labeled examples. The classifier is then retrained on the labeled examples and the informativeness of the remaining examples in the pool is re-evaluated. Depending on the classifier and the sizes of pool and labeled set, retraining and reevaluation can take some time. To minimize wait times, traditional AL implementations select examples in batches of the n most informative examples. However, batch selection might not give the optimum selection (examples in a batch are likely to be redundant, see Brinker (2003)) and wait times can still occur between one batch and the next. When performing annotation with MTurk, wait times are unacceptable. Thus, we perform the retraining and uncertainty rescoring concurrently with the annotation user interface. The unlabeled pool is stored in a priority queue that is ordered according to the examples’ informativeness. The annotation user interface takes the most informative example from the pool and presents it to the annotator. The labeled example is then inserted into a second queue that feeds and updates retraining and rescoring processes. The pool queue then is"
D11-1143,D07-1051,0,0.0156991,"from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. Combined, the two approaches could substantially lower the cost of creat"
D11-1143,W10-1839,0,0.0166089,"tural Language Processing, pages 1546–1556, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics evidence suggests that AL is of general benefit in crowdsourcing. We also show that the effectiveness of MTurk annotation with AL can be further enhanced by using two techniques that increase label quality: adaptive voting and fragment recovery. 2 2.1 Related Work Crowdsourcing Pioneered by Snow et al. (2008), Crowdsourcing, especially using MTurk, has become a widely used service in the NLP community. A number of studies have looked at crowdsourcing for NER. Voyer et al. (2010) use a combination of expert and crowdsourced annotations. Finin et al. (2010) annotate Twitter messages – short sequences of words – and this is reflected in their vertically oriented user interface. Lawson et al. (2010) choose an annotation interface where annotators have to drag the mouse to select entities. Carpenter and Poesio (2010) argue that dragging is less convenient for workers than marking tokens. These papers do not address AL in crowdsourcing. Another important difference is that previous studies on NER have used data sets for which no “linguistic” gold annotation is available. I"
D11-1143,H05-1044,0,0.0091307,"Missing"
D13-1032,P05-1022,0,0.0319742,"the size of the tagset and exponentially on the order of the CRF. This probably explains why CRFs, despite their outstanding accuracy, normally only are applied to tasks with small tagsets such as Named Entity Recognition and Chunking; if they are applied to tasks with bigger tagsets – e.g., to part-of-speech (POS) tagging for English – then they generally are used as 1st -order models. In this paper, we demonstrate that fast and accurate CRF training and tagging is possible for large tagsets of even thousands of tags by approximating the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005; Rush and We use POS tagging and combined POS and morphological (POS+MORPH) tagging to demonstrate the properties and benefits of our approach. POS+MORPH disambiguation is an important preprocessing step for syntactic parsing. It is usually tackled by applying sequence prediction. POS+MORPH tagging is also a good example of a task where CRFs are rarely applied as the tagsets are often so big that even 1st -order dynamic programming is too expensive. A workaround is to restrict the possible tag candidates per position by using either morphological analyzers (MAs), dictionaries or heuristics (H"
D13-1032,chrupala-etal-2008-learning,0,0.192881,"Missing"
D13-1032,P04-1015,0,0.100384,"chastic Gradient Descent (SGD) (Tsuruoka et al., 2009). We would like to create a cascade of increasingly complex lattices and update the weight vector with the gradient of the last lattice. The updates, however, are undefined if the gold sequence is pruned from the lattice. A solution would be to simply reinsert the gold sequence, but this yields poor results as the model never learns to keep the gold sequence in the lower-order lattices. As an alternative we perform the gradient update with the highest lattice still containing the gold sequence. This approach is similar to “early updating” (Collins and Roark, 2004) in perceptron learning, where during beam search an update with the highest scoring partial hypothe324 1: function G ET S UM L ATTICE(sentence, ~τ ) 2: gold-tags ← getTags(sentence) 3: candidates ← getAllCandidates(sentence) 4: lattice ← ZeroOrderLattice(candidates) 5: for i = 1 → n do 6: candidates ← lattice. prune(τi−1 ) 7: if gold-tags 6∈ candidates then 8: return lattice 9: end if 10: if i &gt; 1 then 11: candidates ← mergeStates(candidates) 12: end if 13: candidates ← addTransitions(candidates) 14: lattice ← SequenceLattice(candidates, i) 15: end for 16: return lattice 17: end function Figu"
D13-1032,W02-1001,0,0.351278,"Missing"
D13-1032,E12-1007,1,0.802045,"amouri et al., 2004), parts 1–3 in their latest versions (LDC2010T08, LDC2010T13, LDC2011T09). As training set we use parts 1 and 2 and part 3 up to section ANN20020815.0083. All consecutive sections up to ANN20021015.0096 are used as development set and the remainder as test set. We use the unvocalized and pretokenized transliterations as input. For Czech and Spanish, we use the CoNLL 2009 data sets (Hajiˇc et al., 2009); for German, the TIGER treebank (Brants et al., 2002) with the split from Fraser et al. (2013); for Hungarian, the Szeged treebank (Csendes et al., 2005) with the split from Farkas et al. (2012). For English we use the Penn Treebank (Marcus et al., 1993) with the split from Toutanova et al. (2003). We also compute the possible POS+MORPH tags for every word using MAs. For Arabic we use the AraMorph reimplementation of Buckwalter (2002), for Czech the “free” morphology (Hajiˇc, 2001), for Spanish Freeling (Padr´o and Stanilovsky, 2012), for German DMOR (Schiller, 1995) and for Hungarian Experiments We run POS+MORPH tagging experiments on Arabic (ar), Czech (cs), Spanish (es), German (de) and Hungarian (hu). The following table shows the typetoken (T/T) ratio, the average number of tags"
D13-1032,J13-1005,1,0.761979,"Missing"
D13-1032,gimenez-marquez-2004-svmtool,0,0.0158455,"Missing"
D13-1032,A00-2013,0,0.0755341,"Missing"
D13-1032,P10-1050,0,0.0195055,"losses in accuracy. Lavergne et al. (2010) make use of feature sparsity to significantly speed up training for moderate tagset sizes (&lt; 100) and huge feature spaces. It is unclear if their approach would also work for huge tag sets (&gt; 1000). Coarse-to-fine decoding has been successfully applied to CYK parsing where full dynamic programming is often intractable when big grammars are used (Charniak and Johnson, 2005). Weiss and Taskar (2010) develop cascades of models of increasing complexity in a framework based on perceptron learning and an explicit trade-off between accuracy and efficiency. Kaji et al. (2010) propose a modified Viterbi algorithm that is still optimal but depending on task and especially for big tag sets might be several orders of magnitude faster. While their algorithm can be used to produce fast decoders, there is no such modification for the forward-backward algorithm used during CRF training. the Penn Arabic Treebank. Czech is a highly inflecting Slavic language with a large number of morphological features. Spanish is a Romance language. Based on the statistics above we can see that it has few POS+MORPH ambiguities. It is also the language with the smallest tagset and the only"
D13-1032,P10-1052,0,0.0187993,"Missing"
D13-1032,J93-2004,0,0.0449108,"C2010T08, LDC2010T13, LDC2011T09). As training set we use parts 1 and 2 and part 3 up to section ANN20020815.0083. All consecutive sections up to ANN20021015.0096 are used as development set and the remainder as test set. We use the unvocalized and pretokenized transliterations as input. For Czech and Spanish, we use the CoNLL 2009 data sets (Hajiˇc et al., 2009); for German, the TIGER treebank (Brants et al., 2002) with the split from Fraser et al. (2013); for Hungarian, the Szeged treebank (Csendes et al., 2005) with the split from Farkas et al. (2012). For English we use the Penn Treebank (Marcus et al., 1993) with the split from Toutanova et al. (2003). We also compute the possible POS+MORPH tags for every word using MAs. For Arabic we use the AraMorph reimplementation of Buckwalter (2002), for Czech the “free” morphology (Hajiˇc, 2001), for Spanish Freeling (Padr´o and Stanilovsky, 2012), for German DMOR (Schiller, 1995) and for Hungarian Experiments We run POS+MORPH tagging experiments on Arabic (ar), Czech (cs), Spanish (es), German (de) and Hungarian (hu). The following table shows the typetoken (T/T) ratio, the average number of tags of every word form that occurs more than once in the traini"
D13-1032,padro-stanilovsky-2012-freeling,0,0.0629783,"Missing"
D13-1032,W96-0213,0,0.097698,"the 0-order level. To decrease the number of tag candidates in the 0-order model, we decode in two steps by separating the fully specified tag into a coarse-grained part-of-speech (POS) tag and a finegrained MORPH tag containing the morphological features. We then first build a lattice over POS candidates and apply our pruning strategy. In a second step we expand the remaining POS tags into all the combinations with MORPH tags that were seen in the training set. We thus build a sequence of lattices of both increasing order and increasing tag complexity. 2.5 Feature Set We use the features of Ratnaparkhi (1996) and Manning (2011): the current, preceding and succeeding words as unigrams and bigrams and for rare words prefixes and suffixes up to length 10, and the occurrence of capital characters, digits and special characters. We define a rare word as a word with training set frequency ≤ 10. We concatenate every feature with the POS and MORPH tag and every morphological feature. E.g., for the word “der”, the POS tag art (article) and the MORPH tag gen|sg|fem (genitive, singular, feminine) we 325 Figure 2: Example training run of a pruned 1st -order model on German showing the fraction of pruned gold"
D13-1032,N12-1054,0,0.0175174,"Missing"
D13-1032,C08-1098,1,0.298369,"s and minus indicate models that are significantly better or worse than MA1. We can see that the improvements due to higher-order models are orthogonal to the improvements due to MAs for all languages. This was to be expected as MAs provide additional lexical knowledge while higher-order models provide additional information about the context. For Arabic and German the improvements of higher-order models are bigger than the improvements due to MAs. 4.7 Comparison with Baselines We use the following baselines: SVMTool (Gim´enez and M`arquez, 2004), an SVM-based discriminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging; Morfette (Chrupała et al., 2008), an averaged perceptron with beam search decoder; CRFSuite (Okazaki, 2007), a fast CRF implementation; and the Stanford Tagger (Toutanova et al., 2003), a bidirectional Maximum Entropy Markov Model. For POS+MORPH tagging, all baselines are trained on the concatenation of POS tag and MORPH tag. We run SVMTool with the standard feature set and the optimal c-values ∈ {0.1, 1, 10}. Morfette is run with the default options. For CRFSuite we use l2 -regularized SGD training. We use the optim"
D13-1032,P07-1096,0,0.0225943,"Missing"
D13-1032,H05-1060,0,0.0203354,"lattice than a hard constraint. For some experiments we also use the output of a morphological analyzer (MA). In that case we simply use every analysis of the MA as a simple nominal feature. This approach is attractive because it does not require the output of the MA and the annotation of the treebank to be identical; in fact, it can even be used if treebank annotation and MA use completely different features. Because the weight vector dimensionality is high for large tagsets and productive languages, we use a hash kernel (Shi et al., 2009) to keep the dimensionality constant. 3 Related Work Smith et al. (2005) use CRFs for POS+MORPH tagging, but use a morphological analyzer for candidate selection. They report training times of several days and that they had to use simplified models for Czech. Several methods have been proposed to reduce CRF training times. Stochastic gradient descent can be applied to reduce the training time by a factor of 5 (Tsuruoka et al., 2009) and without drastic losses in accuracy. Lavergne et al. (2010) make use of feature sparsity to significantly speed up training for moderate tagset sizes (&lt; 100) and huge feature spaces. It is unclear if their approach would also work f"
D13-1032,N03-1033,0,0.57353,"ining set we use parts 1 and 2 and part 3 up to section ANN20020815.0083. All consecutive sections up to ANN20021015.0096 are used as development set and the remainder as test set. We use the unvocalized and pretokenized transliterations as input. For Czech and Spanish, we use the CoNLL 2009 data sets (Hajiˇc et al., 2009); for German, the TIGER treebank (Brants et al., 2002) with the split from Fraser et al. (2013); for Hungarian, the Szeged treebank (Csendes et al., 2005) with the split from Farkas et al. (2012). For English we use the Penn Treebank (Marcus et al., 1993) with the split from Toutanova et al. (2003). We also compute the possible POS+MORPH tags for every word using MAs. For Arabic we use the AraMorph reimplementation of Buckwalter (2002), for Czech the “free” morphology (Hajiˇc, 2001), for Spanish Freeling (Padr´o and Stanilovsky, 2012), for German DMOR (Schiller, 1995) and for Hungarian Experiments We run POS+MORPH tagging experiments on Arabic (ar), Czech (cs), Spanish (es), German (de) and Hungarian (hu). The following table shows the typetoken (T/T) ratio, the average number of tags of every word form that occurs more than once in the training set (A) and the number of tags of the mos"
D13-1032,P09-1054,0,0.119698,"adding transitions to the pruned lattice and pruning with threshold τ1 . The only difference to 0-order pruning is that we now have to run forward-backward to calculate the probabilities p(y|~x, t). Note that in theory we could also apply the pruning to transition probabilities of the form p(y, y 0 |~x, t); however, this does not seem to yield more accurate models and is less efficient than state pruning. For higher-order lattices we merge pairs of states into new states, add transitions and prune with threshold τi . We train the model using l1 -regularized Stochastic Gradient Descent (SGD) (Tsuruoka et al., 2009). We would like to create a cascade of increasingly complex lattices and update the weight vector with the gradient of the last lattice. The updates, however, are undefined if the gold sequence is pruned from the lattice. A solution would be to simply reinsert the gold sequence, but this yields poor results as the model never learns to keep the gold sequence in the lower-order lattices. As an alternative we perform the gradient update with the highest lattice still containing the gold sequence. This approach is similar to “early updating” (Collins and Roark, 2004) in perceptron learning, where"
D13-1032,C00-2137,0,0.0176944,"Missing"
D13-1032,W09-1201,0,\N,Missing
D13-1063,E09-1005,0,0.288198,"ng, resulting in precise semantic groups. In particular, we find that the main strongly connected component of the FA graph (the so-called core) is very cyclic in nature and contains a large predominance of short cycles (i.e., co-links and triangles). In contrast to the DD graph, bunches of triangles form well-delimited lexical fields of collective semantic knowledge. This property may be promising for downstream tasks. Further, the methods developed in this paper may be applicable to graph representations that occur in other problems such as word sense disambiguation (e.g., (Heylighen, 2001; Agirre and Soroa, 2009)) or sentiment polarity induction (Hassan and Radev, 2010; Scheible, 2010). To show the semantic coherence of these lexical fields of the FA graph, we perform an experiment with human raters and find that cycles are strongly semantically connected even when compared to close neighbors in the graph. The reader might wonder why sets of pairwise associations can lead to any interesting structure. One of the deep results in graph theory, (Bollob´as, 2001), is that in sparse graphs, i.e., in graphs with few links per node, the number of triangles is extremely rare. Therefore, if one does find many"
D13-1063,W06-3812,0,0.0277608,"who extracted global statistics. We follow the basic methodology of these studies, but extend their approach. First, we conduct deeper analyses by examining the neighborhood of nodes and extracting the statistics of cycles. Second, we compare the properties of FA and DD graphs. Word clustering based on graphs has been the subject of various earlier studies. Close to our work is (Widdows and Dorow, 2002). These authors recognize that nearest-neighbor-based clustering of cooccurrence give rise to semantic groups. This type of approach has since been applied in various modified forms, e.g., by (Biemann, 2006) who performs labelError analysis If we view our results as a resource for a downstream task, it is important to know about possible downsides. First, we note that there are words which are not in a triangle and will thus be missing in the intersection graph. This is an indication that the corresponding word is less well embedded contextually, so conversely, any prediction made about it from the data may be less reliable. Additionally, semantic leaps caused by generalized event knowledge may lead to lesser-connected groups such as (steel, pipe, lead, copper). Jumps like these may or may not be"
D13-1063,P10-1041,0,0.0261418,"e find that the main strongly connected component of the FA graph (the so-called core) is very cyclic in nature and contains a large predominance of short cycles (i.e., co-links and triangles). In contrast to the DD graph, bunches of triangles form well-delimited lexical fields of collective semantic knowledge. This property may be promising for downstream tasks. Further, the methods developed in this paper may be applicable to graph representations that occur in other problems such as word sense disambiguation (e.g., (Heylighen, 2001; Agirre and Soroa, 2009)) or sentiment polarity induction (Hassan and Radev, 2010; Scheible, 2010). To show the semantic coherence of these lexical fields of the FA graph, we perform an experiment with human raters and find that cycles are strongly semantically connected even when compared to close neighbors in the graph. The reader might wonder why sets of pairwise associations can lead to any interesting structure. One of the deep results in graph theory, (Bollob´as, 2001), is that in sparse graphs, i.e., in graphs with few links per node, the number of triangles is extremely rare. Therefore, if one does find many triangles in a graph, they must be not only a signal of n"
D13-1063,W06-1664,0,0.027574,"dge may lead to lesser-connected groups such as (steel, pipe, lead, copper). Jumps like these may or may not be desired in a subsequent application. 6 The Case of the EAT FA dataset The Edinburgh Associative Thesaurus (EAT) (Kiss et al., 1973) is a large dataset of free associations. We extract the EAT FA seed-crux with the previously described methods. We start by generating the initial graph (23’219 vertices and 325’589 edges), then extract its core (7’754 vertices and 247’172 edges) and its seed 1 http://www.mturk.com 677 7 Related Work propagation based on randomized nearest neighbors, or Matsuo et al. (2006) who perform greedy clustering. Hierarchical clustering algorithms (e.g., (Jonyer et al., 2002; Manning et al., 2008)) are related as well, however, the key difference is that in hierarchical clustering, the granularity of a cluster is difficult to determine. Dorow et al. (2005) recognize that triangles form semantically strongly cohesive groups and apply clustering coefficients for word sense disambiguation. Their work focuses on undirected graphs of corpus co-occurrences whereas our work builds on directed associations. Building on this work, we take finer topological graph structures into a"
D13-1063,P10-3005,1,0.84723,"ongly connected component of the FA graph (the so-called core) is very cyclic in nature and contains a large predominance of short cycles (i.e., co-links and triangles). In contrast to the DD graph, bunches of triangles form well-delimited lexical fields of collective semantic knowledge. This property may be promising for downstream tasks. Further, the methods developed in this paper may be applicable to graph representations that occur in other problems such as word sense disambiguation (e.g., (Heylighen, 2001; Agirre and Soroa, 2009)) or sentiment polarity induction (Hassan and Radev, 2010; Scheible, 2010). To show the semantic coherence of these lexical fields of the FA graph, we perform an experiment with human raters and find that cycles are strongly semantically connected even when compared to close neighbors in the graph. The reader might wonder why sets of pairwise associations can lead to any interesting structure. One of the deep results in graph theory, (Bollob´as, 2001), is that in sparse graphs, i.e., in graphs with few links per node, the number of triangles is extremely rare. Therefore, if one does find many triangles in a graph, they must be not only a signal of non-randomness, bu"
D13-1063,C02-1114,0,0.0524032,"ld be used efficiently to derive new ways of measuring semantic similarity between words. Topological analysis of the Florida Word Associations (FA) was started by (Steyvers and Tenenbaum, 2005; Gravino et al., 2012), who extracted global statistics. We follow the basic methodology of these studies, but extend their approach. First, we conduct deeper analyses by examining the neighborhood of nodes and extracting the statistics of cycles. Second, we compare the properties of FA and DD graphs. Word clustering based on graphs has been the subject of various earlier studies. Close to our work is (Widdows and Dorow, 2002). These authors recognize that nearest-neighbor-based clustering of cooccurrence give rise to semantic groups. This type of approach has since been applied in various modified forms, e.g., by (Biemann, 2006) who performs labelError analysis If we view our results as a resource for a downstream task, it is important to know about possible downsides. First, we note that there are words which are not in a triangle and will thus be missing in the intersection graph. This is an indication that the corresponding word is less well embedded contextually, so conversely, any prediction made about it fro"
D13-1063,P94-1019,0,\N,Missing
D14-1128,baccianella-etal-2010-sentiwordnet,0,0.0401304,"(2009) present an approach to classify contextual polarity building on a two-step process. First, they classify if a sentiment word is polar in a phrase and if so, second, they classify its polarity. Our approach can be seen as an extension of this approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccianella et al. (2010), Wiebe and Mihalcea (2006)). However, these approaches are not able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in"
D14-1128,D12-1050,0,0.0129494,"3 ‡ · ‡ * * * 4 ‡ ‡ ‡ 5 6 7 ‡ ‡ * ‡ 8 Table 3: Significant differences of lines 1–8 in Table 2; ‡: p=0.01, *: p=0.05, ·: p=0.1 The first approach is to use a statistical classification model as the sense disambiguation structure. We use liblinear (Fan et al., 2008) with standard parameters for classification based on three different feature types: ngrams, embeddings (embed) and PCDs. Ngram features are all n-grams for n ∈ {1, 2, 3}. As embedding features we use (i) the mean of the input space (R) embeddings and (ii) the mean of the target space (Q) embeddings of the words in the context (see Blacoe and Lapata (2012) for justification of using simple mean). As PCD features we use the PCD predicted by vLBL for the sentiment word of interest, in our case “hard”. We split the set of 4600 contexts introduced in Section 3 into a training set of 4000 and a development set of 600. Table 2 (lines 1–8) shows the classification results on the development set for all feature type combinations. Significant differences between results – computed using the approximate randomization test (Pad´o, 2006) – are given in Table 3. The majority baseline (bl), which assigns a negative label to all examples, reaches F1 = .76. Th"
D14-1128,P11-1015,0,0.059694,"s approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccianella et al. (2010), Wiebe and Mihalcea (2006)). However, these approaches are not able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD approaches, e.g., thesaurus-based WSD (Yarowsky, 1992), could also be used for CESL. 3 Linguistic analysis of sentiment contexts of “hard” We took a random sample of 5000 contexts of “hard” in the Amazon Product Review"
D14-1128,C92-2070,0,0.111056,"ot able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD approaches, e.g., thesaurus-based WSD (Yarowsky, 1992), could also be used for CESL. 3 Linguistic analysis of sentiment contexts of “hard” We took a random sample of 5000 contexts of “hard” in the Amazon Product Review Data (Jindal and Liu, 2008). We use 200 as a test set and set aside 200 for future use. We analyzed the remaining 4600 contexts using a tool we designed for this study, which provides functionality for selecting and sorting contexts, including a keyword in context display. If a reliable pattern has been identified (e.g., the phrase “die hard”), then all contexts matching the pattern can be labeled automatically. Our goal is to iden"
D14-1128,W03-1017,0,0.124243,"contextual features are necessary to support making fine-grained distinctions. Our third contribution is that we experiment with deep learning as a source of such features. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutral adverbial sense ‘intense’ of"
D14-1128,W02-1011,0,0.0210466,"periment with deep learning as a source of such features. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutral adverbial sense ‘intense’ of “hard” (“laugh hard”, “try hard”) vs. the negative adjectival mean1210 Proceedings of the 2014 Conferenc"
D14-1128,W03-0404,0,0.0723446,"s. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutral adverbial sense ‘intense’ of “hard” (“laugh hard”, “try hard”) vs. the negative adjectival mean1210 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EM"
D14-1128,D11-1014,0,0.0363372,"4 13, 15 14, 16 phrases phrases patterns # train 78 2561 # test 5 120 neu 425 19 neu 24 7 neg neu 15 5 0 6 hard beats neu 347 15 hard edge neu 3 1 neg neu 36 375 2 27 hard for, hard on, hard to V be hard at it sent. neu neg Table 1: Sense inventory of “hard”. ing ‘difficult’ (“hard life”, “hard memory”) cannot be easily distinguished based on an ngram representation. Moreover, although ngram approaches could learn the polarity of these phrases they do not generalize to new phrases. More recent compositional approaches to sentiment analysis can outperform lexicon and ngrambased methods (e.g., Socher et al. (2011), Socher et al. (2013)). However, these approaches conflate two different types of contextual effects: differences in sense or lexical meaning (“hard memory” vs. “hard wood”) on the one hand and meaning composition like negation on the other hand. From the point of view of linguistic theory, these are different types of contextual effects that should not be conflated. Recognizing that “hard” occurs in the scope of negation is of no use if the basic polarity of the contextually evoked sense of “hard” (e.g., negative in “no hard memories” vs. neutral in “no hard wood”) is not recognized. Wilson"
D14-1128,D13-1170,0,0.0104151,"es phrases patterns # train 78 2561 # test 5 120 neu 425 19 neu 24 7 neg neu 15 5 0 6 hard beats neu 347 15 hard edge neu 3 1 neg neu 36 375 2 27 hard for, hard on, hard to V be hard at it sent. neu neg Table 1: Sense inventory of “hard”. ing ‘difficult’ (“hard life”, “hard memory”) cannot be easily distinguished based on an ngram representation. Moreover, although ngram approaches could learn the polarity of these phrases they do not generalize to new phrases. More recent compositional approaches to sentiment analysis can outperform lexicon and ngrambased methods (e.g., Socher et al. (2011), Socher et al. (2013)). However, these approaches conflate two different types of contextual effects: differences in sense or lexical meaning (“hard memory” vs. “hard wood”) on the one hand and meaning composition like negation on the other hand. From the point of view of linguistic theory, these are different types of contextual effects that should not be conflated. Recognizing that “hard” occurs in the scope of negation is of no use if the basic polarity of the contextually evoked sense of “hard” (e.g., negative in “no hard memories” vs. neutral in “no hard wood”) is not recognized. Wilson et al. (2009) present"
D14-1128,P02-1053,0,0.00946092,", then powerful contextual features are necessary to support making fine-grained distinctions. Our third contribution is that we experiment with deep learning as a source of such features. We look at two types of deep learning features: word embeddings and neural network language model predictions (Section 4). We show that deep learning features significantly improve the accuracy of context-dependent polarity classification (Section 5). 2 Related work Initial work on sentiment analysis was either based on sentiment lexicons that listed words as positive or negative sentiment indicators (e.g., Turney (2002), Yu and Hatzivassiloglou (2003)), on statistical classification approaches that represent documents as ngrams (e.g., Pang et al. (2002)) or on a combination of both (e.g., Riloff et al. (2003), Whitelaw et al. (2005)). The underlying assumption of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., “hard wood” (neutral) vs. “hard memory” (negative). Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neutr"
D14-1128,P06-1134,0,0.0254656,"to classify contextual polarity building on a two-step process. First, they classify if a sentiment word is polar in a phrase and if so, second, they classify its polarity. Our approach can be seen as an extension of this approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccianella et al. (2010), Wiebe and Mihalcea (2006)). However, these approaches are not able to disambiguate the sense of a word given its context. Previous work on representation learning for sentiment analysis includes (Maas and Ng, 2010) and (Maas et al., 2011). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or documents and does not consider each sentiment word instance at a local level. We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD appro"
D14-1128,J09-3003,0,0.0373371,"(2011), Socher et al. (2013)). However, these approaches conflate two different types of contextual effects: differences in sense or lexical meaning (“hard memory” vs. “hard wood”) on the one hand and meaning composition like negation on the other hand. From the point of view of linguistic theory, these are different types of contextual effects that should not be conflated. Recognizing that “hard” occurs in the scope of negation is of no use if the basic polarity of the contextually evoked sense of “hard” (e.g., negative in “no hard memories” vs. neutral in “no hard wood”) is not recognized. Wilson et al. (2009) present an approach to classify contextual polarity building on a two-step process. First, they classify if a sentiment word is polar in a phrase and if so, second, they classify its polarity. Our approach can be seen as an extension of this approach; the main difference is that we will show in our analysis of “hard” that the polarity of phrases depends on the senses of the words that are used. This is evidence that highaccuracy polarity classification depends on sense disambiguation. There has been previous work on assigning polarity values to senses of words taken from WordNet (e.g., Baccia"
D14-1128,J90-3007,0,\N,Missing
D14-1151,P14-2133,0,0.0130006,"ar. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. Andreas and Klein (2014) investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that high positive similarities are assigned to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is applied to a matrix of thesaurus entries. The val1450 ues representing antonyms are negated. with pattern-based methods in the future. There has been a great deal of wo"
D14-1151,P14-1023,0,0.0448303,"a one at the word index and zeros otherwise (one-hot vectors). However, this approach cannot handle unknown words (Turian et al., 2010) and similarities among words cannot be represented (Mikolov et al., 2013). Therefore, distributed word representations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. Andreas and Klein (2014) investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that hig"
D14-1151,P06-1045,0,0.010783,"a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that high positive similarities are assigned to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is applied to a matrix of thesaurus entries. The val1450 ues representing antonyms are negated. with pattern-based methods in the future. There has been a great deal of work on applying the vector space model and cosine similarity to find synonyms or antonyms. Hagiwara et al. (2006) represent each word as a vector with cooccurrence frequencies of words and contexts as elements, normalized by the inverse document frequency. The authors investigate three types of contextual information (dependency, sentence cooccurrence and proximity) and find that a combination of them leads to the most stable results. Schulte im Walde and K¨oper (2013) build a vector space model on lexico-syntactic patterns and apply a Rocchio classifier to distinguish synonyms from antonyms, among other tasks. Van der Plas and Tiedemann (2006) use automatically aligned translations of the same text in d"
D14-1151,W11-1902,0,0.0553891,"text data and embeddings derived from automatically extracted coreference chains. For the calculation of the vector representations, the word2vec toolkit1 by Mikolov et al. (2013) is applied. We use the skip-gram model for our experiments because its results for semantic similarity are better according to Mikolov et al. (2013). We train a first model on a subset of English Gigaword data.2 In the following sections, we call the resulting embeddings text-based. To improve the semantic similarities of the vectors, we prepare another training text consisting of coreference chains. We use CoreNLP (Lee et al., 2011) to extract coreference chains from the Gigaword corpus. Then we build a skip-gram model on these coreference chains. The extracted coreference chains are provided as an additional resource to this paper3 . Although they have been developed using only a publicly available toolkit, we expect this resource to be helpful for other researchers since the process to extract the coreference chains of such a large text corpus takes several weeks on multi-core machines. In total, we extracted 3.1M coreference chains. 2.7M of them consist of at least two different markables. The median (mean) length of"
D14-1151,P14-2050,0,0.0291224,"010) and similarities among words cannot be represented (Mikolov et al., 2013). Therefore, distributed word representations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. Andreas and Klein (2014) investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that high positive similarities are assigned to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is appli"
D14-1151,D08-1103,0,0.0187486,"to distinguish synonyms from antonyms, among other tasks. Van der Plas and Tiedemann (2006) use automatically aligned translations of the same text in different languages to build context vectors. Based on these vectors, they detect synonyms. 5 In contrast, there are also studies using linguistic knowledge from external resources: Senellart and Blondel (2008) propose a method for synonym detection based on graph similarity in a graph generated using the definitions of a monolingual dictionary. Harabagiu et al. (2006) recognize antonymy by generating antonymy chains based on WordNet relations. Mohammad et al. (2008) look for the word with the highest degree of antonymy to a given target word among five candidates. For this task, they use thesaurus information and the similarity of the contexts of two contrasting words. Lin et al. (2003) use Hearst patterns to distiguish synonyms from antonyms. Work by Turney (2008) is similar except that the patterns are learned. Except for the publicly available coreference resolution system, our approach does not need external resources such as dictionaries or bilingual corpora and no human labor is required. Thus, it can be easily applied to any corpus in any language"
D14-1151,W09-2411,0,0.0269231,"Missing"
D14-1151,N03-1033,0,0.012104,"Missing"
D14-1151,P10-1040,0,0.0343252,"ite shots” (elements of coreference chains in italics). Thus, adjectives with opposite meanings can cooccur in coreference chains just as they can cooccur in window-based contexts. For nouns, it is much less likely that the same coreference chain will contain both a noun and its antonym since – by definition – markables in a coreference chain refer to the same identical entity. 4 Related work Traditionally, words have been represented by vectors of the size of the vocabulary with a one at the word index and zeros otherwise (one-hot vectors). However, this approach cannot handle unknown words (Turian et al., 2010) and similarities among words cannot be represented (Mikolov et al., 2013). Therefore, distributed word representations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy"
D14-1151,C08-1114,0,0.063842,"xternal resources: Senellart and Blondel (2008) propose a method for synonym detection based on graph similarity in a graph generated using the definitions of a monolingual dictionary. Harabagiu et al. (2006) recognize antonymy by generating antonymy chains based on WordNet relations. Mohammad et al. (2008) look for the word with the highest degree of antonymy to a given target word among five candidates. For this task, they use thesaurus information and the similarity of the contexts of two contrasting words. Lin et al. (2003) use Hearst patterns to distiguish synonyms from antonyms. Work by Turney (2008) is similar except that the patterns are learned. Except for the publicly available coreference resolution system, our approach does not need external resources such as dictionaries or bilingual corpora and no human labor is required. Thus, it can be easily applied to any corpus in any language as long as there exists a coreference resolution system in this language. The pattern-based approach (Lin et al., 2003; Turney, 2008) discussed above also needs few resources. In contrast to our work, it relies on patterns and might therefore restrict the number of recognizable synonyms and antonyms to"
D14-1151,P06-2111,0,0.0810456,"Missing"
D14-1151,D12-1111,0,0.334345,"distinguish between semantic 1447 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447–1452, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics his woman text-based my, their, her, your, our man, girl, believer, pharisee, guy coref.-based he, him, himself, zechariah, ancestor girl, prostitute, lupita, betsy, lehia Table 1: Nearest neighbors of “his” / “woman” for text-based & coreference-based embeddings association and true synonymy. As a result, synonyms and antonyms may be mapped to similar word vectors (Yih et al., 2012). For many NLP tasks, however, information about true synonymy or antonymy may be important. In this paper, we develop two different word embeddings: embeddings calculated on raw text data and embeddings derived from automatically extracted coreference chains. For the calculation of the vector representations, the word2vec toolkit1 by Mikolov et al. (2013) is applied. We use the skip-gram model for our experiments because its results for semantic similarity are better according to Mikolov et al. (2013). We train a first model on a subset of English Gigaword data.2 In the following sections, we"
D14-1151,S10-1001,0,\N,Missing
D15-1033,J10-4006,0,0.0455285,"ion significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis (Turney and Littman, 2003). In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words. It is difficult for onehot initialization to learn good embeddings from onl"
D15-1033,C94-1049,0,0.349738,", distributional initialization provides an additional source of information – the global distribution of the word in 2 Method Weighting. We use two different weighting schemes for distributional vectors. Let v1 , . . . , vn be the vocabulary of context words. In BINARY weighting, entry 1 ≤ i ≤ n in the distributional vector of target word w is set to 1 iff vi and w cooccur at a distance of at most ten words in the corpus and to 0 otherwise. In PPMI weighting, entry 1 ≤ i ≤ n in the distributional vector of target word w is set to the PPMI (positive pointwise mutual information, introduced by Niwa and Nitta (1994)) of w and vi . We divide PPMI values by their maximum to ensure they are in [0, 1] because we will combine one-hot vectors (whose values are 0/1) with PPMI weights and it is important that they are on the same scale. We use two different distributional initializations, shown in Figure 1: separate (left) and mixed (right). Combinations of these two initializations with both BINARY and PPMI weighting will be investigated in the experiments. Recall that n is the dimensionality of the distri280 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 280–285,"
D15-1033,P12-1015,0,0.0194937,"1     wk  wk+1 rare  .. words .   wn n }| {z .. 1 | w1 w2 freq. words  w3  .    ..          0 . 0 { z 1 k }| 1 1    1 1 0 0 1 {z k+n 0 ··· .. . ··· 0 1 1 1  wk  wk+1 0 rare  .. words .   wn 0 | } 1 1 0 1 n−k }| {z .. 0 . ··· .. . ··· { 1 0 1 1 1 1 {z n 0 ··· .. . ··· 0 1 } Figure 1: One-hot vectors of frequent words and distributional vectors of rare words are separate in separate initialization (left) and overlap in mixed initialization (right). This example is for BINARY weighting. (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency ≤100, all other words are more frequent. To address this issue, we artificially make all words in the si"
D15-1033,P10-1040,0,0.0532739,"Section 6 and discussion of future work in Section 7. There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (M"
D15-1033,J15-4004,0,0.114722,"Missing"
D15-1033,D10-1076,0,0.0314998,"n. Approaches along these lines include Botha and Blunsom (2014) who represent a word as a sum of embedding vectors of its morphemes. Cui et al. (2014) use a weighted average of vectors of morphologically similar words. Bian et al. (2014) extend a word’s vector with vectors of entity categories and POS tags. This line of work also is partially motivated by improving the embeddings of rare words. Distributional information on the one hand and syntactic/semantic information on the other hand are likely to be complementary, so that a combination of our approach with this prior work is promising. Le et al. (2010) propose three schemes to address word embedding initialization. Reinitialization and iterative reinitialization use vectors from prediction space to initialize the context space during training. This approach is both more complex and less efficient than ours. One-vector initialization initializes all word embeddings with the same 8 A reviewer asks: “If a word is rare, its distributional vector should also be sparse and less informative, which does not guarantee to be a good starting point.” This is true and it suggests that it may not be possible to learn a very high-quality representation fo"
D15-1033,W13-3512,0,0.0838723,"0 . 0 { z 1 k }| 1 1    1 1 0 0 1 {z k+n 0 ··· .. . ··· 0 1 1 1  wk  wk+1 0 rare  .. words .   wn 0 | } 1 1 0 1 n−k }| {z .. 0 . ··· .. . ··· { 1 0 1 1 1 1 {z n 0 ··· .. . ··· 0 1 } Figure 1: One-hot vectors of frequent words and distributional vectors of rare words are separate in separate initialization (left) and overlap in mixed initialization (right). This example is for BINARY weighting. (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency ≤100, all other words are more frequent. To address this issue, we artificially make all words in the six data sets rare. We do this by keeping only θ randomly chosen occurrences in the corpus (for w"
D15-1083,W14-1611,0,0.02002,"and embedding labels in the same space. These methods as well as standard NER systems try to maximize correct classification of mentions in individual contexts whereas we aggregate individual contexts and evaluate on accuracy of entity-type assignments inferred from the entire corpus. In other words, their evaluation is sentence-level whereas ours is corpus-level. Entity set expansion (ESE) is the problem of finding entities in a class (e.g., medications) given a seed set (e.g., {“Ibuprofen”, “Maalox”, “Prozac”}). The standard solution is pattern-based bootstrapping (Thelen and Riloff, 2002; Gupta and Manning, 2014). ESE is different from the problem we address because ESE starts with a small seed set whereas we assume that a large number of examples from a knowledge base (KB) is available. Initial experiments with the system of Gupta • We address the problem of corpus-level entity typing in a knowledge base completion setting. In contrast to other work that has focused on learning relations between entities, we learn types of entities. • We show that context and global models for entity typing provide complementary infor716 also learn embeddings for entities and types. Most prior work on entity embeddin"
D15-1083,P14-1023,0,0.101506,"is paper. We use Freebase, the largest available KB, in this paper. In Freebase, an entity can belong to several classes, e.g., “Barack Obama” is a member of 37 types including “US president” and “author”. One notable type is also defined for each entity, e.g., “US-president” for “Obama” since it is regarded as his most prominent characteristic and the one that would be used to disambiguate references to him, e.g., to distinguish him from somebody else with the same name. There are about 1500 types in Freebase, orLearning embeddings for words is standard in a large body of NLP literature (see Baroni et al. (2014) for an overview). In addition to words, we 717 3.3 ganized by domain; e.g., the domain “food” has types like “food”, “ingredient” and “restaurant”. Some types like “location” are very general, some are very fine-grained, e.g., “Vietnamese urban district”. There are types that have a large number of instances like “citytown” and types that have very few like “camera sensor”. Entities are defined as instances of types. They can have several types based on the semantic classes that the entity they are referring to is a member of – as in the above example of Barack Obama. The first step in extrac"
D15-1083,D12-1082,0,0.0668607,"h instance can have several labels (types). Similar to Zhou and Zhang (2006)’s work on scene classification, we also transform MIML into easier tasks. The global model transforms MIML into a multi-label problem by merging all instances of an example. The context model solves the problem by combining the instance-label scores to example-label scores. and Manning (2014) showed that it was not performing well for our task – this is not surprising given that it is designed for a task with properties quite different from entity typing. More closely related to our work are the OpenIE systems NNPLB (Lin et al., 2012) and PEARL (Nakashole et al., 2013) for fine-grained typing of unlinkable and emerging entities. Both systems first extract relation tuples from a corpus and then type entities based on the tuples they occur in (where NNPLB only uses the subject position for typing). To perform typing, NNPLB propagates activation from known members of a class to other entities whereas PEARL assigns types to the argument slots of relations. The main difference to FIGMENT is that we do not rely on relation extraction. In principle, we can make use of any context, not just subject and object positions. FIGMENT al"
D15-1083,Q15-1023,0,0.0415341,"Missing"
D15-1083,Q14-1037,0,0.0701681,"Missing"
D15-1083,N13-1095,0,0.041223,"Missing"
D15-1083,D11-1142,0,0.0367809,"Missing"
D15-1083,P13-1146,0,0.0123223,"abels (types). Similar to Zhou and Zhang (2006)’s work on scene classification, we also transform MIML into easier tasks. The global model transforms MIML into a multi-label problem by merging all instances of an example. The context model solves the problem by combining the instance-label scores to example-label scores. and Manning (2014) showed that it was not performing well for our task – this is not surprising given that it is designed for a task with properties quite different from entity typing. More closely related to our work are the OpenIE systems NNPLB (Lin et al., 2012) and PEARL (Nakashole et al., 2013) for fine-grained typing of unlinkable and emerging entities. Both systems first extract relation tuples from a corpus and then type entities based on the tuples they occur in (where NNPLB only uses the subject position for typing). To perform typing, NNPLB propagates activation from known members of a class to other entities whereas PEARL assigns types to the argument slots of relations. The main difference to FIGMENT is that we do not rely on relation extraction. In principle, we can make use of any context, not just subject and object positions. FIGMENT also has advantages for noisy text fo"
D15-1083,P05-1045,0,0.0403321,"zation and deal with noisy input. • We show that our approach outperforms a system based on OpenIE relations when the input corpus consists of noisy web pages. In the following, we first discuss related work. Then we motivate our approach and define the problem setting we adopt. We then introduce our models in detail and report and analyze experimental results. Finally, we discuss remaining challenges and possible future work and present our conclusions. 2 Related work Named entity recognition (NER) is the task of detecting and classifying named entities in text. While most NER systems (e.g., Finkel et al. (2005)) only consider a small number of entity classes, recent work has addressed fine-grained NER (Yosef et al., 2012; Ling and Weld, 2012). These methods use a variety of lexical and syntactic features to segment and classify entity mentions. Some more recent work assumes the segmentation is known and only classifies entity mentions. Dong et al. (2015) use distributed representations of words in a hybrid classifier to classify mentions to 20 types. Yogatama et al. (2015) classify mentions to more fine-grained types by using different features for mentions and embedding labels in the same space. Th"
D15-1083,N15-1054,0,0.183285,"KBs are often not completely modeled and have entries that require enhancement. We choose NNPLB as our baseline. The fine-grained typing of entities performed by FIGMENT can be used for knowledge base completion (KBC). Most KBC systems focus on relations between entities, not on types as we do. Some generalize the patterns of relationships within the KB (Nickel et al., 2012; Bordes et al., 2013) while others use a combination of within-KB generalization and information extraction from text (Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Neelakantan and Chang (2015) address entity typing in a way that is similar to FIGMENT. Their method is based on KB information, more specifically entity descriptions in Wikipedia and Freebase. Thus, in contrast to our approach, their system is not able to type entities that are not covered by existing KBs. We infer classes for entities from a large corpus and do not assume that these entities occur in the KB. 3 3.1 Motivation and problem definition Freebase Large scale KBs like Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and Google knowledge graph are important NLP resources. Their structure is rough"
D15-1083,P15-2048,0,0.177624,"work Named entity recognition (NER) is the task of detecting and classifying named entities in text. While most NER systems (e.g., Finkel et al. (2005)) only consider a small number of entity classes, recent work has addressed fine-grained NER (Yosef et al., 2012; Ling and Weld, 2012). These methods use a variety of lexical and syntactic features to segment and classify entity mentions. Some more recent work assumes the segmentation is known and only classifies entity mentions. Dong et al. (2015) use distributed representations of words in a hybrid classifier to classify mentions to 20 types. Yogatama et al. (2015) classify mentions to more fine-grained types by using different features for mentions and embedding labels in the same space. These methods as well as standard NER systems try to maximize correct classification of mentions in individual contexts whereas we aggregate individual contexts and evaluate on accuracy of entity-type assignments inferred from the entire corpus. In other words, their evaluation is sentence-level whereas ours is corpus-level. Entity set expansion (ESE) is the problem of finding entities in a class (e.g., medications) given a seed set (e.g., {“Ibuprofen”, “Maalox”, “Proz"
D15-1083,N13-1008,0,0.0444473,"premise is that even existing entities in KBs are often not completely modeled and have entries that require enhancement. We choose NNPLB as our baseline. The fine-grained typing of entities performed by FIGMENT can be used for knowledge base completion (KBC). Most KBC systems focus on relations between entities, not on types as we do. Some generalize the patterns of relationships within the KB (Nickel et al., 2012; Bordes et al., 2013) while others use a combination of within-KB generalization and information extraction from text (Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Neelakantan and Chang (2015) address entity typing in a way that is similar to FIGMENT. Their method is based on KB information, more specifically entity descriptions in Wikipedia and Freebase. Thus, in contrast to our approach, their system is not able to type entities that are not covered by existing KBs. We infer classes for entities from a large corpus and do not assume that these entities occur in the KB. 3 3.1 Motivation and problem definition Freebase Large scale KBs like Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and Google knowledge graph are"
D15-1083,C12-2133,0,0.537003,"ce by replacing Wikipedia anchors with their corresponding entities. For our global model, we learn entity embedding in a similar way, but on a corpus with automatically annotated entities. For our context model, we learn and use type embeddings jointly with corpus words to improve generalization, a novel contribution of this paper to the best of our knowledge. We learn all our embeddings using word2vec (Mikolov et al., 2013). Our problem can be formulated as multiinstance multi-label (MIML) learning (Zhou and Zhang, 2006), similar to the formulation for relation extraction by Surdeanu et al. (2012). In our problem, each example (entity) can have several instances (contexts) and each instance can have several labels (types). Similar to Zhou and Zhang (2006)’s work on scene classification, we also transform MIML into easier tasks. The global model transforms MIML into a multi-label problem by merging all instances of an example. The context model solves the problem by combining the instance-label scores to example-label scores. and Manning (2014) showed that it was not performing well for our task – this is not surprising given that it is designed for a task with properties quite differen"
D15-1083,D12-1042,0,0.020173,"in the same space by replacing Wikipedia anchors with their corresponding entities. For our global model, we learn entity embedding in a similar way, but on a corpus with automatically annotated entities. For our context model, we learn and use type embeddings jointly with corpus words to improve generalization, a novel contribution of this paper to the best of our knowledge. We learn all our embeddings using word2vec (Mikolov et al., 2013). Our problem can be formulated as multiinstance multi-label (MIML) learning (Zhou and Zhang, 2006), similar to the formulation for relation extraction by Surdeanu et al. (2012). In our problem, each example (entity) can have several instances (contexts) and each instance can have several labels (types). Similar to Zhou and Zhang (2006)’s work on scene classification, we also transform MIML into easier tasks. The global model transforms MIML into a multi-label problem by merging all instances of an example. The context model solves the problem by combining the instance-label scores to example-label scores. and Manning (2014) showed that it was not performing well for our task – this is not surprising given that it is designed for a task with properties quite differen"
D15-1083,W02-1028,0,0.0164773,"ent features for mentions and embedding labels in the same space. These methods as well as standard NER systems try to maximize correct classification of mentions in individual contexts whereas we aggregate individual contexts and evaluate on accuracy of entity-type assignments inferred from the entire corpus. In other words, their evaluation is sentence-level whereas ours is corpus-level. Entity set expansion (ESE) is the problem of finding entities in a class (e.g., medications) given a seed set (e.g., {“Ibuprofen”, “Maalox”, “Prozac”}). The standard solution is pattern-based bootstrapping (Thelen and Riloff, 2002; Gupta and Manning, 2014). ESE is different from the problem we address because ESE starts with a small seed set whereas we assume that a large number of examples from a knowledge base (KB) is available. Initial experiments with the system of Gupta • We address the problem of corpus-level entity typing in a knowledge base completion setting. In contrast to other work that has focused on learning relations between entities, we learn types of entities. • We show that context and global models for entity typing provide complementary infor716 also learn embeddings for entities and types. Most pri"
D15-1083,D14-1167,0,0.315624,"is available. Initial experiments with the system of Gupta • We address the problem of corpus-level entity typing in a knowledge base completion setting. In contrast to other work that has focused on learning relations between entities, we learn types of entities. • We show that context and global models for entity typing provide complementary infor716 also learn embeddings for entities and types. Most prior work on entity embeddings (e.g., Weston et al. (2013), Bordes et al. (2013)) and entity and type embeddings (Zhao et al., 2015) has mainly used KB information as opposed to text corpora. Wang et al. (2014) learn embeddings of words and entities in the same space by replacing Wikipedia anchors with their corresponding entities. For our global model, we learn entity embedding in a similar way, but on a corpus with automatically annotated entities. For our context model, we learn and use type embeddings jointly with corpus words to improve generalization, a novel contribution of this paper to the best of our knowledge. We learn all our embeddings using word2vec (Mikolov et al., 2013). Our problem can be formulated as multiinstance multi-label (MIML) learning (Zhou and Zhang, 2006), similar to the"
D15-1083,D13-1136,0,0.0817487,"ifferent from the problem we address because ESE starts with a small seed set whereas we assume that a large number of examples from a knowledge base (KB) is available. Initial experiments with the system of Gupta • We address the problem of corpus-level entity typing in a knowledge base completion setting. In contrast to other work that has focused on learning relations between entities, we learn types of entities. • We show that context and global models for entity typing provide complementary infor716 also learn embeddings for entities and types. Most prior work on entity embeddings (e.g., Weston et al. (2013), Bordes et al. (2013)) and entity and type embeddings (Zhao et al., 2015) has mainly used KB information as opposed to text corpora. Wang et al. (2014) learn embeddings of words and entities in the same space by replacing Wikipedia anchors with their corresponding entities. For our global model, we learn entity embedding in a similar way, but on a corpus with automatically annotated entities. For our context model, we learn and use type embeddings jointly with corpus words to improve generalization, a novel contribution of this paper to the best of our knowledge. We learn all our embeddings u"
D15-1155,W06-1615,0,0.104074,"Missing"
D15-1155,P12-2071,0,0.0223302,"that processes training examples – usually obtained interactively – in small batches (Bordes et al 2005) All of this work on supervised online learning is not directly relevant to this paper since we address the problem of unsupervised DA Unlike online supervised learners we keep the statistical model unchanged during DA and adopt a representation learning approach: each unlabeled context of a word is used to update its representation There is much work on unsupervised DA for POS tagging including work using constraintbased methods (Subramanya et al 2010; Rush et al 2012) instance weighting (Choi and Palmer 2012) self-training (Huang et al 2009; Huang and Yates 2010) and co-training (K¨ubler and Baucom 2011) All of this work uses batch learning For space reasons we do not discuss supervised DA (e g Daum´e III and Marcu (2006)) unknown wo ds e o sa c on ne ba ch unseens e o sa c on ne ba ch known wo ds 5 F gu e 1 E o a es o unknown wo ds wo ds w h unseen ags and known wo ds o sma u 0 The x ax s ep esen s he numbe o okens o he espec ve ype e g numbe o okens o unknown wo ds more and more is learned with each additional occurrence of an unknown word (top) Interestingly the error of O NL NE increases for u"
D15-1155,W10-2604,0,0.0227752,"teractively – in small batches (Bordes et al 2005) All of this work on supervised online learning is not directly relevant to this paper since we address the problem of unsupervised DA Unlike online supervised learners we keep the statistical model unchanged during DA and adopt a representation learning approach: each unlabeled context of a word is used to update its representation There is much work on unsupervised DA for POS tagging including work using constraintbased methods (Subramanya et al 2010; Rush et al 2012) instance weighting (Choi and Palmer 2012) self-training (Huang et al 2009; Huang and Yates 2010) and co-training (K¨ubler and Baucom 2011) All of this work uses batch learning For space reasons we do not discuss supervised DA (e g Daum´e III and Marcu (2006)) unknown wo ds e o sa c on ne ba ch unseens e o sa c on ne ba ch known wo ds 5 F gu e 1 E o a es o unknown wo ds wo ds w h unseen ags and known wo ds o sma u 0 The x ax s ep esen s he numbe o okens o he espec ve ype e g numbe o okens o unknown wo ds more and more is learned with each additional occurrence of an unknown word (top) Interestingly the error of O NL NE increases for unseens and known words (middle&bottom panels) (even tho"
D15-1155,N09-2054,0,0.0173937,"sually obtained interactively – in small batches (Bordes et al 2005) All of this work on supervised online learning is not directly relevant to this paper since we address the problem of unsupervised DA Unlike online supervised learners we keep the statistical model unchanged during DA and adopt a representation learning approach: each unlabeled context of a word is used to update its representation There is much work on unsupervised DA for POS tagging including work using constraintbased methods (Subramanya et al 2010; Rush et al 2012) instance weighting (Choi and Palmer 2012) self-training (Huang et al 2009; Huang and Yates 2010) and co-training (K¨ubler and Baucom 2011) All of this work uses batch learning For space reasons we do not discuss supervised DA (e g Daum´e III and Marcu (2006)) unknown wo ds e o sa c on ne ba ch unseens e o sa c on ne ba ch known wo ds 5 F gu e 1 E o a es o unknown wo ds wo ds w h unseen ags and known wo ds o sma u 0 The x ax s ep esen s he numbe o okens o he espec ve ype e g numbe o okens o unknown wo ds more and more is learned with each additional occurrence of an unknown word (top) Interestingly the error of O NL NE increases for unseens and known words (middle&b"
D15-1155,R11-1006,0,0.040289,"Missing"
D15-1155,D11-1143,1,0.836772,"Missing"
D15-1155,D12-1131,0,0.0131555,"another supervised learning framework that processes training examples – usually obtained interactively – in small batches (Bordes et al 2005) All of this work on supervised online learning is not directly relevant to this paper since we address the problem of unsupervised DA Unlike online supervised learners we keep the statistical model unchanged during DA and adopt a representation learning approach: each unlabeled context of a word is used to update its representation There is much work on unsupervised DA for POS tagging including work using constraintbased methods (Subramanya et al 2010; Rush et al 2012) instance weighting (Choi and Palmer 2012) self-training (Huang et al 2009; Huang and Yates 2010) and co-training (K¨ubler and Baucom 2011) All of this work uses batch learning For space reasons we do not discuss supervised DA (e g Daum´e III and Marcu (2006)) unknown wo ds e o sa c on ne ba ch unseens e o sa c on ne ba ch known wo ds 5 F gu e 1 E o a es o unknown wo ds wo ds w h unseen ags and known wo ds o sma u 0 The x ax s ep esen s he numbe o okens o he espec ve ype e g numbe o okens o unknown wo ds more and more is learned with each additional occurrence of an unknown word (top) Interest"
D15-1155,Q14-1002,1,0.83975,"Missing"
D15-1155,D10-1017,0,0.015659,"1; Laws et al 2011) is another supervised learning framework that processes training examples – usually obtained interactively – in small batches (Bordes et al 2005) All of this work on supervised online learning is not directly relevant to this paper since we address the problem of unsupervised DA Unlike online supervised learners we keep the statistical model unchanged during DA and adopt a representation learning approach: each unlabeled context of a word is used to update its representation There is much work on unsupervised DA for POS tagging including work using constraintbased methods (Subramanya et al 2010; Rush et al 2012) instance weighting (Choi and Palmer 2012) self-training (Huang et al 2009; Huang and Yates 2010) and co-training (K¨ubler and Baucom 2011) All of this work uses batch learning For space reasons we do not discuss supervised DA (e g Daum´e III and Marcu (2006)) unknown wo ds e o sa c on ne ba ch unseens e o sa c on ne ba ch known wo ds 5 F gu e 1 E o a es o unknown wo ds wo ds w h unseen ags and known wo ds o sma u 0 The x ax s ep esen s he numbe o okens o he espec ve ype e g numbe o okens o unknown wo ds more and more is learned with each additional occurrence of an unknown w"
D15-1272,C10-3009,0,0.0148955,"Missing"
D15-1272,P14-5010,0,0.00199005,"Missing"
D15-1272,chrupala-etal-2008-learning,0,0.0925021,"Missing"
D15-1272,J93-2004,0,0.0488165,"G models not using morphology (+dict) (×) or both (+ ×) are marked. More baseline numbers in the appendix (Table A2). including OOVs, and only requires the same training corpus as a generic tagger (containing tags and lemmata), a resource that is available for many languages. 5 Experiments Datasets. We present experiments on the joint task of lemmatization and tagging in six diverse languages: English, German, Czech, Hungarian, Latin and Spanish. We use the same data sets as in M¨uller and Sch¨utze (2015), but do not use the out-of-domain test sets. The English data is from the Penn Treebank (Marcus et al., 1993), Latin from PROIEL (Haug and Jøhndal, 2008), German and Hungarian from SPMRL 2013 (Seddah et al., 2013), and Spanish and Czech from CoNLL 2009 (Hajiˇc et al., 2009). For German, Hungarian, Spanish and Czech we use the splits from the shared tasks; for English the split from SANCL (Petrov and McDonald, 2012); and for Latin a 8/1/1 split into train/dev/test. For all languages we limit our training data to the first 100,000 tokens. Dataset statistics can be found in Table A4 of the appendix. The lemma of Spanish se is set to be consistent. Baselines. We compare our model to three baselines. (i)"
D15-1272,N15-1055,1,0.894932,"Missing"
D15-1272,W02-1001,0,0.0542005,"g/gnu/aspell/dict Example: for the Spanish noun medidas “measures” with attributes N OUN, C OMMON, P LURAL and F EMININE, we conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum entropy Markov models (Ratnaparkhi, 1996) for morphological tagging and lemmatization, which are queried using a beam search decoder. In our experiments we use the latest version5 of M ORFETTE. This version is based on structured perceptron learning (Collins, 2002) and edit trees (Chrupała, 2008). Models similar to M ORFETTE include those of Bj¨orkelund et al. (2010) and Gesmundo and Samardzic (2012) and have also been used for generation (Duˇsek and Jurˇc´ıcˇ ek, 2013). Wicentowski (2002) similarly treats lemmatization as classification over a deterministically chosen candidate set, but uses distributional information extracted from large corpora as a key source of information. Toutanova and Cherry (2009)’s joint morphological analyzer predicts the set of possible lemmata and coarse-grained POS for a word type. This is different from our problem of lem"
D15-1272,D13-1032,1,0.912146,"Missing"
D15-1272,P14-2102,1,0.754648,"Missing"
D15-1272,D08-1113,0,0.0505869,"Missing"
D15-1272,P13-3023,0,0.0421399,"Missing"
D15-1272,E12-1068,1,0.283993,"isambiguate the lemma of a form, which explains why many NLP systems (Manning et al., 2014; Padr´o and Stanilovsky, 2012) apply a pipeline approach of tagging followed by lemmatization. Conversely, knowing the lemma of a form is often beneficial for tagging, for instance in the presence of syncretism; e.g., since German plural noun phrases do not mark gender, it is important to know the lemma (singular form) to correctly tag gender on the noun. Introduction Lemmatization is important for many NLP tasks, including parsing (Bj¨orkelund et al., 2010; Seddah et al., 2010) and machine translation (Fraser et al., 2012). Lemmata are required whenever we want to map words to lexical resources and establish the relation between inflected forms, particularly critical for morphologically rich languages to address the sparsity of unlemmatized forms. This strongly motivates work on language-independent tokenbased lemmatization, but until now there has been little work (Chrupała et al., 2008). Many regular transformations can be described by simple replacement rules, but lemmatization of unknown words requires more than this. For instance the Spanish paradigms for verbs ending in ir and er share the same 3rd person"
D15-1272,P12-2072,0,0.0321468,"e conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum entropy Markov models (Ratnaparkhi, 1996) for morphological tagging and lemmatization, which are queried using a beam search decoder. In our experiments we use the latest version5 of M ORFETTE. This version is based on structured perceptron learning (Collins, 2002) and edit trees (Chrupała, 2008). Models similar to M ORFETTE include those of Bj¨orkelund et al. (2010) and Gesmundo and Samardzic (2012) and have also been used for generation (Duˇsek and Jurˇc´ıcˇ ek, 2013). Wicentowski (2002) similarly treats lemmatization as classification over a deterministically chosen candidate set, but uses distributional information extracted from large corpora as a key source of information. Toutanova and Cherry (2009)’s joint morphological analyzer predicts the set of possible lemmata and coarse-grained POS for a word type. This is different from our problem of lemmatization and fine-grained morphological tagging of tokens in context. Despite the superficial similarity of the two problems, direct com"
D15-1272,padro-stanilovsky-2012-freeling,0,0.0187324,"Missing"
D15-1272,W96-0213,0,0.203493,"e results of the joint model by initializing it with the parameters of a pretrained tagging model. 4 Related Work In functionality, our system resembles M ORFETTE (Chrupała et al., 2008), which generates lemma 3 ftp://ftp.gnu.org/gnu/aspell/dict Example: for the Spanish noun medidas “measures” with attributes N OUN, C OMMON, P LURAL and F EMININE, we conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum entropy Markov models (Ratnaparkhi, 1996) for morphological tagging and lemmatization, which are queried using a beam search decoder. In our experiments we use the latest version5 of M ORFETTE. This version is based on structured perceptron learning (Collins, 2002) and edit trees (Chrupała, 2008). Models similar to M ORFETTE include those of Bj¨orkelund et al. (2010) and Gesmundo and Samardzic (2012) and have also been used for generation (Duˇsek and Jurˇc´ıcˇ ek, 2013). Wicentowski (2002) similarly treats lemmatization as classification over a deterministically chosen candidate set, but uses distributional information extracted from"
D15-1272,W10-1410,0,0.0704151,"Missing"
D15-1272,P08-1103,0,0.0390074,"int morphological analyzer predicts the set of possible lemmata and coarse-grained POS for a word type. This is different from our problem of lemmatization and fine-grained morphological tagging of tokens in context. Despite the superficial similarity of the two problems, direct comparison is not possible. TC’s model is best thought of as inducing a tagging dictionary for OOV types, mapping them to a set of tag and lemma pairs, whereas L EM MING is a token-level, context-based morphological tagger. We do, however, use TC’s model of lemmatization, a string-to-string transduction model based on Jiampojamarn et al. (2008) (JCK), as a standalone baseline. Our tagging-in-context model is faced with higher complexity of learning and inference since it addresses a more difficult task; thus, while we could in principle use JCK as a replacement for our candidate selection, the edit tree approach – which has high coverage at a low average number of lemma candidates (cf. Section 5) – allows us to train and apply L EMMING efficiently. Smith et al. (2005) proposed a log-linear model for the context-based disambiguation of a morphological dictionary. This has the effect of joint tagging, morphological segmentation and le"
D15-1272,H05-1060,0,0.0361191,"L EM MING is a token-level, context-based morphological tagger. We do, however, use TC’s model of lemmatization, a string-to-string transduction model based on Jiampojamarn et al. (2008) (JCK), as a standalone baseline. Our tagging-in-context model is faced with higher complexity of learning and inference since it addresses a more difficult task; thus, while we could in principle use JCK as a replacement for our candidate selection, the edit tree approach – which has high coverage at a low average number of lemma candidates (cf. Section 5) – allows us to train and apply L EMMING efficiently. Smith et al. (2005) proposed a log-linear model for the context-based disambiguation of a morphological dictionary. This has the effect of joint tagging, morphological segmentation and lemmatization, but, critically, is limited to the entries in the morphological dictionary (without which the approach cannot be used), causing problems of recall. In contrast, L EMMING can analyze any word, 5 https://github.com/ gchrupala/morfette/commit/ ca886556916b6cc1e808db4d32daf720664d17d6 2270 cs 5 6 7 JCK L EMMING -P 4 9 10 11 12 13 L EMMING -J 8 tag+lemma +mrph +dict 3 lemma +dict 2 M AR M OT tag lemma tag+lemma lemma tag"
D15-1272,P09-1055,0,0.102223,"e been used in different previous models. All features are extracted given a form-lemma pair hw, li created with an edit tree e. We use the following three edit tree features of Chrupała (2008). (i) The edit tree e. (ii) The pair he, wi. This feature is crucial for the model to memorize irregular forms, e.g., the lemma of was is be. (iii) For each form affix (of maximum length 10): its conjunction with e. These features are useful in learning orthographic and phonological regularities, e.g., the lemma of signalling is signal, not signall. We define the following alignment features. Similar to Toutanova and Cherry (2009) (TC), we define an alignment between w and l. Our alignments can be read from an edit tree by aligning the characters in LCS nodes character by character and characters in substitution nodes block-wise. Thus the alignment of umgeschaut - umschauen is: u-u, m-m, ge-, s-s, c-c, h-h, a-a, u-u, t-en. Each alignment pair constitutes a feature in our model. These features allow the model to learn that the substitution t/en is likely in German. We also concatenate each alignment pair with its form and lemma character context (of up to length 6) to learn, e.g., that ge is often deleted after um. We"
D15-1272,P09-1054,0,0.0122241,"apitalization, digits, hyphens) and the immediate lexical context. We combine lemmatization and higher-order CRF components in a treestructured CRF. Given a sequence of forms w with lemmata l and morphological+POS tags m, we define a globally normalized model: Q T p(l, m |w) ∝ i hwi (li ) exp(f (li , wi , mi ) θ +g(mi , mi−1 , mi−2 , w, i)T λ), where f and g are the features associated with lemma and tag cliques respectively and θ and λ are weight vectors. The graphical model is shown in Figure 2. We perform inference with belief propagation (Pearl, 1988) and estimate the parameters with SGD (Tsuruoka et al., 2009). We greatly improved the results of the joint model by initializing it with the parameters of a pretrained tagging model. 4 Related Work In functionality, our system resembles M ORFETTE (Chrupała et al., 2008), which generates lemma 3 ftp://ftp.gnu.org/gnu/aspell/dict Example: for the Spanish noun medidas “measures” with attributes N OUN, C OMMON, P LURAL and F EMININE, we conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum"
D15-1272,C00-2137,0,0.0183128,"xperiments showed that correct morphological attributes would substantially improve lemmatization as they help in cases of ambiguity. As an example, number helps to lemmatize the singular German noun Raps “canola”, which looks like the plural of Rap “rap”. Numbers can be found in Table A3 of the appendix. This motivates the necessity of joint tagging and lemmatization. For the final experiments, we run pipeline models on tags predicted by M AR M OT (M¨uller et al., 2013) and compare them to L EMMING -J, the 7 8 Unknown word accuracies in the appendix (Table A1). We use the randomization test (Yeh, 2000) and p = .05. joint model described in Section 3. All L EMMING versions use exactly the same features. Table 2 shows that L EMMING -J outperforms L EMMING P in three measures (see bold tag, lemma & joint (tag+lemma) accuracies) except for English, where we observe a tie in lemma accuracy and a small drop in tag and tag+lemma accuracy. Coupling morphological attributes and lemmatization (lines 8–10 vs 11–13) improves tag+lemma prediction for five languages. Improvements in lemma accuracy of the joint over the best pipeline systems range from .1 (Spanish), over >.3 (German, Hungarian) to ≥.96 (C"
D16-1071,baroni-bisi-2004-using,0,0.0531312,"to be less important) are not the main focus of the paper. Instead, 743 MRLs are the main focus. For these, we show large improvements on several tasks. Recently, K¨oper et al. (2015) compared form and lemma embeddings on English and German focusing on morpho-syntactic and semantic relation tasks. Generally, they found that lemmatization has limited impact. We extensively study MRLs and find a strong improvement on MRLs when using normalization, on intrinsic as well as extrinsic evaluations. Synonymy detection is a well studied problem in the NLP community (Turney, 2001; Turney et al., 2003; Baroni and Bisi, 2004; Ruiz-Casado et al., 2005; Grigonyt˙e et al., 2010). Rei and Briscoe (2014) classify hyponomy relationships through embedding similarity. Our premise is that semantic similarity comprises all of these relations and more. Our ranking-based word relation evaluation addresses this issue. Similar to Melamud et al. (2014), our motivation is that, in contrast to standard word similarity benchmarks, large resources can be automatically generated for any language with a WordNet. This is also exploited by Tsvetkov et al. (2015). Their intrinsic evaluation method requires an annotated corpus, e.g., ann"
D16-1071,P14-1023,0,0.0233132,"ith highly optimized state-of-the-art systems.6 On German, both S TEM and L AMB perform better on all datasets except WS. We set the new stateof-the-art of 0.79 on Gur350 (compared to 0.77 (Szarvas et al., 2011)) and 0.39 on ZG (compared to 0.25 (Botha and Blunsom, 2014)); 0.83 on Gur65 (compared to 0.79 (K¨oper et al., 2015)) is the best performance of a system that does not need additional knowledge bases (cf. Navigli and Ponzetto (2012), Szarvas et al. (2011)). L AMB’s results on Spanish are equally good. 0.82 on MC and 0.58 on WS are again the best per5 cistern.cis.lmu.de/marmot/naacl2015 Baroni et al. (2014)’s numbers are higher on some of the datasets for the best of 48 different parameter configurations. In contrast, we do not tune parameters. 6 formances of a system not requiring an additional knowledge base (cf. Navigli and Ponzetto (2012)). The best performance before was 0.64 for MC and 0.50 for WS (both Hassan and Mihalcea (2009)). S TEM cannot improve over form embeddings, showing the difficulty of Spanish morphology. 4.2 Word Relations Word similarity benchmarks are not available for many languages and are expensive to create. To remedy this situation, we create word similarity benchmark"
D16-1071,W15-2915,1,0.898423,"Missing"
D16-1071,C10-1046,0,0.0714085,"Missing"
D16-1071,I05-1067,0,0.114885,"Missing"
D16-1071,W13-1609,0,0.0277241,"est stemming model most often is better than the best form model. L AMB can benefit more from the POS type restriction than the form models. The distance to the best form model generally increases, especially on German adjectives and Spanish verbs. In all cases except on English adjectives, L AMB yields the best performance. Again, in almost all cases L AMB’s improvement over the formmodels is significant. 4.3 Polarity Classification Our first two evaluations were intrinsic. We now show the benefit of normalization on an extrinsic task. The task is classification of Czech movie reviews (CSFD, Habernal et al. (2013)) into positive, negative or neutral (Table 4). We reimplement lingCNN (Ebert et al., 2015), a Convolutional Neural Network that uses linguistic information to improve polarity classification. This model reaches 747 close to state-of-the-art performance on data of the SemEval 2015 Task 10B (message level polarity). LingCNN takes several features as input: (i) embedding features, (ii) linguistic features at word level and (iii) linguistic features at review level. We reuse the 50-dimensional Wikipedia embeddings from Section 4.2 and compare three experimental conditions: using forms, S TEM and"
D16-1071,S15-2097,0,0.0648061,"Missing"
D16-1071,W97-0802,0,0.395324,"they are clear from context. To measure the quality of a set of embeddings we compute the mean reciprocal rank (MRR) on the rank results of all lemmata: 1 X 1 MRRE = |L| rankE (l) We create large similarity datasets for five languages: Czech (cz), English (en), German (de), Hungarian (hu) and Spanish (es) by extracting all lemmata from the WordNet version of the respective language. For English and Spanish we use the preprocessed WordNets from the Open Multilingual WordNet (Bond and Paik, 2012). We use the Czech and Hungarian WordNets (PALA and SMRZ, 2004; Mih´altz et al., 2008) and GermaNet (Hamp and Feldweg, 1997) for German. We keep all lemmata that have a known form in the form embeddings and that exist in the lemma embeddings. Moreover, we filter out all synsets that contain only one lemma and discard all multiword phrases. The split into development and test sets is done in a way that the distribution of synset sizes (i.e., the number of lemmata per synset) is nearly equal in both sets. The number of lemmata in our evaluation sets can be found in Table 2. For more insight, we report results on all parts-of-speech (POS), as well as separately for nouns (n), verbs (v) and adjectives (a).7 The data is"
D16-1071,D09-1124,0,0.0603925,"is the best performance of a system that does not need additional knowledge bases (cf. Navigli and Ponzetto (2012), Szarvas et al. (2011)). L AMB’s results on Spanish are equally good. 0.82 on MC and 0.58 on WS are again the best per5 cistern.cis.lmu.de/marmot/naacl2015 Baroni et al. (2014)’s numbers are higher on some of the datasets for the best of 48 different parameter configurations. In contrast, we do not tune parameters. 6 formances of a system not requiring an additional knowledge base (cf. Navigli and Ponzetto (2012)). The best performance before was 0.64 for MC and 0.50 for WS (both Hassan and Mihalcea (2009)). S TEM cannot improve over form embeddings, showing the difficulty of Spanish morphology. 4.2 Word Relations Word similarity benchmarks are not available for many languages and are expensive to create. To remedy this situation, we create word similarity benchmarks that leverage WordNets, which are available for a great number of languages. Generally, a representation is deemed good if words related by a lexical relation in WordNet – synonymy, hyponymy etc. – have high cosine similarity with this representation. Since the gold standard necessary for measuring this property of a representation"
D16-1071,J15-4004,0,0.0953443,"Missing"
D16-1071,P14-1062,0,0.0684305,"icators that word is marked positive/negative); (ii) SentiStrength10 (three binary indicators that word is an emoticon marked as positive/negative/neutral); (iii) prefix “ne” (binary negation indicator in Czech).11 All word level features are concatenated to form a single word representation of the review’s input words. The concatenation of these representations is the input to a convolution layer, which has several filters spanning the whole representation height and several representations (i.e., several words) in width. The output of the convolution layer is input to a k-max pooling layer (Kalchbrenner et al., 2014). The max values are concatenated with the following linguistic review level features: (i) the count of elongated words, such as “cooool”; (ii) three count features for the number of positive/negative/neutral emoticons using the SentiStrength list; (iii) a count feature for punctuation sequences, such as “!!!”; (iv) and a feature that counts the number of negated words. (v) A final feature type comprises one count feature each for the number of sentiment words in a review, the sum of sentiment values of these words as provided by the sentiment lexicon, the maximum sentiment value and the senti"
D16-1071,W15-0105,0,0.0358731,"Missing"
D16-1071,W13-3512,0,0.112418,"Missing"
D16-1071,J93-2004,0,0.0619141,"y to be known. For all stemming experiments we use SNOWBALL,2 a widely used stemmer. It normalizes a form based on deterministic rules, such as replace the suffix ‘tional’ by ‘tion’ for English. For lemmatization we use the pipeline version of the freely available, high-quality lemmatizer L EM 2 snowball.tartarus.org MING (M¨ uller et al., 2015). Since it is a languageindependent token-based lemmatizer it is especially suited for our multi-lingual experiments. Moreover, it reaches state-of-the-art performance for the five languages that we study. We train the pipeline using the Penn Treebank (Marcus et al., 1993) for English, SPMRL 2013 shared task data (Seddah et al., 2013) for German and Hungarian, and CoNLL 2009 (Hajiˇc et al., 2009) datasets for Spanish and Czech. We additionally use a unigram list extracted from Wikipedia datasets and the A SPELL dictionary of each language.3 4 Experiments 4.1 Word Similarity Our first experiment evaluates how well S TEM/L AMB embeddings predict human word similarity judgments. Given a pair of words (m, n) with a human-generated similarity value and a set of embeddings E we compute their similarity as cosine similarity. For form embeddings E F , we directly use t"
D16-1071,S13-2053,0,0.0328609,"ated with the following linguistic review level features: (i) the count of elongated words, such as “cooool”; (ii) three count features for the number of positive/negative/neutral emoticons using the SentiStrength list; (iii) a count feature for punctuation sequences, such as “!!!”; (iv) and a feature that counts the number of negated words. (v) A final feature type comprises one count feature each for the number of sentiment words in a review, the sum of sentiment values of these words as provided by the sentiment lexicon, the maximum sentiment value and the sentiment value of the last word (Mohammad et al., 2013). The concatenation of max values and review level features is then forwarded into a fully-connected three-class (positive, negative, neutral) softmax layer. We train lingCNN with AdaGrad (Duchi et al., 2011) and early stopping, batch size = 100, 200 filters per width of 3-6; k-max pooling with k = 5; learning rate 0.01; and `2 regularization (λ = 5 · 10−5 ). We also perform this experiment for English on 10 sentistrength.wlv.ac.uk/ We disregard words with the prefix “nej”, because they indicate superlatives. Exceptions are common negated words with this prefix, such as “nejsi” (engl. “you are"
D16-1071,N15-1055,1,0.88427,"Missing"
D16-1071,D15-1272,1,0.911981,"Missing"
D16-1071,N15-1186,0,0.0222919,"premise is that semantic similarity comprises all of these relations and more. Our ranking-based word relation evaluation addresses this issue. Similar to Melamud et al. (2014), our motivation is that, in contrast to standard word similarity benchmarks, large resources can be automatically generated for any language with a WordNet. This is also exploited by Tsvetkov et al. (2015). Their intrinsic evaluation method requires an annotated corpus, e.g., annotated with WordNet supersenses. Our approach requires only the WordNet. An alternative strategy of dealing with data sparsity is presented by Soricut and Och (2015). They compute morphological features in an unsupervised fashion in order to construct a form embedding by the combination of the word’s morphemes. We address scenarios (such as polarity classification) in which morphological information is less important, thus morpheme embeddings are not needed. 3 Stem/Lemma Creation The main hypothesis of this work is that normalization addresses sparsity issues, especially for MRLs, because although a particular word form might not have been seen in the text, its stem or lemma is more likely to be known. For all stemming experiments we use SNOWBALL,2 a wide"
D16-1071,D15-1243,0,0.0234307,"ll studied problem in the NLP community (Turney, 2001; Turney et al., 2003; Baroni and Bisi, 2004; Ruiz-Casado et al., 2005; Grigonyt˙e et al., 2010). Rei and Briscoe (2014) classify hyponomy relationships through embedding similarity. Our premise is that semantic similarity comprises all of these relations and more. Our ranking-based word relation evaluation addresses this issue. Similar to Melamud et al. (2014), our motivation is that, in contrast to standard word similarity benchmarks, large resources can be automatically generated for any language with a WordNet. This is also exploited by Tsvetkov et al. (2015). Their intrinsic evaluation method requires an annotated corpus, e.g., annotated with WordNet supersenses. Our approach requires only the WordNet. An alternative strategy of dealing with data sparsity is presented by Soricut and Och (2015). They compute morphological features in an unsupervised fashion in order to construct a form embedding by the combination of the word’s morphemes. We address scenarios (such as polarity classification) in which morphological information is less important, thus morpheme embeddings are not needed. 3 Stem/Lemma Creation The main hypothesis of this work is that"
D16-1071,W14-1619,0,\N,Missing
D16-1097,W14-4012,0,0.188779,"Missing"
D16-1097,D14-1179,0,0.0413197,"Missing"
D16-1097,P11-1004,0,0.1291,"Missing"
D16-1097,K15-1017,1,0.890809,"Missing"
D16-1097,N16-1080,1,0.852389,"Missing"
D16-1097,W02-0603,0,0.445604,"el’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved"
D16-1097,D08-1113,0,0.059286,"tation. Baselines. Our first baseline is the joint transduction and segmentation model (JOINT) of Cotterell et al. (2016). It is the current state of the art on the datasets we use and the task of canonical segmentation in general. This model uses a jointly trained, separate transduction and segmentation component. Importantly, the joint model of Cotterell et al. (2016) already contains segment-level features. Thus, reranking this baseline would not provide a similar boost. Our second baseline is a weighted finite-state transducer (WFST) (Mohri et al., 2002) with a loglinear parameterization (Dreyer et al., 2008), again, taken from Cotterell et al. (2016). The WFST baseline is particularly relevant because, like our encoder-decoder, it formulates the problem directly as a string-to-string transduction. Evaluation Metrics. We follow Cotterell et al. (2016) and use the following evaluation measures: error rate, edit distance and morpheme F1 . Error rate is defined as 1 minus the proportion of guesses that are completely correct. Edit distance is the Levenshtein distance between guess and gold standard. For this, guess and gold are each represented as one string with a distinguished character denoting th"
D16-1097,J01-2001,0,0.0390897,"raves et al., 2013). 4 Experiments k∈Sw The reranking model’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance wit"
D16-1097,P16-2090,1,0.887592,"Missing"
D16-1097,W10-2210,0,0.569431,"e reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morphological segmentation using a window LSTM. Even though Wang et al. (2016) also employ"
D16-1097,N16-3005,0,0.0128112,"3). 4 Experiments k∈Sw The reranking model’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More"
D16-1097,D14-1095,0,0.268843,"Missing"
D16-1097,N09-1024,0,0.209303,"for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morpho"
D16-1097,W13-3504,0,0.319601,"the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morphological segmentation using a window LSTM. Even though Wang et al. (2016) also employ a recurrent neural network, we distinguish our approach, in that we focus on canonical mor"
D16-1097,Q15-1026,0,0.180686,"Missing"
D16-1097,P13-1118,0,0.0302509,"Missing"
D16-1256,P99-1037,0,0.586811,"Missing"
D16-1256,D13-1034,0,0.020932,"oding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated"
D16-1256,P11-1004,0,0.175378,"s, which are typically taken to be the smallest meaning-bearing units in language. This work concerns itself with modeling hierarchical structure over these morphemes. Note a simple flat 1 We found post publication that CELEX (Baayen et al., 1993) has annotated words for hierarchical morphological segmentation as well. morphological segmentation can also be straightforwardly derived from the CFG parse tree. Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C¸etino˘glu, 2015). In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation. For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le. We make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone. We experimentally show that this model outperforms a semi-Markov model on flat segmentation. (ii) We release the first morphology treebank, consisting of 7454 English word types, each a"
D16-1256,P14-2102,1,0.861633,"Dreyer et al. (2008) for a thorough exposition. For ease of computation, we can encode this function as a weighted finite-state machine (WFST) (Mohri et al., 2002). This requires, however, that the feature function g factors over the topology of the finite-state encoding. Since our model conditions on the word w, the feature function g can extract features from any part of this string. Features on the output string, u, however, are more restricted. In this work, we employ a bigram model over output characters. This implies that each state remembers exactly one character: the previous one. See Cotterell et al. (2014) for details. We can compute the score for two strings u and w using a weighted generalization of the Levenshtein algorithm. Computing the partition function requires a different dynamic program, which runs in O(|w|2 · |Σ|2 ) time. Note that since |Σ |≈ 26 (lower case English letters), it takes roughly 262 = 676 times longer to compute the partition function than to score a pair of strings. Our model includes several simple feature templates, including features that fire on individual edit actions as well as conjunctions of edit actions and 2326 ROOT W ORD W ORD W ORD P REFIX S UFFIX → → → → →"
D16-1256,N16-1080,1,0.873925,"Missing"
D16-1256,D08-1113,0,0.0295802,"fore describing the joint model 2 For efficiency, we assume u ∈ Σ|w|+k , k = 5. in detail, we first consider its pieces individually. 3.1 Restoring Orthographic Changes To extract a canonical segmentation (Naradowsky and Goldwater, 2009; Cotterell et al., 2016), we restore orthographic changes that occur during word formation. To this end, we define the score function   scoreη (u, a, w) = exp g(u, a, w)> η (1) where a is a monotonic alignment between the strings u and w. The goal is for scoreη to assign higher values to better matched pairs, e.g., (w=untestably, u=untestablely). We refer to Dreyer et al. (2008) for a thorough exposition. For ease of computation, we can encode this function as a weighted finite-state machine (WFST) (Mohri et al., 2002). This requires, however, that the feature function g factors over the topology of the finite-state encoding. Since our model conditions on the word w, the feature function g can extract features from any part of this string. Features on the output string, u, however, are more restricted. In this work, we employ a bigram model over output characters. This implies that each state remembers exactly one character: the previous one. See Cotterell et al. (20"
D16-1256,N16-1024,0,0.0345825,"llel construction), we make use of an importancesampling estimator, derived by Cotterell et al. (2016), which is faster in practice. Roughly speaking, we approximate the hard-to-samplefrom distribution pθ by taking samples from an easy-to-sample-from proposal distribution q. Specifically, we employ a pipeline model for q consisting of WFST and then a WCFG sampled from consecutively. We then reweight the samples using the unnormalized score from pθ . Importance sampling has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016). Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient. 4.2 Decoding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-"
D16-1256,D14-1095,0,0.0766574,"ose words into smaller units, known as morphemes, which are typically taken to be the smallest meaning-bearing units in language. This work concerns itself with modeling hierarchical structure over these morphemes. Note a simple flat 1 We found post publication that CELEX (Baayen et al., 1993) has annotated words for hierarchical morphological segmentation as well. morphological segmentation can also be straightforwardly derived from the CFG parse tree. Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C¸etino˘glu, 2015). In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation. For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le. We make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone. We experimentally show that this model outperforms a semi-Markov model on flat segmentation. (ii) We release the first morphology treeban"
D16-1256,H05-1065,0,0.0395242,"canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To remedy this, we provide tree"
D16-1256,schmid-etal-2004-smor,0,0.0439404,"ted Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To remedy this, we provide tree annotations for a subset of the English portion of CELEX (Baayen et al., 1993). We reannotated"
D16-1256,P08-1109,0,0.0868813,"Missing"
D16-1256,Q15-1026,0,0.186647,"Missing"
D16-1256,P14-1022,0,0.15414,"g the hierarchical structure allows us to select the correct reading; note there are even cases of true ambiguity; e.g., unlockable has two readings: “unable to be locked” and “able to be unlocked.” We also note that theoretical linguists often implicitly assume a context-free treatment of word formation, e.g., by employing brackets to indicate different levels of affixation. Others have explicitly modeled word-internal structure with grammars (Selkirk, 1982; Marvin, 2002). 3 Parsing the Lexicon A novel component of this work is the development of a discriminative parser (Finkel et al., 2008; Hall et al., 2014) for morphology. The goal is to define a probability distribution over all trees that could arise from the input word, after reversal of orthographic and phonological processes. We employ the simple grammar shown in Table 1. Despite its simplicity, it models the order in which morphemes are attached. More formally, our goal is to map a surface form w (e.g., w=untestably) into its underlying canonical form u (e.g., u=untestablely) and then into a parse tree t over its morphemes. We assume u, w ∈ Σ∗ , for some discrete alphabet Σ.2 Note that a parse tree over the string implicitly defines a flat"
D16-1256,P15-1001,0,0.0375468,"olynomial time (using the BarHillel construction), we make use of an importancesampling estimator, derived by Cotterell et al. (2016), which is faster in practice. Roughly speaking, we approximate the hard-to-samplefrom distribution pθ by taking samples from an easy-to-sample-from proposal distribution q. Specifically, we employ a pipeline model for q consisting of WFST and then a WCFG sampled from consecutively. We then reweight the samples using the unnormalized score from pθ . Importance sampling has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016). Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient. 4.2 Decoding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars"
D16-1256,N07-1018,0,0.0454708,"ing has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016). Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient. 4.2 Decoding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014)."
D16-1256,Q13-1021,0,0.0222585,"mportance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To re"
D16-1256,J07-4003,0,0.0364486,"a parse tree (e.g., t=[[un[[test]able]]ly]). Thus, we define the parser score with the following function  scoreω (t, u) = exp   X f (π, u)> ω  (2) π∈Π(t) where Π(t) is the set of anchored productions in the tree t. An anchored production π is a grammar rule in Chomsky normal form attached to a span, e.g., Ai,k → Bi,j Cj,k . Each π is then assigned a weight by the linear function f (π, u)> ω, where the function f extracts relevant features from the anchored production as well as the corresponding span of the underlying form u. This model is typically referred to as a weighted CFG (WCFG) (Smith and Johnson, 2007) or a CRF parser. For f , we define three span features: (i) indicator features on the span’s segment, (ii) an indicator feature that fires if the segment appears in an external corpus3 and (iii) the conjunction of the segment with the label (e.g., P REFIX) of the subtree root. Following Hall et al. (2014), we employ an indicator feature for each production as well as production backoff features. 4 A Joint Model where T (u) is the set of all parse trees for the string u. This involves a sum over all possible underlying orthographic forms and all parse trees for those forms. The joint approach"
D16-1256,P14-1125,0,0.0321234,"Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To remedy this, we provide tree annotations for a subset of the English portion of CELEX (Baayen et al., 1993). We reannotated 7454 English types with a full constituency parse.4 The resource will be freely available for future research. 6.1 Annotation Guidelines The annotation of the morphology treebank was guided by t"
D17-1011,P16-1160,0,0.0135034,"shows. 120 7 Christodouloupoulos and Steedman, 2015; Lison and Tiedemann, 2016). Conclusion Parallel projection. Parallel projection across languages has been used for a variety of NLP tasks. Machine translation aside, which is the most natural task on parallel corpora (Brown et al., 1993), parallel projection has been used for sense disambiguation (Ide, 2000), parsing (Hwa et al., 2005), paraphrasing (Bannard and CallisonBurch, 2005), part-of-speech tagging (Mukerjee et al., 2006), coreference resolution (de Souza and Or˘asan, 2011), event marking (Nordrum, 2015), morphological segmentation (Chung et al., 2016), bilingual analysis of linguistic marking (McEnery and Xiao, 1999; Xiao and McEnery, 2002), as well as language classification (Asgari and Mofrad, ¨ 2016; Ostling and Tiedemann, 2017). We presented SuperPivot, an analysis method for low-resource languages that occur in a superparallel corpus, i.e., in a corpus that contains an order of magnitude more languages than parallel corpora currently in use. We showed that SuperPivot performs well for the crosslingual analysis of the linguistic phenomenon of tense. We produced analysis results for more than 1000 languages, conducting – to the best of"
D17-1011,W16-1208,1,0.787685,"Missing"
D17-1011,P05-1074,0,0.201004,"Missing"
D17-1011,W09-0106,0,0.137516,"h’s Witnesses’ texts. Areas of the map correspond to semantic roles, e.g., the Bible as actor (it tells you to do something) or as object (it was printed). This is a definition of semantic roles that is complementary to and different from prior typological research because it is empirically grounded in real language use across a large number of languages. It allows typologists to investigate traditional questions from a new perspective. The field of typology is important for both theoretical (Greenberg, 1960; Whaley, 1996; Croft, 2002) and computational (Heiden et al., 2000; Santaholma, 2007; Bender, 2009, 2011) linguistics. Typology is concerned with all areas of linguistics: morphology (Song, 2014), syntax (Comrie, 1989; Croft, 2001; Croft and Poole, 2008; Song, 2014), semantic roles (Hartmann et al., 2014; Cysouw, 2014), semantics (KoptjevskajaTamm et al., 2007; Dahl, 2014; W¨alchli and Cysouw, 2012; Sharma, 2009), etc. Typological information is important for many NLP tasks including discourse analysis (Myhill and Myhill, 1992), information retrieval (Pirkola, 2001), POS tagging (Bohnet and Nivre, 2012), parsing (Bohnet and Nivre, 2012; McDonald et al., 2013), machine translation (Hajiˇc e"
D17-1011,Q13-1034,0,0.0609877,"Missing"
D17-1011,N13-1073,0,0.0432454,"past tense word.2 We then search for words in other languages that are highly correlated with this artificial past tense word. As an additional constraint, we do not select the most highly correlated word as the head pivot, but the most highly correlated word in a Creole language. Our rationale is that Creole languages are more regular than other languages because they are young and have not accumulated “historical baggage” that may make computational analysis more difficult. Table 1 lists the three head pivots for F . 3. Projection of head pivot to larger pivot set. We first use fast align (Dyer et al., 2013) to align the head language to all other languages in the corpus. This alignment is on the word level. We compute a score for each word in each language based on the number of times it is aligned to the head pivot, the number of times it is aligned to another word and the total frequencies of head pivot and word. We use χ2 (Casella and Berger, 2008) as the score throughout this paper. Finally, Running example: We sketch the type of analysis that our new method makes possible in §4. The above steps “1. heuristic search for head pivot” and “2. projection of head pivot to larger pivot set” are ba"
D17-1011,J93-2003,0,0.0790836,"bendici´on yu’un hech laj spas . . . ” (literally “a blessing . . . LAJ make”), “eng newliving 40010042 . . . you will surely be rewarded.” Perfective aspect and past are correlated in the real world since most events that are viewed as simple wholes are in the past. But future events can also be viewed this way as the example shows. 120 7 Christodouloupoulos and Steedman, 2015; Lison and Tiedemann, 2016). Conclusion Parallel projection. Parallel projection across languages has been used for a variety of NLP tasks. Machine translation aside, which is the most natural task on parallel corpora (Brown et al., 1993), parallel projection has been used for sense disambiguation (Ide, 2000), parsing (Hwa et al., 2005), paraphrasing (Bannard and CallisonBurch, 2005), part-of-speech tagging (Mukerjee et al., 2006), coreference resolution (de Souza and Or˘asan, 2011), event marking (Nordrum, 2015), morphological segmentation (Chung et al., 2016), bilingual analysis of linguistic marking (McEnery and Xiao, 1999; Xiao and McEnery, 2002), as well as language classification (Asgari and Mofrad, ¨ 2016; Ostling and Tiedemann, 2017). We presented SuperPivot, an analysis method for low-resource languages that occur in"
D17-1011,A00-1002,0,0.190872,"Missing"
D17-1011,W06-1205,0,0.0453164,"world since most events that are viewed as simple wholes are in the past. But future events can also be viewed this way as the example shows. 120 7 Christodouloupoulos and Steedman, 2015; Lison and Tiedemann, 2016). Conclusion Parallel projection. Parallel projection across languages has been used for a variety of NLP tasks. Machine translation aside, which is the most natural task on parallel corpora (Brown et al., 1993), parallel projection has been used for sense disambiguation (Ide, 2000), parsing (Hwa et al., 2005), paraphrasing (Bannard and CallisonBurch, 2005), part-of-speech tagging (Mukerjee et al., 2006), coreference resolution (de Souza and Or˘asan, 2011), event marking (Nordrum, 2015), morphological segmentation (Chung et al., 2016), bilingual analysis of linguistic marking (McEnery and Xiao, 1999; Xiao and McEnery, 2002), as well as language classification (Asgari and Mofrad, ¨ 2016; Ostling and Tiedemann, 2017). We presented SuperPivot, an analysis method for low-resource languages that occur in a superparallel corpus, i.e., in a corpus that contains an order of magnitude more languages than parallel corpora currently in use. We showed that SuperPivot performs well for the crosslingual an"
D17-1011,folch-etal-2000-typtex,0,0.107003,"Missing"
D17-1011,E17-2102,0,0.0766923,"ty of NLP tasks. Machine translation aside, which is the most natural task on parallel corpora (Brown et al., 1993), parallel projection has been used for sense disambiguation (Ide, 2000), parsing (Hwa et al., 2005), paraphrasing (Bannard and CallisonBurch, 2005), part-of-speech tagging (Mukerjee et al., 2006), coreference resolution (de Souza and Or˘asan, 2011), event marking (Nordrum, 2015), morphological segmentation (Chung et al., 2016), bilingual analysis of linguistic marking (McEnery and Xiao, 1999; Xiao and McEnery, 2002), as well as language classification (Asgari and Mofrad, ¨ 2016; Ostling and Tiedemann, 2017). We presented SuperPivot, an analysis method for low-resource languages that occur in a superparallel corpus, i.e., in a corpus that contains an order of magnitude more languages than parallel corpora currently in use. We showed that SuperPivot performs well for the crosslingual analysis of the linguistic phenomenon of tense. We produced analysis results for more than 1000 languages, conducting – to the best of our knowledge – the largest crosslingual computational study performed to date. We extended existing methodology for leveraging parallel corpora for typological analysis by overcoming"
D17-1011,W16-4811,0,0.0201888,"istics. Typology is concerned with all areas of linguistics: morphology (Song, 2014), syntax (Comrie, 1989; Croft, 2001; Croft and Poole, 2008; Song, 2014), semantic roles (Hartmann et al., 2014; Cysouw, 2014), semantics (KoptjevskajaTamm et al., 2007; Dahl, 2014; W¨alchli and Cysouw, 2012; Sharma, 2009), etc. Typological information is important for many NLP tasks including discourse analysis (Myhill and Myhill, 1992), information retrieval (Pirkola, 2001), POS tagging (Bohnet and Nivre, 2012), parsing (Bohnet and Nivre, 2012; McDonald et al., 2013), machine translation (Hajiˇc et al., 2000; Kunchukuttan and Bhattacharyya, 2016) and morphology (Bohnet et al., 2013). Tense is a central phenomenon in linguistics and the languages of the world differ greatly in whether and how they express tense (Traugott, 1978; Bybee and Dahl, 1989; Dahl, 2000, 1985; Santos, 2004; Dahl, 2007; Santos, 2004; Dahl, 2014). Low resource. Even resources with the widest coverage like World Atlas of Linguistic Structures (WALS) (Dryer et al., 2005) have little information for hundreds of languages. Many researchers have taken advantage of parallel information for extracting linguistic knowledge in low-resource settings (Resnik et al., 1997; Re"
D17-1011,L16-1147,0,0.0299248,"st tense pivots are not perfective markers, so that there are verses that are marked with “laj”, but not marked with the past tense pivots of the other four languages. Example: “tzo huixtan 40010042 . . . ja’ch-ac’bat bendici´on yu’un hech laj spas . . . ” (literally “a blessing . . . LAJ make”), “eng newliving 40010042 . . . you will surely be rewarded.” Perfective aspect and past are correlated in the real world since most events that are viewed as simple wholes are in the past. But future events can also be viewed this way as the example shows. 120 7 Christodouloupoulos and Steedman, 2015; Lison and Tiedemann, 2016). Conclusion Parallel projection. Parallel projection across languages has been used for a variety of NLP tasks. Machine translation aside, which is the most natural task on parallel corpora (Brown et al., 1993), parallel projection has been used for sense disambiguation (Ide, 2000), parsing (Hwa et al., 2005), paraphrasing (Bannard and CallisonBurch, 2005), part-of-speech tagging (Mukerjee et al., 2006), coreference resolution (de Souza and Or˘asan, 2011), event marking (Nordrum, 2015), morphological segmentation (Chung et al., 2016), bilingual analysis of linguistic marking (McEnery and Xiao"
D17-1011,mayer-cysouw-2014-creating,0,0.303484,"uation below shows, the “linear alignment” assumption does not seem to do much harm given the general robustness of our method. One design element that increases robustness is that we find the two positions in each verse that are most highly (resp. least highly) correlated with the linguistic feature f . Specifically, we compute the relative position x of each pivot that occurs in the verse and apply a Gaussian filter (σ = 6 where the unit of length is the character), i.e., we set p(x) ≈ 3 3.1 Data, experiments and results Data We use a New Testament subset of the Parallel Bible Corpus (PBS) (Mayer and Cysouw, 2014) that consists of 1556 translations of the Bible in 1169 unique languages. We consider two languages to be different if they have different ISO 639-3 codes. The translations are aligned on the verse level. However, many translations do not have complete coverage, so that most verses are not present in at least one translation. One reason for this is that sometimes several consecutive verses are merged, so that one verse contains material that is in reality not part of it and the merged verses may then be missing from the translation. Thus, there is a trade-off between number of parallel transl"
D17-1011,W07-2438,0,0.0473163,"l corpus of Jehovah’s Witnesses’ texts. Areas of the map correspond to semantic roles, e.g., the Bible as actor (it tells you to do something) or as object (it was printed). This is a definition of semantic roles that is complementary to and different from prior typological research because it is empirically grounded in real language use across a large number of languages. It allows typologists to investigate traditional questions from a new perspective. The field of typology is important for both theoretical (Greenberg, 1960; Whaley, 1996; Croft, 2002) and computational (Heiden et al., 2000; Santaholma, 2007; Bender, 2009, 2011) linguistics. Typology is concerned with all areas of linguistics: morphology (Song, 2014), syntax (Comrie, 1989; Croft, 2001; Croft and Poole, 2008; Song, 2014), semantic roles (Hartmann et al., 2014; Cysouw, 2014), semantics (KoptjevskajaTamm et al., 2007; Dahl, 2014; W¨alchli and Cysouw, 2012; Sharma, 2009), etc. Typological information is important for many NLP tasks including discourse analysis (Myhill and Myhill, 1992), information retrieval (Pirkola, 2001), POS tagging (Bohnet and Nivre, 2012), parsing (Bohnet and Nivre, 2012; McDonald et al., 2013), machine transla"
D17-1011,P13-2017,0,0.0740327,"Missing"
D17-1011,D12-1133,0,\N,Missing
D17-1011,H01-1035,0,\N,Missing
D17-1011,P02-1033,0,\N,Missing
D17-1011,P11-1061,0,\N,Missing
D17-1011,P06-4018,0,\N,Missing
D17-1011,D14-1204,0,\N,Missing
D17-1011,W13-0307,0,\N,Missing
D17-1011,D17-1268,0,\N,Missing
D17-1011,loaiciga-etal-2014-english,0,\N,Missing
D17-1181,P16-1231,0,0.0367121,"Therefore, we propose a single neural network (NN) for both tasks. In contrast to joint training and multitask learning, which calculate taskwise costs, we propose to learn a joint classification layer which is globally normalized on the outputs of both tasks. In particular, we train the NN parameters based on the loss of a linear-chain Figure 1: Examples of our task conditional random field (CRF) (Lafferty et al., 2001). CRF layers for NNs have been introduced for token-labeling tasks like named entity recognition (NER) or part-of-speech tagging (Collobert et al., 2011; Lample et al., 2016; Andor et al., 2016). Instead of labeling each input token as in previous work, we model the joint entity and relation classification problem as a sequence of length three for the CRF layer. In particular, we identify the types of two candidate entities (words or short phrases) given a sentence (we call this entity classification to distinguish it from the token-labeling task NER) as well as the relation between them. To the best of our knowledge, this architecture for combining entity and relation classification in a single neural network is novel. Figure 1 shows an example of how we model the task: For each sen"
D17-1181,N06-1038,0,0.0139505,"scoring function. Recently, Gupta et al. (2016) apply recurrent neural networks to fill the table. They train them in a multitask fashion. Previous work also uses a variety of linguistic features, such as part-of-speech tags. In contrast, we use convolutional neural networks and only word embeddings as input. Furthermore, we are the first to adopt global normalization of neural networks for this task. Several studies propose different variants of non-neural CRF models for information extraction tasks but model them as token-labeling problems (Sutton and McCallum, 2006; Sarawagi et al., 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006). In contrast, we propose a simpler linear-chain CRF model which directly connects entity and relation classes instead of assigning a label to each token of the input sequence. This is more similar to the factor graph by Yao et al. (2010) but computationally simpler. Xu and Sarikaya (2013) also apply a CRF layer on top of continuous representations obtained by a CNN. However, they use it for a token labeling task (semantic slot filling) while we apply the model to a sentence classification task, motivated by the fact that a CNN creates single represe"
D17-1181,C16-1239,1,0.893305,"Missing"
D17-1181,W10-2924,0,0.389986,"is.lmu.de. 2 Related Work Some work on joint entity and relation classification uses distant supervision for building their own datasets, e.g., (Yao et al., 2010; Yaghoobzadeh et al., 2016). Other studies, which are described in more detail in the following, use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004, 2007) as we do in this paper. Roth and Yih (2004) develop constraints and use linear programming to globally normalize entity types and relations. Giuliano et al. (2007) use entity type information for relation extraction but do not train both tasks jointly. Kate and Mooney (2010) train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks. Miwa and Sasaki (2014) use the same dataset but model the tasks as a table filling problem (see Section 4.2). Their model uses both a local and a global scoring function. Recently, Gupta et al. (2016) apply recurrent neural networks to fill the table. They train them in a multitask fashion. Previous work also uses a variety of linguistic features, such as part-of-speech tags. In contrast, we use convolutional neural networks and only word embeddings as input. Furthermore, we a"
D17-1181,N16-1030,0,0.310154,"hat it is a location. Therefore, we propose a single neural network (NN) for both tasks. In contrast to joint training and multitask learning, which calculate taskwise costs, we propose to learn a joint classification layer which is globally normalized on the outputs of both tasks. In particular, we train the NN parameters based on the loss of a linear-chain Figure 1: Examples of our task conditional random field (CRF) (Lafferty et al., 2001). CRF layers for NNs have been introduced for token-labeling tasks like named entity recognition (NER) or part-of-speech tagging (Collobert et al., 2011; Lample et al., 2016; Andor et al., 2016). Instead of labeling each input token as in previous work, we model the joint entity and relation classification problem as a sequence of length three for the CRF layer. In particular, we identify the types of two candidate entities (words or short phrases) given a sentence (we call this entity classification to distinguish it from the token-labeling task NER) as well as the relation between them. To the best of our knowledge, this architecture for combining entity and relation classification in a single neural network is novel. Figure 1 shows an example of how we model t"
D17-1181,D14-1200,0,0.552155,"., (Yao et al., 2010; Yaghoobzadeh et al., 2016). Other studies, which are described in more detail in the following, use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004, 2007) as we do in this paper. Roth and Yih (2004) develop constraints and use linear programming to globally normalize entity types and relations. Giuliano et al. (2007) use entity type information for relation extraction but do not train both tasks jointly. Kate and Mooney (2010) train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks. Miwa and Sasaki (2014) use the same dataset but model the tasks as a table filling problem (see Section 4.2). Their model uses both a local and a global scoring function. Recently, Gupta et al. (2016) apply recurrent neural networks to fill the table. They train them in a multitask fashion. Previous work also uses a variety of linguistic features, such as part-of-speech tags. In contrast, we use convolutional neural networks and only word embeddings as input. Furthermore, we are the first to adopt global normalization of neural networks for this task. Several studies propose different variants of non-neural CRF mod"
D17-1181,W04-2401,0,0.803078,"a single neural network and classify entities and relations at the same time, normalizing their scores globally. Our experiments confirm that a CNN with a CRF output layer outperforms a CNN with locally normalized softmax layers. Our source code is available at http: //cistern.cis.lmu.de. 2 Related Work Some work on joint entity and relation classification uses distant supervision for building their own datasets, e.g., (Yao et al., 2010; Yaghoobzadeh et al., 2016). Other studies, which are described in more detail in the following, use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004, 2007) as we do in this paper. Roth and Yih (2004) develop constraints and use linear programming to globally normalize entity types and relations. Giuliano et al. (2007) use entity type information for relation extraction but do not train both tasks jointly. Kate and Mooney (2010) train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks. Miwa and Sasaki (2014) use the same dataset but model the tasks as a table filling problem (see Section 4.2). Their model uses both a local and a global scoring function. Recently, Gupta et al. (201"
D17-1181,N16-1065,1,0.838351,"Missing"
D17-1181,D10-1099,0,0.18648,"volutional neural networks for a sentence classification task. In particular, we present an architecture which allows us to model joint entity and relation classification with a single neural network and classify entities and relations at the same time, normalizing their scores globally. Our experiments confirm that a CNN with a CRF output layer outperforms a CNN with locally normalized softmax layers. Our source code is available at http: //cistern.cis.lmu.de. 2 Related Work Some work on joint entity and relation classification uses distant supervision for building their own datasets, e.g., (Yao et al., 2010; Yaghoobzadeh et al., 2016). Other studies, which are described in more detail in the following, use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004, 2007) as we do in this paper. Roth and Yih (2004) develop constraints and use linear programming to globally normalize entity types and relations. Giuliano et al. (2007) use entity type information for relation extraction but do not train both tasks jointly. Kate and Mooney (2010) train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks. Miwa and Sasaki (201"
D18-1343,Q17-1010,0,0.095737,"Missing"
D18-1343,D17-1284,0,0.024071,"relation extraction. Crosslingual entity linking is an important related task, where the task is to link mentions of entities in multilingual text to a knowledge base (Tsai and Roth, 2016). Many entities are not sufficiently annotated in Wikipedia, and therefore crosslingual entity linking is necessary to learn informative context representations from multiple languages. Multi-representation of entities. Aggregating information from multiple sources to learn entity representations has been explored for entity typing (Yaghoobzadeh and Sch¨utze, 2017; Yaghoobzadeh et al., 2018), entity linking (Gupta et al., 2017) and relation extraction (Wang and Li, 2016). Here, we add language as a new “dimension” to multi-representations: each language contributes a different CTXT, NAME and DESC representation. Our multilingual and multi-representation models are examples of multiview learning. Xu et al. (2013) and Zhao et al. (2017) review the literature on multiview learning. Amini et al. (2009) cast multilingual text classification, a task related to entity typing, as multiview learning. Qu et al. (2017) address node classification and link prediction by attention-based multiview representations of graph nodes."
D18-1343,I17-1102,0,0.0310039,".1 Table 1: Micro F1 for entity typing. NAME4/NAME10 = name embeddings from 4 or 10 languages. results demonstrate that training one model with common parameters over all inputs is helping the classification for non high-resource views. Multiview learning exploits the complementarity of views: if an entity’s type cannot be inferred from one view, then other views may have the required information. Table 2 shows that using multiple views has a second beneficial effect: even if applied to a single view, a model trained on multiple views performs better. Kan et al. (2016)’s image recognition and Pappas and Popescu-Belis (2017)’s document classification findings are similar. Thus, not only does the increased amount of available information boost performance in the multiview setup, but also we can enable crossview transfer and learn a model that makes better predictions even if information is only available from a single view. 3063 EN DE ES FA CTXT NAME DESC SINGLE CROSS SINGLE CROSS SINGLE CROSS 71.7 71.6 73.4 72.8 77.6 82.0 31.7 32.3 34.1 34.7 28.9 35.7 20.3 21.2 29.8 30.7 23.1 28.6 9.3 9.5 17.2 18.3 12.2 15.3 Table 2: Micro F1 (all entities) for twelve singleview models (SINGLE) and one crossview model (CROSS) 4 R"
D18-1343,P17-2052,0,0.0313962,"OSS 71.7 71.6 73.4 72.8 77.6 82.0 31.7 32.3 34.1 34.7 28.9 35.7 20.3 21.2 29.8 30.7 23.1 28.6 9.3 9.5 17.2 18.3 12.2 15.3 Table 2: Micro F1 (all entities) for twelve singleview models (SINGLE) and one crossview model (CROSS) 4 Related Work Entity and mention typing. In this work, we assume that a predefined set of fine-grained types is given. Entity typing, i.e., predicting types of a knowledge base entity (Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015), is the focus of this paper. Mention typing, i.e., predicting types of a mention in a particular context (Ling and Weld, 2012; Rabinovich and Klein, 2017; Shimaoka et al., 2017; Murty et al., 2018), is a related task. Mention typing models can be evaluated for entity typing when aggregating their predictions (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017, 2018). Therefore, our public and large entity typing dataset, MVET, can be used as an alternative to the small manually annotated mention typing datasets like the commonly used FIGER (Ling and Weld, 2012). We leave this to the future work. Multilingual entity typing. We build multilingual dataset and models for entity typing. Most work on entity typing has been monolingual; e.g.,"
D18-1343,E17-1119,0,0.0297641,"82.0 31.7 32.3 34.1 34.7 28.9 35.7 20.3 21.2 29.8 30.7 23.1 28.6 9.3 9.5 17.2 18.3 12.2 15.3 Table 2: Micro F1 (all entities) for twelve singleview models (SINGLE) and one crossview model (CROSS) 4 Related Work Entity and mention typing. In this work, we assume that a predefined set of fine-grained types is given. Entity typing, i.e., predicting types of a knowledge base entity (Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015), is the focus of this paper. Mention typing, i.e., predicting types of a mention in a particular context (Ling and Weld, 2012; Rabinovich and Klein, 2017; Shimaoka et al., 2017; Murty et al., 2018), is a related task. Mention typing models can be evaluated for entity typing when aggregating their predictions (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017, 2018). Therefore, our public and large entity typing dataset, MVET, can be used as an alternative to the small manually annotated mention typing datasets like the commonly used FIGER (Ling and Weld, 2012). We leave this to the future work. Multilingual entity typing. We build multilingual dataset and models for entity typing. Most work on entity typing has been monolingual; e.g., Yaghoobzadeh and Sch¨u"
D18-1343,P17-1004,0,0.0147581,"ir predictions (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017, 2018). Therefore, our public and large entity typing dataset, MVET, can be used as an alternative to the small manually annotated mention typing datasets like the commonly used FIGER (Ling and Weld, 2012). We leave this to the future work. Multilingual entity typing. We build multilingual dataset and models for entity typing. Most work on entity typing has been monolingual; e.g., Yaghoobzadeh and Sch¨utze (2015) (English); and Suzuki et al. (2016) (Japanese). There is work on mention typing (van Erp and Vossen, 2017). Lin et al. (2017) that uses mono- and crosslingual attention for relation extraction. Crosslingual entity linking is an important related task, where the task is to link mentions of entities in multilingual text to a knowledge base (Tsai and Roth, 2016). Many entities are not sufficiently annotated in Wikipedia, and therefore crosslingual entity linking is necessary to learn informative context representations from multiple languages. Multi-representation of entities. Aggregating information from multiple sources to learn entity representations has been explored for entity typing (Yaghoobzadeh and Sch¨utze, 20"
D18-1343,N16-1072,0,0.0245469,"e the commonly used FIGER (Ling and Weld, 2012). We leave this to the future work. Multilingual entity typing. We build multilingual dataset and models for entity typing. Most work on entity typing has been monolingual; e.g., Yaghoobzadeh and Sch¨utze (2015) (English); and Suzuki et al. (2016) (Japanese). There is work on mention typing (van Erp and Vossen, 2017). Lin et al. (2017) that uses mono- and crosslingual attention for relation extraction. Crosslingual entity linking is an important related task, where the task is to link mentions of entities in multilingual text to a knowledge base (Tsai and Roth, 2016). Many entities are not sufficiently annotated in Wikipedia, and therefore crosslingual entity linking is necessary to learn informative context representations from multiple languages. Multi-representation of entities. Aggregating information from multiple sources to learn entity representations has been explored for entity typing (Yaghoobzadeh and Sch¨utze, 2017; Yaghoobzadeh et al., 2018), entity linking (Gupta et al., 2017) and relation extraction (Wang and Li, 2016). Here, we add language as a new “dimension” to multi-representations: each language contributes a different CTXT, NAME and D"
D18-1343,N15-1142,0,0.0344552,"Missing"
D18-1343,P18-1010,0,0.014254,".7 28.9 35.7 20.3 21.2 29.8 30.7 23.1 28.6 9.3 9.5 17.2 18.3 12.2 15.3 Table 2: Micro F1 (all entities) for twelve singleview models (SINGLE) and one crossview model (CROSS) 4 Related Work Entity and mention typing. In this work, we assume that a predefined set of fine-grained types is given. Entity typing, i.e., predicting types of a knowledge base entity (Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015), is the focus of this paper. Mention typing, i.e., predicting types of a mention in a particular context (Ling and Weld, 2012; Rabinovich and Klein, 2017; Shimaoka et al., 2017; Murty et al., 2018), is a related task. Mention typing models can be evaluated for entity typing when aggregating their predictions (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017, 2018). Therefore, our public and large entity typing dataset, MVET, can be used as an alternative to the small manually annotated mention typing datasets like the commonly used FIGER (Ling and Weld, 2012). We leave this to the future work. Multilingual entity typing. We build multilingual dataset and models for entity typing. Most work on entity typing has been monolingual; e.g., Yaghoobzadeh and Sch¨utze (2015) (English);"
D18-1343,N15-1054,0,0.0274686,"ssview transfer and learn a model that makes better predictions even if information is only available from a single view. 3063 EN DE ES FA CTXT NAME DESC SINGLE CROSS SINGLE CROSS SINGLE CROSS 71.7 71.6 73.4 72.8 77.6 82.0 31.7 32.3 34.1 34.7 28.9 35.7 20.3 21.2 29.8 30.7 23.1 28.6 9.3 9.5 17.2 18.3 12.2 15.3 Table 2: Micro F1 (all entities) for twelve singleview models (SINGLE) and one crossview model (CROSS) 4 Related Work Entity and mention typing. In this work, we assume that a predefined set of fine-grained types is given. Entity typing, i.e., predicting types of a knowledge base entity (Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015), is the focus of this paper. Mention typing, i.e., predicting types of a mention in a particular context (Ling and Weld, 2012; Rabinovich and Klein, 2017; Shimaoka et al., 2017; Murty et al., 2018), is a related task. Mention typing models can be evaluated for entity typing when aggregating their predictions (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017, 2018). Therefore, our public and large entity typing dataset, MVET, can be used as an alternative to the small manually annotated mention typing datasets like the commonly used FIGER (Ling and W"
D18-1343,E17-1111,1,0.901707,"Missing"
D18-1343,D15-1083,1,0.857189,"Missing"
D18-1343,E17-1055,1,0.833813,"Missing"
D18-1363,W16-2007,0,0.0158399,"easonably accurate if no better alternative is available; consider the following SBJV;PST;2;PL ! IND;PST;1;SG;PFV generations: “parlassiez” 7! “parlai”, “finissiez” 7! “finis”, “missiez” 7! “mis”, “prissiez” 7! “pris”. We thus conclude that SHIP indeed learns to select appropriate source forms. 6 Related Work Morphological generation. In the last two years, most work on paradigm completion has been done in the context of the SIGMORPHON 2016 and the CoNLL–SIGMORPHON 2017 shared tasks (Cotterell et al., 2016, 2017a). Due to the success of neural seq2seq models in 2016 (Kann and Sch¨utze, 2016b; Aharoni et al., 2016), systems developed for the 2017 edition were mostly neural (Makarov et al., 2017; Bergmanis et al., 2017; Zhou and Neubig, 2017). Besides the shared task systems, Kann and Sch¨utze (2017) presented a paradigm completion model for a multi-source setting that made use of an attention mechanism to decide which input form to attend to at each time step. They used randomly chosen, independent pairs of source and target forms for training. This differs crucially from the setting we consider in that no complete paradigms were available in their training sets. Only Cotterell et al. (2017b) addressed"
D18-1363,E14-1060,0,0.135897,"ed, e.g., semi-supervised training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is sim"
D18-1363,E17-1049,1,0.875012,"Missing"
D18-1363,K17-2002,1,0.915766,"Missing"
D18-1363,P17-1182,1,0.853174,"Missing"
D18-1363,P17-1031,0,0.0223035,"eural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine translation (Luong and Manning, 2015). One difference is that training set and test set can hardly be called different domains in paradigm completion. Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion. 7 Conclusion We presented two new methods for minimalresource paradigm completion: paradigm transduction and SHIP. Paradigm transduction learns general inflection ru"
D18-1363,W14-4012,0,0.15631,"Missing"
D18-1363,P16-2090,1,0.87624,"Missing"
D18-1363,chrupala-etal-2008-learning,0,0.065546,"Missing"
D18-1363,K17-2001,0,0.678061,",st 1 ,ct ) (5) t=1 st denotes the state of the decoder at step t, and ct is the sum of the hidden representations of the encoder, weighted by an attention mechanism. Additional background on the general model architecture is given in Bahdanau et al. (2015); details on MED can be found in Kann and Sch¨utze (2016b). 3.2 where A is a set of autoencoding examples, e✓ is the encoder, and D is a labeled training set of tuples of source s, morphological source tag tS , morphological target tag tT , and target w. Semi-supervised MED In order to make use of unlabeled data with MED, Kann and Sch¨utze (2017) defined an auxiliary autoencoding task and proposed a multi-task learning approach. For this extension, an additional symbol is added to the input vocabulary. Each input is then of the form (A |M + ) ⌃+ , with A being a novel tag for autoencoding, ⌃ being the alphabet of the language, and M being the set of morphological subtags of the source and the target. As for the basic MED model, all parts of the input are represented by embeddings. The training objective is to maximize the joint likelihood for the tasks of paradigm completion and autoencoding: P S T L(✓) = (s,tS ,tT ,w)2D logp✓ (w |e✓"
D18-1363,E17-2120,0,0.489715,"the lemma’s paradigm. In this work, we address paradigm completion (PC), the morphological task of, given a partial paradigm of a lemma, generating all of its missing forms. For the partial paradigm represented by the input subset {(“Schneemannes”, GEN;SG), (“Schneem¨annern”, DAT;PL)} of the German noun “Schneemann” shown in Figure 1, the goal of PC is to generate the output subset consisting of the six remaining forms. Neural seq2seq models define the state of the art for morphological generation if training sets are large; however, they have been less successful in the low-resource setting (Cotterell et al., 2017a). In this paper, we address an even more extreme minimalresource setting: for some of our experiments, our training sets only contain k ⇡ 10 paradigms. Each paradigm has multiple cells, so the number of forms (as opposed to the number of paradigms) is not necessarily minimal. However, we will see that generalizing from paradigm to paradigm is a key challenge, making the number of paradigms a good measure of the effective training set size. We propose two PC methods for the minimalresource setting: paradigm transduction and source selection with high precision (SHIP). We define a learning alg"
D18-1363,D08-1113,0,0.023465,"cal generation with limited data have been proposed, e.g., semi-supervised training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Ni"
D18-1363,N13-1138,0,0.0363807,"mited data have been proposed, e.g., semi-supervised training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transdu"
D18-1363,N16-1077,0,0.0331872,"Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine tr"
D18-1363,W17-4111,1,0.868442,"Missing"
D18-1363,L18-1293,0,0.0600656,"jections (cf. Figure 4). We then interpret the weight of an edge as a measure of the (un)reliability of the corresponding two source-target relationships. Our intuition is that the fewer different edit trees relate source and target, the more reliable the source is for generating the target. At test time, we find for each target tj a source tk such that nkj  nij 8i 2 J(w). We then use fk [w] to generate fj [w]. Again, Figure 4 shows examples. 4 4.1 Experiments Data We run experiments on the datasets from task 2 of the CoNLL–SIGMORPHON 2017 shared task, which have been created using UniMorph (Kirov et al., 2018). We give a short overview here; see (Cotterell et al., 2017a) for details. The dataset contains, for each of 52 languages, a development set of 50 partial paradigms, a test set of 50 partial paradigms, and three training sets of complete paradigms. Training set sizes are 10 (SET1), 50 (SET2), and 200 (SET3). Recall that we view the number of paradigms (not the number of forms) as the best measure of the amount of training data available. Even for SET3, there are only 200 lemmas per language in the training set, which are additionally distributed over multiple POS tags, compared to &gt;600 lemmas"
D18-1363,W16-2006,0,0.0169763,"y little 3258 BL: COPY BL: MED BL: PT BL: SIG17 SIG17+SHIP MED+PT MED+PT+SHIP SET1 .0810 .0004 .0833 .5012 .5971 .5808 .5793 SET2 .0810 .0432 .0833 .6576 .7355 .7486 .7547 SET3 .0810 .4211 .0775 .7707 .8008 .8454 .8483 Table 1: Accuracy on PC for SIG17+SHIP (the shared task baseline SIG17 with SHIP), MED+PT (MED with paradigm transduction), MED+PT+SHIP (MED with paradigm transduction and SHIP), as well as all baselines (BL). Results are averaged over all languages, and best results are in bold; detailed accuracies for all languages can be found in Appendix A. training data. Its design follows Liu and Mao (2016): SIG17 first aligns each input lemma and output inflected form. Afterwards, it assumes that each aligned pair can be split into a prefix, a stem, and a suffix. Based on this alignment, the system extracts prefix (resp. suffix) rules from the prefix (resp. suffix) pairings. At test time, suitable rules are applied to the input string to generate the target; more details can be found in Cotterell et al. (2017a). 4.4 Results Our results are shown in Table 1. For SET1, SIG17+SHIP obtains the highest accuracy, while, for SET2 and SET3, MED+PT+SHIP performs best. This difference can be easily expla"
D18-1363,2015.iwslt-evaluation.11,0,0.0408532,". (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine translation (Luong and Manning, 2015). One difference is that training set and test set can hardly be called different domains in paradigm completion. Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion. 7 Conclusion We presented two new methods for minimalresource paradigm completion: paradigm transduction and SHIP. Paradigm transduction learns general inflection rules through standard inductive training and idiosyncracies of a test paradigm through transduction. We showed that paradigm transduction effectively mitigates"
D18-1363,N15-1093,0,0.0417257,"sed training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptati"
D18-1363,P17-2014,0,0.0293957,"8); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine translation (Luong and Manning, 2015). One difference is that training set and test set can hardly be called different domains in paradigm completion. Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion. 7 Conclusion We presented two new methods for minimalresource paradigm completion: paradigm transduction and SHIP. Paradigm transduction learns general inflection rules through standard inductive training and id"
D18-1363,L16-1497,0,0.110751,"Completion In this section, we formally define our task, developing the notation for the rest of the paper. Given the set of morphological tags T (w) of a lemma w, we define the paradigm of w as the set of tuples of inflected form fk and tag tk : ⇡(w) = fk [w],tk tk 2T (w) (1) The example in Figure 1 thus corresponds to: ⇡(Schneemann) = “Schneemann”, NOM;SG . . . “Schneem¨anner”, ACC;PL . A training set in our setup consists of complete paradigms, i.e., all inflected forms of each lemma are available. This simulates a setting in which a linguist annotates complete paradigms, as done, e.g., in Sylak-Glassman et al. (2016). In contrast, each element of the test set is a partial paradigm, which we refer to as the input subset. This simulates a setting in which we collect all forms of a lemma occurring in a (manually or automatically) annotated input corpus; this set will generally not be complete. The PC task consists of generating the output subset of the paradigm, i.e., the forms belonging to form-tag pairs which are missing from the collected subset. 3 Method Our approach for PC is based on MED (Morphological Encoder-Decoder), a state-of-the-art model for morphological generation in the high-resource case, wh"
D18-1363,W17-4704,0,0.0338536,"Missing"
D18-1363,Q16-1019,1,0.873013,"Missing"
D18-1363,P17-1029,0,0.0408664,"parlassiez” 7! “parlai”, “finissiez” 7! “finis”, “missiez” 7! “mis”, “prissiez” 7! “pris”. We thus conclude that SHIP indeed learns to select appropriate source forms. 6 Related Work Morphological generation. In the last two years, most work on paradigm completion has been done in the context of the SIGMORPHON 2016 and the CoNLL–SIGMORPHON 2017 shared tasks (Cotterell et al., 2016, 2017a). Due to the success of neural seq2seq models in 2016 (Kann and Sch¨utze, 2016b; Aharoni et al., 2016), systems developed for the 2017 edition were mostly neural (Makarov et al., 2017; Bergmanis et al., 2017; Zhou and Neubig, 2017). Besides the shared task systems, Kann and Sch¨utze (2017) presented a paradigm completion model for a multi-source setting that made use of an attention mechanism to decide which input form to attend to at each time step. They used randomly chosen, independent pairs of source and target forms for training. This differs crucially from the setting we consider in that no complete paradigms were available in their training sets. Only Cotterell et al. (2017b) addressed essentially the same task we do, but they only considered the high-resource setting: their models were trained on hundreds of com"
D19-1111,S15-2102,0,0.153466,"fier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in closed form, is hyperparameterfree and thus more robust than Densifier. We evaluate the three methods on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing gender bias from embeddings. 1 In this work we modify the objective function of Densifier (Rothe et al., 2016) such that a closed form solution becomes available. We call this method DensRay. Following Amir et al. (2015) we compute simple linear SVMs, which we find to perform surprisingly well. We compare these methods on the task of lexicon induction. Introduction Distributed representations for words have been of interest in natural language processing for many years. Word embeddings have been particularly effective and successful. On the downside, embeddings are generally not interpretable. But interpretability is desirable for several reasons. i) Semantically or syntactically similar words can be extracted: e.g., for lexicon induction. ii) Interpretable dimensions can be used to evaluate word spaces by ex"
D19-1111,Q17-1010,0,0.061976,"computes the cosine similarity. Given the result from §4 we extend the above method by computing the cosine similarity in the orthogonal complement, i.e., sim(Ea,−1 , Ev,−1 ). We call this method IntCos (INTerpretable, COSine). Depending on the space used for computing the cosine similarity add the word “Original” or “Complement”. We evaluate this method across two analogy datasets. These are the Google Analogy Dataset (GA) (Mikolov et al., 2013) and BATS (Drozd et al., 2016). As embeddings spaces we use Google News Embeddings (GN) (Mikolov et al., 2013) and FastText subword embeddings (FT) (Bojanowski et al., 2017). We consider the first 80k word embeddings from each space. Table 4 shows the results. The first observation is that there is no clear winner. IntCos Original performs comparably to LRCos with slight improvements for GN/BATS: here the classes are widespread and exhibit low cosine similarity (IntraR and IntraL), which makes them harder to solve. IntCos Complement maintains performance for GN/BATS and is beneficial for Derivational analogies on GN. For most other analogies it harms performance. Within IntCos Original it is favorable to use DensRay as it gives slight performance improvements. Es"
D19-1111,C16-1332,0,0.258662,"rmation (e.g., gender bias). v) Most importantly, interpretable embeddings support the goal of interpretable deep learning models. Orthogonal transformations have been of particular interest in the literature. The reason is twofold: under the assumption that existing word Further, we show how interpretable word spaces can be applied to other tasks: first we use interpretable word spaces for debiasing embeddings. Second we show how they can be used for solving the set-based word analogy task. To this end, we introduce the set-based method IntCos, which is closely related to LRCos introduced by Drozd et al. (2016). We find IntCos to perform comparable to LRCos, but to be preferable for analogies which are hard to solve. Our contributions are: i) We modify Densifier’s objective function and derive an analytical solution for computing interpretable embeddings. ii) We show that the analytical solution performs as well as Densifier but is more robust. iii) We provide evidence that simple linear SVMs are best suited for the task of lexicon induction. iv) We demonstrate how interpretable embedding spaces can be used for debiasing embeddings and solving the set-based word analogy task. The source code of our"
D19-1111,N15-1184,0,0.0328663,"ilarity. Inter: mean cosine similarity between pairs. IntraL/R: mean cosine similarity within the left/right class. Right part shows precision for word analogy task. to obtain interpretable dimensions in an unsupervised manner. Their approach relies on solving complex optimization problems, while we focus on closed form solutions. Senel et al. (2018) use SEMCAT categories in combination with the Bhattacharya distance to identify interpretable directions. Also, oriented PCA (Diamantaras and Kung, 1996) is closely related to our method. However, both methods yield non-orthogonal transformation. Faruqui et al. (2015a) use semantic lexicons to retrofit embedding spaces. Thus they do not fully maintain the structure of the word space, which is in contrast to this work. Interpretable Embedding Algorithms. Another line of work modifies embedding algorithms to yield interpretable dimensions (Koc¸ et al., 2018; Luo et al., 2015; Shin et al., 2018; Zhao et al., 2018). There is also much work that generates sparse embeddings that are claimed to be more interpretable (Murphy et al., 2012; Faruqui et al., 2015b; Fyshe et al., 2015; Subramanian et al., 2018). Instead of learning new embeddings, we aim at making den"
D19-1111,D15-1196,0,0.014955,"form solutions. Senel et al. (2018) use SEMCAT categories in combination with the Bhattacharya distance to identify interpretable directions. Also, oriented PCA (Diamantaras and Kung, 1996) is closely related to our method. However, both methods yield non-orthogonal transformation. Faruqui et al. (2015a) use semantic lexicons to retrofit embedding spaces. Thus they do not fully maintain the structure of the word space, which is in contrast to this work. Interpretable Embedding Algorithms. Another line of work modifies embedding algorithms to yield interpretable dimensions (Koc¸ et al., 2018; Luo et al., 2015; Shin et al., 2018; Zhao et al., 2018). There is also much work that generates sparse embeddings that are claimed to be more interpretable (Murphy et al., 2012; Faruqui et al., 2015b; Fyshe et al., 2015; Subramanian et al., 2018). Instead of learning new embeddings, we aim at making dense embeddings interpretable. 7 Conclusion We investigated analytical methods for obtaining interpretable word embedding spaces. Relevant methods were examined with the tasks of lexicon induction, word analogy and debiasing. We gratefully acknowledge funding through a Zentrum Digitalisierung.Bayern fellowship aw"
D19-1111,C12-1118,0,0.318994,"Missing"
D19-1111,D17-1041,0,0.0937314,"Missing"
D19-1111,perez-rosas-etal-2012-learning,0,0.0604826,"Missing"
D19-1111,S15-2078,0,0.0272707,"Missing"
D19-1111,P15-1144,0,0.0916782,"ilarity. Inter: mean cosine similarity between pairs. IntraL/R: mean cosine similarity within the left/right class. Right part shows precision for word analogy task. to obtain interpretable dimensions in an unsupervised manner. Their approach relies on solving complex optimization problems, while we focus on closed form solutions. Senel et al. (2018) use SEMCAT categories in combination with the Bhattacharya distance to identify interpretable directions. Also, oriented PCA (Diamantaras and Kung, 1996) is closely related to our method. However, both methods yield non-orthogonal transformation. Faruqui et al. (2015a) use semantic lexicons to retrofit embedding spaces. Thus they do not fully maintain the structure of the word space, which is in contrast to this work. Interpretable Embedding Algorithms. Another line of work modifies embedding algorithms to yield interpretable dimensions (Koc¸ et al., 2018; Luo et al., 2015; Shin et al., 2018; Zhao et al., 2018). There is also much work that generates sparse embeddings that are claimed to be more interpretable (Murphy et al., 2012; Faruqui et al., 2015b; Fyshe et al., 2015; Subramanian et al., 2018). Instead of learning new embeddings, we aim at making den"
D19-1111,N16-1091,1,0.883687,"Missing"
D19-1111,N15-1004,0,0.0588184,"Missing"
D19-1111,waltinger-2010-germanpolarityclues,0,0.0332466,"Missing"
D19-1111,H05-1044,0,0.0944463,"Missing"
D19-1111,D18-1521,0,0.0420138,"use SEMCAT categories in combination with the Bhattacharya distance to identify interpretable directions. Also, oriented PCA (Diamantaras and Kung, 1996) is closely related to our method. However, both methods yield non-orthogonal transformation. Faruqui et al. (2015a) use semantic lexicons to retrofit embedding spaces. Thus they do not fully maintain the structure of the word space, which is in contrast to this work. Interpretable Embedding Algorithms. Another line of work modifies embedding algorithms to yield interpretable dimensions (Koc¸ et al., 2018; Luo et al., 2015; Shin et al., 2018; Zhao et al., 2018). There is also much work that generates sparse embeddings that are claimed to be more interpretable (Murphy et al., 2012; Faruqui et al., 2015b; Fyshe et al., 2015; Subramanian et al., 2018). Instead of learning new embeddings, we aim at making dense embeddings interpretable. 7 Conclusion We investigated analytical methods for obtaining interpretable word embedding spaces. Relevant methods were examined with the tasks of lexicon induction, word analogy and debiasing. We gratefully acknowledge funding through a Zentrum Digitalisierung.Bayern fellowship awarded to the first author. This work wa"
D19-1111,P19-1341,1,0.86318,"Missing"
D19-1173,P18-1073,0,0.0274352,"ng set is very small (64K tokens); hence, we do not include fD in the ensemble, and we finetune the BERT language model for 10K rather than 100K steps to avoid overfitting. Like on the DQD tasks, MV-DASE beats its individual views as well as naive view concatenation and averaging (see Table 8). After adding ParaNMT to the ensemble, we achieve competitive results. Future Work 1 2 3 fG fU (USE) fB (BERT) .731 / .647 .793 / .762 .779 / .718 .817 / .799 .793 / .762 .779 / .718 4 5 6 MV-DASE concat avg .825 / .771 .791 / .730 .790 / .729 .842 / .804 .826 / .772 .823 / .771 linear projections, e.g. Artetxe et al. (2018)), but it is unclear whether it holds for sentence embeddings. Potential avenues for non-linear GCCA include Kernel GCCA (Tenenhaus et al., 2015) and Deep GCCA (Benton et al., 2017). More views. A major advantage of MV-DASE is that it is agnostic to the number and specifics of its views. We plan to investigate whether additional or different views (e.g., encoders learned on related domains) can increase performance. 9 Conclusion We have presented a multi-view approach to unsupervised Duplicate Question Detection in lowresource, domain-specific Community Question Answering forums. MV-DASE is a"
D19-1173,S16-1131,0,0.0666841,"Missing"
D19-1173,K15-1013,0,0.0744369,"Missing"
D19-1173,Q17-1010,0,0.373996,"[data dump: 2018-12-20] source and target domains are related. The second is unsupervised DQD via representation learning (Charlet and Damnati, 2017; Lau and Baldwin, 2016), which requires only unlabeled questions. In this paper, we take the unsupervised avenue. A major challenge in the context of domainspecific CQA forums is that language usage may differ from the “generic” domains of existing representations. To illustrate this point, compare the following Nearest Neighbor lists of the word “tree”, based either on generic GloVe embeddings (Pennington et al., 2014) or on FastText embeddings (Bojanowski et al., 2017) that were trained on specific CQA forums: generic (GloVe): trees, branches, leaf chess: searches, prune, modify outdoors: trees, trunk, trunks gis: strtree, rtree, btree wordpress: trees, hierachy, hierarchial gaming: trees, treehouse, skills Charlet and Damnati (2017) and Lau and Baldwin (2016) report that representations trained on in-domain data perform better on unsupervised DQD than generic representations. But in a low-resource setting, the amount of unlabeled indomain data is limited. This can result in low coverage or quality, as illustrated by the in-domain embedding neighbors of “tr"
D19-1173,C18-1140,0,0.0242312,"ew representation learning is an umbrella term for methods that transform different representations of the same entities into a common space. In NLP, it has typically been applied to word embeddings. A famous example is the cross-lingual projection of word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Monolingually, Rastogi et al. (2015) use Generalized Canonical Correlation Analysis (GCCA) to project different word representations into a common space. Yin and Sch¨utze (2016) combine word embeddings by concatenation, truncated Singular Value Decomposition and linear projections; Bollegala and Bao (2018) use autoencoders. Sarma et al. (2018) correlate generic and domain-specific word embeddings by Canonical Correlation Analysis (CCA). All of these methods are post-training, i.e., they are applied to fully trained word embeddings. MV-DASE falls into the same category, albeit at the sentence level (see Section 3.1). Other methods, which we will call in-training, encourage the alignment of embeddings during training (e.g., Bollegala et al. (2015); Yang et al. (2017)). 2.4 Multi-view sentence embeddings Multi-view sentence embeddings are less frequently explored than multi-view word embeddings. O"
D19-1173,P15-1071,0,0.0235857,"into a common space. Yin and Sch¨utze (2016) combine word embeddings by concatenation, truncated Singular Value Decomposition and linear projections; Bollegala and Bao (2018) use autoencoders. Sarma et al. (2018) correlate generic and domain-specific word embeddings by Canonical Correlation Analysis (CCA). All of these methods are post-training, i.e., they are applied to fully trained word embeddings. MV-DASE falls into the same category, albeit at the sentence level (see Section 3.1). Other methods, which we will call in-training, encourage the alignment of embeddings during training (e.g., Bollegala et al. (2015); Yang et al. (2017)). 2.4 Multi-view sentence embeddings Multi-view sentence embeddings are less frequently explored than multi-view word embeddings. One exception is Tang and de Sa (2019), who train a recurrent neural network and an average word embedding encoder jointly on an unlabeled corpus. This method is in-training, i.e., it cannot be used to combine pre-existing encoders. Kiela et al. (2018) dynamically integrate an ensemble of word embeddings into a task-specific LSTM. They require labeled data and the resulting embeddings are task-specific. Sarma et al. (2018) marry domain-adapted w"
D19-1173,D15-1075,0,0.0536863,"etwork and an average word embedding encoder jointly on an unlabeled corpus. This method is in-training, i.e., it cannot be used to combine pre-existing encoders. Kiela et al. (2018) dynamically integrate an ensemble of word embeddings into a task-specific LSTM. They require labeled data and the resulting embeddings are task-specific. Sarma et al. (2018) marry domain-adapted word embeddings (see Section 2.3) with InferSent (Conneau et al., 2017), a bidirectional LSTM sentence 2 http://ixa2.si.ehu.es/stswiki/index. php/STSbenchmark encoder trained on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). They initialize InferSent with the adapted embeddings and then retrain it on SNLI. Note that this approach is not feasible when the training regime of an encoder cannot be reproduced, e.g., when the original training data is not publicly available. 3 Method We now describe MV-DASE as a general framework. For details on the ensemble used in this paper, see Section 3.2. 3.1 Framework GCCA basics. Given zero-mean random vectors x1 ∈ Rd1 , x2 ∈ Rd2 , Canonical Correlation Analysis (CCA) finds linear transformations θ 1 ∈ Rd1 , θ 2 ∈ Rd2 such that θ T1 x1 and θ T2 x2 are maximally correlated. Bac"
D19-1173,S17-2001,0,0.188073,"n-specific and generic, contextualized and noncontextualized (see Table 1). In Sections 4 and 5, we demonstrate that MVDASE is effective at duplicate retrieval on the CQADupStack corpus (Hoogeveen et al., 2015) and on additional low-resource Stack Exchange forums. Significance tests show significant gains over BM25, all single-view systems and domainadversarial supervised training as proposed by Shah et al. (2018). In Sections 6 and 7, we successfully evaluate MV-DASE on two additional benchmarks: the SemEval-2017 DQD shared task (Nakov et al., 2017) as well as the unsupervised STS Benchmark (Cer et al., 2017). 2 2.1 Related Work Duplicate Question Detection Most prior work on DQD (e.g., Bogdanova et al. (2015); Dos Santos et al. (2015); Baldwin et al. (2016); Zhang et al. (2017); Rodrigues et al. (2017); Hoogeveen et al. (2018)) focuses on supervised architectures. As discussed, these approaches are not applicable to forums with few or no labeled duplicates. Shah et al. (2018) tackle label sparsity by domain-adversarial training (ADA). More specifically, they train a bidirectional Long-Short Term Memory Network (LSTM) (Hochreiter and Schmidhuber, 1997) on a label-rich source forum, while minimizin"
D19-1173,N18-1111,0,0.0307075,"ed t-tests, using the 20 test set forums as data points.15 We then find groups of equivalent methods by transitive closure of a ∼ b ≡ p ≥ .05. Group A being ranked higher than group B means that every method in A performs significantly better than every method in B. Two methods in the same group may differ significantly, but there exists a chain between them of methods with insignificant differences. Discussion Comparison with baselines BM25. BM25 is a tough baseline for DQD: In terms of MAP, it is better than or comparable to 14 We also experiment with Multinomial Adversarial Networks (MAN) (Chen and Cardie, 2018), a multi-source multitarget framework that can be trained on all 24 forums jointly. Initial results were not competitive with ADA, so we do not include them here. See supplementary material for details. 15 Ten forums for t-tests involving ADA. low-resource Table 5: Group rankings by transitive closure of paired t-tests. ¬fj is MV-DASE without fj (see Table 6). No particular order inside groups. 5.1 NDCG ¬fB , ¬(fG , fD ) 5 AUC(.05) 2 4.5 MAP MV-DASE 4.4 1 2 3 4 5 6 AUC(.05) MV-DASE, ¬fG , ¬fD CQADupStack MAP 1 15 concat 16 avg -.005 -.001 -.050 -.006 -.069 -.005 -.010 .012 -.063 -.014 -.130 -"
D19-1173,N18-2031,0,0.0208092,"e the supervised domainadversarial method of Shah et al. (2018) (ADA) on the low-resource forums. Recall that ADA requires a related labeled source domain. To achieve this, we pair every low-resource forum (target) with the CQADupStack forum (source) with which it has the highest word trigram overlap. See supplementary material for more details and a table of all source-target mappings.14 Ablation studies We perform a set of experiments where we omit views from the ensemble. We also replace GCCA with naive view concatenation or view averaging. When averaging, we pad lower-dimensional vectors (Coates and Bollegala, 2018). Significance tests We perform paired t-tests, using the 20 test set forums as data points.15 We then find groups of equivalent methods by transitive closure of a ∼ b ≡ p ≥ .05. Group A being ranked higher than group B means that every method in A performs significantly better than every method in B. Two methods in the same group may differ significantly, but there exists a chain between them of methods with insignificant differences. Discussion Comparison with baselines BM25. BM25 is a tough baseline for DQD: In terms of MAP, it is better than or comparable to 14 We also experiment with Mult"
D19-1173,D17-1070,0,0.369553,"ntence similarity scoring (Cer et al., 2017). Arora et al. (2017) show that a weighted average over pre-trained word embeddings, followed by principal component removal, is a strong baseline for STS. We use their weighting scheme, Smooth Inverse Frequency (SIF), in Section 3.2. Averaged word embeddings are insensitive to word order. This stands in contrast to contextualized encoders, such as LSTMs or Transform1631 ers (Vaswani et al., 2017). Contextualized encoders are typically trained as unsupervised language models (Peters et al., 2018; Devlin et al., 2019) or on supervised transfer tasks (Conneau et al., 2017; Cer et al., 2018). At the time of writing, weighted averaged word embeddings achieve better results than contextualized encoders on unsupervised STS.2 2.3 Multi-view word embeddings Multi-view representation learning is an umbrella term for methods that transform different representations of the same entities into a common space. In NLP, it has typically been applied to word embeddings. A famous example is the cross-lingual projection of word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Monolingually, Rastogi et al. (2015) use Generalized Canonical Correlation Analysis (GCCA)"
D19-1173,N19-1423,0,0.496899,"d fG : GloVe fD : FastText (in-domain) noncontextualized fU : USE fB : BERT (domain-finetuned) Table 1: Ensemble used in our experiments. It is therefore desirable to combine the overall quality and coverage of generic representations with the domain-specificity of in-domain representations via multi-view learning. There is a large body of work on multi-view word embeddings (see Section 2.3), including domain adapted word embeddings (Sarma et al., 2018). Recent representation learning techniques go beyond the word level and embed larger contexts (e.g., sentences) jointly (Peters et al., 2018; Devlin et al., 2019; Cer et al., 2018). To reflect this paradigm shift, we take multi-view representation learning from the word to the sentence level and propose MV-DASE (Multi-View Domain Adapted Sentence Embeddings), a framework that combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis (see Section 3.1). MV-DASE uses unlabeled in-domain data only, making it applicable to the problem of unsupervised DQD. As a framework, it is agnostic to the internal specifics of its ensemble. In Section 3.2, we describe an ensemble of different sentence encoders: domain-specific and generic"
D19-1173,P15-2114,0,0.0777251,"Missing"
D19-1173,W18-3012,0,0.0216204,"The resulting corpus is used for BERT domainfinetuning, SIF weight estimation and GCCA. At test time, we measure Pearson’s r between cos(f (s1 ), f (s2 )) and y, where f is an encoder (e.g., MV-DASE) and y is the ground truth similarity of test set pair (s1 , s2 ). In this experiment, the ensemble contains USE (fU ), domain-finetuned BERT (fB ) and fG . For fG , we either use SIF-weighted averaged GloVe vectors (Section 3.2), or unweighted averaged ParaNMT17 word and trigram vectors (Wieting and Gimpel, 2018), which are the current State of the Art on the unsupervised STS Benchmark test set (Ethayarajh, 2018). The unlabeled training set is very small (64K tokens); hence, we do not include fD in the ensemble, and we finetune the BERT language model for 10K rather than 100K steps to avoid overfitting. Like on the DQD tasks, MV-DASE beats its individual views as well as naive view concatenation and averaging (see Table 8). After adding ParaNMT to the ensemble, we achieve competitive results. Future Work 1 2 3 fG fU (USE) fB (BERT) .731 / .647 .793 / .762 .779 / .718 .817 / .799 .793 / .762 .779 / .718 4 5 6 MV-DASE concat avg .825 / .771 .791 / .730 .790 / .729 .842 / .804 .826 / .772 .823 / .771 lin"
D19-1173,E14-1049,0,0.043784,"sed language models (Peters et al., 2018; Devlin et al., 2019) or on supervised transfer tasks (Conneau et al., 2017; Cer et al., 2018). At the time of writing, weighted averaged word embeddings achieve better results than contextualized encoders on unsupervised STS.2 2.3 Multi-view word embeddings Multi-view representation learning is an umbrella term for methods that transform different representations of the same entities into a common space. In NLP, it has typically been applied to word embeddings. A famous example is the cross-lingual projection of word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Monolingually, Rastogi et al. (2015) use Generalized Canonical Correlation Analysis (GCCA) to project different word representations into a common space. Yin and Sch¨utze (2016) combine word embeddings by concatenation, truncated Singular Value Decomposition and linear projections; Bollegala and Bao (2018) use autoencoders. Sarma et al. (2018) correlate generic and domain-specific word embeddings by Canonical Correlation Analysis (CCA). All of these methods are post-training, i.e., they are applied to fully trained word embeddings. MV-DASE falls into the same category, albeit at the sentence"
D19-1173,S17-2053,0,0.053956,"Missing"
D19-1173,D18-2029,0,0.265205,"tText (in-domain) noncontextualized fU : USE fB : BERT (domain-finetuned) Table 1: Ensemble used in our experiments. It is therefore desirable to combine the overall quality and coverage of generic representations with the domain-specificity of in-domain representations via multi-view learning. There is a large body of work on multi-view word embeddings (see Section 2.3), including domain adapted word embeddings (Sarma et al., 2018). Recent representation learning techniques go beyond the word level and embed larger contexts (e.g., sentences) jointly (Peters et al., 2018; Devlin et al., 2019; Cer et al., 2018). To reflect this paradigm shift, we take multi-view representation learning from the word to the sentence level and propose MV-DASE (Multi-View Domain Adapted Sentence Embeddings), a framework that combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis (see Section 3.1). MV-DASE uses unlabeled in-domain data only, making it applicable to the problem of unsupervised DQD. As a framework, it is agnostic to the internal specifics of its ensemble. In Section 3.2, we describe an ensemble of different sentence encoders: domain-specific and generic, contextualized an"
D19-1173,D18-1515,0,0.05176,"Missing"
D19-1173,S17-2051,0,0.0315041,"e this paper focuses on Duplicate Question Detection, MV-DASE is also applicable to other unsupervised sentence-pair tasks. As proof of concept, we test it on the unsupervised STS Benchmark (Cer et al., 2017). Here, the task is to predict similarity scores y ∈ R for sentence pairs (s1 , s2 ). 16 alt.qcri.org/semeval2017/task3/data/ uploads/semeval2017_task3_submissions_ and_scores.zip 1637 MAP MRR 1 2 3 4 fG (GloVe) fD (FastText) fU (USE) fB (BERT) 43.13 43.38 48.22 43.51 47.39 47.67 52.73 48.52 5 6 7 MV-DASE concat avg 51.56 44.66 44.95 56.48 49.84 49.76 8 9 10 11 12 13 Filice et al. (2017)* Charlet and Damnati (2017)* Goyal (2017)* Zhang and Wu (2018) Yang et al. (2018) Gonzalez et al. (2018) 49.00 47.87 47.20 48.53 48.97 48.56 52.41 50.97 53.22 52.75 52.41 14 15 IR baseline* Random baseline* 41.85 29.81 46.42 33.02 We treat the benchmark training set as an unlabeled corpus, i.e., we discard all labels and destroy the original sentence pairings by shuffling. The resulting corpus is used for BERT domainfinetuning, SIF weight estimation and GCCA. At test time, we measure Pearson’s r between cos(f (s1 ), f (s2 )) and y, where f is an encoder (e.g., MV-DASE) and y is the ground truth similarity of test set pa"
D19-1173,S17-2050,0,0.06028,"Missing"
D19-1173,2021.ccl-1.108,0,0.125762,"Missing"
D19-1173,P18-1031,0,0.0239485,"physics programmers stats tex unix webmasters wordpress 23697 41791 46896 38522 17509 39355 33052 42921 71090 48454 17911 49146 1579 3506 2207 1099 1271 1769 1538 890 4939 1648 1143 719 2.4M 3.4M 4.0M 4.6M 2.6M 6.1M 5.6M 7.2M 7.4M 5.5M 2.0M 5.6M low-resource forums BERT is a Transformer that was pre-trained as a masked language model with next sentence prediction. We find that domain-finetuning BERT on S results in improvements over generic BERT (see Table 2, bottom). Note that domain-finetuning refers to unsupervised training as a masked language model, i.e., we only require unlabeled data (Howard and Ruder, 2018). We use default parameters7 except for a reduced batch size of 8. At test time, we take the following approach: BERT segments a token sequence s = (s1 , . . . , sT ) into a subword sequence s0 = ([CLS], s01 , . . . , s0T 0 , [SEP]), where [CLS] and [SEP] are special tokens that were used during pre-training, and T 0 ≥ T . BERT produces one 768-d vector vl,t per subword s0t and layer l ∈ [1, . . . , L], where L is the total number of layers (here: 12). We SIF-weight all vectors according to the probability of their subword (estimated on S) and average over layers and subwords, excluding the sp"
D19-1173,D18-1176,0,0.0216245,"V-DASE falls into the same category, albeit at the sentence level (see Section 3.1). Other methods, which we will call in-training, encourage the alignment of embeddings during training (e.g., Bollegala et al. (2015); Yang et al. (2017)). 2.4 Multi-view sentence embeddings Multi-view sentence embeddings are less frequently explored than multi-view word embeddings. One exception is Tang and de Sa (2019), who train a recurrent neural network and an average word embedding encoder jointly on an unlabeled corpus. This method is in-training, i.e., it cannot be used to combine pre-existing encoders. Kiela et al. (2018) dynamically integrate an ensemble of word embeddings into a task-specific LSTM. They require labeled data and the resulting embeddings are task-specific. Sarma et al. (2018) marry domain-adapted word embeddings (see Section 2.3) with InferSent (Conneau et al., 2017), a bidirectional LSTM sentence 2 http://ixa2.si.ehu.es/stswiki/index. php/STSbenchmark encoder trained on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). They initialize InferSent with the adapted embeddings and then retrain it on SNLI. Note that this approach is not feasible when the training regime of an encode"
D19-1173,W16-1609,0,0.0220805,"cs Given a test query q, we rank all candidates c 6= q from the same forum by cos(f (q), f (c)), where f is an encoder (e.g., MV-DASE). Our metrics are MAP, AUC(.05), Normalized Discounted Cumulative Gain (NDCG), Recall@3 (R@3) and Precision@3 (P@3). AUC(.05), the area under the ROC curve up to a false positive rate of .05, is used by Shah et al. (2018). Note that upper bounds on P@3 and R@3 are not 1, since most duplicates have only one original and a few have more than three. 4.3 • Doc2vec (Le and Mikolov, 2014) trained on the in-domain corpus, using the best DQD hyperparameters reported in Lau and Baldwin (2016). • InferSent V.1.13 (Conneau et al., 2017) • Our re-implementation of domain-adapted CCA word embeddings (Sarma et al. (2018), see Section 2.3). We use the same word embeddings, SIF weights and component removal described in Section 3.2. Denoted “word-level CCA” below. Baselines Unsupervised. Our IR baseline is BM25 (Robertson et al., 1995) as implemented in Elasticsearch 6.5.4 (Gormley and Tong, 2015) with default parameters. We test against all single-view encoders from our ensemble. The remaining unsupervised baselines are: 11 tfhub.dev/google/elmo/2 github.com/allenai/bilm-tf/blob/ master"
D19-1173,D14-1162,0,0.0918823,"works best when 1 archive.org/details/stackexchange [data dump: 2018-12-20] source and target domains are related. The second is unsupervised DQD via representation learning (Charlet and Damnati, 2017; Lau and Baldwin, 2016), which requires only unlabeled questions. In this paper, we take the unsupervised avenue. A major challenge in the context of domainspecific CQA forums is that language usage may differ from the “generic” domains of existing representations. To illustrate this point, compare the following Nearest Neighbor lists of the word “tree”, based either on generic GloVe embeddings (Pennington et al., 2014) or on FastText embeddings (Bojanowski et al., 2017) that were trained on specific CQA forums: generic (GloVe): trees, branches, leaf chess: searches, prune, modify outdoors: trees, trunk, trunks gis: strtree, rtree, btree wordpress: trees, hierachy, hierarchial gaming: trees, treehouse, skills Charlet and Damnati (2017) and Lau and Baldwin (2016) report that representations trained on in-domain data perform better on unsupervised DQD than generic representations. But in a low-resource setting, the amount of unlabeled indomain data is limited. This can result in low coverage or quality, as ill"
D19-1173,N18-1202,0,0.627257,"pecific contextualized fG : GloVe fD : FastText (in-domain) noncontextualized fU : USE fB : BERT (domain-finetuned) Table 1: Ensemble used in our experiments. It is therefore desirable to combine the overall quality and coverage of generic representations with the domain-specificity of in-domain representations via multi-view learning. There is a large body of work on multi-view word embeddings (see Section 2.3), including domain adapted word embeddings (Sarma et al., 2018). Recent representation learning techniques go beyond the word level and embed larger contexts (e.g., sentences) jointly (Peters et al., 2018; Devlin et al., 2019; Cer et al., 2018). To reflect this paradigm shift, we take multi-view representation learning from the word to the sentence level and propose MV-DASE (Multi-View Domain Adapted Sentence Embeddings), a framework that combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis (see Section 3.1). MV-DASE uses unlabeled in-domain data only, making it applicable to the problem of unsupervised DQD. As a framework, it is agnostic to the internal specifics of its ensemble. In Section 3.2, we describe an ensemble of different sentence encoders: domain"
D19-1173,N15-1058,0,0.0635141,"Missing"
D19-1173,S17-1030,0,0.0658184,"Missing"
D19-1173,N19-1112,0,0.0254143,"tors (Pennington et al., 2014) • weighted averaged domain-specific FastText vectors (Bojanowski et al., 2017) • Universal Sentence Encoder (USE) (Cer et al., 2018) • domain-finetuned BERT (Devlin et al., 2019) In this section, we describe the encoders in detail. Note that the choice of encoders is orthogonal to the framework and other resources could be used. Where possible, we base our selection on the literature: We choose USE over InferSent due to better performance on STS (Perone et al., 2018), and BERT over ELMo (Peters et al., 2018) due to better performance on linguistic probing tasks (Liu et al., 2019a). The choice of GloVe for generic word embeddings is based on Sarma et al. (2018). Weighted averaged word embeddings. We denote generic and domain-specific word embeddings of some word type i as wG,i ∈ RdG and no SIF wiki SIF in-domain SIF fG (GloVe) fD (FastText) fB (BERT) .089 .128 .147 .083 .100 .104 .134 .159 .176 generic domain-finetuned fB (BERT) ELMo .138 .176 .103 .155 Table 2: Mean Average Precision (MAP) averaged over heldout forums. Top: MAP as a function of whether and where SIF weights are estimated. Bottom: MAP of generic vs. domain-finetuned BERT and ELMo. Evaluation setup is"
D19-1173,W18-3407,0,0.349361,"Language Processing, pages 1630–1641, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics generic domain-specific contextualized fG : GloVe fD : FastText (in-domain) noncontextualized fU : USE fB : BERT (domain-finetuned) Table 1: Ensemble used in our experiments. It is therefore desirable to combine the overall quality and coverage of generic representations with the domain-specificity of in-domain representations via multi-view learning. There is a large body of work on multi-view word embeddings (see Section 2.3), including domain adapted word embeddings (Sarma et al., 2018). Recent representation learning techniques go beyond the word level and embed larger contexts (e.g., sentences) jointly (Peters et al., 2018; Devlin et al., 2019; Cer et al., 2018). To reflect this paradigm shift, we take multi-view representation learning from the word to the sentence level and propose MV-DASE (Multi-View Domain Adapted Sentence Embeddings), a framework that combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis (see Section 3.1). MV-DASE uses unlabeled in-domain data only, making it applicable to the problem of unsupervised DQD. As a framew"
D19-1173,D18-1131,0,0.356495,"it applicable to the problem of unsupervised DQD. As a framework, it is agnostic to the internal specifics of its ensemble. In Section 3.2, we describe an ensemble of different sentence encoders: domain-specific and generic, contextualized and noncontextualized (see Table 1). In Sections 4 and 5, we demonstrate that MVDASE is effective at duplicate retrieval on the CQADupStack corpus (Hoogeveen et al., 2015) and on additional low-resource Stack Exchange forums. Significance tests show significant gains over BM25, all single-view systems and domainadversarial supervised training as proposed by Shah et al. (2018). In Sections 6 and 7, we successfully evaluate MV-DASE on two additional benchmarks: the SemEval-2017 DQD shared task (Nakov et al., 2017) as well as the unsupervised STS Benchmark (Cer et al., 2017). 2 2.1 Related Work Duplicate Question Detection Most prior work on DQD (e.g., Bogdanova et al. (2015); Dos Santos et al. (2015); Baldwin et al. (2016); Zhang et al. (2017); Rodrigues et al. (2017); Hoogeveen et al. (2018)) focuses on supervised architectures. As discussed, these approaches are not applicable to forums with few or no labeled duplicates. Shah et al. (2018) tackle label sparsity by"
D19-1173,P18-1042,0,0.0472768,"t as an unlabeled corpus, i.e., we discard all labels and destroy the original sentence pairings by shuffling. The resulting corpus is used for BERT domainfinetuning, SIF weight estimation and GCCA. At test time, we measure Pearson’s r between cos(f (s1 ), f (s2 )) and y, where f is an encoder (e.g., MV-DASE) and y is the ground truth similarity of test set pair (s1 , s2 ). In this experiment, the ensemble contains USE (fU ), domain-finetuned BERT (fB ) and fG . For fG , we either use SIF-weighted averaged GloVe vectors (Section 3.2), or unweighted averaged ParaNMT17 word and trigram vectors (Wieting and Gimpel, 2018), which are the current State of the Art on the unsupervised STS Benchmark test set (Ethayarajh, 2018). The unlabeled training set is very small (64K tokens); hence, we do not include fD in the ensemble, and we finetune the BERT language model for 10K rather than 100K steps to avoid overfitting. Like on the DQD tasks, MV-DASE beats its individual views as well as naive view concatenation and averaging (see Table 8). After adding ParaNMT to the ensemble, we achieve competitive results. Future Work 1 2 3 fG fU (USE) fB (BERT) .731 / .647 .793 / .762 .779 / .718 .817 / .799 .793 / .762 .779 / .71"
D19-1173,D17-1312,0,0.0228979,"and Sch¨utze (2016) combine word embeddings by concatenation, truncated Singular Value Decomposition and linear projections; Bollegala and Bao (2018) use autoencoders. Sarma et al. (2018) correlate generic and domain-specific word embeddings by Canonical Correlation Analysis (CCA). All of these methods are post-training, i.e., they are applied to fully trained word embeddings. MV-DASE falls into the same category, albeit at the sentence level (see Section 3.1). Other methods, which we will call in-training, encourage the alignment of embeddings during training (e.g., Bollegala et al. (2015); Yang et al. (2017)). 2.4 Multi-view sentence embeddings Multi-view sentence embeddings are less frequently explored than multi-view word embeddings. One exception is Tang and de Sa (2019), who train a recurrent neural network and an average word embedding encoder jointly on an unlabeled corpus. This method is in-training, i.e., it cannot be used to combine pre-existing encoders. Kiela et al. (2018) dynamically integrate an ensemble of word embeddings into a task-specific LSTM. They require labeled data and the resulting embeddings are task-specific. Sarma et al. (2018) marry domain-adapted word embeddings (see"
D19-1173,D19-1059,0,0.0309907,"Missing"
D19-1173,P16-1128,1,0.905262,"Missing"
D19-5012,D19-1565,0,0.279383,"Missing"
D19-5012,N19-1423,0,0.0354656,"1: Features used in SLC and FLC tasks ing , stereotyping, etc. This inspires1 us in extracting different features (Table 1) including the complexity of text, sentiment, emotion, lexical (POS, NER, etc.), layout, etc. To further investigate, we use topical features (e.g., document-topic proportion) (Blei et al., 2003; Gupta et al., 2019a, 2018) at sentence and document levels in order to determine irrelevant themes, if introduced to the issue being discussed (e.g., Red Herring). For word and sentence representations, we use pre-trained vectors from FastText (Bojanowski et al., 2017) and BERT (Devlin et al., 2019). 2.2 tiple NLP benchmarks. Following Devlin et al. (2019), we fine-tune BERT for binary classification, initializing with a pre-trained model (i.e., BERT-base, Cased). Additionally, we apply a decision function (Table 1) such that a sentence is tagged as propaganda if prediction probability of the classifier is greater than a threshold (τ ). We relax the binary decision boundary to boost recall, similar to Gupta et al. (2019b). Ensemble of Logistic Regression, CNN and BERT: In the final component, we collect predictions (i.e., propaganda label) for each sentence from the three (M = 3) classif"
D19-5012,N18-1098,1,0.894648,"Missing"
D19-5012,W18-5418,1,0.862879,"Missing"
D19-5012,C16-1239,1,0.894378,"Missing"
D19-5012,D17-2002,0,0.044513,"ut of 25 participants) in FLC and SLC tasks, respectively. Introduction In the age of information dissemination without quality control, it has enabled malicious users to spread misinformation via social media and aim individual users with propaganda campaigns to achieve political and financial gains as well as advance a specific agenda. Often disinformation is complied in the two major forms: fake news and propaganda, where they differ in the sense that the propaganda is possibly built upon true information (e.g., biased, loaded language, repetition, etc.). Prior works (Rashkin et al., 2017; Habernal et al., 2017; Barr´on-Cede˜no et al., 2019) in detecting propaganda have focused primarily at document level, typically labeling all articles from a propagandistic news outlet as propaganda and thus, often non-propagandistic articles from the outlet are mislabeled. To this end, Da San Martino et al. (2019) focuses on analyzing the use of propaganda and detecting specific propagandistic 2 2.1 System Description Linguistic, Layout and Topical Features Some of the propaganda techniques (Da San Martino et al., 2019) involve word and phrases that express strong emotional implications, exaggeration, minimizatio"
D19-5012,D14-1181,0,0.00691984,"da label) for each sentence from the three (M = 3) classifiers and thus, obtain M number of predictions for each sentence. We explore two ensemble strategies (Table 1): majority-voting and relax-voting to boost precision and recall, respectively. Sentence-level Propaganda Detection Figure 1 (left) describes the three components of our system for SLC task: features, classifiers and ensemble. The arrows from features-to-classifier indicate that we investigate linguistic, layout and topical features in the two binary classifiers: LogisticRegression and CNN. For CNN, we follow the architecture of Kim (2014) for sentencelevel classification, initializing the word vectors by FastText or BERT. We concatenate features in the last hidden layer before classification. One of our strong classifiers includes BERT that has achieved state-of-the-art performance on mul2.3 Fragment-level Propaganda Detection Figure 1 (right) describes our system for FLC task, where we design sequence taggers (Vu et al., 2016; Gupta et al., 2016) in three modes: (1) LSTMCRF (Lample et al., 2016) with word embeddings (w e) and character embeddings c e, tokenlevel features (t f ) such as polarity, POS, NER, etc. (2) LSTM-CRF+Mu"
D19-5012,N16-1030,0,0.0352781,"gate linguistic, layout and topical features in the two binary classifiers: LogisticRegression and CNN. For CNN, we follow the architecture of Kim (2014) for sentencelevel classification, initializing the word vectors by FastText or BERT. We concatenate features in the last hidden layer before classification. One of our strong classifiers includes BERT that has achieved state-of-the-art performance on mul2.3 Fragment-level Propaganda Detection Figure 1 (right) describes our system for FLC task, where we design sequence taggers (Vu et al., 2016; Gupta et al., 2016) in three modes: (1) LSTMCRF (Lample et al., 2016) with word embeddings (w e) and character embeddings c e, tokenlevel features (t f ) such as polarity, POS, NER, etc. (2) LSTM-CRF+Multi-grain that jointly performs FLC and SLC with FastTextWordEmb and BERTSentEmb, respectively. Here, we add binary 1 some features from datasciencesociety.net/ detecting-propaganda-on-sentence-level/ 93 multi-tasking SLC Layer P/NP Ensemble FLC Layer PFD Layer O O B-Slogan I-Slogan I-Slogan NP NP B-P I-P I-P CRF CRF CRF CRF CRF LSTM LSTM LSTM LSTM LSTM w_e c_e t_f w_e c_e t_f w_e c_e t_f w_e c_e t_f w_e c_e t_f LSTM LSTM LSTM LSTM tweeted BUILD THE WALL Classifi"
D19-5012,D17-1317,0,0.0739464,"rticipants) and 4th (out of 25 participants) in FLC and SLC tasks, respectively. Introduction In the age of information dissemination without quality control, it has enabled malicious users to spread misinformation via social media and aim individual users with propaganda campaigns to achieve political and financial gains as well as advance a specific agenda. Often disinformation is complied in the two major forms: fake news and propaganda, where they differ in the sense that the propaganda is possibly built upon true information (e.g., biased, loaded language, repetition, etc.). Prior works (Rashkin et al., 2017; Habernal et al., 2017; Barr´on-Cede˜no et al., 2019) in detecting propaganda have focused primarily at document level, typically labeling all articles from a propagandistic news outlet as propaganda and thus, often non-propagandistic articles from the outlet are mislabeled. To this end, Da San Martino et al. (2019) focuses on analyzing the use of propaganda and detecting specific propagandistic 2 2.1 System Description Linguistic, Layout and Topical Features Some of the propaganda techniques (Da San Martino et al., 2019) involve word and phrases that express strong emotional implications, ex"
D19-5012,N16-1065,1,0.898228,"Missing"
D19-5012,Q17-1010,0,\N,Missing
D19-5720,W16-3001,0,0.14722,"lity of the correct tag sequence:   X log(p(y|X)) = s(X, y) − log  es(X,ey)  Methodology 3.1 Ayi ,yi+1 + e ), (1) = s(X, y) − logadd s(X, y e ∈YX y lossCRF = − log(p(y|X)) (2) 3.1.2 Hybrid Loss: CRF + Ranking We use a variant of ranking loss function proposed by dos Santos et al. (2015). Ranking maximizes the distance between the true label y + and the most competitive label c− : lossranking = max(0, 1 + (γ ∗ (m+ − y + )) + (γ ∗ (m− + c− )) where γ is the scaling factor that penalizes the predictions, m+ and m− are margins for correct and incorrect labels respectively. We follow Vu et al. (2016) to set the values of margins. The hybrid loss function hence is the sum of CRF tagging loss and ranking loss: BiLSTM-CRF The input to LSTM is a sequence of word features (w1 , w2 , . . . , wn ) and they compute a hidden state for each element in the sequence (h1 , h2 , . . . , hn ). This hidden state can be used to jointly model tagging decisions using CRF (Lafferty et al., 2001). CRF imposes ordering constraints on the tagging decisions e.g. I_Habitat should always be preceded by B_Habitat. For an input sentence, losshybrid = lossCRF + α · lossranking where α ∈ [0, 1], weighs the contributio"
D19-5720,D19-5701,0,0.128657,"rent entity. 2 1. To address NER tasks, we have employed neural network based sequence classifier, i.e., bi-LSTM-CRF and investigated multitasking of named entity detection (NED) and language modeling (LM). We further introduced hybrid loss including CRF and ranking. We also incorporated linguistic features such as POS, orthographic features, etc. We apply the proposed modeling approaches to both English and Spanish texts. Comparing with other systems, our submission (Team: MIC-CIS) is ranked 1st in BB-norm+NER task (Bossy et al., 2019) with standard error rate of 0.7159. In PharmaCoNER task (Gonzalez-Agirre et al., 2019), our submission scored F1-score of 0.8662. Task Description and Contribution We participate in the following three tasks organized by BioNLP workshop 2019: (1) PharmaCoNER: Recognition of pharmaceutical drugs and chemical entities in Spanish text. (2) BB-norm+NER: Recognition of Microorganism, Habitat and Phenotype entities and normalization with NCBI Taxonomy and OntoBiotope habitat concepts. (3) SeeDev Binary RE: Binary Relation extraction of genetic and molecular mechanisms involved in plant seed development. 2. To address RE task, we employed linguistic and entity features in SVM. Our sub"
D19-5720,W17-4419,0,0.051163,"Missing"
D19-5720,Q17-1010,0,0.0263387,"Missing"
D19-5720,D19-5719,0,0.0546425,"Missing"
D19-5720,N18-1003,1,0.893474,"Missing"
D19-5720,W18-5418,1,0.866829,"Missing"
D19-5720,C16-1239,1,0.908442,"Missing"
D19-5720,Q17-1008,0,0.0507928,"Missing"
D19-5720,P17-1194,0,0.0147421,". CRF imposes ordering constraints on the tagging decisions e.g. I_Habitat should always be preceded by B_Habitat. For an input sentence, losshybrid = lossCRF + α · lossranking where α ∈ [0, 1], weighs the contribution of ranking loss in the overall loss value. During training we minimize the hybrid loss and found it to improve the F1 score for both BB-norm+NER and PharmaCoNER tasks. 3.1.3 Multi-Tasking of Named Entity Recognition, Detection and Language Modelling We employed auxiliary objectives of named-entity detection (NED) (Aguilar et al., 2017) and bidirectional language modelling (LM) (Rei, 2017) in our W = (w1 , w2 , . . . , wn ), we consider a matrix P of scores output by the bidirectional LSTM. The size of P is n × k, 134 Algorithm 1 Entity Normalization 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: entities in knowledge base (KB). This is challenging because: (1) not all variations of textual forms for a canonical entity exists in the KB, (2) syntactic variations in the predicted entity mentions due to misspellings, abbreviations, acronyms and boundary errors. For BB-norm+NER task, we used two Biomedical databases OntoBiotope Ontology and NCBI Taxonomy. OntoBiotop"
D19-5720,P15-1061,0,0.0336535,"entifiers in KB by our entity normalization algorithm. 3.1.1 n X where the matrix A express transition scores such that Ai,j represents the score of a transition from the tag i to tag j. We add start and end tag to the set of possible tags, therefore, the size of A is k + 2. During training, we minimize the negative log-probability of the correct tag sequence:   X log(p(y|X)) = s(X, y) − log  es(X,ey)  Methodology 3.1 Ayi ,yi+1 + e ), (1) = s(X, y) − logadd s(X, y e ∈YX y lossCRF = − log(p(y|X)) (2) 3.1.2 Hybrid Loss: CRF + Ranking We use a variant of ranking loss function proposed by dos Santos et al. (2015). Ranking maximizes the distance between the true label y + and the most competitive label c− : lossranking = max(0, 1 + (γ ∗ (m+ − y + )) + (γ ∗ (m− + c− )) where γ is the scaling factor that penalizes the predictions, m+ and m− are margins for correct and incorrect labels respectively. We follow Vu et al. (2016) to set the values of margins. The hybrid loss function hence is the sum of CRF tagging loss and ranking loss: BiLSTM-CRF The input to LSTM is a sequence of word features (w1 , w2 , . . . , wn ) and they compute a hidden state for each element in the sequence (h1 , h2 , . . . , hn )."
D19-5720,N16-1030,0,0.340887,"sks such as RE, text classification, Question Answering (QA) etc., depend on it. Even though several methods have been devised to engineer reliable NER systems; however, most of them don’t explicitly address the extraction (or recognition) of nested entities, especially required in the biomedical domain. Nested entity is defined as an entity or sub-concept which is part of a longer entity (i.e., a parent). For instance in the Figure 1, fish is a nested entity as it is part of a parent entity fish pathogen. In this work, we have also investigated extracting nested entities via two bi-LSTM-CRF (Lample et al., 2016) networks: one for parent detection and another for nested entities with the parent entity. 2 1. To address NER tasks, we have employed neural network based sequence classifier, i.e., bi-LSTM-CRF and investigated multitasking of named entity detection (NED) and language modeling (LM). We further introduced hybrid loss including CRF and ranking. We also incorporated linguistic features such as POS, orthographic features, etc. We apply the proposed modeling approaches to both English and Spanish texts. Comparing with other systems, our submission (Team: MIC-CIS) is ranked 1st in BB-norm+NER task"
D19-5720,W04-1221,0,0.024001,"Missing"
D19-5720,W16-3005,0,0.0708838,"Missing"
D19-5720,W16-3004,0,0.048318,"Missing"
D19-5720,W17-1807,0,0.0350718,"Missing"
D19-5720,P05-1061,0,0.165217,"Missing"
D19-5720,P05-1000,0,0.318259,"Missing"
D19-5720,P16-1123,0,\N,Missing
D19-5730,Q17-1010,0,0.61991,"1 indicates “predicted label” by the DocRanker and Col-2 indicates “prediction probability” (p(q|v)). “Features” inside DocRanker indicates FastText and word2vec pretrained embeddings. sentence s) relative to the ith query word qi , using cosine similarity: TF-IDF (Manning et al., 2008) to compute sentence/document vectors. Embedding Sum Representation (ESR): Word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have been successfully used in computing distributed representation of text snippets (short or long). In ESR scheme, we employ the pre-trained word embeddings from FastText (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013). To represent a text (query, sentence or document), we compute the sum of (pre-trained) word vectors of each word in the text. E.g., ESR for a document d with D words can be computed as: e = PD e(di ) where, e ∈ RE is ESR(d) = d i=1 the pre-trained embedding vector of dimension E for the word di . Query-aware Attention-based Representation (QAR) for Documents and Sentences: Unlike ESR, we reward the maximum matches between a query and document by computing density of matches between them, similar to McDonald et al. (2018). In doing so, we introduce a weight"
D19-5730,W18-4001,1,0.88788,"Missing"
D19-5730,D18-1211,0,0.0204256,"ed word embeddings from FastText (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013). To represent a text (query, sentence or document), we compute the sum of (pre-trained) word vectors of each word in the text. E.g., ESR for a document d with D words can be computed as: e = PD e(di ) where, e ∈ RE is ESR(d) = d i=1 the pre-trained embedding vector of dimension E for the word di . Query-aware Attention-based Representation (QAR) for Documents and Sentences: Unlike ESR, we reward the maximum matches between a query and document by computing density of matches between them, similar to McDonald et al. (2018). In doing so, we introduce a weighted sum of word vectors from pre-trained embeddings and therefore, incorporate importance/attention of certain words in document (or sentence) that appear in the query text. For an enhanced query-aware attention based document (or sentence) representation, we first compute an histogram ai (d) ∈ RD of attention weights for each word k in the document d (or ai (d) = [ai,k ]D k=1 where, ai,k = e(qi )T e(dk ) ||e(qi ) |e(dk )|| for each kth word in the document d. Here, e(w) refers to an embedding vector of the word w. We then compute an query-aware attentionbase"
D19-5730,W18-5418,1,0.874101,"Missing"
D19-5730,D14-1162,0,0.0849732,"es signify parameter sharing, α attention weights are used to compute latent document representation h(v); (Right) RDoC Task-1 system architecture, where the numbered arrow (1) denotes the flow. Col-1 indicates “predicted label” by the DocRanker and Col-2 indicates “prediction probability” (p(q|v)). “Features” inside DocRanker indicates FastText and word2vec pretrained embeddings. sentence s) relative to the ith query word qi , using cosine similarity: TF-IDF (Manning et al., 2008) to compute sentence/document vectors. Embedding Sum Representation (ESR): Word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have been successfully used in computing distributed representation of text snippets (short or long). In ESR scheme, we employ the pre-trained word embeddings from FastText (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013). To represent a text (query, sentence or document), we compute the sum of (pre-trained) word vectors of each word in the text. E.g., ESR for a document d with D words can be computed as: e = PD e(di ) where, e ∈ RE is ESR(d) = d i=1 the pre-trained embedding vector of dimension E for the word di . Query-aware Attention-based Representation (QAR) for Documents an"
D19-5730,C16-1239,1,0.901235,"Missing"
D19-5730,N16-1030,0,0.0587068,"y-title-sentence interactions outperform the traditional BM25-Extra based ranking model by a significant margin. In future, we would like to introduce complementary feature representation via hidden vectors of LSTM jointly with topic models and would like to further investigate the interpretability (Gupta et al., 2015; Gupta and Sch¨utze, 2018) of the proposed neural ranking models in the sense that one can extract salient patterns determining relationship between query and text. Another promising direction would be introduce abstract information, such as part-of-speech and named entity tags (Lample et al., 2016; Gupta et al., 2016) to augment information retrieval (IR). Table 7: RDoC Task-2 analysis: This table shows that the most relevant sentence predicted using reRank(BM25-Extra) is actually not a relevant sentence, but Ensemble {#1, #2, #4} (Table 5) predicts the correct sentence as the most relevant. els (except [#3]) outperform tranditional ranking models, e.g., reRank(BM25-Extra) in terms of query-document relevance score. 4.5 Analysis: RDoC Task-2 Table 7 shows that the most relevant sentence predicted by reRank(BM25-Extra) is actually a non-relevant sentence. But an ensemble of predictions"
E09-1033,J07-4002,1,0.885864,"Missing"
E09-1033,J93-2003,0,0.0126508,"Missing"
E09-1033,D08-1092,0,0.0565955,"al and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining relevant work were published. Burkett and Klein (2008) describe a system for simultaneously improving Chinese and English parses of a Chinese/English bitext. This work is complementary to ours. The system is trained using gold standard trees in both Chinese and English, in contrast with our system which only has access to gold standard trees in English. Their system uses a tree alignment which varies within training, but this does not appear to make a large difference in performance. They use coarsely defined features which are language independent. We use several features similar to their two best performing sets of features, but in contrast wit"
E09-1033,P05-1022,0,0.107013,"e translated from English to German by a graduate student and an additional 3218 sen287 1 2 3 System Baseline Contrastive (5 trials/fold) Contrastive (greedy selection) Train 87.89 88.70 +base +base 0.82 Test 87.89 88.45 0.56 greedy feature selection helps with this (see also section 7). 88.82 0.93 88.55 0.66 6 Previous Work As we mentioned in section 2, work on parse reranking is relevant, but a vital difference is that we use features based only on syntactic projection of the two languages in a bitext. For an overview of different types of features that have been used in parse reranking see Charniak and Johnson (2005). Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et"
E09-1033,W98-1115,0,0.0270442,"glish sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TIGER treebank. For the Europarl corpus, we created 1-best parses for both languages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to t"
E09-1033,W06-1608,0,0.056453,"n style is different, e.g., NP structure in German is flatter. Introduction Parallel text or bitext is an important knowledge source for solving many problems such as machine translation, cross-language information retrieval, and the projection of linguistic resources from one language to another. In this paper, we show that bitext-based features are effective in addressing another NLP problem, increasing the accuracy of statistical parsing. We pursue this approach for a number of reasons. First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver, 2006). Improved parses of bitext should result in improved machine translation. Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext. Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al., 2006). It is well known that different languages encode different types of grammatical information (agreement, case, tense etc.) and that what can be left unspecified in one language mus"
E09-1033,2008.amta-srw.2,0,0.716401,"nguages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007)). We aligned approximately 3.08 million sentence pairs. We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008). Data and Experiments We used the subset of the Wall Street Journal investigated in (Atterer and Schu¨ tze, 2007) for our experiments, which consists of all sentences that have at least one prepositional phrase attachment ambiguity. This difficult subset of sentences seems particularly interesting when investigating the potential of information in bitext for improving parsing performance. The first 500 sentences of this set were translated from English to German by a graduate student and an additional 3218 sen287 1 2 3 System Baseline Contrastive (5 trials/fold) Contrastive (greedy selection)"
E09-1033,J07-3002,1,0.821458,"tly. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TIGER treebank. For the Europarl corpus, we created 1-best parses for both languages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007)). We aligned approximately 3.08 million sentence pairs. We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008). Data and Experiments We used the subset of the Wall Street Journal investigated in (Atterer and Schu¨ tze, 2007) for our experiments, which consists of all sentences that have at least one prepositional phrase attachment ambiguity. This difficult subset of sentences seems particularly interesting when investigating the potential of information in bitext for improving p"
E09-1033,P02-1035,0,0.028046,"hown in figures 1 and 2, respectively, and that the semantically more plausible second parse in figure 2 is correct. How can we determine that the second parse should be favored? Since we are parsing bitext, we can observe the German translation which is “Er sah ein Baby und eine Frau, die graue Haare hatte” (glossed: “he saw a baby and a woman, who gray hair had”). The singular verb in the subordinate clause (“hatte”: “had”) indicates that the subordinate S must be attached low to “woman” (“Frau”) as shown in figure 3. We follow Collins’ (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). Given a new sentence to parse, we first select the best N parse trees according to a generative model. Then we use new features to learn discriminatively how to rerank the parses in this N-best list. We use features derived using projections of the 1-best German parse onto the hypothesized English parse under consideration. In more detail, we take the 100 best English parses from the BitPar parser (Schmid, 2004) and rerank them. We have a good chance of finding the optimal parse among the 100-best1 . An automatically generated word alignment determines translational correspondence between G"
E09-1033,C04-1024,0,0.533724,"d”) indicates that the subordinate S must be attached low to “woman” (“Frau”) as shown in figure 3. We follow Collins’ (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). Given a new sentence to parse, we first select the best N parse trees according to a generative model. Then we use new features to learn discriminatively how to rerank the parses in this N-best list. We use features derived using projections of the 1-best German parse onto the hypothesized English parse under consideration. In more detail, we take the 100 best English parses from the BitPar parser (Schmid, 2004) and rerank them. We have a good chance of finding the optimal parse among the 100-best1 . An automatically generated word alignment determines translational correspondence between German and English. We use features which measure syntactic di3 Model We use a log-linear model to choose the best English parse. The feature functions are functions on the hypothesized English parse e, the German parse g, and the word alignment a, and they assign a score (varying between 0 and infinity) that measures syntactic divergence. The alignment of a sentence pair is a function that, for each English word, r"
E09-1033,P06-2039,0,0.0167168,"2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining relevant work were published. Burkett and Klein (2008)"
E09-1033,steinberger-etal-2006-jrc,0,0.0309533,"nalyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TIGER treebank. For the Europarl corpus, we created 1-best parses for both languages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007)). We aligned approximately 3.08 million sentence pairs. We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008). Data and Experiments We used the subset of the Wall Street Journal investigated in (Atterer and Schu¨ tze, 2007) for our experiments, which consists of all sentences that have at least one prepositional phrase attachment"
E09-1033,N01-1026,0,0.0753461,"see Charniak and Johnson (2005). Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used). We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining rel"
E09-1033,W06-2002,0,0.0277199,"problems in English parses which can be disambiguated using the German translation. Syntactic projection has been used to bootstrap treebanks in resource poor languages. Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al. (2005) and Goyal and Chatterjee (2006). Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English. Hopkins and Kuhn (2006) conducted research with goals similar to ours. They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance. After the submission of our paper for review, two papers outlining relevant work were published. Burkett and Klein (2008) describe a system for simultaneously improving Chinese and English parses of a Chinese/English bitext. This work is complementary to ours. The system is trained using gold standard trees in both Chinese and English, in contrast with our"
E09-1033,N03-1016,0,0.0113093,"additional 1000 reserved sentences) when we trained BitPar on the Penn treebank. Parses. We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967). It computes a compact parse forest for all possible analyses. As all possible analyses are computed, any number of best parses can be extracted. In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003). BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently. For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses. The German parser was trained on the TIGER treebank. For the Europarl corpus, we created 1-best parses for both languages. Word Alignment. We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus. We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better qualit"
E09-1033,N03-1017,0,0.00391606,"erforming sets of features, but in contrast with their work, we also define features which are specifically aimed at English disambiguation problems that we have observed can be resolved Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003). As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments. These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). Experiments. We perform 7-way crossvalidation on 3718 sentences. In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences. Our results are shown in table 1. In row 1, we take the hypothesis ranked best by BitPar. In row 2, we train using the algorithm outlined in section 4. To cancel out any effect caused by a particularly effective or ineffective starting λ value, we perform 5 trials each time. Columns 3 and 5 report the improvement over the baseline on train and test respectively. We reach an improvement of 0.56 over the baseline using t"
E09-1033,2005.mtsummit-papers.11,0,0.0213247,"ure value is calculated in eq. 10. where constituents starting with “DT NN”, e.g., (NP (DT NN NN NN)), are incorrectly split into two NPs, e.g., (NP (DT NN)) and (NP (NN NN)). This feature fires in this case, and projects the (NP (DT NN)) into German. If the German projection is a surprisingly large number of words (as should be the case if the German also consists of a determiner followed by several nouns) then the penalty paid by this feature is large. This feature is important as (NP (DT NN)) is a very common construction. q(i, j) = p(tag(up(i))|tag(j), tag(up(j))) l X i=1 We use Europarl (Koehn, 2005), from which we extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate the probabilistic feature functions described in this section. For the PDepth feature, we estimate English parse depth probability conditioned on German parse depth from Europarl by calculating a simple probability distribution over the 1-best parse pairs for each parallel sentence. A very deep German parse is unlikely to correspond to a flat English parse and we can penalize such a parse using PDepth. The index i refers to a sentence pair in Europarl, as does j. Let li and mi be the depths of t"
E09-1033,J93-2004,0,0.0335382,"Missing"
E09-1033,N06-1020,0,0.053054,"tistical parsing. We pursue this approach for a number of reasons. First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver, 2006). Improved parses of bitext should result in improved machine translation. Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext. Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al., 2006). It is well known that different languages encode different types of grammatical information (agreement, case, tense etc.) and that what can be left unspecified in one language must be made explicit We conduct our research in the framework of N-best parse reranking, but apply it to bitext and add only features based on syntactic projection from German to English. We test the idea that, generally, English parses with more isomorphism with respect to the projected German parse are better. The system takes as input (i) English sentences with a list of automatically generated syntactic parses, (i"
E09-1033,J03-1002,0,0.00441457,"ndard trees in English. Their system uses a tree alignment which varies within training, but this does not appear to make a large difference in performance. They use coarsely defined features which are language independent. We use several features similar to their two best performing sets of features, but in contrast with their work, we also define features which are specifically aimed at English disambiguation problems that we have observed can be resolved Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003). As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments. These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003). Experiments. We perform 7-way crossvalidation on 3718 sentences. In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences. Our results are shown in table 1. In row 1, we take the hypothesis ranked best by BitPar. In row 2, we train using the algorithm outlined in secti"
E09-1033,P03-1021,0,0.00834679,"to the DTNN feature function but trying to counteract a bias towards (NP (NP) (PP)) units (v) A feature function which penalizes aligning clausal units to non-clausal units (vi) The BitPar rank 4 Training Log-linear models are often trained using the Maximum Entropy criterion, but we train our model directly to maximize F1 . We score F1 by comparing hypothesized parses for the discriminative training set with the gold standard. To try to find the optimal λ vector, we perform direct accuracy maximization, meaning that we search for the λ vector which directly optimizes F1 on the training set. Och (2003) has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation. The technique involves calculating an explicit representation of the piecewise constant function gm (x) which evaluates the accuracy of the hypotheses which would be picked by eq. 2 from a set of hypotheses if we hold all weights constant, except for the weight λm , which is set to x. This is calculated in one pass over the data. The algorithm for training is initialized with a choice for λ and is described in figure 4. The function F1 (λ) returns F1 of the pars"
E09-1083,P03-1062,0,\N,Missing
E12-1028,N09-2010,0,0.029291,"Missing"
E12-1028,I11-1130,0,0.0212918,"era gets a negative evaluation, but only the latter reason is informative. In our work, we evaluate the quality of the reason given for a sentiment. The use case we address in this paper requires a short, easy-to-read summary. A well-formed sentence is usually easier to understand than a pro/con table. It also has the advantage that the information conveyed is accurately representing what the user wanted to say – this is not the case for a presentation that involves several complex processing steps and takes linguistic material out of the context that may be needed to understand it correctly. Berend (2011) performs a form of pro/con summarization that does not rely on aspects. However, most of the problems of aspect-based pro/con summarization also apply to this paper: no differentiation between good and bad reasons, the need for human labels to train a classifier, and inferior readability compared to a well-formed sentence. Two previous approaches that have attempted to extract sentences from reviews in the context of summarization are (Beineke et al., 2004) and (Arora et al., 2009). Beineke et al. (2004) train a classifier on rottentomatoes.com summary sentences provided by review authors. Th"
E12-1028,P06-2063,0,0.0342388,"arization (Nenkova and McKeown, 2011) are important subfields of NLP. The work most relevant to this paper is work on summarization methods that addresses the specific requirements of summarization in sentiment analysis. There are two lines of work in this vein with goals similar to ours: (i) aspect-based and pro/con-summarization and (ii) approaches that extract summary sentences from reviews. An aspect is a component or attribute of a product such as “battery”, “lens cap”, “battery life”, and “picture quality” for cameras. Aspectoriented summarization (Hu and Liu, 2004; Zhuang et al., 2006; Kim and Hovy, 2006) collects sentiment assessments for a given set of aspects and returns a list of pros and cons about every aspect for a review or, in some cases, on a per-sentence basis. Aspect-oriented summarization and pro/consummarization differ in a number of ways from supporting sentence summarization. First, aspects and pros&cons are taken from a fixed inventory. The inventory is typically small and does not cover the full spectrum of relevant information. Second, in its most useful form, aspectoriented summarization requires classification of phrases and sentences according to the aspect they belong to"
E12-1028,N03-5008,0,0.0645229,"Missing"
E12-1028,W01-0100,0,\N,Missing
E14-4039,P04-1035,0,0.0856623,"ntribute to the polarity of the entity but contains a term with a known lexical non-neutral polarity. For example, movie reviews often have plot summaries which contain subjective descriptions, e.g., “April loves her new home and friends.”, containing “loves”, commonly a subjective positive term. Other domains contain different types of nonrelevant content: Music reviews may contain track listings, product reviews on retail platforms contain complaints that do not concern the product, e.g., about shipping and handling. Filtering such nonrelevant content can help to improve sentiment analysis (Pang and Lee, 2004). Sentiment relevance (Scheible and Sch¨utze, 2013; Taboada 2 Stacked Denoising Autoencoders The stacked denoising autoencoder (SDA, (Vincent et al., 2010)) is a neural network (NN) model for unsupervised feature representation learning. 200 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 200–204, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics An autoencoder takes an input vector x, uses an NN layer with a (possibly) nonlinear activation function to generate a hidden feature representatio"
E14-4039,P13-1094,1,0.896144,"Missing"
E14-4039,W09-3909,0,0.0475829,"Missing"
E14-4039,P11-2100,0,0.0377919,"Missing"
E14-4039,P12-2018,0,0.0245805,"tation techniques that require large hand-crafted databases. In this paper, we present experiments on SR classification with automatically learned feature representations on multiple domains. We show that a combination of transfer learning and in-task supervision using features learned unsupervisedly by the stacked denoising autoencoder significantly outperforms a bag-of-words baseline for in-domain and cross-domain classification. 1 Introduction Many approaches to sentiment analysis rely on term-based clues to detect the polarity of sentences or documents, using the bag-of-words (BoW) model (Wang and Manning, 2012). One drawback of this approach is that the polarity of a clue is often treated as fixed, which can be problematic when content is not intended to contribute to the polarity of the entity but contains a term with a known lexical non-neutral polarity. For example, movie reviews often have plot summaries which contain subjective descriptions, e.g., “April loves her new home and friends.”, containing “loves”, commonly a subjective positive term. Other domains contain different types of nonrelevant content: Music reviews may contain track listings, product reviews on retail platforms contain compl"
E17-1003,D15-1044,0,0.16552,"et al., 2010). It consists of two medium-sized corpora from different domains (Wikipedia and biomedical) that allow us to run a large number of comparative experiments with different neural networks and exhaustively investigate different dimensions of attention. Convolutional and recurrent neural networks (CNNs and RNNs) perform well on many NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Zeng et al., 2014; Zhang and Wang, 2015). CNNs are most often used with pooling. More recently, attention mechanisms have been successfully integrated into CNNs and RNNs (Bahdanau et al., 2015; Rush et al., 2015; Hermann et al., 2015; Rockt¨aschel et al., 2016; Yang et al., 2016; He and Golub, 2016; Yin et al., 2016). Both pooling and attention can be thought of as selection mechanisms that help the network focus on the most relevant parts of a layer, either an input or a hidden layer. This is especially beneficial for long input sequences, e.g., long sentences or entire documents. We apply CNNs and RNNs to uncertainty detection and compare them to a number of baselines. We show that attention-based CNNs and RNNs are effective for uncertainty detection. On a Wikipedia benchmark, we improve the state"
E17-1003,D14-1181,0,0.0201755,"(wiki: 45.1k, 6 number of tokens per sentence after tokenization with Stanford tokenizer (Manning et al., 2014). 28 1 0.8 F1 score Model Baseline CNN CNN attention-only CNN combined CNN combined CNN combined CNN combined CNN RNN 0.6 0.4 0.2 0 0 50 100 150 average all k-max 86.89 86.39 Figure 6: F1 results for different sentence lengths k-max sequence per-dim per-pos 87.00 87.22 on the 2-class Stanford Sentiment Treebank (SST2) dataset7 (Socher et al., 2013). For a baseline model, we train a CNN similar to our uncertainty CNN but with convolutional filters of different widths, as proposed in (Kim, 2014), and extend it with our attention layer. As cues for external attention, we use the most frequent positive phrases from the train set. Our model is much simpler than the state-of-the-art models for SST-2 but still achieves reasonable results.8 The results in Table 6 show the same trends as the CNN results in Table 3, suggesting that our methods are applicable to other tasks as well. Table 7 shows that the benefit of sequencepreserving attention is indeed task dependent. For sentiment analysis on SST-2, sequence-preserving methods outperform the sequence-agnostic ones. 7 Related Work Uncertain"
E17-1003,J12-2002,0,0.0245123,"Missing"
E17-1003,D13-1170,0,0.00198607,"he CNN clearly outperforms the RNN on wiki. The datasets vary in several aspects, such as average sentence lengths (wiki: 21, bio: 27)6 , size of vocabularies (wiki: 45.1k, 6 number of tokens per sentence after tokenization with Stanford tokenizer (Manning et al., 2014). 28 1 0.8 F1 score Model Baseline CNN CNN attention-only CNN combined CNN combined CNN combined CNN combined CNN RNN 0.6 0.4 0.2 0 0 50 100 150 average all k-max 86.89 86.39 Figure 6: F1 results for different sentence lengths k-max sequence per-dim per-pos 87.00 87.22 on the 2-class Stanford Sentiment Treebank (SST2) dataset7 (Socher et al., 2013). For a baseline model, we train a CNN similar to our uncertainty CNN but with convolutional filters of different widths, as proposed in (Kim, 2014), and extend it with our attention layer. As cues for external attention, we use the most frequent positive phrases from the train set. Our model is much simpler than the state-of-the-art models for SST-2 but still achieves reasonable results.8 The results in Table 6 show the same trends as the CNN results in Table 3, suggesting that our methods are applicable to other tasks as well. Table 7 shows that the benefit of sequencepreserving attention is"
E17-1003,P14-5010,0,0.00370693,"Missing"
E17-1003,P15-1003,0,0.0236629,"her sequence classification tasks are future work. We made our code publicly available for future research (http://cistern.cis.lmu.de). Attention has been mainly used for recurrent neural networks (Bahdanau et al., 2015; Rush et al., 2015; Hermann et al., 2015; Rockt¨aschel et al., 2016; Peng et al., 2015; Yang et al., 2016). We integrate attention into CNNs and show that this is beneficial for uncertainty detection. Few studies in vision integrated attention into CNNs (Stollenga et al., 2014; Xiao et al., 2015; Chen et al., 2015) but this has not been used often in NLP so far. Exceptions are Meng et al. (2015), Wang et al. (2016) and Yin et al. (2016). Meng et al. (2015) used several layers of local and global attention in a complex machine translation model with a large number of parameters. Our reimplementation of their network performed poorly for uncertainty detection (51.51/66.57 on wiki/bio); we suspect that the reason is that Meng et al. (2015)’s training set was an order of magnitude larger than ours. Our approach makes effective use of a much smaller training set. Yin et al. (2016) compared attention based input representations and attention Acknowledgments Heike Adel is a recipient of the"
E17-1003,J12-2004,0,0.0326198,"Missing"
E17-1003,W10-3002,0,0.0274252,"phrases “suggested”, “not” and “more likely” that correspond almost perfectly to the true uncertainty cues. K-max pooling of standard CNNs, on the other hand, can only select the k maximum values per dimension, i.e., it can pick at most k uncertainty cues per dimension. Comparison to State of the Art Table 5 compares our models with the state of the art on the uncertainty detection benchmark datasets. On Wikipedia, our CNN outperforms the state of the art by more than three points. On bio, the best model uses a large number of manually designed features and an exhaustive corpus preprocessing (Tang et al., 2010). Our models achieve comparable results without preprocessing or feature engineering. 5.1 ly , islathe n so me wad tim s kn e s ow n Braas zil an , d rep migso res ht en t sathe islame nd as Brathe zil of th f we e co st as t Ire of lan d . pooling internal external preserving attention was that the semantic meaning of a sentence can vary depending on where an uncertainty cue occurs. However, the core of uncertainty detection is keyword and keyphrase detection; so, the overall sentence structure might be less important for this task. For tasks with a stronger natural language understanding com"
E17-1003,J12-2005,0,0.0110455,"6 show the same trends as the CNN results in Table 3, suggesting that our methods are applicable to other tasks as well. Table 7 shows that the benefit of sequencepreserving attention is indeed task dependent. For sentiment analysis on SST-2, sequence-preserving methods outperform the sequence-agnostic ones. 7 Related Work Uncertainty Detection. Uncertainty has been extensively studied in linguistics and NLP (Kiparsky and Kiparsky, 1970; Karttunen, 1973; Karttunen and Zaenen, 2005), including modality (Saur´ı and Pustejovsky, 2012; De Marneffe et al., 2012; Szarvas et al., 2012) and negation (Velldal et al., 2012; Baker et al., 2012). Szarvas et al. (2012), Vincze (2014b) and Zhou et al. (2015) conducted cross domain experiments. Domains studied include news (Saur´ı and Pustejovsky, 2009), biomedicine (Vincze et al., 2008), Wikipedia (Ganter and Strube, 2009) and social media (Wei et al., 2013). Corpora such as FactBank (Saur´ı and Pustejovsky, 2009) are annotated in detail with respect to perspective, level of factuality and polarR 85.1 78.3 49.6 52.0 Those values suggest that the RNN predicts uncertainty more reluctantly than the CNN. 6 test set 84.84 83.56 85.22 86.11 86.06 86.89 Table 7: Accuracy"
E17-1003,W08-0606,0,0.0728198,"Missing"
E17-1003,C14-1174,0,0.0870926,"pare them to other configurations along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features. 1 Introduction For many natural language processing (NLP) tasks, it is essential to distinguish uncertain (nonfactual) from certain (factual) information. Such tasks include information extraction, question answering, medical information retrieval, opinion detection, sentiment analysis (Karttunen and Zaenen, 2005; Vincze, 2014a; D´ıaz et al., 2016) and knowledge base population (KBP). In KBP, we need to distinguish, e.g., “X may be Basque” and “X was rumored to be Basque” (uncertain) from “X is Basque” (certain) to decide whether to add the fact “Basque(X)” to a knowledge base. In this paper, we use the term uncertain information to refer to speculation, opinion, vagueness and ambiguity. We focus our experiments on the uncertainty detection (UD) dataset from the CoNLL2010 hedge cue detection task (Farkas 22 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:"
E17-1003,P16-1123,0,0.00769578,"Missing"
E17-1003,C16-1329,0,0.00381269,"model behaviors: The CNN performs better than the RNN independent of the number of OOVs (Figure in appendix). Another important difference between CNN and RNN is the distribution of precision and recall. While on bio, precision and recall are almost equal for both models, the values vary on wiki: P 52.5 58.6 75.2 76.3 F H H W H W Table 6: Accuracy on SST-2, different focus and source of attention. 200 length of sentence CNN CNN + external attention RNN RNN + external attention S I I I E E Outlook: Different Task 7 http://nlp.stanford.edu/sentiment The state-of-the-art accuracy is about 89.5 (Zhou et al., 2016; Yin and Sch¨utze, 2015). To investigate whether our attention methods are also applicable to other tasks, we evaluate them 8 29 based pooling. Instead, our goal is to keep the convolutional and pooling layers unchanged and combine their strengths with attention. Allamanis et al. (2016) applied a convolutional layer to compute attention weights. In this work, we concentrate on the commonly used feed forward layers for that. Comparing them to other options, such as convolution, is an interesting direction for future work. Attention in the literature computes a weighted average with internal at"
E17-1003,P13-2011,0,0.018144,"ence-agnostic ones. 7 Related Work Uncertainty Detection. Uncertainty has been extensively studied in linguistics and NLP (Kiparsky and Kiparsky, 1970; Karttunen, 1973; Karttunen and Zaenen, 2005), including modality (Saur´ı and Pustejovsky, 2012; De Marneffe et al., 2012; Szarvas et al., 2012) and negation (Velldal et al., 2012; Baker et al., 2012). Szarvas et al. (2012), Vincze (2014b) and Zhou et al. (2015) conducted cross domain experiments. Domains studied include news (Saur´ı and Pustejovsky, 2009), biomedicine (Vincze et al., 2008), Wikipedia (Ganter and Strube, 2009) and social media (Wei et al., 2013). Corpora such as FactBank (Saur´ı and Pustejovsky, 2009) are annotated in detail with respect to perspective, level of factuality and polarR 85.1 78.3 49.6 52.0 Those values suggest that the RNN predicts uncertainty more reluctantly than the CNN. 6 test set 84.84 83.56 85.22 86.11 86.06 86.89 Table 7: Accuracy on SST-2, sequence-agnostic vs. sequence-preserving attention. bio: 25.3k), average number of out-of-vocabulary (OOV) words per sentence w.r.t. our word embeddings (wiki: 4.5, bio: 6.5), etc. All of those features can influence model performance, especially because of the different way"
E17-1003,N16-1174,0,0.0621975,"t domains (Wikipedia and biomedical) that allow us to run a large number of comparative experiments with different neural networks and exhaustively investigate different dimensions of attention. Convolutional and recurrent neural networks (CNNs and RNNs) perform well on many NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Zeng et al., 2014; Zhang and Wang, 2015). CNNs are most often used with pooling. More recently, attention mechanisms have been successfully integrated into CNNs and RNNs (Bahdanau et al., 2015; Rush et al., 2015; Hermann et al., 2015; Rockt¨aschel et al., 2016; Yang et al., 2016; He and Golub, 2016; Yin et al., 2016). Both pooling and attention can be thought of as selection mechanisms that help the network focus on the most relevant parts of a layer, either an input or a hidden layer. This is especially beneficial for long input sequences, e.g., long sentences or entire documents. We apply CNNs and RNNs to uncertainty detection and compare them to a number of baselines. We show that attention-based CNNs and RNNs are effective for uncertainty detection. On a Wikipedia benchmark, we improve the state of the art by more than 3.5 F1 points. Despite the success of attent"
E17-1003,K15-1021,1,0.917216,"Missing"
E17-1003,Q16-1019,1,0.911846,"at allow us to run a large number of comparative experiments with different neural networks and exhaustively investigate different dimensions of attention. Convolutional and recurrent neural networks (CNNs and RNNs) perform well on many NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Zeng et al., 2014; Zhang and Wang, 2015). CNNs are most often used with pooling. More recently, attention mechanisms have been successfully integrated into CNNs and RNNs (Bahdanau et al., 2015; Rush et al., 2015; Hermann et al., 2015; Rockt¨aschel et al., 2016; Yang et al., 2016; He and Golub, 2016; Yin et al., 2016). Both pooling and attention can be thought of as selection mechanisms that help the network focus on the most relevant parts of a layer, either an input or a hidden layer. This is especially beneficial for long input sequences, e.g., long sentences or entire documents. We apply CNNs and RNNs to uncertainty detection and compare them to a number of baselines. We show that attention-based CNNs and RNNs are effective for uncertainty detection. On a Wikipedia benchmark, we improve the state of the art by more than 3.5 F1 points. Despite the success of attention in prior work, the design space of"
E17-1003,C14-1220,0,0.0115799,"g Different Dimensions of Attention for Uncertainty Detection ¨ Heike Adel and Hinrich Schutze Center for Information and Language Processing (CIS) LMU Munich, Germany heike@cis.lmu.de Abstract et al., 2010). It consists of two medium-sized corpora from different domains (Wikipedia and biomedical) that allow us to run a large number of comparative experiments with different neural networks and exhaustively investigate different dimensions of attention. Convolutional and recurrent neural networks (CNNs and RNNs) perform well on many NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Zeng et al., 2014; Zhang and Wang, 2015). CNNs are most often used with pooling. More recently, attention mechanisms have been successfully integrated into CNNs and RNNs (Bahdanau et al., 2015; Rush et al., 2015; Hermann et al., 2015; Rockt¨aschel et al., 2016; Yang et al., 2016; He and Golub, 2016; Yin et al., 2016). Both pooling and attention can be thought of as selection mechanisms that help the network focus on the most relevant parts of a layer, either an input or a hidden layer. This is especially beneficial for long input sequences, e.g., long sentences or entire documents. We apply CNNs and RNNs to un"
E17-1049,W16-2007,0,0.235502,"Missing"
E17-1049,N15-1107,0,0.0463434,"gically rich languages still constitute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person singular present tense of bring is generated. One generalization of inflection is morphological reinflection (MRI) (Cotterell et al., 2016a), where we must produce an inflected form from a triple of target tag, source form and source tag. The inflection task is the special case where the source form is the lemma. As an ex"
E17-1049,D15-1041,0,0.0289893,"sense, the verbal paradigm is partitioned into subparadigms. To see why multi-source models could help in this case, starting only from the infinitive treffen makes it difficult to predict subjunctive form tr¨afest, but the additional information of the fellow subjunctive form tr¨afe makes the task easier. Introduction Morphologically rich languages still constitute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person"
E17-1049,W14-4012,0,0.178924,"Missing"
E17-1049,K15-1017,1,0.918613,"Missing"
E17-1049,Q15-1031,1,0.856892,"transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has ac"
E17-1049,N16-1080,1,0.875264,"Missing"
E17-1049,W02-0603,0,0.06034,"09) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has achieved widespread adoption. Bayesian methods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contai"
E17-1049,D09-1011,0,0.0711054,"orming weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Cre"
E17-1049,D08-1113,0,0.275655,"Missing"
E17-1049,N13-1138,0,0.107651,"y transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. pe"
E17-1049,D13-1105,0,0.022021,"ods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521"
E17-1049,N16-1077,0,0.179268,"echanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on"
E17-1049,N16-1101,0,0.0232653,"nd P that encode that the target form is 3rd person plural. We omit the tags from the diagram to which the model hardly attends. al., 2015), parsing (Vinyals et al., 2014) and automatic speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The first work on multi-source models was presented for machine translation. Zoph and Knight (2016) made simultaneous use of source sentences in multiple languages in order to find the best match possible in the target language. Unlike our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms f"
E17-1049,E14-1060,0,0.17837,"cific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for t"
E17-1049,W16-2010,1,0.877615,"Missing"
E17-1049,P16-2090,1,0.891599,"Missing"
E17-1049,P13-2017,0,0.0266029,"Missing"
E17-1049,D15-1272,1,0.856915,"Missing"
E17-1049,N15-1093,0,0.071106,"ute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person singular present tense of bring is generated. One generalization of inflection is morphological reinflection (MRI) (Cotterell et al., 2016a), where we must produce an inflected form from a triple of target tag, source form and source tag. The inflection task is the special case where the source form is the lemma. As an example, we may again consider generati"
E17-1049,W16-2003,0,0.0239677,"th inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521 6 Conclusion Generation of unknown inflections in morphologically rich languages is an important task that remains unsolved. We provide a new angle on the problem by considering systems that are allowed to have multiple inflected forms as input. To this end, we define the task of multi-source morphological reinflection as a generalization of singlesource MRI (Cotterell et al., 2016a) and present a model that solves the task. We extend an attentionbased RNN encoder-decoder architecture from the sin"
E17-1049,N16-1076,1,0.804979,"ke our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and a"
E17-1049,W13-3504,0,0.0535729,"for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has achieved widespread adoption. Bayesian methods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinfl"
E17-1049,P15-2111,0,0.0314828,"ointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521 6 Conclusion Generation of unknown inflections in morphologically rich languages is an important task that remains unsolved. We provide a new angle on the problem by considering systems that are allowed to have multiple inflected forms as input. To this end, we define the task of multi-source morphological reinflection as a generalization of singlesour"
E17-1049,N16-1004,0,0.0178675,"; Bahdanau et Figure 4: Attention heatmap for the multi-source model. The example is for the German verb wiegen ‘to weigh’. The model learns to focus most of its attention on forms that share the irregular subjunctive stem w¨og in addition to the target subtags 3 and P that encode that the target form is 3rd person plural. We omit the tags from the diagram to which the model hardly attends. al., 2015), parsing (Vinyals et al., 2014) and automatic speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The first work on multi-source models was presented for machine translation. Zoph and Knight (2016) made simultaneous use of source sentences in multiple languages in order to find the best match possible in the target language. Unlike our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. D"
E17-1055,D15-1041,0,0.0294513,"ntations into one multi-level representation. Character-subword level neural networks. Character-level convolutional neural networks (CNNs) are applied by dos Santos and Zadrozny (2014) to part of speech (POS) tagging, by dos Santos and Guimar˜aes (2015), Ma and Hovy (2016), and Chiu and Nichols (2016) to named entity recognition (NER), by Zhang et al. (2015) and Zhang and LeCun (2015) to sentiment analysis and text categorization, and by Kim et al. (2016) to language modeling (LM). Characterlevel LSTM is applied by Ling et al. (2015b) to LM and POS tagging, by Lample et al. (2016) to NER, by Ballesteros et al. (2015) to parsing morphologically rich languages, and by Cao and Rei (2016) to learning word embeddings. Bojanowski et al. (2016) learn word embeddings by representing words with the average of their character ngrams (subwords) embeddings. Similarly, Chen et al. (2015) extends word2vec for Chinese with joint modeling with characters. Fine-grained entity typing. Our task is to infer fine-grained types of KB entities. KB completion is an application of this task. Yaghoobzadeh and Sch¨utze (2015)’s FIGMENT system addresses this task with only contextual information; they do not use character-level and"
E17-1055,N16-1030,0,0.0172529,"ion of several different representations into one multi-level representation. Character-subword level neural networks. Character-level convolutional neural networks (CNNs) are applied by dos Santos and Zadrozny (2014) to part of speech (POS) tagging, by dos Santos and Guimar˜aes (2015), Ma and Hovy (2016), and Chiu and Nichols (2016) to named entity recognition (NER), by Zhang et al. (2015) and Zhang and LeCun (2015) to sentiment analysis and text categorization, and by Kim et al. (2016) to language modeling (LM). Characterlevel LSTM is applied by Ling et al. (2015b) to LM and POS tagging, by Lample et al. (2016) to NER, by Ballesteros et al. (2015) to parsing morphologically rich languages, and by Cao and Rei (2016) to learning word embeddings. Bojanowski et al. (2016) learn word embeddings by representing words with the average of their character ngrams (subwords) embeddings. Similarly, Chen et al. (2015) extends word2vec for Chinese with joint modeling with characters. Fine-grained entity typing. Our task is to infer fine-grained types of KB entities. KB completion is an application of this task. Yaghoobzadeh and Sch¨utze (2015)’s FIGMENT system addresses this task with only contextual information;"
E17-1055,W16-1603,0,0.0229306,"networks. Character-level convolutional neural networks (CNNs) are applied by dos Santos and Zadrozny (2014) to part of speech (POS) tagging, by dos Santos and Guimar˜aes (2015), Ma and Hovy (2016), and Chiu and Nichols (2016) to named entity recognition (NER), by Zhang et al. (2015) and Zhang and LeCun (2015) to sentiment analysis and text categorization, and by Kim et al. (2016) to language modeling (LM). Characterlevel LSTM is applied by Ling et al. (2015b) to LM and POS tagging, by Lample et al. (2016) to NER, by Ballesteros et al. (2015) to parsing morphologically rich languages, and by Cao and Rei (2016) to learning word embeddings. Bojanowski et al. (2016) learn word embeddings by representing words with the average of their character ngrams (subwords) embeddings. Similarly, Chen et al. (2015) extends word2vec for Chinese with joint modeling with characters. Fine-grained entity typing. Our task is to infer fine-grained types of KB entities. KB completion is an application of this task. Yaghoobzadeh and Sch¨utze (2015)’s FIGMENT system addresses this task with only contextual information; they do not use character-level and word-level features of entity names. Neelakantan and Chang (2015) and"
E17-1055,N15-1142,0,0.530806,"is paper, we present methods for learning multi-level representations of entities on three complementary levels: character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-theart learning methods on each level and find large differences, e.g., for deep learning models, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level; for word2vec (Mikolov et al., 2013) on the word level; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of representation contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities. 1 Introduction Knowledge about entities is essential for understanding human language. This knowledge can be attributional (e.g., canFly, isEdible), type-based (e.g., isFood, isPolitician, isDisease) or re"
E17-1055,Q16-1026,0,0.0475753,"tion (Socher et al., 2013; Wang et al., 2014). Novel entity representation methods we introduce in this paper are representation based on fasttext (Bojanowski et al., 2016) subword embeddings, several character-level representations, “order-aware” entity-level embeddings and the combination of several different representations into one multi-level representation. Character-subword level neural networks. Character-level convolutional neural networks (CNNs) are applied by dos Santos and Zadrozny (2014) to part of speech (POS) tagging, by dos Santos and Guimar˜aes (2015), Ma and Hovy (2016), and Chiu and Nichols (2016) to named entity recognition (NER), by Zhang et al. (2015) and Zhang and LeCun (2015) to sentiment analysis and text categorization, and by Kim et al. (2016) to language modeling (LM). Characterlevel LSTM is applied by Ling et al. (2015b) to LM and POS tagging, by Lample et al. (2016) to NER, by Ballesteros et al. (2015) to parsing morphologically rich languages, and by Cao and Rei (2016) to learning word embeddings. Bojanowski et al. (2016) learn word embeddings by representing words with the average of their character ngrams (subwords) embeddings. Similarly, Chen et al. (2015) extends word2v"
E17-1055,D15-1083,1,0.643972,"Missing"
E17-1055,W16-1313,0,0.0616016,"on. Our objective is binary cross entropy summed over types:  X  − mt log pt + (1 − mt ) log (1 − pt ) t where mt is the truth and pt the prediction. The key difficulty when trying to compute P (t|e) is in learning a good representation for entity e. We make use of contexts and name of e to represent its feature vector on the three levels of entity, word and character. but they rely on entity descriptions in KBs, which in many settings are not available. The problem of Fine-grained mention typing (FGMT) (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Del Corro et al., 2015; Shimaoka et al., 2016; Ren et al., 2016) is related to our task. FGMT classifies single mentions of named entities to their context dependent types whereas we attempt to identify all types of a KB entity from the aggregation of all its mentions. FGMT can still be evaluated in our task by aggregating the mention level decisions but as we will show in our experiments for one system, i.e., FIGER (Ling and Weld, 2012), our entity embedding based models are better in entity typing. 3 3.1 Entity-level representation Distributional representations or embeddings are commonly used for words. The underlying hypothesis is th"
E17-1055,P16-1023,1,0.864778,"Missing"
E17-1055,D14-1167,0,0.118647,"Missing"
E17-1055,N03-1031,0,\N,Missing
E17-1066,D12-1050,0,0.0373463,"ut sentence relations. Wang and Manning (2010) cope with the alignment between a sentence pair by using a probabilistic model that models tree-edit operations on dependency parse trees. Their model treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. Guo and Diab (2012) identify the degree of sentence similarity by modeling the missing words (words that are not in the sentence) so as to relieve the sparseness issue of sentence modeling. Yih et DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch¨utze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt¨aschel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin"
E17-1066,S14-2055,0,0.651314,"a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin et al. (2016a) 700 al. (2013) try to improve the shallow semantic component, lexical semantics, by formulating sentence pair as a semantic matching problem with a latent word-alignment structure as in (Chang et al., 2010). More fine-grained word overlap and alignment between two sentences are explored in (Lai and Hockenmaier, 2014), in which negation, hypernym/hyponym, synonym and antonym relations are used. Yao et al. (2013) extend word-toword alignment to phrase-to-phrase alignment by a semi-Markov CRF. Such approaches often require more computational resources. In addition, using syntactic/semantic parsing during run-time to find the best matching between structured representation of sentences is not trivial. DNN for sentence pair classification. There recently has been great interest in using DNNs for classifying sentence pairs as they can reduce the burden of feature engineering. For TE, Bowman et al. (2015b) emplo"
E17-1066,D15-1075,0,0.49385,"le” (C + ) or → “an old man is standing in the background” (C − ) . We can see that the kept parts have stronger alignments (green color), and changed parts have weaker alignments (blue color). Here, by “strong” / “weak” we mean how semantically close the two aligned phrases are. To successfully identify the relationships of (Q, C + ) or (Q, C − ), studying the changed parts is crucial. Hence, we argue that TE should pay more attention to weaker alignments. Introduction How to model a pair of sentences is a critical issue in many NLP tasks, including textual entailment (Marelli et al., 2014a; Bowman et al., 2015a; Yin et al., 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016). A key challenge common to these tasks is the lack of explicit alignment annotation between the sentences of the pair. Thus, inferring and assessing the semantic relations between words and phrases in the two sentences is a core issue. 699 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 699–709, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics In AS, we try to figure out:"
E17-1066,W15-4002,0,0.215916,"le” (C + ) or → “an old man is standing in the background” (C − ) . We can see that the kept parts have stronger alignments (green color), and changed parts have weaker alignments (blue color). Here, by “strong” / “weak” we mean how semantically close the two aligned phrases are. To successfully identify the relationships of (Q, C + ) or (Q, C − ), studying the changed parts is crucial. Hence, we argue that TE should pay more attention to weaker alignments. Introduction How to model a pair of sentences is a critical issue in many NLP tasks, including textual entailment (Marelli et al., 2014a; Bowman et al., 2015a; Yin et al., 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016). A key challenge common to these tasks is the lack of explicit alignment annotation between the sentences of the pair. Thus, inferring and assessing the semantic relations between words and phrases in the two sentences is a core issue. 699 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 699–709, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics In AS, we try to figure out:"
E17-1066,P15-1107,0,0.0651829,"Missing"
E17-1066,N10-1066,0,0.0323079,"ion-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin et al. (2016a) 700 al. (2013) try to improve the shallow semantic component, lexical semantics, by formulating sentence pair as a semantic matching problem with a latent word-alignment structure as in (Chang et al., 2010). More fine-grained word overlap and alignment between two sentences are explored in (Lai and Hockenmaier, 2014), in which negation, hypernym/hyponym, synonym and antonym relations are used. Yao et al. (2013) extend word-toword alignment to phrase-to-phrase alignment by a semi-Markov CRF. Such approaches often require more computational resources. In addition, using syntactic/semantic parsing during run-time to find the best matching between structured representation of sentences is not trivial. DNN for sentence pair classification. There recently has been great interest in using DNNs for clas"
E17-1066,D15-1166,0,0.0523332,"014) present two CNN architectures for paraphrasing, sentence completion (SC), tweet-response matching tasks. Yin and Sch¨utze (2015b) propose the MultiGranCNN architecture to model general sentence matching based on phrase matching on multiple levels of granularity. Wan et al. (2016) try to match two sentences in AS and SC by multiple sentence representations, each coming from the local representations of two LSTMs. Attention-based DNN for alignment. DNNs have been successfully developed to detect alignFigure 2: Gated Recurrent Unit ments, e.g., in machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). In addition, attention-based alignment is also applied in natural language inference (e.g., Rockt¨aschel et al. (2016),Wang and Jiang (2016)). However, most of this work aligns word-by-word. As Figure 1 shows, many sentence relations can be better identified through phrase level alignments. This is one motivation of our work. 3 Model This section first gives a brief introduction of GRU and how it performs phrase representation learning, then describes the different attentive poolings for phrase alignments w.r.t TE and AS tasks. 3.1"
E17-1066,W14-4012,0,0.0510546,"Missing"
E17-1066,S14-2001,0,0.436523,"“near a man with a smile” (C + ) or → “an old man is standing in the background” (C − ) . We can see that the kept parts have stronger alignments (green color), and changed parts have weaker alignments (blue color). Here, by “strong” / “weak” we mean how semantically close the two aligned phrases are. To successfully identify the relationships of (Q, C + ) or (Q, C − ), studying the changed parts is crucial. Hence, we argue that TE should pay more attention to weaker alignments. Introduction How to model a pair of sentences is a critical issue in many NLP tasks, including textual entailment (Marelli et al., 2014a; Bowman et al., 2015a; Yin et al., 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016). A key challenge common to these tasks is the lack of explicit alignment annotation between the sentences of the pair. Thus, inferring and assessing the semantic relations between words and phrases in the two sentences is a core issue. 699 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 699–709, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics In AS,"
E17-1066,marelli-etal-2014-sick,0,0.626921,"“near a man with a smile” (C + ) or → “an old man is standing in the background” (C − ) . We can see that the kept parts have stronger alignments (green color), and changed parts have weaker alignments (blue color). Here, by “strong” / “weak” we mean how semantically close the two aligned phrases are. To successfully identify the relationships of (Q, C + ) or (Q, C − ), studying the changed parts is crucial. Hence, we argue that TE should pay more attention to weaker alignments. Introduction How to model a pair of sentences is a critical issue in many NLP tasks, including textual entailment (Marelli et al., 2014a; Bowman et al., 2015a; Yin et al., 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016). A key challenge common to these tasks is the lack of explicit alignment annotation between the sentences of the pair. Thus, inferring and assessing the semantic relations between words and phrases in the two sentences is a core issue. 699 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 699–709, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics In AS,"
E17-1066,P12-1091,0,0.0313142,"sentence pair modeling. Heilman and Smith (2010) describe tree edit models that generalize tree edit distance by allowing operations that better account for complex reordering phenomena and by learning from data how different edits should affect the model’s decisions about sentence relations. Wang and Manning (2010) cope with the alignment between a sentence pair by using a probabilistic model that models tree-edit operations on dependency parse trees. Their model treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. Guo and Diab (2012) identify the degree of sentence similarity by modeling the missing words (words that are not in the sentence) so as to relieve the sparseness issue of sentence modeling. Yih et DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch¨utze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt¨aschel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al.,"
E17-1066,D14-1162,0,0.0808389,"Missing"
E17-1066,N10-1145,0,0.0400312,"2013) or phrase level (Yao et al., 2013) both have been studied before. For example, Yih et al. (2013) make use of WordNet (Miller, 1995) and Probase (Wu et al., 2012) for identifying hyper- and hyponymy. Yao et al. (2013) use POS tags, WordNet and paraphrase database for alignment identification. Their approaches rely on manual feature design and linguistic resources. We develop a deep neural network (DNN) to learn representations of phrases of arbitrary lengths. As a result, alignments can be searched in a more automatic and exhaustive way. 2 Related Work Non-DNN for sentence pair modeling. Heilman and Smith (2010) describe tree edit models that generalize tree edit distance by allowing operations that better account for complex reordering phenomena and by learning from data how different edits should affect the model’s decisions about sentence relations. Wang and Manning (2010) cope with the alignment between a sentence pair by using a probabilistic model that models tree-edit operations on dependency parse trees. Their model treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. Guo and Diab (2012) identify the degree of sente"
E17-1066,D15-1044,0,0.0508194,"completion (SC), tweet-response matching tasks. Yin and Sch¨utze (2015b) propose the MultiGranCNN architecture to model general sentence matching based on phrase matching on multiple levels of granularity. Wan et al. (2016) try to match two sentences in AS and SC by multiple sentence representations, each coming from the local representations of two LSTMs. Attention-based DNN for alignment. DNNs have been successfully developed to detect alignFigure 2: Gated Recurrent Unit ments, e.g., in machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). In addition, attention-based alignment is also applied in natural language inference (e.g., Rockt¨aschel et al. (2016),Wang and Jiang (2016)). However, most of this work aligns word-by-word. As Figure 1 shows, many sentence relations can be better identified through phrase level alignments. This is one motivation of our work. 3 Model This section first gives a brief introduction of GRU and how it performs phrase representation learning, then describes the different attentive poolings for phrase alignments w.r.t TE and AS tasks. 3.1 GRU Introduction GRU is a simplified version of LSTM. Both a"
E17-1066,S14-2131,0,0.0612997,"Missing"
E17-1066,C16-1164,1,0.890503,"Missing"
E17-1066,N16-1152,0,0.0812557,"Missing"
E17-1066,S14-2044,0,0.0448741,"Missing"
E17-1066,N16-1170,0,0.113226,"s. Their model treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. Guo and Diab (2012) identify the degree of sentence similarity by modeling the missing words (words that are not in the sentence) so as to relieve the sparseness issue of sentence modeling. Yih et DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch¨utze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt¨aschel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin et al. (2016a) 700 al. (2013) try to improve the shallow semantic component, lexical semantics, by formulating sentence pair as a semantic matching problem with a latent word-align"
E17-1066,C10-1131,0,0.147573,"Missing"
E17-1066,D15-1237,0,0.0833109,"Missing"
E17-1066,D13-1056,0,0.0606989,"Missing"
E17-1066,P13-1171,0,0.0317934,"trary granularity. Based on phrase representations, we can detect phrase alignments of different intensities. (ii) We propose attentive pooling to achieve flexible choice among alignments, depending on the characteristics of the task. (iii) We achieve state-of-the-art on TE task. The above analysis suggests that: (i) alignments connecting two sentences can happen between phrases of arbitrary granularity; (ii) phrase alignments can have different intensities; (iii) tasks of different properties require paying different attention to alignments of different intensities. Alignments at word level (Yih et al., 2013) or phrase level (Yao et al., 2013) both have been studied before. For example, Yih et al. (2013) make use of WordNet (Miller, 1995) and Probase (Wu et al., 2012) for identifying hyper- and hyponymy. Yao et al. (2013) use POS tags, WordNet and paraphrase database for alignment identification. Their approaches rely on manual feature design and linguistic resources. We develop a deep neural network (DNN) to learn representations of phrases of arbitrary lengths. As a result, alignments can be searched in a more automatic and exhaustive way. 2 Related Work Non-DNN for sentence pair modeling. Heilm"
E17-1066,N15-1091,1,0.884921,"Missing"
E17-1066,P15-1007,1,0.841148,"Missing"
E17-1066,Q16-1019,1,0.643899,"Missing"
E17-1074,W16-1208,0,0.0169233,"ams. For example, a random segmentation of “a rose is a rose is a rose” might be “[a ros][e is a ros][e is][a rose]”. This segmentation does not contain the segment “rose” and this part of the corpus can then not be exploited to learn a good embedding for the fourgram “rose”. However, with multiple random segmentation, it is likely that this part of the corpus does give rise to the segment “rose” in one of the segmentations and can contribute information to learning a good embedding for “rose”. We took the idea of random segmentation from work on biological sequences (Asgari and Mofrad, 2015; Asgari and Mofrad, 2016). Such sequences have no delimiters, so they are a good model if 787 one believes that delimiter-based segmentation is problematic for text. 3.2 Ngram equivalence classes/Permutation Form-meaning homomorphism premise. Nonsymbolic representation learning does not preprocess the training corpus by means of tokenization and considers many ngrams that would be ignored in tokenized approaches because they span token boundaries. As a result, the number of ngrams that occur in a corpus is an order of magnitude larger for tokenization-free approaches than for tokenization-based approaches. See supplem"
E17-1074,P14-2111,0,0.023886,"beddings are context-free: a given word w like “king” is represented by the same embedding independent of the context in which w occurs. Position embeddings are context-free as well: if the maximum size of a character ngram is k max , then the position embedding of the center of a string s of length 2k max − 1 is the same independent of the context in which s occurs. It is conceivable that text representations could be context-sensitive. For example, the hidden states of a character language model have been used as a kind of nonsymbolic text representation (Chrupala, 2013; Evang et al., 2013; Chrupala, 2014) and these states are context-sensitive. However, such models will in general be a second level of representation; e.g., the hidden states of a character language model generally use character embeddings as the first level of representation. Conversely, position embeddings can also be the basis for a context-sensitive second-level text representation. We have to start somewhere when we represent text. Position embeddings are motivated by the desire to provide a representation that can be computed easily and quickly (i.e., without taking 793 context into account), but that on the other hand is"
E17-1074,P16-1160,0,0.0155851,"contributions in this paper. (i) We propose the first generic method for training text representation models without the need for tokenization and address the challenging sparseness issues that make this difficult. (ii) We propose the first nonsymbolic utilization method that fully represents sequence information – in contrast to utilization methods like bag-of-ngrams that discard sequence information that is not directly encoded in the character ngrams themselves. 2 symbolic methods can perform better than morphological analysis, but the foregoing discussion motivates us to investigate them. Chung et al. (2016) focus on problems with the tokens produced by segmentation algorithms. Equally important is the problem that tokenization fails to capture structure across multiple tokens. The job of dealing with cross-token structure is often given to downstream components of the pipeline, e.g., components that recognize multiwords and named entitites in English or in fact any word in a language like Chinese that uses no overt delimiters. However, there is no linguistic or computational reason in principle why we should treat the recognition of a unit like “electromechanical” (containing no space) as fundam"
E17-1074,D13-1146,0,0.025562,"embeddings. Word embeddings are context-free: a given word w like “king” is represented by the same embedding independent of the context in which w occurs. Position embeddings are context-free as well: if the maximum size of a character ngram is k max , then the position embedding of the center of a string s of length 2k max − 1 is the same independent of the context in which s occurs. It is conceivable that text representations could be context-sensitive. For example, the hidden states of a character language model have been used as a kind of nonsymbolic text representation (Chrupala, 2013; Evang et al., 2013; Chrupala, 2014) and these states are context-sensitive. However, such models will in general be a second level of representation; e.g., the hidden states of a character language model generally use character embeddings as the first level of representation. Conversely, position embeddings can also be the basis for a context-sensitive second-level text representation. We have to start somewhere when we represent text. Position embeddings are motivated by the desire to provide a representation that can be computed easily and quickly (i.e., without taking 793 context into account), but that on t"
E17-1074,N16-1155,0,0.0601788,"e a second level of representation; e.g., the hidden states of a character language model generally use character embeddings as the first level of representation. Conversely, position embeddings can also be the basis for a context-sensitive second-level text representation. We have to start somewhere when we represent text. Position embeddings are motivated by the desire to provide a representation that can be computed easily and quickly (i.e., without taking 793 context into account), but that on the other hand is much richer than the symbolic alphabet. Processing text vs. speech vs. images. Gillick et al. (2016) write: “It is worth noting that noise is often added . . . to images . . . and speech where the added noise does not fundamentally alter the input, but rather blurs it. [bytes allow us to achieve] something like blurring with text.” It is not clear to what extent blurring on the byte level is useful; e.g., if we blur the bytes of the word “university” individually, then it is unlikely that the noise generated is helpful in, say, providing good training examples in parts of the space that would otherwise be unexplored. In contrast, the text representation we have introduced in this paper can b"
E17-1074,D14-1162,0,0.0858166,"gorithms can be understood as estimating the parameters of the model from a unit-context matrix C where each row corresponds to a unit ui , each column to a context cj and each cell Cij measures the degree of association between ui and cj . For example, the skipgram model is closely related to an SVD factorization of a pointwise mutual information matrix (Levy and Goldberg, 2014). Many text representation learning algorithms are formalized as matrix factorization (e.g., (Deerwester et al., 1990; Hofmann, 1999; Stratos et al., 2015)), but there may be no big difference between implicit (e.g., (Pennington et al., 2014)) and explicit factorization methods; see also (Mohamed, 2011; Rastogi et al., 2015). Our goal in this paper is not to develop new matrix factorization methods. Instead, we will focus on defining the unit-context matrix in such a way that no symbolic assumption has to be made. This unit-context matrix can then be processed by any existing or still to be invented algorithm. Definition of units and contexts. How to define units and contexts without relying on segmentation boundaries? In initial experiments, we simply generated all character ngrams of length up to k max (where k max is a paramete"
E17-1074,N15-1058,0,0.0542096,"Missing"
E17-1074,P15-1124,0,0.0413388,"Missing"
E17-1074,D16-1157,0,0.187589,"haracters. See §4. 785 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 785–796, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (ii) methods, i.e., character-level models like fastText skipgram (Bojanowski et al., 2016). Bag-of-ngram models, group (iii) models, are text representation utilization models that typically compute the representation of a text as the sum of the embeddings of all character ngrams occurring in it, e.g., WordSpace (Sch¨utze, 1992) and CHARAGRAM (Wieting et al., 2016). WordSpace and CHARAGRAM are examples of mixed training-utilization models: training is performed on tokenized text (words and phrases), utilization is nonsymbolic. We make two contributions in this paper. (i) We propose the first generic method for training text representation models without the need for tokenization and address the challenging sparseness issues that make this difficult. (ii) We propose the first nonsymbolic utilization method that fully represents sequence information – in contrast to utilization methods like bag-of-ngrams that discard sequence information that is not direc"
E17-1074,D15-1083,1,0.878364,"Missing"
E17-1111,N16-1097,1,0.0622983,"Missing"
E17-1111,D15-1086,0,0.0129727,"ifies mentions into fine-grained types. We make a similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we do not need test sentences annotated with context dependent entity types. This task was also used to evaluate embedding learning methods (Yaghoobzadeh and Sch¨utze, 2016). Entity types for relation extraction. Several studies have integrated entity type information into relation extraction – either coarse-grained (Hoffmann et al., 2011; Zhou et al., 2005) or finegrained (Liu et al., 2014; Du et al., 2015; Augenstein et al., 2015; Vlachos and Clark, 2014; Yao et al., 2010; Ling and Weld, 2012) entity types. In contrast to most of this work, but similar to Yao et al. (2010), we do not incorporate binary entity type values, but probabilistic outputs. Thus, we allow the relation extraction system to compensate for errors of entity typing. Additionally, we compare this approach to various other possibilities, to investigate which approach performs best. Yao et al. (2010) found that joint training of entity typing and relation extraction is better than a pipeline 1184 model; we show that this result also holds for neural n"
E17-1111,D15-1103,0,0.110967,"et al., 2014). Work on entity typing or unary relations for KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Dong et al., 2015; Del Corro et al., 2015; Ren et al., 2016a; Ren et al., 2016b; Shimaoka et al., 2016). Some of this work (cf. Yogatama et al. (2015), Dong et al. (2015)) treats entity segment boundaries as given and classifies mentions into fine-grained types. We make a similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we do not need test sentences annotated with context dependent entity types. This task was also used to evaluate embedding learning methods (Yaghoobzadeh and Sch¨utze, 2016). Entity types for relation extraction. Several studies h"
E17-1111,U15-1004,0,0.0145727,"s given and classifies mentions into fine-grained types. We make a similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we do not need test sentences annotated with context dependent entity types. This task was also used to evaluate embedding learning methods (Yaghoobzadeh and Sch¨utze, 2016). Entity types for relation extraction. Several studies have integrated entity type information into relation extraction – either coarse-grained (Hoffmann et al., 2011; Zhou et al., 2005) or finegrained (Liu et al., 2014; Du et al., 2015; Augenstein et al., 2015; Vlachos and Clark, 2014; Yao et al., 2010; Ling and Weld, 2012) entity types. In contrast to most of this work, but similar to Yao et al. (2010), we do not incorporate binary entity type values, but probabilistic outputs. Thus, we allow the relation extraction system to compensate for errors of entity typing. Additionally, we compare this approach to various other possibilities, to investigate which approach performs best. Yao et al. (2010) found that joint training of entity typing and relation extraction is better than a pipeline 1184 model; we show that this resul"
E17-1111,P05-1045,0,0.0836387,"e completion (KBC). Most KBC systems focus on identifying triples R(e1 , r, e2 ) missing from a KB (Nickel et al., 2012; Bordes et al., 2013; Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Work on entity typing or unary relations for KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Dong et al., 2015; Del Corro et al., 2015; Ren et al., 2016a; Ren et al., 2016b; Shimaoka et al., 2016). Some of this work (cf. Yogatama et al. (2015), Dong et al. (2015)) treats entity segment boundaries as given and classifies mentions into fine-grained types. We make a similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we"
E17-1111,P11-1055,0,0.145634,"ply MIML to finegrained entity typing for the first time and show that it outperforms the state-of-the-art supervised method based on entity embeddings. (iii) We show that a novel way of integrating noisy entity type predictions into a relation extraction model and joint training of the two tasks lead to large improvements of RE performance. We release code and data for future research.1 2 Related Work Noise mitigation for distant supervision. Distant supervision can be used to train information extraction systems, e.g., in relation extraction (e.g., Mintz et al. (2009), Riedel et al. (2010), Hoffmann et al. (2011), Zeng et al. (2015)) and entity typing (e.g., Ling and Weld (2012), Yogatama et al. (2015), Dong et al. (2015)). To mitigate the noisy label problem, multi-instance (MI) learning has been introduced and applied in relation extraction (Riedel et al., 2010; Ritter et al., 2013). Surdeanu et al. (2012) introduced multi-instance multi-label (MIML) learning to extend MI learning for multilabel relation extraction. Those models are based on manually designed features. Zeng et al. (2015) and Lin et al. (2016) introduced MI learning methods for neural networks. We introduce MIML algorithms for neural"
E17-1111,P14-1062,0,0.00384887,"(r|c) Wout P(t|ce1) Wout Wout → c → c Wh Wh Ф(c) Ф(c) concat concat pooling conv left e1 → c P(t|ce2) pooling conv conv left e2 t2 Wt P(t|ce1) pooling right Wh t1 Wt P(t|ce2) concat pooling pooling conv conv right left entity typing Ф(c) pooling conv e1 middle pooling conv e2 right relation extraction sentence Figure 1: Our architecture for joint entity typing and relation extraction and right of the relation arguments. The parts “overlap”, i.e., the left (resp. right) argument is included in both left (resp. right) and middle parts. For each of the three parts, convolution and 3-max pooling (Kalchbrenner et al., 2014) is performed. ~ The context representation φ(c) ∈ R3·3·n is the concatenation of the pooling results. 4.1.1 Integration of Entity Types We concatenate the entity type representations t~1 ∈ Rτ and t~2 ∈ Rτ of the relation arguments ~ to the CNN representation of the context, φ(c): ~ 0 = [φ(c) ~ φ(c) : t~1 : t~2 ] Our context representation ~c is then:   ~ 0 ~c = tanh Wh φ(c) (13) (14) where Wh ∈ Rh×(3·3·n+2τ ) is the weight matrix. This is also depicted in Figure 1, right column, ~ third layer from the top: t~1 , t~2 , Φ(c). We calculate ~ ~ 1 2 t and t from the predictions of the entity typ"
E17-1111,P16-1200,0,0.449175,"It relaxes the distant supervision assumption to the assumption that at least one instance of a bag (collection of all sentences containing the given entity/entity pair) expresses the type/relationship given in the KB. Multi-instance multi-label (MIML) learning is a generalization of MI in which one bag can have several labels (Surdeanu et al., 2012). Most MI and MIML methods are based on hand crafted features. Recently, Zeng et al. (2015) introduced an end-to-end approach to MI learning based on neural networks. Their MI method takes the most confident instance as the prediction of the bag. Lin et al. (2016) further improved that method by taking other instances into account as well; they proposed MI learning based on selective attention as an alternative way of relaxing the impact of noisy labels on RE. In selective attention, a weighted average of instance representations is calculated first and then used to compute the prediction of a bag. In this paper, we introduce two multi-label versions of MI. (i) MIML-MAX takes the maximum instance for each label. (ii) MIML-ATT applies, for each label, selective attention to the instances. We apply MIML-MAX and MIML-ATT to finegrained ET. In contrast to"
E17-1111,C14-1199,0,0.0626479,"Missing"
E17-1111,P09-1113,0,0.147453,"ms best. 1 Introduction Knowledge bases (KBs) are important resources for natural language processing tasks like question answering and entity linking. However, KBs are far from complete (e.g., Socher et al. (2013)). Therefore, methods for automatic knowledge base completion (KBC) are beneficial. Two subtasks of KBC are entity typing (ET) and relation extraction (RE). We address both tasks in this paper. As in other information extraction tasks, obtaining labeled training data for ET and RE is challenging. The challenge grows as labels become more fine-grained. Therefore, distant supervision (Mintz et al., 2009) is widely used. It reduces the need for manually created resources. Distant supervision assumes that if an entity has a type (resp. two entities have a relationship) in a KB, then all sentences mentioning that entity (resp. those two entities) express that type (resp. that relationship). However, that assumption is too strong and gives rise to many noisy labels. Different techniques to deal with that problem have been investigated. The main technique is multi-instance (MI) learning (Riedel et al., 2010). It relaxes the distant supervision assumption to the assumption that at least one instanc"
E17-1111,N15-1054,0,0.0354999,"e challenging because of dif1 cistern.cis.lmu.de ferent dependencies and the multi-label nature of the problem. Also, there seems to be a difference between how entity relations and entity types are expressed in text. Our experiments support that hypothesis. Knowledge base completion (KBC). Most KBC systems focus on identifying triples R(e1 , r, e2 ) missing from a KB (Nickel et al., 2012; Bordes et al., 2013; Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Work on entity typing or unary relations for KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Dong et al., 2015; Del Corro et al., 2015; Ren et al., 2016a; Ren et al., 2016b; Shimaoka et al., 2016). Some of this work (cf. Yogatama et al. (2015"
E17-1111,D12-1042,0,0.154947,"Missing"
E17-1111,D16-1144,0,0.00846975,"on entity typing or unary relations for KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Dong et al., 2015; Del Corro et al., 2015; Ren et al., 2016a; Ren et al., 2016b; Shimaoka et al., 2016). Some of this work (cf. Yogatama et al. (2015), Dong et al. (2015)) treats entity segment boundaries as given and classifies mentions into fine-grained types. We make a similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we do not need test sentences annotated with context dependent entity types. This task was also used to evaluate embedding learning methods (Yaghoobzadeh and Sch¨utze, 2016). Entity types for relation extraction. Several studies have integrated ent"
E17-1111,N13-1008,0,0.0148558,"ter dataset and also to the most widely used datasets for evaluating MI (cf. Riedel et al. (2010)). This makes our setup more challenging because of dif1 cistern.cis.lmu.de ferent dependencies and the multi-label nature of the problem. Also, there seems to be a difference between how entity relations and entity types are expressed in text. Our experiments support that hypothesis. Knowledge base completion (KBC). Most KBC systems focus on identifying triples R(e1 , r, e2 ) missing from a KB (Nickel et al., 2012; Bordes et al., 2013; Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Work on entity typing or unary relations for KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Dong et al., 2015"
E17-1111,Q13-1030,0,0.0219792,"g of the two tasks lead to large improvements of RE performance. We release code and data for future research.1 2 Related Work Noise mitigation for distant supervision. Distant supervision can be used to train information extraction systems, e.g., in relation extraction (e.g., Mintz et al. (2009), Riedel et al. (2010), Hoffmann et al. (2011), Zeng et al. (2015)) and entity typing (e.g., Ling and Weld (2012), Yogatama et al. (2015), Dong et al. (2015)). To mitigate the noisy label problem, multi-instance (MI) learning has been introduced and applied in relation extraction (Riedel et al., 2010; Ritter et al., 2013). Surdeanu et al. (2012) introduced multi-instance multi-label (MIML) learning to extend MI learning for multilabel relation extraction. Those models are based on manually designed features. Zeng et al. (2015) and Lin et al. (2016) introduced MI learning methods for neural networks. We introduce MIML algorithms for neural networks. In contrast to most MI/MIML methods, which are applied in relation extraction, we apply MIML to the task of finegrained entity typing. Ritter et al. (2013) applied MI on a Twitter dataset with ten types. Our dataset has a larger number of classes or types (namely 10"
E17-1111,W16-1313,0,0.025816,"or KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Dong et al., 2015; Del Corro et al., 2015; Ren et al., 2016a; Ren et al., 2016b; Shimaoka et al., 2016). Some of this work (cf. Yogatama et al. (2015), Dong et al. (2015)) treats entity segment boundaries as given and classifies mentions into fine-grained types. We make a similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we do not need test sentences annotated with context dependent entity types. This task was also used to evaluate embedding learning methods (Yaghoobzadeh and Sch¨utze, 2016). Entity types for relation extraction. Several studies have integrated entity type information into relation extractio"
E17-1111,W14-4501,0,0.0339205,"Missing"
E17-1111,D14-1167,0,0.00472646,"to the most widely used datasets for evaluating MI (cf. Riedel et al. (2010)). This makes our setup more challenging because of dif1 cistern.cis.lmu.de ferent dependencies and the multi-label nature of the problem. Also, there seems to be a difference between how entity relations and entity types are expressed in text. Our experiments support that hypothesis. Knowledge base completion (KBC). Most KBC systems focus on identifying triples R(e1 , r, e2 ) missing from a KB (Nickel et al., 2012; Bordes et al., 2013; Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Work on entity typing or unary relations for KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012; Ling and Weld, 2012; Yogatama et al., 2015; Dong et al., 2015; Del Corro et al.,"
E17-1111,D13-1136,0,0.00677073,"r types (namely 102) and input examples, compared to that Twitter dataset and also to the most widely used datasets for evaluating MI (cf. Riedel et al. (2010)). This makes our setup more challenging because of dif1 cistern.cis.lmu.de ferent dependencies and the multi-label nature of the problem. Also, there seems to be a difference between how entity relations and entity types are expressed in text. Our experiments support that hypothesis. Knowledge base completion (KBC). Most KBC systems focus on identifying triples R(e1 , r, e2 ) missing from a KB (Nickel et al., 2012; Bordes et al., 2013; Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Work on entity typing or unary relations for KBC is more recent (Yao et al., 2013; Neelakantan and Chang, 2015; Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh et al., 2017). In this paper, we build a KBC system for unary and binary relations using contextual information of words and entities. Named entity recognition (NER) and typing. NER systems (e.g., Finkel et al. (2005), Collobert et al. (2011)) used to consider only a small set of entity types. Recent work also addresses finegrained NER (Yosef et al., 2012;"
E17-1111,D15-1083,1,0.0626889,"Missing"
E17-1111,P16-1023,1,0.893465,"Missing"
E17-1111,E17-1055,1,0.900677,"Missing"
E17-1111,D10-1099,0,0.00772677,"similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we do not need test sentences annotated with context dependent entity types. This task was also used to evaluate embedding learning methods (Yaghoobzadeh and Sch¨utze, 2016). Entity types for relation extraction. Several studies have integrated entity type information into relation extraction – either coarse-grained (Hoffmann et al., 2011; Zhou et al., 2005) or finegrained (Liu et al., 2014; Du et al., 2015; Augenstein et al., 2015; Vlachos and Clark, 2014; Yao et al., 2010; Ling and Weld, 2012) entity types. In contrast to most of this work, but similar to Yao et al. (2010), we do not incorporate binary entity type values, but probabilistic outputs. Thus, we allow the relation extraction system to compensate for errors of entity typing. Additionally, we compare this approach to various other possibilities, to investigate which approach performs best. Yao et al. (2010) found that joint training of entity typing and relation extraction is better than a pipeline 1184 model; we show that this result also holds for neural network models and when the number of entity"
E17-1111,P15-2048,0,0.201742,"tate-of-the-art supervised method based on entity embeddings. (iii) We show that a novel way of integrating noisy entity type predictions into a relation extraction model and joint training of the two tasks lead to large improvements of RE performance. We release code and data for future research.1 2 Related Work Noise mitigation for distant supervision. Distant supervision can be used to train information extraction systems, e.g., in relation extraction (e.g., Mintz et al. (2009), Riedel et al. (2010), Hoffmann et al. (2011), Zeng et al. (2015)) and entity typing (e.g., Ling and Weld (2012), Yogatama et al. (2015), Dong et al. (2015)). To mitigate the noisy label problem, multi-instance (MI) learning has been introduced and applied in relation extraction (Riedel et al., 2010; Ritter et al., 2013). Surdeanu et al. (2012) introduced multi-instance multi-label (MIML) learning to extend MI learning for multilabel relation extraction. Those models are based on manually designed features. Zeng et al. (2015) and Lin et al. (2016) introduced MI learning methods for neural networks. We introduce MIML algorithms for neural networks. In contrast to most MI/MIML methods, which are applied in relation extraction, w"
E17-1111,C12-2133,0,0.0679351,"Missing"
E17-1111,D15-1203,0,0.661321,"es rise to many noisy labels. Different techniques to deal with that problem have been investigated. The main technique is multi-instance (MI) learning (Riedel et al., 2010). It relaxes the distant supervision assumption to the assumption that at least one instance of a bag (collection of all sentences containing the given entity/entity pair) expresses the type/relationship given in the KB. Multi-instance multi-label (MIML) learning is a generalization of MI in which one bag can have several labels (Surdeanu et al., 2012). Most MI and MIML methods are based on hand crafted features. Recently, Zeng et al. (2015) introduced an end-to-end approach to MI learning based on neural networks. Their MI method takes the most confident instance as the prediction of the bag. Lin et al. (2016) further improved that method by taking other instances into account as well; they proposed MI learning based on selective attention as an alternative way of relaxing the impact of noisy labels on RE. In selective attention, a weighted average of instance representations is calculated first and then used to compute the prediction of a bag. In this paper, we introduce two multi-label versions of MI. (i) MIML-MAX takes the ma"
E17-1111,P05-1053,0,0.0225032,"ong et al. (2015)) treats entity segment boundaries as given and classifies mentions into fine-grained types. We make a similar assumption, but in contrast to NER, we evaluate on the corpus-level entity typing task of Yaghoobzadeh and Sch¨utze (2015); thus, we do not need test sentences annotated with context dependent entity types. This task was also used to evaluate embedding learning methods (Yaghoobzadeh and Sch¨utze, 2016). Entity types for relation extraction. Several studies have integrated entity type information into relation extraction – either coarse-grained (Hoffmann et al., 2011; Zhou et al., 2005) or finegrained (Liu et al., 2014; Du et al., 2015; Augenstein et al., 2015; Vlachos and Clark, 2014; Yao et al., 2010; Ling and Weld, 2012) entity types. In contrast to most of this work, but similar to Yao et al. (2010), we do not incorporate binary entity type values, but probabilistic outputs. Thus, we allow the relation extraction system to compensate for errors of entity typing. Additionally, we compare this approach to various other possibilities, to investigate which approach performs best. Yao et al. (2010) found that joint training of entity typing and relation extraction is better t"
E17-2119,M98-1028,0,0.0912131,"lected president” and “Obama graduated from Harvard in 1991”, fine-grained entity mention classification should predict “politician” for the first and “lawyer” for the second. In contrast, given a corpus containing these two sentences, fine-grained entity typing should predict the types {“politician”, “lawyer”} for “Obama”. Related work Named entity recognition (NER) is the joint problem of entity mention segmentation and entity mention classification (Finkel et al., 2005; McCallum and Li, 2003). Most work on NER uses a small set of coarse-grained labels like person and location, e.g., MUC-7 (Chinchor and Robinson, 1998). Most work on the fine-grained FIGER (Ling and Weld, 2012) and HYENA (Yosef et al., 2012) taxonomies has cast NER as a two-step process (Elsner et al., 2009; Ritter et al., 2011; Collins and Singer, 1999) of entity mention segmentation followed by entity mention classification. The reason for two-step is the high complexity of joint models for fine-grained entity recognition. A joint model like CRF (Lafferty et al., 2001) has a state space corresponding to segmentation type times semantic types. Introducing a larger class set into A common approach for solving hierarchical problems has been f"
E17-2119,D14-1179,0,0.0224187,"Missing"
E17-2119,W99-0613,0,0.391166,"aining these two sentences, fine-grained entity typing should predict the types {“politician”, “lawyer”} for “Obama”. Related work Named entity recognition (NER) is the joint problem of entity mention segmentation and entity mention classification (Finkel et al., 2005; McCallum and Li, 2003). Most work on NER uses a small set of coarse-grained labels like person and location, e.g., MUC-7 (Chinchor and Robinson, 1998). Most work on the fine-grained FIGER (Ling and Weld, 2012) and HYENA (Yosef et al., 2012) taxonomies has cast NER as a two-step process (Elsner et al., 2009; Ritter et al., 2011; Collins and Singer, 1999) of entity mention segmentation followed by entity mention classification. The reason for two-step is the high complexity of joint models for fine-grained entity recognition. A joint model like CRF (Lafferty et al., 2001) has a state space corresponding to segmentation type times semantic types. Introducing a larger class set into A common approach for solving hierarchical problems has been flat classification, i.e., not making direct use of the hierarchy. But exploiting the hierarchical organization of the classes reduces complexity, makes better use of training data in learning and enhances"
E17-2119,N09-1019,0,0.0308058,"e second. In contrast, given a corpus containing these two sentences, fine-grained entity typing should predict the types {“politician”, “lawyer”} for “Obama”. Related work Named entity recognition (NER) is the joint problem of entity mention segmentation and entity mention classification (Finkel et al., 2005; McCallum and Li, 2003). Most work on NER uses a small set of coarse-grained labels like person and location, e.g., MUC-7 (Chinchor and Robinson, 1998). Most work on the fine-grained FIGER (Ling and Weld, 2012) and HYENA (Yosef et al., 2012) taxonomies has cast NER as a two-step process (Elsner et al., 2009; Ritter et al., 2011; Collins and Singer, 1999) of entity mention segmentation followed by entity mention classification. The reason for two-step is the high complexity of joint models for fine-grained entity recognition. A joint model like CRF (Lafferty et al., 2001) has a state space corresponding to segmentation type times semantic types. Introducing a larger class set into A common approach for solving hierarchical problems has been flat classification, i.e., not making direct use of the hierarchy. But exploiting the hierarchical organization of the classes reduces complexity, makes bette"
E17-2119,D12-1082,0,0.0310343,"– .723 .545 .748 .716 .586±.016 .793±.005 .742±.005 .543 .695 .681 .589 .763 .749 FIGER, L&W Yogatama et al. Shimaoka et al. PthDCode HYENA, Ren et al. FIGER, Ren et al. joint models already increases the complexity of learning drastically, furthermore the multilabel nature of fine-grained entity mention classification explodes the state space of the exponential model further (Ling and Weld, 2012). Utilizing fine-grained entity information enhances the performance for tasks like named entity disambiguation (Yosef et al., 2012), relation extraction (Ling and Weld, 2012) and question answering (Lin et al., 2012; Lee et al., 2006). A major challenge with fine grained entity mention classification is the scarcity of human annotated datasets. Currently, most of the datasets are collected through distant supervision, utilizing Wikipedia texts with anchor links to obtain entity mentions and using knowledge bases like Freebase and YAGO to obtain candidate types for the mention. This introduces noise and complexities like unrelated labels, redundant labels and large sizes of candidate label sets. To address these challenges, Ling and Weld (2012) mapped Freebase types to their own tag set with 113 types, Yo"
E17-2119,P05-1045,0,0.0236656,"f predicting the complete set of types of the entity that a mention refers to (Yaghoobzadeh and Sch¨utze, 2017). For the sentences “Obama was elected president” and “Obama graduated from Harvard in 1991”, fine-grained entity mention classification should predict “politician” for the first and “lawyer” for the second. In contrast, given a corpus containing these two sentences, fine-grained entity typing should predict the types {“politician”, “lawyer”} for “Obama”. Related work Named entity recognition (NER) is the joint problem of entity mention segmentation and entity mention classification (Finkel et al., 2005; McCallum and Li, 2003). Most work on NER uses a small set of coarse-grained labels like person and location, e.g., MUC-7 (Chinchor and Robinson, 1998). Most work on the fine-grained FIGER (Ling and Weld, 2012) and HYENA (Yosef et al., 2012) taxonomies has cast NER as a two-step process (Elsner et al., 2009; Ritter et al., 2011; Collins and Singer, 1999) of entity mention segmentation followed by entity mention classification. The reason for two-step is the high complexity of joint models for fine-grained entity recognition. A joint model like CRF (Lafferty et al., 2001) has a state space cor"
E17-2119,W03-0430,0,0.0726252,"lete set of types of the entity that a mention refers to (Yaghoobzadeh and Sch¨utze, 2017). For the sentences “Obama was elected president” and “Obama graduated from Harvard in 1991”, fine-grained entity mention classification should predict “politician” for the first and “lawyer” for the second. In contrast, given a corpus containing these two sentences, fine-grained entity typing should predict the types {“politician”, “lawyer”} for “Obama”. Related work Named entity recognition (NER) is the joint problem of entity mention segmentation and entity mention classification (Finkel et al., 2005; McCallum and Li, 2003). Most work on NER uses a small set of coarse-grained labels like person and location, e.g., MUC-7 (Chinchor and Robinson, 1998). Most work on the fine-grained FIGER (Ling and Weld, 2012) and HYENA (Yosef et al., 2012) taxonomies has cast NER as a two-step process (Elsner et al., 2009; Ritter et al., 2011; Collins and Singer, 1999) of entity mention segmentation followed by entity mention classification. The reason for two-step is the high complexity of joint models for fine-grained entity recognition. A joint model like CRF (Lafferty et al., 2001) has a state space corresponding to segmentati"
E17-2119,P16-2090,1,0.885701,"Missing"
E17-2119,D13-1032,1,0.821344,"Missing"
E17-2119,D14-1162,0,0.0860587,"g curve Evaluation. Like prior work, we use three F1 metrics, strict, loose macro and loose micro, that differ in the definition of precision P and recall R. Let n be the number of mentions, Ti the true set of tags of mention i and Yi P the predicted set. Then, we define P =P R = 1/n ni=1 δ(Yi = Ti ) for strict; P P = 1/n ni=1 (|Yi ∩ Ti |)/(|Yi |) and n R = 1/n P i |) for loose macro; i=1 (|Yi ∩ Ti |)/(|T P n n |Y ∩ T |)/( and P = ( i i i=1 |Yi |) and R = i=1 P Pn n ( i=1 |Yi ∩ Ti |)/( i=1 |Ti |)) for loose micro. Parameter Settings. We use pre-trained word embeddings of size 300 provided by (Pennington et al., 2014). OOV vectors are randomly initialized. Similar to (Shimaoka et al., 2017), all hidden states h of the encoder-decoder were set to 100 dimension and mention lengths m to 5. Window size is w = 15. We bracket left and right contexts with special start and end symbols. For short left / right contexts, we bracket with additional different start / end symbols that are masked out for calculation of loss and attention weights. Another special symbol EOL (End Of Label) is appended to short paths, so that all hierarchical paths have the same length. We use ADAM (Kingma and Ba, 2014) with learning rate"
E17-2119,E17-1055,1,0.812583,"Missing"
E17-2119,P15-2048,0,0.354698,"ighted sum ci and the current state si−1 are used to predict the distribution yi over entity classes (non-dashed *-nodes in Figure 1): yi = g (si−1 , ci ) (3) Dataset. We use the Wiki dataset (Ling and Weld, 2012) published by Ren et al. (2016).1 It consists of 2.69 million mentions obtained from 1.5 million sentences sampled from Wikipedia articles. These mentions are tagged with 113 types with a maximum of two levels of hierarchy. Ling and Weld (2012) also created a test set of 434 sentences that contain 562 gold entity mentions. Similar to prior work (Ling and Weld, 2012; Ren et al., 2016; Yogatama et al., 2015; Shimaoka et al., 2017), we randomly sample a training set of 2 million and a disjoint dev set of size 500. where g is a feedforward network with elementwise sigmoid. Finally, PthDCode uses prediction 1 https://drive.google.com/file/d/ 0B2ke42d0kYFfVC1fazdKYnVhYWs 753 and report the performances of the models that are stable in all form of metrics on dev set. The reason for evaluating on range of models is nature of collection of dev and test data. We use cv = σ/µ, the coefficient of variation (Brown, 1998), to select and combine models in application. After an initial training stage, we comp"
E17-2119,D11-1141,0,0.0607908,", given a corpus containing these two sentences, fine-grained entity typing should predict the types {“politician”, “lawyer”} for “Obama”. Related work Named entity recognition (NER) is the joint problem of entity mention segmentation and entity mention classification (Finkel et al., 2005; McCallum and Li, 2003). Most work on NER uses a small set of coarse-grained labels like person and location, e.g., MUC-7 (Chinchor and Robinson, 1998). Most work on the fine-grained FIGER (Ling and Weld, 2012) and HYENA (Yosef et al., 2012) taxonomies has cast NER as a two-step process (Elsner et al., 2009; Ritter et al., 2011; Collins and Singer, 1999) of entity mention segmentation followed by entity mention classification. The reason for two-step is the high complexity of joint models for fine-grained entity recognition. A joint model like CRF (Lafferty et al., 2001) has a state space corresponding to segmentation type times semantic types. Introducing a larger class set into A common approach for solving hierarchical problems has been flat classification, i.e., not making direct use of the hierarchy. But exploiting the hierarchical organization of the classes reduces complexity, makes better use of training dat"
E17-2119,W16-1313,0,0.168698,"hy, e.g., in topic classification. In this paper, we address fine-grained entity mention classification, another problem with a hierarchical class structure. In this task, each mention can have several fine-grained types, e.g., Obama is both a politician and an author in a context in which his election is related to his prior success as a best-selling author; thus, the problem is multilabel at the lowest level of the hierarchy. Two standard approaches to hierarchical classification are flat and local classification. In flat classification (e.g., FIGER (Ling and Weld, 2012), Attentive Encoder (Shimaoka et al., 2016; Shimaoka et al., 2017)), the task is formalized as a flat multiclass multilabel problem. In local classification (Gillick et al., 2014; Yosef et al., 2012; Yogatama 2 Model Figure 1 displays our model PthDCode. We use lowercase italics for variables, uppercase italics for sequences, lowercase bold for vectors and uppercase bold for matrices. Sentence S = hx1 , . . . , x|S |i is a sequence of words, represented as embeddings xi , each of dimension d. The classes of an entity are represented as y, a vector of l binary indicators, each indicating whether the corresponding class is correct. Hidd"
E17-2119,C12-2133,0,0.153616,"Missing"
E17-2119,E17-1119,0,0.559772,"sification. In this paper, we address fine-grained entity mention classification, another problem with a hierarchical class structure. In this task, each mention can have several fine-grained types, e.g., Obama is both a politician and an author in a context in which his election is related to his prior success as a best-selling author; thus, the problem is multilabel at the lowest level of the hierarchy. Two standard approaches to hierarchical classification are flat and local classification. In flat classification (e.g., FIGER (Ling and Weld, 2012), Attentive Encoder (Shimaoka et al., 2016; Shimaoka et al., 2017)), the task is formalized as a flat multiclass multilabel problem. In local classification (Gillick et al., 2014; Yosef et al., 2012; Yogatama 2 Model Figure 1 displays our model PthDCode. We use lowercase italics for variables, uppercase italics for sequences, lowercase bold for vectors and uppercase bold for matrices. Sentence S = hx1 , . . . , x|S |i is a sequence of words, represented as embeddings xi , each of dimension d. The classes of an entity are represented as y, a vector of l binary indicators, each indicating whether the corresponding class is correct. Hidden states of forward and"
E95-1020,J93-3004,0,0.00809212,"2a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (""certain"" and ""right"") with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematicJ How should a word like ""plant"" be categorized if it has uses both as a verb and as a noun? How can a categorization be considered meaningful if the infinitive marker ""to"" is not distinguished from the homophonous preposition? In a previous paper (Schfitze, 1993), we trained a neural network to disambiguate"
E95-1020,H92-1030,0,0.154991,"ections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal rep"
E95-1020,H90-1055,0,0.0271086,"ruct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (""certain"" and ""right"") with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the wides"
E95-1020,P93-1035,0,0.0511321,"ext, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical c"
E95-1020,P90-1031,0,0.0305051,"ing techniques that can analyze text without expensive and time-consuming adaptation to new domains and genres. This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words. In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Chur"
E95-1020,P93-1034,1,0.53106,"n the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (""certain"" and ""right"") with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematicJ How should a word like ""plant"" be categorized if it has uses both as a verb and as a noun? How can a categorization be considered meaningful if the infinitive marker ""to"" is not distinguished from the homophonous preposition? In a previous paper (Schfitze, 1993), we trained a neural network to disambiguate part-of-speech *Although Biber (1993) classifies collocations, these can also be ambiguous. For example, ""for certain"" has both senses of ""certain"": ""particular"" and ""sure"". 141 word onto onto seemed seemed side left right left right nearest n e i g h b o r s into toward away off together against beside around down reduce among regarding against towards plus toward using unlike appeared might would remained had became could must should seem seems wanted want going meant tried expect likely Table h Words with most similar left and right neighbors fo"
E95-1020,A92-1018,0,\N,Missing
E95-1020,H93-1047,0,\N,Missing
I13-1023,J93-2004,0,0.0437894,"Missing"
I13-1023,J94-2001,0,0.425739,"Missing"
I13-1023,W06-1615,0,0.89101,"hich it occurs. For syntactic categorization, the immediate left and right neighbors of a word are the most informative aspect of context. Based on this reasoning, we create a feature representation for each word that has three components: left context information, right context information and shape information. We will refer to left/right context information as distributional information. Let f be the function that maps a word w to its (full) feature vector. We then define f as follows:   f left (w) f (w) = f right (w)  f shape (w) al., 1993) of Wall Street Journal (WSJ) text. Following Blitzer et al. (2006), we use sections 2-21 for training. We also use 100,000 WSJ sentences from 1988 as unlabeled data in training. We evaluate on six different TDs. The first TD is the Penn BioTreebank data set distributed by Blitzer. It consists of development and test sets of 500 sentences each and an unlabeled set of 100,000 sentences of BIO text. The remaining five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). We will use WEB to refer to these five TDs collectively. Each WEB TD has an unlabeled training set of 100,000 sentences and development"
I13-1023,D07-1118,0,0.405635,"ons corresponding to the d = 100 largest singular values. We compute distributional vectors either on target data only (i.e., on TD-R) or on the union of source and target data (i.e., SD+TD-R). We compare these two alternatives and show in our experiments that SD distributional information does not consistently improve performance. Shape features. Suffixes are likely to be helpful because regular processes of inflectional and derivational morphology do not change in English when going from one domain to the next. Many POS taggers incorporate information from suffixes to build robust features (Miller et al., 2007). For a selected suffix s, we simply set the dimension corresponding to s in f shape (w) to 1 if w ends in s and to 0 otherwise. We either select all suffixes or the top 100, depending on the experiment. In addition to suffixes, we investigate two other representational variables related to shape: case and digits. For case, we compare keeping case information as is with converting all uppercase characters to lowercase characters. For digits, we compare keeping digits as is with converting all digits to the digit 0; e.g., $1,643 is converted to $0,000). We call these two transformations case no"
I13-1023,N12-1043,0,0.0162864,"re trained until Based on the intuition that each of the three sources of information is equally important, each of the three component vectors is normalized to unit length. For both distributional and shape features, we have a choice of either using all possible features or a subset consisting of the most frequent features. We directly compare these two possibilities, using recommended values from the literature for the subset condition: the 250 most frequent features (indicator words) for distributional vectors (Schütze, 1995) and the 100 most frequent features (suffixes) for shape vectors (Müller et al., 2012). Each component vector has an additional binary feature that is set to 1 if the rest of the vector is zero, and 0 otherwise to avoid numerical issues with zero vectors. Distributional features. The ith entry xi of f left (w) is the number of times that the indicator word ti occurs immediately to the left of w: xi = freq (bigram(ti , w)) where ti is the word with frequency rank i in the corpus. f right (w) is defined analogously. Many different ways of defining and transforming distributional features have been proposed in the literature. We systematically investigate the following variables:"
I13-1023,P12-2071,0,0.194823,"Missing"
I13-1023,I11-1086,0,0.0365175,"Missing"
I13-1023,P00-1035,0,0.0802963,"Missing"
I13-1023,E95-1020,1,0.736355,"RFs, including features for state, emission and transition probabilities. CRFs are trained until Based on the intuition that each of the three sources of information is equally important, each of the three component vectors is normalized to unit length. For both distributional and shape features, we have a choice of either using all possible features or a subset consisting of the most frequent features. We directly compare these two possibilities, using recommended values from the literature for the subset condition: the 250 most frequent features (indicator words) for distributional vectors (Schütze, 1995) and the 100 most frequent features (suffixes) for shape vectors (Müller et al., 2012). Each component vector has an additional binary feature that is set to 1 if the rest of the vector is zero, and 0 otherwise to avoid numerical issues with zero vectors. Distributional features. The ith entry xi of f left (w) is the number of times that the indicator word ti occurs immediately to the left of w: xi = freq (bigram(ti , w)) where ti is the word with frequency rank i in the corpus. f right (w) is defined analogously. Many different ways of defining and transforming distributional features have be"
I13-1023,P11-1061,0,0.0575941,"Missing"
I13-1023,P05-1044,0,0.0324488,"Missing"
I13-1023,P07-1033,0,0.504835,"Missing"
I13-1023,D10-1017,0,0.0964723,"Missing"
I13-1023,N03-1033,0,0.0293613,"Missing"
I13-1023,P12-2047,0,0.103061,"Missing"
I13-1023,W10-2605,0,0.121971,"Missing"
I13-1023,P07-1094,0,0.077426,"Missing"
I13-1023,zhang-kordoni-2006-automated,0,0.0322576,"Missing"
I13-1023,P09-1056,0,0.687409,"ge number of experiments efficiently. Our experiments demonstrate that word classification accuracies are comparable with or higher than sequence labeling in POS DA for unknown words (cf. Table 2). We use  LIBSVM (Chang and Lin, 2011) to k train 2 one-vs-one classifiers on the training set, where k is the number of POS tags in the latter. The SVMs were trained with untuned default parameters; in particular, C = 1. For sequence classification, we use CRFSuite (Okazaki, 2007), a Conditional Random Field (CRF) toolkit. Apart from the word features described below, we use the base feature set of Huang and Yates (2009) for CRFs, including features for state, emission and transition probabilities. CRFs are trained until Based on the intuition that each of the three sources of information is equally important, each of the three component vectors is normalized to unit length. For both distributional and shape features, we have a choice of either using all possible features or a subset consisting of the most frequent features. We directly compare these two possibilities, using recommended values from the literature for the subset condition: the 250 most frequent features (indicator words) for distributional vec"
I13-1023,W10-2604,0,0.101983,"Missing"
I13-1023,D12-1120,0,0.365173,"Missing"
I13-1023,J96-2001,0,\N,Missing
I13-1023,J95-2001,0,\N,Missing
I13-1023,J88-1003,0,\N,Missing
I13-1023,E95-1022,0,\N,Missing
I13-1023,C08-1098,0,\N,Missing
I13-1023,W07-1516,0,\N,Missing
I13-1023,P95-1026,0,\N,Missing
I13-1023,C10-2146,0,\N,Missing
I13-1023,P07-1034,0,\N,Missing
I13-1023,P11-2009,0,\N,Missing
I13-1023,P06-1043,0,\N,Missing
I13-1023,R11-1006,0,\N,Missing
I13-1023,gimenez-marquez-2004-svmtool,0,\N,Missing
I13-1023,N09-2054,0,\N,Missing
I13-1023,I05-3005,0,\N,Missing
I13-1023,W10-2608,0,\N,Missing
I13-1104,C10-1011,0,0.015182,"that specifies the set of candidate terms for all languages) – in our case, we choose German since it expresses term boundaries very directly in its linguistic forms (capitalized nouns, single word compounding). German terms are defined as a capitalized token with at least 4 letters. For each unordered language pair {Lterm ,Li }, we define each term in Li as a sequence of tokens that are aligned to a term in Lterm . In Figure 3, “liquid phase hydrogenation” is defined as term since it is aligned to “Fl¨ussigphasenhydrierung” . For reducing errors due to poor word alignment4 , we apply M ATE (Bohnet, 2010) part of speech (PoS) tagger on phrases in languages other PROCESS : A method or event that results in a change of state (e.g., stretching, molding process, redundancy control, . . . ). TECHNICAL QUALITY : A basic or essential attribute which is measurable or shared by all members of a group (e.g., power consumption, piston speed, light reflection index, . . . ). The sources for the English seed sets have been WordNet lexicographer classes (Ciaramita and Johnson, 2003) and Wikipedia6 word lists. For each semantic class and language we induced lexicons of 2000 terms. For each lexicon we evaluat"
I13-1104,C10-2010,0,0.0309838,"pendent we restrict our demonstration of its potential to two languages: German and English. The parallel corpus. We use patent data distributed by the European Patent Office (EPO3 ) between 1998 and 2008. Most European patents provide their claims (the part of a patent defining the scope of protection) in German, English and French. We constructed a German-English parallel corpus out of 177,317 patent documents. Creation of Moses phrase table. For each unordered language pair, we create a M OSES (Koehn et al., 2007) phrase table in several steps. We first apply sentence alignment (G ARGANTUA Braune and Fraser (2010)), then word alignment (MG IZA ++ Gao and Vogel (2008)) to the data. And finally, we apply the statistical machine translation tool M OSES to the parallel word-aligned data. The resulting data structure is a phrase table of word-aligned phrases in two languages as shown in Figure 3, where the third line indicates the word alignment. English (JJ|VBG|NN)* NN (IN NN+)? Figure 4: PoS pattern for term filtering Patterns are extracted from the phrase tables as well. For each phrase in Lterm and Li we use the remaining tokens surrounding each term as bootstrapping pattern associated with this term (e"
I13-1104,W03-1022,0,0.0270711,"ion” is defined as term since it is aligned to “Fl¨ussigphasenhydrierung” . For reducing errors due to poor word alignment4 , we apply M ATE (Bohnet, 2010) part of speech (PoS) tagger on phrases in languages other PROCESS : A method or event that results in a change of state (e.g., stretching, molding process, redundancy control, . . . ). TECHNICAL QUALITY : A basic or essential attribute which is measurable or shared by all members of a group (e.g., power consumption, piston speed, light reflection index, . . . ). The sources for the English seed sets have been WordNet lexicographer classes (Ciaramita and Johnson, 2003) and Wikipedia6 word lists. For each semantic class and language we induced lexicons of 2000 terms. For each lexicon we evaluated a sample of 200 terms. Two annotators first rated 50 terms for each language and class as TRUE or FALSE. Then they discussed disagreements. Afterwards, they rated the remaining terms in each lexicon sample. We achieved a total inter-annotator agreement of κ = .701 (Cohen’s 3 www.epo.org Since our corpus is not large enough for perfect word alignment, it can be supported by a part of speech tagger. To keep the process completely language-independent, this step may al"
I13-1104,P91-1017,0,0.306455,"Curran et al., 2007). For instance, the ambiguity found in female names such as Iris and Rose may cause the induced terms to drift into flower names (McIntosh and Curran, 2009). Examples from the patent domain, that we are focusing on in this work, are PROCESSES that may drift into the semantic class of OBJECTS when terms such as energy storage and spring coupling are induced. Previous work has used the cross-lingual correspondence between variations in linguistic structure and variations in ambiguity as a form of naturally occurring supervision in unsupervised learning for a number of tasks (Dagan et al., 1991; 844 International Joint Conference on Natural Language Processing, pages 844–848, Nagoya, Japan, 14-18 October 2013. where Pi is the number of patterns containing termi . Finally Basilisk adds the t (originally 5) highest-ranked terms to the lexicon (line 8) and process repeats. ble method we are able to induce lexicons for any language given a parallel corpus. We do not need seed lists for all languages, which are often sparse. Translating1 the English seed list automatically results in high-quality lexicons for all other languages. Finally, many pattern-based lexicon bootstrapping methods"
I13-1104,W08-0509,0,0.0190449,"two languages: German and English. The parallel corpus. We use patent data distributed by the European Patent Office (EPO3 ) between 1998 and 2008. Most European patents provide their claims (the part of a patent defining the scope of protection) in German, English and French. We constructed a German-English parallel corpus out of 177,317 patent documents. Creation of Moses phrase table. For each unordered language pair, we create a M OSES (Koehn et al., 2007) phrase table in several steps. We first apply sentence alignment (G ARGANTUA Braune and Fraser (2010)), then word alignment (MG IZA ++ Gao and Vogel (2008)) to the data. And finally, we apply the statistical machine translation tool M OSES to the parallel word-aligned data. The resulting data structure is a phrase table of word-aligned phrases in two languages as shown in Figure 3, where the third line indicates the word alignment. English (JJ|VBG|NN)* NN (IN NN+)? Figure 4: PoS pattern for term filtering Patterns are extracted from the phrase tables as well. For each phrase in Lterm and Li we use the remaining tokens surrounding each term as bootstrapping pattern associated with this term (e.g., “Verfahren zur selektiven <X&gt;” is defined as patt"
I13-1104,W09-1703,0,0.152481,"xts. Languages are not isomorphic: ambiguous terms and contexts are frequently language-specific. In our example above, the English term energy storage is ambiguous, however, in German, each reading has its own translation. Energy storage is translated with Energiespeicher in the OBJECT reading and Energiespeicherung in the PROCESS reading. Our multilingual ensemble lexicon bootstrapping system is inspired by Basilisk (Thelen and Riloff, 2002). Previous work has addressed semantic drift in Basilisk by conflict resolution between several classes (Thelen and Riloff, 2002), by using web queries (Igo and Riloff, 2009) and by combining Basilisk in an ensemble with an SVM tagger and a coreference resolution system (Qadir and Riloff, 2012). These approaches are monolingual. Instead we use a multilingual ensemble method where the induced lexicons of several languages constrain each other. Apart from addressing semantic drift, the multilingual setting we propose has several other advantages. First, one language may leave implicit what another expresses directly in linguistic forms. In German, common nouns are capitalized and compound nouns are written as one word. We propagate German noun information via word a"
I13-1104,P07-2045,0,0.00261452,"ing are stored in a dictionary DICTterm↔i . Although our method is multilingual and language-independent we restrict our demonstration of its potential to two languages: German and English. The parallel corpus. We use patent data distributed by the European Patent Office (EPO3 ) between 1998 and 2008. Most European patents provide their claims (the part of a patent defining the scope of protection) in German, English and French. We constructed a German-English parallel corpus out of 177,317 patent documents. Creation of Moses phrase table. For each unordered language pair, we create a M OSES (Koehn et al., 2007) phrase table in several steps. We first apply sentence alignment (G ARGANTUA Braune and Fraser (2010)), then word alignment (MG IZA ++ Gao and Vogel (2008)) to the data. And finally, we apply the statistical machine translation tool M OSES to the parallel word-aligned data. The resulting data structure is a phrase table of word-aligned phrases in two languages as shown in Figure 3, where the third line indicates the word alignment. English (JJ|VBG|NN)* NN (IN NN+)? Figure 4: PoS pattern for term filtering Patterns are extracted from the phrase tables as well. For each phrase in Lterm and Li w"
I13-1104,S10-1003,0,0.098618,"Missing"
I13-1104,P09-1045,0,0.0257162,"source for many natural language processing (NLP) tasks like information extraction or anaphora resolution. Methods for automatically bootstrapping semantic lexicons given a seed list often struggle with lexicon accuracy decrease in higher iterations depending on corpus size (Igo and Riloff, 2009). One reason for this is semantic drift, which occurs when erroneous terms and/or contexts are introduced into and then dominate the iterative process (Curran et al., 2007). For instance, the ambiguity found in female names such as Iris and Rose may cause the induced terms to drift into flower names (McIntosh and Curran, 2009). Examples from the patent domain, that we are focusing on in this work, are PROCESSES that may drift into the semantic class of OBJECTS when terms such as energy storage and spring coupling are induced. Previous work has used the cross-lingual correspondence between variations in linguistic structure and variations in ambiguity as a form of naturally occurring supervision in unsupervised learning for a number of tasks (Dagan et al., 1991; 844 International Joint Conference on Natural Language Processing, pages 844–848, Nagoya, Japan, 14-18 October 2013. where Pi is the number of patterns cont"
I13-1104,S12-1028,0,0.0211001,"the English term energy storage is ambiguous, however, in German, each reading has its own translation. Energy storage is translated with Energiespeicher in the OBJECT reading and Energiespeicherung in the PROCESS reading. Our multilingual ensemble lexicon bootstrapping system is inspired by Basilisk (Thelen and Riloff, 2002). Previous work has addressed semantic drift in Basilisk by conflict resolution between several classes (Thelen and Riloff, 2002), by using web queries (Igo and Riloff, 2009) and by combining Basilisk in an ensemble with an SVM tagger and a coreference resolution system (Qadir and Riloff, 2012). These approaches are monolingual. Instead we use a multilingual ensemble method where the induced lexicons of several languages constrain each other. Apart from addressing semantic drift, the multilingual setting we propose has several other advantages. First, one language may leave implicit what another expresses directly in linguistic forms. In German, common nouns are capitalized and compound nouns are written as one word. We propagate German noun information via word alignment to English and thereby learn both single words as well as most multiword expressions (MWEs) without the need for"
I13-1104,W02-1028,0,0.368538,"e preeminently suitable to remedy problems related to semantic drift in iterative bootstrapping, where lexical and structural ambiguity give rise to erroneous terms and/or contexts. Languages are not isomorphic: ambiguous terms and contexts are frequently language-specific. In our example above, the English term energy storage is ambiguous, however, in German, each reading has its own translation. Energy storage is translated with Energiespeicher in the OBJECT reading and Energiespeicherung in the PROCESS reading. Our multilingual ensemble lexicon bootstrapping system is inspired by Basilisk (Thelen and Riloff, 2002). Previous work has addressed semantic drift in Basilisk by conflict resolution between several classes (Thelen and Riloff, 2002), by using web queries (Igo and Riloff, 2009) and by combining Basilisk in an ensemble with an SVM tagger and a coreference resolution system (Qadir and Riloff, 2012). These approaches are monolingual. Instead we use a multilingual ensemble method where the induced lexicons of several languages constrain each other. Apart from addressing semantic drift, the multilingual setting we propose has several other advantages. First, one language may leave implicit what anoth"
I13-1104,W10-3304,1,0.89472,"Missing"
I13-1104,W09-2413,0,\N,Missing
I13-1188,C10-1011,0,0.0363546,"terns we use. Experimental setup and results are presented in Section 4. In Section 5, we discuss and analyze these results. The last two sections describe related work and conclusions, respectively. 2 Data, task description and evaluation methodology Data. We use the patent data distributed by the European Patent Office1 (EPO) as our corpus. We extract the description (the main part of a patent) from 561,676 English patents filed between 1998 and 2008 and perform sentence splitting and tokenization using Treetagger (Schmid, 1994) and lemmatization and part-of-speech (POS) tagging using MATE (Bohnet, 2010). Sentences up to a size of 100 tokens extracted from a sample of 25,000 patents are parsed by M ATE. The resulting EPO corpus consists of roughly 4.6 billion tokens. Task description. The task we address is semantic tagging of patents. The research reported here was conducted as part of a project on computational linguistics analysis of patent text. We want to be able to support functionalities like color-coding entities of a particular semantic class for quick perusal; or searching for entities in a particular semantic class. Our longterm goal is to support semantic tagging for a large varie"
I13-1188,P99-1016,0,0.328772,"lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and Charniak (1998); Caraballo (1999); Goyal et al. (2010)). Most of this previous work uses a pairwise perspective (i.e., a focus on whether two words cooccur in a coordination). However, we use coordinations in a Basilisk approach, for which patterns contain several terms in general. We therefore do not split up coordinations in pairs but keep the complete coordination intact. Coordinations in technical domains frequently contain more than 2 elements. We argue that bootstrapping methods, known to be particularly sensitive to the ambiguity of terms and contexts and prone to semantic drift, benefit from the strong semantic cohere"
I13-1188,W09-2201,0,0.0256643,"Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This restriction to coordinat"
I13-1188,W03-0415,0,0.0235698,"ce in named entity recognition (NER). Work on automatic extraction of gazetteers for NER includes (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327 our approach because it uses knowledge bases like Wikipedia or is only applicable to traditional named entities (NEs). Traditional NEs like person are capitalized. Substances are not. Our work also differs in its focus on coordinations and technical text. Coordinations have been frequently used in work on lexical acquisition. Caraballo (1999) builds a hierarchy of coordinated nouns and their hypernyms. Cederberg and Widdows (2003) use coordinations to estimate the semantic relatedness of nouns. Widdows and Dorow (2002) and Qiu et al. (2011) cluster nouns and evaluate the semantic homogeneity of the clusters. Etzioni et al. (2005) use Hearst patterns to bootstrap lexicons. They also consider coordinations when selecting candidates. This previous work on coordinations is unsupervised and not focused on learning a particular semantic class that is defined by a seed set. Roark and Charniak (1998) use a variety of syntactic constructions, including coordinations, for bootstrapping. Our approach is different in that we do no"
I13-1188,W03-1022,0,0.0430402,"ernym relation such as “other products” in “copolymers, polyisobutene and other products”. We treat the conjuncts that survive filtering as an unordered set, i.e., we ignore their order in the text. The set is discarded and not used by BasiliskC if it only contains one element. 4 Experimental setup and results Experimental setup. We evaluate performance of the two algorithms Basilisk-G and Basilisk-C introduced above. We run experiments on EPO (Section 2) with the goal of learning the classes SUBSTANCE and DISEASE. Our seed set (Figure 1, line 1) consists of the 4223 substances distributed by Ciaramita and Johnson (2003) as part of SuperSenseTagger and 239 diseases extracted from Simple English Wikipedia5 . For Basilisk-C, we extracted 9.7 million unique coordinations, out of a total of 25 million. For Basilisk-G, we found 1.6 billion unique context patterns. In order to be able to run experiments quickly, we introduce frequency thresholds for MWEs, patterns and MWE-pattern combinations. We only consider MWEs and patterns that occur at least θ1 = 10 times and MWE-pattern combinations that occur at least θ2 = 3 times in EPO. These thresholds are unlikely to diminish lexicon quality since many rare instances of"
I13-1188,P07-1030,0,0.0191319,"ly promising resource for lexical bootstrapping in technical domains like patents. However, as exemplified by our experiments with Wikipedia, Basilisk-C shows similar performance on other domains, given that the members of the semantic class appear often in coordinations. 6 Related work We have chosen a semisupervised approach to lexical bootstrapping here since it is reasonable to expect that in the type of application scenario we have in mind resources are available to create a seed set. There are also completely unsupervised approaches to lexical bootstrapping (e.g., Lin and Pantel (2002); Davidov et al. (2007); Van Durme and Pas¸ca (2008); Dalvi et al. (2012)), but they usually cannot match the quality of approaches like ours that use human input such as a seed set. The bootstrapping approach we have adopted here starts with a seed set and then iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and some of which use syntact"
I13-1188,W03-0425,0,0.0302442,"e 1 www.epo.org for the patent domain and a large proportion of patents contain substances. A disease is an abnormal condition that affects the body of an organism. We selected disease as a clearly nontechnical category to be able to investigate potential differences of lexical bootstrapping algorithms for categories with very different properties. Gazetteers are crucial for good performance in machine-learning-based semantic tagging (Ratinov and Roth, 2009), e.g., the best performing systems for recognition of person, location and organization named entities all use gazetteer features (e.g., Florian et al. (2003)). It is in this context that we address the task of bootstrapping lexicons from corpora: for most semantic classes of interest in the patent domain high-coverage lexicons are not available. Evaluation methodology. Since our primary task is semantic tagging, we evaluate the quality of the bootstrapped lexicon directly on this task, i.e., on the task of tagging members of the semantic class in text – rather than evaluating the lexicon in a type-based evaluation as a set of terms without context as most previous work has done. A tagging-based evaluation directly measures what we need for our app"
I13-1188,D10-1008,0,0.0505988,"Missing"
I13-1188,P10-1029,0,0.0147699,"ients) based on Basilisk, that learns from coordinated verbs. This work is focused on verbs with the same patient polarity in binary coordinations extracted from a web corpus. Our approach is based on coordinations of any size from a large patent corpus and focuses on semantic lexicon induction. One distinguishing characteristic of our work is the patent domain. Other work on technical or scientific domains includes press releases of pharmaceutical companies (Phillips and Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude"
I13-1188,W02-1017,0,0.13351,"annot match the quality of approaches like ours that use human input such as a seed set. The bootstrapping approach we have adopted here starts with a seed set and then iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and some of which use syntactic analysis (e.g., Roark and Charniak (1998); Riloff and Jones (1999); Phillips and Riloff (2002)). Our approach does not make use of syntactic analysis but relies on POS patterns. Some recent work attempts to improve Basilisk’s accuracy. Igo and Riloff (2009) enhance precision by checking candidate terms using web queries. Qadir and Riloff (2012) combine Basilisk in an ensemble with an SVM tagger and a coreference resolution system. Our focus is learning technical terminology from very large corpora using coordinations, but any work that improves the accuracy of basic Basilisk could also be beneficial in our setting. Gazetteers are crucial for good performance in named entity recognition"
I13-1188,S12-1028,0,0.110734,". For Basilisk-C, we extracted 9.7 million unique coordinations, out of a total of 25 million. For Basilisk-G, we found 1.6 billion unique context patterns. In order to be able to run experiments quickly, we introduce frequency thresholds for MWEs, patterns and MWE-pattern combinations. We only consider MWEs and patterns that occur at least θ1 = 10 times and MWE-pattern combinations that occur at least θ2 = 3 times in EPO. These thresholds are unlikely to diminish lexicon quality since many rare instances of MWEs are due to OCR errors or failures of our RE-based recognition of NPs (see also (Qadir and Riloff, 2012)). Using the thresholds θ1 and θ2 , 4 This version of Basilisk uses the same RE to detect NPs as Basilisk-G. 5 simple.wikipedia.org/wiki/List of diseases there were 3.2 million unique MWEs, 56 million unique patterns and 121 million unique MWEpattern combinations. This is the raw data we run Basilisk-G on. As discussed in Section 2, our evaluation methodology directly evaluates the semantic lexicon on the task of interest: semantic tagging of patents. The tagging method we use is simple lexicon lookup. While tagging MWEs we exploit the compositional structure of entities by merging adjacent or"
I13-1188,W09-1119,0,0.0106988,"us on the semantic classes SUBSTANCE and DISEASE. A substance is a particular kind of physical matter with uniform properties. Substances are of obvious relevance 1 www.epo.org for the patent domain and a large proportion of patents contain substances. A disease is an abnormal condition that affects the body of an organism. We selected disease as a clearly nontechnical category to be able to investigate potential differences of lexical bootstrapping algorithms for categories with very different properties. Gazetteers are crucial for good performance in machine-learning-based semantic tagging (Ratinov and Roth, 2009), e.g., the best performing systems for recognition of person, location and organization named entities all use gazetteer features (e.g., Florian et al. (2003)). It is in this context that we address the task of bootstrapping lexicons from corpora: for most semantic classes of interest in the patent domain high-coverage lexicons are not available. Evaluation methodology. Since our primary task is semantic tagging, we evaluate the quality of the bootstrapped lexicon directly on this task, i.e., on the task of tagging members of the semantic class in text – rather than evaluating the lexicon in"
I13-1188,W97-0313,0,0.932796,"as}@ims.uni-stuttgart.de Abstract We address the task of bootstrapping a semantic lexicon from a list of seed terms and a large corpus. By restricting to a small subset of semantically strong patterns, i.e., coordinations, we improve results significantly. We show that the restriction to coordinations has several additional benefits, such as improved extraction of multiword expressions, and the possibility to scale up previous efforts. 1 Introduction High-quality semantic lexicons are needed for many natural language processing (NLP) tasks like information extraction and discourse processing (Riloff and Shepherd, 1997). Building such lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and C"
I13-1188,P98-2182,0,0.915716,"herd, 1997). Building such lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and Charniak (1998); Caraballo (1999); Goyal et al. (2010)). Most of this previous work uses a pairwise perspective (i.e., a focus on whether two words cooccur in a coordination). However, we use coordinations in a Basilisk approach, for which patterns contain several terms in general. We therefore do not split up coordinations in pairs but keep the complete coordination intact. Coordinations in technical domains frequently contain more than 2 elements. We argue that bootstrapping methods, known to be particularly sensitive to the ambiguity of terms and contexts and prone to semantic drift, benefit from the stro"
I13-1188,W02-1028,0,0.629108,"ons, we improve results significantly. We show that the restriction to coordinations has several additional benefits, such as improved extraction of multiword expressions, and the possibility to scale up previous efforts. 1 Introduction High-quality semantic lexicons are needed for many natural language processing (NLP) tasks like information extraction and discourse processing (Riloff and Shepherd, 1997). Building such lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and Charniak (1998); Caraballo (1999); Goyal et al. (2010)). Most of this previous work uses a pairwise perspective (i.e., a focus on whether two words cooccur in a coordination). However, we use coordinations in a Basili"
I13-1188,W06-2809,0,0.133124,"Missing"
I13-1188,C02-1114,0,0.0470596,"des (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327 our approach because it uses knowledge bases like Wikipedia or is only applicable to traditional named entities (NEs). Traditional NEs like person are capitalized. Substances are not. Our work also differs in its focus on coordinations and technical text. Coordinations have been frequently used in work on lexical acquisition. Caraballo (1999) builds a hierarchy of coordinated nouns and their hypernyms. Cederberg and Widdows (2003) use coordinations to estimate the semantic relatedness of nouns. Widdows and Dorow (2002) and Qiu et al. (2011) cluster nouns and evaluate the semantic homogeneity of the clusters. Etzioni et al. (2005) use Hearst patterns to bootstrap lexicons. They also consider coordinations when selecting candidates. This previous work on coordinations is unsupervised and not focused on learning a particular semantic class that is defined by a seed set. Roark and Charniak (1998) use a variety of syntactic constructions, including coordinations, for bootstrapping. Our approach is different in that we do not require parsing and that we cover MWEs in general, not just heads or compounds with a co"
I13-1188,C02-1154,0,0.0384348,"ical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This restriction to coordinations, a relation that is known to impose strong semantic coherence upon its members and as such a possible remedy for semantic drift, leads to significant improvements for the task of semantic tagging, compared to an unrestricted version of Basilisk. We further extended original Basilis"
I13-1188,C00-2137,0,0.0228439,"Missing"
I13-1188,W09-1703,0,0.0655695,"iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and some of which use syntactic analysis (e.g., Roark and Charniak (1998); Riloff and Jones (1999); Phillips and Riloff (2002)). Our approach does not make use of syntactic analysis but relies on POS patterns. Some recent work attempts to improve Basilisk’s accuracy. Igo and Riloff (2009) enhance precision by checking candidate terms using web queries. Qadir and Riloff (2012) combine Basilisk in an ensemble with an SVM tagger and a coreference resolution system. Our focus is learning technical terminology from very large corpora using coordinations, but any work that improves the accuracy of basic Basilisk could also be beneficial in our setting. Gazetteers are crucial for good performance in named entity recognition (NER). Work on automatic extraction of gazetteers for NER includes (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327"
I13-1188,D07-1073,0,0.0141213,"nt work attempts to improve Basilisk’s accuracy. Igo and Riloff (2009) enhance precision by checking candidate terms using web queries. Qadir and Riloff (2012) combine Basilisk in an ensemble with an SVM tagger and a coreference resolution system. Our focus is learning technical terminology from very large corpora using coordinations, but any work that improves the accuracy of basic Basilisk could also be beneficial in our setting. Gazetteers are crucial for good performance in named entity recognition (NER). Work on automatic extraction of gazetteers for NER includes (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327 our approach because it uses knowledge bases like Wikipedia or is only applicable to traditional named entities (NEs). Traditional NEs like person are capitalized. Substances are not. Our work also differs in its focus on coordinations and technical text. Coordinations have been frequently used in work on lexical acquisition. Caraballo (1999) builds a hierarchy of coordinated nouns and their hypernyms. Cederberg and Widdows (2003) use coordinations to estimate the semantic relatedness of nouns. Widdows and Dorow (2002) and Qiu et al. (2011) cluster"
I13-1188,P08-1119,0,0.0186558,"companies (Phillips and Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This r"
I13-1188,C02-1144,0,0.017373,"seem to be a particularly promising resource for lexical bootstrapping in technical domains like patents. However, as exemplified by our experiments with Wikipedia, Basilisk-C shows similar performance on other domains, given that the members of the semantic class appear often in coordinations. 6 Related work We have chosen a semisupervised approach to lexical bootstrapping here since it is reasonable to expect that in the type of application scenario we have in mind resources are available to create a seed set. There are also completely unsupervised approaches to lexical bootstrapping (e.g., Lin and Pantel (2002); Davidov et al. (2007); Van Durme and Pas¸ca (2008); Dalvi et al. (2012)), but they usually cannot match the quality of approaches like ours that use human input such as a seed set. The bootstrapping approach we have adopted here starts with a seed set and then iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and so"
I13-1188,U08-1013,0,0.0194942,"omplex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This restriction to coordinations, a relation that is known to impose strong semantic coherence upon its members and as such a possible remedy for semantic drift, leads to significant improvements for the task of semantic tagging, compared to an unrestricted version of Basilisk. We further extended original Basilisk to include MWEs, as these are predominant in tec"
I13-1188,P09-1045,0,0.0195536,"patient polarity verbs (i.e., verbs that impart positive or negative states on their patients) based on Basilisk, that learns from coordinated verbs. This work is focused on verbs with the same patient polarity in binary coordinations extracted from a web corpus. Our approach is based on coordinations of any size from a large patent corpus and focuses on semantic lexicon induction. One distinguishing characteristic of our work is the patent domain. Other work on technical or scientific domains includes press releases of pharmaceutical companies (Phillips and Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in"
J07-4002,I05-1019,0,0.0270176,"Missing"
J07-4002,J04-4004,0,0.362556,"ereas PP reattachment results arguably do not. We will show in this article that the performance of reattachment methods is higher when an oracle is available. This means that the current evaluation methodology for PP attachment does not produce realistic performance numbers. In particular, it is not clear whether reattachment methods improve the parses of state-of-the-art parsers. This argues for a new evaluation methodology for PP attachment, one that does not assume that an oracle is available. To create a more realistic setup for PP reattachment, we replace the oracle with Bikel’s parser (Bikel 2004). With the removal of the oracle and the introduction of the parser, the baseline performance also changes. It is no longer the performance of always choosing the most frequent attachment. Instead, it is the attachment performance of the parser. In fact, we will see that it is surprisingly difficult for reattachment methods to beat this baseline performance. The fact that standard parsers perform well on PP attachment also prompted us to investigate the performance of a standard parser on traditional oracle-based PPattachment. We find that standard parsers do well, further questioning the soun"
J07-4002,P05-1022,0,0.0846596,"Missing"
J07-4002,W95-0103,0,0.380255,"anges. It is no longer the performance of always choosing the most frequent attachment. Instead, it is the attachment performance of the parser. In fact, we will see that it is surprisingly difficult for reattachment methods to beat this baseline performance. The fact that standard parsers perform well on PP attachment also prompted us to investigate the performance of a standard parser on traditional oracle-based PPattachment. We find that standard parsers do well, further questioning the soundness of the traditional evaluation methodology. We compare our results with three other approaches: Collins and Brooks (1995; henceforth C&B), Olteanu and Moldovan (2005; henceforth O&M), and Toutanova, Manning, and Ng (2004; henceforth TM&N). We call these three methods PP reattachers. We selected these three methods because they perform best on the widely used PP attachment evaluation set created by Ratnaparkhi, Reynar, and Roukos (1994). We call this data set RRR. RRR consists of 20,801 training and 3,097 test quadruples of the form (v,n1,p,n2). The performance of the three reattachers on RRR is shown in Table 1.2 This article is structured as follows. Section 2 compares the performance of the reattachers on ora"
J07-4002,J05-1003,0,0.105609,"Missing"
J07-4002,P06-2029,0,0.412098,"Missing"
J07-4002,J93-1005,0,0.826448,"instead as an integral component of a parsing system, without using information from the gold-standard oracle. 1. Introduction One of the main challenges in natural language parsing is the resolution of ambiguity. One frequently studied type of ambiguity is prepositional phrase (PP) attachment. Given the quadruple (v,n1,p,n2), where v is the head of a verb phrase, n1 is the head of an NP1 dominated by v, p is the head of a prepositional phrase, and n2 the head of an NP2 embedded in the PP, the task of PP attachment is to determine whether we should attach the PP to the verb v or the noun n1 (Hindle and Rooth 1993).1 Work on PP attachment resolution generally assumes that there is an oracle that provides the quadruple (v,n1,p,n2), where we define an oracle as a mechanism that provides information that is not present in the data in their naturally occurring form. In PP attachment, the oracle is usually implemented by extracting the quadruple (v,n1,p,n2) from the gold-standard parse trees. In an application, a PP attachment module would be used to change the attachment of prepositional phrases in preliminary syntactic analyses produced by a parser—or reattach them. The problem with oracle-based work on PP"
J07-4002,J06-3002,0,0.0547788,"significant number of quadruples (more than 1,000) in the RRR training set do not occur in the RRR-sent training set due to differences between the 0.5 and 3 treebanks. Conversely, at least as many of the quadruples we extracted from the training set in RRR-sent do not match quadruples in RRR. 5. Related Work Almost all prior work on PP attachment has adopted what we call the oracle-based approach (Stetina and Nagao 1997; Ratnaparkhi 1998; Pantel and Lin 2000; Olteanu 2004; Bharati, Rohini, Vishnu, Bendre, and Sangal 2005), including several recent papers (Calvo, Gelbukh, and Kilgarriff 2005; Merlo and Ferrer 2006; Volk 2006; Srinivas and Bhattacharyya 2007). None of these papers attempt to improve a parser in a realistic parsing situation. However, a few studies were published recently that do evaluate PP attachment ¨ without oracles (Olteanu 2004; Atterer and Schutze 2006; Foth and Menzel 2006). Our experimental results suggest that the evaluation strategy of integrating PP reattachers into a baseline parser should be adopted more widely. As we only use a parser and no additional resources such as WordNet, dictionaries, Web data, or named-entity recognizers and stemmers, we restrict ourselves to comp"
J07-4002,H05-1035,0,0.611868,"lways choosing the most frequent attachment. Instead, it is the attachment performance of the parser. In fact, we will see that it is surprisingly difficult for reattachment methods to beat this baseline performance. The fact that standard parsers perform well on PP attachment also prompted us to investigate the performance of a standard parser on traditional oracle-based PPattachment. We find that standard parsers do well, further questioning the soundness of the traditional evaluation methodology. We compare our results with three other approaches: Collins and Brooks (1995; henceforth C&B), Olteanu and Moldovan (2005; henceforth O&M), and Toutanova, Manning, and Ng (2004; henceforth TM&N). We call these three methods PP reattachers. We selected these three methods because they perform best on the widely used PP attachment evaluation set created by Ratnaparkhi, Reynar, and Roukos (1994). We call this data set RRR. RRR consists of 20,801 training and 3,097 test quadruples of the form (v,n1,p,n2). The performance of the three reattachers on RRR is shown in Table 1.2 This article is structured as follows. Section 2 compares the performance of the reattachers on oracle-based reattachment with that of a standar"
J07-4002,P00-1014,0,0.748532,"ave a tenuous relation at best to performance on PP attachment if no oracle is available. The results of Experiments 1 and 2 are not directly comparable. A significant number of quadruples (more than 1,000) in the RRR training set do not occur in the RRR-sent training set due to differences between the 0.5 and 3 treebanks. Conversely, at least as many of the quadruples we extracted from the training set in RRR-sent do not match quadruples in RRR. 5. Related Work Almost all prior work on PP attachment has adopted what we call the oracle-based approach (Stetina and Nagao 1997; Ratnaparkhi 1998; Pantel and Lin 2000; Olteanu 2004; Bharati, Rohini, Vishnu, Bendre, and Sangal 2005), including several recent papers (Calvo, Gelbukh, and Kilgarriff 2005; Merlo and Ferrer 2006; Volk 2006; Srinivas and Bhattacharyya 2007). None of these papers attempt to improve a parser in a realistic parsing situation. However, a few studies were published recently that do evaluate PP attachment ¨ without oracles (Olteanu 2004; Atterer and Schutze 2006; Foth and Menzel 2006). Our experimental results suggest that the evaluation strategy of integrating PP reattachers into a baseline parser should be adopted more widely. As we"
J07-4002,P98-2177,0,0.695988,"traditional task have a tenuous relation at best to performance on PP attachment if no oracle is available. The results of Experiments 1 and 2 are not directly comparable. A significant number of quadruples (more than 1,000) in the RRR training set do not occur in the RRR-sent training set due to differences between the 0.5 and 3 treebanks. Conversely, at least as many of the quadruples we extracted from the training set in RRR-sent do not match quadruples in RRR. 5. Related Work Almost all prior work on PP attachment has adopted what we call the oracle-based approach (Stetina and Nagao 1997; Ratnaparkhi 1998; Pantel and Lin 2000; Olteanu 2004; Bharati, Rohini, Vishnu, Bendre, and Sangal 2005), including several recent papers (Calvo, Gelbukh, and Kilgarriff 2005; Merlo and Ferrer 2006; Volk 2006; Srinivas and Bhattacharyya 2007). None of these papers attempt to improve a parser in a realistic parsing situation. However, a few studies were published recently that do evaluate PP attachment ¨ without oracles (Olteanu 2004; Atterer and Schutze 2006; Foth and Menzel 2006). Our experimental results suggest that the evaluation strategy of integrating PP reattachers into a baseline parser should be adopte"
J07-4002,H94-1048,0,0.853765,"Missing"
J07-4002,W97-0109,0,0.651712,"nvolved. Results on the traditional task have a tenuous relation at best to performance on PP attachment if no oracle is available. The results of Experiments 1 and 2 are not directly comparable. A significant number of quadruples (more than 1,000) in the RRR training set do not occur in the RRR-sent training set due to differences between the 0.5 and 3 treebanks. Conversely, at least as many of the quadruples we extracted from the training set in RRR-sent do not match quadruples in RRR. 5. Related Work Almost all prior work on PP attachment has adopted what we call the oracle-based approach (Stetina and Nagao 1997; Ratnaparkhi 1998; Pantel and Lin 2000; Olteanu 2004; Bharati, Rohini, Vishnu, Bendre, and Sangal 2005), including several recent papers (Calvo, Gelbukh, and Kilgarriff 2005; Merlo and Ferrer 2006; Volk 2006; Srinivas and Bhattacharyya 2007). None of these papers attempt to improve a parser in a realistic parsing situation. However, a few studies were published recently that do evaluate PP attachment ¨ without oracles (Olteanu 2004; Atterer and Schutze 2006; Foth and Menzel 2006). Our experimental results suggest that the evaluation strategy of integrating PP reattachers into a baseline parse"
J07-4002,W06-2112,0,0.261919,"uadruples (more than 1,000) in the RRR training set do not occur in the RRR-sent training set due to differences between the 0.5 and 3 treebanks. Conversely, at least as many of the quadruples we extracted from the training set in RRR-sent do not match quadruples in RRR. 5. Related Work Almost all prior work on PP attachment has adopted what we call the oracle-based approach (Stetina and Nagao 1997; Ratnaparkhi 1998; Pantel and Lin 2000; Olteanu 2004; Bharati, Rohini, Vishnu, Bendre, and Sangal 2005), including several recent papers (Calvo, Gelbukh, and Kilgarriff 2005; Merlo and Ferrer 2006; Volk 2006; Srinivas and Bhattacharyya 2007). None of these papers attempt to improve a parser in a realistic parsing situation. However, a few studies were published recently that do evaluate PP attachment ¨ without oracles (Olteanu 2004; Atterer and Schutze 2006; Foth and Menzel 2006). Our experimental results suggest that the evaluation strategy of integrating PP reattachers into a baseline parser should be adopted more widely. As we only use a parser and no additional resources such as WordNet, dictionaries, Web data, or named-entity recognizers and stemmers, we restrict ourselves to comparing non-o"
J07-4002,P06-2004,1,\N,Missing
J11-4008,J92-4003,0,0.606061,"Missing"
J11-4008,H92-1021,0,0.136577,"rds should be employed, a view shared here. Their research does not present an explicit treatment of half-contextualization, however, nor considers halfcontextualization and the signiﬁcance of inward distributional information as insights which meet language modeling needs. Furthermore, our evaluation also differs in that it involves comparison against a whole-context model and a modiﬁed KN trigram model, rather than a simple word trigram model. Our use of a mixed n-gram class of both bigrams and unigrams also represents a marked difference between approaches. With regard to context direction Essen and Steinbiss (1992) also look at left and right contexts similarly to our approach. However, they do not compare half-context with whole-context approaches and they pursue a less efﬁcient similarity-based approach in contrast to the class-based approach proposed here. Finally, Dagan, Lee, and Pereira’s (1999) similarity-based language model uses a similar word to the observed word as the conditioning context used to generate the next word in the sequence. Again, no comparison to whole-context approaches is made. A similarity-based approach is also difﬁcult to use for large corpora as it would necessitate the cal"
J11-4008,P02-1024,0,0.395944,"ge models are inherently asymmetric: The role of the predictor and the predicted are different. This asymmetry shows up in word-based models to a limited extent: In most models the unit of prediction is a word; predictors include n-grams of any size in principle, not just words. However, in a class-based model the asymmetry between predictor and predicted is more important: There is no justiﬁcation for the premise (made, for example, in the Brown model) that the classes that are optimal for predictors are also the classes that are optimal for predictees. This observation has also been made by Gao et al. (2002), albeit without explicit reference to half contexts. We view our approach as better motivated since the asymmetry of our model is not posited, but follows from an analysis of the information sources needed for probabilistic inference in language modeling. Linguistic theory also provides evidence for half-context models. In many theories, there is a single formal concept that can be instantiated either by arguments of prepositions or by arguments of transitive verbs. For example, there are few if any syntactic differences between the arguments that can appear after a preposition like by and af"
J11-4008,N10-1046,0,0.0365343,"Missing"
J11-4008,E95-1020,1,0.685032,"Missing"
J11-4008,D08-1096,1,0.899676,"Missing"
J11-4008,I08-2089,0,0.0315174,"r large corpora as it would necessitate the calculation of similarity of every word to every other word in the corpus. Similarities 851 Computational Linguistics Volume 37, Number 4 can be computed more efﬁciently for a subset of words on a smaller corpus, but then many of the rare events that class and similarity based methods are most beneﬁcial for will not be covered. Our analyses in Section 5.2 and Section 5.3 demonstrate that half-context modeling is most beneﬁcial for rare events. Similar concerns apply to other similarity-based models, such as those proposed by Bengio et al. (2003) and Schwenk and Koehn (2008). 4. Parameter Estimation In this section we describe how the parameters of the model in Figure 1 are estimated. These parameters belong to two broad categories, namely, those which model the HC distributions (Pr and Pl ) and are used in the construction of clusters, and those which capture emission probabilities Pe and sequence probabilities Pseq that are used when the model is applied. Estimates were calculated on the basis of the training set part of a corpus of WSJ articles, 1987–1989, consisting of almost 50 million words, which will be described in more detail subsequently. 4.1 Clusterin"
J11-4008,P08-1086,0,0.186637,"(3) the use of absolute discounting to concentrate the effect of class-based generalization on rare hard-to-estimate events while leaving robust estimates based on frequent events largely unchanged. 3.4 Additional Related Work In addition to the motivating articles discussed earlier, other relevant work includes the randomization techniques applied by Emami and Jelinek (2005) to class-based n-gram language models. Half-context clusters are not at odds with a randomized approach as they could easily be implemented in such a fashion. Other related research includes the “mixed” model employed by Uszkoreit and Brants (2008), in which a word bigram (as opposed to a class of bigrams) probabilistically generates a class. They use, in our terminology, whole-context classes. The experiments reported subsequently suggest that HC classes are preferable to WC classes in the Brown-type set-up (classes generating classes); we plan to investigate whether this is also true in a mixed model in future work. Bassiou and Kotropoulos (2011) investigate two word-clustering techniques that operate on long-distance bigram probabilities (of varying distances) within a context and on interpolated long-distance bigram probabilities, b"
J13-1005,E06-2001,0,0.074573,"e derivational rule applied at the node in question. The last and biggest group of the pattern features is formed by the bilexical dependencies. They are based on the head word of the constituent node in question and its daughters. Versley and Rehbein (2009) have also introduced features that exploit statistical information gathered from an external data set and aim to resolve PP attachment ambiguity. Mutual information values were gathered on the association between nouns and immediately following prepositions, as well as between prepositions and closely following verbs on the DE-WaC corpus (Baroni and Kilgarriff 2006). These feature values were then used at NP→PP and VP→PP daughter attachments. A total of 2.7 million features ﬁred in the Tiger train. We ignored features ﬁring in less than ﬁve sentences for computational efﬁciency, resulting in 117,000 extremely sparse features. 7.3 Monolingual Reranking Experiments We rerank 100-best lists from BitPar (Schmid 2004), which uses the grammar extraction procedure and lexical resources introduced in Section 3. In each of the experiments we extracted the grammar from the Tiger train and used it to obtain the 100-best parses for the sentences of the evaluation co"
J13-1005,H91-1060,0,0.502125,"l reranking, which will be discussed later. 70 Fraser et al. Knowledge Sources for Parsing German to the same format as the gold standard trees by undoing Steps 2, 3, and 4 of Section 3.1. This conversion involves four steps: 1. Demarkovization removes all the auxiliary nodes introduced by markovization and raises their children to the next non-auxiliary node. 2. The added unary-branching nodes are eliminated. 3. The original grammatical function labels NK inside of NPs and PPs, and CJ inside of coordinated phrases, are restored. 4. All feature annotations are deleted. We use PARSEVAL scores (Black et al. 1991) and the standard evaluation tool evalb16 to compare the converted parse trees with the gold standard parse trees using labeled F-score. We report accuracies for all test sentences and not just sentences of length up to 40. We do not evaluate parsers with gold standard POS tags, but instead automatically infer them. These considerations make our evaluation setting as close to the real-world setting as possible. We report results for evaluations with and without grammatical functions. We report PARSEVAL scores with grammatical functions inside parentheses after the results using only basic cons"
J13-1005,A00-1031,0,0.0930958,"ecause some of the rules which are not markovized are also covered by markovization rules. 3.7 Dealing with Unknown Words and Unseen POS Tags BitPar includes a sophisticated POS guesser that uses several strategies to deal with unknown words and unseen POS tags of known words. Unknown words are divided into eight classes11 based on regular expressions that are manually deﬁned. These classes distinguish between lower-case words, capitalized words, all upper-case words, hyphenated words, numbers, and so forth. For each word class, BitPar builds a sufﬁx tree (Weischedel et al. 1993; Schmid 1995; Brants 2000) from the sufﬁxes of all words in the lexicon up to a length of 7. At each node of the sufﬁx tree, it sums up the conditional POS probabilities (given the word) over all known words with that sufﬁx. By summing POS probabilities rather than frequencies, all words have the same weight, which is appropriate here because we need to model the POS probabilities of infrequent words. BitPar computes POS probability estimates for each node using the sum of probabilities as a pseudo-frequency for each tag. The estimates are recursively smoothed with the Witten-Bell method using the smoothed POS probabil"
J13-1005,N10-1015,0,0.0304786,"Missing"
J13-1005,D08-1092,0,0.164596,"(+1.06) +0.78 (+1.00) +0.09 (+0.01) +0.78 (+0.70) The parse tree in Figure 6 demonstrates the value of bilingual features. It was produced by the monolingual reranker and it incorrectly combines the two adverbs aber and ebenso into an adverbial phrase and places this under the VP. The bilingual reranker instead attaches the two adverbs separately at the S level. The attachment to the S node indicates that the two adverbs modify the modal verb kann and not the full verb sagen. This is triggered by the feature POSPar2Prj. 8.3 Previous Work on Bitext Parsing Bitext parsing was also addressed by Burkett and Klein (2008). In that work, they use feature functions deﬁned on triples of (English parse tree, Chinese parse tree, alignment) which are combined in a log-linear model, much as we do. In later work (Burkett, Blitzer, and Klein 2010), they developed a uniﬁed joint model for solving the same problem using a weakly synchronized grammar. To train these models they use a small parallel Treebank that contains gold standard trees for parallel sentences in Chinese and English, whereas we only require gold standard trees for the language we are reranking. Another important difference is that Burkett and Klein (20"
J13-1005,P11-2037,0,0.0377588,"Missing"
J13-1005,P04-1082,0,0.0497864,"Missing"
J13-1005,P05-1022,0,0.827942,"at the gain of the two sets of reranking features (monolingual and bilingual) is additive, suggesting that they capture different types of information. The resulting parser is currently the best constituent parser for German (with or without bilingual features). In particular, we show that the baseline parser without reranking is competitive with the previous state of the art (the Berkeley parser) and that the re-ranking can add an important gain. 2. Previous Work Constituent parsing for English is well studied. The best generative constituent parsers are currently the Brown reranking parser (Charniak and Johnson 2005), the extension of this parser with self training by McClosky, Charniak, and Johnson (2006b), and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic 62 Fraser et al. Knowledge Sources for Parsing German context-free grammar (PCFG) parser with latent feature annotations. Charniak and Johnson (2005) and Huang (2008) have introduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCF"
J13-1005,P97-1003,0,0.510396,"es without a subject. We also mark conjunct clauses with the feature nosubj if they are neither headed by an imperative nor contain a child node with the grammatical function SB (subject) or EP (expletive). This is useful in order to correctly parse coordinations where the subject is dropped in the second conjunct. 3.6 Markovization The Tiger Treebank uses rather ﬂat structures where nodes have up to 25 child nodes. This causes sparse data problems because only some of the possible rules of that length actually appear in the training corpus. The sparse data problem is solved by markovization (Collins 1997; Klein and Manning 2003), which splits long rules into a set of shorter rules. The shorter rules generate the child nodes of the original rule one by one. First, the left siblings of the head child of the rule are generated from left to right, then the right siblings are generated from right to left. Finally, the head is generated. Figure 4 shows the markovization of the rule NP → NM NN PP PP. The auxiliary symbols that are used here encode information about the parent category, the head child, and previously generated children. Because all auxiliary symbols encode the head category, the head"
J13-1005,A92-1018,0,0.0352465,"Missing"
J13-1005,W06-2929,0,0.0602476,"Missing"
J13-1005,P03-1013,0,0.0390987,"generative constituent parsers are currently the Brown reranking parser (Charniak and Johnson 2005), the extension of this parser with self training by McClosky, Charniak, and Johnson (2006b), and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic 62 Fraser et al. Knowledge Sources for Parsing German context-free grammar (PCFG) parser with latent feature annotations. Charniak and Johnson (2005) and Huang (2008) have introduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previ"
J13-1005,P01-1024,0,0.0588643,"ment by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previous work on German parse reranking. Forst (2007) presented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied reranking to German dependency parsing. Versley and Rehbein (2009) developed a reranking method for German constituent parsers. The work by Versley and Rehbein and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar parser. We also reﬁne treebank labels to increase parsing performance, but add more information"
J13-1005,P08-1109,0,0.0939797,"Missing"
J13-1005,W07-1203,0,0.0147603,"d parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previous work on German parse reranking. Forst (2007) presented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied reranking to German dependency parsing. Versley and Rehbein (2009) developed a reranking method for German constituent parsers. The work by Versley and Rehbein and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar parser. We also reﬁne treebank labels to increase parsing performance, but add more information and achieve a larger improvement. We use the monolingual feature set of Versley and Rehbein in our reranker, but add further monolingual features as well as bilingu"
J13-1005,2008.amta-srw.2,0,0.0296618,"-HD VP-OC Man kann AVP-MO one can ADV-MO ADV-HD aber ebenso but just-as-well VVINF-HD , sagen say S-OC , KOUS-CP PPER-SB ADJD-PD VAFIN-HD dass sie anspruchsvoll sind that they demanding are Figure 6 Erroneous parse produced by the reranker using only monolingual features, which is corrected by bilingual features. The sentence means One can, however, just as well say that they are demanding. 81 Computational Linguistics Volume 39, Number 1 improve ranking of German BitPar parses in the held-out test sets, which is a form of self-training. Two other interesting studies in this area are those of Fossum and Knight (2008) and of Huang, Jiang, and Liu (2009). They improve English prepositional phrase attachment using features from a Chinese sentence. Unlike our approach, however, they do not require a Chinese syntactic parse as the word order in Chinese is sufﬁcient to unambiguously determine the correct attachment point of the prepositional phrase in the English sentence without using a Chinese syntactic parse. We know of no other work that has investigated to what extent monolingual and bilingual features in parse reranking are complementary. In particular, the work on bitext parsing by Burkett and Klein (200"
J13-1005,E09-1033,1,0.938949,"nominal head. The extraction of verbal heads is somewhat more complicated. In order to obtain the correct verbal head of a clause irrespective of the verb position (verb-ﬁrst, verbsecond, verb-ﬁnal), we extract all verbs that are dominated by the clause and a possibly empty sequence of VP-OC or VP-PD (statal passive) nodes and an optional VZ-HD node. Then we take the ﬁrst non-ﬁnite verb, or alternatively the ﬁrst ﬁnite verb if all verbs were ﬁnite. In order to avoid sparse data problems caused by the many different inﬂections of German verbs, we lemmatize the verbs. ¨ 21 In Fraser, Wang, and Schutze (2009) we used Minimum Error Rate Training. Once we made this change to maximum entropy the results on small feature sets became similar (details omitted). 22 An exception to this is that if a PP argument dominates a node of category PROAV-PH, it is considered a PROAV-PH argument. An example is the sentence Er [he] wartet [waits] (PP-OP (PROAV-PH darauf [for this]), (S-RE dass [that] sie [she] kommt [comes])). 74 Fraser et al. Knowledge Sources for Parsing German Table 2 Arguments used in extracted subcategorization frames. NP-SB, PN-SB, CNP-SB, S-SB, VP-SB NP-OA, PN-OA, CNP-OA NP-DA, PN-DA, CNP-DA"
J13-1005,N06-1024,0,0.0274491,"Missing"
J13-1005,W08-1007,0,0.160992,"sing is which type of parsing formalism to adopt, constituency or dependency. It is a widely held belief that dependency structures are better suited to represent syntactic analyses for morphologically rich languages because they allow non-projective structures (the equivalent of discontinuous constituents in constituency parsing). As Tsarfaty et al. (2010) point out, however, this is not the same as proving that dependency parsers function better than constituency parsers for parsing morphologically rich languages. In fact, most state-of-the-art dependency parsers (McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al. 2010a) generate purely projective dependency structures that are optionally transformed into non-projective structures in a post-processing step. Comparable post-processing techniques have been used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006; Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents and might work for other languages, as well. ¨ The overview paper of the Parsing German Shared Task (Kubler 2008) reports higher accuracies for detecting grammatical functions with dependency parsers than with constituent parsers, but"
J13-1005,W08-2122,0,0.027458,". The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previous work on German parse reranking. Forst (2007) presented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied reranking to German dependency parsing. Versley and Rehbein (2009) developed a reranking method for German constituent parsers. The work by Versley and Rehbein and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar parser. We also reﬁne treebank labels to increase parsing performance, but add more information and achieve a larger improvement. We use the mon"
J13-1005,P08-1067,0,0.0386001,"ious state of the art (the Berkeley parser) and that the re-ranking can add an important gain. 2. Previous Work Constituent parsing for English is well studied. The best generative constituent parsers are currently the Brown reranking parser (Charniak and Johnson 2005), the extension of this parser with self training by McClosky, Charniak, and Johnson (2006b), and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic 62 Fraser et al. Knowledge Sources for Parsing German context-free grammar (PCFG) parser with latent feature annotations. Charniak and Johnson (2005) and Huang (2008) have introduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been deve"
J13-1005,D09-1127,0,0.0481369,"Missing"
J13-1005,J98-4004,0,0.233603,"s the chart as a large bit vector. This representation is memory efﬁcient and allows full parsing (without search space pruning) with large treebank grammars. BitPar is also quite fast because the basic parsing operations are parallelized by means of (single-instruction) and-operations on bitvectors. BitPar can either be used to compute the most likely parse (Viterbi parse), or the full set of parses in the form of a parse forest, or the n-best parse trees. 3.1 Grammar The grammar and lexicon used by our generative parser are extracted from the Tiger2 Treebank (Brants et al. 2002). Similar to Johnson (1998) and Klein and Manning (2003) we improve the accuracy of the unlexicalized parser by reﬁning the non-terminal symbols of the grammar to encode relevant contextual information. This reﬁnement weakens the strong independence assumptions of PCFGs and improves parsing accuracy. The extraction of the grammar and lexicon involves the following steps: 1. Discontinuous constituents are eliminated (Section 3.2). 2. Treebank annotations are transformed (Section 3.4) and augmented (Section 3.5). 3. Grammar rules, lexical rules, and their frequencies are extracted from the annotated parse trees. 4. The gr"
J13-1005,P03-1054,0,0.408779,"rge bit vector. This representation is memory efﬁcient and allows full parsing (without search space pruning) with large treebank grammars. BitPar is also quite fast because the basic parsing operations are parallelized by means of (single-instruction) and-operations on bitvectors. BitPar can either be used to compute the most likely parse (Viterbi parse), or the full set of parses in the form of a parse forest, or the n-best parse trees. 3.1 Grammar The grammar and lexicon used by our generative parser are extracted from the Tiger2 Treebank (Brants et al. 2002). Similar to Johnson (1998) and Klein and Manning (2003) we improve the accuracy of the unlexicalized parser by reﬁning the non-terminal symbols of the grammar to encode relevant contextual information. This reﬁnement weakens the strong independence assumptions of PCFGs and improves parsing accuracy. The extraction of the grammar and lexicon involves the following steps: 1. Discontinuous constituents are eliminated (Section 3.2). 2. Treebank annotations are transformed (Section 3.4) and augmented (Section 3.5). 3. Grammar rules, lexical rules, and their frequencies are extracted from the annotated parse trees. 4. The grammar is markovized (Section"
J13-1005,2005.mtsummit-papers.11,0,0.0116501,"to German, and sums all the span differences. It is measured in words. In addition to PPParentPrjWord we implement two bonus features, NonPPWord and NonPPPer. The former simply calculates the number of words that 79 Computational Linguistics Volume 39, Number 1 do not belong to PP phrases in the sentence, and the latter computes the non-PP proportion in a character-based fashion. These can be thought of as tunable parameters which adjust PPParentPrjWord to not disfavor large PPs. The other selected projection features are described in Table 4. Probabilistic Feature Functions. We use Europarl (Koehn 2005), from which we extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate the probabilistic feature functions described in this section. We describe the feature PTag, despite the fact that it was not selected by the feature analysis, because several variations (described next) were selected. PTag measures tagging inconsistency based on estimating the probability for each English word that it has a particular POS tag, given the aligned German word’s POS tag. To avoid noisy feature values due to outliers and parse errors, we bound the value of PTag at 5.26 We use relati"
J13-1005,W06-1614,0,0.0820292,"Missing"
J13-1005,P04-1042,0,0.0790196,"Missing"
J13-1005,N06-1020,0,0.165602,"Missing"
J13-1005,P06-1043,0,0.0877529,"Missing"
J13-1005,E06-1011,0,0.0241518,"key question for MR&LC parsing is which type of parsing formalism to adopt, constituency or dependency. It is a widely held belief that dependency structures are better suited to represent syntactic analyses for morphologically rich languages because they allow non-projective structures (the equivalent of discontinuous constituents in constituency parsing). As Tsarfaty et al. (2010) point out, however, this is not the same as proving that dependency parsers function better than constituency parsers for parsing morphologically rich languages. In fact, most state-of-the-art dependency parsers (McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al. 2010a) generate purely projective dependency structures that are optionally transformed into non-projective structures in a post-processing step. Comparable post-processing techniques have been used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006; Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents and might work for other languages, as well. ¨ The overview paper of the Parsing German Shared Task (Kubler 2008) reports higher accuracies for detecting grammatical functions with dependency parsers than with con"
J13-1005,W98-0509,0,0.0308318,"roduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previous work on German parse reranking. Forst (2007) presented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied reranking to German dependency parsing. Versley and Rehbein (2009) developed a reranking method for German constituent parsers. The work by Versley and Rehbein and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar parser. We also reﬁne treebank labels to increase parsing performa"
J13-1005,N07-1051,0,0.0190713,"information. The resulting parser is currently the best constituent parser for German (with or without bilingual features). In particular, we show that the baseline parser without reranking is competitive with the previous state of the art (the Berkeley parser) and that the re-ranking can add an important gain. 2. Previous Work Constituent parsing for English is well studied. The best generative constituent parsers are currently the Brown reranking parser (Charniak and Johnson 2005), the extension of this parser with self training by McClosky, Charniak, and Johnson (2006b), and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic 62 Fraser et al. Knowledge Sources for Parsing German context-free grammar (PCFG) parser with latent feature annotations. Charniak and Johnson (2005) and Huang (2008) have introduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Wo"
J13-1005,W08-1005,0,0.136381,"ge Sources for Parsing German context-free grammar (PCFG) parser with latent feature annotations. Charniak and Johnson (2005) and Huang (2008) have introduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previous work on German parse reranking. Forst (2007) presented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied reranking to German dependency parsing. Versley and Rehbein (2009) developed a reranking method for German constituent parsers. The work by Versley and Rehbein a"
J13-1005,W06-1608,0,0.0290991,"for German previously by Versley and Rehbein (2009). We show that the richer subcategorization-based framework for monolingual reranking is effective; it has comparable performance to the sparse feature set—moreover, they complement each other. For bilingual reranking, we present our approach to bitext parsing, where a German parse is found that minimizes syntactic divergence with an automatically generated parse of its English translation. We pursue this approach for a number of reasons. First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver 2006). Improved parses of bitext should result in improved machine translation. Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext. Third, we hope that the improved parses of bitext can serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky, Charniak, and Johnson 2006a). We show that the three different knowledge sources we use in this paper (lexical knowledge, monolingual features, and bilingual features) are valuable separately. W"
J13-1005,P05-1034,0,0.0754638,"Missing"
J13-1005,W08-1006,0,0.02696,"tent feature annotations. Charniak and Johnson (2005) and Huang (2008) have introduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previous work on German parse reranking. Forst (2007) presented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied reranking to German dependency parsing. Versley and Rehbein (2009) developed a reranking method for German constituent parsers. The work by Versley and Rehbein and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexica"
J13-1005,N10-1049,0,0.0149329,"search question.3 Constituent parses often provide more information than dependency parses. An example is the coordination ambiguity in old men and women versus old men and children. The correct constituent parse for the ﬁrst expression contains a coordination at the noun level whereas the parse for the second expression coordinates at the level of NPs. The dependency structures of both expressions, on the other hand, are usually identical and thus unable to reﬂect the fact that old modiﬁes women but not children. It is possible, in principle, to encode the difference in dependency trees (cf. Rambow 2010), 2 This is due to how the evalb tool used to calculate PARSEVAL works. If a constituent is not perfectly matched, the grammatical function is considered to be wrong, even if there was a partial match (at the token level). This is not a problem with dependency-based evaluation. For further discussion of the PARSEVAL metric and dependency-based evaluation see, for example, Rehbein and van Genabith (2007) and Tsarfaty, Nivre, and Andersson (2012). 3 Two possible solutions are to use TedEval (Tsarfaty, Nivre, and Andersson 2012), or to conduct an analysis of grammatical functions at the token lev"
J13-1005,W07-2460,0,0.0876378,"Missing"
J13-1005,C04-1056,0,0.0313236,"ky, Charniak, and Johnson (2006b), and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic 62 Fraser et al. Knowledge Sources for Parsing German context-free grammar (PCFG) parser with latent feature annotations. Charniak and Johnson (2005) and Huang (2008) have introduced a signiﬁcant improvement by feature-rich discriminative reranking as well. The number of treebank constituent parsers for German is smaller. Dubey and Keller (2003) adapted Collins’s (1997) lexicalized parser to German. An unlexicalized PCFG parser similar to our generative parser was presented by Schiehlen (2004). The best constituent parser participating in the ACL-08 Workshop on Parsing German ¨ (Kubler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser was also adapted to German (Rafferty and Manning 2008). German dependency parsers ¨ have been developed by Menzel and Schroder (1998), Duchier and Debusmann (2001), Hall and Nivre (2008), Henderson et al. (2008), and Seeker et al. (2010a), to name a few. There is also some previous work on German parse reranking. Forst (2007) presented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied reranking to"
J13-1005,C04-1024,1,0.911442,"y and Rehbein (2009) developed a reranking method for German constituent parsers. The work by Versley and Rehbein and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar parser. We also reﬁne treebank labels to increase parsing performance, but add more information and achieve a larger improvement. We use the monolingual feature set of Versley and Rehbein in our reranker, but add further monolingual features as well as bilingual features. 3. Generative Parsing Framework Our generative parser is an unlexicalized PCFG parser which is based on the BitPar parser (Schmid 2004). BitPar uses a fast bitvector-based implementation of the wellknown Cocke-Younger-Kasami algorithm and stores the chart as a large bit vector. This representation is memory efﬁcient and allows full parsing (without search space pruning) with large treebank grammars. BitPar is also quite fast because the basic parsing operations are parallelized by means of (single-instruction) and-operations on bitvectors. BitPar can either be used to compute the most likely parse (Viterbi parse), or the full set of parses in the form of a parse forest, or the n-best parse trees. 3.1 Grammar The grammar and l"
J13-1005,P06-1023,1,0.822361,"tuents in constituency parsing). As Tsarfaty et al. (2010) point out, however, this is not the same as proving that dependency parsers function better than constituency parsers for parsing morphologically rich languages. In fact, most state-of-the-art dependency parsers (McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al. 2010a) generate purely projective dependency structures that are optionally transformed into non-projective structures in a post-processing step. Comparable post-processing techniques have been used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006; Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents and might work for other languages, as well. ¨ The overview paper of the Parsing German Shared Task (Kubler 2008) reports higher accuracies for detecting grammatical functions with dependency parsers than with constituent parsers, but the direct comparison is not fair as it required phrase boundaries to be correct on the constituent side while the tokens were the unit of evaluation on the dependency side.2 How to carry out an absolutely fair comparison of the two representations is still an open research question.3 Consti"
J13-1005,schmid-etal-2004-smor,1,0.827637,"Missing"
J13-1005,C10-2129,0,0.260946,"parsing formalism to adopt, constituency or dependency. It is a widely held belief that dependency structures are better suited to represent syntactic analyses for morphologically rich languages because they allow non-projective structures (the equivalent of discontinuous constituents in constituency parsing). As Tsarfaty et al. (2010) point out, however, this is not the same as proving that dependency parsers function better than constituency parsers for parsing morphologically rich languages. In fact, most state-of-the-art dependency parsers (McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al. 2010a) generate purely projective dependency structures that are optionally transformed into non-projective structures in a post-processing step. Comparable post-processing techniques have been used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006; Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents and might work for other languages, as well. ¨ The overview paper of the Parsing German Shared Task (Kubler 2008) reports higher accuracies for detecting grammatical functions with dependency parsers than with constituent parsers, but the direct comparis"
J13-1005,P10-1111,0,0.0405122,"Missing"
J13-1005,P08-1066,0,0.0270618,"Missing"
J13-1005,E12-1006,0,0.13257,"Missing"
J13-1005,W10-1401,0,0.107052,"Missing"
J13-1005,C10-1123,0,0.0139946,"ion, acting together. The most natural way of doing this in a language like German is to perform this integration of the two knowledge sources directly as part of parsing. We do this by annotating constituent labels with grammatical function where appropriate. In contrast with syntactic parses of strongly conﬁgurational languages like English, syntactic parses of German are not useful for most tasks without having 4 We do note, however, that there are a few translation systems which use a dependency representation directly (e.g., Quirk, Menezes, and Cherry 2005; Shen, Xu, and Weischedel 2008; Tu et al. 2010). 61 Computational Linguistics Volume 39, Number 1 grammatical functions indicated. It is not even possible to access the basic subcategorization of the verb (such as determining the subject) without grammatical functions. We argue that MR&LC languages like German should always be evaluated on labelscum-grammatical-function. Our last main contribution in this paper concerns the fact that we believe that MR&LC languages give rise to more ambiguity than languages that are predominantly conﬁgurational or morphological. As an example consider the German sentence “Die [the] Katze [cat] jagt [hunts]"
J13-1005,W09-3820,0,0.334604,"disambiguation. We believe that this distinguishing characteristic of MR&LC languages makes it necessary to tap additional knowledge sources. In this paper, we look at two such knowledge sources: monolingual reranking (which captures global properties of well-formed parses for additional disambiguation) and bilingual reranking (which exploits parallel text in a different language for disambiguation). For monolingual reranking, we deﬁne a novel set of rich features based on subcategorization frames. We compare our compact feature set with a sparse feature set designed for German previously by Versley and Rehbein (2009). We show that the richer subcategorization-based framework for monolingual reranking is effective; it has comparable performance to the sparse feature set—moreover, they complement each other. For bilingual reranking, we present our approach to bitext parsing, where a German parse is found that minimizes syntactic divergence with an automatically generated parse of its English translation. We pursue this approach for a number of reasons. First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver 2006). Improved parses of b"
J13-1005,J93-2006,0,0.218488,"re spurious ambiguities. They arise because some of the rules which are not markovized are also covered by markovization rules. 3.7 Dealing with Unknown Words and Unseen POS Tags BitPar includes a sophisticated POS guesser that uses several strategies to deal with unknown words and unseen POS tags of known words. Unknown words are divided into eight classes11 based on regular expressions that are manually deﬁned. These classes distinguish between lower-case words, capitalized words, all upper-case words, hyphenated words, numbers, and so forth. For each word class, BitPar builds a sufﬁx tree (Weischedel et al. 1993; Schmid 1995; Brants 2000) from the sufﬁxes of all words in the lexicon up to a length of 7. At each node of the sufﬁx tree, it sums up the conditional POS probabilities (given the word) over all known words with that sufﬁx. By summing POS probabilities rather than frequencies, all words have the same weight, which is appropriate here because we need to model the POS probabilities of infrequent words. BitPar computes POS probability estimates for each node using the sum of probabilities as a pseudo-frequency for each tag. The estimates are recursively smoothed with the Witten-Bell method usin"
J13-1005,W08-1008,0,\N,Missing
J13-1005,P02-1018,0,\N,Missing
J13-1005,J05-1003,0,\N,Missing
J15-2001,W13-2257,0,0.0166003,"ight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and Quirk (2007) proposed improved future cost estimation to enable higher distortion limits in phrasal MT. Green, Galley, and Manning (2010) additionally proposed discriminative distortion models to achieve better translation accuracy than the baseline phrase-based system for a distortion limit of 15 words. Bisazza and Federico (2013) recently proposed a novel method to dynamically select which longrange reorderings to consider during the hypothesis extension process in a phrasebased decoder and showed an improvement in a German–English task by increasing the distortion limit to 18. Spurious Phrasal Segmentation. A problem with the phrase-based model is that there is no unique correct phrasal segmentation of a sentence. Therefore, all possible ways of segmenting a bilingual sentence consistent with the word alignment are learned and used. This leads to two problems: (i) phrase frequencies are obtained by counting all possi"
J15-2001,J93-2003,0,0.090203,"tics Volume 41, Number 2 better than state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems (Ncode) on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM. 1. Introduction Statistical Machine Translation (SMT) advanced near the beginning of the century from word-based models (Brown et al. 1993) towards more advanced models that take contextual information into account. Phrase-based (Koehn, Och, and Marcu 2003; Och and Ney 2004) and N-gram-based (Casacuberta and Vidal 2004; Marino ˜ et al. 2006) models are two instances of such frameworks. Although the two models have some common properties, they are substantially different. The present work is a step towards combining the benefits and remedying the flaws of these two frameworks. Phrase-based systems have a simple but effective mechanism that learns larger chunks of translation called bilingual phrases.1 Memorizing larger units enabl"
J15-2001,N10-2003,0,0.0347044,"Missing"
J15-2001,N13-1003,0,0.0149619,"ccount how previous words were translated and reordered. Although such an independence assumption is useful to reduce sparsity, it is overly generalizing and does not help to disambiguate good reorderings from the bad ones. Moreover, a vast majority of extracted phrases are singletons and the corresponding probability of orientation given phrase-pair estimates are based on a single observation. Due to sparsity, the model falls back to use one-word phrases instead, the orientation of which is ambiguous and can only be judged based on context that is ignored. This drawback has been addressed by Cherry (2013) by using sparse features for reordering models. Hard Distortion Limit. The lexicalized reordering model fails to filter out bad largescale reorderings effectively (Koehn 2010). A hard distortion limit is therefore required during decoding in order to produce good translations. A distortion limit beyond eight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and"
J15-2001,J07-2003,0,0.0395574,"mework. We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1). 6.1 Baseline Our Moses (Koehn et al. 2007) baseline systems are based on the setup described in Durrani et al. (2013b). We trained our systems with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield 2011) used at runtime, distortion limit of 6, minimum Bayes-risk decoding (Kumar and Byrne 2004), cube pruning (Huang and Chiang 2007), and the no-reordering-over-punctuation heuristic. We used factored models (Koehn and Hoang 2007), for German–English and English–German. We trained the lexicalized reordering model (Koehn et al. 2005) with msd-bidirectional-fe settings. 6.2 Results Table 5 shows that the OSM results in higher gains than the lexicalized reordering model on top of a plain phrase-based baseline (Pb). The average improvement obtained using the lexicalized reordering model (Pblex ) over the baseline (Pb) is 0.50. In comparison, the average improvement obtained by using the OSM (Pbosm ) over the baseline (Pb) is 0"
J15-2001,N07-2035,0,0.0710623,"Missing"
J15-2001,2005.iwslt-1.25,0,0.0712731,"taining content words (which are less frequent than functional words). For example, the alignment link hinunterschuttete ¨ – ‘down’ is deleted and only the link hinunterschuttete ¨ – ‘poured’ is retained because ‘down’ occurs more frequently than ‘poured’. Crego and Yvon (2009) used split tokens to deal with this phenomenon. For MTU-based decoding we also need to deal with unaligned target words. For each unaligned target word, we determine the (left or right) neighbor that it appears more frequently with and align it with the same source word as this neighbor. Crego, de Gispert, and Marino ˜ (2005) and Marino ˜ et al. (2006) instead used lexical probabilities p( f |e) obtained from IBM Model 1 (Brown et al. 1993) to decide whether to attach left or right. A more sophisticated strategy based on part-of-speech entropy was proposed by Gispert and Marino ˜ (2006). 4.2 Initial Evaluation We evaluated our systems on German-to-English, French-to-English, and Spanish-toEnglish news translation for the purpose of development and evaluation. We used data from the eighth version of the Europarl Corpus and the News Commentary made available for the translation task of the Eighth Workshop on Statist"
J15-2001,2007.mtsummit-papers.16,0,0.0557062,"ce 164 Durrani et al. Operation Sequence Model versa. Notice how the reordering decision is triggered by the translation decision in the example. The probability of a gap insertion operation after the generation of the auxiliaries wurden ¨ – ‘would’ will be high because reordering is necessary in order to move the second part of the German verb complex (stimmen) to its correct position at the end of the clause. Complex reorderings can be achieved by inserting multiple gaps and/or recursively inserting a gap within a gap. Consider the generation of the example in Figure 3 (borrowed from Chiang [2007]). The generation of this bilingual sentence pair proceeds as follows: Generate(Aozhou, Australia) Generate(shi, is) Insert Gap Generate(zhiyi, one of ) At this point, the (partial) Chinese and English sentences look like this: Aozhou shi zhiyi ↓ Australia is one of The translator now jumps back and recursively inserts a gap inside of the gap before continuing translation: Jump Back (1) Insert Gap Generate(shaoshu, the few) Generate(guojia, countries) Aozhou shi shaoshu guojia ↓ zhiyi Australia is one of the few countries The rest of the sentence pair is generated as follows: Jump Back (1) Ins"
J15-2001,2009.eamt-1.10,0,0.0196584,"aligned and discontinuous targets. If a source word is aligned with multiple target words that are not consecutive, first the link to the least frequent target word is identified, and the group (consecutive adjacent words) of links containing this word is retained while the others are deleted. The intuition here is to keep the alignments containing content words (which are less frequent than functional words). For example, the alignment link hinunterschuttete ¨ – ‘down’ is deleted and only the link hinunterschuttete ¨ – ‘poured’ is retained because ‘down’ occurs more frequently than ‘poured’. Crego and Yvon (2009) used split tokens to deal with this phenomenon. For MTU-based decoding we also need to deal with unaligned target words. For each unaligned target word, we determine the (left or right) neighbor that it appears more frequently with and align it with the same source word as this neighbor. Crego, de Gispert, and Marino ˜ (2005) and Marino ˜ et al. (2006) instead used lexical probabilities p( f |e) obtained from IBM Model 1 (Brown et al. 1993) to decide whether to attach left or right. A more sophisticated strategy based on part-of-speech entropy was proposed by Gispert and Marino ˜ (2006). 4.2"
J15-2001,C10-2023,0,0.0480813,"Missing"
J15-2001,N13-1001,1,0.781265,"the subsequent parts of discontinuous target cepts to appear after the first word of the cept. During decoding we use phrase-internal alignments to hypothesize such a linearization. This is done only for the estimation of the OSM, and the target for all other purposes is generated in its original order. This heuristic allows us to deal with target discontinuities without extending the operation sequence model in complicated ways. It results in better BLEU accuracy in comparison with the post-editing of the alignments method described in Section 4.1. For details and empirical results refer to Durrani et al. (2013a) (see Table 2 therein, compare Rows 4 and 5). Note that the OSM, like the discontinuous phrase-based model (Galley and Manning 2010), allows all possible geometries as shown in Figure 7. However, because our decoder only uses continuous phrases, we cannot hypothesize (ii) and (iii) unless they appear inside of a phrase. But our model could be integrated into a discontinuous phrase-based system to overcome this limitation. 6. Further Comparative Experiments Our model, like the reordering models (Tillmann and Zhang 2005; Galley and Manning 2008) used in phrase-based decoders, is lexicalized. H"
J15-2001,P13-2071,1,0.853331,"the subsequent parts of discontinuous target cepts to appear after the first word of the cept. During decoding we use phrase-internal alignments to hypothesize such a linearization. This is done only for the estimation of the OSM, and the target for all other purposes is generated in its original order. This heuristic allows us to deal with target discontinuities without extending the operation sequence model in complicated ways. It results in better BLEU accuracy in comparison with the post-editing of the alignments method described in Section 4.1. For details and empirical results refer to Durrani et al. (2013a) (see Table 2 therein, compare Rows 4 and 5). Note that the OSM, like the discontinuous phrase-based model (Galley and Manning 2010), allows all possible geometries as shown in Figure 7. However, because our decoder only uses continuous phrases, we cannot hypothesize (ii) and (iii) unless they appear inside of a phrase. But our model could be integrated into a discontinuous phrase-based system to overcome this limitation. 6. Further Comparative Experiments Our model, like the reordering models (Tillmann and Zhang 2005; Galley and Manning 2008) used in phrase-based decoders, is lexicalized. H"
J15-2001,W13-2212,1,0.943997,"the subsequent parts of discontinuous target cepts to appear after the first word of the cept. During decoding we use phrase-internal alignments to hypothesize such a linearization. This is done only for the estimation of the OSM, and the target for all other purposes is generated in its original order. This heuristic allows us to deal with target discontinuities without extending the operation sequence model in complicated ways. It results in better BLEU accuracy in comparison with the post-editing of the alignments method described in Section 4.1. For details and empirical results refer to Durrani et al. (2013a) (see Table 2 therein, compare Rows 4 and 5). Note that the OSM, like the discontinuous phrase-based model (Galley and Manning 2010), allows all possible geometries as shown in Figure 7. However, because our decoder only uses continuous phrases, we cannot hypothesize (ii) and (iii) unless they appear inside of a phrase. But our model could be integrated into a discontinuous phrase-based system to overcome this limitation. 6. Further Comparative Experiments Our model, like the reordering models (Tillmann and Zhang 2005; Galley and Manning 2008) used in phrase-based decoders, is lexicalized. H"
J15-2001,C14-1041,1,0.834353,". It also shows the model sizes when filtered on news-test2013. A similar amount of reduction could be achieved by applying filtering to the OSMs following the language model filtering described by Heafield and Lavie (2010). 15 We also tried to amalgamate lexically driven OSM and generalized OSMs into a single model rather than using these as separate features. However, this attempt was unsuccessful (See Durrani et al. [2014] for details). 16 We also found using morphological tags and automatic word clusters to be useful in our recent IWSLT evaluation campaign (Birch, Durrani, and Koehn 2013; Durrani et al. 2014). 17 The code for the OSM in Moses can be greatly optimized but requires major modifications to source and target phrase classes in Moses. 182 Durrani et al. Operation Sequence Model Table 8 Wall-clock decoding times (in minutes) on WMT-13. Into English From English Pblex Pblex+osm Pblex Pblex+osm DE FR ES 61 108 111 88 Δ 27 163 Δ 55 142 Δ 31 143 113 74 158 Δ 15 154 Δ 41 109 Δ 35 Avg 93 131 Δ 38 110 140 Δ 30 Table 9 Data sizes (in number of sentences) and memory usage (in giga-bytes). Columns: Phrase translation and lexicalized reordering tables give overall model sizes/sizes when filtered on"
J15-2001,P11-1105,1,0.82518,"Missing"
J15-2001,D08-1089,0,0.42418,"d tasks of translating German–English, French–English, and Spanish–English pairs. Our integration gives statistically significant improvements over submission quality baseline systems. Section 7 concludes. 2. Previous Work 2.1 Phrase-Based SMT The phrase-based model (Koehn et al. 2003; Och and Ney 2004) segments a bilingual sentence pair into phrases that are continuous sequences of words. These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to its previous phrase (Tillmann and Zhang 2005) or block of phrases (Galley and Manning 2008). Phrase-based models memorize local dependencies such as short reorderings, translations of idioms, and the insertion and deletion of words sensitive to local context. Phrase-based systems, however, have the following drawbacks. Handling of Non-local Dependencies. Phrase-based SMT models dependencies between words and their translations inside of a phrase well. However, dependencies across phrase boundaries are ignored because of the strong phrasal independence assumption. Consider the bilingual sentence pair shown in Figure 1(a). Reordering of the German word stimmen is internal to the phras"
J15-2001,N10-1129,0,0.038666,"Missing"
J15-2001,W11-2123,0,0.0840724,"extracting the MTUs within the phrase pair and using phrase internal alignments. The OSM is used as a feature in the log-linear framework. We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1). 6.1 Baseline Our Moses (Koehn et al. 2007) baseline systems are based on the setup described in Durrani et al. (2013b). We trained our systems with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield 2011) used at runtime, distortion limit of 6, minimum Bayes-risk decoding (Kumar and Byrne 2004), cube pruning (Huang and Chiang 2007), and the no-reordering-over-punctuation heuristic. We used factored models (Koehn and Hoang 2007), for German–English and English–German. We trained the lexicalized reordering model (Koehn et al. 2005) with msd-bidirectional-fe settings. 6.2 Results Table 5 shows that the OSM results in higher gains than the lexicalized reordering model on top of a plain phrase-based baseline (Pb). The average improvement obtained using the lexicalized reordering model (Pblex ) over"
J15-2001,P07-1019,0,0.00922897,"linear framework. We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1). 6.1 Baseline Our Moses (Koehn et al. 2007) baseline systems are based on the setup described in Durrani et al. (2013b). We trained our systems with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield 2011) used at runtime, distortion limit of 6, minimum Bayes-risk decoding (Kumar and Byrne 2004), cube pruning (Huang and Chiang 2007), and the no-reordering-over-punctuation heuristic. We used factored models (Koehn and Hoang 2007), for German–English and English–German. We trained the lexicalized reordering model (Koehn et al. 2005) with msd-bidirectional-fe settings. 6.2 Results Table 5 shows that the OSM results in higher gains than the lexicalized reordering model on top of a plain phrase-based baseline (Pb). The average improvement obtained using the lexicalized reordering model (Pblex ) over the baseline (Pb) is 0.50. In comparison, the average improvement obtained by using the OSM (Pbosm ) over the baseline (Pb) is 0"
J15-2001,koen-2004-pharaoh,0,0.186183,"used in our model is the Source Gap Width. This feature only applies in the case of a discontinuous translation unit and computes the distance between the words of a gappy cept. Let f = f1 . . . , fi , . . . , fn be a gappy source cept where xi is the index of the ith source word in the cept f . The value of the gap-width penalty is calculated as: wj = n  xi − xi−1 − 1 i=2 4. MTU-Based Search We explored two decoding strategies in this work. Our first decoder complements the model and only uses minimal translation units in left-to-right stack-based decoding, similar to that used in Pharaoh (Koehn 2004a). The overall process can be roughly divided into the following steps: (i) extraction of translation units, (ii) future cost estimation, (iii) hypothesis extension, and (iv) recombination and pruning. The last two steps are repeated iteratively until all the words in the source sentence have been translated. Our hypotheses maintain the index of the last source word covered (j), the position of the right-most source word covered so far (Z), the number of open gaps, the number of gaps so far inserted, the previously generated operations, the generated target string, and the accumulated values"
J15-2001,W04-3250,1,0.372367,"Missing"
J15-2001,J10-4005,0,0.083559,"ambiguate good reorderings from the bad ones. Moreover, a vast majority of extracted phrases are singletons and the corresponding probability of orientation given phrase-pair estimates are based on a single observation. Due to sparsity, the model falls back to use one-word phrases instead, the orientation of which is ambiguous and can only be judged based on context that is ignored. This drawback has been addressed by Cherry (2013) by using sparse features for reordering models. Hard Distortion Limit. The lexicalized reordering model fails to filter out bad largescale reorderings effectively (Koehn 2010). A hard distortion limit is therefore required during decoding in order to produce good translations. A distortion limit beyond eight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and Quirk (2007) proposed improved future cost estimation to enable higher distortion limits in phrasal MT. Green, Galley, and Manning (2010) additionally proposed discriminative d"
J15-2001,2005.iwslt-1.8,1,0.753813,"Missing"
J15-2001,D07-1091,1,0.765644,"Missing"
J15-2001,N03-1017,1,0.0601065,"information available in phrases can be used to improve the search performance and translation quality. Finally, we probe whether integrating our model into the phrase-based SMT framework addresses the mentioned drawbacks and improves translation quality. Section 6 provides an empirical evaluation of our integration on six standard tasks of translating German–English, French–English, and Spanish–English pairs. Our integration gives statistically significant improvements over submission quality baseline systems. Section 7 concludes. 2. Previous Work 2.1 Phrase-Based SMT The phrase-based model (Koehn et al. 2003; Och and Ney 2004) segments a bilingual sentence pair into phrases that are continuous sequences of words. These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to its previous phrase (Tillmann and Zhang 2005) or block of phrases (Galley and Manning 2008). Phrase-based models memorize local dependencies such as short reorderings, translations of idioms, and the insertion and deletion of words sensitive to local context. Phrase-based systems, however, have the following drawbacks. Handling of Non-local Dependenc"
J15-2001,N04-1022,0,0.17139,"Missing"
J15-2001,J06-4004,0,0.0271931,"Missing"
J15-2001,2007.mtsummit-papers.43,0,0.0235298,"rry (2013) by using sparse features for reordering models. Hard Distortion Limit. The lexicalized reordering model fails to filter out bad largescale reorderings effectively (Koehn 2010). A hard distortion limit is therefore required during decoding in order to produce good translations. A distortion limit beyond eight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German–English and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and Quirk (2007) proposed improved future cost estimation to enable higher distortion limits in phrasal MT. Green, Galley, and Manning (2010) additionally proposed discriminative distortion models to achieve better translation accuracy than the baseline phrase-based system for a distortion limit of 15 words. Bisazza and Federico (2013) recently proposed a novel method to dynamically select which longrange reorderings to consider during the hypothesis extension process in a phrasebased decoder and showed an improvement in a German–English task by increasing the distortion limit to 18. Spurious Phrasal Segmenta"
J15-2001,W11-2124,0,0.297085,"odel to the OSM and to see whether we can improve the performance further by using both models together. Our integration of the OSM into Moses gave a statistically significant improvement over a competitive baseline system in most cases. In order to assess the contribution of improved reordering versus the contribution of better modeling with MTUs in the OSM-augmented Moses system, we removed the reordering operations from the stream of operations. This is equivalent to integrating the conventional N-gram tuple sequence model (Marino ˜ et al. 2006) into a phrasebased decoder, as also tried by Niehues et al. (2011). Small gains were observed in most cases, showing that much of the improvement obtained by the OSM is due to better reordering. Generalized Operation Sequence Model. The primary strength of the OSM over the lexicalized reordering model is its ability to take advantage of the wider contextual information. In an error analysis we found that the lexically driven OSM often falls back to very small context sizes because of data sparsity. We show that this problem can be addressed by learning operation sequences over generalized representations such as POS tags. The article is organized into seven"
J15-2001,P03-1021,0,0.10251,"nments. The corpus conversion algorithm (Algorithm 1) maps each bilingual sentence pair given its alignment into a unique sequence of operations deterministically, thus maintaining a 1-to-1 correspondence. This property of the model is useful because it addresses the spurious phrasal segmentation problem in phrase-based models. A phrase-based model assigns different scores to a derivation based on which phrasal segmentation is chosen. Unlike this, the OSM assigns only one score because the model does not suffer from spurious ambiguity. 3.6.1 Discriminative Model. We use a log-linear approach (Och 2003) to make use of standard features along with several novel features that we introduce to improve endto-end accuracy. We search for a target string E that maximizes a linear combination of feature functions: Eˆ = arg max E ⎧ J ⎨ ⎩ j=1 λj hj (F, E) ⎫ ⎬ ⎭ where λj is the weight associated with the feature hj (F, E). Apart from the OSM and standard features such as target-side language model, length bonus, distortion limit, and IBM lexical features (Koehn, Och, and Marcu 2003), we used the following new features: Deletion Penalty. Deleting a source word (Generate Source Only (X)) is a common oper"
J15-2001,J03-1002,0,0.0268454,"4.2 Initial Evaluation We evaluated our systems on German-to-English, French-to-English, and Spanish-toEnglish news translation for the purpose of development and evaluation. We used data from the eighth version of the Europarl Corpus and the News Commentary made available for the translation task of the Eighth Workshop on Statistical Machine Translation.7 The bilingual corpora contained roughly 2M bilingual sentence pairs, which we obtained by concatenating news commentary (≈ 184K sentences) and Europarl for the estimation of the translation model. Word alignments were generated with GIZA++ (Och and Ney 2003), using the grow-diag-final-and heuristic8 (Koehn et al. 2005). All data are lowercased, and we use the Moses tokenizer. We took news-test-2008 as the dev set for optimization and news-test 2009-2012 for testing. The feature weights are tuned with Z-MERT (Zaidan 2009). 4.2.1 Baseline Systems. We compared our system with (i) Moses9 (Koehn et al. 2007), (ii) Phrasal10 (Cer et al. 2010), and (iii) Ncode11 (Crego, Yvon, and Marino ˜ 2011). We used 7 http://www.statmt.org/wmt13/translation-task.html 8 We also tested other symmetrization heuristics such as “Union” and “Intersection” but found the GD"
J15-2001,J04-4002,0,0.218255,"rd translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM. 1. Introduction Statistical Machine Translation (SMT) advanced near the beginning of the century from word-based models (Brown et al. 1993) towards more advanced models that take contextual information into account. Phrase-based (Koehn, Och, and Marcu 2003; Och and Ney 2004) and N-gram-based (Casacuberta and Vidal 2004; Marino ˜ et al. 2006) models are two instances of such frameworks. Although the two models have some common properties, they are substantially different. The present work is a step towards combining the benefits and remedying the flaws of these two frameworks. Phrase-based systems have a simple but effective mechanism that learns larger chunks of translation called bilingual phrases.1 Memorizing larger units enables the phrase-based model to learn local dependencies such as short-distance reorderings, idiomatic collocations, and insertions and del"
J15-2001,P05-1069,0,0.175007,"l evaluation of our integration on six standard tasks of translating German–English, French–English, and Spanish–English pairs. Our integration gives statistically significant improvements over submission quality baseline systems. Section 7 concludes. 2. Previous Work 2.1 Phrase-Based SMT The phrase-based model (Koehn et al. 2003; Och and Ney 2004) segments a bilingual sentence pair into phrases that are continuous sequences of words. These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to its previous phrase (Tillmann and Zhang 2005) or block of phrases (Galley and Manning 2008). Phrase-based models memorize local dependencies such as short reorderings, translations of idioms, and the insertion and deletion of words sensitive to local context. Phrase-based systems, however, have the following drawbacks. Handling of Non-local Dependencies. Phrase-based SMT models dependencies between words and their translations inside of a phrase well. However, dependencies across phrase boundaries are ignored because of the strong phrasal independence assumption. Consider the bilingual sentence pair shown in Figure 1(a). Reordering of th"
J15-2001,P11-1086,0,0.0241791,"ns over a very competitive Moses baseline system. We showed that considering both translation and reordering context is important and ignoring reordering context results in a significant reduction in the performance. We also showed that an OSM based on surface forms suffers from data sparsity and that an OSM based on a generalized representation with part-of-speech tags improves the translation quality by considering a larger context. In the future we would like to study whether the insight of using minimal units for modeling and search based on composed rules would hold for hierarchical SMT. Vaswani et al. (2011) recently showed that a Markov model over the derivation history of minimal rules can obtain the same translation quality as using grammars formed with composed rules, which we believe is quite promising. Acknowledgments We would like to thank the anonymous reviewers and Andreas Maletti and Franc¸ois Yvon for their helpful feedback and suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreements 287658 (EU-Bridge) and 287688 (MateCat). Alexander Fraser was funded by Deutsche Forschungsgemeinsc"
J15-2001,N13-1002,0,0.0122981,"(TSM) of Marino ˜ et al. (2006), except that we use phrase-internal reordering rather than POS-based rewrite rules to do the source linearization. Table 6 shows an average improvement of just 0.13 on top of the baseline phrase-based system with lexicalized reordering, which is much lower than the 0.46 points obtained with the full operation sequence model. Bilingual translation models (without reordering) have been integrated into phrase-based systems before, either inside the decoder (Niehues et al. 2011) or to rerank the N-best candidate translations in the output of a phrase-based system (Zhang et al. 2013). Both groups reported improvements of similar magnitude when using a targetorder left-to-right TSM model for German–English and French–English translation with shared task data, but higher gains on other data sets and language pairs. Zhang et al. (2013) showed further gains by combining models with target and source left-to-right and right-to-left orders. The assumption of generating the target in monotonic order is a weakness of our work that can be addressed following Zhang et al. (2013). By generating MTUs in source order and allowing gaps and jumps on the target side, the model will be ab"
J15-2001,N10-1140,0,\N,Missing
J15-2001,2006.iwslt-evaluation.17,0,\N,Missing
J15-2001,P07-2045,1,\N,Missing
J15-2001,J04-2004,0,\N,Missing
J15-2001,2014.iwslt-evaluation.6,1,\N,Missing
J15-2001,2013.iwslt-evaluation.3,1,\N,Missing
J17-2003,J84-3009,0,0.122732,"Missing"
J17-2003,2012.iwslt-papers.6,0,0.0419685,"Missing"
J17-2003,W10-2407,0,0.0148307,"threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is generally small. The systems thus do not solely rely on it to mine transliteration pairs. They use both the list of transliteration pairs and unlabeled data for training. We are only aware of two unsupervised systems (requiring no labeled data). One of them was proposed by Fei Huang (2005). He extracts named entity pairs from a bilingual corpus, converts all words into Latin script by romanization, and classifies them into transliterations and non-transliterations based on the edit distance. This system still req"
J17-2003,2014.eamt-1.17,0,0.0188764,"ations. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) exploited the closeness between Urdu and Hindi using transliteration mining. They created synthetic Hindi-English parallel data by transliterating Urdu to Hindi. When they used the data for machine translation, it substantially improved translation quality. 10. Conclusions We presented a statistical model for mining transliteration pairs from a list of candidate pairs in a fully unsupervised fashion. Our model consists of sub-models for transliterations and non-transliterations that are interpolated. The transliteration sub-model is an n-gram model over 1–1, 0–1, and 1–0 character pairs and t"
J17-2003,P10-1048,1,0.805721,"language; (3) punctuation errors: one word has an additional punctuation symbol that makes the word pair a non-transliteration; (4) gold standard error: errors in the gold standard; (5) worst errors: word pairs that are far from being considered as transliteration pairs. Table 13 shows the number of errors of each type. The affix-based and pronunciation errors are the top errors made by the system. Both of them plus punctuation errors come under the broad definition of close transliterations. These word pairs are helpful because they provide useful character-level transliteration information. Durrani et al. (2010) incorporated our unsupervised transliteration mining system into machine translation. They showed that for language pairs with fewer transliterations, the close transliterations help to build a stronger transliteration system. Table 13 Types of errors made by the unsupervised transliteration mining system on the English/Arabic language pair. The numbers are based on randomly selected 100 word pairs that were wrongly classified by the mining system. Error Type Affix-based Error Gold Standard Error 372 Count Error Type Count Error Type Count 38 9 Pronunciation Error Worst Error 22 21 Punctuatio"
J17-2003,E14-4029,1,0.848098,"ost of the target language words in the cross-product list. The unsupervised system starts learning wrong transliterations because of their high frequency. Durrani et al. (2010) preprocess the candidate list before mining the transliteration pairs and remove words pairs with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) exploited the closeness between Urdu and Hindi using transliteration mining. They created synthetic Hindi-English parallel data by transliterating Urdu to Hindi. When they used the"
J17-2003,eisele-chen-2010-multiun,0,0.0445614,"Missing"
J17-2003,D11-1128,0,0.0606321,"Missing"
J17-2003,J93-1004,0,0.666995,"multigram sequences: p1 (e, f) = X p1 (a) (1) a∈Align(e,f ) where Align(e, f) returns all possible multigram sequences for the transliteration pair (e, f). In a unigram model, the probability of a multigram sequence a is the product of the probabilities of the multigrams it contains: p1 (a) = p1 (a1 a2 ...a|a |) = |a| Y p1 (aj ) (2) j=1 where |a |is the length of the sequence a. The non-transliteration sub-model generates source and target words that are unrelated. We model such pairs with two separate character unigram models (a source and a target model) whose probabilities are multiplied (Gale and Church 1993). Their parameters are learned from monolingual corpora and not updated during EM training. The non-transliteration sub-model is defined as follows: p2 (e, f) = pE (e)pF (f) (3) Q|e| Q|f| pE (e) = i=1 pE (ei ) and pF (f) = i=1 pF ( fi ) The transliteration mining model is obtained by interpolating the transliteration model p1 (e, f) and the non-transliteration model p2 (e, f): p(e, f) = (1 − λ )p1 (e, f) + λp2 (e, f) (4) where λ is the prior probability of non-transliteration. Interpolation with the non-transliteration model allows the transliteration model to concentrate on modeling translite"
J17-2003,W10-2405,0,0.150552,"it is attractive to extract transliteration pairs automatically from a noisy list of transliteration candidates, which can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration"
J17-2003,N03-1017,0,0.0598518,"Missing"
J17-2003,W15-3912,0,0.014598,"with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) exploited the closeness between Urdu and Hindi using transliteration mining. They created synthetic Hindi-English parallel data by transliterating Urdu to Hindi. When they used the data for machine translation, it substantially improved translation quality. 10. Conclusions We presented a statistical model for mining transliteration pairs from a list of candidate pairs in a fully unsupervised fashion. Our model consists of sub-models for transliterations and non-tr"
J17-2003,P04-1021,0,0.327325,"Missing"
J17-2003,W05-0809,0,0.0518742,"Missing"
J17-2003,W10-2412,0,0.014251,"ich can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is generally small. The systems thus do not solely rely on i"
J17-2003,W10-2408,0,0.0497492,"Missing"
J17-2003,J03-1002,0,0.0745413,"Missing"
J17-2003,I11-1015,1,0.907278,"utomatically from a noisy list of transliteration candidates, which can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is"
J17-2003,P11-1044,1,0.884668,"utomatically from a noisy list of transliteration candidates, which can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is"
J17-2003,P12-1049,1,0.795048,"Missing"
J17-2003,2013.iwslt-evaluation.8,1,0.860414,"ration Mining We observed that most of the word pairs in class 5 (worst errors) contain stop words. Because stop words are the most frequent words in a corpus, they occur with most of the target language words in the cross-product list. The unsupervised system starts learning wrong transliterations because of their high frequency. Durrani et al. (2010) preprocess the candidate list before mining the transliteration pairs and remove words pairs with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) ex"
J17-2003,W13-2228,1,0.851534,"ration Mining We observed that most of the word pairs in class 5 (worst errors) contain stop words. Because stop words are the most frequent words in a corpus, they occur with most of the target language words in the cross-product list. The unsupervised system starts learning wrong transliterations because of their high frequency. Durrani et al. (2010) preprocess the candidate list before mining the transliteration pairs and remove words pairs with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) ex"
J17-2003,P07-1109,0,0.0293419,"t distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is generally small. The systems thus do not solely rely on it to mine transliteration pairs. They use both the list of transliteration pairs and unlabeled data for training. We are only aware of two unsupervised systems (requiring no labeled data). One of them was proposed by Fei Huang (2005). He extracts named entity pairs from a bilingual corpus, converts all words into Latin script by romanization, and classifies them into transliterations and non-transliterations based on the edit distance. This s"
J17-2003,W06-1630,0,0.0426347,"ent and accurate and can be used in three different training settings—unsupervised, semi-supervised, and supervised learning. Our method directly learns character correspondences between two scripts from a noisy unlabeled list of word pairs which contains both transliterations and non-transliterations. When such a list is extracted from an aligned bilingual corpus, for instance, it contains, 1 There are other approaches to transliteration mining that exploit phonetic similarity between languages (Aransa, Schwenk, and Barrault 2012) and make use of temporal information available with the data (Tao et al. 2006). We do not discuss them here because they are out of the scope of this work. 350 Sajjad et al. Statistical Models for Transliteration Mining apart from transliterations, also both translations and misalignments, which we will call non-transliterations. Our statistical model interpolates a transliteration sub-model and a nontransliteration sub-model. The intuition behind using two sub-models is that the transliteration pairs and non-transliteration pairs, which make up the unlabeled training data, have rather different characteristics and need to be modeled separately. Transliteration word pai"
J17-2003,W12-4410,0,\N,Missing
J17-2003,J93-2003,0,\N,Missing
J17-2003,C96-2141,0,\N,Missing
J17-2003,W98-1005,0,\N,Missing
J17-2003,W07-0703,0,\N,Missing
J17-2003,J07-3002,1,\N,Missing
J17-2003,W02-0505,0,\N,Missing
J17-2003,C08-1068,0,\N,Missing
J17-2003,N07-1046,0,\N,Missing
J17-2003,W09-3528,0,\N,Missing
J17-2003,N07-1047,0,\N,Missing
J17-2003,P06-1103,0,\N,Missing
J17-2003,P07-1119,0,\N,Missing
J17-2003,P06-1010,0,\N,Missing
J17-2003,P08-1045,0,\N,Missing
J17-2003,P06-2025,0,\N,Missing
J17-2003,W09-3525,0,\N,Missing
J17-2003,W09-3522,0,\N,Missing
J17-2003,W07-0711,0,\N,Missing
J17-2003,W04-3250,0,\N,Missing
J17-2003,2010.iwslt-papers.7,0,\N,Missing
J17-2003,I08-8003,0,\N,Missing
J17-2003,J98-4003,0,\N,Missing
J17-2003,W10-2404,0,\N,Missing
J17-2003,P00-1056,0,\N,Missing
J17-2003,W05-0606,0,\N,Missing
J17-2003,W09-3507,0,\N,Missing
J17-2003,W09-3504,0,\N,Missing
J17-2003,P03-1021,0,\N,Missing
J17-3004,D11-1100,0,0.0752217,"Missing"
J17-3004,N15-1132,0,0.0592488,"Missing"
J17-3004,D14-1110,0,0.0491666,"Missing"
J17-3004,P16-1156,1,0.860909,"Missing"
J17-3004,N15-1184,0,0.120334,"Missing"
J17-3004,C14-1048,0,0.0167745,"e. Reisinger and Mooney (2010) and Huang et al. (2012) also presented methods that learn multiple embeddings per word by clustering the contexts. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy-based model was proposed by Bordes et al. (2012) to create disambiguated meaning embeddings, and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al. 2013a) to learn multiple word embeddings. Another interesting approach to create sense-specific word embeddings uses bilingual resources (Guo et al. 2014). The downside of this approach is that parallel data are needed. Although all these embeddings correspond to different word senses, there is no clear mapping between them and a resource like WordNet. 5.3 Sense Embeddings Related to Lexical Resources Recently, Bhingardive et al. (2015) used WordNet to create sense embeddings similar to the naive method in this article. They used these sense embeddings to extract the most frequent synset. Chen, Liu, and Sun (2014) modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They used glosses to initialize sense embedding"
J17-3004,I05-1067,0,0.045224,"nerally improve the quality of embeddings—see Huang et al. (2012) for a similar argument. We will provide a deeper evaluation of this in Section 4.4. 4.4 Word Similarity The results of the previous experiments motivate us to test the new embeddings also on Word Similarity test sets, namely, MC (Miller and Charles 1991), MEN (Bruni, Tran, and Baroni 2014), RG (Rubenstein and Goodenough 1965), SIMLEX (Hill, Reichart, and Korhonen 2014), RW (Luong, Socher, and Manning 2013), and WordSim-353 (Finkelstein et al. 2001) for English (using embeddings autoextended based on WordNet) and GUR65, GUR-350 (Gurevych 2005) and ZG-222 (Zesch and Gurevych 2006) for German (using embeddings autoextended based on GermaNet). Because the simple sum of the lexeme vectors (method AvgSim, line 7, Table 6) ignores the context and outperforms the underlying word embeddings (line 5), we expect a similar performance improvement on other Word Similarity test sets. Note that AutoExtend makes available three different word embeddings: 1. the original word embeddings W0 = W, i.e., the input to AutoExtend 2. the word embeddings W1 that we obtain when we add lexeme vectors of the encoding part (see Equation (30)) 3. the word embe"
J17-3004,W97-0802,0,0.357631,"Missing"
J17-3004,P12-1092,0,0.149917,"Missing"
J17-3004,P15-1010,0,0.0469498,"Missing"
J17-3004,N15-1070,0,0.036331,"Missing"
J17-3004,P14-1062,0,0.027775,"Missing"
J17-3004,D15-1242,0,0.126795,"Missing"
J17-3004,S01-1004,0,0.0138092,"Missing"
J17-3004,P13-2087,0,0.0274207,"s. This is done during embedding learning, in contrast to our post-processing method. Zhong et al. (2015) improved this by requiring the embedding vector not only to fit the structured constraints in the knowledge base but also to be equal to the embedding vector computed from the text description. 5.5 Post-processing Embeddings This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings—without (re)training them. There is an increasing amount of work on taking existing word embeddings and combining them with a lexical resource. Labutov and Lipson (2013) re-embedded existing word embeddings in supervised training, not to create new embeddings for senses or entities, but to obtain better predictive performance on a task while not changing the space of embeddings. A similar approach was chosen by Faruqui et al. (2015) and called retrofitting. That work is also related to our work in that it uses WordNet. However, it only uses the similarity relations in order to change embeddings for known objects (i.e., words). They did not use additive relations nor did they compute embeddings for non-word objects. Jauhar, Dyer, and Hovy (2015) also used the"
J17-3004,P15-1145,0,0.0323306,"mbeddings with BabelNet, which is a superset of WordNet. They used a state-of-the-art WSD system to generate a large sense annotated corpus that is used to train sense embeddings. In 613 Computational Linguistics Volume 43, Number 3 contrast, our approach can be used to improve WSD without relying on input from an existing WSD system. 5.4 Embeddings Using Lexical Resources Other work tried to combine distributed word representations and semantic resources to create better or specialized embeddings. These include the ADMM by Fried and Duh (2014) and the work of Wang, Mohamed, and Hirst (2015). Liu et al. (2015) also used WordNet to create ordinal similarity inequalities to extend the Skip-gram model into a Semantic Word Embedding model. In the Relation Constrained Model, Yu and Dredze (2014) used word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian, Gao, and Liu (2014) used not only semantic but also morphological and syntactic knowledge to compute more ¨ effective word embeddings. Cotterell, Schutze, and Eisner (2016) focus on generating embeddings for inflected forms not observed during training based on morphological resourc"
J17-3004,W13-3512,0,0.105097,"Missing"
J17-3004,W04-0807,0,0.132489,"Missing"
J17-3004,Q14-1019,0,0.135276,"Missing"
J17-3004,D14-1113,0,0.180999,"s case, there is only one embedding, so there is no difference between AvgSim and AvgSimC. It is interesting to note that even if we do not take the context into account (method AvgSim) the lexeme embeddings outperform the original word embeddings. As AvgSim simply adds up all lexemes of a word, this is equivalent to the motivation Table 6 Spearman correlation (ρ × 100) on SCWS. Best result per column in bold. Results significantly worse than the best (bold) result are marked † for α = 0.05 and ‡ for α = 0.10 (one-tailed Z-test). AvgSim † AvgSimC 1 2 3 4 Huang et al. (2012) Tian et al. (2014) Neelakantan et al. (2014) Chen, Liu, and Sun (2014) 62.8 – 67.2† 66.2‡ 65.7† 65.4† 69.3† 68.9† 5 6 7 words (word2vec) synsets lexemes 66.7† 63.2† 68.3† 66.7† 63.5† 70.2† 609 Computational Linguistics Volume 43, Number 3 we proposed in the beginning of the article (Equation (8)). Thus, replacing a word’s embedding by the sum of the embeddings of its senses could generally improve the quality of embeddings—see Huang et al. (2012) for a similar argument. We will provide a deeper evaluation of this in Section 4.4. 4.4 Word Similarity The results of the previous experiments motivate us to test the new embeddings also on Wo"
J17-3004,D14-1162,0,0.0870807,"Missing"
J17-3004,N10-1013,0,0.246145,"k and defends the house against another pack of zombies ... However, the data set is the closest we could find for sense similarity. Synset and lexeme embeddings are obtained by running AutoExtend. We set α = 0.2 and β = 0.2 based on Section 4.4. Lexeme embeddings are the natural choice for this task as human subjects are provided with two words and a context for each and then have to assign a similarity score. For completeness, we also run experiments for synsets. For each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (respectively, synset) vector l either as the simple average of the lexeme (respectively, synset) vectors l(ij) (respectively, s( j) ) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (respectively, synset) vectors weighted by cosine similarity to c (method AvgSimC). The latter method is supposed to give higher weights to lexemes that better fit the context. Table 6 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, including Huang et al. (2012) and Tian et al. (2014). Lexeme embeddings perform better than"
J17-3004,J98-1004,1,0.214639,"ork on embeddings (e.g., Bengio, Ducharme, and Vincent 2003; Mnih and Hinton 2007; Collobert et al. 2011; Mikolov et al. 2013a; Pennington, Socher, and Manning 2014), including methods that are SVD-based (Levy and Goldberg 2014; Stratos, Collins, and Hsu 2015). All of these models differ from AutoExtend in that they produce only a single embedding for each word, but all of them can be used as input for AutoExtend. 5.2 Sense Embeddings Not Related to Lexical Resources There are several approaches to finding embeddings for senses, variously called mean¨ ing, sense, and multiple word embeddings. Schutze (1998) created sense representations by clustering context representations derived from co-occurrence. The centroid of its cluster is used as a representation of a sense. Reisinger and Mooney (2010) and Huang et al. (2012) also presented methods that learn multiple embeddings per word by clustering the contexts. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy-based model was proposed by Bordes et al. (2012) to create disambiguated meaning embeddings, and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gra"
J17-3004,P15-1124,0,0.0611435,"Missing"
J17-3004,C14-1016,0,0.093962,"e test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (respectively, synset) vector l either as the simple average of the lexeme (respectively, synset) vectors l(ij) (respectively, s( j) ) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (respectively, synset) vectors weighted by cosine similarity to c (method AvgSimC). The latter method is supposed to give higher weights to lexemes that better fit the context. Table 6 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, including Huang et al. (2012) and Tian et al. (2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms. A simple baseline is to use the underlying word2vec embeddings directly (line 5). In this case, there is only one embedding, so there is no difference between AvgSim and AvgSimC. It is interesting to note that even if we do not take the context into account (method AvgSim) the lexeme embeddings outperform the original word embeddings. As AvgSim simply adds up"
J17-3004,P15-2075,0,0.316474,"Missing"
J17-3004,D14-1167,0,0.0352751,"o used WordNet to create ordinal similarity inequalities to extend the Skip-gram model into a Semantic Word Embedding model. In the Relation Constrained Model, Yu and Dredze (2014) used word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian, Gao, and Liu (2014) used not only semantic but also morphological and syntactic knowledge to compute more ¨ effective word embeddings. Cotterell, Schutze, and Eisner (2016) focus on generating embeddings for inflected forms not observed during training based on morphological resources. Wang et al. (2014) used Freebase to learn embeddings for entities and words. This is done during embedding learning, in contrast to our post-processing method. Zhong et al. (2015) improved this by requiring the embedding vector not only to fit the structured constraints in the knowledge base but also to be equal to the embedding vector computed from the text description. 5.5 Post-processing Embeddings This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings—without (re)training them. There is an increasing amount of work on taking existing word"
J17-3004,D15-1083,1,0.907268,"Missing"
J17-3004,P14-2089,0,0.0294789,"n 613 Computational Linguistics Volume 43, Number 3 contrast, our approach can be used to improve WSD without relying on input from an existing WSD system. 5.4 Embeddings Using Lexical Resources Other work tried to combine distributed word representations and semantic resources to create better or specialized embeddings. These include the ADMM by Fried and Duh (2014) and the work of Wang, Mohamed, and Hirst (2015). Liu et al. (2015) also used WordNet to create ordinal similarity inequalities to extend the Skip-gram model into a Semantic Word Embedding model. In the Relation Constrained Model, Yu and Dredze (2014) used word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian, Gao, and Liu (2014) used not only semantic but also morphological and syntactic knowledge to compute more ¨ effective word embeddings. Cotterell, Schutze, and Eisner (2016) focus on generating embeddings for inflected forms not observed during training based on morphological resources. Wang et al. (2014) used Freebase to learn embeddings for entities and words. This is done during embedding learning, in contrast to our post-processing method. Zhong et al. (2015)"
J17-3004,W06-1104,0,0.0454268,"of embeddings—see Huang et al. (2012) for a similar argument. We will provide a deeper evaluation of this in Section 4.4. 4.4 Word Similarity The results of the previous experiments motivate us to test the new embeddings also on Word Similarity test sets, namely, MC (Miller and Charles 1991), MEN (Bruni, Tran, and Baroni 2014), RG (Rubenstein and Goodenough 1965), SIMLEX (Hill, Reichart, and Korhonen 2014), RW (Luong, Socher, and Manning 2013), and WordSim-353 (Finkelstein et al. 2001) for English (using embeddings autoextended based on WordNet) and GUR65, GUR-350 (Gurevych 2005) and ZG-222 (Zesch and Gurevych 2006) for German (using embeddings autoextended based on GermaNet). Because the simple sum of the lexeme vectors (method AvgSim, line 7, Table 6) ignores the context and outperforms the underlying word embeddings (line 5), we expect a similar performance improvement on other Word Similarity test sets. Note that AutoExtend makes available three different word embeddings: 1. the original word embeddings W0 = W, i.e., the input to AutoExtend 2. the word embeddings W1 that we obtain when we add lexeme vectors of the encoding part (see Equation (30)) 3. the word embeddings W2 that we obtain when we add"
J17-3004,D15-1031,0,0.0204221,"Yu and Dredze (2014) used word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian, Gao, and Liu (2014) used not only semantic but also morphological and syntactic knowledge to compute more ¨ effective word embeddings. Cotterell, Schutze, and Eisner (2016) focus on generating embeddings for inflected forms not observed during training based on morphological resources. Wang et al. (2014) used Freebase to learn embeddings for entities and words. This is done during embedding learning, in contrast to our post-processing method. Zhong et al. (2015) improved this by requiring the embedding vector not only to fit the structured constraints in the knowledge base but also to be equal to the embedding vector computed from the text description. 5.5 Post-processing Embeddings This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings—without (re)training them. There is an increasing amount of work on taking existing word embeddings and combining them with a lexical resource. Labutov and Lipson (2013) re-embedded existing word embeddings in supervised training, not to create new"
J17-3004,P10-4014,0,0.0588266,"chutze 2015). 4. Experiments and Evaluation We evaluate AutoExtend embeddings on the following tasks: WSD, Entity Linking, Word-in-Context Similarity, Word Similarity, and Synset Alignment. Our results depend directly on the quality of the underlying word embeddings. We would expect even better evaluation results as word representation learning methods improve. Using a new and improved set of underlying embeddings in AutoExtend is simple: It is a simple switch of the input file that contains the word embeddings. 4.1 Word Sense Disambiguation We use IMS (It Makes Sense) for our WSD evaluation (Zhong and Ng 2010). As in the original paper, preprocessing consists of sentence splitting, tokenization, POS tagging, and lemmatization; the classifier is a linear SVM. In our experiments (Table 4), we run IMS with each feature set by itself to assess the relative strengths of individual feature sets (lines 1–7) and on feature set combinations to determine which combination is best for WSD (lines 8, 12–15). We use SensEval-2 as development set for SensEval-3 and vice versa. This gives us a weighting of α = β = 0.4 for both sets. IMS implements three standard WSD feature sets: part of speech (POS), surrounding"
J17-3004,J15-4004,0,\N,Missing
J17-3004,P94-1019,0,\N,Missing
J17-3004,J14-1003,0,\N,Missing
J98-1004,P91-1019,0,0.0408175,"That is, we will not be concerned with the sense-labeling c o m p o n e n t of w o r d sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on w o r d sense discrimination also liberates us of a serious constraint c o m m o n to other work on w o r d sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. What makes our approach unique is that, since we n a r r o w the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses. * Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo A"
J98-1004,W96-0104,0,0.122144,"beling c o m p o n e n t of w o r d sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on w o r d sense discrimination also liberates us of a serious constraint c o m m o n to other work on w o r d sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. What makes our approach unique is that, since we n a r r o w the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses. * Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Lingu"
J98-1004,P97-1010,0,0.0235994,"ikely to be useful in IR, are not taken into account and that frequent senses are further subdivided. Good evidence for the potential utility of disambiguation in information retrieval was provided by Krovetz and Croft (1992). They showed that there is a considerable amount of ambiguity even in technical text (which is often assumed to be less ambiguous than nonspecialized writing). Many technical terms have nontechnical meanings that are used in addition to more specialized senses even in technical text (e.g., window and application in computer magazines, convertible in automobile magazines [Krovetz 1997]). Krovetz and Croft also showed that sense mismatches (i.e., spurious matching words that were used in different senses in query and document) occurred significantly more often in nonrelevant than in relevant documents. This suggests that eliminating spurious matches could improve the separation between nonrelevant and relevant documents and hence the overall quality of retrieval results. In order to show that context-group discrimination is an approach to disambiguation that is beneficial in information retrieval, we will now summarize the experiment presented in Schfitze and Pedersen (1995"
J98-1004,W93-0102,0,0.0792356,"Missing"
J98-1004,H93-1051,0,0.0253934,"Missing"
J98-1004,C94-1049,0,0.0714722,"ually is. Focusing solely on w o r d sense discrimination also liberates us of a serious constraint c o m m o n to other work on w o r d sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. What makes our approach unique is that, since we n a r r o w the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses. * Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 We therefore call our approach automatic w o r d sense discrimination, since we do not require manually constructed sources of knowledge. In m a n y applicatio"
J98-1004,W97-0322,0,0.305411,"vectors that reflect syntactic or subcategorization behavior of different senses, such as the output of a shallow parser as used in Pereira, Tishby, and Lee (1993). For example, one good indicator of the two senses of the word interest is a preposition occurring to its right. The phrase interest in invokes the 'feeling of attention' sense, the phrase interest on, the sense 'charge on borrowed money.' It seems plausible that performance could be improved for words whose senses are less sensitive to topical distinctions if such &quot;proximity&quot; information is integrated. In some recent experiments, Pedersen and Bruce (1997) have used proximity features (tags of close words and the presence or absence of close functions words 116 Schfttze Automatic Word Sense Discrimination and content words) with some promising results. This suggests that a combination of the topical features used here and proximity features m a y give optimal performance of context-group discrimination. 4 We have used only one source of information (topical features) in the interest of simplicity, not because we see any inherent advantage of topical features compared to a combination of multiple sources of evidence. Our justification for the ba"
J98-1004,P93-1024,0,0.147407,"Missing"
J98-1004,C92-2070,0,0.0361287,"sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on w o r d sense discrimination also liberates us of a serious constraint c o m m o n to other work on w o r d sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. What makes our approach unique is that, since we n a r r o w the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses. * Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Ling"
J98-1004,P95-1026,0,0.390048,"Missing"
J98-1004,P94-1038,0,\N,Missing
J98-1004,P93-1022,0,\N,Missing
J98-1004,P94-1020,0,\N,Missing
J98-1004,J92-4003,0,\N,Missing
J98-1004,P91-1034,0,\N,Missing
J98-1004,P91-1017,0,\N,Missing
K15-1017,W06-1655,0,0.0677899,"as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 English Finnish German Indonesian Turkish Zulu 878k 2,928k 2,338k 88k 617k 123k Train+Tune+Dev Train Tune Dev 800 100 100 8"
K15-1017,W06-2920,0,0.0387228,"e labels are needed for many tasks, for instance in sentiment analysis detecting morphologically encoded negation, as in Turkish, is crucial. In other words, for many applications UMS is insufficient. (ii) The LMS framework allows us to learn a probabilistic model of morphotactics. Working with LMS results in higher UMS accuracy. So even in applications that only need segments and no labels, LMS is beneficial. Note that the concatenation of labels across segments yields a bundle of morphological attributes similar to those found in the CoNLL datasets often used to train morphological taggers (Buchholz and Marsi, 2006)— thus LMS helps to unify UMS and morphological tagging. We believe that LMS is a needed extension of current work in morphological segmentation. Our framework concisely allows the model to capture interdependencies among various morphemes and model relations between entire mor2 Like en in English open, en in German Offen is part of the root. 165 5 4 3 2 1 0 German English P REFIX :D ERIV:V ERB P REFIX :D ERIV:V ERB P REFIX :D ERIV:V ERB P REFIX :D ERIV P REFIX S EGMENT Ent de ROOT:N OUN ROOT:N OUN ROOT:N OUN ROOT ROOT S EGMENT eis frost S UFFIX :D ERIV:N OUN S UFFIX :D ERIV:N OUN S UFFIX :D E"
K15-1017,chrupala-etal-2008-learning,0,0.145492,"Missing"
K15-1017,W02-1001,0,0.168778,"7 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be sp"
K15-1017,W02-0603,0,0.824231,"present memory-based approaches to discriminative learning of morphological segmentation. This is the previous work most similar to our work. They address the problem of LMS. We distinguish our work from theirs in that we define a cross-lingual schema for defining a hierarchical tagset for LMS. Morever, we tackle the problem with a feature-rich log-linear model, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesia"
K15-1017,W04-0106,0,0.110462,"ence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (201"
K15-1017,W05-0701,0,0.101986,"Missing"
K15-1017,P08-1115,0,0.064437,"s: (i) morphological segmentation, (ii) stemming and (iii) morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised di"
K15-1017,D13-1032,1,0.778252,"y predicting LMS and then discarding the labels. Our primary baseline is the state-of-the-art supervised system CRF-M ORPH of Ruokolainen et al. (2013). We ran the version of the system that the authors published on their website.8 We optimized the model’s two hyperparameters on Tune: the number of epochs and the maximal length of ngram character features. The system also supports Harris’s letter successor variety (LSV) features (Section 5), extracted from large unannotated corpora, our second baseline. For completeness, we also compare C HIPMUNK with a first-order CRF and a higher-order CRF (Müller et al., 2013), both used the same n-gram features as CRF-M ORPH, but without the LSV features.9 We evaluate all models using the traditional macro F1 of the segmentation boundaries. Discussion. The UMS results on held-out data are displayed in Table 5. Our most complex model beats the best baseline by between 1 (German) and 3 (Finnish) points F1 on all six languages. We additionally provide extensive ablation studies to highlight the contribution of our novel features. We find that the properties of each specific language highly influences which features are most effective. For the agglutinative languages,"
K15-1017,N13-1004,0,0.0261228,"For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised discriminative model 1 Terminological notes: We use root to refer to a morpheme with concrete mea"
K15-1017,D14-1095,0,0.0748928,"1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised discriminative model 1 Terminological notes: We use root to refer to a morpheme with concrete meaning, stem to refer to the concatenation of all roots and derivational affixes, root dete"
K15-1017,J01-2001,0,0.672752,"s (1999) and Marsi et al. (2005) present memory-based approaches to discriminative learning of morphological segmentation. This is the previous work most similar to our work. They address the problem of LMS. We distinguish our work from theirs in that we define a cross-lingual schema for defining a hierarchical tagset for LMS. Morever, we tackle the problem with a feature-rich log-linear model, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167"
K15-1017,W04-3236,0,0.00874839,"ish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 English Finnish German Indonesian Turkish Zulu 878k 2,928k 2,338k 88k 617k 123k Train+Tune+Dev Train Tune Dev 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 Test 694 835 751 2500 763 9040 Table"
K15-1017,C14-1111,0,0.0697741,"ate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. Th"
K15-1017,N09-1024,0,0.0623345,"dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese"
K15-1017,W13-3504,0,0.276986,"h Zulu Table 2: Sizes of the various affix gazetteers. 4 119,839 6,690,417 364,564 35,269 80,261 73,525 Table 3: Number of words covered by the respective ASPELL dictionary Features features that fire if a segment is valid for a given spell checker. Spell-check features function effectively as a proxy for a “root detector”. We use the open-source ASPELL dictionaries as they are freely available in 91 languages. Table 3 shows the coverage of these dictionaries. Integrating the Features. Our model uses the features discussed in this section and additionally the simple n-gram context features of Ruokolainen et al. (2013). The n-gram features look at variable length substrings of the word on both the right and left side of each potential boundary. We create conjunctive features from the cross-product between the morphotactic tagset (Section 2) and the features. We introduce several novel features for LMS. We exploit existing resources, e.g., spell checkers and Wiktionary, to create straightforward and effective features and we incorporate ideas from related areas: named-entity recognition (NER) and morphological tagging. Affix Features and Gazetteers. In contrast to syntax and semantics, the morphology of a la"
K15-1017,W10-2210,0,0.124893,"llowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the expon"
K15-1017,E14-4017,0,0.108741,". Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008"
K15-1017,W10-2211,0,0.0842879,"from Wikipedia and the corpus of Krisnawati and Schulz (2013). Table 4 shows the important statistics of our datasets. In all evaluations, we use variants of the standard MorphoChallenge evaluation approach. Importantly, for word types with multiple correct segmentations, this involves finding the maximum score by comparing our hypothesized segmentation with each correct segmentation, as is standardly done in MorphoChallenge. Experiments We experimented on six languages from diverse language families. The segmentation data for English, Finnish and Turkish was taken from MorphoChallenge 2010 (Kurimo et al., 2010).5 Despite typically being used for UMS tasks, the MorphoChallenge datasets do contain morpheme level 6 https://github.com/desmond86/ Indonesian-English-Bilingual-Corpus 7 We used both Tune and Dev in order to both optimize hyperparameters on held-out data (Tune) and perform qualitative error analysis on separate held-out data (Dev). 5 http://research.ics.aalto.fi/events/ morphochallenge2010/ 168 CRF-M ORPH CRF-M ORPH +LSV First-order CRF Higher-order CRF C HIPMUNK C HIPMUNK +Morph C HIPMUNK +Affix C HIPMUNK +Dict C HIPMUNK +Dict,+Affix,+Morph English 83.23 84.45 84.66 84.66 84.40 83.27 83.81"
K15-1017,W06-2918,0,0.0380945,"resources, e.g., spell checkers and Wiktionary, to create straightforward and effective features and we incorporate ideas from related areas: named-entity recognition (NER) and morphological tagging. Affix Features and Gazetteers. In contrast to syntax and semantics, the morphology of a language is often simple to document and a list of the most common morphs can be found in any good grammar book. Wiktionary, for example, contains affix lists for all the six languages used in our experiments.4 Providing a supervised learner with such a list is a great boon, just as gazetteer features aid NER (Smith and Osborne, 2006)— perhaps even more so since suffixes and prefixes are generally closed-class; hence these lists are likely to be comprehensive. These features are binary and fire if a given substring occurs in the gazetteer list. In this paper, we simply use suffix lists from English Wiktionary, except for Zulu, for which we use a prefix list, see Table 2. We also include a feature that fires on the conjunction of tags and substrings observed in the training data. In the level 5 tagset this allows us to link all allomorphs of a given morpheme. In the lower level tagsets, this links related morphemes. Virpioj"
K15-1017,C10-1115,0,0.0740834,"Missing"
K15-1017,P99-1037,0,0.623548,"Missing"
K15-1017,P08-1101,0,0.0645359,"olainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 English Finnish German Indonesian Turkish Zulu 878k 2,928k 2,338k 88k 617k 123k Train+Tune+Dev Train Tune Dev 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 Test 694 835 751 2500 763 9040 Table 4: Dataset sizes (numbe"
K15-1021,P12-1092,0,0.0486454,"lov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repreWe propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tas"
K15-1021,D12-1050,0,0.0682553,"ble-range features of sentences and (ii) exploring the combination of multiple public embedding versions to initialize words in sentences. We also employ two “tricks” to further enhance system performance: mutual learning and pretraining. In remaining parts, Section 2 presents related work. Section 3 gives details of our classification model. Section 4 introduces two tricks that enhance system performance: mutual-learning and pretraining. Section 5 reports experimental results. Section 6 concludes this work. 2 Related Work Much prior work has exploited deep neural networks to model sentences. Blacoe and Lapata (2012) represented a sentence by element-wise addition, multiplication, or recursive autoencoder over embeddings of component single words. Yin and Sch¨utze (2014) ex205 al., 2014). In our work, there is no limit to the type of embedding versions we can use and they leverage not only the diversity of corpora, but also the different principles of learning algorithms. 3 Model Description We now describe the architecture of our model MVCNN, illustrated in Figure 1. Multichannel Input. The input of MVCNN includes multichannel feature maps of a considered sentence, each is a matrix initialized by a diffe"
K15-1021,P14-1062,0,0.693581,"essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks 204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics tended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu e"
K15-1021,D14-1181,0,0.232089,"g them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks 204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics tended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu et al. (2014)"
K15-1021,P03-1054,0,0.0501969,"ially helpful for pretraining the embeddings of unknown words. 5 5.2 Standard Sentiment Treebank (Socher et al., 2013). This small-scale dataset includes two tasks predicting the sentiment of movie reviews. The output variable is binary in one experiment and can have five possible outcomes in the other: {negative, somewhat negative, neutral, somewhat positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators in the same way as the sentences were labeled. Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014). Sentiment1402 (Go et al., 2009). This is a large-scale dataset of tweets about sentiment classification, where a tweet is automatically labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and"
K15-1021,C14-1008,0,0.0186952,"Missing"
K15-1021,P12-2018,0,0.103568,"Missing"
K15-1021,P04-1035,0,0.028464,"labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. We preprocess the tweets minimally as follows. 1) The equivalence class symbol “url” (resp. “username”) replaces all URLs (resp. all words that start with the @ symbol, e.g., @thomasss). 2) A sequence of k > 2 repetitions of a letter c (e.g., “cooooooool”) is replaced by two occurrences of c (e.g., “cool”). 3) All tokens are lowercased. Subj. Subjectivity classification dataset3 released by (Pang and Lee, 2004) has 5000 subjective sentences and 5000 objective sentences. We report the result of 10-fold cross validation as baseline systems did. Experiments We test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments. 5.1 Datasets and Experimental Setup Hyperparameters and Training In each of the experiments, the top of the network is a logistic regression that predicts the probability distribution over classes given the input sentence. The network is trained to minimize cross-entrop"
K15-1021,P14-3006,1,0.68726,"Missing"
K15-1021,D14-1162,0,0.0988936,"t al. (2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task. However, these two papers either learned word representations on the same corpus (Turian et al., 2010) or enhanced the embedding quality by extending training corpora, not learning algorithms (Luo et sentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&W (Collobert et al., 2011) in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words. Our expectation is that the combination of these embedding versions, trained by different NNs on different corpora, should contain more information than each version individually. We want to leverage this diversity of different embedding versions to extract higher quality sentence features and thereby improve sentence classification performance. The letters “M” and “V” in the name “MVCNN” of our arc"
K15-1021,D11-1014,0,0.0184745,"ddition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks 204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics tended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu et al. (2014) used one layer of co"
K15-1021,D12-1110,0,0.0708373,"Missing"
K15-1021,D13-1170,0,0.270778,"ures layer by layer, then those features are justified by all constituent words. During pretraining, all the model parameters, including mutichannel input, convolution parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks. In sum, this pretraining is designed to produce good initial values for both model parameters and word embeddings. It is especially helpful for pretraining the embeddings of unknown words. 5 5.2 Standard Sentiment Treebank (Socher et al., 2013). This small-scale dataset includes two tasks predicting the sentiment of movie reviews. The output variable is binary in one experiment and can have five possible outcomes in the other: {negative, somewhat negative, neutral, somewhat positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators"
K15-1021,P10-1040,0,0.0401607,"2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repreWe propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-"
K17-2002,E17-1005,0,0.00549682,"ach prepended with a + X s∈S log pθ (s |e(s)), where D is the labeled training data, with each example consisting of a lemma l, a morphological tag t and an inflected form w, and S is a set of autoencoding examples. The function e represents the encoder, which depends on θ. In the setting with no outside resources we experiment with two variants of the sequence autoencoder. The first of these, AE-TD, uses the 3 Multitask learning for NLP using encoder-decoder networks typically assumes that the separate tasks either have distinct encoders, distinct decoders, or both (e.g., Luong et al., 2016; Alonso and Plank, 2017; Bollmann et al., 2017). Here, we use the same encoder and decoder for both tasks. In preliminary experiments, we tried pre-training the autoencoder instead (see, e.g., Dai and Le, 2015; Kamper et al., 2015), but found that interspersing examples gave a clear advantage. 2 Of course, the morphological variants we find may be noisy, so a better method for identifying these might still improve upon random strings. 32 Word 1⇔Word 2 deceive⇔deception receive⇔reception perceive⇔perception conceive⇔conception lemmas and target forms in the training data as inputs to the autoencoder, yielding up to t"
K17-2002,W02-0606,0,0.108183,"oduce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Fi"
K17-2002,P17-1031,0,0.0164218,"s∈S log pθ (s |e(s)), where D is the labeled training data, with each example consisting of a lemma l, a morphological tag t and an inflected form w, and S is a set of autoencoding examples. The function e represents the encoder, which depends on θ. In the setting with no outside resources we experiment with two variants of the sequence autoencoder. The first of these, AE-TD, uses the 3 Multitask learning for NLP using encoder-decoder networks typically assumes that the separate tasks either have distinct encoders, distinct decoders, or both (e.g., Luong et al., 2016; Alonso and Plank, 2017; Bollmann et al., 2017). Here, we use the same encoder and decoder for both tasks. In preliminary experiments, we tried pre-training the autoencoder instead (see, e.g., Dai and Le, 2015; Kamper et al., 2015), but found that interspersing examples gave a clear advantage. 2 Of course, the morphological variants we find may be noisy, so a better method for identifying these might still improve upon random strings. 32 Word 1⇔Word 2 deceive⇔deception receive⇔reception perceive⇔perception conceive⇔conception lemmas and target forms in the training data as inputs to the autoencoder, yielding up to twice as many autoencoder"
K17-2002,N15-1186,0,0.0377683,"mples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Finally, all word pairs with the same differences"
K17-2002,P17-1029,0,0.0531855,"Missing"
K17-2002,J11-2002,0,0.0734975,"Missing"
K17-2002,P17-1182,1,0.740658,"Missing"
K17-2002,W16-2010,1,0.383881,"Missing"
K17-2002,W17-4111,1,0.800423,"Missing"
K17-2002,W10-2211,0,0.0774086,"ion training pairs (any duplicate lemmas or target forms are included only once). Our second autoencoder variant, AE-RS, uses randomly generated strings as inputs, which means we can produce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities"
K17-2002,W02-0604,0,0.507225,"mber of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Finally, all word pairs wi"
K17-2002,W00-0712,0,0.0402176,"puts, which means we can produce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Tabl"
K17-2003,K17-2001,0,0.0969552,"Missing"
K17-2003,W16-2007,0,0.144695,"Missing"
K17-2003,2015.iwslt-evaluation.11,0,0.0461088,"d to thus make use of all known forms. However, they suppose to be able to compute and use good estimates for the probabilities p(fi (wl )|fj (wl )) for source form fj (wl ) and target form fi (wl ), since they use at least 632 entire paradigms per part of speech and language for training. Using a minimum spanning tree, they approximate a solution to the maximum-a-posteriori 7 Fine-Tuning for Multi-Source Input For sequence-to-sequence models for neural machine translation, it has been shown that specialized models for a certain domain are able to obtain better performances than general ones (Luong and Manning, 2015). One way to perform such a domain adaptation is fine-tuning: a general model, which has been trained on out-of-domain data, is further trained on (newly) available indomain data, cf. Figure 3. This brings the conditional probability p(y1 , ..., ym |x1 , ..., xn ) for an output sequence (y1 , ..., ym ) given an input sequence (x1 , ..., xn ) closer to the target distribution. Here, we propose to improve multi-source morphological reinflection by treating each paradigm as a separate domain and performing “domain adaptation” everytime a new paradigm should be completed by the model. In particula"
K17-2003,W14-4012,0,0.0687042,"Missing"
K17-2003,P17-1029,0,0.0817964,"Missing"
K17-2003,D14-1179,0,0.0574271,"Missing"
K17-2003,P17-2061,0,0.0413652,"Missing"
K17-2003,E17-2120,0,\N,Missing
michelbacher-etal-2010-building,1998.amta-tutorials.5,0,\N,Missing
michelbacher-etal-2010-building,D09-1124,0,\N,Missing
michelbacher-etal-2010-building,P99-1067,0,\N,Missing
michelbacher-etal-2010-building,J08-4004,0,\N,Missing
michelbacher-etal-2010-building,zesch-etal-2008-extracting,0,\N,Missing
michelbacher-etal-2010-building,P99-1004,0,\N,Missing
N10-1113,D08-1092,0,0.0835525,"e necessary to resolve this ambiguity. Work on projecting semantic roles (Pad´o and Lapata, 2009; Fung et al., 2007) requires both syntactic parsing and semantic role labeling and is concerned with filling in the complete information in a semantic frame. Our approach is simpler and concerned only with syntactic disambiguation, not semantic projection. We focus only on difficult cases of subject-object ambiguity and although we do not always make a prediction, we obtain levels of precision that projection approaches making no use of knowledge of German syntax cannot achieve. In bitext parsing, Burkett and Klein (2008) and Fraser et al. (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated treebanks. We focus only on subjectobject disambiguation in German, and annotated a new gold standard. We work on sentences that a partial parser has determined to be ambiguous. Fossum and Knight (2008) and Huang et al. (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. The latter work integrated the PP-attachment c"
N10-1113,J96-2004,0,0.0529047,"ng a graphical user interface (GUI). The GUI showed the ambiguous words in context and gave the annotator four different subject-object labels to choose from for each ambiguous word: subject, object, expletive es and none. Because the syntactic expletive “es” (English gloss: ‘it’) is frequent in German, as in “es scheint zu regnen” ‘it appears to be raining,’ we created a separate label for expletive “es”, which is not treated as a subject.2 The statistics are shown in table 1. 1000 sentences were annotated by all four annotators. Inter-annotator agreement was sufficient (κ = 0.77 on average (Carletta, 1996)). Evaluation Measures. The output of our algorithm labels each word that FSPar classified as ambiguous with one of the three possible labels subject, 1 Figure 2: Disambiguation Algorithm We used standard heuristics for improving word alignment (Och and Ney, 2003; Koehn et al., 2003), but there were many misalignments of ambiguous 738 FSPar has a very high precision in detecting subject-object ambiguities, as can be seen in Table 1 (approximately 0.955, the sum of two left columns divided by sum of all cells). We tried to get an idea of recall using the smaller gold standard. We made conservat"
N10-1113,2008.amta-srw.2,0,0.480568,"do not always make a prediction, we obtain levels of precision that projection approaches making no use of knowledge of German syntax cannot achieve. In bitext parsing, Burkett and Klein (2008) and Fraser et al. (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated treebanks. We focus only on subjectobject disambiguation in German, and annotated a new gold standard. We work on sentences that a partial parser has determined to be ambiguous. Fossum and Knight (2008) and Huang et al. (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. The latter work integrated the PP-attachment constraint (detected from the Chinese translation) directly into an English shift-reduce parser. As we have shown in the labeling experiment, integrating our subjectobject disambiguation into BitPar could result in further increases beyond 100-best reranking. 6 Conclusion We demonstrated the utility of bitext-based disambiguation of grammatical roles. We automatically created a large corpus of 164,874 disambiguated subject-objec"
N10-1113,E09-1033,1,0.840854,"Missing"
N10-1113,2007.tmi-papers.10,0,0.0718207,"Missing"
N10-1113,D09-1127,0,0.0380513,"on, we obtain levels of precision that projection approaches making no use of knowledge of German syntax cannot achieve. In bitext parsing, Burkett and Klein (2008) and Fraser et al. (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated treebanks. We focus only on subjectobject disambiguation in German, and annotated a new gold standard. We work on sentences that a partial parser has determined to be ambiguous. Fossum and Knight (2008) and Huang et al. (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. The latter work integrated the PP-attachment constraint (detected from the Chinese translation) directly into an English shift-reduce parser. As we have shown in the labeling experiment, integrating our subjectobject disambiguation into BitPar could result in further increases beyond 100-best reranking. 6 Conclusion We demonstrated the utility of bitext-based disambiguation of grammatical roles. We automatically created a large corpus of 164,874 disambiguated subject-object decisions with a preci"
N10-1113,N03-1017,0,0.00268218,") is frequent in German, as in “es scheint zu regnen” ‘it appears to be raining,’ we created a separate label for expletive “es”, which is not treated as a subject.2 The statistics are shown in table 1. 1000 sentences were annotated by all four annotators. Inter-annotator agreement was sufficient (κ = 0.77 on average (Carletta, 1996)). Evaluation Measures. The output of our algorithm labels each word that FSPar classified as ambiguous with one of the three possible labels subject, 1 Figure 2: Disambiguation Algorithm We used standard heuristics for improving word alignment (Och and Ney, 2003; Koehn et al., 2003), but there were many misalignments of ambiguous 738 FSPar has a very high precision in detecting subject-object ambiguities, as can be seen in Table 1 (approximately 0.955, the sum of two left columns divided by sum of all cells). We tried to get an idea of recall using the smaller gold standard. We made conservative assumptions about recall errors which we manually checked on a small sample, details are omitted. Using these assumptions led to an estimate for recall of 0.733, but true recall is likely higher. 2 German “es” is also frequently used as a non-expletive, where it can take a syntac"
N10-1113,2005.mtsummit-papers.11,0,0.0173974,"ambiguities are much less frequent in languages that possess a less flexible syntax than German. In English, the translation of the sentence “Die Maus jagt die Katze” is not ambiguous. If we have access to this translation, we can use this information to disambiguate the German sentence. The English translation is viewed as a surrogate for both contextual knowledge from the text and for world knowledge. We present a method for disambiguating the subject and object roles in German sentences. We use Algorithm Data and Word Alignment. We use the aligned English and German sentences in Europarl (Koehn, 2005) for our experiments. The corpus contains long and complex sentences. To establish translational correspondence between parallel sentences we use GIZA++ (Och and Ney, 2003). Its input is a tokenized parallel corpus. We lemmatized the text prior to aligning it. Procedure. Figure 1 shows the architecture of our system. The boxes signify data sets, while the lines are processes applied to the data sets. The paper presents two applications. The first is the creation of a set of disambiguated German sentences (which involves word alignments in the upper right corner, and the use of parsers in the m"
N10-1113,J03-1002,0,0.00939333,"” is not ambiguous. If we have access to this translation, we can use this information to disambiguate the German sentence. The English translation is viewed as a surrogate for both contextual knowledge from the text and for world knowledge. We present a method for disambiguating the subject and object roles in German sentences. We use Algorithm Data and Word Alignment. We use the aligned English and German sentences in Europarl (Koehn, 2005) for our experiments. The corpus contains long and complex sentences. To establish translational correspondence between parallel sentences we use GIZA++ (Och and Ney, 2003). Its input is a tokenized parallel corpus. We lemmatized the text prior to aligning it. Procedure. Figure 1 shows the architecture of our system. The boxes signify data sets, while the lines are processes applied to the data sets. The paper presents two applications. The first is the creation of a set of disambiguated German sentences (which involves word alignments in the upper right corner, and the use of parsers in the middle of the graphic). We also present a reranking of the N -best parses produced by BitPar (Schmid, 2004), a state of the art statistical parser (bottom of the graphic). F"
N10-1113,E03-1087,0,0.0266438,"hat the German word to be disambiguated be aligned to the English subject or object. For this reason, we implemented second guessing based on a dictionary that lists for every German word the 10 most frequently aligned English words (found using the word alignment of all of Europarl). If an ambiguous German word was either unaligned or not aligned to the English subject or object, it was checked whether a dictionary translation was part of the parallel sentence and marked as subject or object by MINIPAR. If so, this dictionary word was used for disambiguation. 3 Figure 1: System Architecture (Schiehlen, 2003), a fast shallow dependency parser. FSPar has extensive lexical knowledge which helps it to find subject-object ambiguities with high accuracy, but it does not try to resolve such ambiguities. The key to our approach is to project syntactic roles from English text. For English parsing we used MINIPAR (Lin, 1998). Based on FSPar’s analysis, all German sentences with a subject-object ambiguity (about a third) were selected from EuroParl. The parallel English sentences were parsed with MINIPAR. Words marked as ambiguous by FSPar were then processed using our algorithm. If an ambiguous German word"
N10-1113,C04-1024,0,0.0121683,"nal correspondence between parallel sentences we use GIZA++ (Och and Ney, 2003). Its input is a tokenized parallel corpus. We lemmatized the text prior to aligning it. Procedure. Figure 1 shows the architecture of our system. The boxes signify data sets, while the lines are processes applied to the data sets. The paper presents two applications. The first is the creation of a set of disambiguated German sentences (which involves word alignments in the upper right corner, and the use of parsers in the middle of the graphic). We also present a reranking of the N -best parses produced by BitPar (Schmid, 2004), a state of the art statistical parser (bottom of the graphic). For processing of German we chose FSPar 737 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 737–740, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics German words. In order for the procedure to work, we require that the German word to be disambiguated be aligned to the English subject or object. For this reason, we implemented second guessing based on a dictionary that lists for every German word the 10 most frequently aligned English word"
N10-1113,N01-1026,0,0.359374,"Missing"
N12-1055,N07-1011,0,0.0133946,", 2011). Active learning. The only existing publication on AL for CR that we are aware of is (Gasperin, 2009). She uses a mention-pair model on a biomedical corpus. The classifier is Naive Bayes and the AL method uncertainty sampling (Lewis and Gale, 1994). The results are negative: AL is not better than random sampling. In preliminary experiments, we replicated this result for our corpus and our system: Uncertainty sampling is not better than random sampling for CR. Uncertainty sampling can fail if uncertainty assessments are too unstable for successful example selection (cf. Dwyer and Holte (2007)). This seems to be the case for the decision trees we use. Naive Bayes is also known to give bad uncertainty assessments (Domingos and Pazzani, 508 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 508–512, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 1997). We therefore adopted a query-by-committee approach combined with a class-balancing strategy. 3 Active learning for CR The classifier in the mention-pair model is faced with a severe class imbalance: there are many more d"
N12-1055,W09-1901,0,0.146707,"et al., 2001). This model classifies links (pairs of two mentions) as coreferent or disreferent, followed by a clustering stage that partitions entities based on the link decisions. Our AL method is partially based on the class balancing strategy proposed by Soon et al. (2001). While models other than mention-pair have been proposed (Culotta et al., 2007), none performs clearly better as evidenced by recent shared evaluations such as SemEval 2010 (Recasens et al., 2010) and CoNLL 2011 (Pradhan et al., 2011). Active learning. The only existing publication on AL for CR that we are aware of is (Gasperin, 2009). She uses a mention-pair model on a biomedical corpus. The classifier is Naive Bayes and the AL method uncertainty sampling (Lewis and Gale, 1994). The results are negative: AL is not better than random sampling. In preliminary experiments, we replicated this result for our corpus and our system: Uncertainty sampling is not better than random sampling for CR. Uncertainty sampling can fail if uncertainty assessments are too unstable for successful example selection (cf. Dwyer and Holte (2007)). This seems to be the case for the decision trees we use. Naive Bayes is also known to give bad uncer"
N12-1055,N06-2015,0,0.096443,"Missing"
N12-1055,P11-1079,0,0.0222288,"Missing"
N12-1055,P02-1014,0,0.627752,"Missing"
N12-1055,N04-1012,0,0.0241247,"xpressions in natural language text refer to the same real-world entity – is an important NLP task. One popular approach to CR is supervised classification. This approach needs manually labeled training data that is expensive to create. Active learning (AL) is a technique that can reduce this cost by setting up an interactive training/annotation loop that selects and annotates training examples that are maximally useful for the classifier that is being trained. However, while AL has been proven successful for many other NLP tasks, such as partof-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002) and named entity recognition (Tomanek et al., 2007), AL has not been successfully applied to coreference resolution so far. Related work Coreference resolution. The perhaps most widely used supervised learning approach to CR is the mention-pair model (Soon et al., 2001). This model classifies links (pairs of two mentions) as coreferent or disreferent, followed by a clustering stage that partitions entities based on the link decisions. Our AL method is partially based on the class balancing strategy proposed by Soon et al. (2001). While models other"
N12-1055,W11-1901,0,0.029224,"resolution. The perhaps most widely used supervised learning approach to CR is the mention-pair model (Soon et al., 2001). This model classifies links (pairs of two mentions) as coreferent or disreferent, followed by a clustering stage that partitions entities based on the link decisions. Our AL method is partially based on the class balancing strategy proposed by Soon et al. (2001). While models other than mention-pair have been proposed (Culotta et al., 2007), none performs clearly better as evidenced by recent shared evaluations such as SemEval 2010 (Recasens et al., 2010) and CoNLL 2011 (Pradhan et al., 2011). Active learning. The only existing publication on AL for CR that we are aware of is (Gasperin, 2009). She uses a mention-pair model on a biomedical corpus. The classifier is Naive Bayes and the AL method uncertainty sampling (Lewis and Gale, 1994). The results are negative: AL is not better than random sampling. In preliminary experiments, we replicated this result for our corpus and our system: Uncertainty sampling is not better than random sampling for CR. Uncertainty sampling can fail if uncertainty assessments are too unstable for successful example selection (cf. Dwyer and Holte (2007))"
N12-1055,W09-2411,0,0.0449945,"Missing"
N12-1055,W07-1516,0,0.0161507,"the task of determining if two expressions in natural language text refer to the same real-world entity – is an important NLP task. One popular approach to CR is supervised classification. This approach needs manually labeled training data that is expensive to create. Active learning (AL) is a technique that can reduce this cost by setting up an interactive training/annotation loop that selects and annotates training examples that are maximally useful for the classifier that is being trained. However, while AL has been proven successful for many other NLP tasks, such as partof-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002) and named entity recognition (Tomanek et al., 2007), AL has not been successfully applied to coreference resolution so far. Related work Coreference resolution. The perhaps most widely used supervised learning approach to CR is the mention-pair model (Soon et al., 2001). This model classifies links (pairs of two mentions) as coreferent or disreferent, followed by a clustering stage that partitions entities based on the link decisions. Our AL method is partially based on the class balancing strategy proposed by"
N12-1055,J01-4004,0,0.780431,"e training/annotation loop that selects and annotates training examples that are maximally useful for the classifier that is being trained. However, while AL has been proven successful for many other NLP tasks, such as partof-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002) and named entity recognition (Tomanek et al., 2007), AL has not been successfully applied to coreference resolution so far. Related work Coreference resolution. The perhaps most widely used supervised learning approach to CR is the mention-pair model (Soon et al., 2001). This model classifies links (pairs of two mentions) as coreferent or disreferent, followed by a clustering stage that partitions entities based on the link decisions. Our AL method is partially based on the class balancing strategy proposed by Soon et al. (2001). While models other than mention-pair have been proposed (Culotta et al., 2007), none performs clearly better as evidenced by recent shared evaluations such as SemEval 2010 (Recasens et al., 2010) and CoNLL 2011 (Pradhan et al., 2011). Active learning. The only existing publication on AL for CR that we are aware of is (Gasperin, 2009"
N12-1055,D07-1051,0,0.0333255,"popular approach to CR is supervised classification. This approach needs manually labeled training data that is expensive to create. Active learning (AL) is a technique that can reduce this cost by setting up an interactive training/annotation loop that selects and annotates training examples that are maximally useful for the classifier that is being trained. However, while AL has been proven successful for many other NLP tasks, such as partof-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002) and named entity recognition (Tomanek et al., 2007), AL has not been successfully applied to coreference resolution so far. Related work Coreference resolution. The perhaps most widely used supervised learning approach to CR is the mention-pair model (Soon et al., 2001). This model classifies links (pairs of two mentions) as coreferent or disreferent, followed by a clustering stage that partitions entities based on the link decisions. Our AL method is partially based on the class balancing strategy proposed by Soon et al. (2001). While models other than mention-pair have been proposed (Culotta et al., 2007), none performs clearly better as evi"
N12-1055,S10-1001,0,\N,Missing
N15-1091,D12-1050,0,0.0247681,"ing set we also add (label, S2 , S1 ) to make best use of the training data; these additions are nonredundant because the interaction feature matrices (Section 4.1) are asymmetric. Systems are evaluated by accuracy and F1 . 6.2 Paraphrase detection systems Since we want to show that Bi-CNN-MI performs better than previous NN work, we compare with three NN approaches: NLM, ARC and RAE (Ta908 ble 1).2 We also include the majority baseline (“baseline”) and MT (Madnani et al., 2012). RAE (Socher et al., 2011) and MT were discussed in Sections 1 and 2. We now briefly describe the other prior work. Blacoe and Lapata (2012) compute the vector representation of a sentence from the neural language model (NLM) embeddings (computed based on (Collobert and Weston, 2008)) of the words of the sentence as the sum of the word embeddings (NLM+), as the element-wise multiplication of the word embeddings (NLM ), or by means of the recursive autoencoder (NLM RAE, Socher et al. (2011)). The representations of the two paraphrase candidates are then concatenated as input to an SVM classifier. See Blacoe and Lapata (2012) for details. The ARC model (Hu et al., 2014) is a convolutional architecture similar to (Collobert and Westo"
N15-1091,P08-1007,0,0.007669,"eresting target in that space is minimized. Interestingness is more like topic relevance, based mainly on the aggregate meaning of lots of keywords. Additionally, their model is a document-level model and is not multigranular. Madnani et al. (2012) treated paraphrase relationship as a kind of mutual translation, hence combined eight kinds of machine translation metrics including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), TERp (Snover et al., 2009), METEOR (Denkowski and Lavie, 2010), SEPIA (Habash and Elkholy, 2008), BAD903 GER (Parker, 2008) and MAXSIM (Chan and Ng, 2008). These features are not multigranular; rather they are low-level only; high-level features – e.g., a representation of the entire sentence – are not considered. Bach et al. (2014) claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. Their conclusion also endorses Socher et al. (2011)’s and our work, for both take similarities between component phrases into account. We discussed Socher et al. (2011)’s RAE and Hu et al. (2014)’s ARC-I in Section 1. In addition to similarity matrices there are two other important aspects of (Socher et a"
N15-1091,P09-1053,0,0.0132115,"ype of embedding learning is that no labels are needed. (iv) CNN-LM only adds one hidden layer over CNNSM. It keeps simple architecture like PV-DM (Le and Mikolov, 2014), CBOW (Mikolov et al., 2013) and LBL (Mnih and Teh, 2012), enabling the CNNSM as main training target. In summary, the key contribution of CNN-LM is that we pretrain convolutional filters. Architectural elements from the literature are combined to support effective pretraining of convolutional filters. 6 6.1 Experiments Data set and evaluation metrics We use the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004; Das and Smith, 2009). The training set contains 2753 true and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairs, respectively. For each triple (label, S1 , S2 ) in the training set we also add (label, S2 , S1 ) to make best use of the training data; these additions are nonredundant because the interaction feature matrices (Section 4.1) are asymmetric. Systems are evaluated by accuracy and F1 . 6.2 Paraphrase detection systems Since we want to show that Bi-CNN-MI performs better than previous NN work, we compare with three NN approaches: NLM, ARC and RAE (Ta908 ble 1).2 We also include the majo"
N15-1091,N10-1031,0,0.0101382,"a latent space in such a way that the distance between the source document and its corresponding interesting target in that space is minimized. Interestingness is more like topic relevance, based mainly on the aggregate meaning of lots of keywords. Additionally, their model is a document-level model and is not multigranular. Madnani et al. (2012) treated paraphrase relationship as a kind of mutual translation, hence combined eight kinds of machine translation metrics including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), TERp (Snover et al., 2009), METEOR (Denkowski and Lavie, 2010), SEPIA (Habash and Elkholy, 2008), BAD903 GER (Parker, 2008) and MAXSIM (Chan and Ng, 2008). These features are not multigranular; rather they are low-level only; high-level features – e.g., a representation of the entire sentence – are not considered. Bach et al. (2014) claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. Their conclusion also endorses Socher et al. (2011)’s and our work, for both take similarities between component phrases into account. We discussed Socher et al. (2011)’s RAE and Hu et al. (2014)’s ARC-I in Section"
N15-1091,C04-1051,0,0.266368,"advantage of this type of embedding learning is that no labels are needed. (iv) CNN-LM only adds one hidden layer over CNNSM. It keeps simple architecture like PV-DM (Le and Mikolov, 2014), CBOW (Mikolov et al., 2013) and LBL (Mnih and Teh, 2012), enabling the CNNSM as main training target. In summary, the key contribution of CNN-LM is that we pretrain convolutional filters. Architectural elements from the literature are combined to support effective pretraining of convolutional filters. 6 6.1 Experiments Data set and evaluation metrics We use the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004; Das and Smith, 2009). The training set contains 2753 true and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairs, respectively. For each triple (label, S1 , S2 ) in the training set we also add (label, S2 , S1 ) to make best use of the training data; these additions are nonredundant because the interaction feature matrices (Section 4.1) are asymmetric. Systems are evaluated by accuracy and F1 . 6.2 Paraphrase detection systems Since we want to show that Bi-CNN-MI performs better than previous NN work, we compare with three NN approaches: NLM, ARC and RAE (Ta908 ble 1).2 We"
N15-1091,D14-1002,0,0.0156907,"nd Li (2013) developed a deep NN to match short texts, where interactions between components within the two objects were considered. These interactions were obtained via LDA (Blei et al., 2003). A two-dimensional interaction space is formed, then those local decisions will be sent to the corresponding neurons in upper layers to get rounds of fusion, finally logistic regression in the output layer produces the final matching score. Drawbacks of this approach are that LDA parameters are not optimized for the paraphrase task and that the interactions are formed on the level of single words only. Gao et al. (2014) model interestingness between two documents with deep NNs. They map sourcetarget document pairs to feature vectors in a latent space in such a way that the distance between the source document and its corresponding interesting target in that space is minimized. Interestingness is more like topic relevance, based mainly on the aggregate meaning of lots of keywords. Additionally, their model is a document-level model and is not multigranular. Madnani et al. (2012) treated paraphrase relationship as a kind of mutual translation, hence combined eight kinds of machine translation metrics including"
N15-1091,D13-1090,0,0.104616,"n also endorses Socher et al. (2011)’s and our work, for both take similarities between component phrases into account. We discussed Socher et al. (2011)’s RAE and Hu et al. (2014)’s ARC-I in Section 1. In addition to similarity matrices there are two other important aspects of (Socher et al., 2011). First, the similarity matrices are converted to a fixed size feature vector by dynamic pooling. We adopt this approach in BiCNN-MI; see Section 4.2 for details. Second, (Socher et al., 2011) is partially based on parsing as is some other work on paraphrase identification (e.g., Wan et al. (2006), Ji and Eisenstein (2013)). Parsing is a potentially powerful tool for identifying the important meaning units of a sentence, which can then be the basis for determining meaning equivalence. However, reliance on parsing makes these approaches less flexible. For example, there are no high-quality parsers available for some domains and some languages. Our approach is in principle applicable for any domain and language. It is also unclear how we would identify comparable units in the parse trees of S1 and S2 if the parse trees have different height and the sentences different lengths. A key property of Bi-CNN-MI is that"
N15-1091,P14-1062,0,0.75282,"ighly accurate parsers are not available. We extend the basic idea of RAE by exploring stacked convolution layers which on one hand use sliding windows to split sentences into flexible phrases, furthermore, higher layers are able to ex901 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 901–911, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tract more abstract features of longer-range phrases by combining phrases in lower layers. A representative way of doing this in deep learning is the work by Kalchbrenner et al. (2014), the second prior NN architecture that we draw on. They use convolution to learn representations at multiple levels (Collobert and Weston, 2008). The motivation for convolution is that natural language consists of long sequences in which many short subsequences contribute in a stable way to the structure and meaning of the long sequence regardless of the position of the subsequence within the long sequence. Thus, it is advantageous to learn convolutional filters that detect a particular feature regardless of position. Kalchbrenner et al. (2014)’s architecture extends this idea in two importan"
N15-1091,N12-1019,0,0.338829,"prices by ten percent.” – (A2) “GM, Ford and Chrysler have raised car prices by five percent.” Example A1/A2 shows that paraphrase identification requires comparison at the word level. A1 cannot be a paraphrase of A2 because the numbers “ten” and “five” are different. (B1) “Mary gave birth to a son in 2000.” – (B2) “He is 14 years old and his mother is Mary.” PI for B1/B2 can only succeed at the sentence level since B1/B2 express the same meaning using very different means. Most work on paraphrase identification has focused on only one level of granularity: either on lowlevel features (e.g., Madnani et al. (2012)) or on the sentence level (e.g., ARC-I, Hu et al. (2014)). An exception is the RAE model (Socher et al., 2011). It computes representations on all levels of a parse tree: each node – including nodes corresponding to words, phrases and the entire sentence – is represented as a vector. RAE then computes a n1 × n2 comparison matrix of the two trees derived from S1 and S2 respectively, where n1 , n2 are the number of nodes and each comparison is the Euclidean distance between two vectors. This is then the basis for paraphrase classification. RAE (Socher et al., 2011) is one of three prior NN arch"
N15-1091,P02-1040,0,0.11438,"interestingness between two documents with deep NNs. They map sourcetarget document pairs to feature vectors in a latent space in such a way that the distance between the source document and its corresponding interesting target in that space is minimized. Interestingness is more like topic relevance, based mainly on the aggregate meaning of lots of keywords. Additionally, their model is a document-level model and is not multigranular. Madnani et al. (2012) treated paraphrase relationship as a kind of mutual translation, hence combined eight kinds of machine translation metrics including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), TERp (Snover et al., 2009), METEOR (Denkowski and Lavie, 2010), SEPIA (Habash and Elkholy, 2008), BAD903 GER (Parker, 2008) and MAXSIM (Chan and Ng, 2008). These features are not multigranular; rather they are low-level only; high-level features – e.g., a representation of the entire sentence – are not considered. Bach et al. (2014) claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. Their conclusion also endorses Socher et al. (2011)’s and our work, for both take similarities bet"
N15-1091,2006.amta-papers.25,0,0.0401235,"hey map sourcetarget document pairs to feature vectors in a latent space in such a way that the distance between the source document and its corresponding interesting target in that space is minimized. Interestingness is more like topic relevance, based mainly on the aggregate meaning of lots of keywords. Additionally, their model is a document-level model and is not multigranular. Madnani et al. (2012) treated paraphrase relationship as a kind of mutual translation, hence combined eight kinds of machine translation metrics including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), TERp (Snover et al., 2009), METEOR (Denkowski and Lavie, 2010), SEPIA (Habash and Elkholy, 2008), BAD903 GER (Parker, 2008) and MAXSIM (Chan and Ng, 2008). These features are not multigranular; rather they are low-level only; high-level features – e.g., a representation of the entire sentence – are not considered. Bach et al. (2014) claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. Their conclusion also endorses Socher et al. (2011)’s and our work, for both take similarities between component phrases into account. We discussed So"
N15-1091,P10-1040,0,0.0209249,"mns, respectively; each cell is its own dynamic pool. 5 Training 5.1 Supervised training Dynamic pooling gives us fixed size interaction feature matrices for sentence, ngram and unigram levels. As shown in Figure 1, the concatenation of these features (Fs , Fln , Fsn and Fu ) is the input to a logistic regression layer for paraphrase classification. We have now described all three parts of Bi-CNN-MI: CNN-SM, CNN-IM and logistic regression. Bi-CNN-MI with all its parameters – including word embeddings and convolution weights – is trained on MSRP. We initialize embeddings with those provided by Turian et al. (2010)1 (based on Collobert and Weston (2008)). For layer sn, we have f sn = 6 feature maps and set filter width msn = 3. For layer ln, we have f ln = 14 feature maps and set filter width mln = 5 and k top = 4. Dynamic pooling 1 metaoptimize.com/projects/wordreprs/ Figure 2: Partition methods in dynamic pooling. Original matrix with size 4 × 5 is mapped into matrix with size 3 × 3 and matrix with size 6 × 7, respectively. Each dynamic pool is distinguished by a border of empty white space around it. Figure 3: Unsupervised architecture: CNN-LM sizes are 10 × 10, 10 × 10, 6 × 6, 2 × 2 for unigram, sho"
N15-1091,U06-1019,0,0.0179334,"ng. Their conclusion also endorses Socher et al. (2011)’s and our work, for both take similarities between component phrases into account. We discussed Socher et al. (2011)’s RAE and Hu et al. (2014)’s ARC-I in Section 1. In addition to similarity matrices there are two other important aspects of (Socher et al., 2011). First, the similarity matrices are converted to a fixed size feature vector by dynamic pooling. We adopt this approach in BiCNN-MI; see Section 4.2 for details. Second, (Socher et al., 2011) is partially based on parsing as is some other work on paraphrase identification (e.g., Wan et al. (2006), Ji and Eisenstein (2013)). Parsing is a potentially powerful tool for identifying the important meaning units of a sentence, which can then be the basis for determining meaning equivalence. However, reliance on parsing makes these approaches less flexible. For example, there are no high-quality parsers available for some domains and some languages. Our approach is in principle applicable for any domain and language. It is also unclear how we would identify comparable units in the parse trees of S1 and S2 if the parse trees have different height and the sentences different lengths. A key prop"
N15-1140,N03-2002,0,0.0197962,"Missing"
N15-1140,A00-2013,0,0.0289825,"Missing"
N15-1140,2005.mtsummit-papers.11,0,0.0356163,"Missing"
N15-1140,W13-3512,0,0.357292,"is a noun in the nominative case and also both singular and feminine. Each tag is composed of meaningful sub-tag units that are shared across whole tags, e.g., the feature N OM fires on both adjectives and nouns. hoff (2003) introduced factored LMs, which effectively add tiers, allowing easy incorporation of morphological structure as well as part-of-speech (POS) tags. More recently, Müller and Schütze (2011) trained a class-based LM using common suffixes— often indicative of morphology—achieving stateof-the-art results when interpolated with a KneserNey LM. In neural probabilistic modeling, Luong et al. (2013) described a recursive neural network LM, whose topology was derived from the output of M ORFESSOR, an unsupervised morphological segmentation tool (Creutz and Lagus, 2005). Similarly, Qiu et al. (2014) augmented WORD 2 VEC (Mikolov et al., 2013) to embed morphs as well as whole words—also taking advantage of M ORFES SOR . LMs were tackled by dos Santos and Zadrozny (2014) with a convolutional neural network with a k-best max-pooling layer to extract character level n-grams, efficiently inserting orthographic features into the LM—use of the vectors in down-stream POS tagging achieved state-of-"
N15-1140,P11-2092,0,0.017703,"an accompanying English translation. Each word is annotated with a complex morphological tag and its corresponding coarsegrained POS tag. For instance, Stadt is annotated with N.N OM .S G .F EM indicating that it is a noun in the nominative case and also both singular and feminine. Each tag is composed of meaningful sub-tag units that are shared across whole tags, e.g., the feature N OM fires on both adjectives and nouns. hoff (2003) introduced factored LMs, which effectively add tiers, allowing easy incorporation of morphological structure as well as part-of-speech (POS) tags. More recently, Müller and Schütze (2011) trained a class-based LM using common suffixes— often indicative of morphology—achieving stateof-the-art results when interpolated with a KneserNey LM. In neural probabilistic modeling, Luong et al. (2013) described a recursive neural network LM, whose topology was derived from the output of M ORFESSOR, an unsupervised morphological segmentation tool (Creutz and Lagus, 2005). Similarly, Qiu et al. (2014) augmented WORD 2 VEC (Mikolov et al., 2013) to embed morphs as well as whole words—also taking advantage of M ORFES SOR . LMs were tackled by dos Santos and Zadrozny (2014) with a convolution"
N15-1140,C14-1015,0,0.0949489,"nd nouns. hoff (2003) introduced factored LMs, which effectively add tiers, allowing easy incorporation of morphological structure as well as part-of-speech (POS) tags. More recently, Müller and Schütze (2011) trained a class-based LM using common suffixes— often indicative of morphology—achieving stateof-the-art results when interpolated with a KneserNey LM. In neural probabilistic modeling, Luong et al. (2013) described a recursive neural network LM, whose topology was derived from the output of M ORFESSOR, an unsupervised morphological segmentation tool (Creutz and Lagus, 2005). Similarly, Qiu et al. (2014) augmented WORD 2 VEC (Mikolov et al., 2013) to embed morphs as well as whole words—also taking advantage of M ORFES SOR . LMs were tackled by dos Santos and Zadrozny (2014) with a convolutional neural network with a k-best max-pooling layer to extract character level n-grams, efficiently inserting orthographic features into the LM—use of the vectors in down-stream POS tagging achieved state-of-the-art results in Portuguese. Finally, most similar to our model, Botha and Blunsom (2014) introduced the additive logbilinear model (LBL++). Best summarized as a neural factored LM, the LBL++ created"
N15-1140,N01-1024,0,0.0883059,"tributes, e.g., case, gender and number, is also important. An example of an annotated German phrase is found in table 1. This often leads to a large tag set; e.g., in the morphological tag set of Hajiˇc (2000), English had 137 tags whereas morphologically-rich Czech had 970 tags! Clearly, much of the information needed to determine a word’s morphological tag is encoded in the word itself. For example, the suffix ed is generally indicative of the past tense in English. However, distributional similarity has also been shown to be an important cue for morphology (Yarowsky and Wicentowski, 2000; Schone and Jurafsky, 2001). Much as contextual signatures are reliably exploited approximations to the semantics of the lexicon (Harris, 1954)—you shall know the meaning of the word by the company it keeps (Firth, 1957)—they can be similarly exploited for morphological analysis. This is not an unexpected result—in German, e.g., we would expect nouns that follow an adjective in the genitive case to also be in the genitive case themselves. Much of what our model is designed to accomplish is the isolation of the components of the contextual signature that are indeed predictive of morphology. 3 Log-Bilinear Model The LBL i"
N15-1140,P10-1040,0,0.0151709,"out the tags for the subset of the data for which we do not have annotation. 5 In our evaluation, we attempt to intrinsically determine whether it is indeed true that words similar in the embedding space are morphologically related. Qualitative evaluation, shown in figure 1, indicates that this is the case. 5.1 Semi-Supervised Learning In the fully supervised case, the method we proposed above requires a corpus annotated with morphological tags to train. This conflicts with a key use case of word-embeddings—they allow the easy incorporation of large, unannotated corpora into supervised tasks (Turian et al., 2010). To resolve this, we train our model on a partially annotated corpus. The key idea here is that we only need a partial set of labeled data to steer the embeddings to ensure they 1289 Evaluation MorphoDist We introduce a new evaluation metric for morphologically-driven embeddings to quantitatively score models. Roughly, the question we want to evaluate is: are words that are similar in the embedded space also morphologically related? Given a word w and its embedding qw , let Mw be the set of morphological tags associated with w represented by bit vectors. This is a set because words may have s"
N15-1140,P00-1027,0,0.021624,"an, considering other nominal attributes, e.g., case, gender and number, is also important. An example of an annotated German phrase is found in table 1. This often leads to a large tag set; e.g., in the morphological tag set of Hajiˇc (2000), English had 137 tags whereas morphologically-rich Czech had 970 tags! Clearly, much of the information needed to determine a word’s morphological tag is encoded in the word itself. For example, the suffix ed is generally indicative of the past tense in English. However, distributional similarity has also been shown to be an important cue for morphology (Yarowsky and Wicentowski, 2000; Schone and Jurafsky, 2001). Much as contextual signatures are reliably exploited approximations to the semantics of the lexicon (Harris, 1954)—you shall know the meaning of the word by the company it keeps (Firth, 1957)—they can be similarly exploited for morphological analysis. This is not an unexpected result—in German, e.g., we would expect nouns that follow an adjective in the genitive case to also be in the genitive case themselves. Much of what our model is designed to accomplish is the isolation of the components of the contextual signature that are indeed predictive of morphology. 3"
N15-1140,petrov-etal-2012-universal,0,\N,Missing
N15-1154,J10-4006,0,0.0228766,"ction 5 presents experimental setup and results. Section 6 concludes. 2 Related work The key for good performance in paraphrase identification is the design of good features. We now discuss relevant prior work based on the linguistic granularity of feature learning. The first line is compositional semantics, which learns representations for words and then composes them to representations of sentences. Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al., 2011)). They showed that addition over word embeddings is competitive, despite its simplicity. The second category directly seeks sentence-level features. Ji and Eisenstein (2013) explored unigrams, bigrams and dependency pairs as sentence features. They proposed TF-KLD to weight features and used non-negative factorization to learn latent sentence representations. Our method TF-KLDKNN is an extension of their work. The third li"
N15-1154,D12-1050,0,0.550543,"vestigates representation learning via deep learning in paraphrase identification task, which aims to determine whether two sentences have the same meaning. One main innovation of deep learning is that it learns distributed word representations (also called “word embeddings”) to deal with various Natural Language Processing (NLP) tasks. Our goal is to use and refine embeddings to get competitive performance. We adopt a supervised classification approach to paraphrase identification like most top performing systems. Our focus is representation learning of sentences. Following prior work (e.g., Blacoe and Lapata (2012)), we compute the vector of a sentence as the sum of the vectors of its components. But unlike prior work we use single words, continuous phrases and discontinuous phrases as the components, not just single words. Our rationale is that many semantic units are formed by multiple words – e.g., the Ji and Eisenstein (2013) show that not all words are equally important for paraphrase identification. They propose TF-KLD, a discriminative weighting scheme to address this problem. While they do not represent sentences as vectors composed of other vectors, TF-KLD is promising for a vector-based approa"
N15-1154,P09-1053,0,0.0571302,"tegory directly seeks sentence-level features. Ji and Eisenstein (2013) explored unigrams, bigrams and dependency pairs as sentence features. They proposed TF-KLD to weight features and used non-negative factorization to learn latent sentence representations. Our method TF-KLDKNN is an extension of their work. The third line directly computes features for sentence pairs. Wan et al. (2006) used N-gram overlap, dependency relation overlap, dependency tree-edit distance and difference of sentence lengths. Finch et al. (2005) and Madnani et al. (2012) combined several machine translation metrics. Das and Smith (2009) presented a generative model over two sentences’ dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given 1369 sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al. (2006), Ul-Qayyum and Altaf (2012). Our wo"
N15-1154,C04-1051,0,0.819475,"ace. We then represent a sentence as the sum of the vectors of its units, weighted by TFKLD-KNN. We use (Madnani et al., 2012) as our baseline system. They used simple features – eight different machine translation metrics – yet got good performance. Based on above new sentence representations, we compute three kinds of features to describe a pair of sentences – cosine similarity, element-wise sum and absolute element-wise difference – and show that combining them with the features from Madnani et al. (2012) gets state-of-the-art performance on the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004). 1368 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1368–1373, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics In summary, our first contribution lies in embedding learning of continuous and discontinuous phrases. Our second contribution is the weighting scheme TF-KLD-KNN. This paper is structured as follows. Section 2 reviews related work. Section 3 describes our method for learning embeddings of units. Section 4 introduces a measure of unit discriminativity that can be used for differential w"
N15-1154,I05-5003,0,0.0369812,"wed that addition over word embeddings is competitive, despite its simplicity. The second category directly seeks sentence-level features. Ji and Eisenstein (2013) explored unigrams, bigrams and dependency pairs as sentence features. They proposed TF-KLD to weight features and used non-negative factorization to learn latent sentence representations. Our method TF-KLDKNN is an extension of their work. The third line directly computes features for sentence pairs. Wan et al. (2006) used N-gram overlap, dependency relation overlap, dependency tree-edit distance and difference of sentence lengths. Finch et al. (2005) and Madnani et al. (2012) combined several machine translation metrics. Das and Smith (2009) presented a generative model over two sentences’ dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given 1369 sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this"
N15-1154,D13-1090,0,0.42447,"sing (NLP) tasks. Our goal is to use and refine embeddings to get competitive performance. We adopt a supervised classification approach to paraphrase identification like most top performing systems. Our focus is representation learning of sentences. Following prior work (e.g., Blacoe and Lapata (2012)), we compute the vector of a sentence as the sum of the vectors of its components. But unlike prior work we use single words, continuous phrases and discontinuous phrases as the components, not just single words. Our rationale is that many semantic units are formed by multiple words – e.g., the Ji and Eisenstein (2013) show that not all words are equally important for paraphrase identification. They propose TF-KLD, a discriminative weighting scheme to address this problem. While they do not represent sentences as vectors composed of other vectors, TF-KLD is promising for a vector-based approach as well since the insight that units are of different importance still applies. A shortcoming of TF-KLD is its failure to define weights for words that do not occur in the training set. We propose TF-KLD-KNN, an extension of TF-KLD that computes the weight of an unknown unit as the average of the weights of its k nea"
N15-1154,N12-1019,0,0.150935,"entences as vectors composed of other vectors, TF-KLD is promising for a vector-based approach as well since the insight that units are of different importance still applies. A shortcoming of TF-KLD is its failure to define weights for words that do not occur in the training set. We propose TF-KLD-KNN, an extension of TF-KLD that computes the weight of an unknown unit as the average of the weights of its k nearest neighbors. We determine nearest neighbors by cosine measure over embedding space. We then represent a sentence as the sum of the vectors of its units, weighted by TFKLD-KNN. We use (Madnani et al., 2012) as our baseline system. They used simple features – eight different machine translation metrics – yet got good performance. Based on above new sentence representations, we compute three kinds of features to describe a pair of sentences – cosine similarity, element-wise sum and absolute element-wise difference – and show that combining them with the features from Madnani et al. (2012) gets state-of-the-art performance on the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004). 1368 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pa"
N15-1154,W06-1603,0,0.0239495,"Missing"
N15-1154,U06-1019,0,0.0193425,"along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al., 2011)). They showed that addition over word embeddings is competitive, despite its simplicity. The second category directly seeks sentence-level features. Ji and Eisenstein (2013) explored unigrams, bigrams and dependency pairs as sentence features. They proposed TF-KLD to weight features and used non-negative factorization to learn latent sentence representations. Our method TF-KLDKNN is an extension of their work. The third line directly computes features for sentence pairs. Wan et al. (2006) used N-gram overlap, dependency relation overlap, dependency tree-edit distance and difference of sentence lengths. Finch et al. (2005) and Madnani et al. (2012) combined several machine translation metrics. Das and Smith (2009) presented a generative model over two sentences’ dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given 1369 sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then p"
N16-1065,P15-1061,0,0.36422,"on task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to th"
N16-1065,S10-1006,0,0.0250728,"Missing"
N16-1065,D14-1181,0,0.016276,"ed task participants used, i.a., support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010). Recently, their results on this data set were outperformed by applying NNs (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015). 534 Proceedings of NAACL-HLT 2016, pages 534–539, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Zeng et al. (2014) built a CNN based only on the context between the relation arguments and extended it with several lexical features. Kim (2014) and others used convolutional filters of different sizes for CNNs. Nguyen and Grishman (2015) applied this to relation classification and obtained improvements over single filter sizes. Dos Santos et al. (2015) replaced the softmax layer of the CNN with a ranking layer. They showed improvements and published the best result so far on the SemEval dataset, to our knowledge. Socher et al. (2012) used another NN architecture for relation classification: recursive neural networks that built recursive sentence representations based on syntactic parsing. In contrast, Zhang and Wang (2015) investigat"
N16-1065,W15-1506,0,0.342383,"Eval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pay"
N16-1065,S10-1057,0,0.0908329,"w that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for rela"
N16-1065,D12-1110,0,0.101558,"neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation ar"
N16-1065,S10-1049,0,0.342279,"nal and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The"
N16-1065,C14-1220,0,0.254999,"state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between th"
N16-1065,W09-2415,0,\N,Missing
N16-1080,P11-1004,0,0.257107,"n-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian. 1 funniest funyest fun y est memikulmu menpikulmu men pikul mu Zulassung zulassenung zu lassen ung Orthography Underlying Form Segmentation Figure 1: Examples of canonical segmentation for English (top), Indonesian (middle) and German (bottom). Introduction Morphological segmentation is useful for NLP applications, such as, automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C ¸ etino˘glu, 2015). Prior work cast the problem as surface segmentation: a word form w is segmented into a sequence of substrings whose concatenation is w. In this paper, we introduce the problem of canonical segmentation: w is analyzed as a sequence of canonical morphemes, based on a set of word forms that have been “canonically” annotated for supervised learning. Each canonical morpheme c corresponds to a surface morph s, defined as its orthographic manifestation, i.e., as the substring of w that is generated by applying editing operations like insertion and deleti"
N16-1080,P14-2102,1,0.936568,"ntation factor f (Sarawagi and Cohen, 2004), relating it to previous semi-CRF models of segmentation.2 To fit the model, we maximize the N log-likelihood PN of the training data {(si , ui , wi )}i=1 , L(θ) = i=1 log p(si , ui |wi ), with respect to the model parameters θ. Optimization is done with gradient-based methods—requiring the computation of log Z(w) and ∇ log Z(w), which is intractable.3 Thus, we turn to sampling (Rubinstein and Kroese, 2011) and stochastic gradient methods. Features Our model includes several simple feature templates. The transduction factor of the model is based on (Cotterell et al., 2014): we include features that fire on individual edit actions as well as conjunctions of edit actions and characters on the surrounding context. For the semi-Markov factor, we use the feature set of Cotterell et al. (2015a), which 2 Our transduction factor maps surface forms w to UR strings u of bounded length by imposing an insertion limit k. Thus, |u |≤ |w |+ k. Our experiments use k = 5. 3 Since the semi-CRF features fire on substrings, we would need a dynamic programming state for each substring of each of the exponentially many settings of u. includes indicator features on individual segment"
N16-1080,K15-1017,1,0.872874,"Missing"
N16-1080,Q15-1031,1,0.872986,"i , ui |wi ), with respect to the model parameters θ. Optimization is done with gradient-based methods—requiring the computation of log Z(w) and ∇ log Z(w), which is intractable.3 Thus, we turn to sampling (Rubinstein and Kroese, 2011) and stochastic gradient methods. Features Our model includes several simple feature templates. The transduction factor of the model is based on (Cotterell et al., 2014): we include features that fire on individual edit actions as well as conjunctions of edit actions and characters on the surrounding context. For the semi-Markov factor, we use the feature set of Cotterell et al. (2015a), which 2 Our transduction factor maps surface forms w to UR strings u of bounded length by imposing an insertion limit k. Thus, |u |≤ |w |+ k. Our experiments use k = 5. 3 Since the semi-CRF features fire on substrings, we would need a dynamic programming state for each substring of each of the exponentially many settings of u. includes indicator features on individual segments, conjunctions of segments and segment labels and conjunctions of segments and left and right context on the input string. We also include a feature that checks whether the segment is a word in ASPELL (or a monolingua"
N16-1080,W02-0603,0,0.540523,"instantiation of the indirect estimator leverages an efficient dynamic program to compute the expected features under p(·|u(i) ). This has the effect of decreasing the number of samples required to get a useful estimate of the gradient. Computing p¯(u(i) ) is a side effect of the dynamic program, namely the normalization constant. As a proposal distribution q, we use the following locally normalized distribution, exp(ω &gt;g(u, w)) . &gt; 0 u0 exp(ω g(u , w)) q(u) = P 3 (8) Related Work Most work on morphological segmentation has been unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. F"
N16-1080,N07-1020,0,0.0319622,"nt independently and decode sequentially. This approach is faster both at train and at test but suffers from cascading errors. Second, we train a joint model, the transduction and the segmentation components are trained to work well together. 5 https://github.com/desmond86/ Indonesian-English-Bilingual-Corpus 6 http://ryancotterell.github.io/ canonical-segmentation/ 667 error distance F1 which has been “corrupted” by the orthographic process. Our approach, however, is capable of restoring the underlying morphemes, e.g., stopping 7→ stop-ing. We note two exceptions to the above statement. Both Dasgupta and Ng (2007) and Naradowsky and Goldwater (2009) incorporate basic, heuristic spelling rules into unsupervised induction algorithms. Relatedly, Cotterell et al. (2015b) induced a phonology in an unsupervised manner. In contrast, our model is fully supervised and supports rich features, which enable accurate prediction on new words. en de id en de id en de id Joint Pipeline SemiCRF WFST 0.27 (.02) 0.33 (.01) 0.33 (.01) 0.63 (.00) 0.41 (.03) 0.53 (.02) 0.65 (.01) 0.74 (.01) 0.10 (.01) 0.22 (.01) 0.27 (.01) 0.71 (.00) 0.98 (.34) 0.63 (.04) 0.68 (.01) 1.35 (.01) 1.01 (.07) 1.10 (.04) 1.32 (.04) 4.24 (.20) 0.1"
N16-1080,D09-1011,0,0.0530571,"We present experiments on three languages: English, German and Indonesian. 2 Model, Inference and Training Our goal is canonical segmentation: identifying both the canonical morphemes and the morphs (their orthographic manifestations) of a word. This task involves segmenting the input as well as accounting for orthographic changes occurring in the word formation processes. Let w be the surface form, u the orthographic underlying representation (UR) of w, and s a labeled segmentation of u. Note: all random 1 Note that funnest is a word of (colloquial) English. 665 variables are string-valued (Dreyer and Eisner, 2009). For example, consider the word unhappiness: unhappiness | {z } transduction 7→ w u segmentation 7→ unhappyness | {z } [ prefix un][stem happy][suffix ness] . | {z } s Note that our notion of an orthographic UR closely resembles the phonological concept of a UR (Kenstowicz, 1994) and, indeed, many orthographic variations are manifestations of phonology. We model this process as a globally normalized log-linear model of the conditional distribution, p(s, u |w) =   1 exp η &gt; f (s, u)+ω &gt; g(u, w) , Z(w) where θ = {η, ω} are the model parameters, f and g are, respectively, feature functions of"
N16-1080,D08-1113,0,0.364328,"closely resembles the phonological concept of a UR (Kenstowicz, 1994) and, indeed, many orthographic variations are manifestations of phonology. We model this process as a globally normalized log-linear model of the conditional distribution, p(s, u |w) =   1 exp η &gt; f (s, u)+ω &gt; g(u, w) , Z(w) where θ = {η, ω} are the model parameters, f and g are, respectively, feature functions of the segmentation-UR and UR-surface-form pairs and P Z(w) = exp η &gt;f (s0 , u0 ) + ω &gt;g(u0 , w) is 0 0 s ,u the partition function. We can view this model as a conjunction of a finite-state transduction factor g (Dreyer et al., 2008) and a semi-Markov segmentation factor f (Sarawagi and Cohen, 2004), relating it to previous semi-CRF models of segmentation.2 To fit the model, we maximize the N log-likelihood PN of the training data {(si , ui , wi )}i=1 , L(θ) = i=1 log p(si , ui |wi ), with respect to the model parameters θ. Optimization is done with gradient-based methods—requiring the computation of log Z(w) and ∇ log Z(w), which is intractable.3 Thus, we turn to sampling (Rubinstein and Kroese, 2011) and stochastic gradient methods. Features Our model includes several simple feature templates. The transduction factor of"
N16-1080,J01-2001,0,0.869368,"Missing"
N16-1080,W10-2210,0,0.804269,"tion q, we use the following locally normalized distribution, exp(ω &gt;g(u, w)) . &gt; 0 u0 exp(ω g(u , w)) q(u) = P 3 (8) Related Work Most work on morphological segmentation has been unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distribution"
N16-1080,W10-2211,0,0.0348371,". . u(m) ∼ q. We use m = 1000 in our experiments. Conditioned on each sample value for u, we use exact semi-CRF Viterbi decoding to select s. 4.3 Evaluation Measures Evaluating morphological segmentation is tricky. The standard measure for the supervised task is border F1 , which measures how often the segmentation boundaries posited by the model are correct. However, this measure assumes that the concatenation of the segments is identical to the input string (i.e., surface segmentation) and is thus not applicable to canonical segmentation. On the other hand, the Morpho Challenge competition (Kurimo et al., 2010) uses a measure that samples a large number of word pairs from a linguistic gold standard. A form is considered correct if the gold standard contains at least one overlapping morph and the model posits at least one overlapping morph—this is problematic because for languages with multi-morphemic words (e.g., German), one should consider all morphs. Moreover, we can actually recover the linguistically annotated gold standard in contrast to unsupervised methods. Instead, we report results under three measures: error rate, edit distance and morpheme F1 . Error rate is the proportion of analyses th"
N16-1080,J97-2003,0,0.0956147,"rage edit distance. Bottom: Mean morpheme F1 (higher better). Standard deviation in parentheses. Best result on each line in bold. Baseline: Semi-CRF Segmenter The first baseline is a semi-CRF (Sarawagi and Cohen, 2004) that segments the orthographic form into morphs without canonicalization. Earlier work by Cotterell et al. (2015a) applied this model to supervised morphological segmentation. We use the feature set as Cotterell et al. (2015a), but we do not incorporate their augmented morphotactic state space. Baseline: WFST Segmenter Our second baseline is a weighted finite-state transducer (Mohri, 1997) with a log-linear parameterization (Dreyer et al., 2008). We use the stochastic contextual edit model of Cotterell et al. (2014). We employ context n-gram features (up to 6-grams) on the input string to the left and right of the edit location in addition to 2-gram features on the lower string. The context features are then conjoined with the exact edit action. We refer the reader to Cotterell et al. (2014) for more details. The segmentation boundaries are marked as a distinguished symbol in the target string. This model is not entirely suited for the task as it makes it difficult to include t"
N16-1080,D14-1095,0,0.263957,"a sequence of substrings, e.g., funniest 7→ funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian. 1 funniest funyest fun y est memikulmu menpikulmu men pikul mu Zulassung zulassenung zu lassen ung Orthography Underlying Form Segmentation Figure 1: Examples of canonical segmentation for English (top), Indonesian (middle) and German (bottom). Introduction Morphological segmentation is useful for NLP applications, such as, automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C ¸ etino˘glu, 2015). Prior work cast the problem as surface segmentation: a word form w is segmented into a sequence of substrings whose concatenation is w. In this paper, we introduce the problem of canonical segmentation: w is analyzed as a sequence of canonical morphemes, based on a set of word forms that have been “canonically” annotated for supervised learning. Each canonical morpheme c corresponds to a surface morph s, defined as its orthographic manifestation, i.e., as the substring of w that is generated by apply"
N16-1080,N09-1024,0,0.430023,"Most work on morphological segmentation has been unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed th"
N16-1080,W13-3504,0,0.417711,", 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed that modeling morphotactics with a semi-CRF improves results further. The previously described approaches only attempt to split words into a sequence of stem and affixes— making it difficult to restore the underlying str"
N16-1080,E14-4017,0,0.16445,"gmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed that modeling morphotactics with a semi-CRF improves results further. The previously described approaches only attempt to split words into a sequence of stem and affixes— making it difficult to restore the underlying structure 4 Informally, the indirect importance sampling estimate converges to the true expectation as m → ∞. 4 Experiments We provide canonical segmentation experiments in three languages: English, German and Ind"
N16-1080,Q15-1026,0,0.0243284,"Missing"
N16-1080,P05-1044,0,0.0289335,"unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed that modeling morphotactics with a semi-CRF improves re"
N16-1080,P99-1037,0,0.808147,"Missing"
N16-1080,P13-1118,0,0.0853262,"Missing"
N16-1091,S15-2102,0,0.0310771,"nal. Sentiment lexicons are often created semiautomatically, e.g., by extending manually labeled seed sets of sentiment words or adding for each word its syno-/antonyms. Alternatively, words frequently cooccurring with a seed set of manually labeled sentiment words are added (Turney, 2002; Kiritchenko et al., 2014). Heerschop et al. (2011) used WordNet together with a PageRank-based algorithm to propagate the sentiment of the seed set to unknown words. Scheible (2010) presented a semi-automatic approach based on machine translation of sentiment lexicons. The winning system of SemEval2015 10E (Amir et al., 2015) was based on structured skipgram embeddings with 600 dimensions and support vector regression with RBF kernels. Hamdan et al. (2015), the second ranked team, used the average of six sentiment lexicons as a final sentiment score, a method that cannot be applied to low resource languages. We showed that the lexicons created by D ENSIFIER achieve better performance than other semi-automatically created lexicons. Tang et al. (2014b) train sentiment specific embeddings by extending Collobert & Weston’s model and Tang et al. (2014a)’s skip-gram model. The first model automatically labels tweets as"
N16-1091,W15-2915,1,0.881529,"Missing"
N16-1091,N15-1184,0,0.01525,"than τ = 4 Here, we tune αs on train (equals trial data of SemEval2015 task 10E). This seems to work due to the different 774 .654 (line 6) for the setup in which we used several large resources, but still better than all previous work. This indicates that D ENSIFIER is especially suited for languages or domains for which little training data is available. 6 Related Work To the best of our knowledge, this paper is the first to train an orthogonal transformation to reorder word embedding dimensions into ultradense subspaces. However, there is much prior work on postprocessing word embeddings. Faruqui et al. (2015) perform postprocessing based on a semantic lexicon with the goal of finetuning word embeddings. Their transformation is not orthogonal and therefore does not preserve distances. They show that their approach optimizes word embeddings for a given application, i.e., word similarity, but also that it worsens them for other applications like detecting syntactic relations. Faruqui et al. (2015)’s approach also does not have the beneobjectives for training (maximize/minimize difference) and development (correlation). fit of ultradense embeddings, in particular the benefit of increased efficiency. I"
N16-1091,W13-1609,0,0.0432148,"Missing"
N16-1091,S15-2095,0,0.0583775,"uggests that word embeddings represent sentiment and concreteness much better than frequency. The reason for this likely is the learning objective of word embeddings: modeling the context. Infrequent words can occur in frequent contexts. Thus, the frequency information in a single word embedding is limited. In contrast negative words are likely to occur in negative contexts. The nine output lexicons in Table 1 – each a list of words annotated with predicted strength on one of three properties – are available at www.cis.lmu. de/˜sascha/Ultradense/. 771 1 2 3 4 5 6 7 8 system Amir et al. (2015) Hamdan et al. (2015) Zhang et al. (2015) ¨ Ozdemir and Bergler (2015) Plotnikova et al. (2015) D ENSIFIER Sentiment140 D ENSIFIER, trial only all .626† .621† .591† .584† .577† .654† .508† .627† τ ∩ .650† .538† Table 3: Results for Lexicon Creation. The first column gives the correlation with the entire test lexicon of SemEval2015 10E, the last column only on the intersection of our output lexicon and Sentiment140. Of the 1315 words of task 10E, 985 and 1308 are covered by D ENSIFIER and Sentiment140, respectively. †: significantly worse than the best (bold) result in the same column (α = .05, Fisher z-transformat"
N16-1091,P14-1062,0,0.00905621,"reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information – sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space. 1 Introduction Embeddings are useful for many tasks, including word similarity (e.g., Pennington et al. (2014)), named entity recognition (NER) (e.g., Collobert et al. (2011)) and sentiment analysis (e.g., Kim (2014), Kalchbrenner et al. (2014), Severyn and Moschitti (2015)). Embeddings are generic representations, containing different types of information about a word. Statistical models can be trained to make best use of these generic representations for a specific application like NER or sentiment analysis (Ebert et al., 2015). Our hypothesis in this paper is that the information useful for any given task is contained in an ultradense subspace Eu . We propose the new method D ENSIFIER to identify Eu . Given a set of word embeddings, D ENSIFIER learns an orthogonal transformation of the original space Eo on a task-specific trainin"
N16-1091,D14-1181,0,0.0157522,"D ENSI FIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information – sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space. 1 Introduction Embeddings are useful for many tasks, including word similarity (e.g., Pennington et al. (2014)), named entity recognition (NER) (e.g., Collobert et al. (2011)) and sentiment analysis (e.g., Kim (2014), Kalchbrenner et al. (2014), Severyn and Moschitti (2015)). Embeddings are generic representations, containing different types of information about a word. Statistical models can be trained to make best use of these generic representations for a specific application like NER or sentiment analysis (Ebert et al., 2015). Our hypothesis in this paper is that the information useful for any given task is contained in an ultradense subspace Eu . We propose the new method D ENSIFIER to identify Eu . Given a set of word embeddings, D ENSIFIER learns an orthogonal transformation of the original space E"
N16-1091,S13-2053,0,0.0354605,"the same column (α = .05, Fisher z-transformation). 4.3 Determining Association Strength We also evaluate lexicon creation on SemEval2015 Task 10E. As before, the task is to predict the sentiment score of words/phrases. We use the trial data of the task to tune the hyperparameter, αs = .4. Out-of-vocabulary words were predicted as neutral (7/1315). Table 3 shows that the lexicon computed by D ENSIFIER (line 5, Table 1) has a τ of .654 (line 6, column all), significantly better than all other systems, including the winner of SemEval 2015 (τ = .626, line 1). D ENSIFIER also beats Sentiment140 (Mohammad et al., 2013), a widely used semi-automatic sentiment lexicon. The last column is τ on the intersection of D ENSIFIER and Sentiment140. It shows that D ENSIFIER again performs significantly better than Sentiment140. correlation 0.8 0.6 0.4 Ultradense PCA Random 0.2 0 0 50 100 150 200 250 300 size of subspace correlation 0.7 0.6 0.5 Sentiment Concreteness 0.4 0.3 10 1 10 2 10 3 10 4 size of lexicon Figure 2: Kendall’s τ versus subspace size (top) and training resource size (bottom). See lines 6 & 8, Table 1, for train/test split. 4.4 Text Polarity Classification We now show that ultradense embeddings decrea"
N16-1091,S15-2081,0,0.0359555,"Missing"
N16-1091,D14-1162,0,0.100653,"Missing"
N16-1091,perez-rosas-etal-2012-learning,0,0.0160388,"Missing"
N16-1091,S15-2103,0,0.0418664,"Missing"
N16-1091,S15-2078,0,0.0616064,"Missing"
N16-1091,P15-1173,1,0.857606,"Missing"
N16-1091,P10-3005,0,0.0207123,"our method is also related to Oriented PCA (Diamantaras and Kung, 1996). However in contrast to PCA a solution for Oriented PCA is not orthogonal. Sentiment lexicons are often created semiautomatically, e.g., by extending manually labeled seed sets of sentiment words or adding for each word its syno-/antonyms. Alternatively, words frequently cooccurring with a seed set of manually labeled sentiment words are added (Turney, 2002; Kiritchenko et al., 2014). Heerschop et al. (2011) used WordNet together with a PageRank-based algorithm to propagate the sentiment of the seed set to unknown words. Scheible (2010) presented a semi-automatic approach based on machine translation of sentiment lexicons. The winning system of SemEval2015 10E (Amir et al., 2015) was based on structured skipgram embeddings with 600 dimensions and support vector regression with RBF kernels. Hamdan et al. (2015), the second ranked team, used the average of six sentiment lexicons as a final sentiment score, a method that cannot be applied to low resource languages. We showed that the lexicons created by D ENSIFIER achieve better performance than other semi-automatically created lexicons. Tang et al. (2014b) train sentiment spec"
N16-1091,S15-2079,0,0.0137472,"lexicon creation task in which words are annotated with three types of lexical information – sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space. 1 Introduction Embeddings are useful for many tasks, including word similarity (e.g., Pennington et al. (2014)), named entity recognition (NER) (e.g., Collobert et al. (2011)) and sentiment analysis (e.g., Kim (2014), Kalchbrenner et al. (2014), Severyn and Moschitti (2015)). Embeddings are generic representations, containing different types of information about a word. Statistical models can be trained to make best use of these generic representations for a specific application like NER or sentiment analysis (Ebert et al., 2015). Our hypothesis in this paper is that the information useful for any given task is contained in an ultradense subspace Eu . We propose the new method D ENSIFIER to identify Eu . Given a set of word embeddings, D ENSIFIER learns an orthogonal transformation of the original space Eo on a task-specific training set. The orthogonality of th"
N16-1091,C14-1018,0,0.046026,"ed set to unknown words. Scheible (2010) presented a semi-automatic approach based on machine translation of sentiment lexicons. The winning system of SemEval2015 10E (Amir et al., 2015) was based on structured skipgram embeddings with 600 dimensions and support vector regression with RBF kernels. Hamdan et al. (2015), the second ranked team, used the average of six sentiment lexicons as a final sentiment score, a method that cannot be applied to low resource languages. We showed that the lexicons created by D ENSIFIER achieve better performance than other semi-automatically created lexicons. Tang et al. (2014b) train sentiment specific embeddings by extending Collobert & Weston’s model and Tang et al. (2014a)’s skip-gram model. The first model automatically labels tweets as positive/negative based on emoticons, a process that cannot be easily transferred to other domains like news. The second uses the Urban Dictionary to expand a small list of 350 sentiment seeds. In our work, we 775 showed that a training resource of about the same size is sufficient without an additional dictionary. D ENSIFIER differs from this work in that it does not need a text corpus, but can transform existing, publicly ava"
N16-1091,P14-1146,0,0.041572,"ed set to unknown words. Scheible (2010) presented a semi-automatic approach based on machine translation of sentiment lexicons. The winning system of SemEval2015 10E (Amir et al., 2015) was based on structured skipgram embeddings with 600 dimensions and support vector regression with RBF kernels. Hamdan et al. (2015), the second ranked team, used the average of six sentiment lexicons as a final sentiment score, a method that cannot be applied to low resource languages. We showed that the lexicons created by D ENSIFIER achieve better performance than other semi-automatically created lexicons. Tang et al. (2014b) train sentiment specific embeddings by extending Collobert & Weston’s model and Tang et al. (2014a)’s skip-gram model. The first model automatically labels tweets as positive/negative based on emoticons, a process that cannot be easily transferred to other domains like news. The second uses the Urban Dictionary to expand a small list of 350 sentiment seeds. In our work, we 775 showed that a training resource of about the same size is sufficient without an additional dictionary. D ENSIFIER differs from this work in that it does not need a text corpus, but can transform existing, publicly ava"
N16-1091,P02-1053,0,0.0144376,"dding spaces of different languages to exploit similarities. They normalized word embeddings in a first step, something that did not improve our results. As a reviewer pointed out, our method is also related to Oriented PCA (Diamantaras and Kung, 1996). However in contrast to PCA a solution for Oriented PCA is not orthogonal. Sentiment lexicons are often created semiautomatically, e.g., by extending manually labeled seed sets of sentiment words or adding for each word its syno-/antonyms. Alternatively, words frequently cooccurring with a seed set of manually labeled sentiment words are added (Turney, 2002; Kiritchenko et al., 2014). Heerschop et al. (2011) used WordNet together with a PageRank-based algorithm to propagate the sentiment of the seed set to unknown words. Scheible (2010) presented a semi-automatic approach based on machine translation of sentiment lexicons. The winning system of SemEval2015 10E (Amir et al., 2015) was based on structured skipgram embeddings with 600 dimensions and support vector regression with RBF kernels. Hamdan et al. (2015), the second ranked team, used the average of six sentiment lexicons as a final sentiment score, a method that cannot be applied to low re"
N16-1091,H05-1044,0,0.104317,"corpus (in billion). #types: size of output lexicon (in million). For each resource, we give its size (“#words”) and the size of the intersection of resource and embedding set (“∩”). Kendall’s τ is computed on “∩”. publicly available. We use word2vec to train 400dimensional embeddings for English on a 2013 Twitter corpus of size 5.4 ×109 . For Czech, German and Spanish, we train embeddings on web data of sizes 3.3, 8.0 and 3.8 ×109 , respectively. We use the following lexicon resources for sentiment: SubLex 1.0 (Veselovsk´a and Bojar, 2013) for Czech; WHM for English [the combination of MPQA (Wilson et al., 2005), Opinion Lexicon (Hu and Liu, 2004) and NRC Emotion lexicons (Mohammad and Turney, 2013)]; FEEL (Abdaoui et al., 2014) for French; German Polarity Clues (Waltinger, 2010) for German; and the sentiment lexicon of P´erez-Rosas et al. (2012) for Spanish. For concreteness, we use BWK, a lexicon of 40,000 English words (Brysbaert et al., 2014). For frequency, we exploit the fact that word2vec stores words in frequency order; thus, the ranking provided by word2vec is our lexicon resource for frequency. For a resource/embedding-set pair (l, E), we intersect the vocabulary of l with the top 80,000 wo"
N16-1091,N15-1104,0,0.0165744,"t also that it worsens them for other applications like detecting syntactic relations. Faruqui et al. (2015)’s approach also does not have the beneobjectives for training (maximize/minimize difference) and development (correlation). fit of ultradense embeddings, in particular the benefit of increased efficiency. In a tensor framework, Rothe and Sch¨utze (2015) transform the word embeddings to sense (synset) embeddings. In their work, all embeddings live in the same space whereas we explicitly want to change the embedding space to create ultradense embeddings with several desirable properties. Xing et al. (2015) restricted the work of Mikolov et al. (2013) to an orthogonal transformation to ensure that normalized embeddings stay normalized. This transformation is learned between two embedding spaces of different languages to exploit similarities. They normalized word embeddings in a first step, something that did not improve our results. As a reviewer pointed out, our method is also related to Oriented PCA (Diamantaras and Kung, 1996). However in contrast to PCA a solution for Oriented PCA is not orthogonal. Sentiment lexicons are often created semiautomatically, e.g., by extending manually labeled s"
N16-1091,Q15-1017,0,0.0155736,"ons, a process that cannot be easily transferred to other domains like news. The second uses the Urban Dictionary to expand a small list of 350 sentiment seeds. In our work, we 775 showed that a training resource of about the same size is sufficient without an additional dictionary. D ENSIFIER differs from this work in that it does not need a text corpus, but can transform existing, publicly available word embeddings. D ENSIFIER is independent of the embedding learning algorithm and therefore extensible to other word embedding models like GloVe (Pennington et al., 2014), to phrase embeddings (Yu and Dredze, 2015) and even to sentence embeddings (Kiros et al., 2015). 7 Conclusion We have introduced D ENSIFIER, a method that is trained to focus embeddings used for an application to an ultradense subspace that contains the information relevant for the application. In experiments on SemEval, we demonstrate two benefits of the ultradense subspace. (i) Information is preserved even if we focus on a subspace that is smaller by a factor of 100 than the original space. This means that unnecessary noisy information is removed from the embeddings and robust learning without overfitting is better supported. (ii)"
N16-1091,S15-2094,0,0.034315,"Missing"
N16-1091,waltinger-2010-germanpolarityclues,0,\N,Missing
N16-1097,D14-1164,0,0.519065,"in Section 7 and describe the resources we publish in Section 8. 2 Related Work Slot filling. The participants of the SF Shared Task (Surdeanu, 2013) are provided with a large text corpus. For evaluation, they get a collection of queries and need to provide fillers for predefined relations and an offset of a context which can serve as a justification. Most participants apply pipeline based systems. Pink et al. (2014) analyzed sources of recall losses in these pipelines. The results of the systems show the difficulty of the task: In the 2014 evaluation, the top-ranked system had an F1 of .37 (Angeli et al., 2014a). To train their models, most groups use distant supervision (Mintz et al., 2009). The topranked systems apply machine learning based approaches rather than manually developed patterns or models (Surdeanu and Ji, 2014). The methods for extracting and scoring candidates range from pattern based approaches (Gonz`alez et al., 2012; Liu and Zhao, 2012; Li et al., 2012; Qiu et al., 2012; Roth et al., 2014) over rule based systems (Varma et al., 2012) to classifiers (Malon et al., 2012; Roth et al., 2013). The top ranked system from 2013 used SVMs and patterns for evaluating filler candidates (Rot"
N16-1097,P15-1061,0,0.0291321,"ng candidates range from pattern based approaches (Gonz`alez et al., 2012; Liu and Zhao, 2012; Li et al., 2012; Qiu et al., 2012; Roth et al., 2014) over rule based systems (Varma et al., 2012) to classifiers (Malon et al., 2012; Roth et al., 2013). The top ranked system from 2013 used SVMs and patterns for evaluating filler candidates (Roth et al., 2013); their results suggest that n-gram based features are sufficient to build reliable classifiers for the relation classification module. They also show that SVMs outperform patterns. CNNs for relation classification. Zeng et al. (2014) and Dos Santos et al. (2015) apply CNNs to the relation classification SemEval Shared Task data from 2010 and show that CNNs outperform other models. We train CNNs on noisy distant supervised data since (in contrast to the SemEval Shared Task) clean training sets are not available. Malon et al. (2012) describe a CNN for slot filling that is based on the output of a parser. We plan to explore parsing for creating a more linguistically motivated input representation in the future. Baseline models. In this paper, we will compare our methods against traditional relation classification models: Mintz++ (Mintz et al., 2009; Sur"
N16-1097,W07-0717,0,0.0358367,"split the context even earlier and apply the convolutional filters to each part separately. Genre dependency. There are many studies showing the genre dependency of machine learning models. In 2012, the SANCL Shared Task fo830 cused on evaluating models on web data that have been trained on news data (Petrov and McDonald, 2012). The results show that POS tagging performance can decline a lot when the genre is changed. For other NLP tasks like machine translation or sentiment analysis, this is also a well-known challenge and domain adaptation has been extensively studied (Glorot et al., 2011; Foster and Kuhn, 2007). We do not investigate domain adaptation per se, but show that the genre composition of the slot filling source corpus poses challenges to genre independent models. 3 Challenges of Slot Filling Slot filling includes NLP challenges of various natures. Given a large evaluation corpus, systems first need to find documents relevant to the entity of the query. This involves challenges like alternate names for the same entity, misspellings of names and ambiguous names (different entities with the same name). Then for each relevant document, sentences with mentions of the entity need to be extracted"
N16-1097,S10-1006,0,0.0304932,"Missing"
N16-1097,P14-1062,0,0.0100402,"a feature vector which contains skip n-gram features. They wildcard tokens in the middle of the n-gram (cf. Roth et al. (2013)). In particular, we use skip 3-grams, skip 4-grams and skip 5-grams. A possible skip 4-gram of the context “, founder and director of”, for example, would be the string “founder of”, a pattern that could not have been directly extracted from this context otherwise. We train one linear SVM (Fan et al., 2008) for each relation and feature set and tune parameter C on dev. Convolutional neural networks (CNNs). CNNs are increasingly applied in NLP (Collobert et al., 2011; Kalchbrenner et al., 2014). They extract ngram based features independent of the position in the sentence and create (sub-)sentence representations. The two most important aspects that make this possible are convolution and pooling. Max pooling (Collobert et al., 2011) detects the globally most relevant features obtained by local convolution. Another promising aspect of CNNs for relation classification is that they use an embedding based in831 put representation. With word embeddings, similar words are represented by similar vectors and, thus, we can recognize (near-)synonyms – synonyms of relation triggers as well as"
N16-1097,P14-5010,0,0.00361726,"oth arguments of a relation in the tuple set: (i) the TAC source corpus (TAC, 2014), (ii) a snapshot of Wikipedia (May 2014), (iii) the Freebase description fields, (iv) a subset of Clueweb2 , (v) a New York Times corpus (LDC2008T19). The resulting sentences are positive training examples. Based on the tuple set, we selected negative examples by scanning the corpora for sentences that (i) contain a mention of a name occurring in a tuple, (ii) do not contain the correct filler, (iii) contain a mention different from the correct filler, but with the same named entity type (based on CoreNLP NER (Manning et al., 2014)). All negative examples for date slots, for instance, are sentences containing an incorrect date. This procedure gave us a large but noisy training set for most slots. In order to reduce incorrect labels, we applied a self-training procedure: We trained SVMs on the SF dataset created by Angeli et al. (2014b). With the resulting SVMs, we predicted labels for our training set. If the predicted label did not match the distant supervised label, we deleted the corresponding training example (Min et al., 2012). This procedure was conducted in several iterations on different chunks of the training s"
N16-1097,P09-1113,0,0.786149,"ot filling. The participants of the SF Shared Task (Surdeanu, 2013) are provided with a large text corpus. For evaluation, they get a collection of queries and need to provide fillers for predefined relations and an offset of a context which can serve as a justification. Most participants apply pipeline based systems. Pink et al. (2014) analyzed sources of recall losses in these pipelines. The results of the systems show the difficulty of the task: In the 2014 evaluation, the top-ranked system had an F1 of .37 (Angeli et al., 2014a). To train their models, most groups use distant supervision (Mintz et al., 2009). The topranked systems apply machine learning based approaches rather than manually developed patterns or models (Surdeanu and Ji, 2014). The methods for extracting and scoring candidates range from pattern based approaches (Gonz`alez et al., 2012; Liu and Zhao, 2012; Li et al., 2012; Qiu et al., 2012; Roth et al., 2014) over rule based systems (Varma et al., 2012) to classifiers (Malon et al., 2012; Roth et al., 2013). The top ranked system from 2013 used SVMs and patterns for evaluating filler candidates (Roth et al., 2013); their results suggest that n-gram based features are sufficient to"
N16-1097,D14-1089,0,0.0543792,"eebase and some slots even comprise more than one Freebase relation. While most relation classification benchmarks either use newswire or web data, the SF task includes documents from both domains (and discussion fora). Another difference to traditional relation classification benchmarks arises from the pipeline aspect of slot filling. Depending on the previous steps, the input for the relation classification models can be incomplete, noisy, include coreferent mentions, etc. The official SF Shared Task evaluations only assess whole systems (with potential subsequent faults in their pipelines (Pink et al., 2014)). Thus, we expect component wise comparisons to be a valuable addition to the Shared Task: With comparisons of single components, teams would be able to improve their modules more specifically. To start with one of the most important components, we have created a benchmark for slot filling relation classification, based on 2012 – 2014 TAC Shared Task data. It will be described below and published along with this paper.1 In addition to presenting model results on this benchmark dataset, we also show that these results correlate with end-to-end SF results. Hence, optimizing a model on this data"
N16-1097,N13-1008,0,0.181303,"o benchmark for slot filling. Therefore, it is not possible to directly compare results that were submitted to the Shared Task to new results. Comparable manual annotations for new results, for instance, cannot be easily obtained. There are also many different system components, such as document retrieval from the evaluation corpus and coreference resolution, that affect Shared Task performance and that are quite different in nature from relation classification. Even in the subtask of relation classification, it is not possible to directly use existing relation classification benchmarks (e.g. Riedel et al. (2013), Hendrickx et al. (2010)) since data and relations can be quite different. Many benchmark relations, for instance, correspond to Freebase relations but not all slots are modeled in Freebase and some slots even comprise more than one Freebase relation. While most relation classification benchmarks either use newswire or web data, the SF task includes documents from both domains (and discussion fora). Another difference to traditional relation classification benchmarks arises from the pipeline aspect of slot filling. Depending on the previous steps, the input for the relation classification mod"
N16-1097,D12-1042,0,0.12848,"15) apply CNNs to the relation classification SemEval Shared Task data from 2010 and show that CNNs outperform other models. We train CNNs on noisy distant supervised data since (in contrast to the SemEval Shared Task) clean training sets are not available. Malon et al. (2012) describe a CNN for slot filling that is based on the output of a parser. We plan to explore parsing for creating a more linguistically motivated input representation in the future. Baseline models. In this paper, we will compare our methods against traditional relation classification models: Mintz++ (Mintz et al., 2009; Surdeanu et al., 2012) and MIMLRE (Surdeanu et al., 2012). Mintz++ is a model based on the Mintz features (lexical and syntactic features for relation extraction). It was developed by Surdeanu et al. (2012) and used as a baseline model by them. MIMLRE is a graphical model designed to cope with multiple instances and multiple labels in distant supervised data. It is trained with Expectation Maximization. Another baseline model which we use in this work is a piecewise convolutional neural network (Zeng et al., 2015). This recently published network is designed especially for the relation classification task which all"
N16-1097,P15-1018,0,0.0516255,"of parameters and increases robustness. We also found in initial experiments that sharing filter weights across left, middle, right outperformed not sharing weights. The results of convolution are pooled using k-max pooling (Kalchbrenner et al., 0|1 softmax h hidden units fully connected MLP flatten concat flatten k-max pooling 1/0 k-max pooling k-max pooling *W *W wordvector, case indicator middle context where qm is the score of model m and αm is its weight (optimized on dev using grid search). All weights sum to 1. For a comparison of different combination possibilities, see, for example, (Viswanathan et al., 2015). 5 w1 w2 … wc-1 wc &lt;> wc+1 wc+2 … w2c-1 w2c &lt;> w2c+1w2c+2 … w3c-1w3c left context m=1...M flatten n *W ear combination of the scores of the models: X q CMB = α m qm right context split at relation arguments input sentence Figure 1: Contextwise CNN for relation classification 2014): only the k = 3 maximum values of each filter application are kept. The pooling results are then concatenated to a single vector and extended by a flag indicating whether the name or the filler appeared first in the sentence. In initial experiments, we found that a fully connected hidden layer after convolution and"
N16-1097,N16-1065,1,0.803401,"Missing"
N16-1097,C14-1220,0,0.0678781,"ds for extracting and scoring candidates range from pattern based approaches (Gonz`alez et al., 2012; Liu and Zhao, 2012; Li et al., 2012; Qiu et al., 2012; Roth et al., 2014) over rule based systems (Varma et al., 2012) to classifiers (Malon et al., 2012; Roth et al., 2013). The top ranked system from 2013 used SVMs and patterns for evaluating filler candidates (Roth et al., 2013); their results suggest that n-gram based features are sufficient to build reliable classifiers for the relation classification module. They also show that SVMs outperform patterns. CNNs for relation classification. Zeng et al. (2014) and Dos Santos et al. (2015) apply CNNs to the relation classification SemEval Shared Task data from 2010 and show that CNNs outperform other models. We train CNNs on noisy distant supervised data since (in contrast to the SemEval Shared Task) clean training sets are not available. Malon et al. (2012) describe a CNN for slot filling that is based on the output of a parser. We plan to explore parsing for creating a more linguistically motivated input representation in the future. Baseline models. In this paper, we will compare our methods against traditional relation classification models: Min"
N16-1097,D15-1203,0,0.39768,"roach is a convolutional neural network (CNN). CNNs have been applied to NLP tasks like sentiment analysis, part-of-speech tagging and semantic role labeling. They can recognize phrase patterns independent of their position in the sentence. Furthermore, they make use of word embeddings that directly reflect word similarity (Mikolov et al., 2013). Hence, we expect them to be robust models for the task of classifying filler candidates and to generalize well to unseen test data. In this work, we train different variants of CNNs: As a baseline, we reimplement the recently developed piecewise CNN (Zeng et al., 2015). Then, we extend this model by splitting the contexts not only for 828 Proceedings of NAACL-HLT 2016, pages 828–838, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics pooling but also for convolution (contextwise CNN). Currently, there is no benchmark for slot filling. Therefore, it is not possible to directly compare results that were submitted to the Shared Task to new results. Comparable manual annotations for new results, for instance, cannot be easily obtained. There are also many different system components, such as document retrieval from the eva"
N18-1003,W12-3004,0,\N,Missing
N18-1003,D11-1142,0,\N,Missing
N18-1003,D15-1056,0,\N,Missing
N18-1003,P15-1034,0,\N,Missing
N18-1003,D13-1079,0,\N,Missing
N18-1005,Q16-1031,0,0.0258793,"character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several languages. They report"
N18-1005,D16-1097,1,0.923198,"Missing"
N18-1005,P17-1182,1,0.891715,"Missing"
N18-1005,W10-2210,0,0.513559,"only on the available annotated data. Differences Between Multi-task Training and Data Augmentation The difference between MTT-U (resp. MTT-R) and DA-U (resp. MTT-U) is a single element in the input sequence (the one representing the task). Semi-supervised MORFESSOR (MORF). We further compare to the semi-supervised version 51 auxiliary task of autoencoding corpus data are m = 4 for Mexicanero, Nahuatl and Wixarika and m = 1 for Yorem Nokki. For multi-task training with autoencoding of random strings we select m = 8 for Mexicanero, Nahuatl and Yorem Nokki and m = 4 for Wixarika. of MORFESSOR (Kohonen et al., 2010), a wellknown morphological segmentation system. During training, we tune the hyperparameters for each language on the respective development set. The best performing model is applied to the test set. FlatCat (FC). Our next baseline is FlatCat (Gr¨onroos et al., 2014), a variant of MORFESSOR. It consists of a hidden Markov model for segmentation. The states of the model correspond either to a word boundary and one of the four morph categories stem, prefix, suffix, and nonmorpheme. It can work in an unsupervised way, but, similar to the previous baseline, can make effective use of small amounts"
N18-1005,D11-1005,0,0.0291798,"end of a morpheme, or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentati"
N18-1005,K15-1017,1,0.9191,"Missing"
N18-1005,N16-1080,1,0.908981,"Missing"
N18-1005,W02-0603,0,0.941577,"important for polysynthetic languages with a high morpheme-to-word ratio and a consequently large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Mexicanero frq. m. 136 ni 128 ki 114 ti 105 u 70 s 44 mo 42 ka 39 a 31"
N18-1005,P08-1115,0,0.048669,"ure research. 1 Recovering morphemes provides information about unknown words and is thus especially important for polysynthetic languages with a high morpheme-to-word ratio and a consequently large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Associa"
N18-1005,W13-3504,0,0.166783,"s with a high morpheme-to-word ratio and a consequently large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Mexicanero frq. m. 136 ni 128 ki 114 ti 105 u 70 s 44 mo 42 ka 39 a 31 nich 31 $i 24 ta 24 l 22 tahtanili 21"
N18-1005,J01-2001,0,0.0744251,"(Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several languages. They reported better results than earlier work, including semi-supervised approaches. In the following year, they extended their approach to be able to use unlabeled data as well, further improving performance (Ruokolainen et al., 2014). Cott"
N18-1005,E14-4017,0,0.574739,"large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Mexicanero frq. m. 136 ni 128 ki 114 ti 105 u 70 s 44 mo 42 ka 39 a 31 nich 31 $i 24 ta 24 l 22 tahtanili 21 no 17 ya 17 t 17 ke 17 ita 16 piya 15 an applicable to"
N18-1005,K17-1020,0,0.0489552,"q approaches for morphological segmentation we know of focused on canonical segmentation (Cotterell et al., 2016) which differs from the surface segmentation task considered here in that it restores changes to the surface form of morphemes which occurred during word formation. Kann et al. (2016) also used an encoder-decoder RNN and combined it with a neural reranker. While our model architecture was inspired by them, their model was purely supervised. Additionally, they did not investigate the applicability of their neural seq2seq model in low-resource settings or for polysynthetic languages. Ruzsics and Samardzic (2017) extended the standard encoder-decoder architecture for canonical segmentation to contain a language model over segments and improved results. However, a big difference to our work is that they still used more than ten times as much training data as we have available for the indigenous Mexican languages we are working on here. Another neural approach—this time for surface segmentation—was presented by Wang et al. 9 Conclusion and Future Work We first investigated the applicability of neural seq2seq models to morphological surface segmentation for polysynthetic languages in minimalresource sett"
N18-1005,C14-1111,0,0.425303,"Missing"
N18-1005,L16-1666,0,0.0646046,"ent set separately for each language. We explore m times the amount of instances in the original training set, with m ∈ {1, 2, 4, 8}. The reasons why we expect our data augmentation methods to lead to better segmentation models are similar to those for multi-task training. We call models trained on datasets augmented with unlabeled corpus data or random strings DAU or DA-R, respectively. 5.3 Experiments Data We apply our models to the datasets described in §3. For the multi-task training and data augmentation using unlabeled data, we use (unsegmented) words from a parallel corpus collected by Gutierrez-Vasques et al. (2016) for Nahuatl and the closely related Mexicanero. For Wixarika we use data from Mager et al. (2018) and for Yorem Nokki we use text from Maldonado Mart´ınez et al. (2010). 6.2 Baselines Now, we will describe the baselines we use to evaluate the overall performance of our approaches. Supervised seq2seq RNN (S2S). As a first baseline, we employ a fully supervised neural model without data augmentation or multi-task training, i.e., an attention-based encoder-decoder RNN (Bahdanau et al., 2015) which has been trained only on the available annotated data. Differences Between Multi-task Training and"
N18-1005,P11-2120,0,0.0307297,"or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several l"
N18-1005,N16-1161,0,0.0252151,"r either as the beginning, middle or end of a morpheme, or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied"
N18-1005,Q14-1005,0,0.0319037,"lem and use LSTMs to classify every character either as the beginning, middle or end of a morpheme, or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explic"
N18-1005,P08-1101,0,0.0603964,"or many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several languages. They reported better results than earlier work, including semi-supervised approaches. In the following year, they extended their approach to be able to use unlabeled data as well, further improving performance (Ruokolainen et al., 2014). Cotterell et al. (2015) trained a semi-Markov CRF (semi-CRF) (Sarawagi and Cohen, 2005) jointly on morphological segmentation, stemming and tagging. For the similar problem of Chinese word segmentation, Zhang and Clark (2008) trained a model jointly on part-of-speech tagging. However, we are not aware of any prior work on multi-task training or data augmentation for neural segmentation models. In fact, the two only neural seq2seq approaches for morphological segmentation we know of focused on canonical segmentation (Cotterell et al., 2016) which differs from the surface segmentation task considered here in that it restores changes to the surface form of morphemes which occurred during word formation. Kann et al. (2016) also used an encoder-decoder RNN and combined it with a neural reranker. While our model archite"
N18-1005,Q17-1024,0,\N,Missing
N18-1098,W13-0102,0,0.0282296,"rithm, same context, multiple words input, inference, word vector, word vectors, vector representation, semantic representation, distributional models, semantic space, space model, semantic parser, vector representations, neural language, logical forms, cosine similarity, clustering algorithm Table 4: Topics (top 15 words) with the highest and lowest drifts (cosine) observed in DTM and RNN-RSM 3.4 Beyond perplexities, we also compute topic coherence (Chang et al., 2009; Newman et al., 2009; Das et al., 2015) to determine the meaningful topics captured. We use the coherence measure proposed by Aletras and Stevenson (2013) that retrieves co-occurrence counts for the set of topic words using Wikipedia as a reference corpus to identify context features (window=5) for each topic word. Relatedness between topic words and context features is measured using normalized pointwise mutual information (NPMI), resulting in a single vector for every topic word. The coherence (COH) score is computed as the arithmetic mean of the cosine similarities between all word pairs. Higher scores imply more coherent topics. We use Palmetto4 library to estimate coherence. Quantitative: We compute mean and median coherence scores for eac"
N18-1098,P15-1077,0,0.0341608,"orithm, space models, similar word, frequent word, meaning representation, lexical acquisition, new algorithm, same context, multiple words input, inference, word vector, word vectors, vector representation, semantic representation, distributional models, semantic space, space model, semantic parser, vector representations, neural language, logical forms, cosine similarity, clustering algorithm Table 4: Topics (top 15 words) with the highest and lowest drifts (cosine) observed in DTM and RNN-RSM 3.4 Beyond perplexities, we also compute topic coherence (Chang et al., 2009; Newman et al., 2009; Das et al., 2015) to determine the meaningful topics captured. We use the coherence measure proposed by Aletras and Stevenson (2013) that retrieves co-occurrence counts for the set of topic words using Wikipedia as a reference corpus to identify context features (window=5) for each topic word. Relatedness between topic words and context features is measured using normalized pointwise mutual information (NPMI), resulting in a single vector for every topic word. The coherence (COH) score is computed as the arithmetic mean of the cosine similarities between all word pairs. Higher scores imply more coherent topics"
N18-1098,D15-1235,0,0.0861899,"the dynamics of the underlying topics discovered over time. Problem Statement: We aim to generate temporal topical trends or automatic overview timelines of topics for a time sequence collection of documents. This involves the following three tasks in dynamic topic analysis: (1) Topic Structure Detection (TSD): Identifying main topics in the document collection. (2) Topic Evolution Detection (TED): Detecting the emergence of a new topic Probabilistic static topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Wang and McCallum, 2006; Hall et al., 2008; Gollapalli and Li, 2015) have been investigated to examine the emergence of topics from historical documents. Another variant known as Replicated Softmax (RSM) (Hinton and Salakhutdinov, 2009) has demonstrated better generalization in log-probability and retrieval, compared to LDA. Prior works (Iwata et al., 2010; Pruteanu-Malinici et al., 2010; Saha and Sindhwani, 2012; Schein et al., 2016) have investigated Bayesian modeling of topics in time-stamped documents. Particularly, Blei and Lafferty (2006) developed a LDA based dynamic topic model (DTM) to capture the evolution of topics in a time sequence collection of d"
N18-1098,C16-1239,1,0.574659,"Missing"
N18-1098,D08-1038,0,0.0172877,"tified by modeling the dynamics of the underlying topics discovered over time. Problem Statement: We aim to generate temporal topical trends or automatic overview timelines of topics for a time sequence collection of documents. This involves the following three tasks in dynamic topic analysis: (1) Topic Structure Detection (TSD): Identifying main topics in the document collection. (2) Topic Evolution Detection (TED): Detecting the emergence of a new topic Probabilistic static topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Wang and McCallum, 2006; Hall et al., 2008; Gollapalli and Li, 2015) have been investigated to examine the emergence of topics from historical documents. Another variant known as Replicated Softmax (RSM) (Hinton and Salakhutdinov, 2009) has demonstrated better generalization in log-probability and retrieval, compared to LDA. Prior works (Iwata et al., 2010; Pruteanu-Malinici et al., 2010; Saha and Sindhwani, 2012; Schein et al., 2016) have investigated Bayesian modeling of topics in time-stamped documents. Particularly, Blei and Lafferty (2006) developed a LDA based dynamic topic model (DTM) to capture the evolution of topics in a tim"
N18-1098,N16-1065,1,0.703332,"Missing"
N19-1048,Q17-1010,0,0.0954838,"or both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the mediumfrequency range. 1 Introduction Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavicencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to fine-tune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, 2 Related Work Methods to train surface-form models to mimic word embeddings include those of Luong et al. 489 Proceedings of NAACL-HLT 2019, pages 489–494 c Minneapolis, Minnesota, June 2 - Jun"
N19-1048,P16-1156,1,0.869748,"Missing"
N19-1048,P18-1002,0,0.281719,"ding to Informative Contexts ¨ Timo Schick Hinrich Schutze Sulzer GmbH Center for Information and Language Processing Munich, Germany LMU Munich, Germany timo.schick@sulzer.de inquiries@cislmu.org Abstract these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g. Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable"
N19-1048,D15-1161,0,0.0533169,"logical information into word embeddings. Vuli´c et al. (2017) retrofit embeddings using a set of language-specific rules. Models that directly incorporate surface-form information into embedding learning include fastText (Bojanowski et al., 2017), LexVec (Salle and Villavicencio, 2018) and Charagram (Wieting et al., 2016). While many approaches to learning embeddings for novel words exclusively make use of context information (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019)’s form-context model combines surface-form and context information. Ling et al. (2015) also use attention in embedding learning, but their attention is within a context (picking words), not across contexts (picking contexts). Also, their attention is based only on word type and distance, not on the more complex factors available in our attentive mimicking model, e.g., the interaction with the word’s surface form. 3 3.1 context form α = σ(u&gt; [v(w,C) ; v(w,C) ] + b) with u ∈ R2d , b ∈ R being learnable parameters and σ denoting the sigmoid function. 3.2 Context Attention FCM pays equal attention to all contexts of a word but often, only few contexts are actually suitable for infe"
N19-1048,W18-3013,1,0.85209,"Missing"
N19-1048,D17-1010,0,0.311732,"attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and mediumfrequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word embeddings on various datasets. Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word’s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequen"
N19-1048,N16-1091,1,0.837186,"Missing"
N19-1048,W18-1209,0,0.0404943,"requency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the mediumfrequency range. 1 Introduction Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavicencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to fine-tune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, 2 Related Work Methods to train surface-form models to mimic word embeddings include those of Luong et al. 489 Proceedings of NAACL-HLT 2019, pages 489–494 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for"
N19-1048,P17-1006,0,0.343097,"and then, surface-form information is used either to fine-tune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, 2 Related Work Methods to train surface-form models to mimic word embeddings include those of Luong et al. 489 Proceedings of NAACL-HLT 2019, pages 489–494 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics embeddings, modeled as (2013) (morpheme-based) and Pinter et al. (2017) (character-level). In the area of fine-tuning methods, Cotterell et al. (2016) introduce a Gaussian graphical model that incorporates morphological information into word embeddings. Vuli´c et al. (2017) retrofit embeddings using a set of language-specific rules. Models that directly incorporate surface-form information into embedding learning include fastText (Bojanowski et al., 2017), LexVec (Salle and Villavicencio, 2018) and Charagram (Wieting et al., 2016). While many approaches to learning embeddings for novel words exclusively make use of context information (Lazaridou et al., 2017; Her"
N19-1048,D16-1157,0,0.031391,"rforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the mediumfrequency range. 1 Introduction Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavicencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to fine-tune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, 2 Related Work Methods to train surface-form models to mimic word embeddings include those of Luong et al. 489 Proceedings of NAACL-HLT 2019, pages 489–494 c Minneapolis"
N19-1048,W13-3512,0,\N,Missing
N19-1280,P16-1039,0,0.0219249,") are used. Our model combines biLSTMs and grConv to model both the context of characters (LSTM) and the compositionality of language (grConv). Joint Segmentation and POS Tagging. The top performing models of EN, JA, VI and ZH use a pipeline of tokenizer and word-based POS tagger but do not treat both tasks jointly (Bj¨orkelund et al., 2017; Dozat et al., 2017; Kanayama et al., 2017; Qian and Liu, 2017). Especially for Chinese, there is a lot of work on joint word segmentation and POS tagging, e.g., (Zhang and Clark, 2008; Sun, 2011; Hatori et al., 2012; Zheng et al., 2013; Kong et al., 2015; Cai and Zhao, 2016; Chen et al., 2017; Shao et al., 2017), of which some use CRFs to predict one POS tag per character. However, this is hard to transfer to languages like English and Vietnamese where single characters are less informative and tokens are much longer, resulting in a larger combinatory label space. Thus, we choose a semi-Markov formalization to directly model segments. Semi-Markov CRFs for Sequence Tagging. Zhuo et al. (2016) and Ye and Ling (2018) apply semi-CRFs to word-level inputs for named entity recognition. In contrast, we model characterbased POS tagging. Thus, the expected length of our"
N19-1280,W14-4012,0,0.124298,"Missing"
N19-1280,K17-3002,0,0.145555,"86.33 92.46 86.64 Our Tokens POS 98.79 93.45 93.86 91.34 88.06 77.67 93.82 88.15 93.66 87.65 Table 2: Tokenization and joint token-POS F1 on UD v2.0. Best scores are in bold, second-best are underlined. this to the different way of feature creation: While grConv hierarchically combines context-enhanced n-grams, SRNN constructs segments in a sequential order. The latter may be less suited for compositional segments like “airport”. Baselines for UD v2.0. We compare to the top performing models for EN, JA, VI, ZH from the CoNLL 2017 shared task: UDPipe 1.2 (Straka and Strakov´a, 2017), Stanford (Dozat et al., 2017), FBAML (Qian and Liu, 2017), TRL (Kanayama et al., 2017), and IMS (Bj¨orkelund et al., 2017). Multilingual Results (UD v2.0). Table 2 provides our results. While for each language another shared task system performs best, our system performs consistently well across languages (best or second-best except for EN), leading to the best average scores for both tokenization and POS tagging. Moreover, it matches the state of the art for Chinese (ZH) and Vietnamese (VI), two languages with very different characteristics in tokenization. 3.2 who generate grammatical errors. We leave the investigation"
N19-1280,N16-1155,0,0.131216,"nts for computing running averages of the gradient and its square are 0.9 and 0.999, respectively. A term of 1e-8 is added to the denominator for numerical stability. We use character embeddings of size 60 and three stacked biLSTM layers with 100 hidden units for each direction. For the semi-CRF, we set the maximum segment length to L = 23 as tokens of bigger length are rarely seen in the training sets. To avoid overfitting, we apply dropout with a probability of 0.25 on each layer including the input. For input dropout, we randomly replace a character embedding with a zero vector, similar to Gillick et al. (2016). This avoids overfitting to local character patterns. Moreover, we employ early stopping on the development set with a minimum of 20 training epochs. We run our experiments on a gpu which speeds up the training compared to multiple cpu cores considerably. We assume that it especially benefits from parallelizing the computation of each level of the grConv pyramid. 3.1 Multilingual Experiments on Clean Data Data and Evaluation. To compare our model to state-of-the-art character-based POS taggers, we evaluate its accuracy on the English part of the Universal Dependencies (UD) v1.2 dataset (Nivre"
N19-1280,P12-1110,0,0.0275049,"at et al., 2017) or CNNs (Ma and Hovy, 2016; Yu et al., 2017) are used. Our model combines biLSTMs and grConv to model both the context of characters (LSTM) and the compositionality of language (grConv). Joint Segmentation and POS Tagging. The top performing models of EN, JA, VI and ZH use a pipeline of tokenizer and word-based POS tagger but do not treat both tasks jointly (Bj¨orkelund et al., 2017; Dozat et al., 2017; Kanayama et al., 2017; Qian and Liu, 2017). Especially for Chinese, there is a lot of work on joint word segmentation and POS tagging, e.g., (Zhang and Clark, 2008; Sun, 2011; Hatori et al., 2012; Zheng et al., 2013; Kong et al., 2015; Cai and Zhao, 2016; Chen et al., 2017; Shao et al., 2017), of which some use CRFs to predict one POS tag per character. However, this is hard to transfer to languages like English and Vietnamese where single characters are less informative and tokens are much longer, resulting in a larger combinatory label space. Thus, we choose a semi-Markov formalization to directly model segments. Semi-Markov CRFs for Sequence Tagging. Zhuo et al. (2016) and Ye and Ling (2018) apply semi-CRFs to word-level inputs for named entity recognition. In contrast, we model ch"
N19-1280,K17-3028,0,0.067207,"Missing"
N19-1280,K17-3001,0,0.0247637,"Missing"
N19-1280,D18-1541,0,0.0142838,"dropout for these experiments, since the corrupt tokenization already acts as a regularizer. Data. We are not aware of a POS tagging dataset with corrupted tokenization. Thus, we create one based on UD v1.2 (EN). For each token, we either delete the space after it with probability P = pd or insert a space between two characters with P = pi : ”The fox chased the rabbit” → ”The f ox cha sed therabbit”. We vary pd and pi to construct three datasets with different noise levels (LOW, MID, HIGH, see Table 3). We note that there are more sophisticated ways of creating “errors” in text. An example is Kasewa et al. (2018) pi 0.05 0.11 0.33 The f DET NOUN ox DET cha NOUN VERB rabbit NOUN sed VERB therabbit {DET|NOUN} Figure 2: Example of label assignment. Baseline. We compare our joint model to a traditional pipeline of tokenizer (UDpipe 1.0)5 and token-level POS tagger (MarMot).6 We re-train MarMot on the corrupted datasets. Evaluation. We evaluate the models on the noisy datasets using two different metrics: (i) tokenization and joint token-POS F1 as in Table 2, and (ii) a relaxed variant of POS tag accuracies. With the latter, we can assess the performance of MarMot without penalizing it for potential errors"
N19-1280,N16-1030,0,0.52214,"r-based neural networks (NNs) gained popularity for different tasks, ranging from text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016) to machine translation (Luong and Manning, 2016). Character-level models are attractive since they can effectively model morphological variants of words and build representations even for unknown words, suffering less from out-of-vocabulary problems (Pinter et al., 2017). However, most character-level models still rely on tokenization and use characters only for creating more robust token representations (Santos and Zadrozny, 2014; Lample et al., 2016; Ma and Hovy, 2016; Plank et al., 2016). This leads to high performance on well-formatted text or text with misspellings (Yu et al., 2017; Sakaguchi et al., 2017) but ties the performance to the quality of the tokenizer. While humans are very robust to * Work was done at Center for Information and Language Processing, LMU Munich. noise caused by insertion of spaces (e.g., “car nival”) or deletion of spaces (“deeplearning”), this can cause severe underperformance of machine learning models. Similar challenges arise for languages with difficult tokenization, such as Chinese or Vietnamese. For t"
N19-1280,P16-1100,0,0.0239711,"er and segment representations. We demonstrate its effectiveness in multilingual settings and when token boundaries are noisy: It matches state-of-the-art part-of-speech taggers for various languages and significantly outperforms them on a noisy English version of a benchmark dataset. Our code and the noisy dataset are publicly available at http: //cistern.cis.lmu.de/semiCRF. 1 Introduction Recently, character-based neural networks (NNs) gained popularity for different tasks, ranging from text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016) to machine translation (Luong and Manning, 2016). Character-level models are attractive since they can effectively model morphological variants of words and build representations even for unknown words, suffering less from out-of-vocabulary problems (Pinter et al., 2017). However, most character-level models still rely on tokenization and use characters only for creating more robust token representations (Santos and Zadrozny, 2014; Lample et al., 2016; Ma and Hovy, 2016; Plank et al., 2016). This leads to high performance on well-formatted text or text with misspellings (Yu et al., 2017; Sakaguchi et al., 2017) but ties the performance to t"
N19-1280,P16-1101,0,0.249573,"ks (NNs) gained popularity for different tasks, ranging from text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016) to machine translation (Luong and Manning, 2016). Character-level models are attractive since they can effectively model morphological variants of words and build representations even for unknown words, suffering less from out-of-vocabulary problems (Pinter et al., 2017). However, most character-level models still rely on tokenization and use characters only for creating more robust token representations (Santos and Zadrozny, 2014; Lample et al., 2016; Ma and Hovy, 2016; Plank et al., 2016). This leads to high performance on well-formatted text or text with misspellings (Yu et al., 2017; Sakaguchi et al., 2017) but ties the performance to the quality of the tokenizer. While humans are very robust to * Work was done at Center for Information and Language Processing, LMU Munich. noise caused by insertion of spaces (e.g., “car nival”) or deletion of spaces (“deeplearning”), this can cause severe underperformance of machine learning models. Similar challenges arise for languages with difficult tokenization, such as Chinese or Vietnamese. For text with difficult"
N19-1280,N15-1055,0,0.0726295,"Missing"
N19-1280,D17-1010,0,0.0255811,"ms them on a noisy English version of a benchmark dataset. Our code and the noisy dataset are publicly available at http: //cistern.cis.lmu.de/semiCRF. 1 Introduction Recently, character-based neural networks (NNs) gained popularity for different tasks, ranging from text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016) to machine translation (Luong and Manning, 2016). Character-level models are attractive since they can effectively model morphological variants of words and build representations even for unknown words, suffering less from out-of-vocabulary problems (Pinter et al., 2017). However, most character-level models still rely on tokenization and use characters only for creating more robust token representations (Santos and Zadrozny, 2014; Lample et al., 2016; Ma and Hovy, 2016; Plank et al., 2016). This leads to high performance on well-formatted text or text with misspellings (Yu et al., 2017; Sakaguchi et al., 2017) but ties the performance to the quality of the tokenizer. While humans are very robust to * Work was done at Center for Information and Language Processing, LMU Munich. noise caused by insertion of spaces (e.g., “car nival”) or deletion of spaces (“dee"
N19-1280,P16-2067,0,0.117803,"ularity for different tasks, ranging from text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016) to machine translation (Luong and Manning, 2016). Character-level models are attractive since they can effectively model morphological variants of words and build representations even for unknown words, suffering less from out-of-vocabulary problems (Pinter et al., 2017). However, most character-level models still rely on tokenization and use characters only for creating more robust token representations (Santos and Zadrozny, 2014; Lample et al., 2016; Ma and Hovy, 2016; Plank et al., 2016). This leads to high performance on well-formatted text or text with misspellings (Yu et al., 2017; Sakaguchi et al., 2017) but ties the performance to the quality of the tokenizer. While humans are very robust to * Work was done at Center for Information and Language Processing, LMU Munich. noise caused by insertion of spaces (e.g., “car nival”) or deletion of spaces (“deeplearning”), this can cause severe underperformance of machine learning models. Similar challenges arise for languages with difficult tokenization, such as Chinese or Vietnamese. For text with difficult or noisy tokenization"
N19-1280,K17-3015,0,0.067305,"POS 98.79 93.45 93.86 91.34 88.06 77.67 93.82 88.15 93.66 87.65 Table 2: Tokenization and joint token-POS F1 on UD v2.0. Best scores are in bold, second-best are underlined. this to the different way of feature creation: While grConv hierarchically combines context-enhanced n-grams, SRNN constructs segments in a sequential order. The latter may be less suited for compositional segments like “airport”. Baselines for UD v2.0. We compare to the top performing models for EN, JA, VI, ZH from the CoNLL 2017 shared task: UDPipe 1.2 (Straka and Strakov´a, 2017), Stanford (Dozat et al., 2017), FBAML (Qian and Liu, 2017), TRL (Kanayama et al., 2017), and IMS (Bj¨orkelund et al., 2017). Multilingual Results (UD v2.0). Table 2 provides our results. While for each language another shared task system performs best, our system performs consistently well across languages (best or second-best except for EN), leading to the best average scores for both tokenization and POS tagging. Moreover, it matches the state of the art for Chinese (ZH) and Vietnamese (VI), two languages with very different characteristics in tokenization. 3.2 who generate grammatical errors. We leave the investigation of other methods for generat"
N19-1280,I17-1018,0,0.0170615,"and grConv to model both the context of characters (LSTM) and the compositionality of language (grConv). Joint Segmentation and POS Tagging. The top performing models of EN, JA, VI and ZH use a pipeline of tokenizer and word-based POS tagger but do not treat both tasks jointly (Bj¨orkelund et al., 2017; Dozat et al., 2017; Kanayama et al., 2017; Qian and Liu, 2017). Especially for Chinese, there is a lot of work on joint word segmentation and POS tagging, e.g., (Zhang and Clark, 2008; Sun, 2011; Hatori et al., 2012; Zheng et al., 2013; Kong et al., 2015; Cai and Zhao, 2016; Chen et al., 2017; Shao et al., 2017), of which some use CRFs to predict one POS tag per character. However, this is hard to transfer to languages like English and Vietnamese where single characters are less informative and tokens are much longer, resulting in a larger combinatory label space. Thus, we choose a semi-Markov formalization to directly model segments. Semi-Markov CRFs for Sequence Tagging. Zhuo et al. (2016) and Ye and Ling (2018) apply semi-CRFs to word-level inputs for named entity recognition. In contrast, we model characterbased POS tagging. Thus, the expected length of our character segments is considerably larg"
N19-1280,K17-3009,0,0.0613755,"Missing"
N19-1280,P11-1139,0,0.0274633,", 2016; Dozat et al., 2017) or CNNs (Ma and Hovy, 2016; Yu et al., 2017) are used. Our model combines biLSTMs and grConv to model both the context of characters (LSTM) and the compositionality of language (grConv). Joint Segmentation and POS Tagging. The top performing models of EN, JA, VI and ZH use a pipeline of tokenizer and word-based POS tagger but do not treat both tasks jointly (Bj¨orkelund et al., 2017; Dozat et al., 2017; Kanayama et al., 2017; Qian and Liu, 2017). Especially for Chinese, there is a lot of work on joint word segmentation and POS tagging, e.g., (Zhang and Clark, 2008; Sun, 2011; Hatori et al., 2012; Zheng et al., 2013; Kong et al., 2015; Cai and Zhao, 2016; Chen et al., 2017; Shao et al., 2017), of which some use CRFs to predict one POS tag per character. However, this is hard to transfer to languages like English and Vietnamese where single characters are less informative and tokens are much longer, resulting in a larger combinatory label space. Thus, we choose a semi-Markov formalization to directly model segments. Semi-Markov CRFs for Sequence Tagging. Zhuo et al. (2016) and Ye and Ling (2018) apply semi-CRFs to word-level inputs for named entity recognition. In"
N19-1280,D12-1122,0,0.0297771,"Figure 1: Overview of our model. Illustration of gating for grConv taken from (Zhuo et al., 2016). b = (b1 , . . . , b|Y |)&gt; ∈ R|Y |are trained parameters, f (sj , x) ∈ RD is the feature representation of the labeled segment sj , |Y |is the number of output classes and D is the length of the segment representation. For training and decoding, we use the semiMarkov analogies of the forward and Viterbi algorithm, respectively (Sarawagi and Cohen, 2005). In order to avoid numerical instability, all computations are performed in log-space. 2.2.1 Segment-level Features Sarawagi and Cohen (2005) and Yang and Cardie (2012) compute segment-level features by handcrafted rules. Recent work learns the features automatically with NNs (Kong et al., 2015; Zhuo et al., 2016). This avoids the manual design of new features for new languages/tasks. We adopt Gated Recursive Convolutional Neural Networks (grConv) (Cho et al., 2014; Zhuo et al., 2016) since they allow to hierarchically combine features for segments. We argue that this is especially useful for compositionality in language. An example is the word “airport” which can be composed of the segments “air” and “port”. GrConv constructs features by recursively combini"
N19-1280,P18-2038,0,0.085078,"segment representations, we replace it with a Segmental Recurrent Neural Network (SRNN) (Kong et al., 2015).4 SRNN uses dynamic programming and biLSTMs to create segment representations. Its performance is slightly worse compared to grConv (last row of Table 1). We attribute 3 http://cistern.cis.lmu.de/marmot/ In an initial experiment, we also replaced it with a simpler method that creates a segment representation by subtracting the character biLSTM hidden state of the segment start from the hidden state of the segment end. This is one of the segment-level features employed, for instance, by Ye and Ling (2018). However, this approach did not lead to promising results in our case. We assume that more sophisticated methods like grConv or SRNN are needed in this setup. 2738 4 EN JA VI ZH Avg UDPipe 1.2 Tokens POS 99.03 93.50 90.97 88.19 84.26 75.29 89.55 83.47 90.95 85.11 Stanford Tokens POS 98.67 95.11 89.68 88.14 82.47 75.28 88.91 85.26 89.93 85.95 FBAML Tokens POS 98.98 94.09 93.32 91.04 83.80 75.84 94.57 88.36 92.67 87.33 TRL Tokens POS 94.31 82.41 98.59 98.45 85.41 74.53 83.64 71.31 90.49 81.68 IMS Tokens POS 98.67 93.29 91.68 89.07 86.67 77.88 92.81 86.33 92.46 86.64 Our Tokens POS 98.79 93.45 9"
N19-1280,W17-4118,0,0.139808,"ng (Kim et al., 2016) to machine translation (Luong and Manning, 2016). Character-level models are attractive since they can effectively model morphological variants of words and build representations even for unknown words, suffering less from out-of-vocabulary problems (Pinter et al., 2017). However, most character-level models still rely on tokenization and use characters only for creating more robust token representations (Santos and Zadrozny, 2014; Lample et al., 2016; Ma and Hovy, 2016; Plank et al., 2016). This leads to high performance on well-formatted text or text with misspellings (Yu et al., 2017; Sakaguchi et al., 2017) but ties the performance to the quality of the tokenizer. While humans are very robust to * Work was done at Center for Information and Language Processing, LMU Munich. noise caused by insertion of spaces (e.g., “car nival”) or deletion of spaces (“deeplearning”), this can cause severe underperformance of machine learning models. Similar challenges arise for languages with difficult tokenization, such as Chinese or Vietnamese. For text with difficult or noisy tokenization, more robust models are needed. In order to address this challenge, we propose a model that does"
N19-1280,P08-1101,0,0.0546916,"al., 2016; Plank et al., 2016; Dozat et al., 2017) or CNNs (Ma and Hovy, 2016; Yu et al., 2017) are used. Our model combines biLSTMs and grConv to model both the context of characters (LSTM) and the compositionality of language (grConv). Joint Segmentation and POS Tagging. The top performing models of EN, JA, VI and ZH use a pipeline of tokenizer and word-based POS tagger but do not treat both tasks jointly (Bj¨orkelund et al., 2017; Dozat et al., 2017; Kanayama et al., 2017; Qian and Liu, 2017). Especially for Chinese, there is a lot of work on joint word segmentation and POS tagging, e.g., (Zhang and Clark, 2008; Sun, 2011; Hatori et al., 2012; Zheng et al., 2013; Kong et al., 2015; Cai and Zhao, 2016; Chen et al., 2017; Shao et al., 2017), of which some use CRFs to predict one POS tag per character. However, this is hard to transfer to languages like English and Vietnamese where single characters are less informative and tokens are much longer, resulting in a larger combinatory label space. Thus, we choose a semi-Markov formalization to directly model segments. Semi-Markov CRFs for Sequence Tagging. Zhuo et al. (2016) and Ye and Ling (2018) apply semi-CRFs to word-level inputs for named entity recog"
N19-1280,D13-1061,0,0.0445441,"Missing"
N19-1280,P16-1134,0,0.286107,"extended one-hot encoding to produce a character embedding. The character embeddings are fed into a bidirectonal LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997) that computes contextaware representations. These representations form the input to the segment-level feature extractor. 2.2 semi-CRF Character-based Input Representation character-based input representation 2.1 <1,3,DET&gt; θL This section describes our model which is also depicted in Figure 1. space 1-hot feature vector 2 ... ... ... ... ... ... T h e f o x Figure 1: Overview of our model. Illustration of gating for grConv taken from (Zhuo et al., 2016). b = (b1 , . . . , b|Y |)&gt; ∈ R|Y |are trained parameters, f (sj , x) ∈ RD is the feature representation of the labeled segment sj , |Y |is the number of output classes and D is the length of the segment representation. For training and decoding, we use the semiMarkov analogies of the forward and Viterbi algorithm, respectively (Sarawagi and Cohen, 2005). In order to avoid numerical instability, all computations are performed in log-space. 2.2.1 Segment-level Features Sarawagi and Cohen (2005) and Yang and Cardie (2012) compute segment-level features by handcrafted rules. Recent work learns th"
N19-1398,N16-1012,0,0.147129,"hdanau et al. (2014) and See et al. (2017) respectively. 3.2 Baseline Setting As we reimplemented (Bahdanau et al., 2014) and (See et al., 2017) models, we initially evaluate them on a standard task of headline generation.5 We use popular headline generation corpus, Gigaword (Napoles et al., 2012), with 3.8M training examples. We fetched the test set from Rush et al. (2015) and report the results on it. The results are compared with the state-of-the-art headline generation methods like Nallapati et al. (Nallapati et al., 2016), ABS (Rush et al., 2015), ABS+ (Rush et al., 2015), and RAS-Elman (Chopra et al., 2016). Since our aim for this experiment is to demonstrate the strength of the models, we limit the model parameters to the extent that we produce comparable results in less computation time. Table 8 compares performances of seq2seq and seq2seq pointer models with other state-of-the-art methods. The results indicate that the implementations have performance competitive with other state-of-the-art methods. seq2seq seq2seq point seq2seq seq2seq point Validation Rouge-1 Rouge-2 Rouge-L 15.77 03.52 13.53 21.57 07.03 18.64 15.26 21.05 Test 03.38 07.11 13.15 18.49 Table 9: Rouge F1 scores for seq2seq mod"
N19-1398,W14-2303,0,0.0131875,"tences in the news articles due to the inverted pyramid news writing style. 3969 guns legally. But none of the purchases set off any red flags, with an interesting sentence fragment containing the phrase red flags. This suggests that the methodology that uses plain frequency of words is not sufficient for determining interesting information. |term w in domain d| |terms in domain d| |domains| idfdomain (w) = log |domains containing w| dr(w, d) = tfdomain (w, d) × idfdomain (w) tfdomain (w, d) = (2) Thus, we look at unusualness at a level lower than the corpus. We rely on domain relevance (dr) (Schulder and Hovy, 2014), an adapted TFIDF (term frequency inverse document frequency) metric that measures the impact of a word in a domain and, therefore, identifies unusual words in a specific domain, and is computed using Eq. 2. A word is assigned a very low dr score if the word is either non-frequent in the domain and too frequent among other domains (unusualness) or non-frequent in all domains (rare); see Table 4. As a very low dr score corresponds to unusualness, a presence of very low dr values among the nonoverlapping words of the ShortcutText suggest a high likelihood of it being a teaser, and therefore, we"
N19-1398,P17-1099,0,0.45762,"benchmark and baseline systems for the process of generating teasers. A teaser is a short reading suggestion for an article that is illustrative and includes curiosity-arousing elements to entice potential readers to read particular news items. Teasers are one of the main vehicles for transmitting news to social media users. We compile a novel dataset of teasers by systematically accumulating tweets and selecting those that conform to the teaser definition. We have compared a number of neural abstractive architectures on the task of teaser generation and the overall best performing system is See et al. (2017)’s seq2seq with pointer network. 1 headline clickbait teaser bona-fide yes no yes teasing no yes yes abstractive low high high Table 1: The table shows three categories of ShortcutTexts and their properties Introduction A considerable number of people get their news in some digital format.1 The trend has made many publishers and editors shift their focus to the web and experiment with new techniques to lure an Internet-savvy generation of readers to read their news stories. Therefore, there has been a noticeable increase in the sharing of short illustrative pieces of texts about the news on so"
N19-1398,D15-1014,0,0.0928141,"ial media platform whose role as a news conduit is rapidly increasing. An indicative tweet is a Twitter post containing a link to an external web page that is primarily composed of text. The presence of the URL in an indicative tweet signals that it functions to help users decide whether to read the article, and the short length confirms it as a ShortcutText like a headline or teaser. Lloret and Palomar (2013) made an early attempt at generating indicative tweets using off-the-shelf extractive summarization models, and graded the generated texts as informative but uninteresting. Additionally, Sidhaye and Cheung (2015)’s analysis showed extractive summarization as an inappropriate method for generating such tweets as the overlaps between the tweets and the corresponding articles often are low. Our study shows that teasers, bona fide indicative tweets, do exhibit significant, though not complete, overlaps, and, therefore, are not appropriate for extractive but certainly for abstractive summarization. Our contributions: 1) To the best of our knowledge, this is the first attempt to compare different types of ShortcutTexts associated with a news article. Furthermore, we introduce a novel concept of a teaser, an"
N19-1398,N15-1112,0,0.0695765,"Missing"
N19-1398,K16-1028,0,0.0401364,"Missing"
N19-1398,W12-3018,0,0.0336829,"Missing"
N19-1398,D15-1044,0,0.317724,"3.78 32.67 31.21 34.81 Rouge-2 11.32 11.88 15.97 15.59 12.96 15.59 Rouge-L 26.42 26.96 31.15 30.64 28.87 32.05 Table 8: Rouge scores on the standard task of Headline Generation (Gigaword). seq2seq and seq2seq point are reimplementations of Bahdanau et al. (2014) and See et al. (2017) respectively. 3.2 Baseline Setting As we reimplemented (Bahdanau et al., 2014) and (See et al., 2017) models, we initially evaluate them on a standard task of headline generation.5 We use popular headline generation corpus, Gigaword (Napoles et al., 2012), with 3.8M training examples. We fetched the test set from Rush et al. (2015) and report the results on it. The results are compared with the state-of-the-art headline generation methods like Nallapati et al. (Nallapati et al., 2016), ABS (Rush et al., 2015), ABS+ (Rush et al., 2015), and RAS-Elman (Chopra et al., 2016). Since our aim for this experiment is to demonstrate the strength of the models, we limit the model parameters to the extent that we produce comparable results in less computation time. Table 8 compares performances of seq2seq and seq2seq pointer models with other state-of-the-art methods. The results indicate that the implementations have performance c"
N19-1398,C14-1083,0,0.0354499,"Missing"
N19-1398,P10-1058,0,0.087794,"Missing"
P06-2004,A00-2021,0,0.0696852,"Missing"
P06-2004,H94-1048,0,0.115504,"The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning. 1 Introduction The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank (Marcus et al., 1993) and the prepositional phrase data set first described in (Ratnaparkhi et al., 1994). However, the production of training sets is expensive. They are not available for many domains and languages. This motivates research on combining supervised with unsupervised learning since unannotated text is in ample supply for most domains in the major languages of the world. The question arises how much annotated and unannotated data is necessary in combination learning strategies. We investigate this question for two attachment ambiguity problems: relative clause (RC) attachment and prepositional phrase (PP) attachment. The supervised component is Collins’ parser (Collins, 1997), train"
P06-2004,C02-1108,0,0.0254157,"Missing"
P06-2004,W06-2913,1,0.872526,"Missing"
P06-2004,W95-0103,0,0.202944,"Missing"
P06-2004,P97-1003,0,0.0579481,"attachment (highA), low attachment (lowA), verb attachment (verbA), and noun attachment (nounA) according to the gold standard. All instances of RC and PP attachments were extracted from development and test sets, yielding about 250 RC ambiguities and 12,000 PP ambiguities per set (Table 2). An RC attachment ambiguity was defined as a sentence containing the pattern NP1 Prep NP2 which. For example, the relative clause in Example 1 can either attach to mechanism or to System. (1) Methods Collins parser. Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel (Collins, 1997; Bikel, 2004). For each ambiguity, we check whether the attachment ambiguity is resolved correctly by the 5 parsers corresponding to the different training sets. If the attachment ambiguity is not recognized (e.g., because parsing failed), then the corresponding ambiguity is excluded for that instance of the parser. As a result, the size of the effective test set varies from parser to parser (see Table 4). The test set, sections 13-24, is larger than in most studies because a single section does not contain a sufficient number of RC attachment ambiguities for a meaningful evaluation. which-cl"
P06-2004,P98-2234,0,0.0547436,"Missing"
P06-2004,J93-1005,0,0.393123,"Missing"
P06-2004,J93-2004,0,\N,Missing
P06-2004,J04-4004,0,\N,Missing
P06-2004,C98-2229,0,\N,Missing
P11-1097,P04-3013,0,0.031719,"accuracy; e.g., Poibeau and Kosseim (2000) showed that newswire-trained NER systems perform poorly when applied to email data (a drop of F1 from .9 to .5). Recent work in machine learning has made substantial progress in understanding how cross-domain features can be used in effective ways (Ben-David et al., 2010). The development of such features however is to a large extent an empirical problem. From this perspective, one of the most successful approaches to adaptation for NER is based on generating shared feature representations between source and target domains, via unsupervised methods (Ando, 2004; Turian et al., 2010). Turian et al. (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. Our approach fits within this line of work in that it empirically investigates features with good cross-domain generalization properties. The main contribution of this paper is the design and evaluation of a novel family of features extracted from the largest and most up-to-date repository of world knowledge, the web. Another source of world knowledge for NER is Wikipedia:"
P11-1097,D08-1107,0,0.0414478,"anized as follows. Section 2 discusses related work. We describe standard NER features in Section 3. One main contribution of this paper is the large array of piggyback features that we propose in Section 4. We describe the data sets we use and our experimental setup in Sections 5–6. The results in Section 7 show that piggyback features significantly increase NER performance. This is the second main contribution of the paper. We discuss challenges of using piggyback features – due to the cost of querying search engines – and present our conclusions and future work in Section 8. 2 Related work Barr et al. (2008) found that capitalization of NEs in web queries is inconsistent and not a reliable cue for NER. Guo et al. (2009) exploit query logs for NER in queries. This is also promising, but the context in search results is richer and potentially more informative than that of other queries in logs. The insight that search results provide useful additional context for natural language expressions is not new. Perhaps the oldest and best known application is pseudo-relevance feedback which uses words and phrases from search results for query expansion (Rocchio, 1971; Xu and Croft, 1996). Search counts or"
P11-1097,A00-1031,0,0.126768,"y divided in groups. We will refer to the target token – the token we define the feature vector for – as w0 . Its left neighbor is w−1 and its right neighbor w1 . Table 1 provides a summary of all features. Feature group BASE. The first class of features, BASE, is standard in NER. The binary feature WORD(k,i) (line 1) is 1 iff wi , the ith word in the dictionary, occurs at position k with respect to w0 . The dictionary consists of all words in the training set. The analogous feature for part of speech, POS(k,t) (line 2), is 1 iff wk has been tagged with 967 PoS t, as determined by TnT tagger (Brants, 2000). We also encode surface properties of the word with simple regular expressions, e.g., x-ray is encoded as x-x and 9/11 as d/dd (SHAPE, line 3). For these features, k ∈ {−1, 0, 1}. Finally, we encode prefixes and suffixes, up to three characters long, for w0 (line 4). Feature group GAZ. Gazetteer features (lines 5 & 6) are an efficient and effective way of building world knowledge into an NER model. A gazetteer is simply a list of phrases that belong to a particular semantic category. We use gazetteers from (i) GATE (Cunningham et al., 2002): countries, first/last names, trigger words; (ii) Wo"
P11-1097,W06-1670,1,0.868559,"Missing"
P11-1097,W02-1001,0,0.0174466,"Missing"
P11-1097,D07-1074,0,0.00539582,"a (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. Our approach fits within this line of work in that it empirically investigates features with good cross-domain generalization properties. The main contribution of this paper is the design and evaluation of a novel family of features extracted from the largest and most up-to-date repository of world knowledge, the web. Another source of world knowledge for NER is Wikipedia: Kazama and Torisawa (2007) show that pseudocategories extracted from Wikipedia help for in-domain NER. Cucerzan (2007) uses Wikipedia and web search frequencies to improve NE disambiguation, including simple web search frequencies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 BASE: lexical and input-text part-of-speech features binary: wk = wi binary: wk has part-of-speech t SHAPE (k,i) binary: wk has (regular expression) shape regexpi PREFIX (j) binary: w0 has prefix j (analogously for suffixes) GAZ: gazetteer features GAZ - Bl (k,i) binary: wk is the initial word of a phrase, consisting of l words, whose gaz. category is i GAZ - Il (k,i) binary: wk is a non-initial word in a phrase, consisting of l"
P11-1097,W10-0713,0,0.128418,"Missing"
P11-1097,C08-1029,0,0.0145022,"ful additional context for natural language expressions is not new. Perhaps the oldest and best known application is pseudo-relevance feedback which uses words and phrases from search results for query expansion (Rocchio, 1971; Xu and Croft, 1996). Search counts or search results have also been used for sentiment analysis (Turney, 2002), for transliteration (Grefenstette et al., 2004), candidate selection in machine translation (Lapata and Keller, 2005), text similarity measurements (Sahami and Heilman, 2006), incorrect parse tree filtering (Yates et al., 2006), and 966 paraphrase evaluation (Fujita and Sato, 2008). The specific NER application we address is most similar to the work of Farkas et al. (2007), but they mainly used frequency statistics as opposed to what we view as the main strength of search results: the ability to get additional contextually similar uses of the token that is to be classified. Lawson et al. (2010), Finin et al. (2010), and Yetisgen-Yildiz et al. (2010) investigate how to best use Amazon Mechanical Turk (AMT) for NER. We use AMT as a tool, but it is not our focus. NLP settings where training and test sets are from different domains have received considerable attention in re"
P11-1097,D07-1073,0,0.00815638,"Turian et al., 2010). Turian et al. (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. Our approach fits within this line of work in that it empirically investigates features with good cross-domain generalization properties. The main contribution of this paper is the design and evaluation of a novel family of features extracted from the largest and most up-to-date repository of world knowledge, the web. Another source of world knowledge for NER is Wikipedia: Kazama and Torisawa (2007) show that pseudocategories extracted from Wikipedia help for in-domain NER. Cucerzan (2007) uses Wikipedia and web search frequencies to improve NE disambiguation, including simple web search frequencies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 BASE: lexical and input-text part-of-speech features binary: wk = wi binary: wk has part-of-speech t SHAPE (k,i) binary: wk has (regular expression) shape regexpi PREFIX (j) binary: w0 has prefix j (analogously for suffixes) GAZ: gazetteer features GAZ - Bl (k,i) binary: wk is the initial word of a phrase, consisting of l words, whose gaz."
P11-1097,W10-0712,0,0.0226141,"is (Turney, 2002), for transliteration (Grefenstette et al., 2004), candidate selection in machine translation (Lapata and Keller, 2005), text similarity measurements (Sahami and Heilman, 2006), incorrect parse tree filtering (Yates et al., 2006), and 966 paraphrase evaluation (Fujita and Sato, 2008). The specific NER application we address is most similar to the work of Farkas et al. (2007), but they mainly used frequency statistics as opposed to what we view as the main strength of search results: the ability to get additional contextually similar uses of the token that is to be classified. Lawson et al. (2010), Finin et al. (2010), and Yetisgen-Yildiz et al. (2010) investigate how to best use Amazon Mechanical Turk (AMT) for NER. We use AMT as a tool, but it is not our focus. NLP settings where training and test sets are from different domains have received considerable attention in recent years. These settings are difficult because many machine learning approaches assume that source and target are drawn from the same distribution; this is not the case if they are from different domains. Systems applied out of domain typically incur severe losses in accuracy; e.g., Poibeau and Kosseim (2000) showed"
P11-1097,P09-1116,0,0.0294758,"Missing"
P11-1097,N03-1028,0,0.0279328,"Missing"
P11-1097,P10-1040,0,0.0897849,".g., Poibeau and Kosseim (2000) showed that newswire-trained NER systems perform poorly when applied to email data (a drop of F1 from .9 to .5). Recent work in machine learning has made substantial progress in understanding how cross-domain features can be used in effective ways (Ben-David et al., 2010). The development of such features however is to a large extent an empirical problem. From this perspective, one of the most successful approaches to adaptation for NER is based on generating shared feature representations between source and target domains, via unsupervised methods (Ando, 2004; Turian et al., 2010). Turian et al. (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. Our approach fits within this line of work in that it empirically investigates features with good cross-domain generalization properties. The main contribution of this paper is the design and evaluation of a novel family of features extracted from the largest and most up-to-date repository of world knowledge, the web. Another source of world knowledge for NER is Wikipedia: Kazama and Torisawa ("
P11-1097,P02-1053,0,0.00185217,"stent and not a reliable cue for NER. Guo et al. (2009) exploit query logs for NER in queries. This is also promising, but the context in search results is richer and potentially more informative than that of other queries in logs. The insight that search results provide useful additional context for natural language expressions is not new. Perhaps the oldest and best known application is pseudo-relevance feedback which uses words and phrases from search results for query expansion (Rocchio, 1971; Xu and Croft, 1996). Search counts or search results have also been used for sentiment analysis (Turney, 2002), for transliteration (Grefenstette et al., 2004), candidate selection in machine translation (Lapata and Keller, 2005), text similarity measurements (Sahami and Heilman, 2006), incorrect parse tree filtering (Yates et al., 2006), and 966 paraphrase evaluation (Fujita and Sato, 2008). The specific NER application we address is most similar to the work of Farkas et al. (2007), but they mainly used frequency statistics as opposed to what we view as the main strength of search results: the ability to get additional contextually similar uses of the token that is to be classified. Lawson et al. (20"
P11-1097,W06-1604,0,0.0260334,"in logs. The insight that search results provide useful additional context for natural language expressions is not new. Perhaps the oldest and best known application is pseudo-relevance feedback which uses words and phrases from search results for query expansion (Rocchio, 1971; Xu and Croft, 1996). Search counts or search results have also been used for sentiment analysis (Turney, 2002), for transliteration (Grefenstette et al., 2004), candidate selection in machine translation (Lapata and Keller, 2005), text similarity measurements (Sahami and Heilman, 2006), incorrect parse tree filtering (Yates et al., 2006), and 966 paraphrase evaluation (Fujita and Sato, 2008). The specific NER application we address is most similar to the work of Farkas et al. (2007), but they mainly used frequency statistics as opposed to what we view as the main strength of search results: the ability to get additional contextually similar uses of the token that is to be classified. Lawson et al. (2010), Finin et al. (2010), and Yetisgen-Yildiz et al. (2010) investigate how to best use Amazon Mechanical Turk (AMT) for NER. We use AMT as a tool, but it is not our focus. NLP settings where training and test sets are from diffe"
P11-1097,W10-0728,0,0.0387602,"tette et al., 2004), candidate selection in machine translation (Lapata and Keller, 2005), text similarity measurements (Sahami and Heilman, 2006), incorrect parse tree filtering (Yates et al., 2006), and 966 paraphrase evaluation (Fujita and Sato, 2008). The specific NER application we address is most similar to the work of Farkas et al. (2007), but they mainly used frequency statistics as opposed to what we view as the main strength of search results: the ability to get additional contextually similar uses of the token that is to be classified. Lawson et al. (2010), Finin et al. (2010), and Yetisgen-Yildiz et al. (2010) investigate how to best use Amazon Mechanical Turk (AMT) for NER. We use AMT as a tool, but it is not our focus. NLP settings where training and test sets are from different domains have received considerable attention in recent years. These settings are difficult because many machine learning approaches assume that source and target are drawn from the same distribution; this is not the case if they are from different domains. Systems applied out of domain typically incur severe losses in accuracy; e.g., Poibeau and Kosseim (2000) showed that newswire-trained NER systems perform poorly when a"
P11-1097,W03-0419,0,\N,Missing
P11-1152,N03-2002,0,0.0457214,"model could not be replaced with a hierarchical model and we would expect this to improve results. The key novelty of our clustering method is that clusters are formed based on rare events in the training corpus. This type of clustering has been applied to other problems before, in particular to unsupervised part-of-speech tagging (Sch¨utze, 1995; Clark, 2003; Reichart et al., 2010). However, the importance of rare events for clustering in language modeling has not been investigated before. Our work is most similar to the lattice-based language models proposed by Dupont and Rosenfeld (1997). Bilmes and Kirchhoff (2003) generalize lattice-based language models further by allowing arbitrary factors in addition to words and classes. We use a special case of lattice-based language models in this paper. Our contributions are that we introduce the novel idea of rare-event clustering into language modeling and that we show that the modified model performs better than a strong word-trigram 1517 symbol P [[w]] c(wji ) n1+ (•wji ) denotation P w (sum over all unigrams w) count of wji # of distinct w occurring before wji 3.2 Table 1: Notation used for Kneser-Ney. baseline. 3 Models In this section, we introduce the th"
P11-1152,J92-4003,0,0.641933,"Missing"
P11-1152,P96-1041,0,0.355309,"(w2 )pKN (w3 ) where βi (v) is equal to a parameter αi if the history (w12 or w2 ) is part of a cluster and 0 otherwise: [[w]] n1+ (•w2 w) pKN (w3 ) = ′ n1+ (•w P 3 )−d (n1+ (•w3 )) [[w]] n1+ (•w) γ1 if c(w3 ) > 0 if c(w3 ) = 0 P [[w]] d′ (n1+ (•w)) γ1 = P [[w]] n1+ (•w) d′ , n1+ (•w23 ) − d′′ (n1+ (•w23 )) P [[w]] n1+ (•w2 w) + γ2 (w2 )[β2 (w2 )pB (w3 |w2 ) +(1 − β2 (w2 ))pDR (w3 )] P [[w]] d′′ (n1+ (•w2 w)) γ2 (w2 ) = P ( c(w13 ) − d′′′ (c(w13 )) P [[w]] c(w12 w) pDR (w3 |w12 ) = The parameters d′′ , and d′′′ are the discounts for unigrams, bigrams and trigrams, respectively, as defined by Chen and Goodman (1996, p. 20, (26)). Note that our notation deviates from C&G in that they use the single symbol D1 for the three different values d′ (1), d′′ (1), and d′′′ (1) etc. βi (v) = ( αi if v ∈ B2−(i−1) 0 otherwise B1 (resp. B2 ) is the set of unigram (resp. bigram) histories that is covered by the clusters. We cluster bigram histories and unigram histories separately and write pB (w3 |w1 w2 ) for the bigram cluster model and pB (w3 |w2 ) for the unigram cluster model. Clustering and the estimation of these two distributions are described in Section 4. 1518 The unigram distribution of the DupontRosenfeld"
P11-1152,N09-1053,0,0.0159653,"opposed to classes) (Whittaker and Woodland, 2001; Emami and Jelinek, 2005; Uszkoreit and Brants, 2008). In this work, we use the Brown type of model: it is simpler and has fewer parameters. Models that condition classes on lexical n-grams could be extended in a way similar to what we propose here. Classes have been used with good results in a number of applications, e.g., in speech recognition (Yokoyama et al., 2003), sentiment analysis (Wiegand and Klakow, 2008), and question answering (Momtazi and Klakow, 2009). Classes have also been shown to improve the performance of exponential models (Chen, 2009). Our use of classes of lexical n-grams for n > 1 has several precedents in the literature (Suhm and Waibel, 1994; Kuo and Reichl, 1999; Deligne and Sagisaka, 2000; Justo and Torres, 2009). The novelty of our approach is that we integrate phrase-level classes into a KN model. Hierarchical clustering (McMahon and Smith, 1996; Zitouni and Zhou, 2007; Zitouni and Zhou, 2008) has the advantage that the size of the class to be used in a specific context is not fixed, but can be chosen at an optimal level of the hierarchy. There is no reason why our non-hierarchical flat model could not be replaced"
P11-1152,E03-1009,0,0.0147878,"996; Zitouni and Zhou, 2007; Zitouni and Zhou, 2008) has the advantage that the size of the class to be used in a specific context is not fixed, but can be chosen at an optimal level of the hierarchy. There is no reason why our non-hierarchical flat model could not be replaced with a hierarchical model and we would expect this to improve results. The key novelty of our clustering method is that clusters are formed based on rare events in the training corpus. This type of clustering has been applied to other problems before, in particular to unsupervised part-of-speech tagging (Sch¨utze, 1995; Clark, 2003; Reichart et al., 2010). However, the importance of rare events for clustering in language modeling has not been investigated before. Our work is most similar to the lattice-based language models proposed by Dupont and Rosenfeld (1997). Bilmes and Kirchhoff (2003) generalize lattice-based language models further by allowing arbitrary factors in addition to words and classes. We use a special case of lattice-based language models in this paper. Our contributions are that we introduce the novel idea of rare-event clustering into language modeling and that we show that the modified model perform"
P11-1152,J96-2003,0,0.0895946,"been used with good results in a number of applications, e.g., in speech recognition (Yokoyama et al., 2003), sentiment analysis (Wiegand and Klakow, 2008), and question answering (Momtazi and Klakow, 2009). Classes have also been shown to improve the performance of exponential models (Chen, 2009). Our use of classes of lexical n-grams for n > 1 has several precedents in the literature (Suhm and Waibel, 1994; Kuo and Reichl, 1999; Deligne and Sagisaka, 2000; Justo and Torres, 2009). The novelty of our approach is that we integrate phrase-level classes into a KN model. Hierarchical clustering (McMahon and Smith, 1996; Zitouni and Zhou, 2007; Zitouni and Zhou, 2008) has the advantage that the size of the class to be used in a specific context is not fixed, but can be chosen at an optimal level of the hierarchy. There is no reason why our non-hierarchical flat model could not be replaced with a hierarchical model and we would expect this to improve results. The key novelty of our clustering method is that clusters are formed based on rare events in the training corpus. This type of clustering has been applied to other problems before, in particular to unsupervised part-of-speech tagging (Sch¨utze, 1995; Cla"
P11-1152,W10-2911,0,0.0130542,"and Zhou, 2007; Zitouni and Zhou, 2008) has the advantage that the size of the class to be used in a specific context is not fixed, but can be chosen at an optimal level of the hierarchy. There is no reason why our non-hierarchical flat model could not be replaced with a hierarchical model and we would expect this to improve results. The key novelty of our clustering method is that clusters are formed based on rare events in the training corpus. This type of clustering has been applied to other problems before, in particular to unsupervised part-of-speech tagging (Sch¨utze, 1995; Clark, 2003; Reichart et al., 2010). However, the importance of rare events for clustering in language modeling has not been investigated before. Our work is most similar to the lattice-based language models proposed by Dupont and Rosenfeld (1997). Bilmes and Kirchhoff (2003) generalize lattice-based language models further by allowing arbitrary factors in addition to words and classes. We use a special case of lattice-based language models in this paper. Our contributions are that we introduce the novel idea of rare-event clustering into language modeling and that we show that the modified model performs better than a strong w"
P11-1152,E95-1020,1,0.568109,"Missing"
P11-1152,P08-1086,0,0.0223896,"nomial discounting mechanism that does better than either in Section 6. Section 7 presents our conclusions. 2 Related work A large number of different class-based models have been proposed in the literature. The well-known model by Brown et al. (1992) is a class sequence model, in which p(u|w) is computed as the product of a class transition probability and an emission probability, p(g(u)|g(w))p(u|g(u)), where g(u) is the class of u. Other approaches condition the probability of a class on n-grams of lexical items (as opposed to classes) (Whittaker and Woodland, 2001; Emami and Jelinek, 2005; Uszkoreit and Brants, 2008). In this work, we use the Brown type of model: it is simpler and has fewer parameters. Models that condition classes on lexical n-grams could be extended in a way similar to what we propose here. Classes have been used with good results in a number of applications, e.g., in speech recognition (Yokoyama et al., 2003), sentiment analysis (Wiegand and Klakow, 2008), and question answering (Momtazi and Klakow, 2009). Classes have also been shown to improve the performance of exponential models (Chen, 2009). Our use of classes of lexical n-grams for n > 1 has several precedents in the literature ("
P13-1094,W06-1641,0,0.0204614,"Missing"
P13-1094,N03-5008,0,0.0751379,"Missing"
P13-1094,P04-1035,0,0.809855,"c words Many publications have addressed subjectivity in sentiment analysis. Two important papers that are based on the original philosophical definition of the term (internal state of mind vs. external reality) are (Wilson and Wiebe, 2003) and (Riloff and Wiebe, 2003). As we argue above, if the goal is to identify parts of a document that are useful/nonuseful for sentiment analysis, then S-relevance is a better notion to use. Researchers have implicitly deviated from the philosophical definition because they were primarily interested in satisfying the needs of a particular task. For example, Pang and Lee (2004) use a minimum cut graph model for review summarization. Because they do not directly evaluate the results of subjectivity classification, it is not clear to what extent their method is able to identify subjectivity correctly. In general, it is not possible to know what the underlying concepts of a statistical classification are if no detailed annotation guidelines exist and no direct evaluation of manually labeled data is performed. Our work is most closely related to (Taboada et al., 2009) who define a fine-grained classification that is similar to sentiment relevance on the highest level. H"
P13-1094,W02-1011,0,0.0288081,"Missing"
P13-1094,H05-1043,0,0.194491,"Missing"
P13-1094,W96-0213,0,0.44893,"the unsupervised sequence model introduced in Section 4. As described in Section 4, each document is represented as a graph of sentences and weights between sentences and source/sink nodes representing SR/SNR are set to the confidence values obtained from the distantly trained MaxEnt classifier. We then apply MinCut as described in the following paragraphs and select the most confident examples as training material for a new classifier. feature set is referred to as named entities (NE). 5.3 Sequential Features Following previous sequence classification work with Maximum Entropy models (e.g., (Ratnaparkhi, 1996)), we use selected features of adjacent sentences. If a sentence contains a feature F, we add the feature F+1 to the following sentence. For example, if a &lt;CHARACTER> feature occurs in a sentence, &lt;CHARACTER+1> is added to the following sentence. For S-relevance classification, we perform this operation only for NE features as they are restricted to a few classes and thus will not enlarge the feature space notably. We refer to this feature set as sequential features (SQ). 6 Distant Supervision Since a large labeled resource for sentiment relevance classification is not yet available, we invest"
P13-1094,W03-1014,0,0.2356,"Missing"
P13-1094,W09-3909,0,0.0939287,"us on high-precision retrieval (cf. (Liu, 2010)). However, finding a few S-relevant items with high precision is much easier than the task we address: exhaustive classification of all sentences. Another contribution is that we show that generalization based on semantic classes improves Srelevance classification. While previous work has shown the utility of other types of feature generalization for sentiment and subjectivity analysis (e.g., syntax and part-of-speech (Riloff and Wiebe, 2003)), semantic classes have so far not been ex4 Methods Due to the sequential properties of S-relevance (cf. Taboada et al. (2009)), we impose the discourse constraint that an S-relevant (resp. S-nonrelevant) sentence tends to follow an S-relevant (resp. Snonrelevant) sentence. Following Pang and Lee (2004), we use minimum cut (MinCut) to formalize this discourse constraint. For a document with n sentences, we create a graph with n + 2 nodes: n sentence nodes and source and sink nodes. We define source and sink to represent the classes S-relevance and Snonrelevance, respectively, and refer to them as SR and SNR. The individual weight ind(s, x) between a sentence s and the source/sink node x ∈ {SR, SNR} is weighted accord"
P13-1094,N09-2046,0,0.0474494,"Missing"
P13-1094,W03-2102,0,0.0650405,"oach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well. 1 ¨ Hinrich Schutze Center for Information and Language Processing University of Munich, Germany Introduction It is generally recognized in sentiment analysis that only a subset of the content of a document contributes to the sentiment it conveys. For this reason, some authors distinguish the categories subjective and objective (Wilson and Wiebe, 2003). Subjective statements refer to the internal state of mind of a person, which cannot be observed. In contrast, objective statements can be verified by observing and checking reality. Some sentiment analysis systems filter out objective language and predict sentiment based on subjective language only because objective statements do not directly reveal sentiment. Even though the categories subjective/objective are well-established in philosophy, we argue that 954 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 954–963, c Sofia, Bulgaria, August 4-9"
P13-1094,D10-1102,0,0.0449697,"Missing"
P13-1094,N07-1033,0,0.0507615,"Missing"
P13-1094,J97-1003,0,\N,Missing
P13-1094,C10-1011,0,\N,Missing
P13-1094,H05-2017,0,\N,Missing
P13-1094,W08-1803,0,\N,Missing
P14-1131,N09-1003,0,0.0734136,"Missing"
P14-1131,P12-3026,0,0.0293915,"Missing"
P14-1131,W09-0212,0,0.48241,"rations of the random walk. This is often true of this type of algorithm; cf. (Sch¨utze and Walsh, 2008). LexRank (Erkan and Radev, 2004) is similar to PPR+cos in that it combines PageRank and cosine; it initializes the sentence similarity matrix of a document using cosine and then applies PageRank to compute lexical centrality. Despite this superficial relatedness, applications like lexicon extraction that look for similar entities and applications that look for central entities are quite different. In addition to faster versions of SimRank, there has also been work on extensions of SimRank. Dorow et al. (2009) and Laws et al. (2010) extend SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include wo"
P14-1131,P10-1006,0,0.0311364,"lines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity, PageRank and SimRank. CoSimRank can either be interpreted as an efficient version of Sim1393 Rank or as a version of Personalized PageRank for similarity measurement. The novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only ne"
P14-1131,C10-2070,1,0.918405,"Missing"
P14-1131,C10-2049,0,0.0196641,"r graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity, PageRank and SimRank. CoSimRank can either be interpreted as an efficient version of Sim1393 Rank or as a version of Personalized PageRank for similarity measurement. The novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all"
P14-1131,D07-1061,0,0.358635,"such applications because it is based on a local formulation of the similarity measure.2 Apart from SimRank, many other similarity measures have been proposed. Leicht et al. (2006) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs. However, most graphs in NLP are not bipartite and Jeh and Widom (2002) also proposed a SimRank variant for bipartite graphs. Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors. We will refer to this measure as PPR+cos. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs. Agirre et al. (2009) use PPR+cos for WordNet and for crosslingual studies. Like CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. They also experimented with Euclidean distance and KL2 A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA’s (Lempel and Moran, 2000) relationship to HITS (Kleinberg, 1999) i"
P14-1131,D08-1095,0,0.0214451,"ix of a document using cosine and then applies PageRank to compute lexical centrality. Despite this superficial relatedness, applications like lexicon extraction that look for similar entities and applications that look for central entities are quite different. In addition to faster versions of SimRank, there has also been work on extensions of SimRank. Dorow et al. (2009) and Laws et al. (2010) extend SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muth"
P14-1131,W12-4104,0,0.0905809,"ar entities and applications that look for central entities are quite different. In addition to faster versions of SimRank, there has also been work on extensions of SimRank. Dorow et al. (2009) and Laws et al. (2010) extend SimRank to edge weights, edge labels and multiple graphs. We use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below. A similar graph of dependency structures was built by Minkov and Cohen (2008). They applied different similarity measures, e.g., cosine of dependency vectors or a new algorithm called pathconstrained graph walk, on synonym extraction (Minkov and Cohen, 2012). We compare CoSimRank with their results in our experiments (see Section 6). Some other applications of SimRank or other graph based similarity measures in NLP include work on document similarity (Li et al., 2009), the transfer of sentiment information between languages (Scheible et al., 2010) and named entity disambiguation (Han and Zhao, 2010). Hoang and Kan (2010) use SimRank for related work summarization. Muthukrishnan et al. (2010) combine link based similarity and content based similarity for document clustering and classification. These approaches use at least one of cosine similarity"
P14-1131,C08-1082,0,0.0407031,"Missing"
P14-1131,W08-2006,0,0.0199979,"efore use it as one of our baselines below. This method is also used by Chang et al. (2013) for semantic relatedness. They also experimented with Euclidean distance and KL2 A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA’s (Lempel and Moran, 2000) relationship to HITS (Kleinberg, 1999) in that different aspects of similarity are decoupled. divergence. Interestingly, a simpler method performed best when comparing with human similarity judgments. In this method only the entries corresponding to the compared nodes are used for a similarity score. Rao et al. (2008) compared PPR+cos to other graph based similarity measures like shortest-path and bounded-length random walks. PPR+cos performed best except for a new similarity measure based on commute time. We do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair. One reason CoSimRank is efficient is that we need only compute a few iterations of the random walk. This is often true of this type of algorithm; cf. (Sch¨utze and Walsh, 2008). LexRank (Erkan and Radev, 2004) is similar to PPR+cos in that it combines PageRank and cosine; it initial"
P14-1131,rapp-etal-2012-identifying,0,0.0125762,"han 40 times faster on synonym extraction and six times faster on lexicon extraction. SimRank is at a disadvantage because it computes all similarities in the graph regardless of the size of the test set; it is particularly inefficient on synonym extraction because the English graph contains a large number † significantly worse than CoSimRank (α = 0.05, onetailed Z-Test) Comparison with WINTIAN Here we address inducing a bilingual lexicon from a seed set based on grammatical relations found by a parser. An alternative approach is to induce a bilingual lexicon from Wikipedia’s interwiki links (Rapp et al., 2012). These two approaches have different strengths and weaknesses; e.g., the interwiki-link-based approach does not require a seed set, but it can only be applied to comparable corpora that consist of corresponding – although not necessarily “parallel” – documents. Despite these differences it is still interesting to compare the two algorithms. Rapp et al. (2012) kindly provided their test set to us. It contains 1000 English words and a single correct German translation for each. We evaluate on a subset we call TS774 that consists of the 774 test word pairs that are in the intersection of words c"
P14-1131,C10-2127,1,0.894816,"Missing"
P14-1131,D08-1096,1,0.885494,"Missing"
P14-2008,N13-1067,0,0.186519,"function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. Recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering. While this result conf"
P14-2008,N12-1073,0,0.0122798,"tation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. Recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering. While this result confirms that citation classification is feasible, there are two drawbacks to this approach: (i) it requires a large annotated corpus for supervised classification, which in the case of sci"
P14-2008,P11-3015,0,0.235564,"how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. Recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering. While this result confirms that citation classification is feasible, there are two drawbacks to this approach: (i) it requires a large annotated corpus for supervised classification,"
P14-2008,P07-1034,0,0.0494594,"ents, we follow the domain adaptation approaches described in (Daum´e III, 2007) and train on product review and citation data before testing on citations. 4 4.1 4.2 Easy Domain Adaptation The results in Section 4.1 are for semi-supervised domain adaptation: the case where we have some large annotated corpus (Amazon product reviews) and a large unannotated corpus (citations). There have been a number of other successful attempts at fully supervised domain adaptation, where it is assumed that some small amount of data is annotated in the target domain (Chelba and Acero, 2004; Daum´e III, 2007; Jiang and Zhai, 2007). To see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daum´e III (2007). The results of this comparison can be seen in Table 2. Briefly, “All” trains on source and target data; “Weight” is the same as “All” except that instances may be weighted differently based on their domain (weights are chosen on a development set); “Pred” trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; “LinInt” linearly interpolates predictions using the source-only and target-only models (the interpo"
P14-2008,C12-1082,1,0.846082,"Missing"
P14-2008,bird-etal-2008-acl,0,0.0434384,"Missing"
P14-2008,P07-1056,0,0.627445,"as CITD. The two citation corpora comprising CITD both come from the ACL Anthology (Bird et al., 2008): the IMS corpus uses the ACL proceedings from 2004 and the DFKI corpus uses parts of the proceedings from 2007 and 2008. Since mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL, 1979–2003, 2005–2006, and 2009. There are a number of non-citation corpora available that contain polarity annotation. For these experiments we use the Multi-Domain Sentiment Dataset4 (henceforth MDSD), introduced by Blitzer et al. (2007). We use the version of the MDSD that includes positive and negative labels for product reviews taken from Amazon.com in the following domains: books, dvd, electronics, and kitchen. For each domain there are 1000 positive reviews and 1000 negative reviews that comprise the “labeled” data, and then roughly 4000 more reviews in the “unlabeled”5 data. Reviews tive scientific citation. On the other hand, because there is a limited amount of annotated citation data available, by leveraging large amounts of annotated polarity data we could potentially even improve citation classification. We treat c"
P14-2008,W04-3237,0,0.0336534,"target data. In our second set of experiments, we follow the domain adaptation approaches described in (Daum´e III, 2007) and train on product review and citation data before testing on citations. 4 4.1 4.2 Easy Domain Adaptation The results in Section 4.1 are for semi-supervised domain adaptation: the case where we have some large annotated corpus (Amazon product reviews) and a large unannotated corpus (citations). There have been a number of other successful attempts at fully supervised domain adaptation, where it is assumed that some small amount of data is annotated in the target domain (Chelba and Acero, 2004; Daum´e III, 2007; Jiang and Zhai, 2007). To see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daum´e III (2007). The results of this comparison can be seen in Table 2. Briefly, “All” trains on source and target data; “Weight” is the same as “All” except that instances may be weighted differently based on their domain (weights are chosen on a development set); “Pred” trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; “LinInt” linearly interpolates predictions using the source"
P14-2008,W02-1011,0,0.018151,"ssed so that for each review you find a list of unigrams and bigrams with their frequency within the review. Unigrams from a stop list of 55 stop words are removed, but stop words in bigrams remain. Table 1 shows the distribution of polarity labels in the corpora we use for our experiments. We combine the DFKI and IMS corpora into the CITD corpus. We omit the citations labeled neutral from the DFKI corpus because the IMS corpus does not contain neutral annotation nor does the MDSD. It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al., 2002). The citation corpora presented above are both unbalanced and both have a highly skewed distribution. The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as “unlabeled” rather balanced. For this reason, in line with previous work using MDSD, we balance the labeled portion of the CITD corpus. This is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences. The IMS corpus can have multiple labeled citations per sentence: there are 122 sentences containing the 172 negative"
P14-2008,C08-1087,0,0.0741127,"Missing"
P14-2008,P07-1033,0,0.1645,"Missing"
P14-2008,I11-1070,0,0.0436208,"Missing"
P14-2008,D11-1014,0,0.0120907,"ing as possible to ensure that the features we use are meaningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al., 2011) and for sentiment domain adaptation (Glorot et al., 2011). In this paper we examine one of these approaches, marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012), which has been successful in classifying the polarity of Amazon product reviews across product domains. Since mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant domain like scientific citations. 3 3.1 Experimental Setup Corpora We are interested in domain adaptation for citation classification and therefore ne"
P14-2008,W06-1312,0,0.585607,"We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. Recent work classifying citations in scientific literature has shown that it is possible to improve classification results with ex"
P14-2008,W06-1613,0,0.256605,"We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. Recent work classifying citations in scientific literature has shown that it is possible to improve classification results with ex"
P14-2008,H05-1044,0,0.0320657,"ount of annotated citation data available, by leveraging large amounts of annotated polarity data we could potentially even improve citation classification. We treat citation polarity classification as a sentiment analysis domain adaptation task and therefore must be careful not to define features that are too domain specific. Previous work in citation polarity classification focuses on finding new citation features to improve classification, borrowing a few from text classification in general (e.g., ngrams), and perhaps others from sentiment analysis problems (e.g., the polarity lexicon from Wilson et al. (2005)). We would like to do as little feature engineering as possible to ensure that the features we use are meaningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have be"
P14-3006,D12-1050,0,0.249853,"as an initial exploration. Profound investigation is left as future work. The meaning difference in the second pair originates from the synonym substitution between “take over as chief financial officer” and “fill the position”. The embedding of the phrase “take over” matches the embedding of the single word “fill” in this context. Table 4 shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intricate features and achieves performance numbers higher than what we report here (Ji and Eisenstein, 2013; Madnani et al., 2012; Blacoe and Lapata, 2012). Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task. We are interested in how phrase embeddings make an impact on this task. To that end, we perform an analysis on test examples where word embeddings are better than phrase embeddings and vice versa. Table 5 shows four pairs, of which “phrase embedding” outperforms “word embedding” in the Methods baseline word embedding phrase embedding Accuracy 0.684 0.695 0.713 Our implementation discovers the contained phrases in the fourth pair perfectly. Yet, “word embeddin"
P14-3006,D11-1014,0,0.02376,"in continuous form and phrases that must or can occur discontinuously? (iii) Given a sentence that contains the parts of a discontinuous phrase in correct order, how do we determine that the cooccurrence of the two parts constitutes an instance of the discontinuous phrase? (iv) Which tasks benefit most significantly from the introduction of generalized phrases? Related work To date, approaches to extend embedding (or more generally “representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by me"
P14-3006,W13-3206,0,0.0594534,"Missing"
P14-3006,D12-1110,0,0.0219984,"d phrases that must or can occur discontinuously? (iii) Given a sentence that contains the parts of a discontinuous phrase in correct order, how do we determine that the cooccurrence of the two parts constitutes an instance of the discontinuous phrase? (iv) Which tasks benefit most significantly from the introduction of generalized phrases? Related work To date, approaches to extend embedding (or more generally “representation”) beyond individual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by (Socher et al., 2010; Socher et al., 2011; Socher et al., 2012; Blacoe and Lapata, 2012), in which distributed representations of phrases or even sentences are calculated from the distributed representations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a robust composition function still faces big hurdles; cf. Table 5.1 in (Mitchell and Lapata, 2010). Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units. Phrase representations can also be derived by methods other than deep"
P14-3006,C04-1051,0,0.0438159,"ficance test, we use the test of equal proportion, p &lt; .05, throughout. 4.2 Paraphrase identification task Paraphrase identification depends on semantic analysis. Standard approaches are unlikely to assign a high similarity score to the two sentences “he started the machine” and “he turned the machine on”. In our approach, embedding of the phrase “turned on” can greatly help us to infer correctly that the sentences are paraphrases. Hence, phrase embeddings and in particular embeddings of discontinuous phrases seem promising in paraphrase detection task. We use the Microsoft Paraphrase Corpus (Dolan et al., 2004) for evaluation. It consists of a training set with 2753 true paraphrase pairs and 1323 false paraphrase pairs, along with a test set with 1147 true and 578 false pairs. After discarding pairs in which neither sentence contains phrases, 3027 training pairs (2123 true vs. 904 false) and 1273 test pairs (871 true vs. 402 false) remain. The results show that phrase embeddings have an obvious advantage in this classification task, both for k = 2 and 2 ≤ k ≤ 3. This validates our hypothesis that learning embeddings for discontinuous linguistic units is promising. In our error analysis, we found two"
P14-3006,Q13-1029,0,0.0522431,"Missing"
P14-3006,D13-1090,0,0.00401295,"ctly based on naive intuitive knowledge, acting as an initial exploration. Profound investigation is left as future work. The meaning difference in the second pair originates from the synonym substitution between “take over as chief financial officer” and “fill the position”. The embedding of the phrase “take over” matches the embedding of the single word “fill” in this context. Table 4 shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intricate features and achieves performance numbers higher than what we report here (Ji and Eisenstein, 2013; Madnani et al., 2012; Blacoe and Lapata, 2012). Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task. We are interested in how phrase embeddings make an impact on this task. To that end, we perform an analysis on test examples where word embeddings are better than phrase embeddings and vice versa. Table 5 shows four pairs, of which “phrase embedding” outperforms “word embedding” in the Methods baseline word embedding phrase embedding Accuracy 0.684 0.695 0.713 Our implementation discovers the contained phrases i"
P14-3006,W13-3512,0,0.00646277,"uch embeddings perform better than word form embeddings. 1 Motivation One advantage of recent work in deep learning on natural language processing (NLP) is that linguistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013), phrases and word sequences (Socher et al., 2010; Mikolov et al., 2013).1 Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? Building on the prior work in (Socher et al., 2010; Mikolov et al., 2013), we generalize the notion of phrase to include skip-bigrams (SkipBs) and lexicon entries, 1 Socher et al. use the term “word sequence”. Mikolov et al. use the term “phrase” for word sequences that are mostly frequent continuous collocations. 41 Proceedings of the ACL 2014 Student Research Workshop, pages 41–47"
P14-3006,N12-1019,0,0.0145098,"tive knowledge, acting as an initial exploration. Profound investigation is left as future work. The meaning difference in the second pair originates from the synonym substitution between “take over as chief financial officer” and “fill the position”. The embedding of the phrase “take over” matches the embedding of the single word “fill” in this context. Table 4 shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intricate features and achieves performance numbers higher than what we report here (Ji and Eisenstein, 2013; Madnani et al., 2012; Blacoe and Lapata, 2012). Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task. We are interested in how phrase embeddings make an impact on this task. To that end, we perform an analysis on test examples where word embeddings are better than phrase embeddings and vice versa. Table 5 shows four pairs, of which “phrase embedding” outperforms “word embedding” in the Methods baseline word embedding phrase embedding Accuracy 0.684 0.695 0.713 Our implementation discovers the contained phrases in the fourth pair perf"
P15-1007,D14-1070,0,0.0229723,"Missing"
P15-1007,D12-1050,0,0.136614,"igned to ensure multigranular comparability. For general matching, we need the ability to match short sequences in one chunk with long sequences in the other chunk. For example, what is expressed by a single word in one chunk (“reignite” in q+ in the figure) may be expressed by a sequence of several words in its paraphrase (“fan the flames of” in p). To meet this objective, we learn representations for words, phrases and the entire sentence that are all mutually comparable; in particular, these representations all have the same dimensionality and live in the same space. Most prior work (e.g., Blacoe and Lapata (2012; Hu et al. (2014)) has neglected the need for multigranular comparability and performed matching within fixed levels only, e.g., only words were Introduction Many natural language processing (NLP) tasks can be posed as classifying the relationship between two T EXT C HUNKS (cf. Li et al. (2012), Bordes et al. (2014b)) where a T EXT C HUNK can be a sentence, a clause, a paragraph or any other sequence of words that forms a unit. Paraphrasing (Figure 1, top) is one task that we address in this paper and that can be formalized as classifying a T EXT C HUNK relation. The two classes correspond to"
P15-1007,D13-1090,0,0.0967982,"of matching, we instead need the ability to match a unit on a lower level of granularity in one chunk with a unit on a higher level of granularity in the other chunk. Unlike (Socher et al., 2011), our model does not rely on parsing and it can more exhaustively search the hypothesis space of possible matchings, including matchings that correspond to conflicting segmentations of the input chunks (see Section 5). 2 Related Work Paraphrase identification (PI) is a typical task of sentence matching and it has been frequently studied (Qiu et al., 2006; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013). Socher et al. (2011) utilized parsing to model the hierarchical structure of sentences and uses unfolding recursive autoencoders to learn representations for single words and phrases acting as nonleaf nodes in the tree. The main difference to MultiGranCNN is that we stack multiple convolution layers to model flexible phrases and learn representations for them, and aim to address more general sentence correspondence. Bach et al. (2014) claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. Their conclusion also endorses (Socher et al.,"
P15-1007,P14-1062,0,0.0209554,"of pooling from the previous layer of convolution – as we will explain presently. The configuration is the same (e.g., all Wl ∈ Rd×wd ) because, by design, all g-phrase representations have the same dimensionality d. This also ensures that each g-phrase representation can be directly compared with each other g-phrase representation. We use dynamic k-max pooling to extract the kl top values from each dimension after convolution in the lth block and the kL top values in the final block. We set L−l kl = max(α, d |S|e) (2) L where l = 1, · · · , L is the block index, and α = 4 is a constant (cf. Kalchbrenner et al. (2014)) that makes sure a reasonable minimum number of values is passed on to the next layer. We set kL = 1 (not 4, cf. Kalchbrenner et al. (2014)) because our design dictates that all g-phrase representations, including the representation of the T EXT C HUNK itself, have the same dimensionality. Example: for L = 4, |S |= 20, the ki are [15, 10, 5, 1]. Dynamic k-max pooling keeps the most important features and allows us to stack multiple blocks to extract hiearchical features: units on consecutive layers correspond to larger and larger parts of the T EXT C HUNK thanks to the subset selection proper"
P15-1007,D14-1067,0,0.173564,"ase (“fan the flames of” in p). To meet this objective, we learn representations for words, phrases and the entire sentence that are all mutually comparable; in particular, these representations all have the same dimensionality and live in the same space. Most prior work (e.g., Blacoe and Lapata (2012; Hu et al. (2014)) has neglected the need for multigranular comparability and performed matching within fixed levels only, e.g., only words were Introduction Many natural language processing (NLP) tasks can be posed as classifying the relationship between two T EXT C HUNKS (cf. Li et al. (2012), Bordes et al. (2014b)) where a T EXT C HUNK can be a sentence, a clause, a paragraph or any other sequence of words that forms a unit. Paraphrasing (Figure 1, top) is one task that we address in this paper and that can be formalized as classifying a T EXT C HUNK relation. The two classes correspond to the sentences being (e.g., the pair &lt;p, q+ >) or not being (e.g., the pair &lt;p, q− >) paraphrases of each other. Another task we look at is clause coherence (Figure 1, bottom). Here the two T EXT C HUNK relation classes correspond to the second clause being (e.g., the pair &lt;x, y+ >) or not being (e.g., the pair &lt;x,"
P15-1007,N12-1019,0,0.122665,"lution to the problem of matching, we instead need the ability to match a unit on a lower level of granularity in one chunk with a unit on a higher level of granularity in the other chunk. Unlike (Socher et al., 2011), our model does not rely on parsing and it can more exhaustively search the hypothesis space of possible matchings, including matchings that correspond to conflicting segmentations of the input chunks (see Section 5). 2 Related Work Paraphrase identification (PI) is a typical task of sentence matching and it has been frequently studied (Qiu et al., 2006; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013). Socher et al. (2011) utilized parsing to model the hierarchical structure of sentences and uses unfolding recursive autoencoders to learn representations for single words and phrases acting as nonleaf nodes in the tree. The main difference to MultiGranCNN is that we stack multiple convolution layers to model flexible phrases and learn representations for them, and aim to address more general sentence correspondence. Bach et al. (2014) claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. Their conclusion als"
P15-1007,D14-1002,0,0.0208078,"on-plus-pooling component, the match feature CNN (mfCNN) whose output is fed to a multilayer perceptron (MLP) that produces the final match score. Section 6 will give details. We use convolution-plus-pooling for both word sequences and match features because we want to compute increasingly abstract features at multiple levels of granularity. To ensure that g-phrases are mutually comparable when computing the s1 × s2 match feature matrix, we impose the constraint that all g-phrase representations live in the same space and have the same dimensionality. formed on the level of single words only. Gao et al. (2014) modeled interestingness between two documents with deep NNs. They mapped source-target document pairs to feature vectors in a latent space in such a way that the distance between the source document and its corresponding interesting target in that space was minimized. Interestingness is more like topic relevance, based mainly on the aggregated meaning of keywords, as opposed to more structural relationships as is the case for paraphrasing and clause coherence. We briefly discussed (Hu et al., 2014)’s ARC in Section 1. MultiGranCNN is partially inspired by ARC, but introduces multigranular com"
P15-1007,W06-1603,0,0.0565162,"Missing"
P15-1007,D14-1071,0,0.0150158,"eral sentence correspondence. Bach et al. (2014) claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. Their conclusion also endorses (Socher et al., 2011)’s and our work, for both take interactions between component phrases into account. QA is another representative sentence matching problem. Yu et al. (2014) modeled sentence representations in a simplified CNN, finally finding the match score by projecting question and answer candidates into the same space. Other relevant QA work includes (Bordes et al., 2014c; Bordes et al., 2014a; Yang et al., 2014; Iyyer et al., 2014) For more general matching, Chopra et al. (2005) and Liu (2013) used a Siamese architecture of shared-weight neural networks (NNs) to model two objects simultaneously, matching their representations and then learning a specific type of sentence relation. We adopt parts of their architecture, but we model phrase representations as well as sentence representations. Li and Xu (2012) gave a comprehensive introduction to query-document matching and argued that query and document match at different levels: term, phrase, word sense, topic, structure etc. This also applies to sent"
P15-1173,P14-1062,0,0.0118186,"t the two constraints (i) and (ii) hold as follows: X w(i) = l(i,j) (1) 1794 j s(j) = X i l(i,j) (2) These two equations are underspecified. We therefore introduce the matrix E (i,j) ∈ Rn×n : l(i,j) = E (i,j) w(i) (3) We make the assumption that the dimensions in Eq. 3 are independent of each other, i.e., E (i,j) is a diagonal matrix. Our motivation for this assumption is: (i) This makes the computation technically feasible by significantly reducing the number of parameters and by supporting parallelism. (ii) Treating word embeddings on a per-dimension basis is a frequent design choice (e.g., Kalchbrenner et al. (2014)). Note that we allow E (i,j) < 0 and in general the distribution weights for each dimension (diagonal entries of E (i,j) ) will be different. Our assumption can be interpreted as word w(i) distributing its embedding activations to its lexemes on each dimension separately. Therefore, Eqs. 1-2 can be written as follows: X w(i) = E (i,j) w(i) (4) j s(j) = X E (i,j) w(i) (5) i Note that from Eq. 4 it directly follows that: X E (i,j) = In ∀i (6) with In being the identity matrix. Let W be a |W |× n matrix where n is the dimensionality of the embedding space, |W |is the number of words and each row"
P15-1173,S01-1004,0,0.0240799,"r lexemes and synsets. Thus, we can compute nearest neighbors across all three data types as shown in Figure 2. We evaluate the embeddings on WSD and on similarity performance. Our results depend directly on the quality of the underlying word embeddings, in our case word2vec embeddings. We would expect even better evaluation results as word representation learning methods improve. Using a new and improved set of underlying embeddings is simple: it is a simple switch of the input file that contains the word embeddings. 3.1 Word Sense Disambiguation For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system named IMS (Zhong and Data, experiments and evaluation We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of ≈1011 tokens, using word2vec CBOW (Mikolov et al., 2013c). Many words in the word2vec vocabulary are not in WordNet, words synsets lexemes WordNet 147,478 117,791 207,272 ∩ word2vec 54,570 73,844 106,167 Table 2: # of items in WordNet and after intersection with word2vec vectors 1797 nearest neighbors of W/suit The S-raw feature set simply consists of the n(k + 1) elements of centr"
P15-1173,P13-2087,0,0.0210262,"re is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. There is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to create new embeddings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings. Lexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interest"
P15-1173,W04-0807,0,0.187432,"n compute nearest neighbors across all three data types as shown in Figure 2. We evaluate the embeddings on WSD and on similarity performance. Our results depend directly on the quality of the underlying word embeddings, in our case word2vec embeddings. We would expect even better evaluation results as word representation learning methods improve. Using a new and improved set of underlying embeddings is simple: it is a simple switch of the input file that contains the word embeddings. 3.1 Word Sense Disambiguation For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system named IMS (Zhong and Data, experiments and evaluation We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of ≈1011 tokens, using word2vec CBOW (Mikolov et al., 2013c). Many words in the word2vec vocabulary are not in WordNet, words synsets lexemes WordNet 147,478 117,791 207,272 ∩ word2vec 54,570 73,844 106,167 Table 2: # of items in WordNet and after intersection with word2vec vectors 1797 nearest neighbors of W/suit The S-raw feature set simply consists of the n(k + 1) elements of centroid and synset vectors: S/suit (busines"
P15-1173,D14-1110,0,0.512236,"et ns sy s Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. There is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to cre"
P15-1173,C14-1048,0,0.030559,"ings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings. Lexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998"
P15-1173,P12-1092,0,0.892075,"re. But for completeness, we also run experiments for synsets. For each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, including (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms. A simple baseline is to use the underlying word2vec embeddings directly (line 5). In this case, there is only one embedding, so there is no difference between AvgSim and AvgSimC. It is interesting that even if we do not take the context into account (method AvgSim) the lexeme embeddings outperform the original word embeddings. As AvgSi"
P15-1173,P10-1023,0,0.0462333,"n translations (Mikolov et al., 2013b). Let X be a matrix where each row is a word embedding in language 1 and Y a matrix where each row is a word embedding in language 2. For each row the words of X and Y are a translation of each other. We then want to minimize the following objective: argminkLX − Y k L (26) We can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by: L = (X T ∗ X)−1 (X T ∗ Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept. 6 Related Work Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches"
P15-1173,D14-1113,0,0.390055,"gle-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations lex em es et ns sy s Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “"
P15-1173,D14-1162,0,0.106071,"-of-the-art performance on word similarity and word sense disambiguation tasks. 1 Introduction Unsupervised methods for word embeddings (also called “distributed word representations”) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pronunciation with a particular meaning, i.e., a lexeme is a conju"
P15-1173,N10-1013,0,0.454966,"in context came from is not available. However, the dataset is the closest we could find for sense similarity. Synset and lexeme embeddings are obtained by running AutoExtend. Based on the results of the WSD task, we set α = 0.2 and β = 0.5. Lexeme embeddings are the natural choice for this task as human subjects are provided with two words and a context for each and then have to assign a similarity score. But for completeness, we also run experiments for synsets. For each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, including (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more pre"
P15-1173,J98-1004,1,0.286068,"Missing"
P15-1173,C14-1016,0,0.549568,"we also run experiments for synsets. For each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, including (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms. A simple baseline is to use the underlying word2vec embeddings directly (line 5). In this case, there is only one embedding, so there is no difference between AvgSim and AvgSimC. It is interesting that even if we do not take the context into account (method AvgSim) the lexeme embeddings outperform the original word embeddings. As AvgSim simply adds up all lex"
P15-1173,P14-2089,0,0.177774,"mbeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. There is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to create new embeddings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings. Lexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN ("
P15-1173,P10-4014,0,0.713976,"Missing"
P15-1173,J15-4004,0,\N,Missing
P15-1173,W06-1104,0,\N,Missing
P15-1173,I05-1067,0,\N,Missing
P15-1173,W13-3512,0,\N,Missing
P16-1023,N09-1003,0,0.1486,"Missing"
P16-1023,Q15-1016,0,0.0427024,"for k = 1).6 We take part of ClueWeb, which is annotated with Freebase entities using automatic annotation of FACC17 (Gabrilovich et al., 2013), as our corpus. We then replace all mentions of entities with their Freebase identifier and learn embeddings of words and entities in the same space. Our corpus has around 6 million sentences with at least one annotated entity. We calculate embeddings using our different models. Our hyperparameters: for learning-based models: dim=100, neg=10, iterations=20, window=1, sub=10−3 ; for PPMI: SVD-dim=100, neg=1, window=1, cds=0.75, sub=10−3 , eig=0.5. See (Levy et al., 2015) for more information about the meaning of hyperparameters. Table 1 gives results on test for all (about 60,000 entities), head (freq > 100; about 12,200 entities) and tail (freq &lt; 5; about 10,000 entities). The MLP models consistently outperform 1NN on Table 1: Entity typing results using embeddings learned with different models. global statistics. (ii) Embedding learning can have different effectiveness for sparse vs. non-sparse events. Thus, models of representations should be evaluated with respect to their ability to deal with sparseness; evaluation data sets should include rare as well a"
P16-1023,J10-4006,0,0.0479666,"ated in a classification setting, in which we apply a linear SVM (Fan et al., 2008) to classify embeddings. Finally, we compare the classification results for different embedding models and analyze and summarize them. Selecting embedding models. Since this paper is about developing a new evaluation methodology, the choice of models is not important as long as the models can serve to show that the proposed methodology reveals interesting differences with respect to the criteria. On the highest level, we can distinguish two types of distributional representations. Count vectors (Sahlgren, 2006; Baroni and Lenci, 2010; Turney and Pantel, 2010) live in a highdimensional vector space in which each dimension roughly corresponds to a (weighted) count of cooccurrence in a large corpus. Learned vectors are learned from large corpora using machine learning methods: unsupervised methods such as LSI (e.g., Deerwester et al. (1990), Levy and Goldberg (2014b)) and supervised methods such as neural networks (e.g., Mikolov et al. (2013)) and regression (e.g., Pennington et al. (2014)). Because of the recent popularity of learning-based methods, we consider one count-based and five learning-based distributional represen"
P16-1023,D15-1200,0,0.295059,"only the overall direction of vectors is considered or, equivalently, that words are modeled as points in a space and only their fullspace distance/closeness is considered. In contrast, we test embeddings in a classification setting and different subspaces of embeddings are analyzed. Tsvetkov et al. (2015) evaluate embeddings based on their correlations with WordNetbased linguistic embeddings. However, correlation does not directly evaluate how accurately and completely an application can extract a particular piece of information from an embedding. Extrinsic evaluations are also common (cf. (Li and Jurafsky, 2015; K¨ohn, 2015; Lai et al., 2015)). Li and Jurafsky (2015) conclude that embedding evaluation must go beyond human-judgement tasks like similarity and analogy. They suggest to evaluate on NLP tasks. K¨ohn (2015) gives similar suggestions and also recommends the use of supervised methods for evaluation. Lai et al. (2015) evaluate embeddings in different tasks with different setups and show the contradictory results of embedding models on different tasks. Idiosyn237 1 2 3 4 5 6 7 8 (Comrie, 1989). (i) Words have a large number of facets, including phonetic, morphological, syntactic, semantic and"
P16-1023,P14-1023,0,0.165665,"that is not an analysis of the causes of the improvement. Therefore, extrinsic evaluations do not satisfy our goals. Intrinsic evaluation analyzes the generic quality of embeddings. Currently, this evaluation mostly is done by testing overall distance/similarity of words in the embedding space, i.e., it is based on viewing word representations as points and then computing full-space similarity. The assumption is that the high dimensional space is smooth and similar words are close to each other. Several datasets have been developed for this purpose, mostly the result of human judgement; see (Baroni et al., 2014) for an overview. We refer to these evaluations as point-based and as full-space because they consider embeddings as points in the space – sub-similarities in subspaces are generally ignored. Point-based intrinsic evaluation computes a score based on the full-space similarity of two words: a single number that generally does not say anything about the underlying reasons for a lower or higher value of full-space similarity. This makes it hard to interpret the results of point-based evaluation and may be the reason that contradictory results have been published; e.g., based on We introduce a new"
P16-1023,N15-1142,0,0.0465702,"and Goldberg (2014b)) and supervised methods such as neural networks (e.g., Mikolov et al. (2013)) and regression (e.g., Pennington et al. (2014)). Because of the recent popularity of learning-based methods, we consider one count-based and five learning-based distributional representation models. The learning-based models are: (i) vLBL (henceforth: LBL) (vectorized log-bilinear language model) (Mnih and Kavukcuoglu, 2013), (ii) SkipGram (henceforth: SKIP) (skipgram bagof-word model), (iii) CBOW (continuous bag-ofword model (Mikolov et al., 2013), (iv) Structured SkipGram (henceforth SSKIP), (Ling et al., 2015) and CWindow (henceforth CWIN) (continuous window model) (Ling et al., 2015). These models learn word embeddings for input and target spaces using neural network models. For a given context, represented by the input space representations of the left and right neighbors ~vi−1 and ~vi+1 , LBL, CBOW and CWIN predict the target space ~vi by combining the contexts. LBL combines ~vi−1 and ~vi+1 linearly with position dependent weights and CBOW (resp. CWIN) combines them by adding (resp. concatenation). SKIP and SSKIP predict the context words vi−1 or vi+1 given the input space ~vi . For SSKIP, conte"
P16-1023,D14-1162,0,0.113637,"luation evaluates embeddings in an NLP application or task and (ii) intrinsic evalu236 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 236–246, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics crasies of different downstream tasks can affect extrinsic evaluations and result in contradictions. point-based evaluation, some papers have claimed that count-based representations perform as well as learning-based representations (Levy and Goldberg, 2014a). Others have claimed the opposite (e.g., Mikolov et al. (2013), Pennington et al. (2014), Baroni et al. (2014)). Given the limits of current evaluations, we propose a new methodology for intrinsic evaluation of embeddings by identifying generic fundamental criteria for embedding models that are important for representing features of words accurately and consistently. We develop corpus-based tests using supervised classification that directly show whether the representations contain the information necessary to meet the criteria or not. The fine-grained corpus-based supervision makes the sub-similarities of words important by looking at the subspaces of word embeddings relevant to"
P16-1023,N10-1013,0,0.0876124,"Missing"
P16-1023,P12-1092,0,0.231901,"Missing"
P16-1023,P16-2083,1,0.891482,"Missing"
P16-1023,N16-1091,1,0.885471,"Missing"
P16-1023,D15-1246,0,0.0496166,"Missing"
P16-1023,P15-2119,0,0.0772715,"supported by a real natural-language corpus. 5.1 Learned lessons (i) Two words with clearly different context distributions should receive different representations. Aggregation models fail to do so by calculating 242 PPMI LBL CBOW CWIN SKIP SSKIP all entities MLP 1NN 61.6 44.0 63.5 51.7 63.0 53.5 66.1 53.0 64.5 57.1 66.2 52.8 head entities MLP 1NN 69.2 63.8 72.7 66.4 71.7 69.4 73.5 68.6 69.9 71.5 73.9 68.5 tail entities MLP 1NN 43.0 28.5 44.1 32.8 39.1 29.9 46.8 31.4 49.8 34.0 45.5 31.4 Learning taxonomic properties or types of words has been used as an evaluation method for word embeddings (Rubinstein et al., 2015). Since available word typing datasets are quite small (cf. Baroni et al. (2014), Rubinstein et al. (2015)), entity typing can be a promising alternative, which enables to do supervised classification instead of unsupervised clustering. Entities, like other words, have many properties and therefore belong to several semantic types, e.g., “Barack Obama” is a POLITICIAN, AUTHOR and AWARD WINNER . We perform entity typing by learning types of knowledge base entities from their embeddings; this requires looking at subspaces because each entity can belong to multiple types. We adopt the setup of Ya"
P16-1023,W14-1618,0,0.451888,"eddings. We distinguish two different types of evaluation in this paper: (i) extrinsic evaluation evaluates embeddings in an NLP application or task and (ii) intrinsic evalu236 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 236–246, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics crasies of different downstream tasks can affect extrinsic evaluations and result in contradictions. point-based evaluation, some papers have claimed that count-based representations perform as well as learning-based representations (Levy and Goldberg, 2014a). Others have claimed the opposite (e.g., Mikolov et al. (2013), Pennington et al. (2014), Baroni et al. (2014)). Given the limits of current evaluations, we propose a new methodology for intrinsic evaluation of embeddings by identifying generic fundamental criteria for embedding models that are important for representing features of words accurately and consistently. We develop corpus-based tests using supervised classification that directly show whether the representations contain the information necessary to meet the criteria or not. The fine-grained corpus-based supervision makes the sub"
P16-1023,D15-1036,0,0.0354266,"hould fully represent all senses of an ambiguous word. This criterion becomes more difficult to satisfy as distributions of senses become more skewed, but a robust model must be able to overcome this. Accurate and consistent representation of multifacetedness. This criterion addresses settings with large numbers of facets. It is based on the following linguistic phenomenon, a phenomenon that occurs frequently crosslinguistically Related Work Baroni et al. (2014) evaluate embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Schnabel et al. (2015) introduce tasks with more fine-grained datasets. These tasks are unsupervised and generally based on cosine similarity; this means that only the overall direction of vectors is considered or, equivalently, that words are modeled as points in a space and only their fullspace distance/closeness is considered. In contrast, we test embeddings in a classification setting and different subspaces of embeddings are analyzed. Tsvetkov et al. (2015) evaluate embeddings based on their correlations with WordNetbased linguistic embeddings. However, correlation does not directly evaluate how accurately and"
P16-1023,D15-1243,0,0.0310802,"k Baroni et al. (2014) evaluate embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Schnabel et al. (2015) introduce tasks with more fine-grained datasets. These tasks are unsupervised and generally based on cosine similarity; this means that only the overall direction of vectors is considered or, equivalently, that words are modeled as points in a space and only their fullspace distance/closeness is considered. In contrast, we test embeddings in a classification setting and different subspaces of embeddings are analyzed. Tsvetkov et al. (2015) evaluate embeddings based on their correlations with WordNetbased linguistic embeddings. However, correlation does not directly evaluate how accurately and completely an application can extract a particular piece of information from an embedding. Extrinsic evaluations are also common (cf. (Li and Jurafsky, 2015; K¨ohn, 2015; Lai et al., 2015)). Li and Jurafsky (2015) conclude that embedding evaluation must go beyond human-judgement tasks like similarity and analogy. They suggest to evaluate on NLP tasks. K¨ohn (2015) gives similar suggestions and also recommends the use of supervised methods"
P16-1023,D15-1083,1,0.809884,"Missing"
P16-1023,P10-4014,0,0.0768217,"Missing"
P16-1128,P14-2131,0,0.0703569,"prior work has studied differences in performance of different embedding sets. For example, Chen et al. (2013) show that the embedding sets HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012) have great variance in quality and characteristics of the semantics captured. Hill et al. (2014; 2015a) show that embeddings learned by NN machine translation models can outperform three representative monolingual embedding sets: word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and CW (Collobert and Weston, 2008). Bansal et al. (2014) find that Brown clustering, SENNA, CW, Huang and word2vec yield significant gains for dependency parsing. Moreover, using these representations together achieved the best results, suggesting their complementarity. These prior studies motivate us to explore an ensemble approach. Each embedding set is trained by a different NN on a different corpus, hence can be treated as a distinct description of words. We want to leverage this diversity to learn better-performing word embeddings. Our expectation is that the ensemble contains more information than each component embedding set. The ensemble ap"
P16-1128,J15-4004,0,0.0540221,"RND 22.4 7.7 4.9 62.4 54.3 60.0 – syntactic AVG ml 1 TO N+ 22.4 22.7 22.9 4.1 10.9 11.4 5.0 5.0 5.5 15.1 74.9 – 13.6 70.1 – 15.0 76.8 – – – 76.3 RND 24.1 4.8 10.5 36.2 31.5 34.7 – total AVG ml 1 TO N+ 24.2 24.4 24.5 3.3 15.8 16.2 10.5 10.3 11.4 16.3 81.0 – 15.4 77.9 – 16.1 82.0 – – – 81.1 Table 4: Comparison of effectiveness of four methods for learning OOV embeddings. RND: random initialization. AVG: average of embeddings of known words. ml: M UTUAL L EARNING. RW(21) means there are still 21 OOVs for the vocabulary union. 1356 6.1 Word Similarity and Analogy Tasks We evaluate on SimLex-999 (Hill et al., 2015b), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and RW (Luong et al., 2013). For completeness, we also show results for MC30, the validation set. The word analogy task proposed in (Mikolov et al., 2013b) consists of questions like, “a is to b as c is to ?”. The dataset contains 19,544 such questions, divided into a semantic subset of size 8869 and a syntactic subset of size 10,675. Accuracy is reported. We also collect the state-of-the-art report for each task. SimLex-999: (Wieting et al., 2015), WS353: (Halawi et al., 2012). Not all state-ofthe-art results are included in"
P16-1128,P12-1092,0,0.0765137,"hieved remarkable results in NLP (Collobert and Weston, 2008; Sutskever et al., 2014; Yin and Sch¨utze, 2015). One reason for these results are word embeddings, compact distributed word representations learned in an unsupervised manner from large corpora (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013a; Pennington et al., 2014). Some prior work has studied differences in performance of different embedding sets. For example, Chen et al. (2013) show that the embedding sets HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012) have great variance in quality and characteristics of the semantics captured. Hill et al. (2014; 2015a) show that embeddings learned by NN machine translation models can outperform three representative monolingual embedding sets: word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and CW (Collobert and Weston, 2008). Bansal et al. (2014) find that Brown clustering, SENNA, CW, Huang and word2vec yield significant gains for dependency parsing. Moreover, using these representations together achieved the best results, suggesting their complementarity. These prior studies motivate us"
P16-1128,W13-3512,0,0.0266789,"5.0 5.5 15.1 74.9 – 13.6 70.1 – 15.0 76.8 – – – 76.3 RND 24.1 4.8 10.5 36.2 31.5 34.7 – total AVG ml 1 TO N+ 24.2 24.4 24.5 3.3 15.8 16.2 10.5 10.3 11.4 16.3 81.0 – 15.4 77.9 – 16.1 82.0 – – – 81.1 Table 4: Comparison of effectiveness of four methods for learning OOV embeddings. RND: random initialization. AVG: average of embeddings of known words. ml: M UTUAL L EARNING. RW(21) means there are still 21 OOVs for the vocabulary union. 1356 6.1 Word Similarity and Analogy Tasks We evaluate on SimLex-999 (Hill et al., 2015b), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and RW (Luong et al., 2013). For completeness, we also show results for MC30, the validation set. The word analogy task proposed in (Mikolov et al., 2013b) consists of questions like, “a is to b as c is to ?”. The dataset contains 19,544 such questions, divided into a semantic subset of size 8869 and a syntactic subset of size 10,675. Accuracy is reported. We also collect the state-of-the-art report for each task. SimLex-999: (Wieting et al., 2015), WS353: (Halawi et al., 2012). Not all state-ofthe-art results are included in Table 3. One reason is that a fair comparison is only possible on the shared vocabulary, so met"
P16-1128,D14-1162,0,0.114207,"rmance of metaembeddings compared to individual embedding sets. One advantage of metaembeddings is the increased vocabulary coverage. We release our metaembeddings publicly at http:// cistern.cis.lmu.de/meta-emb. 1 Introduction Recently, deep neural network (NN) models have achieved remarkable results in NLP (Collobert and Weston, 2008; Sutskever et al., 2014; Yin and Sch¨utze, 2015). One reason for these results are word embeddings, compact distributed word representations learned in an unsupervised manner from large corpora (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013a; Pennington et al., 2014). Some prior work has studied differences in performance of different embedding sets. For example, Chen et al. (2013) show that the embedding sets HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012) have great variance in quality and characteristics of the semantics captured. Hill et al. (2014; 2015a) show that embeddings learned by NN machine translation models can outperform three representative monolingual embedding sets: word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and CW (Collobert and Weston, 2"
P16-1128,N15-1058,0,0.0311582,"Missing"
P16-1128,Q14-1002,1,0.417449,"Missing"
P16-1128,D14-1101,0,0.107008,"multiple embedding sets with different properties being available is likely to persist for the forseeable future. Metaembedding learning is a simple and efficient way of taking advantage of this diversity. As we will show below they combine several complementary embedding sets and the resulting metaembeddings are stronger than each individual set. 2 Related Work Related work has focused on improving performance on specific tasks by using several embedding sets simultaneously. To our knowledge, there is no work that aims to learn generally useful metaembeddings from individual embedding sets. Tsuboi (2014) incorporates word2vec and GloVe embeddings into a POS tagging system and finds that using these two embedding sets together is better than using them individually. Similarly, Turian et al. (2010) find that using Brown clusters, CW embeddings and HLBL embeddings for Name Entity Recognition and chunking tasks together gives better performance than using these representations individually. Luo et al. (2014) adapt CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets – a Wikipedia corpus, search clickthrough data and user query data – for web search ranking and for word simi"
P16-1128,P10-1040,0,0.0653723,"ural network (NN) models have achieved remarkable results in NLP (Collobert and Weston, 2008; Sutskever et al., 2014; Yin and Sch¨utze, 2015). One reason for these results are word embeddings, compact distributed word representations learned in an unsupervised manner from large corpora (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013a; Pennington et al., 2014). Some prior work has studied differences in performance of different embedding sets. For example, Chen et al. (2013) show that the embedding sets HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012) have great variance in quality and characteristics of the semantics captured. Hill et al. (2014; 2015a) show that embeddings learned by NN machine translation models can outperform three representative monolingual embedding sets: word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and CW (Collobert and Weston, 2008). Bansal et al. (2014) find that Brown clustering, SENNA, CW, Huang and word2vec yield significant gains for dependency parsing. Moreover, using these representations together achieved the best results, suggesting their complementarity."
P16-1128,K15-1021,1,0.774588,"Missing"
P16-1128,D15-1155,1,0.891534,"Missing"
P16-1128,N16-1178,0,0.0283603,"ng them individually. Similarly, Turian et al. (2010) find that using Brown clusters, CW embeddings and HLBL embeddings for Name Entity Recognition and chunking tasks together gives better performance than using these representations individually. Luo et al. (2014) adapt CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets – a Wikipedia corpus, search clickthrough data and user query data – for web search ranking and for word similarity. They show that using these embeddings together gives stronger results than using them individually. Both (Yin and Sch¨utze, 2015) and (Zhang et al., 2016) try to incorporate multiple embedding sets into channels of convolutional neural network system for sentence classification tasks. The better performance also hints the complementarity of component embedding sets, however, such kind of incorporation brings large numbers of training parameters. In sum, these papers show that using multiple embedding sets is beneficial. However, they either use embedding sets trained on the same corpus (Turian et al., 2010) or enhance embedding sets by more training data, not by innovative learning algorithms (Luo et al., 2014), or make the system architectures"
P16-1128,Q15-1025,0,\N,Missing
P16-1156,N15-1140,1,0.856025,"Missing"
P16-1156,K15-1017,1,0.876559,"Missing"
P16-1156,W13-3512,0,0.247022,"ative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from PPDB (Ganitkevitch et al., 2013), which allows them to smooth and extend PPDB just as we do to WORD 2 VEC output. Using morphological resources to enhance embeddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neural network (Goller and Kuchler, 1996; Socher, 2014) to generate compositional word embeddings. Our model is much simpler and faster to train. Their evaluation was limited to English and focused on rare English words. dos Santos and Zadrozny (2014) introduced a neural tagging architecture (Collobert et al., 2011) with a characterlevel convolutional layer. Qiu et al. (2014) and Botha and Blunsom (2014) both use M ORFESSOR segmentations t"
P16-1156,Q15-1031,1,0.858459,"tion, we fit a directed Gaussian graphical model (GGM) that simultaneously considers (i) each word’s embedding (obtained from an embedding model like WORD 2 VEC) and (ii) its morphological analysis (obtained from a lexical resource). We then use this model to smooth the provided embeddings, and to generate embeddings for unseen inflections. For a lemma covered by the resource, the GGM can produce embeddings for all its forms (if at least one of these forms has a known embedding); this can be extended to words not covered using a guesser like M ORFESSOR (Creutz and Lagus, 2007) or C HIP M UNK (Cotterell et al., 2015a). A major difference of our approach from related techniques is that our model uses existing morphological resources (e.g., morphological lexicons or finite-state analyzers) rather than semantic resources (e.g., WordNet (Miller et al., 1990) and PPDB (Ganitkevitch et al., 2013)). The former tend to be larger: we often can analyze more words than we have semantic representations for. It would be possible to integrate our GGM into the training procedure for a word embedding system, making that system sensitive to morphological attributes. However, the postprocessing approach in our present pap"
P16-1156,N13-1090,0,0.511556,"in the literature. One of the most interesting discoveries is that these representations capture meaningful morpho-syntactic and semantic properties through very simple linear relations: in a semantic vector space, we observe that vtalked − vtalk ≈ vdrank − vdrink . (1) That this equation approximately holds across many morphologically related 4-tuples indicates that the learned embeddings capture a feature of English morphology—adding the past tense feature roughly corresponds to adding a certain vector. Moreover, manipulating this equation yields what we will call the vector offset method (Mikolov et al., 2013c) for approximating other vectors. For instance, if we only know the vectors for the Spanish words comieron (ate), comemos (eat) and bebieron (drank), we can produce an approximation of the vector for bebemos (drink), as shown in Figure 1. Many languages exhibit much richer morphology than English. While English nouns commonly take two forms – singular and plural— Czech nouns take 12 and Turkish nouns take over 30. This increase in word forms per lemma creates considerable data sparsity. Fortunately, for many languages there exist large morphological lexicons, or better yet, morphological too"
P16-1156,N15-1184,0,0.0886934,"omit i from all Wk . After convergence, set wi ← k∈Mi mk . 4 Note that it is not necessary to define it as λ0 I, introducing a new scale parameter λ0 , since doubling λ0 would have the same effect on the MAP update rules as halving λ and Σi . 1654 Viterbi EM can be regarded as block coordinate descent on the negative log-likelihood function, with E and M steps both improving this common objective along different variables. We update the parameters (M step above) after each 10 passes of updating the latent vectors (section 5’s E step). 7 Related Work Our postprocessing strategy is inspired by Faruqui et al. (2015), who designed a retrofitting procedure to modify pre-trained vectors such that their relations match those found in semantic lexicons. We focus on morphological resources, rather than semantic lexicons, and employ a generative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from"
P16-1156,W15-1518,0,0.0367522,"Missing"
P16-1156,N13-1092,0,0.105894,"Missing"
P16-1156,C14-1015,0,0.0215187,"ddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neural network (Goller and Kuchler, 1996; Socher, 2014) to generate compositional word embeddings. Our model is much simpler and faster to train. Their evaluation was limited to English and focused on rare English words. dos Santos and Zadrozny (2014) introduced a neural tagging architecture (Collobert et al., 2011) with a characterlevel convolutional layer. Qiu et al. (2014) and Botha and Blunsom (2014) both use M ORFESSOR segmentations to augment WORD 2 VEC and a logbilinear (LBL) language model (Mnih and Hinton, 2007), respectively. Similar to us, they have an additive model of the semantics of morphemes, i.e., the embedding of the word form is the sum of the embeddings of its constituents. In contrast to us, however, both include the word form itself in the sum. Finally, Cotterell and Sch¨utze (2015) jointly trained an LBL language model and a morphological tagger (Hajiˇc, 2000) to encourage the embeddings to encode rich morphology. With the exception of (Cott"
P16-1156,A00-2013,0,0.0304152,"Missing"
P16-1156,D09-1124,0,0.00984588,"e confined to rare predicting words.) See Appendix B for more analysis. 1658 Forms / Lemma Skip-Gram GGM English 1.8 58.9 58.9 German 6.3 36.2 37.6 Spanish 8.1 37.8 40.3 Table 4: Word similarity results (correlations) using the WS353 dataset in the three languages, in which it is available. Since all the words in WS-353 are lemmata, we report the average inflected form to lemma ratio for forms appearing in the datasets. 8.3 9 Experiment 3: Word Similarity As a third and final experiment, we consider word similarity using the WS-353 data set (Finkelstein et al., 2001), translated into Spanish (Hassan and Mihalcea, 2009) and German (Leviant, 2016).11 The datasets are composed of 353 pairs of words. Multiple native speakers were then asked to give an integral value between 1 and 10 indicating the similarity of that pair, and those values were then averaged. In each case, we train the GGM on the whole Wikipedia corpus for the language. Since in each language every word in the WS-353 set is in fact a lemma, we use the latent embedding our GGM learns in the experiment. In Spanish, for example, we use the learned latent morpheme embedding for the lemma BEBER (recall this takes information from every element in the"
P16-1156,P15-2111,0,0.0948168,"must instead select from its paradigm the word type, such as beb´eis, that expresses the contextually appropriate properties. Noun tokens in a language may similarly be required to be inflected for properties such as case, gender, and number. A content word is chosen by specifying a lemma (which selects a particular paradigm) together with some inflectional attributes (which select a particular slot within that paradigm). For example, [ Lemma=EAT, Person=3, Number=S INGULAR, Tense=P RESENT ] is a bundle of attribute-value pairs that would be jointly expressed in English by the word form eats (Sylak-Glassman et al., 2015). The regularities observed by Mikolov et al. (2013c) hold between words with similar attributevalue pairs. In Spanish, the word beben “they drink” (Table 1) can be analyzed as expressing the bundle [ Lemma=BEBER, Person=3, Number=P LURAL, Tense=P RESENT ]. Its vector similarity to bebemos “we drink” is due to the fact that both word forms have the same lemma BE BER . Likewise, the vector similarity of beben to comieron “they ate” is due to the conceptual similarity of their lemmas, BEBER “drink” and COMER “eat”. Conversely, that beben is similar to preguntan “they ask” is caused by shared inf"
P16-1156,W15-0105,0,0.0336122,"Missing"
P16-1156,P15-2008,0,0.0459499,"Missing"
P16-1156,Q15-1025,0,0.0118304,"long different variables. We update the parameters (M step above) after each 10 passes of updating the latent vectors (section 5’s E step). 7 Related Work Our postprocessing strategy is inspired by Faruqui et al. (2015), who designed a retrofitting procedure to modify pre-trained vectors such that their relations match those found in semantic lexicons. We focus on morphological resources, rather than semantic lexicons, and employ a generative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from PPDB (Ganitkevitch et al., 2013), which allows them to smooth and extend PPDB just as we do to WORD 2 VEC output. Using morphological resources to enhance embeddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neur"
P16-2083,H05-1044,0,0.0246162,"an orthogonal matrix. For training Eq. 2 is weighted with α∗ and Eq. 3 with 1 − α∗ . We do a batch gradient descent where each batch contains the same number of positive and negative examples. This means the number of examples in the lexica – which give rise to more negative than positive examples – does not influence the training. 3 ∗∈{p,c,f,m} (v,w)∈L∗6∼ Setup and Method Eqs. 2/3 can be combined to train an orthogonal transformation matrix. We use pretrained 300dimensional English word embeddings (Mikolov et al., 2013) (W2V). To train the transformation matrix, we use a combination of MPQA (Wilson et al., 2005), Opinion Lexicon (Hu and Liu, 2004) and NRC Emotion lexicons (Mohammad and Turney, 2013) for polarity; BWK, a lexicon of 40,000 English words (Brysbaert et al., 2014), for concreteness; the order in the word embedding file for frequency; and the training set of the FLORS tagger (Schnabel and Sch¨utze, 2014) for POS. The application of the transformation maFor notational simplicity, u∗v will either refer to a ∗ vector in Rd or to a vector in Rd where all dimensions ∈ / D∗ are set to zero. For training, the orthogonal transformation Q we assume we have a lexicon resource. Let L∗6∼ be a set of w"
P16-2083,D14-1151,1,0.86371,"Missing"
P16-2083,D12-1111,0,0.0149992,"Missing"
P16-2083,D15-1155,1,0.895539,"Missing"
P16-2083,N15-1140,1,0.77822,"Missing"
P16-2083,Q13-1023,0,0.033414,"Missing"
P16-2083,N16-1091,1,0.138983,"Missing"
P16-2083,Q14-1002,1,0.3719,"Missing"
P16-2083,K15-1026,0,0.0207418,"arity Spectrum Creation, Morphological Analogy and POS Tagging. Our evaluation shows that our method outperforms previous work and is applicable to different types of information. We have published test sets and word embeddings at http://www.cis.lmu. de/˜sascha/Ultradense/. Yih et al. (2012) also tackled the problem of antonyms having similar embeddings. In their model, the antonym is the inverse of the entire vector whereas in our work the antonym is only the inverse in an ultradense subspace. Our model is more intuitive since antonyms invert only part of the meaning, not the entire meaning. Schwartz et al. (2015) present a method that switches an antonym parameter on or off (depending on whether a high antonym-synonym similarity is useful for an application) and learn multiple embedding spaces. We only need a single space, but consider different subspaces of this space. An unsupervised approach using linguistic patterns that ranks adjectives according to their intensity was presented by de Melo and Bansal (2013). Sharma et al. (2015) present a corpus-independent approach for the same problem. Our results (Table 1) suggest that polarity should not be considAcknowledgments This research was supported by"
P16-2083,D15-1300,0,0.0303295,"Missing"
P16-2090,E14-1060,0,0.155742,"Missing"
P16-2090,N15-1107,0,0.19027,"ss of downstream tasks like machine translation and question answering. Accordingly, learning morphological inflection patterns from labeled data is an important challenge. The task of morphological reinflection (MRI) consists of producing an inflected form for a given source form, source tag and target tag. A special case is morphological inflection (MI), the task of finding an inflected form for a given lemma and target tag. An English example is “tree”+PLURAL → “trees”. Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In this work, we propose the neural encoderdecoder MED – Morphological Encoder-Decoder – a character-level sequence-to-sequence attention model that is a language-independent solution for 1 ryancotterell.github.io/ sigmorphon2016/ 555 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 555–560, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics alizations across tag pairs. 2 with st being an RNN hidden state for time t and ct being the weighted sum of the annotations (h1 , ..., h"
P16-2090,W14-4012,0,0.247307,"Missing"
P16-2090,D15-1272,1,0.873371,"Missing"
P16-2090,N15-1093,0,0.341306,"translation and question answering. Accordingly, learning morphological inflection patterns from labeled data is an important challenge. The task of morphological reinflection (MRI) consists of producing an inflected form for a given source form, source tag and target tag. A special case is morphological inflection (MI), the task of finding an inflected form for a given lemma and target tag. An English example is “tree”+PLURAL → “trees”. Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In this work, we propose the neural encoderdecoder MED – Morphological Encoder-Decoder – a character-level sequence-to-sequence attention model that is a language-independent solution for 1 ryancotterell.github.io/ sigmorphon2016/ 555 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 555–560, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics alizations across tag pairs. 2 with st being an RNN hidden state for time t and ct being the weighted sum of the annotations (h1 , ..., hTx ) produced by the encoder, using t"
P16-2090,W98-1239,0,0.140041,"Missing"
P16-2090,D08-1113,0,0.723939,"reinflection in a language. Thus, if the training set is not large enough in this respect, then POET will not be beneficial. Figure 1: Overview of MED 3 Experiments where S(x) and T (x) are source and target tags of x and e(x) is e(σ(x), τ (x)), the edit tree that transforms the source form into the target form. Let ρ be a target form predicted by the MRI system for the source form σ and let s and t be source and target tags. POET does not change ρ if e(σ, ρ) ∈ Es,t . Otherwise it replaces ρ with τ :  0 τ if e(σ, τ 0 ) ∈ Es,t , |ρ, τ 0 |= 1 τ := ρ else We compare MED with the three models of Dreyer et al. (2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al. (2015)’s encoder-decoder model.3 We call the latter MODEL*TAG as it requires training as many models as there are target tags. We evaluate MED on two MRI tasks: CELEX and SIGMORPHON16. CELEX. This task is based on complete inflection tables for German extracted from CELEX. For this experiment we follow Dreyer et al. (2008). We use four pairs of morphological tags and corresponding word forms from the German pa"
P16-2090,N13-1138,0,0.345793,"ot be beneficial. Figure 1: Overview of MED 3 Experiments where S(x) and T (x) are source and target tags of x and e(x) is e(σ(x), τ (x)), the edit tree that transforms the source form into the target form. Let ρ be a target form predicted by the MRI system for the source form σ and let s and t be source and target tags. POET does not change ρ if e(σ, ρ) ∈ Es,t . Otherwise it replaces ρ with τ :  0 τ if e(σ, τ 0 ) ∈ Es,t , |ρ, τ 0 |= 1 τ := ρ else We compare MED with the three models of Dreyer et al. (2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al. (2015)’s encoder-decoder model.3 We call the latter MODEL*TAG as it requires training as many models as there are target tags. We evaluate MED on two MRI tasks: CELEX and SIGMORPHON16. CELEX. This task is based on complete inflection tables for German extracted from CELEX. For this experiment we follow Dreyer et al. (2008). We use four pairs of morphological tags and corresponding word forms from the German part of the CELEX morphological database. The 4 different transduction tasks are: 13SIA → 13SKE, 2PIE → 13PKE, 2P"
P16-2090,D13-1105,0,0.0358913,"and irregular verbs (e.g., abhing vs. abh¨angte). For SIGMORPHON16, Table 2 shows that MED outperforms the baseline for all eight languages. Absolute performance and variance is probably influenced by type of morphology (e.g., templatic vs. agglutinative), regularity of the language, number of different tag pairs and other factors. MED performs well even for complex and diverse languages like Arabic, Finnish, Navajo and Turkish, suggesting that the type of attentionbased encoder-decoder we use – single-model, using an explicit morphological representation – is a good choice for MRI. 558 2013; Eskander et al., 2013; Nicolai et al., 2015). Chrupała (2008) defined edit trees and Chrupała (2008) and M¨uller et al. (2015) use them for morphological tagging and lemmatization. In the last years, RNN encoder-decoder models and RNNs in general were applied to several NLP tasks. For example, they proved to be useful for machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) and speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). MED bears some resemblance to Faruqui et al. (2015)’s work. However, they train one network for every t"
P17-1182,E14-1060,0,0.185562,"Missing"
P17-1182,Q16-1031,0,0.0196567,"we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a mo"
P17-1182,P16-1184,0,0.0440907,"slation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised"
P17-1182,W14-4012,0,0.19462,"Missing"
P17-1182,P17-2061,0,0.0549325,"Missing"
P17-1182,D11-1005,0,0.0462193,"sfer with transfer from a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, a"
P17-1182,P15-1166,0,0.0464722,"o, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future res"
P17-1182,N13-1138,0,0.107841,"ical taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2016; Kann and Sch¨utze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves an"
P17-1182,P08-1115,0,0.0686385,"nt of tools for computational morphology with limited annotated data. In many languages, individual lexical entries may be realized as distinct inflections of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In morphologically rich languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these"
P17-1182,N16-1077,0,0.272749,"to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario. To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language. This technique allows 1993 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1993–2003 c Vancouver, Can"
P17-1182,N16-1101,0,0.0242819,"(2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future research for paradigm completion. (iii) Whereas training RNNs in machine translation is hard, we only expe"
P17-1182,Q15-1031,1,0.880055,"Missing"
P17-1182,Q17-1024,0,0.0696793,"Missing"
P17-1182,P16-1156,1,0.900318,"Missing"
P17-1182,D16-1097,1,0.909381,"Missing"
P17-1182,P08-1084,0,0.0879979,"r, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-M"
P17-1182,P16-2090,1,0.823613,"Missing"
P17-1182,P11-2120,0,0.0812648,"rom a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative meth"
P17-1182,L16-1498,0,0.017455,"e transfer and use it as a baseline for all target languages. 5 Accuracy 0.8 Exp. 1: Transfer Learning for Paradigm Completion In this experiment, we investigate to what extent our model transfers morphological knowledge from a high-resource source language to a low-resource target language. We experimentally answer three questions. (i) Is transfer learning possible for morphology? (ii) How much annotated data do we need in the low-resource target language? (iii) How closely related must the two languages be to achieve good results? Data. Based on complete inflection tables from unimorph.org (Kirov et al., 2016), we create datasets as follows. Each training set consists of 12,000 samples in the high-resource source 0.6 0.4 0.2 Languages Pt Ca It Fr Ar Es 0.0 0 50·2 50·21 50·22 50·23 50·24 50·25 50·26 50·27 Number of Samples Experiments We run four experiments on 21 distinct pairings of languages to show the feasibility of morphological transfer and analyze our method. We first discuss details common to all experiments. We keep hyperparameters during all experiments (and for all languages) fixed to the following values. Encoder and decoder RNNs each have 100 hidden units and the size of all subtag, ch"
P17-1182,D14-1095,0,0.0808608,"be realized as distinct inflections of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In morphologically rich languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox applica"
P17-1182,P12-1066,0,0.0488593,"language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been propose"
P17-1182,N15-1093,0,0.123208,"of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2016; Kann and Sch¨utze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Gr"
P17-1182,H05-1108,0,0.117788,"Missing"
P17-1182,petrov-etal-2012-universal,0,0.0908597,"Missing"
P17-1182,Q15-1026,0,0.0607982,"Missing"
P17-1182,D10-1103,0,0.0291944,"tion (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al"
P17-1182,P15-2111,0,0.0361423,"Past Indicative Indicative Sg Pl Sg Pl sue˜no sue˜nas sue˜na so˜namos so˜na´ is sue˜nan so˜ne´ so˜naste so˜no´ so˜namos so˜nasteis so˜naron Table 1: Partial inflection table for the Spanish verb so˜nar. Introduction Low-resource natural language processing (NLP) remains an open problem for many tasks of interest. Furthermore, for most languages in the world, highcost linguistic annotation and resource creation are unlikely to be undertaken in the near future. In the case of morphology, out of the 7000 currently spoken (Lewis, 2009) languages, only about 200 have computer-readable annotations (Sylak-Glassman et al., 2015) – although morphology is easy to annotate compared to syntax and semantics. Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system for a low-resource language. In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN). This allows for the development of tools for computational morphology with limited annotated data. In many languages, individual lexical entries may be realized as distinct inflections of a single lemma depending on the syntacti"
P17-1182,N16-1161,0,0.0339616,"for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the t"
P17-1182,Q14-1005,0,0.342964,"54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the re"
P17-1182,D15-1083,1,0.848421,"Missing"
P17-1182,H01-1035,0,0.102341,"-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual a"
P17-1182,N16-1004,0,0.0198626,"nt state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future research for paradigm comp"
P17-1182,W16-2007,0,\N,Missing
P18-1032,W16-1601,0,0.134404,"Missing"
P18-1032,W17-5221,0,0.289767,"Missing"
P18-1032,D16-1216,0,0.0273919,"this does not invalidate them for comparisons. We also point out that there are characteristics of explanations that may be desirable but are not reflected by the pointing game. Consider Fig 3 (bottom). Both explanations get hit points, but the lrp explanation appears “cleaner” than limssems p , with relevance concentrated on fewer tokens. 6 6.1 Explanation evaluation Related work Explanation methods Explanation methods can be divided into local and global methods (Doshi-Velez and Kim, 2017). Global methods infer general statements about what a DNN has learned, e.g., by clustering documents (Aubakirova and Bansal, 2016) or n-grams (K´ad´ar et al., 2017) according to the neurons that they activate. Li et al. (2016a) compare embeddings of specific words with reference points to measure how drastically they were changed during training. In computer vision, Simonyan et al. (2014) optimize the input space to maximize the activation of a specific neuron. Global explanation methods are of limited value for explaining a specific prediction as they represent average behavior. Therefore, we focus on local methods. Local explanation methods explain a decision taken for one specific input at a time. We have attempted to"
P18-1032,W14-4012,0,0.0334824,"Missing"
P18-1032,D14-1162,0,0.0797095,"Missing"
P18-1032,N16-3020,0,0.488903,"t al., 2017). To calculate the omitN (resp. occN ) relevance of word xt , we delete (resp. occlude), one at a time, all N -grams that contain xt , and average the change in the unnormalized class score from Eq 2:  P φ[omit|occ]N (t, k, X) = N e1 . . . ~e|X |]) j=1 s(k, [~  ¯ et+j . . . ~e|X |]) 1 −s(k, [~e1 . . . ~et−N −1+j ]kEk[~ N where ~et are embedding vectors, k denotes con¯ is either a sequence of length catenation and E zero (φomit ) or a sequence of N baseline (here: all-zero) embedding vectors (φocc ). 3.6 LIMSSE: LIME for NLP Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016) is a framework for explaining predictions of complex classifiers. LIME approximates the behavior of classifier f in the neighborhood of input X with an interpretable (here: linear) model. The interpretable model is trained on samples Z1 . . . ZN (here: N = 3000), which are randomly drawn from X, with “gold labels” f (Z1 ) . . . f (ZN ). Since RNNs and CNNs respect word order, we cannot use the bag of words sampling method from the original description of LIME. Instead, we introduce Local Interpretable Model-agnostic Substring-based Explanations (LIMSSE). LIMSSE uniformly samples a length ln ("
P18-1032,D16-1011,0,0.0596498,"cause an all-zero “placeholder” is less disruptive than word removal. But despite some high scores, it is less consistent than other explanation methods. Magnitude-sensitive LIMSSE (limssems ) consistently outperforms black-box LIMSSE (limssebb ), which suggests that numerical outputs should be used for approximation where possible. In the hybrid document experiment, magnitude-sensitive LIMSSE outperforms the other explanation methods (exceptions: C03, C05). However, it fails in the morphosyntactic agreement experiment (C16-C27). In fact, we expect LIMSSE to be unsuited for large context 346 (Lei et al., 2016)), as these are very specific architectures that may not be not applicable to all tasks. problems, as it cannot discover dependencies whose range is bigger than a given text sample. In Fig 3 (top), limssems p highlights any singular noun without taking into account how that noun fits into the overall syntactic structure. 5.2 6.2 According to Doshi-Velez and Kim (2017)’s taxonomy of explanation evaluation paradigms, application-grounded paradigms test how well an explanation method helps real users solve real tasks (e.g., doctors judge automatic diagnoses); human-grounded paradigms rely on prox"
P18-1032,N16-1082,0,0.450675,"φdecomp (t, k, X) = nl(t, k, X) − nl(t − 1, k, X). For GRU, we change the definition of net load: nl(t, k, X) = w ~k · ( T Y ~zj ) ~ht  j=t+1 where ~z are GRU update gates. gt,d · it,d ct,d + esign(ct,d ) 3.5 This is equivalent to applying Eq 8 while treating ~it as a diagonal weight matrix. The gate neurons Input perturbation methods Input perturbation methods assume that the removal or masking of relevant inputs changes the 343 output (Zeiler and Fergus, 2014). Omissionbased methods remove inputs completely (K´ad´ar et al., 2017), while occlusion-based methods replace them with a baseline (Li et al., 2016b). In computer vision, perturbations are usually applied to patches, as neighboring pixels tend to correlate (Zintgraf et al., 2017). To calculate the omitN (resp. occN ) relevance of word xt , we delete (resp. occlude), one at a time, all N -grams that contain xt , and average the change in the unnormalized class score from Eq 2:  P φ[omit|occ]N (t, k, X) = N e1 . . . ~e|X |]) j=1 s(k, [~  ¯ et+j . . . ~e|X |]) 1 −s(k, [~e1 . . . ~et−N −1+j ]kEk[~ N where ~et are embedding vectors, k denotes con¯ is either a sequence of length catenation and E zero (φomit ) or a sequence of N baseline (her"
P18-1032,Q16-1037,0,0.0484033,"Missing"
P18-1075,2005.mtsummit-papers.11,0,0.0291577,"gative examples are randomly generated for each positive one in the training lexicon. We used default parameters as reported by Heyman et al. (2017) except for the t classification thresholds (used at prediction time). We finetuned these on dev. We note that the system works with pre-trained MWEs as well (and report these as official baseline results) but it requires BWEs for candidate generation at prediction time, thus we use BWEs for the system’s input for all experiments. In preliminary work, we had found that MWE and BWE results are similar. the English and Dutch data from Europarl (v7) (Koehn, 2005), a corpus of 2 million sentence pairs. Although Europarl is a parallel corpus, we use it in a monolingual way and shuffle each side of the corpus before training. By using massive cheap data we create high-quality MWEs in each language which are still domain-specific (due to inclusion of medical data). To obtain an out-ofdomain seed lexicon, we translated the English words in BNC to Dutch using Google Translate (just as we did before for the Twitter CLSC task). We then use the out-of-domain BNC and the indomain medical seed lexicons in separate experiments to create BWEs with post-hoc mapping"
P18-1075,C12-2008,0,0.0275641,"dependent (in that it only requires unlabeled in-domain text), which is an important strength. 2.2 Cross-Lingual Sentiment Analysis Sentiment analysis is widely applied, and thus ideally we would have access to high quality supervised models in all human languages. Unfortunately, good quality labeled datasets are missing for many languages. Training models on resource rich languages and applying them to resource poor languages is therefore highly desirable. Crosslingual sentiment classification (CLSC) tackles this problem (Mihalcea et al., 2007; Banea et al., 2010; Wan, 2009; Lu et al., 2011; Balamurali and Joshi, 2012; Gui et al., 2013). Recent CLSC approaches use BWEs as features of deep learning architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using sourcelanguage supervised training data. Following this approach we perform CLSC on Spanish tweets using English training data. Even though Spanish is not resource-poor we simulate this by using only English annotated data. 2.3 Bilingual Lexicon Induction (BLI) BLI is an important task that has been addressed by a large amount of previous work. The goal of BLI is to automatically e"
P18-1075,C10-1004,0,0.0183517,"le approach is, in contrast, effectively task independent (in that it only requires unlabeled in-domain text), which is an important strength. 2.2 Cross-Lingual Sentiment Analysis Sentiment analysis is widely applied, and thus ideally we would have access to high quality supervised models in all human languages. Unfortunately, good quality labeled datasets are missing for many languages. Training models on resource rich languages and applying them to resource poor languages is therefore highly desirable. Crosslingual sentiment classification (CLSC) tackles this problem (Mihalcea et al., 2007; Banea et al., 2010; Wan, 2009; Lu et al., 2011; Balamurali and Joshi, 2012; Gui et al., 2013). Recent CLSC approaches use BWEs as features of deep learning architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using sourcelanguage supervised training data. Following this approach we perform CLSC on Spanish tweets using English training data. Even though Spanish is not resource-poor we simulate this by using only English annotated data. 2.3 Bilingual Lexicon Induction (BLI) BLI is an important task that has been addressed by a large amount"
P18-1075,P15-1027,0,0.435942,"l target-language in-domain unlabeled text. Both of the approaches we study in this work fit this criterion, the delightfully simple method for adapting BWEs can improve the performance of any off-the-shelf classifier that is based on BWEs, while the broadly applicable semi-supervised approach of H¨ausser et al. (2017) can improve the performance of any off-the-shelf classifier. Bilingual Word Embeddings Many approaches have been proposed for creating high quality BWEs using different bilingual signals. Following Mikolov et al. (2013b), many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vuli´c and Moens, 2015; Duong et al., 2016) to train BWEs. In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et al. (2016) have previously studied domain ad"
P18-1075,L16-1147,0,0.0226462,"guages and then map those into the same space using post-hoc mapping (Mikolov et al., 2013b). We train MWEs for both languages by concatenating monolingual out-of-domain and in-domain data. The out-of-domain data allows us to create accurate distributed representations of common vocabulary while the in-domain data embeds domain specific words. We then map the two MWEs using a small seed lexicon to create the adapted BWEs. Because post-hoc mapping only requires a seed lexicon as bilingual signal it can Training Data for Twitter Specific BWEs As comparable non-twitter data we use OpenSubtitles (Lison and Tiedemann, 2016) which contains 49.2M English and Spanish subtitle sentences respectively (Subtitle). The reason behind choosing Subtitles is that although it is out-of-domain it contains slang words similar to tweets thus serving as a strong baseline in our setup. We experiment with two monolingual twitter data sets: (i) 22M tweets: Downloaded2 English (17.2M) and Spanish (4.8M) tweets using the public 1 2 812 https://github.com/dav/word2vec We downloaded for a month starting on 2016-10-15. Twitter Streaming API 3 with language filters en and es BWEs and feeds them to a bi-directional gated neural network. P"
P18-1075,P11-1033,0,0.0232914,"fectively task independent (in that it only requires unlabeled in-domain text), which is an important strength. 2.2 Cross-Lingual Sentiment Analysis Sentiment analysis is widely applied, and thus ideally we would have access to high quality supervised models in all human languages. Unfortunately, good quality labeled datasets are missing for many languages. Training models on resource rich languages and applying them to resource poor languages is therefore highly desirable. Crosslingual sentiment classification (CLSC) tackles this problem (Mihalcea et al., 2007; Banea et al., 2010; Wan, 2009; Lu et al., 2011; Balamurali and Joshi, 2012; Gui et al., 2013). Recent CLSC approaches use BWEs as features of deep learning architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using sourcelanguage supervised training data. Following this approach we perform CLSC on Spanish tweets using English training data. Even though Spanish is not resource-poor we simulate this by using only English annotated data. 2.3 Bilingual Lexicon Induction (BLI) BLI is an important task that has been addressed by a large amount of previous work. The goal"
P18-1075,D16-1136,0,0.0187249,"l. (2017) can improve the performance of any off-the-shelf classifier. Bilingual Word Embeddings Many approaches have been proposed for creating high quality BWEs using different bilingual signals. Following Mikolov et al. (2013b), many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vuli´c and Moens, 2015; Duong et al., 2016) to train BWEs. In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et al. (2016) have previously studied domain adaptation of bilingual word embeddings, showing it to be highly effective for improving downstream tasks. However, importantly, their proposed methods are based on specialized domain lexicons (such as, e.g., sentiment lexicons) which contain task specific word relations. Our delightfully simple approach is,"
P18-1075,W17-2617,0,0.0412645,"Missing"
P18-1075,N15-1184,0,0.034825,"ikolov et al. (2013b), many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vuli´c and Moens, 2015; Duong et al., 2016) to train BWEs. In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et al. (2016) have previously studied domain adaptation of bilingual word embeddings, showing it to be highly effective for improving downstream tasks. However, importantly, their proposed methods are based on specialized domain lexicons (such as, e.g., sentiment lexicons) which contain task specific word relations. Our delightfully simple approach is, in contrast, effectively task independent (in that it only requires unlabeled in-domain text), which is an important strength. 2.2 Cross-Lingual Sentiment Analysis Sentiment analysis is widely applied, and thus"
P18-1075,P07-1123,0,0.0650837,". Our delightfully simple approach is, in contrast, effectively task independent (in that it only requires unlabeled in-domain text), which is an important strength. 2.2 Cross-Lingual Sentiment Analysis Sentiment analysis is widely applied, and thus ideally we would have access to high quality supervised models in all human languages. Unfortunately, good quality labeled datasets are missing for many languages. Training models on resource rich languages and applying them to resource poor languages is therefore highly desirable. Crosslingual sentiment classification (CLSC) tackles this problem (Mihalcea et al., 2007; Banea et al., 2010; Wan, 2009; Lu et al., 2011; Balamurali and Joshi, 2012; Gui et al., 2013). Recent CLSC approaches use BWEs as features of deep learning architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using sourcelanguage supervised training data. Following this approach we perform CLSC on Spanish tweets using English training data. Even though Spanish is not resource-poor we simulate this by using only English annotated data. 2.3 Bilingual Lexicon Induction (BLI) BLI is an important task that has been address"
P18-1075,E14-1049,0,0.030169,"ge sources beyond having access to plentiful target-language in-domain unlabeled text. Both of the approaches we study in this work fit this criterion, the delightfully simple method for adapting BWEs can improve the performance of any off-the-shelf classifier that is based on BWEs, while the broadly applicable semi-supervised approach of H¨ausser et al. (2017) can improve the performance of any off-the-shelf classifier. Bilingual Word Embeddings Many approaches have been proposed for creating high quality BWEs using different bilingual signals. Following Mikolov et al. (2013b), many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vuli´c and Moens, 2015; Duong et al., 2016) to train BWEs. In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et a"
P18-1075,N15-1157,0,0.0227138,"many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vuli´c and Moens, 2015; Duong et al., 2016) to train BWEs. In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et al. (2016) have previously studied domain adaptation of bilingual word embeddings, showing it to be highly effective for improving downstream tasks. However, importantly, their proposed methods are based on specialized domain lexicons (such as, e.g., sentiment lexicons) which contain task specific word relations. Our delightfully simple approach is, in contrast, effectively task independent (in that it only requires unlabeled in-domain text), which is an important strength. 2.2 Cross-Lingual Sentiment Analysis Sentiment analysis is widely applied, and thus ideally we would have acce"
P18-1075,N16-1091,1,0.92208,"Missing"
P18-1075,P14-1006,0,0.0266157,"BWEs can improve the performance of any off-the-shelf classifier that is based on BWEs, while the broadly applicable semi-supervised approach of H¨ausser et al. (2017) can improve the performance of any off-the-shelf classifier. Bilingual Word Embeddings Many approaches have been proposed for creating high quality BWEs using different bilingual signals. Following Mikolov et al. (2013b), many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vuli´c and Moens, 2015; Duong et al., 2016) to train BWEs. In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et al. (2016) have previously studied domain adaptation of bilingual word embeddings, showing it to be highly effective for improving downstream tasks. However, importantly, their proposed methods are ba"
P18-1075,E17-1102,0,0.0391898,"Missing"
P18-1075,P16-1024,0,0.053412,"Missing"
P18-1075,P15-2118,0,0.0630264,"Missing"
P18-1075,D14-1181,0,0.0116884,"Missing"
P18-1075,P09-1027,0,0.0514212,"ontrast, effectively task independent (in that it only requires unlabeled in-domain text), which is an important strength. 2.2 Cross-Lingual Sentiment Analysis Sentiment analysis is widely applied, and thus ideally we would have access to high quality supervised models in all human languages. Unfortunately, good quality labeled datasets are missing for many languages. Training models on resource rich languages and applying them to resource poor languages is therefore highly desirable. Crosslingual sentiment classification (CLSC) tackles this problem (Mihalcea et al., 2007; Banea et al., 2010; Wan, 2009; Lu et al., 2011; Balamurali and Joshi, 2012; Gui et al., 2013). Recent CLSC approaches use BWEs as features of deep learning architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using sourcelanguage supervised training data. Following this approach we perform CLSC on Spanish tweets using English training data. Even though Spanish is not resource-poor we simulate this by using only English annotated data. 2.3 Bilingual Lexicon Induction (BLI) BLI is an important task that has been addressed by a large amount of previou"
P18-1075,D13-1153,0,0.0180411,"t language, an impressive result considering that we require no target-language annotated data. The method also yields impressive improvements for bilingual lexicon induction compared with baselines trained on in-domain data. We show that this system requires the high-quality domain-adapted bilingual word embeddings we previously created to use unlabeled data well. 810 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 810–820 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 2.1 Previous Work Xiao and Guo (2013) proposed a cross-lingual log-bilinear document model to learn distributed representations of words, which can capture both the semantic similarities of words across languages and the predictive information with respect to the classification task. Similarly, Tang and Wan (2014) jointly embedded texts in different languages into a joint semantic space representing sentiment. Zhou et al. (2014) employed aligned sentences in the BWE learning process, but in the sentiment classification process only representations in the source language are used for training, and representations in the target lan"
P18-1075,N15-1104,0,0.0432713,"access to plentiful target-language in-domain unlabeled text. Both of the approaches we study in this work fit this criterion, the delightfully simple method for adapting BWEs can improve the performance of any off-the-shelf classifier that is based on BWEs, while the broadly applicable semi-supervised approach of H¨ausser et al. (2017) can improve the performance of any off-the-shelf classifier. Bilingual Word Embeddings Many approaches have been proposed for creating high quality BWEs using different bilingual signals. Following Mikolov et al. (2013b), many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vuli´c and Moens, 2015; Duong et al., 2016) to train BWEs. In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et al. (2016) have prev"
P18-1075,P15-1042,0,0.0252714,"classification task. Similarly, Tang and Wan (2014) jointly embedded texts in different languages into a joint semantic space representing sentiment. Zhou et al. (2014) employed aligned sentences in the BWE learning process, but in the sentiment classification process only representations in the source language are used for training, and representations in the target language are used for predicting labels. An important weakness of these three works was that aligned sentences were required. Some work has trained sentiment-specific BWEs using annotated sentiment information in both languages (Zhou et al., 2015, 2016), which is desirable, but this is not applicable to our scenario. Our goal is to adapt BWEs to a specific domain without requiring additional task-specific engineering or knowledge sources beyond having access to plentiful target-language in-domain unlabeled text. Both of the approaches we study in this work fit this criterion, the delightfully simple method for adapting BWEs can improve the performance of any off-the-shelf classifier that is based on BWEs, while the broadly applicable semi-supervised approach of H¨ausser et al. (2017) can improve the performance of any off-the-shelf cl"
P18-1075,P16-1133,0,0.0177361,"nt training data instead of English. BWEs trained on Subtitle using posthoc mapping. The difference between the two is that the embeddings of (ii) are enriched with English words which can be beneficial for the classification of Spanish tweets because they often contain a few English words. We do not compare with word embedding adaptation methods relying on specialized resources. The point of our work is to study task-independent methods and to the best of our knowledge ours is the first such attempt. Similarly, we do not compare against machine translation based sentiment classifiers (e.g., (Zhou et al., 2016)) because for their adaptation in-domain parallel data would be needed. Table 1 gives results for both classifiers. It shows that the adaptation of Subtitle based BWEs with data from Twitter (22M tweets and BACKGROUND) clearly outperforms the Baseline in all cases. The target-aware system performed poorly with the baseline BWEs and could benefit significantly from the adaptation approach. The target-ignorant performed better with the baseline BWEs but could also benefit from the adaptation. Comparing results with the Twitter-dataset-only based BWEs, the 22M tweets performed better even though"
P18-1141,Q16-1022,0,0.0496476,"Missing"
P18-1141,P14-1006,0,0.122748,"ual embeddings, shared representation of words across languages (Klementiev et al., 2012). Such embeddings can be beneficial in machine translation in sparse data settings because multilingual embeddings provide meaning representations of source and target in the same space. Similarly, in transfer learning, models trained in one language on multilingual embeddings can be deployed in other languages (Zeman and Resnik, 2008; McDonald et al., 2011; Tsvetkov et al., 2014). Automatically learned embeddings have the added advantage of requiring fewer resources for training (Klementiev et al., 2012; Hermann and Blunsom, 2014b; Guo et al., 2016). Thus, massively multilingual word embeddings (i.e., covering 100s or 1000s of languages) are likely to be important in NLP. The basic information many embedding learners use is word-context information; e.g., the embedding of a word is optimized to predict a representation of its context. We instead learn embeddings from word-concept information. As a first approximation, a concept is a set of semantically similar words. Figure 1 shows an example concept and also indicates one way we learn concepts: we interpret cliques in the dictionary graph as concepts. The nodes of th"
P18-1141,E17-1102,0,0.0392696,"Missing"
P18-1141,D17-1011,1,0.856467,"Missing"
P18-1141,C12-1089,0,0.143169,"highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches. 1 Figure 1: Example of a CLIQUE concept: “water” Introduction Vector space representations of words are widely used because they improve performance on monolingual tasks. This success has generated interest in multilingual embeddings, shared representation of words across languages (Klementiev et al., 2012). Such embeddings can be beneficial in machine translation in sparse data settings because multilingual embeddings provide meaning representations of source and target in the same space. Similarly, in transfer learning, models trained in one language on multilingual embeddings can be deployed in other languages (Zeman and Resnik, 2008; McDonald et al., 2011; Tsvetkov et al., 2014). Automatically learned embeddings have the added advantage of requiring fewer resources for training (Klementiev et al., 2012; Hermann and Blunsom, 2014b; Guo et al., 2016). Thus, massively multilingual word embeddin"
P18-1141,E17-1084,0,0.0466535,"Missing"
P18-1141,N13-1073,0,0.327681,"ngo Creole; Central Africa 1895 Algorithm 1 χ2 -based dictionary induction 268 315 242 191 177 208 231 226 194 192 Table 2: Our ten pivot languages, the languages in PBC with the lowest number of types. Tokens in 1000s. Tok Pisin and Bislama are English-based and Sango is a Ngbandi-based creole. PNG = Papua New Guinea UTF-8 has properties different from Chinese UTF8. Thus, universal language processing is easier to design on the byte level. We refer to this ngram representation as CHAR and to standard tokenization as WORD. 2.3 Dictionary induction Alignment-based dictionary. We use fastalign (Dyer et al., 2013) to compute word alignments and use GDFA for symmetrization. All alignment edges that occurred at least twice are added to the dictionary graph. Initial experiments indicated that alignment-based dictionaries have poor quality for CHAR, probably due to the fact that overlapping ngram representations of sentences have properties quite different from the tokenized sentences that aligners are optimized for. Thus we use this dictionary induction method only for WORD and developed the following alternative for CHAR. Correlation-based dictionary (χ2 ). χ2 is a greedy algorithm, shown in Figure 2, th"
P18-1141,E14-1049,0,0.0743377,"Missing"
P18-1141,N16-1155,0,0.0416912,"pes based on this selection because most editions do not contain a few of the selected 6458 verses. 2.2 Character-level modeling (CHAR) We will see that tokenization-based models have poor performance on a subset of the 1259 languages. To overcome tokenization problems, we represent a verse of length m bytes, as a sequence of m − (n − 1) + 2 overlapping byte n-grams. In this paper, “n-gram” always refers to “byte ngram”. We pad the verse with initial and final space, resulting in two additional n-grams (hence “+2”). This representation is in the spirit of earlier byte-level processing, e.g., (Gillick et al., 2016). There are several motivations for this. (i) We can take advantage of byte-level generalizations. (ii) This is robust if there is noise in the byte encoding. (iii) Characters have different properties in different languages and encodings, e.g., English 1521 tokens types iso name family; (example) region lhu Lahu Sino-Tibetan; Thailand 1452 ahk Akha Sino-Tibetan; China 1550 hak Hakka Chinese Chinese; China 1596 ium Iu Mien Hmong-Mien; Laos 1779 tpi Tok Pisin Creole; PNG 1815 mio Pinotepa Mixtec Oto-Manguean; Oaxaca 1828 cya Highland Chatino Oto-Manguean; Oaxaca 1868 bis Bislama Creole; Vanuatu"
P18-1141,2009.mtsummit-posters.11,1,0.830059,"Missing"
P18-1141,R09-1040,0,0.13124,"vot languages and one target language as a bag of words (BOW) and consider this bag as a context.3 Levy et al. (2017) show that sentence ID features (interpretable as an abstract representation of the word’s context) are effective. We use a corpus with lines consisting of pairs of an identifier of a 2 We use code.google.com/archive/p/word2vec The actual implementation slightly differs to avoid very long lines. It does only consider two pivot languages at a time, but writes each verse multiple times. 3 verse and a unit extracted from that verse as input to word2vec and call this baseline S-ID. Lardilleux and Lepage (2009) propose a simple and efficient baseline: sample-based concept induction. Words that strictly occur in the same verses are assigned to the same concept. To increase coverage, they propose to sample many different subcorpora.4 We induce concepts using this method and project them analogous to CLIQUE. We call this baseline SAMPLE. One novel contribution of this paper is roundtrip evaluation of embeddings. We learn embeddings based on a dictionary. The question arises: are the embeddings simply reproducing the information already in the dictionary or are they improving the performance of roundtri"
P18-1141,E17-1072,0,0.33965,"dding learning algorithms that define contexts and then sample pairs of an input word (more generally, an input unit) and a context word (more generally, a context unit) from each context. The only difference is that our contexts are concepts. For simplicity, we use word2vec (Mikolov et al., 2013a) as the implementation of this model.2 2.6 Baselines Baselines for multilingual embedding learning. One baseline is inspired by (Vuli´c and Moens, 2015). We consider words of one aligned verse in the pivot languages and one target language as a bag of words (BOW) and consider this bag as a context.3 Levy et al. (2017) show that sentence ID features (interpretable as an abstract representation of the word’s context) are effective. We use a corpus with lines consisting of pairs of an identifier of a 2 We use code.google.com/archive/p/word2vec The actual implementation slightly differs to avoid very long lines. It does only consider two pivot languages at a time, but writes each verse multiple times. 3 verse and a unit extracted from that verse as input to word2vec and call this baseline S-ID. Lardilleux and Lepage (2009) propose a simple and efficient baseline: sample-based concept induction. Words that stri"
P18-1141,W15-1521,0,0.0268834,"network approaches include (Hermann and Blunsom, 2014a) (BiCVM) and (Sarath Chandar et al., 2014) (autoencoders). Again, we have not enough data for training neural networks of this size. Søgaard et al. (2015) learn an interlingual space by using Wikipedia articles as concepts and applying inverted indexing. Levy et al. (2017) show that what we call S-ID is a strongly performing embedding learning method. We use S-ID as a baseline. Group C combines mono- and multilingual information in the embedding learning objective. Klementiev et al. (2012) add a word-alignment based term in the objective. Luong et al. (2015) extend Mikolov et al. (2013a)’s skipgram model to a bilingual model. Gouws et al. (2015) introduce a crosslingual term in the objective, which does not rely on any word-pair or alignment information. For n editions, including O(n2 ) bilingual terms in the objective function does not scale. Group D creates pseudocorpora by merging data from multiple languages into a single corpus. One such method, due to Vuli´c and Moens (2015), is our baseline BOW. ¨ Ostling (2014) generates multilingual concepts using a Chinese Restaurant process, a computationally expensive method. Wang et al. (2016) base t"
P18-1141,D17-1268,0,0.0243288,"ates pseudocorpora by merging data from multiple languages into a single corpus. One such method, due to Vuli´c and Moens (2015), is our baseline BOW. ¨ Ostling (2014) generates multilingual concepts using a Chinese Restaurant process, a computationally expensive method. Wang et al. (2016) base their concepts on cliques. We extend their notion of clique from the bilingual to the multilingual case. Ammar et al. (2016) use connected components. Our baseline SAMPLE, based on (Lardilleux and Lepage, 2007, 2009), samples aligned sentences from a multilingual corpus and extracts perfect alignments. Malaviya et al. (2017), Asgari and Sch¨utze ¨ (2017), Ostling and Tiedemann (2017) and Tiedemann (2018) perform evaluation on the language level (e.g., typology prediction) for 1000+ languages or perform experiments on 1000+ languages without evaluating each language. We present the first work that evaluates on 1000+ languages on the sentence level on a difficult task. Somers (2005) criticizes RT evaluation on the sentence level; but see Aiken and Park (2010). We demonstrated that when used on the word/unit level, it distinguishes weak from strong embeddings and correlates well with an independent sentiment evaluat"
P18-1141,mayer-cysouw-2014-creating,0,0.20655,"query q ∈ Lq . Retrieve the target unit t ∈ Lt that is closest to p. Retrieve the pivot word p0 ∈ Lp that is closest to t. Retrieve the unit q 0 ∈ Lq that is closest to p0 . If q = q 0 , this is an exact hit. We run this experiment for all pivot and target languages. Note that roundtrip evaluation tests the capability of a system to go from any language to any other language. In an embedding space, this requires two hops. In a highly multilingual dataset of n languages in which not all O(n2 ) bilingual dictionaries exist, this requires four hops. 3 Experiments and results 3.1 Data We use PBC (Mayer and Cysouw, 2014). The version we pulled on 2017-12-11 contains 1664 Bible editions in 1259 languages (based on ISO 639-3 codes) after we discarded editions that have low coverage of the New Testament. We use 7958 verses that have good coverage in these 1664 editions. The data is verse aligned; a verse of the New Testament can consist of multiple sentences. We randomly split verses 6458/1500 into train/test. 3.2 Evaluation For sentiment analysis, we represent a verse as the IDF-weighted sum of its embeddings. Sentiment classifiers (linear SVMs) are trained on the training set of the World English Bible edition"
P18-1141,D11-1006,0,0.0371145,"r” Introduction Vector space representations of words are widely used because they improve performance on monolingual tasks. This success has generated interest in multilingual embeddings, shared representation of words across languages (Klementiev et al., 2012). Such embeddings can be beneficial in machine translation in sparse data settings because multilingual embeddings provide meaning representations of source and target in the same space. Similarly, in transfer learning, models trained in one language on multilingual embeddings can be deployed in other languages (Zeman and Resnik, 2008; McDonald et al., 2011; Tsvetkov et al., 2014). Automatically learned embeddings have the added advantage of requiring fewer resources for training (Klementiev et al., 2012; Hermann and Blunsom, 2014b; Guo et al., 2016). Thus, massively multilingual word embeddings (i.e., covering 100s or 1000s of languages) are likely to be important in NLP. The basic information many embedding learners use is word-context information; e.g., the embedding of a word is optimized to predict a representation of its context. We instead learn embeddings from word-concept information. As a first approximation, a concept is a set of sema"
P18-1141,P97-1063,0,0.402767,"anguages on the sentence level on a difficult task. Somers (2005) criticizes RT evaluation on the sentence level; but see Aiken and Park (2010). We demonstrated that when used on the word/unit level, it distinguishes weak from strong embeddings and correlates well with an independent sentiment evaluation. Any alignment algorithm can be used for dictionary induction. We only used a member of the IBM class of models (Dyer et al., 2013), but presumably we could improve results by using either higher performing albeit slower aligners or non-IBM aligners (e.g., (Och and Ney, 2003; Tiedemann, 2003; Melamed, 1997)). Other alignment algorithms include 2D linking (Kobdani et al., 2009), sampling based methods (e.g., Vulic ¨ and Moens (2012)) and EFMARAL (Ostling and Tiedemann, 2016). EFMARAL is especially intriguing as it is based on IBM1 and Agi´c et al. (2016) find IBM2-based models to favor closely related languages more than models based on IBM1. However, the challenge is that we need to compute tens of thousands of alignments, so speed is of the essence. We ran character-based and word-based induction separately; combining them is promising future research; cf. (Heyman et al., 2017). There is much w"
P18-1141,J03-1002,0,0.0101017,"first work that evaluates on 1000+ languages on the sentence level on a difficult task. Somers (2005) criticizes RT evaluation on the sentence level; but see Aiken and Park (2010). We demonstrated that when used on the word/unit level, it distinguishes weak from strong embeddings and correlates well with an independent sentiment evaluation. Any alignment algorithm can be used for dictionary induction. We only used a member of the IBM class of models (Dyer et al., 2013), but presumably we could improve results by using either higher performing albeit slower aligners or non-IBM aligners (e.g., (Och and Ney, 2003; Tiedemann, 2003; Melamed, 1997)). Other alignment algorithms include 2D linking (Kobdani et al., 2009), sampling based methods (e.g., Vulic ¨ and Moens (2012)) and EFMARAL (Ostling and Tiedemann, 2016). EFMARAL is especially intriguing as it is based on IBM1 and Agi´c et al. (2016) find IBM2-based models to favor closely related languages more than models based on IBM1. However, the challenge is that we need to compute tens of thousands of alignments, so speed is of the essence. We ran character-based and word-based induction separately; combining them is promising future research; cf. (Heym"
P18-1141,E14-4024,0,0.242936,"information in the embedding learning objective. Klementiev et al. (2012) add a word-alignment based term in the objective. Luong et al. (2015) extend Mikolov et al. (2013a)’s skipgram model to a bilingual model. Gouws et al. (2015) introduce a crosslingual term in the objective, which does not rely on any word-pair or alignment information. For n editions, including O(n2 ) bilingual terms in the objective function does not scale. Group D creates pseudocorpora by merging data from multiple languages into a single corpus. One such method, due to Vuli´c and Moens (2015), is our baseline BOW. ¨ Ostling (2014) generates multilingual concepts using a Chinese Restaurant process, a computationally expensive method. Wang et al. (2016) base their concepts on cliques. We extend their notion of clique from the bilingual to the multilingual case. Ammar et al. (2016) use connected components. Our baseline SAMPLE, based on (Lardilleux and Lepage, 2007, 2009), samples aligned sentences from a multilingual corpus and extracts perfect alignments. Malaviya et al. (2017), Asgari and Sch¨utze ¨ (2017), Ostling and Tiedemann (2017) and Tiedemann (2018) perform evaluation on the language level (e.g., typology predic"
P18-1141,E17-2102,0,0.0220708,"ages into a single corpus. One such method, due to Vuli´c and Moens (2015), is our baseline BOW. ¨ Ostling (2014) generates multilingual concepts using a Chinese Restaurant process, a computationally expensive method. Wang et al. (2016) base their concepts on cliques. We extend their notion of clique from the bilingual to the multilingual case. Ammar et al. (2016) use connected components. Our baseline SAMPLE, based on (Lardilleux and Lepage, 2007, 2009), samples aligned sentences from a multilingual corpus and extracts perfect alignments. Malaviya et al. (2017), Asgari and Sch¨utze ¨ (2017), Ostling and Tiedemann (2017) and Tiedemann (2018) perform evaluation on the language level (e.g., typology prediction) for 1000+ languages or perform experiments on 1000+ languages without evaluating each language. We present the first work that evaluates on 1000+ languages on the sentence level on a difficult task. Somers (2005) criticizes RT evaluation on the sentence level; but see Aiken and Park (2010). We demonstrated that when used on the word/unit level, it distinguishes weak from strong embeddings and correlates well with an independent sentiment evaluation. Any alignment algorithm can be used for dictionary indu"
P18-1141,W99-0602,0,0.113614,"pts that cover around 8k of the total vocabulary of 24k English words (WORD). As an alternative to cliques, Ammar et al. (2016) use connected components (CCs). The reachability relation (induced by CC) is the transitive closure of the edge relation. This results in semantically unrelated words being in the same concept for very low levels of noise. In contrast, cliques are more “strict”: only node subsets are considered whose corresponding edge relation is already transitive (or almost so for ν = 0.6). Transitivity across languages often does not hold in alignments or dictionaries; see, e.g., Simard (1999). This is why we only consider cliques (which reflect already existent transitivity) rather than CCs, which impose transitivity where it does not hold naturally. 2.4.2 N (t) (target neighborhood) concept induction Let N (t) be the neighborhood of target node t in the multipartite dictionary graph, i.e., the set of pivot words that are linked to t. We refer to N (t) as target neighborhood. Figure 4 shows an example of such a target neighborhood, the set N (t) consisting of four words.1 A target neighborhood concept consists of a set T of pivot words and all target words t for which T = N (t) ho"
P18-1141,P15-1165,0,0.0424929,"Missing"
P18-1141,U05-1019,0,0.0189545,"al to the multilingual case. Ammar et al. (2016) use connected components. Our baseline SAMPLE, based on (Lardilleux and Lepage, 2007, 2009), samples aligned sentences from a multilingual corpus and extracts perfect alignments. Malaviya et al. (2017), Asgari and Sch¨utze ¨ (2017), Ostling and Tiedemann (2017) and Tiedemann (2018) perform evaluation on the language level (e.g., typology prediction) for 1000+ languages or perform experiments on 1000+ languages without evaluating each language. We present the first work that evaluates on 1000+ languages on the sentence level on a difficult task. Somers (2005) criticizes RT evaluation on the sentence level; but see Aiken and Park (2010). We demonstrated that when used on the word/unit level, it distinguishes weak from strong embeddings and correlates well with an independent sentiment evaluation. Any alignment algorithm can be used for dictionary induction. We only used a member of the IBM class of models (Dyer et al., 2013), but presumably we could improve results by using either higher performing albeit slower aligners or non-IBM aligners (e.g., (Och and Ney, 2003; Tiedemann, 2003; Melamed, 1997)). Other alignment algorithms include 2D linking (K"
P18-1141,E03-1026,0,0.0958877,"luates on 1000+ languages on the sentence level on a difficult task. Somers (2005) criticizes RT evaluation on the sentence level; but see Aiken and Park (2010). We demonstrated that when used on the word/unit level, it distinguishes weak from strong embeddings and correlates well with an independent sentiment evaluation. Any alignment algorithm can be used for dictionary induction. We only used a member of the IBM class of models (Dyer et al., 2013), but presumably we could improve results by using either higher performing albeit slower aligners or non-IBM aligners (e.g., (Och and Ney, 2003; Tiedemann, 2003; Melamed, 1997)). Other alignment algorithms include 2D linking (Kobdani et al., 2009), sampling based methods (e.g., Vulic ¨ and Moens (2012)) and EFMARAL (Ostling and Tiedemann, 2016). EFMARAL is especially intriguing as it is based on IBM1 and Agi´c et al. (2016) find IBM2-based models to favor closely related languages more than models based on IBM1. However, the challenge is that we need to compute tens of thousands of alignments, so speed is of the essence. We ran character-based and word-based induction separately; combining them is promising future research; cf. (Heyman et al., 2017)."
P18-1141,P14-1024,0,0.0230133,"space representations of words are widely used because they improve performance on monolingual tasks. This success has generated interest in multilingual embeddings, shared representation of words across languages (Klementiev et al., 2012). Such embeddings can be beneficial in machine translation in sparse data settings because multilingual embeddings provide meaning representations of source and target in the same space. Similarly, in transfer learning, models trained in one language on multilingual embeddings can be deployed in other languages (Zeman and Resnik, 2008; McDonald et al., 2011; Tsvetkov et al., 2014). Automatically learned embeddings have the added advantage of requiring fewer resources for training (Klementiev et al., 2012; Hermann and Blunsom, 2014b; Guo et al., 2016). Thus, massively multilingual word embeddings (i.e., covering 100s or 1000s of languages) are likely to be important in NLP. The basic information many embedding learners use is word-context information; e.g., the embedding of a word is optimized to predict a representation of its context. We instead learn embeddings from word-concept information. As a first approximation, a concept is a set of semantically similar words."
P18-1141,P16-1157,0,0.0339305,"Missing"
P18-1141,E12-1046,0,0.0583073,"Missing"
P18-1141,C12-1166,0,0.0573241,"Missing"
P18-1141,P15-2118,0,0.0468425,"Missing"
P18-1141,W14-1613,0,0.0551922,"Missing"
P18-1141,I08-3008,0,0.032048,"a CLIQUE concept: “water” Introduction Vector space representations of words are widely used because they improve performance on monolingual tasks. This success has generated interest in multilingual embeddings, shared representation of words across languages (Klementiev et al., 2012). Such embeddings can be beneficial in machine translation in sparse data settings because multilingual embeddings provide meaning representations of source and target in the same space. Similarly, in transfer learning, models trained in one language on multilingual embeddings can be deployed in other languages (Zeman and Resnik, 2008; McDonald et al., 2011; Tsvetkov et al., 2014). Automatically learned embeddings have the added advantage of requiring fewer resources for training (Klementiev et al., 2012; Hermann and Blunsom, 2014b; Guo et al., 2016). Thus, massively multilingual word embeddings (i.e., covering 100s or 1000s of languages) are likely to be important in NLP. The basic information many embedding learners use is word-context information; e.g., the embedding of a word is optimized to predict a representation of its context. We instead learn embeddings from word-concept information. As a first approximation, a c"
P18-1141,D13-1141,0,0.0380356,"Missing"
P18-2086,D15-1075,0,0.0577839,"al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 540–545 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics premise Representation for pair (P, H) rpos entail less likely more likely row-wise max-pooling mi hypothesis 1 hypothesis 2 attentive convolution Figure 1: The motivation of considering alignment positions in TE. The same color in"
P18-2086,D16-1244,0,0.159967,"Missing"
P18-2086,W05-1209,0,0.0304846,"turned into better QA performance (Khot et al., 2018). Khot et al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 540–545 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics premise Representation for pair (P, H) rpos entail less likely more likely row-wise max-pooling mi hypothesis 1 hypothesis 2 attentive convolution Figure 1: The motivation"
P18-2086,N18-1202,0,0.0469423,"etween A and B. This challenge is expected to be handled by rules. Ambiguity: The pair #3 looks like having a similar challenge with the pair #2. We guess the annotators treat “· · · a vertebral column or backbone” and “ · · · the backbone (or vertebral column)” as the same convention, which may be debatable. Complex discourse relation: The premise in Table 3: D E I S T E vs. baselines on SNLI. D E I S T E S CI TAIL has exactly the same system layout and hyperparameters as the one reported on S CI TAIL in Table 2; D E I S T Etune : tune hyperparameters on SNLI dev. State-of-the-art refers to (Peters et al., 2018). Ensemble results are not considered. 3 Experiments Dataset. S CI TAIL3 (Khot et al., 2018) is for textual entailment in binary classification: entailment or neutral. Accuracy is reported. Training setup. All words are initialized by 300D Word2Vec (Mikolov et al., 2013) embeddings, and are fine-tuned during training. The whole system is trained by AdaGrad (Duchi et al., 2011). Other hyperparameter values include: learning rate 0.01, dm =50 for position embeddings M, hidden size 300, batch size 50, filter width 3. Baselines. (i) Decomposable Attention Model (Decomp-Att) (Parikh et al., 2016):"
P18-2086,P17-1152,0,0.139231,"question answering problem. All hypotheses H were obtained by rewriting (question, correct answer) pairs; all premises P are relevant web sentences collected by an Information retrieval (IR) method; then (P , H) pairs are annotated via crowdsourcing. Table 1 shows examples. By this construction, a substantial performance gain on S CI TAIL can be turned into better QA performance (Khot et al., 2018). Khot et al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th An"
P18-2086,N16-1170,0,0.0813247,"Missing"
P18-2086,D16-1053,0,0.0742271,"Missing"
P18-2086,E17-1066,1,0.895944,"Missing"
P18-2086,Q16-1019,1,0.882957,"Missing"
P18-2086,N18-2017,0,0.0202319,"en et al., 2017): Enhance LSTM by encoding syntax and semantics from parsing information. (iii) Ngram Overlap: An overlap baseline, considering unigrams, oneskip bigrams and one-skip trigrams. (iv) DGEM (Khot et al., 2018): A decomposed graph entailment model, the current state-of-the-art. (v) AttentiveConvNet (Yin and Sch¨utze, 2017a): Our top-performing textual entailment system on SNLI dataset, equipped with RNN-style attention mechanism in convolution.4 In addition, to check if S CI TAIL can be easily resolved by features from only premises or hypotheses (like the problem of SNLI shown by Gururangan et al. (2018)), we put a vanilla CNN (convolution&max-pooling) over merely hypothesis or premise to derive the pair label. 3 4 Please refer to (Khot et al., 2018) for more details. https://github.com/yinwenpeng/Attentive Convolution 543 # 1 2 3 4 5 6 (Premise P , Hypothesis H) Pair G/P Challenge (P ) Front – The boundary between two different air masses. language 1/0 (H) In weather terms, the boundary between two air masses is called front. conventions (P ) . . . the notochord forms the backbone (or vertebral column). language 1/0 (H) Backbone is another name for the vertebral column. conventions (P ) · ·"
P18-2086,marelli-etal-2014-sick,0,0.0331684,"ce (Khot et al., 2018). Khot et al. (2018) report that S CI TAIL challenges neural entailment models that show outstanding performance on SNLI, e.g., Decomposable Attention Model (Parikh et al., 2016) and Enhanced LSTM (Chen et al., 2017). We propose D E I S T E for S CI TAIL. Given word-to-word inter-sentence interactions between Introduction Textual entailment (TE) is a fundamental problem in natural language understanding and has been studied intensively recently using multiple benchmarks – PASCAL RTE challenges (Dagan et al., 2006, 2013), Paragraph-Headline (Burger and Ferro, 2005), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015). In particular, SNLI – while much easier than earlier datasets 1 0 1 0 1 2 https://github.com/yinwenpeng/SciTail RTE-{5,6,7} is an exception to this rule. 540 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 540–545 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics premise Representation for pair (P, H) rpos entail less likely more likely row-wise max-pooling mi hypothesis 1 hypothesis 2 attentive convolution Figure 1: The motivation of considering alignment pos"
P19-1034,W09-1703,0,0.0957579,"Missing"
P19-1034,P16-1197,0,0.0147658,"ic lexicons? We provide evidence that automatic adaptation works better. (3) Words often have specific meanings in a domain and this increases the risk that a word is misjudged if only the generic meaning is present to the annotator. This seems to be the primary reason for the problems of manual lexicons in our experiments. Thus, if manual lexicon creation is the only option, then it is important to present words in context, not in isolation, so that the domain-specific sense can be recognized. 2 Related Work In empirical finance, researchers have exploited various text resources, e.g., news (Kazemian et al., 2016), microblogs (Cortis et al., 2017), twitter (Zamani and Schwartz, 2017) and company disclosures (Nopp and Hanbury, 2015; Kogan et al., 2009). Deep learning has been used for learning document representations (Ding et al., 2015; Akhtar et al., 2017). However, the methodology of empirical finance requires interpretable re3 347 http://www.wjh.harvard.edu/˜inquirer/ economic drivers of such a financial outcome is of central interest in the field of finance. Some of these determinants may be correlated with sentiment. To understand the role of sentiment in explaining financial variables we therefor"
P19-1034,N09-1031,0,0.0482098,"the risk that a word is misjudged if only the generic meaning is present to the annotator. This seems to be the primary reason for the problems of manual lexicons in our experiments. Thus, if manual lexicon creation is the only option, then it is important to present words in context, not in isolation, so that the domain-specific sense can be recognized. 2 Related Work In empirical finance, researchers have exploited various text resources, e.g., news (Kazemian et al., 2016), microblogs (Cortis et al., 2017), twitter (Zamani and Schwartz, 2017) and company disclosures (Nopp and Hanbury, 2015; Kogan et al., 2009). Deep learning has been used for learning document representations (Ding et al., 2015; Akhtar et al., 2017). However, the methodology of empirical finance requires interpretable re3 347 http://www.wjh.harvard.edu/˜inquirer/ economic drivers of such a financial outcome is of central interest in the field of finance. Some of these determinants may be correlated with sentiment. To understand the role of sentiment in explaining financial variables we therefore need to isolate the complementary information of our text variables. This is achieved by including in our regressions – as control variabl"
P19-1034,P17-1157,0,0.025075,"Missing"
P19-1034,I13-1097,0,0.0465238,"Missing"
P19-1034,N16-1091,1,0.899966,"Missing"
P19-1034,D17-1052,0,0.0656475,"Missing"
P19-1034,P05-1017,0,0.0622025,"Missing"
P19-1086,D18-1316,0,0.0512828,"es, but are called de facto implicatives by Pavlick and Callison-Burch (2016) – who argue for the importance of datadriven approaches to detecting de facto implicatives. The meta rule discovery method just described is such a data-driven approach. 7 Related Work NLI challenge datasets. A lot of work exists that aims at uncovering weaknesses in state-of-the-art NLI models. Several approaches are based on modifications of popular datasets, such as SNLI or MultiNLI. These modifications range from simple rule-based transformations (Naik et al., 2018) to rewritings generated by genetic algorithms (Alzantot et al., 2018) or adversarial neural networks (Zhao et al., 2018). Lalor et al. (2016) constructed an NLI test set by judging the difficulty of the sentence pairs in a small SNLI subset based on crowdsourced human responses via Item Response Theory. These works are related as they, too, challenge existing NLI models with new data but orthogonal to ours as their goal is not to measure a model’s knowledge about lexical inference in context. Glockner et al. (2018) modified SNLI by replacing one word from a given sentence by a synonym, (co-)hyponym, hypernym or antonym to build a test set that requires NLI syst"
P19-1086,P11-1062,0,0.221513,"aselines. Input to these baselines are either the dependency paths or the sentences that were presented to the crowd workers. Baselines that require a threshold to be used as binary classifiers are tuned on SherLIiC-dev. Lemma baseline. Following Levy and Dagan (2016), this baseline classifies an InfCand as valid if the following holds true for the premise p and hypothesis h after lemmatization: (1) p contains all of h’s content words,3 (2) p’s and h’s predicates are identical, and (3) the relations’ active/passive voice matches their arguments’ alignment. Rule collection baselines. Berant I (Berant et al., 2011) and Berant II (Berant, 2012)4 are entailment graphs. PPDB is the largest collection (XXXL) of PPDB 2.0 (Pavlick et al., 2015).5 Patty is a collection of relational patterns, consisting of ontological types, POS placeholders and words. We use the version extracted from Wikipedia with Freebase types (Nakashole et al., 2012). Schoenmackers is the rule collection released by Schoenmackers et al. (2010). Chirps is an ever-growing6 predicate paraphrase database extracted via event coreference in news Tweets (Shwartz et al., 2017b). All Rules denotes the union of all of these rule bases. For rules w"
P19-1086,D15-1075,0,0.156747,"Missing"
P19-1086,P17-1152,0,0.0301997,"ric measures based on the distributional inclusion hypothesis have been proposed (Kotlerman et al., 2010; Santus et al., 2014; Shwartz et al., 2017a; Roller et al., 2018). We consider WeedsPrec (Weeds et al., 2004) and invCL (Lenci and Benotto, 2012), which have strong empirical results on hypernym detection. We use cooccurrence counts with entity pairs as distributional representation of a relation. Supervised NLI models. As LIiC is a special case of NLI, our dataset can also be used to evaluate the generalization capabilities of supervised models trained on large NLI datasets. We pick ESIM (Chen et al., 2017), a state-of-the-art supervised NLI model, trained on MultiNLI (Williams et al., 2018) as provided by the framework Jack the Reader (Weissenborn et al., 2018). Input to ESIM are the sentences from the annotation process with placeholders instantiated by entities randomly picked from the example lists that had also been shown to the crowd workers (cf. Fig. 1). As we want to measure ESIM’s capacity to detect entailment, we map its prediction of both neutral and contradiction to our non-entailment class. Sherlock+ESR. We also evaluate the candidate scoring method inspired by Schoenmackers et al."
P19-1086,D11-1142,0,0.0485782,"Which country annexed country[B]?” The answer candidate “country[A] administers country[B]” might be considered valid, given that it is unlikely that one country annexes B and another country administers it. The inference administer ⇒ annex, however, does not hold. Because of these difficulties, we follow the more traditional approach (Zeichner et al., 2012) of asking about consequences of a given fact (the premise). Relation extraction. Some works (Schoenmackers et al., 2010; Berant, 2012; Zeichner et al., 2012) rely on the output from open information extraction systems (Banko et al., 2007; Fader et al., 2011). A more flexible approach is to represent relations as lexicalized paths in dependency graphs (Lin and Pantel, 2001; Szpektor et al., 2004), sometimes with semantic postprocessing (Shwartz et al., 2017b) and/or retransforming into textual patterns (Nakashole et al., 2012). We, too, choose the latter. 909 Relation typing. Typing relations has become standard in inference mining because of its usefulness for sense disambiguation (Schoenmackers et al., 2010; Nakashole et al., 2012; Yao et al., 2012; Lewis and Steedman, 2013). Still some resources only provide types for one argument slot of their"
P19-1086,N13-1092,0,0.104943,"Missing"
P19-1086,P18-2103,0,0.198611,"I systems. 1 (2) (3) (4) (5) Table 1: Examples of SherLIiC InfCands and NLI challenges they cover. ORGF=organization founder, EMPL =employer, AUTH =book author, LOC =location, POL =politician, PER =person. Introduction Lexical inference (LI) can be seen as a focused variant of natural language inference (NLI), also called recognizing textual entailment (Dagan et al., 2013). Recently, Gururangan et al. (2018) showed that annotation artifacts in current NLI testbeds distort our impression of the performance of state of the art systems, giving rise to the need for new evaluation methods for NLI. Glockner et al. (2018) investigated LI as a way of evaluating NLI systems and found that even simple cases are challenging to current systems. In this paper, we release SherLIiC, a testbed specifically designed for evaluating a system’s ability to solve the hard problem of modeling lexical entailment in context. 1 https://github.com/mnschmit/SherLIiC ORGF [A] is granting to EMPL [B] ⇒ ORGF[A] is giving to EMPL[B] synonymy + ORGF [A] is supporter of ORGF [B] derivation ⇒ ORGF[A] is backing ORGF[B] typical AUTH [A] is president of LOC [B] ⇒ AUTH[A] is representing LOC[B] actions script PER [A] is interviewing AUTH [B"
P19-1086,E17-2068,0,0.0165259,"2001; Szpektor et al., 2004; Sekine, 2005; Yao et al., 2012; Lewis and Steedman, 2013). The two most important alternatives are bilingual pivoting (Ganitkevitch et al., 2013) – which identifies identically translated phrases in bilingual corpora – and event coreference in the news (Xu et al., 2014; Zhang et al., 2015; Shwartz et al., 2017b) – which relies on lexical variability in two articles or headlines referring to the same event. We specifically focus on distributional information for our InfCand collection because current models of lexical semantics are also mainly based on that (e.g., Grave et al., 2017). Our goal is not to build a resource free of typical mistakes made by distributional approaches but to provide a benchmark to study the progress on overcoming them (cf. § 5). Another difference to aforementioned works is that we explicitly model unidirectional entailment as opposed to bidirectional synonymy (cf. Table 4, (1)). Here one can distinguish a learning-based approach (Berant, 2012), where an SVM classifier with various features is trained on lexical ontologies like WordNet, followed by the application of global transitivity constrains to enhance consistency, and probabilistic models"
P19-1086,N18-2017,0,0.0393579,"Missing"
P19-1086,D18-2024,0,0.0147673,"but not by w2v+untyped rel. We therefore combine these two methods by using, for each tsg, the method that works better for that tsg on dev. (For tsgs not occurring in dev, we take the method that works better for the individual types occurring in the tsg. We use untyped embeddings if all else fails.) We refer to this combination as w2v+tsg rel emb. Knowledge graph embedding baselines. As SherLIiC-TEG has the structure of a knowledge graph (KG), we also evaluate the two KG embedding methods TransE (Bordes et al., 2013) and ComplEx (Trouillon et al., 2016), as provided by the OpenKE framework (Han et al., 2018). Asymmetric baselines. Entailment models built upon cosine similarity are symmetric whereas entailment is not. Therefore many asymmmetric measures based on the distributional inclusion hypothesis have been proposed (Kotlerman et al., 2010; Santus et al., 2014; Shwartz et al., 2017a; Roller et al., 2018). We consider WeedsPrec (Weeds et al., 2004) and invCL (Lenci and Benotto, 2012), which have strong empirical results on hypernym detection. We use cooccurrence counts with entity pairs as distributional representation of a relation. Supervised NLI models. As LIiC is a special case of NLI, our"
P19-1086,C92-2082,0,0.262418,"r no types at all (Zeichner et al., 2012; Berant, 2012; Shwartz et al., 2017b). Our InfCands are typed in both argument slots, which both facilitates disambiguation and makes them more general. Some works (Yao et al., 2012; Lewis and Steedman, 2013) learn distributions over latent type signatures for their relations via topic modeling. A large disadvantage of latent types is their lack of intuitive interpretability. By design, our KG types are meaningful and human-interpretable. Schoenmackers et al. (2010) type common nouns based on cooccurrence with class nouns identified by Hearst patterns (Hearst, 1992) and later try to filter out unreasonable typings by using frequency thresholds and PMI. As KG entities are manually labeled with their correct types, we do not need this kind of heuristics. Furthermore, in contrast to this ad-hoc type system, KG types are the result of a KG design process. Notably, Freebase types function as interfaces, i.e., permit type-specific properties to be added, and are thus inherently motivated by relations between entities. Lexical ontologies, such as WordNet (as used by Levy and Dagan, 2016) likewise lack this connection between relations and types. Moreover, relat"
P19-1086,D16-1062,0,0.0168895,"6) – who argue for the importance of datadriven approaches to detecting de facto implicatives. The meta rule discovery method just described is such a data-driven approach. 7 Related Work NLI challenge datasets. A lot of work exists that aims at uncovering weaknesses in state-of-the-art NLI models. Several approaches are based on modifications of popular datasets, such as SNLI or MultiNLI. These modifications range from simple rule-based transformations (Naik et al., 2018) to rewritings generated by genetic algorithms (Alzantot et al., 2018) or adversarial neural networks (Zhao et al., 2018). Lalor et al. (2016) constructed an NLI test set by judging the difficulty of the sentence pairs in a small SNLI subset based on crowdsourced human responses via Item Response Theory. These works are related as they, too, challenge existing NLI models with new data but orthogonal to ours as their goal is not to measure a model’s knowledge about lexical inference in context. Glockner et al. (2018) modified SNLI by replacing one word from a given sentence by a synonym, (co-)hyponym, hypernym or antonym to build a test set that requires NLI systems to use lexical knowledge. They rely on WordNet’s lexical taxonomy. T"
P19-1086,S12-1012,0,0.0280683,"baselines. As SherLIiC-TEG has the structure of a knowledge graph (KG), we also evaluate the two KG embedding methods TransE (Bordes et al., 2013) and ComplEx (Trouillon et al., 2016), as provided by the OpenKE framework (Han et al., 2018). Asymmetric baselines. Entailment models built upon cosine similarity are symmetric whereas entailment is not. Therefore many asymmmetric measures based on the distributional inclusion hypothesis have been proposed (Kotlerman et al., 2010; Santus et al., 2014; Shwartz et al., 2017a; Roller et al., 2018). We consider WeedsPrec (Weeds et al., 2004) and invCL (Lenci and Benotto, 2012), which have strong empirical results on hypernym detection. We use cooccurrence counts with entity pairs as distributional representation of a relation. Supervised NLI models. As LIiC is a special case of NLI, our dataset can also be used to evaluate the generalization capabilities of supervised models trained on large NLI datasets. We pick ESIM (Chen et al., 2017), a state-of-the-art supervised NLI model, trained on MultiNLI (Williams et al., 2018) as provided by the framework Jack the Reader (Weissenborn et al., 2018). Input to ESIM are the sentences from the annotation process with placeho"
P19-1086,P16-2041,0,0.198312,"is paper, we release SherLIiC, a testbed specifically designed for evaluating a system’s ability to solve the hard problem of modeling lexical entailment in context. 1 https://github.com/mnschmit/SherLIiC ORGF [A] is granting to EMPL [B] ⇒ ORGF[A] is giving to EMPL[B] synonymy + ORGF [A] is supporter of ORGF [B] derivation ⇒ ORGF[A] is backing ORGF[B] typical AUTH [A] is president of LOC [B] ⇒ AUTH[A] is representing LOC[B] actions script PER [A] is interviewing AUTH [B] ⇒ PER[A] is asking AUTH[B] knowledge common sense ORGF[A] claims LOC[B] knowledge ⇒ ORGF[A] is wanting LOC[B] (1) troponymy Levy and Dagan (2016) identified contextsensitive – as opposed to “context-free” – entailment as an important evaluation criterion and created a dataset for LI in context (LIiC). In their data, WordNet (Miller, 1995; Fellbaum, 2005) synsets serve as context for one side of a binary relation, but the other side is still instantiated with a single concrete expression. We aim to improve this setting in two ways. First, we type our relations on both sides, thus making them more general. Types provide a context that can help in disambiguation and at the same time allow generalization over contexts because arguments of"
P19-1086,W02-0109,0,0.0472805,"avlick et al., 2015).5 Patty is a collection of relational patterns, consisting of ontological types, POS placeholders and words. We use the version extracted from Wikipedia with Freebase types (Nakashole et al., 2012). Schoenmackers is the rule collection released by Schoenmackers et al. (2010). Chirps is an ever-growing6 predicate paraphrase database extracted via event coreference in news Tweets (Shwartz et al., 2017b). All Rules denotes the union of all of these rule bases. For rules with type (or POS) constraints, we ignore these constraints Baselines 3 We use the stop word list of nltk (Loper and Bird, 2002). We use default threshold 0. 5 We ignore stop words and punctuation for phrases. 6 We use the version downloaded on May 28, 2019. 4 We split our annotated data 25:75 into SherLIiCdev and SherLIiC-test, stratifying on annotated la905 to boost recall. We will see that even with these recall-enhancing measures, the majority of our correct InfCands is not covered by existing rule bases. Word2vec baselines. word2vec is based on Mikolov et al. (2013b)’s pre-trained word embeddings of size 300. We average them to obtain a vector representation of relations consisting of multiple words and use cosine"
P19-1086,D08-1103,0,0.0268321,"rnor of B nsubj–go–xcomp–X–obj A is going to beat B nsubj–try–xcomp–X–obj A tries to compete with B nsubj–decide–xcomp–X–obj A decides to move to B nsubjpass–expect–xcomp–X–obj A is expected to visit B ⇒ nsubj–X–obj A buys B ⇒ nsubj–X–obj A leaves B ⇔ nsubj–X–obj A is governor of B ⇒ nsubj–X–obj A beats B ⇒ nsubj–X–obj A competes with B ⇒ nsubj–X–obj A moves to B ⇒ nsubj–X–obj A visits B Table 5: Most frequent meta rules (top), character level meta rules (middle), and implicative verb meta rules (bottom). Bold: Words corresponding to X. rence: antonyms tend to be close in the embedding space (Mohammad et al., 2008; Mrkˇsi´c et al., 2016). The other examples show other types of correlation that models relying entirely on distributional information will fall for: the outcome of events like “coming into a country” or “seeking something from someone” are in general uncertain although possible outcomes like “remaining in said country” (3) or “being given the object of desire” (4) will be highly correlated with them. Finally, better models will also take into account the simultaneity constraint: “winning a war” and “declaring a war” (5) rarely happen at the same time. 908 6 Meta Rules and Implicative Verbs I"
P19-1086,N16-1018,0,0.0271012,"Missing"
P19-1086,C18-1198,0,0.0396635,"rn V to X ⇒ X. A lot of these verbs are not traditional implicatives, but are called de facto implicatives by Pavlick and Callison-Burch (2016) – who argue for the importance of datadriven approaches to detecting de facto implicatives. The meta rule discovery method just described is such a data-driven approach. 7 Related Work NLI challenge datasets. A lot of work exists that aims at uncovering weaknesses in state-of-the-art NLI models. Several approaches are based on modifications of popular datasets, such as SNLI or MultiNLI. These modifications range from simple rule-based transformations (Naik et al., 2018) to rewritings generated by genetic algorithms (Alzantot et al., 2018) or adversarial neural networks (Zhao et al., 2018). Lalor et al. (2016) constructed an NLI test set by judging the difficulty of the sentence pairs in a small SNLI subset based on crowdsourced human responses via Item Response Theory. These works are related as they, too, challenge existing NLI models with new data but orthogonal to ours as their goal is not to measure a model’s knowledge about lexical inference in context. Glockner et al. (2018) modified SNLI by replacing one word from a given sentence by a synonym, (co-)h"
P19-1086,D12-1104,0,0.539553,"following holds true for the premise p and hypothesis h after lemmatization: (1) p contains all of h’s content words,3 (2) p’s and h’s predicates are identical, and (3) the relations’ active/passive voice matches their arguments’ alignment. Rule collection baselines. Berant I (Berant et al., 2011) and Berant II (Berant, 2012)4 are entailment graphs. PPDB is the largest collection (XXXL) of PPDB 2.0 (Pavlick et al., 2015).5 Patty is a collection of relational patterns, consisting of ontological types, POS placeholders and words. We use the version extracted from Wikipedia with Freebase types (Nakashole et al., 2012). Schoenmackers is the rule collection released by Schoenmackers et al. (2010). Chirps is an ever-growing6 predicate paraphrase database extracted via event coreference in news Tweets (Shwartz et al., 2017b). All Rules denotes the union of all of these rule bases. For rules with type (or POS) constraints, we ignore these constraints Baselines 3 We use the stop word list of nltk (Loper and Bird, 2002). We use default threshold 0. 5 We ignore stop words and punctuation for phrases. 6 We use the version downloaded on May 28, 2019. 4 We split our annotated data 25:75 into SherLIiCdev and SherLIiC-"
P19-1086,D16-1240,0,0.0240149,"abstract way but shows already that the general principle of a conjunction Y ∧ X implying one of its components X can be learned. If we search for meta rules whose X is part of a lemma (rather than a longer dependency path), we discover cases of derivational morphology such as agent nouns (e.g., ruler, leader) and sense preserving verb prefixes (e.g., re-write, over-react). Finally, we observe several implicative verbs (verbs that entail their complement clauses) in their typical pattern V to X ⇒ X. A lot of these verbs are not traditional implicatives, but are called de facto implicatives by Pavlick and Callison-Burch (2016) – who argue for the importance of datadriven approaches to detecting de facto implicatives. The meta rule discovery method just described is such a data-driven approach. 7 Related Work NLI challenge datasets. A lot of work exists that aims at uncovering weaknesses in state-of-the-art NLI models. Several approaches are based on modifications of popular datasets, such as SNLI or MultiNLI. These modifications range from simple rule-based transformations (Naik et al., 2018) to rewritings generated by genetic algorithms (Alzantot et al., 2018) or adversarial neural networks (Zhao et al., 2018). La"
P19-1086,P15-2070,0,0.029115,"Missing"
P19-1086,P18-2057,0,0.0144718,"We refer to this combination as w2v+tsg rel emb. Knowledge graph embedding baselines. As SherLIiC-TEG has the structure of a knowledge graph (KG), we also evaluate the two KG embedding methods TransE (Bordes et al., 2013) and ComplEx (Trouillon et al., 2016), as provided by the OpenKE framework (Han et al., 2018). Asymmetric baselines. Entailment models built upon cosine similarity are symmetric whereas entailment is not. Therefore many asymmmetric measures based on the distributional inclusion hypothesis have been proposed (Kotlerman et al., 2010; Santus et al., 2014; Shwartz et al., 2017a; Roller et al., 2018). We consider WeedsPrec (Weeds et al., 2004) and invCL (Lenci and Benotto, 2012), which have strong empirical results on hypernym detection. We use cooccurrence counts with entity pairs as distributional representation of a relation. Supervised NLI models. As LIiC is a special case of NLI, our dataset can also be used to evaluate the generalization capabilities of supervised models trained on large NLI datasets. We pick ESIM (Chen et al., 2017), a state-of-the-art supervised NLI model, trained on MultiNLI (Williams et al., 2018) as provided by the framework Jack the Reader (Weissenborn et al.,"
P19-1086,E14-4008,0,0.0194912,"e use untyped embeddings if all else fails.) We refer to this combination as w2v+tsg rel emb. Knowledge graph embedding baselines. As SherLIiC-TEG has the structure of a knowledge graph (KG), we also evaluate the two KG embedding methods TransE (Bordes et al., 2013) and ComplEx (Trouillon et al., 2016), as provided by the OpenKE framework (Han et al., 2018). Asymmetric baselines. Entailment models built upon cosine similarity are symmetric whereas entailment is not. Therefore many asymmmetric measures based on the distributional inclusion hypothesis have been proposed (Kotlerman et al., 2010; Santus et al., 2014; Shwartz et al., 2017a; Roller et al., 2018). We consider WeedsPrec (Weeds et al., 2004) and invCL (Lenci and Benotto, 2012), which have strong empirical results on hypernym detection. We use cooccurrence counts with entity pairs as distributional representation of a relation. Supervised NLI models. As LIiC is a special case of NLI, our dataset can also be used to evaluate the generalization capabilities of supervised models trained on large NLI datasets. We pick ESIM (Chen et al., 2017), a state-of-the-art supervised NLI model, trained on MultiNLI (Williams et al., 2018) as provided by the f"
P19-1086,D10-1106,0,0.368008,"on: (1) p contains all of h’s content words,3 (2) p’s and h’s predicates are identical, and (3) the relations’ active/passive voice matches their arguments’ alignment. Rule collection baselines. Berant I (Berant et al., 2011) and Berant II (Berant, 2012)4 are entailment graphs. PPDB is the largest collection (XXXL) of PPDB 2.0 (Pavlick et al., 2015).5 Patty is a collection of relational patterns, consisting of ontological types, POS placeholders and words. We use the version extracted from Wikipedia with Freebase types (Nakashole et al., 2012). Schoenmackers is the rule collection released by Schoenmackers et al. (2010). Chirps is an ever-growing6 predicate paraphrase database extracted via event coreference in news Tweets (Shwartz et al., 2017b). All Rules denotes the union of all of these rule bases. For rules with type (or POS) constraints, we ignore these constraints Baselines 3 We use the stop word list of nltk (Loper and Bird, 2002). We use default threshold 0. 5 We ignore stop words and punctuation for phrases. 6 We use the version downloaded on May 28, 2019. 4 We split our annotated data 25:75 into SherLIiCdev and SherLIiC-test, stratifying on annotated la905 to boost recall. We will see that even wi"
P19-1086,I05-5011,0,0.0734529,"ith a pattern and each possible type of this entity. It is unclear how the combinatorial explosion and the resulting sparsity affects pattern quality. Our approach of successively splitting a typewise heterogenous relation into its k largest homogenous subrelations aims at finding only the most typical types for an action and our definition of type signature as intersection of all common types avoids unnecessary redundancy. Entailment candidate collection. Distributional features are a common choice for paraphrase detection and relation clustering (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005; Yao et al., 2012; Lewis and Steedman, 2013). The two most important alternatives are bilingual pivoting (Ganitkevitch et al., 2013) – which identifies identically translated phrases in bilingual corpora – and event coreference in the news (Xu et al., 2014; Zhang et al., 2015; Shwartz et al., 2017b) – which relies on lexical variability in two articles or headlines referring to the same event. We specifically focus on distributional information for our InfCand collection because current models of lexical semantics are also mainly based on that (e.g., Grave et al., 2017). Our goal is not to bu"
P19-1086,E17-1007,0,0.340543,"tches their arguments’ alignment. Rule collection baselines. Berant I (Berant et al., 2011) and Berant II (Berant, 2012)4 are entailment graphs. PPDB is the largest collection (XXXL) of PPDB 2.0 (Pavlick et al., 2015).5 Patty is a collection of relational patterns, consisting of ontological types, POS placeholders and words. We use the version extracted from Wikipedia with Freebase types (Nakashole et al., 2012). Schoenmackers is the rule collection released by Schoenmackers et al. (2010). Chirps is an ever-growing6 predicate paraphrase database extracted via event coreference in news Tweets (Shwartz et al., 2017b). All Rules denotes the union of all of these rule bases. For rules with type (or POS) constraints, we ignore these constraints Baselines 3 We use the stop word list of nltk (Loper and Bird, 2002). We use default threshold 0. 5 We ignore stop words and punctuation for phrases. 6 We use the version downloaded on May 28, 2019. 4 We split our annotated data 25:75 into SherLIiCdev and SherLIiC-test, stratifying on annotated la905 to boost recall. We will see that even with these recall-enhancing measures, the majority of our correct InfCands is not covered by existing rule bases. Word2vec baseli"
P19-1086,S17-1019,0,0.377773,"tches their arguments’ alignment. Rule collection baselines. Berant I (Berant et al., 2011) and Berant II (Berant, 2012)4 are entailment graphs. PPDB is the largest collection (XXXL) of PPDB 2.0 (Pavlick et al., 2015).5 Patty is a collection of relational patterns, consisting of ontological types, POS placeholders and words. We use the version extracted from Wikipedia with Freebase types (Nakashole et al., 2012). Schoenmackers is the rule collection released by Schoenmackers et al. (2010). Chirps is an ever-growing6 predicate paraphrase database extracted via event coreference in news Tweets (Shwartz et al., 2017b). All Rules denotes the union of all of these rule bases. For rules with type (or POS) constraints, we ignore these constraints Baselines 3 We use the stop word list of nltk (Loper and Bird, 2002). We use default threshold 0. 5 We ignore stop words and punctuation for phrases. 6 We use the version downloaded on May 28, 2019. 4 We split our annotated data 25:75 into SherLIiCdev and SherLIiC-test, stratifying on annotated la905 to boost recall. We will see that even with these recall-enhancing measures, the majority of our correct InfCands is not covered by existing rule bases. Word2vec baseli"
P19-1086,W04-3206,0,0.403894,"nlikely that one country annexes B and another country administers it. The inference administer ⇒ annex, however, does not hold. Because of these difficulties, we follow the more traditional approach (Zeichner et al., 2012) of asking about consequences of a given fact (the premise). Relation extraction. Some works (Schoenmackers et al., 2010; Berant, 2012; Zeichner et al., 2012) rely on the output from open information extraction systems (Banko et al., 2007; Fader et al., 2011). A more flexible approach is to represent relations as lexicalized paths in dependency graphs (Lin and Pantel, 2001; Szpektor et al., 2004), sometimes with semantic postprocessing (Shwartz et al., 2017b) and/or retransforming into textual patterns (Nakashole et al., 2012). We, too, choose the latter. 909 Relation typing. Typing relations has become standard in inference mining because of its usefulness for sense disambiguation (Schoenmackers et al., 2010; Nakashole et al., 2012; Yao et al., 2012; Lewis and Steedman, 2013). Still some resources only provide types for one argument slot of their binary relations (Levy and Dagan, 2016) or no types at all (Zeichner et al., 2012; Berant, 2012; Shwartz et al., 2017b). Our InfCands are t"
P19-1086,C04-1146,0,0.0513218,"Missing"
P19-1086,N18-1101,0,0.0427977,"Kotlerman et al., 2010; Santus et al., 2014; Shwartz et al., 2017a; Roller et al., 2018). We consider WeedsPrec (Weeds et al., 2004) and invCL (Lenci and Benotto, 2012), which have strong empirical results on hypernym detection. We use cooccurrence counts with entity pairs as distributional representation of a relation. Supervised NLI models. As LIiC is a special case of NLI, our dataset can also be used to evaluate the generalization capabilities of supervised models trained on large NLI datasets. We pick ESIM (Chen et al., 2017), a state-of-the-art supervised NLI model, trained on MultiNLI (Williams et al., 2018) as provided by the framework Jack the Reader (Weissenborn et al., 2018). Input to ESIM are the sentences from the annotation process with placeholders instantiated by entities randomly picked from the example lists that had also been shown to the crowd workers (cf. Fig. 1). As we want to measure ESIM’s capacity to detect entailment, we map its prediction of both neutral and contradiction to our non-entailment class. Sherlock+ESR. We also evaluate the candidate scoring method inspired by Schoenmackers et al. (2010) that created the data in the first place. We again combine the three scores des"
P19-1086,Q14-1034,0,0.0221122,"ubrelations aims at finding only the most typical types for an action and our definition of type signature as intersection of all common types avoids unnecessary redundancy. Entailment candidate collection. Distributional features are a common choice for paraphrase detection and relation clustering (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005; Yao et al., 2012; Lewis and Steedman, 2013). The two most important alternatives are bilingual pivoting (Ganitkevitch et al., 2013) – which identifies identically translated phrases in bilingual corpora – and event coreference in the news (Xu et al., 2014; Zhang et al., 2015; Shwartz et al., 2017b) – which relies on lexical variability in two articles or headlines referring to the same event. We specifically focus on distributional information for our InfCand collection because current models of lexical semantics are also mainly based on that (e.g., Grave et al., 2017). Our goal is not to build a resource free of typical mistakes made by distributional approaches but to provide a benchmark to study the progress on overcoming them (cf. § 5). Another difference to aforementioned works is that we explicitly model unidirectional entailment as oppo"
P19-1086,P12-1075,0,0.115954,"et al., 2012) rely on the output from open information extraction systems (Banko et al., 2007; Fader et al., 2011). A more flexible approach is to represent relations as lexicalized paths in dependency graphs (Lin and Pantel, 2001; Szpektor et al., 2004), sometimes with semantic postprocessing (Shwartz et al., 2017b) and/or retransforming into textual patterns (Nakashole et al., 2012). We, too, choose the latter. 909 Relation typing. Typing relations has become standard in inference mining because of its usefulness for sense disambiguation (Schoenmackers et al., 2010; Nakashole et al., 2012; Yao et al., 2012; Lewis and Steedman, 2013). Still some resources only provide types for one argument slot of their binary relations (Levy and Dagan, 2016) or no types at all (Zeichner et al., 2012; Berant, 2012; Shwartz et al., 2017b). Our InfCands are typed in both argument slots, which both facilitates disambiguation and makes them more general. Some works (Yao et al., 2012; Lewis and Steedman, 2013) learn distributions over latent type signatures for their relations via topic modeling. A large disadvantage of latent types is their lack of intuitive interpretability. By design, our KG types are meaningful"
P19-1086,P12-2031,0,0.376222,"e task of judging the appropriateness of candidate answers. Their hypothesis is that an answer is only appropriate if it entails the predicate of the question. This is often but by no means always true; certain questions imply additional information. Consider: “Which country annexed country[B]?” The answer candidate “country[A] administers country[B]” might be considered valid, given that it is unlikely that one country annexes B and another country administers it. The inference administer ⇒ annex, however, does not hold. Because of these difficulties, we follow the more traditional approach (Zeichner et al., 2012) of asking about consequences of a given fact (the premise). Relation extraction. Some works (Schoenmackers et al., 2010; Berant, 2012; Zeichner et al., 2012) rely on the output from open information extraction systems (Banko et al., 2007; Fader et al., 2011). A more flexible approach is to represent relations as lexicalized paths in dependency graphs (Lin and Pantel, 2001; Szpektor et al., 2004), sometimes with semantic postprocessing (Shwartz et al., 2017b) and/or retransforming into textual patterns (Nakashole et al., 2012). We, too, choose the latter. 909 Relation typing. Typing relations"
P19-1086,Q15-1009,0,0.0231762,"at finding only the most typical types for an action and our definition of type signature as intersection of all common types avoids unnecessary redundancy. Entailment candidate collection. Distributional features are a common choice for paraphrase detection and relation clustering (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005; Yao et al., 2012; Lewis and Steedman, 2013). The two most important alternatives are bilingual pivoting (Ganitkevitch et al., 2013) – which identifies identically translated phrases in bilingual corpora – and event coreference in the news (Xu et al., 2014; Zhang et al., 2015; Shwartz et al., 2017b) – which relies on lexical variability in two articles or headlines referring to the same event. We specifically focus on distributional information for our InfCand collection because current models of lexical semantics are also mainly based on that (e.g., Grave et al., 2017). Our goal is not to build a resource free of typical mistakes made by distributional approaches but to provide a benchmark to study the progress on overcoming them (cf. § 5). Another difference to aforementioned works is that we explicitly model unidirectional entailment as opposed to bidirectional"
P19-1086,Q13-1015,0,\N,Missing
P19-1086,P06-4018,0,\N,Missing
P19-1341,S15-2102,0,0.360774,"t lexicons for a broad range of typologically diverse languages. We use BPEs as basic units and show that they work well across languages. (ii) We carry out extensive evaluation to confirm correctness and high quality of the created lexicons. (iii) We make our code, the 1593 ZS seed sentiment lexicons and 200 generic sentiment lexicons freely available to the community. This is the up-to-now largest sentiment resource in terms of language coverage that has been published. 2 Related Work Monolingual Lexicon Induction. Sentiment lexicons for many languages have been induced. Eskander and Rambow (2015), Wang and Ku (2016), and Rouces et al. (2018) create Arabic, Chinese, and Swedish sentiment lexicons, respectively. Monolingually induced sentiment lexicons for specific domains like Twitter and finance are also devised (Mohammad et al., 2013; Hamilton et al., 2016). These methods are specialized such that applying them to other languages is non-trivial. For example, Eskander and Rambow (2015) link AraMorph (Buckwalter, 2004) with SentiWordNet by additionally considering part-ofspeech information, which may not be available in lexical resources in other languages. Inducing Chinese sentiment l"
P19-1341,P17-1042,0,0.0285847,"oduces subword regularization that utilizes multiple subword sequences to improve the robustness of NMT models. Sennrich et al. (2016)’s subword-nmt2 requires preprocessing (specifically, tokenization) for non-segmented languages, however, sentencepiece3 (Kudo and Richardson, 2018) used by Kudo (2018) requires no preprocessing even for non-segmented languages. This research indicates the potential of language-agnostic NMT. Effective representations of words (Sch¨utze, 1993), e.g., word embeddings (Mikolov et al., 2013; Pennington et al., 2014), have been extended to be bilingual (Ruder, 2017; Artetxe et al., 2017) or multilingual (Dufter et al., 2018), with (Artetxe et al., 2018) and without (Conneau et al., 2017) supervision. Artetxe and Schwenk (2018) train a language-agnostic BiLSTM encoder creating universal sentence representations of 93 languages, and performing strongly in crosslingual tasks. Lample and Conneau (2019) show that pretraining the encoders with a crosslingual language model objective helps in achieving state3507 2 3 github.com/rsennrich/subword-nmt github.com/google/sentencepiece of-the-art results in crosslingual classification and NMT. This research demonstrates the strength of la"
P19-1341,baccianella-etal-2010-sentiwordnet,0,0.0600705,"ain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show – across typologically diverse languages in PBC+ – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.1 1 Introduction Lexicons play an important role in sentiment analysis. Sentiment lexicons are available for highresource languages like English (Pang et al., 2008; Baccianella et al., 2010; Mohammad and Turney, 2013), but not for many low-resource languages. Researchers are trying to fill this gap by inducing lexicons monolingually (Badaro et al., 2014; Eskander and Rambow, 2015; Rouces et al., 2018) as well as multilingually (Chen and Skiena, 2014), often by transfer from high-resource to low-resource languages. The world’s languages are heterogeneous – of particular relevance for us is heterogeneity with respect to morphology and with respect to marking token boundaries. This heterogeneity poses difficulties when designing a universal approach 1 cistern.cis.lmu.de to lexicon"
P19-1341,W14-3623,0,0.0255154,"ality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.1 1 Introduction Lexicons play an important role in sentiment analysis. Sentiment lexicons are available for highresource languages like English (Pang et al., 2008; Baccianella et al., 2010; Mohammad and Turney, 2013), but not for many low-resource languages. Researchers are trying to fill this gap by inducing lexicons monolingually (Badaro et al., 2014; Eskander and Rambow, 2015; Rouces et al., 2018) as well as multilingually (Chen and Skiena, 2014), often by transfer from high-resource to low-resource languages. The world’s languages are heterogeneous – of particular relevance for us is heterogeneity with respect to morphology and with respect to marking token boundaries. This heterogeneity poses difficulties when designing a universal approach 1 cistern.cis.lmu.de to lexicon induction that works for all languages – implementing a high quality tokenizer and morphological analyzer for each language is not feasible short-term. Given the smal"
P19-1341,W17-5104,0,0.0139161,"word embeddings provided by Bojanowski et al. (2017) for 157 languages. Additionally, Heinzerling and Strube (2018) create generic BPE embeddings for 257 languages by segmenting Wikipedia articles using sentencepiece then running GloVe on the segmented corpora. As discussed above (§3.1), some BPEs in the PBC+ ZS lexicons are words, some are subwords – so we can utilize both sets. 4 4.1 1999) – to ensure ZS lexicons encode clear sentiment signals. The PBC+ ZS lexicon of language L is then the set of all high-confidence sentimentbearing BPEs from L. Evaluation. Following Abdaoui et al. (2017), Bar-Haim et al. (2017), Rouces et al. (2018), we evaluate the quality of PBC+ ZS lexicons based on gold sentiment lexicons in Japanese (JA) (concatenation of Kobayashi et al. (2005); Higashiyama et al. (2008)), Czech (CZ) (Veselovsk´a and Bojar, 2013), German (DE) (Waltinger, 2010), Spanish (ES) (Perez-Rosas et al., 2012), French (FR) (Abdaoui et al., 2017) and English (EN) (WHM lexicon, the concatenation of Wilson et al. (2005), Hu and Liu (2004) and Mohammad and Turney (2013), created by Rothe et al. (2016)). F1 is evaluation metric. We always compute F1 on the intersection of our and gold lexicon. Gold lexicons"
P19-1341,Q17-1010,0,0.0707104,"es high quality and coverage sentiment lexicons in a domain adaptation setup. Densifier forces QL to be orthogonal to preserve the structure of the embedding space. As we are only interested in accurate sentiment prediction, we replace the orthogonality with l2 regularization: λ 2 2 kP QL kF . The orthogonal constraint in Densifier – computing an SVD after each batch update – is expensive (O(d3 )) and requires non-trivial training regime (Rothe et al., 2016). We will show that our formalization delivers comparable results. In our experiments, we can use the generic word embeddings provided by Bojanowski et al. (2017) for 157 languages. Additionally, Heinzerling and Strube (2018) create generic BPE embeddings for 257 languages by segmenting Wikipedia articles using sentencepiece then running GloVe on the segmented corpora. As discussed above (§3.1), some BPEs in the PBC+ ZS lexicons are words, some are subwords – so we can utilize both sets. 4 4.1 1999) – to ensure ZS lexicons encode clear sentiment signals. The PBC+ ZS lexicon of language L is then the set of all high-confidence sentimentbearing BPEs from L. Evaluation. Following Abdaoui et al. (2017), Bar-Haim et al. (2017), Rouces et al. (2018), we eval"
P19-1341,P14-2063,0,0.372426,"and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.1 1 Introduction Lexicons play an important role in sentiment analysis. Sentiment lexicons are available for highresource languages like English (Pang et al., 2008; Baccianella et al., 2010; Mohammad and Turney, 2013), but not for many low-resource languages. Researchers are trying to fill this gap by inducing lexicons monolingually (Badaro et al., 2014; Eskander and Rambow, 2015; Rouces et al., 2018) as well as multilingually (Chen and Skiena, 2014), often by transfer from high-resource to low-resource languages. The world’s languages are heterogeneous – of particular relevance for us is heterogeneity with respect to morphology and with respect to marking token boundaries. This heterogeneity poses difficulties when designing a universal approach 1 cistern.cis.lmu.de to lexicon induction that works for all languages – implementing a high quality tokenizer and morphological analyzer for each language is not feasible short-term. Given the small number of native speakers in low-resource languages (Goldhahn et al., 2016), crowdsourcing cannot"
P19-1341,Y09-2024,0,0.0632116,"Missing"
P19-1341,P18-1141,1,0.762653,"Missing"
P19-1341,D15-1304,0,0.330796,"eral-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.1 1 Introduction Lexicons play an important role in sentiment analysis. Sentiment lexicons are available for highresource languages like English (Pang et al., 2008; Baccianella et al., 2010; Mohammad and Turney, 2013), but not for many low-resource languages. Researchers are trying to fill this gap by inducing lexicons monolingually (Badaro et al., 2014; Eskander and Rambow, 2015; Rouces et al., 2018) as well as multilingually (Chen and Skiena, 2014), often by transfer from high-resource to low-resource languages. The world’s languages are heterogeneous – of particular relevance for us is heterogeneity with respect to morphology and with respect to marking token boundaries. This heterogeneity poses difficulties when designing a universal approach 1 cistern.cis.lmu.de to lexicon induction that works for all languages – implementing a high quality tokenizer and morphological analyzer for each language is not feasible short-term. Given the small number of native speakers"
P19-1341,J15-1002,0,0.0170702,"016). These methods are specialized such that applying them to other languages is non-trivial. For example, Eskander and Rambow (2015) link AraMorph (Buckwalter, 2004) with SentiWordNet by additionally considering part-ofspeech information, which may not be available in lexical resources in other languages. Inducing Chinese sentiment lexicons (Wang and Ku, 2016) needs properly tokenized corpora, which is not a hard requirement in Swedish. In contrast, we aim to design a method applicable to typologically diverse languages and we apply it to 1500+ languages. Bi/Multi-Lingual Lexicon Induction. Gao et al. (2015) propose a graph based method for learning sentiment lexicons in target language by leveraging English sentiment lexicons. They rely on a high-quality word alignment, which is difficult to produce if languages are typologically diverse and the size of the parallel corpus is small. Chen and Skiena (2014) devise a knowledge graph eng fra jpn The book of the history of Jesus Christ , son of David , son of Abraham : Le livre de l’histoire de J´esus Christ , fils de David , fils d’Abraham : アブラハムの子，ダビデの子， イエス･キリストについての歴史の書 : Table 1: PBC+ verse 40001001 in three languages based method to build sent"
P19-1341,N16-1155,0,0.0305105,"ition” to refer to a single translation. Table 1 shows a verse in three languages. As shown, the Japanese (jpn) verse is not tokenized. 3.1 BPE Segmentation Given the linguistic heterogeneity of the world’s languages, it is crucial to first decide which type of linguistic unit to use to represent a language L in the multilingual space. The word, the linguistic unit typically generated from whitespace tokenization, is not ideal for universal approaches because non-segmented languages require carefully designed tokenizers. Character (or byte) n-gram is an alternative unit (Wieting et al., 2016; Gillick et al., 2016; Sch¨utze, 2017; Dufter et al., 2018), but the optimum length n varies across languages, e.g., n = 2 may be suitable for Chinese (Foo and Li, 2004), but clearly not for English. In our desire to design a universal approach, we use sentencepiece to segment PBC+ editions in all 1593 languages into sequences of BPE segments. We will show that this segmentation works across languages. The widely used BPE segmentation algorithm subword-nmt only considers BPE segments within words (Sennrich et al., 2016) and some frequent BPEs are essentially valid words. 4 We use github.com/ehsanasgari/1000Langs s"
P19-1341,D16-1057,0,0.0244798,"make our code, the 1593 ZS seed sentiment lexicons and 200 generic sentiment lexicons freely available to the community. This is the up-to-now largest sentiment resource in terms of language coverage that has been published. 2 Related Work Monolingual Lexicon Induction. Sentiment lexicons for many languages have been induced. Eskander and Rambow (2015), Wang and Ku (2016), and Rouces et al. (2018) create Arabic, Chinese, and Swedish sentiment lexicons, respectively. Monolingually induced sentiment lexicons for specific domains like Twitter and finance are also devised (Mohammad et al., 2013; Hamilton et al., 2016). These methods are specialized such that applying them to other languages is non-trivial. For example, Eskander and Rambow (2015) link AraMorph (Buckwalter, 2004) with SentiWordNet by additionally considering part-ofspeech information, which may not be available in lexical resources in other languages. Inducing Chinese sentiment lexicons (Wang and Ku, 2016) needs properly tokenized corpora, which is not a hard requirement in Swedish. In contrast, we aim to design a method applicable to typologically diverse languages and we apply it to 1500+ languages. Bi/Multi-Lingual Lexicon Induction. Gao"
P19-1341,L18-1473,0,0.0626743,"Missing"
P19-1341,E17-1072,0,0.0308323,"” (in love) and “何と言えばよいでしょうか” (what should I say). We will use the term “BPE” to refer to all BPE segments produced by sentencepiece, including subwords, words and cross-token units like phrases. Figure 1 (a) shows some sample units. As shown, the English segments can be words or subwords (underlined). Dominant contexts of shown subwords – insp: inspiration, inspired; crim: crime, criminals; blasphe: blasphemy, blasphemed; hest: highest, richest. 3.2 Multilingual Space Creation We next create the multilingual space hosting BPEs in 1593 languages of PBC+. We use the Sentence ID (S-ID) method (Levy et al. (2017), cf. also Le and Mikolov (2014)), a strong baseline in multilingual embedding learning. Given a sentence-aligned parallel corpus, the SID method first creates an embedding training corpus by recording co-occurrences between the sentence ID and the sentence’s words (the New Testament verse ID and BPEs in our case) in all languages. Figure 2 shows examples from the training corpus; each BPE is associated with a 3-digit ISO 639-3 language code. After that, an embedding learner is applied to the created corpus to learn the multilingual space. We use word2vecskipgram (Mikolov et al., 2013) as our"
P19-1341,mayer-cysouw-2014-creating,0,0.0273552,"me this heterogeneity and provide sentiment resources for low-resource languages, we present a new approach to sentiment lexicon induction that is universal – that is, it is applicable to the full range of typologically different languages – and apply it to 1593 languages. Our method first takes a parallel corpus as input and applies BPE (Gage, 1994) segmentation to it. We then create a multilingual BPE embedding space, from which a ZS (zero-shot) lexicon for each language L is extracted by zero-shot transfer from English sentiment to L. We use PBC+, an expansion of the Parallel Bible Corpus (Mayer and Cysouw, 2014), as our parallel corpus. The ZS lexicons show high quality, but are specific to the domain of PBC+ (the Bible). We then adapt them to the general domain. For brevity, we also use generic to refer to general-domain. Our method is universal and language-agnostic – it does not require language-dependent preprocessing. We carry out intrinsic and extrinsic, automatic and human evaluations on 95 languages. Intrinsic evaluation shows that our approach produces word ratings that strongly correlate with gold standard lexicons and human judgments. Extrinsic evaluation on Twitter sentiment classificatio"
P19-1341,P18-1007,0,0.123826,"pus, an approach that is better applicable across diverse languages. We do not require resources like Wiktionary. We cover more languages than Chen and Skiena (2014) and more words (e.g., 300K for Amharic). Language-Agnostic NLP. Language-agnostic NLP has demonstrated strong performance in areas such as neural machine translation (NMT) and universal representation learning. A particular difficulty is languages that do not mark token boundaries by whitespace such as Japanese. We refer to them as non-segmented languages. Sennrich et al. (2016) show the strength of BPE in translating rare words. Kudo (2018) introduces subword regularization that utilizes multiple subword sequences to improve the robustness of NMT models. Sennrich et al. (2016)’s subword-nmt2 requires preprocessing (specifically, tokenization) for non-segmented languages, however, sentencepiece3 (Kudo and Richardson, 2018) used by Kudo (2018) requires no preprocessing even for non-segmented languages. This research indicates the potential of language-agnostic NMT. Effective representations of words (Sch¨utze, 1993), e.g., word embeddings (Mikolov et al., 2013; Pennington et al., 2014), have been extended to be bilingual (Ruder, 2"
P19-1341,D18-2012,0,0.0127091,"ong performance in areas such as neural machine translation (NMT) and universal representation learning. A particular difficulty is languages that do not mark token boundaries by whitespace such as Japanese. We refer to them as non-segmented languages. Sennrich et al. (2016) show the strength of BPE in translating rare words. Kudo (2018) introduces subword regularization that utilizes multiple subword sequences to improve the robustness of NMT models. Sennrich et al. (2016)’s subword-nmt2 requires preprocessing (specifically, tokenization) for non-segmented languages, however, sentencepiece3 (Kudo and Richardson, 2018) used by Kudo (2018) requires no preprocessing even for non-segmented languages. This research indicates the potential of language-agnostic NMT. Effective representations of words (Sch¨utze, 1993), e.g., word embeddings (Mikolov et al., 2013; Pennington et al., 2014), have been extended to be bilingual (Ruder, 2017; Artetxe et al., 2017) or multilingual (Dufter et al., 2018), with (Artetxe et al., 2018) and without (Conneau et al., 2017) supervision. Artetxe and Schwenk (2018) train a language-agnostic BiLSTM encoder creating universal sentence representations of 93 languages, and performing s"
P19-1341,S13-2053,0,0.0352978,"ated lexicons. (iii) We make our code, the 1593 ZS seed sentiment lexicons and 200 generic sentiment lexicons freely available to the community. This is the up-to-now largest sentiment resource in terms of language coverage that has been published. 2 Related Work Monolingual Lexicon Induction. Sentiment lexicons for many languages have been induced. Eskander and Rambow (2015), Wang and Ku (2016), and Rouces et al. (2018) create Arabic, Chinese, and Swedish sentiment lexicons, respectively. Monolingually induced sentiment lexicons for specific domains like Twitter and finance are also devised (Mohammad et al., 2013; Hamilton et al., 2016). These methods are specialized such that applying them to other languages is non-trivial. For example, Eskander and Rambow (2015) link AraMorph (Buckwalter, 2004) with SentiWordNet by additionally considering part-ofspeech information, which may not be available in lexical resources in other languages. Inducing Chinese sentiment lexicons (Wang and Ku, 2016) needs properly tokenized corpora, which is not a hard requirement in Swedish. In contrast, we aim to design a method applicable to typologically diverse languages and we apply it to 1500+ languages. Bi/Multi-Lingual"
P19-1341,D14-1162,0,0.081651,"(2016) show the strength of BPE in translating rare words. Kudo (2018) introduces subword regularization that utilizes multiple subword sequences to improve the robustness of NMT models. Sennrich et al. (2016)’s subword-nmt2 requires preprocessing (specifically, tokenization) for non-segmented languages, however, sentencepiece3 (Kudo and Richardson, 2018) used by Kudo (2018) requires no preprocessing even for non-segmented languages. This research indicates the potential of language-agnostic NMT. Effective representations of words (Sch¨utze, 1993), e.g., word embeddings (Mikolov et al., 2013; Pennington et al., 2014), have been extended to be bilingual (Ruder, 2017; Artetxe et al., 2017) or multilingual (Dufter et al., 2018), with (Artetxe et al., 2018) and without (Conneau et al., 2017) supervision. Artetxe and Schwenk (2018) train a language-agnostic BiLSTM encoder creating universal sentence representations of 93 languages, and performing strongly in crosslingual tasks. Lample and Conneau (2019) show that pretraining the encoders with a crosslingual language model objective helps in achieving state3507 2 3 github.com/rsennrich/subword-nmt github.com/google/sentencepiece of-the-art results in crosslingu"
P19-1341,perez-rosas-etal-2012-learning,0,0.127862,"s in the PBC+ ZS lexicons are words, some are subwords – so we can utilize both sets. 4 4.1 1999) – to ensure ZS lexicons encode clear sentiment signals. The PBC+ ZS lexicon of language L is then the set of all high-confidence sentimentbearing BPEs from L. Evaluation. Following Abdaoui et al. (2017), Bar-Haim et al. (2017), Rouces et al. (2018), we evaluate the quality of PBC+ ZS lexicons based on gold sentiment lexicons in Japanese (JA) (concatenation of Kobayashi et al. (2005); Higashiyama et al. (2008)), Czech (CZ) (Veselovsk´a and Bojar, 2013), German (DE) (Waltinger, 2010), Spanish (ES) (Perez-Rosas et al., 2012), French (FR) (Abdaoui et al., 2017) and English (EN) (WHM lexicon, the concatenation of Wilson et al. (2005), Hu and Liu (2004) and Mohammad and Turney (2013), created by Rothe et al. (2016)). F1 is evaluation metric. We always compute F1 on the intersection of our and gold lexicon. Gold lexicons are also used in intrinsic evaluation of generic DA lexicons (Table 6). Additionally, the English WHM lexicon is also used in the evaluation of the universality of our approach (Table 8). Experiments Datasets and Settings We use the 7958 New Testament verses in PBC+ that were also used by Dufter et a"
P19-1341,S15-2078,0,0.0646037,"concentrates sentiment information in an embedding vector to a 1-dimensional ultradense sentiment space, resulting in a real-valued generic sentiment score. We minimize the objective using stochastic gradient descent (SGD). After training, the generic sentiment score of BPE w in language L is computed as sw = P QL ew . We refer to this method as REG and we call a lexicon computed by REG a generic DA (domain-adapted) lexicon since we always adapt from the Bible to the general domain in this paper. REG is inspired by Densifier (Rothe et al., 2016), which is state of the art on SemEval2015 10E (Rosenthal et al., 2015) – determining 3509 strength of association of Twitter terms with sentiment. Rothe et al. (2016) show that Densifier induces high quality and coverage sentiment lexicons in a domain adaptation setup. Densifier forces QL to be orthogonal to preserve the structure of the embedding space. As we are only interested in accurate sentiment prediction, we replace the orthogonality with l2 regularization: λ 2 2 kP QL kF . The orthogonal constraint in Densifier – computing an SVD after each batch update – is expensive (O(d3 )) and requires non-trivial training regime (Rothe et al., 2016). We will show t"
P19-1341,N16-1091,1,0.911682,"Missing"
P19-1341,D16-1157,0,0.0178624,"+. We use the term “edition” to refer to a single translation. Table 1 shows a verse in three languages. As shown, the Japanese (jpn) verse is not tokenized. 3.1 BPE Segmentation Given the linguistic heterogeneity of the world’s languages, it is crucial to first decide which type of linguistic unit to use to represent a language L in the multilingual space. The word, the linguistic unit typically generated from whitespace tokenization, is not ideal for universal approaches because non-segmented languages require carefully designed tokenizers. Character (or byte) n-gram is an alternative unit (Wieting et al., 2016; Gillick et al., 2016; Sch¨utze, 2017; Dufter et al., 2018), but the optimum length n varies across languages, e.g., n = 2 may be suitable for Chinese (Foo and Li, 2004), but clearly not for English. In our desire to design a universal approach, we use sentencepiece to segment PBC+ editions in all 1593 languages into sequences of BPE segments. We will show that this segmentation works across languages. The widely used BPE segmentation algorithm subword-nmt only considers BPE segments within words (Sennrich et al., 2016) and some frequent BPEs are essentially valid words. 4 We use github.com/e"
P19-1341,L18-1662,0,0.156296,"ons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.1 1 Introduction Lexicons play an important role in sentiment analysis. Sentiment lexicons are available for highresource languages like English (Pang et al., 2008; Baccianella et al., 2010; Mohammad and Turney, 2013), but not for many low-resource languages. Researchers are trying to fill this gap by inducing lexicons monolingually (Badaro et al., 2014; Eskander and Rambow, 2015; Rouces et al., 2018) as well as multilingually (Chen and Skiena, 2014), often by transfer from high-resource to low-resource languages. The world’s languages are heterogeneous – of particular relevance for us is heterogeneity with respect to morphology and with respect to marking token boundaries. This heterogeneity poses difficulties when designing a universal approach 1 cistern.cis.lmu.de to lexicon induction that works for all languages – implementing a high quality tokenizer and morphological analyzer for each language is not feasible short-term. Given the small number of native speakers in low-resource langu"
P19-1341,H05-1044,0,0.220483,"exicons encode clear sentiment signals. The PBC+ ZS lexicon of language L is then the set of all high-confidence sentimentbearing BPEs from L. Evaluation. Following Abdaoui et al. (2017), Bar-Haim et al. (2017), Rouces et al. (2018), we evaluate the quality of PBC+ ZS lexicons based on gold sentiment lexicons in Japanese (JA) (concatenation of Kobayashi et al. (2005); Higashiyama et al. (2008)), Czech (CZ) (Veselovsk´a and Bojar, 2013), German (DE) (Waltinger, 2010), Spanish (ES) (Perez-Rosas et al., 2012), French (FR) (Abdaoui et al., 2017) and English (EN) (WHM lexicon, the concatenation of Wilson et al. (2005), Hu and Liu (2004) and Mohammad and Turney (2013), created by Rothe et al. (2016)). F1 is evaluation metric. We always compute F1 on the intersection of our and gold lexicon. Gold lexicons are also used in intrinsic evaluation of generic DA lexicons (Table 6). Additionally, the English WHM lexicon is also used in the evaluation of the universality of our approach (Table 8). Experiments Datasets and Settings We use the 7958 New Testament verses in PBC+ that were also used by Dufter et al. (2018) to create the multilingual BPE embedding space. To cover as many BPEs as we can, we segment each PB"
P19-1341,E17-1074,1,0.890174,"Missing"
P19-1341,P16-1162,0,0.0537347,"oach uses BPE embeddings to extract alignment signals from the parallel corpus, an approach that is better applicable across diverse languages. We do not require resources like Wiktionary. We cover more languages than Chen and Skiena (2014) and more words (e.g., 300K for Amharic). Language-Agnostic NLP. Language-agnostic NLP has demonstrated strong performance in areas such as neural machine translation (NMT) and universal representation learning. A particular difficulty is languages that do not mark token boundaries by whitespace such as Japanese. We refer to them as non-segmented languages. Sennrich et al. (2016) show the strength of BPE in translating rare words. Kudo (2018) introduces subword regularization that utilizes multiple subword sequences to improve the robustness of NMT models. Sennrich et al. (2016)’s subword-nmt2 requires preprocessing (specifically, tokenization) for non-segmented languages, however, sentencepiece3 (Kudo and Richardson, 2018) used by Kudo (2018) requires no preprocessing even for non-segmented languages. This research indicates the potential of language-agnostic NMT. Effective representations of words (Sch¨utze, 1993), e.g., word embeddings (Mikolov et al., 2013; Pennin"
P19-1341,P14-1146,0,0.0319682,", the SID method first creates an embedding training corpus by recording co-occurrences between the sentence ID and the sentence’s words (the New Testament verse ID and BPEs in our case) in all languages. Figure 2 shows examples from the training corpus; each BPE is associated with a 3-digit ISO 639-3 language code. After that, an embedding learner is applied to the created corpus to learn the multilingual space. We use word2vecskipgram (Mikolov et al., 2013) as our embedding learner. 3.3 Zero-Shot Transfer of English Sentiment Embeddings encode sentiment information (Pennington et al., 2014; Tang et al., 2014; Amir et al., 2015; Rothe et al., 2016). We exploit this for zero-shot transfer of English sentiment to the other 1592 languages. We train two linear SVMs to classify sentiment of English BPE embeddings as positive vs. non-positive (POS) and as negative vs. non-negative (NEG). We use this setup – as opposed to binary classification positive vs. negative – to address the fact that some long BPE segments in non-segmented 3508 z (w1 , l1 ) ಅಲಗ ಯ (w2 , l2 ) glorieuse résister 励まし sincérité የሚበልጥ ML ∈ Rn×d (w3 , l3 ) ... (wm , lm ) 泣き avaricieux PBC+ ZS lexicon of L curse Generic Embeddings of L 麻"
P19-1341,waltinger-2010-germanpolarityclues,0,0.3023,"discussed above (§3.1), some BPEs in the PBC+ ZS lexicons are words, some are subwords – so we can utilize both sets. 4 4.1 1999) – to ensure ZS lexicons encode clear sentiment signals. The PBC+ ZS lexicon of language L is then the set of all high-confidence sentimentbearing BPEs from L. Evaluation. Following Abdaoui et al. (2017), Bar-Haim et al. (2017), Rouces et al. (2018), we evaluate the quality of PBC+ ZS lexicons based on gold sentiment lexicons in Japanese (JA) (concatenation of Kobayashi et al. (2005); Higashiyama et al. (2008)), Czech (CZ) (Veselovsk´a and Bojar, 2013), German (DE) (Waltinger, 2010), Spanish (ES) (Perez-Rosas et al., 2012), French (FR) (Abdaoui et al., 2017) and English (EN) (WHM lexicon, the concatenation of Wilson et al. (2005), Hu and Liu (2004) and Mohammad and Turney (2013), created by Rothe et al. (2016)). F1 is evaluation metric. We always compute F1 on the intersection of our and gold lexicon. Gold lexicons are also used in intrinsic evaluation of generic DA lexicons (Table 6). Additionally, the English WHM lexicon is also used in the evaluation of the universality of our approach (Table 8). Experiments Datasets and Settings We use the 7958 New Testament verses i"
P19-1341,L16-1428,0,0.0288468,"ns for a broad range of typologically diverse languages. We use BPEs as basic units and show that they work well across languages. (ii) We carry out extensive evaluation to confirm correctness and high quality of the created lexicons. (iii) We make our code, the 1593 ZS seed sentiment lexicons and 200 generic sentiment lexicons freely available to the community. This is the up-to-now largest sentiment resource in terms of language coverage that has been published. 2 Related Work Monolingual Lexicon Induction. Sentiment lexicons for many languages have been induced. Eskander and Rambow (2015), Wang and Ku (2016), and Rouces et al. (2018) create Arabic, Chinese, and Swedish sentiment lexicons, respectively. Monolingually induced sentiment lexicons for specific domains like Twitter and finance are also devised (Mohammad et al., 2013; Hamilton et al., 2016). These methods are specialized such that applying them to other languages is non-trivial. For example, Eskander and Rambow (2015) link AraMorph (Buckwalter, 2004) with SentiWordNet by additionally considering part-ofspeech information, which may not be available in lexical resources in other languages. Inducing Chinese sentiment lexicons (Wang and Ku"
P19-1574,P18-2043,0,0.0135221,"are represented in embeddings. While WSD and EL are important, they conflate (a) the evaluation of the information content of an embedding with (b) a model’s ability to extract that information based on contextual clues. We mostly focus on (a) here. Also, in contrast to WSD datasets, WIKI-PSE is not based on inferred sense tags and not based on artificial ambiguity, i.e., pseudowords (Gale et al., 1992; Sch¨utze, 1992), but on real senses marked by Wikipedia hyperlinks. There has been work in generating dictionary definitions from word embeddings (Noraset et al., 2017; Bosc and Vincent, 2018; Gadetsky et al., 2018). Gadetsky et al. (2018) explicitly adress ambiguity and generate definitions for words conditioned on their embeddings and selected contexts. This also conflates (a) and (b). Some prior work also looks at how ambiguity affects word embeddings. Arora et al. (2018) posit that a word embedding is a linear combination of its sense embeddings and that senses can be extracted via sparse coding. Mu et al. (2017) argue that sense and word vectors are linearly related and show that word embeddings are intersections of sense subspaces. Working with synthetic data, Yaghoobzadeh and Sch¨utze (2016) evalu"
P19-1574,W16-2507,0,0.021121,"chutze 1 Microsoft Research Montr´eal Center for Data Science, New York University 3 IXA NLP Group, University of the Basque Country 4 CIS, LMU Munich yayaghoo@microsoft.com 2 Abstract tools to analyze them. However, the main tool for analyzing their semantic content is still lookWord embeddings typically represent differing at nearest neighbors of embeddings. Nearest ent meanings of a word in a single conflated neighbors are based on full-space similarity nevector. Empirical analysis of embeddings of glecting the multifacetedness property of words ambiguous words is currently limited by the (Gladkova and Drozd, 2016) and making them unsmall size of manually annotated resources stable (Wendlandt et al., 2018). and by the fact that word senses are treated as unrelated individual concepts. We present As an alternative, we propose diagnostic clasa large dataset based on manual Wikipedia ansification of embeddings into semantic classes notations and word senses, where word senses as a probing task to reveal their meaning confrom different words are related by semantic tent. We will refer to semantic classes as Sclasses. This is the basis for novel diagnosclasses. We use S-classes such as food, drug tic tests f"
P19-1574,P12-1092,0,0.0647545,"n transittransit line, transit, finance-currency, disease, chemistry, body part, finance-stock exchange, law, medicine-medical treatment, medicinedrug, broadcast-tv channel, medicine-symptom, biology, visual art-color Table 1: S-classes in WIKI-PSE sorted by frequency. Figure 1: Example of how we build WIKI-PSE. There are three sentences linking “apple” to different entities. There are two mentions (m2 ,m3 ) with the organization sense (S-class) and one mention (m1 ) with the food sense (S-class). nated from embeddings, i.e., that a separate embedding is needed for each sense (Sch¨utze, 1998; Huang et al., 2012; Neelakantan et al., 2014; Li and Jurafsky, 2015; Camacho-Collados and Pilehvar, 2018). This can improve performance on contextual word similarity, but a recent study (Dubossarsky et al., 2018) questions this finding. WIKI-PSE allows us to compute sense embeddings; we will analyze their effect on word embeddings in our diagnostic classifications. 3 WIKI-PSE Resource class in the corpus. There exist sense annotated corpora like SemCor (Miller et al., 1993), but due to the cost of annotation, those corpora are usually limited in size, which can hurt the quality of the trained word embeddings –"
P19-1574,E09-1045,0,0.0468017,"Missing"
P19-1574,H93-1061,0,0.577046,"(2006) and Izquierdo et al. (2009) for word erally represented well in a single-vector emsense disambiguation, but have not been used for bedding – if the sense is frequent. (ii) A clasanalyzing embeddings. sifier can accurately predict whether a word Analysis based on S-classes is only promising if is single-sense or multi-sense, based only on we have high-quality S-class annotations. Existing its embedding. (iii) Although rare senses are datasets are either too small to train embeddings, not well represented in single-vector embeddings, this does not have negative impact on an e.g., SemCor (Miller et al., 1993), or artificially NLP application whose performance depends generated (Yaghoobzadeh and Sch¨utze, 2016). on frequent senses. Therefore, we build WIKI-PSE, a WIKIpediabased resource for Probing Semantics in word Em1 Introduction beddings. We focus on common and proper nouns, and use their S-classes as proxies for senses. Word embeddings learned by methods like For example, “lamb” has the senses food and Word2vec (Mikolov et al., 2013) and Glove (Penliving-thing. nington et al., 2014) have had a big impact on Embeddings do not explicitly address ambigunatural language processing (NLP) and inform"
P19-1574,J17-3004,1,0.894956,"Missing"
P19-1574,D14-1162,0,0.0902657,"Missing"
P19-1574,N18-1202,0,0.0635981,"ns, and use their S-classes as proxies for senses. Word embeddings learned by methods like For example, “lamb” has the senses food and Word2vec (Mikolov et al., 2013) and Glove (Penliving-thing. nington et al., 2014) have had a big impact on Embeddings do not explicitly address ambigunatural language processing (NLP) and informaity; multiple senses of a word are crammed into a tion retrieval (IR). They are effective and effisingle vector. This is not a problem in some applicient for many tasks. More recently, contextualcations (Li and Jurafsky, 2015); one possible exized embeddings like ELMo (Peters et al., 2018) planation is that this is an effect of sparse coding and BERT (Devlin et al., 2018) have further imthat supports the recovery of individual meanings proved performance. To understand both word from a single vector (Arora et al., 2018). But amand contextualized embeddings, which still rely on biguity has an adverse effect in other scenarios, word/subword embeddings at their lowest layer, e.g., Xiao and Guo (2014) see the need of filterwe must peek inside the blackbox embeddings. ing out embeddings of ambiguous words in depenGiven the importance of word embeddings, atdency parsing. tempts have"
P19-1574,J14-4005,0,0.0355501,"Missing"
P19-1574,J98-1004,1,0.561454,"Missing"
P19-1574,E17-1119,0,0.0136715,"n only work if the individual S-classes are recognizable, which is not the case for rare senses in regular word embeddings. NLP Application Experiments Our primary goal is to probe meanings in word embeddings without confounding factors like contextual usage. However, to give insights on how our probing results relate to NLP tasks, we evaluate our embeddings when used to represent word tokens.7 Note that our objective here is not to improve over other baselines, but to perform analysis. We select mention, sentence and sentence-pair classification datasets. For mention classification, we adapt Shimaoka et al. (2017)’s setup:8 training, evaluation (FIGER dataset) and implementation. The task is to predict the contextual fine-grained types of entity mentions. We lowercase the dataset to match the vocabularies of GLOVE(6B), FASTTEXT(Wiki) and our embeddings. For sentence and sentence-pair classifications, we use the SentEval9 (Conneau and Kiela, 2018) setup for four datasets: MR (Pang and Lee, 2005) (positive/negative sentiment prediction for movie reviews) , CR (Hu and Liu, 2004) (positive/negative sentiment prediction for product reviews), SUBJ (Pang and Lee, 2004) (subjectivity/objectivity prediction) an"
P19-1574,J17-4004,0,0.0247803,"ector embeddings has little negative impact – this indicates that for many common NLP benchmarks only frequent senses are needed. 2 Related Work S-classes (semantic classes) are a central concept in semantics and in the analysis of semantic phenomena (Yarowsky, 1992; Ciaramita and Johnson, 2003; Senel et al., 2018). They have been used for analyzing ambiguity by Kohomban and Lee (2005), Ciaramita and Altun (2006), and Izquierdo et al. (2009), inter alia. There are some datasets designed for interpreting word embedding dimensions using S-classes, e.g., SEMCAT (Senel et al., 2018) and HyperLex (Vulic et al., 2017). The main differentiator of our work is our probing approach using supervised classification of word embeddings. Also, we do not use WordNet senses but Wikipedia entity annotations since WordNettagged corpora are small. In this paper, we probe word embeddings with supervised classification. Probing the layers of neural networks has become very popular. Conneau et al. (2018) probe sentence embeddings on how well they predict linguistically motivated classes. Hupkes et al. (2018) apply diagnostic classifiers to test hypotheses about the hidden states of RNNs. Focusing on embeddings, Kann et al."
P19-1574,N18-1190,0,0.0191456,"oup, University of the Basque Country 4 CIS, LMU Munich yayaghoo@microsoft.com 2 Abstract tools to analyze them. However, the main tool for analyzing their semantic content is still lookWord embeddings typically represent differing at nearest neighbors of embeddings. Nearest ent meanings of a word in a single conflated neighbors are based on full-space similarity nevector. Empirical analysis of embeddings of glecting the multifacetedness property of words ambiguous words is currently limited by the (Gladkova and Drozd, 2016) and making them unsmall size of manually annotated resources stable (Wendlandt et al., 2018). and by the fact that word senses are treated as unrelated individual concepts. We present As an alternative, we propose diagnostic clasa large dataset based on manual Wikipedia ansification of embeddings into semantic classes notations and word senses, where word senses as a probing task to reveal their meaning confrom different words are related by semantic tent. We will refer to semantic classes as Sclasses. This is the basis for novel diagnosclasses. We use S-classes such as food, drug tic tests for an embedding’s content: we probe and living-thing to define word senses. Sword embeddings"
P19-1574,W14-1613,0,0.0158333,"ve and effisingle vector. This is not a problem in some applicient for many tasks. More recently, contextualcations (Li and Jurafsky, 2015); one possible exized embeddings like ELMo (Peters et al., 2018) planation is that this is an effect of sparse coding and BERT (Devlin et al., 2018) have further imthat supports the recovery of individual meanings proved performance. To understand both word from a single vector (Arora et al., 2018). But amand contextualized embeddings, which still rely on biguity has an adverse effect in other scenarios, word/subword embeddings at their lowest layer, e.g., Xiao and Guo (2014) see the need of filterwe must peek inside the blackbox embeddings. ing out embeddings of ambiguous words in depenGiven the importance of word embeddings, atdency parsing. tempts have been made to construct diagnostic 5740 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5740–5753 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics We present the first comprehensive empirical analysis of ambiguity in word embeddings. Our resource, WIKI-PSE, enables novel diagnostic tests that help explain how (and how well) e"
P19-1574,D15-1083,1,0.89959,"Missing"
P19-1574,P16-1023,1,0.930111,"Missing"
P19-1574,C92-2070,0,0.622512,"(i) Single-vector embeddings can represent many non-rare senses well. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) In experiments with five common datasets for mention, sentence and sentencepair classification tasks, the lack of representation of rare senses in single-vector embeddings has little negative impact – this indicates that for many common NLP benchmarks only frequent senses are needed. 2 Related Work S-classes (semantic classes) are a central concept in semantics and in the analysis of semantic phenomena (Yarowsky, 1992; Ciaramita and Johnson, 2003; Senel et al., 2018). They have been used for analyzing ambiguity by Kohomban and Lee (2005), Ciaramita and Altun (2006), and Izquierdo et al. (2009), inter alia. There are some datasets designed for interpreting word embedding dimensions using S-classes, e.g., SEMCAT (Senel et al., 2018) and HyperLex (Vulic et al., 2017). The main differentiator of our work is our probing approach using supervised classification of word embeddings. Also, we do not use WordNet senses but Wikipedia entity annotations since WordNettagged corpora are small. In this paper, we probe wo"
P19-1574,D15-1200,0,\N,Missing
P19-1574,C04-1051,0,\N,Missing
P19-1574,W03-1022,0,\N,Missing
P19-1574,W06-1670,0,\N,Missing
P19-1574,P04-1035,0,\N,Missing
P19-1574,P05-1005,0,\N,Missing
P19-1574,D14-1113,0,\N,Missing
P19-1574,N15-1142,0,\N,Missing
P19-1574,K16-1006,0,\N,Missing
P19-1574,P16-1191,0,\N,Missing
P19-1574,L18-1269,0,\N,Missing
P19-1574,D18-1200,0,\N,Missing
P19-1574,D18-1181,0,\N,Missing
P19-1574,N19-1423,0,\N,Missing
P19-1574,Q18-1034,0,\N,Missing
P93-1034,J92-4003,0,0.00472513,"good predictor of similar syntactic category. The nearest neighbors of athlete, clerk, declaration, and dome are singular nouns, the nearest neighbors of bowers and gibbs are family names, the nearest neighbors of desirable and sole are adjectives, and the nearest neighbors of financings are plural nouns, in each case without exception. The neighborhoods of armaments, cliches and luxuries (nouns), and b'nai and northwestern (NP-initial modifiers) fail to respect finer grained syntactic 1The cosine between two vectors corresponds to the normalized correlation coefficient: cos(c~(~,ff)) = 2Cf. (Brown et al. 1992) where the same idea of improving generalization and accuracy by looking at word classes instead of individual words is used. 252 A 4250 D C B + + + + I I I I 3000 3000 3000 Figure 1: The setup o f t h e m a t r i x f o r t h e 3000 first singular value decomposition. Table 1: Ten r a n d o m and three selected words and their nearest neighbors in category space 1. word nearest neighbors accompanied submitted banned financed developed authorized headed canceled awarded barred almost virtually merely formally fully quite officially just nearly only less causing reflecting forcing providing crea"
P97-1005,C90-1001,0,0.116861,"tive vs. involved&quot; or &quot;&apos;narrative vs. non-narrative.&quot; Factors are not used for genre classification (the values of a text on the various dimensions are often not informative with respect to genre). Rather, factors are used to validate hypotheses about the functions of various linguistic features. The traditional literature on genre is rich with classificatory schemes and systems, some of which might in retrospect be analyzed as simple attribute systems. (For general discussions of literary theories of genre, see, e.g., Butcher (1932), Dubrow (1982), Fowler (1982), Frye (1957), Hernadi (1972), Hobbes (1908), Staiger (1959), and Todorov (1978).) We will refer here to the attributes used in classifying genres as GENERIC FACETS. A facet is simply a property which distinguishes a class of texts that answers to certain practical interests~ and which is moreover associated with a characteristic set of computable structural or linguistic properties, whether categorical or statistical, which we will describe as &quot;generic cues.&quot; In principle, a given text can be described in terms of an indefinitely large number of facets. For example, a newspaper story about a Balkan peace initiative is an example of a B"
P97-1005,C94-2174,0,0.742061,"n used in previous work on genre. The experiments in this paper are based on 55 cues from the last three groups: lexical, characterlevel and derivative cues. These cues are easily computable in contrast to the structural cues that have figured prominently in previous work on genre. S t r u c t u r a l Cues Examples of structural cues are passives, nominalizations, topicalized sentences, and counts of the frequency of syntactic categories (e.g.. part-of-speech tags). These cues are not much discussed in the traditional literature on genre, but have come to the fore in recent work (Biber, 1995; Karlgren and Cutting, 1994). For purposes of automatic classification they have the limitation that they require tagged or parsed texts. 2.2 Lexical Cues Most facets are correlated with lexical cues. Examples of ones that we use are terms of address (e.g., Mr., Ms.). which predominate in papers like the New ~brk Times: Latinate affixes, which signal certain highbrow registers like scientific articles or scholarly works; and words used in expressing dates, which are common in certain types of narrative such as news stories. 2.3 3 3.1 Character-Level Cues D e r i v a t i v e Cues Derivative cues are ratios and variation m"
poesio-etal-2010-babyexp,J07-2002,0,\N,Missing
poesio-etal-2010-babyexp,P98-2127,0,\N,Missing
poesio-etal-2010-babyexp,C98-2122,0,\N,Missing
Q14-1002,W06-1615,0,0.950577,"he next two sections describe experimental data, setup and results. Results are discussed in Section 4. We compare FLORS to alternative word representations in Section 5 and to related work in Section 6. Section 7 presents our conclusions. 2 Experimental data and setup Data. Our source domain is the Penn Treebank (Marcus et al., 1993) of Wall Street Journal (WSJ) 15 Transactions of the Association for Computational Linguistics, 2 (2014) 15–26. Action Editor: Sharon Goldwater. c Submitted 9/2013; Revised 11/2013; Published 2/2014. 2014 Association for Computational Linguistics. text. Following Blitzer et al. (2006), we use sections 2-21 for training and 100,000 WSJ sentences from 1988 as unlabeled data in training. We evaluate on six different TDs. The first five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 22 and 23 of the WSJ for in-domain development and testing, respectively. Each SANCL TD has an unlabeled training set of 100,000 sentences and development and test sets of about 1000 labeled sentences each. The sixth TD is BIO, the Penn BioTreebank data set distributed by Blitzer. It c"
Q14-1002,A00-1031,0,0.157091,"s used at SANCL and (iii) POS DA results published for BIO. Most of our experiments use taggers from category (i) because we can ensure that experimental conditions are directly comparable. The four baselines in category (i) are shown in Table 1. Three have near state-of-the-art performance on WSJ: SVMTool (Giménez and Màrquez, 2004), Stanford 1 One could also compute these suffixes for _w (w prefixed by underscore) instead of for w to include words as distinguishable special suffixes. We test this alternative in Table 2, line 15. 17 (Toutanova et al., 2003) (a birectional MEMM) and C&P. TnT (Brants, 2000) is included as a representative of fast and simple HMM taggers. In addition, C&P is a tagger that has been extensively tested in DA scenarios with excellent results. Unless otherwise stated, we train all models using their default configuration files. We use the optimized parameter configuration published by C&P for the C&P model. Test set results will be compared with the SANCL taggers (category (ii)) at the end of Section 3. As far as category (iii) is concerned, most work on POS DA has been evaluated on BIO. We discuss our concerns about the BIO evaluation sets in Section 4, but also show"
Q14-1002,J92-4003,0,0.470007,"s of known words and high rates of unknown words. We show that FLORS achieves excellent DA tagging results on the five domains of the SANCL 2012 shared task (Petrov and McDonald, 2012) and outperforms three state-of-the-art taggers on Blitzer et al.’s (2006) biomedical data. FLORS is also simpler and faster than other POS DA methods. It is simple in that the input representation consists of three simple types of features: distributional count features and two types of binary features, suffix and shape features. Many other word representations that are used for improving generalization (e.g., (Brown et al., 1992; Collobert et al., 2011)) are costly to train or have difficulty handling unknown words. Our representations are fast to build and can be created on-the-fly for unknown words that occur during testing. The learning architecture is simple and fast as well. We train k binary one-vs-all classifiers that use local context only and no sequence information (where k is the number of tags). Thus, tagging complexity is O(k). Many other learning setups for DA are more complex; e.g., they learn representations (as opposed to just counting), they learn several classifiers for different subclasses of word"
Q14-1002,P12-2071,0,0.0603748,"newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 22 and 23 of the WSJ for in-domain development and testing, respectively. Each SANCL TD has an unlabeled training set of 100,000 sentences and development and test sets of about 1000 labeled sentences each. The sixth TD is BIO, the Penn BioTreebank data set distributed by Blitzer. It consists of dev and test sets of 500 sentences each and 100,000 unlabeled sentences. Classification setup. Similar to SVMTool (Giménez and Màrquez, 2004) and Choi and Palmer (2012) (henceforth: C&P), we use local context only for tagging instead of performing sequence classification. For a word w occurring as token vi in a sentence, we build a feature vector for a local window of size 2l + 1 around vi . The representation of the object to be classified is this feature vector and the target class is the POS tag of vi . We use the linear L2-regularized L2-loss SVM implementation provided by LIBLINEAR (Fan et al., 2008) to train k one-vs-all classifiers on the training set where k is the number of POS tags in the training set (in our case k = 45). We train with untuned def"
Q14-1002,W02-1001,0,0.214083,"Missing"
Q14-1002,N04-1001,0,0.0144432,"when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our ap24 proach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions"
Q14-1002,P12-2047,0,0.00452673,"71∗ 56.28∗ emails ALL OOV 89.44 62.61 88.21∗ 57.83∗ 89.44 63.13 89.02∗ 63.20 wsj ALL OOV 96.59 90.37 96.29∗ 85.55∗ 96.72 90.48 96.48∗ 87.50 Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n: number of indicator words. A column’s best result is bold. Moreover, FLORS representations consist of simple counts whereas SCL solves a separate optimization problem for each pivot feature. Umansky-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature"
Q14-1002,gimenez-marquez-2004-svmtool,0,0.473918,"erent TDs. The first five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 22 and 23 of the WSJ for in-domain development and testing, respectively. Each SANCL TD has an unlabeled training set of 100,000 sentences and development and test sets of about 1000 labeled sentences each. The sixth TD is BIO, the Penn BioTreebank data set distributed by Blitzer. It consists of dev and test sets of 500 sentences each and 100,000 unlabeled sentences. Classification setup. Similar to SVMTool (Giménez and Màrquez, 2004) and Choi and Palmer (2012) (henceforth: C&P), we use local context only for tagging instead of performing sequence classification. For a word w occurring as token vi in a sentence, we build a feature vector for a local window of size 2l + 1 around vi . The representation of the object to be classified is this feature vector and the target class is the POS tag of vi . We use the linear L2-regularized L2-loss SVM implementation provided by LIBLINEAR (Fan et al., 2008) to train k one-vs-all classifiers on the training set where k is the number of POS tags in the training set (in our case k = 45)"
Q14-1002,P09-1056,0,0.168491,"nts: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f (w) = f left (w)⊕f right (w)⊕f suffix (w)⊕f shape (w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of f left (w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci , w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci , w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x). tf-weighting has been used by other researchers (Huang and Yates, 2009) and showed good performance in our own"
Q14-1002,W10-2604,0,0.351626,"y-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constrai"
Q14-1002,D12-1120,0,0.0420048,"w since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as"
Q14-1002,N09-2054,0,0.0462408,"al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our ap24 proach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tag"
Q14-1002,P07-1034,0,0.487345,"nces from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions in each domain. The goal is to make these two distributions align by using instance-specific weights during training. Jiang and Zhai (2007) propose a framework that integrates prior knowledge from different data sets into the learning objective by weights. In related work, C&P train generalized and domain-specific models. An input sentence is tagged by the model that is most similar to the sentence. FLORS could be easily extended along these lines, an experiment we plan for the future. In terms of the basic classification setup, our POS tagger is most similar to the SVM-based approaches of Giménez and Màrquez (2004) and C&P. However, we do not use a left-to-right approach when tagging sentences. Moreover, SVMTool trains two separ"
Q14-1002,R11-1006,0,0.0383363,"tion model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions in each domain. The goal is to make these two distributions align by using instance-specific weights during training. Jiang and Zhai (2007) propose a framework that integrates prior knowledge from different data sets into the learning objective by weights. In related work,"
Q14-1002,J93-2004,0,0.0606473,"omplexity is O(k). Many other learning setups for DA are more complex; e.g., they learn representations (as opposed to just counting), they learn several classifiers for different subclasses of words (e.g., known vs. unknown) or they combine left-toright and right-to-left taggings. The next two sections describe experimental data, setup and results. Results are discussed in Section 4. We compare FLORS to alternative word representations in Section 5 and to related work in Section 6. Section 7 presents our conclusions. 2 Experimental data and setup Data. Our source domain is the Penn Treebank (Marcus et al., 1993) of Wall Street Journal (WSJ) 15 Transactions of the Association for Computational Linguistics, 2 (2014) 15–26. Action Editor: Sharon Goldwater. c Submitted 9/2013; Revised 11/2013; Published 2/2014. 2014 Association for Computational Linguistics. text. Following Blitzer et al. (2006), we use sections 2-21 for training and 100,000 WSJ sentences from 1988 as unlabeled data in training. We evaluate on six different TDs. The first five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 2"
Q14-1002,P06-1043,0,0.0573792,"ut-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as a form of constraint since feature weights will be shared among all words. Subramanya et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our ap24 proach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification"
Q14-1002,D07-1118,0,0.0476555,"Missing"
Q14-1002,N07-1051,0,0.0162186,"indices and FLORS uses distributional representations. Models 2–4 use combinations of features (e.g., tag-word) as well. (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Miller et al. (2007)) we simply use all (lowercase) suffixes to avoid the need for selecting a subset of suffixes; and we treat all words equally as opposed to using suffix features for only a subset of words. For suffix s, we set the dimension corresponding to s in f suffix (w) to 1 if lowercased w ends in s and to 0 otherwise. Note that w is a suffix of itself.1 Shape features. We use the Berkeley parser word signatures (Petrov and Klein, 2007). Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., does the word contain a digit, hyphen, uppercase character) and morphological (e.g., does the word end in -ed or -ing) features. There are 50 unique signatures in WSJ. We set the dimension of f shape (w) that corresponds to the signature of w to 1 and all other dimensions to 0. We note that the shape features we use were designed for English and probably would have to be adjusted for other languages. Baselines. We address the problem of unsupervised domain adaptation for POS"
Q14-1002,W09-1119,0,0.0108927,"e typically trained on a large amount of unlabeled text and fine-tuned for specific NLP tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, either alone or in combination with other features. To compare f dist (w) (our distributional representations) with Brown clusters, we induced 1000 Brown clusters on the joint corpus data DALL (see Section 2) using the publicly available implementation of Liang (2005). We padded sentences with hBOUNDARYi tokens on each side and used path prefixes of length 4, 6, 10 and 20 as features for each word (cf. Ratinov and Roth (2009), Turian et al. (2010)). C&W embeddings are provided by Collobert et al. (2011): 50-dimensional vectors for 130,000 words from WSJ, trained on Wikipedia. Similar to our distributional representations fdist (w), the embeddings also contain a hBOUNDARYi token (which they call PADDING). Moreover, they have a special embedding for unknown words (called UNKNOWN) which we use whenever we encounter a word that is not in their lookup table. We preprocess our raw tokens the same way they do (lowercase and replace sequences of digits by “0”) before we look up a representation during training and testing"
Q14-1002,W96-0213,0,0.713967,"orthography p±{0,1,2,3} , v±{0,1,2,3} , affixes, orthography, word length p±{0,1,2,3} , v±{0,1,2,3} , affixes, orthography distributions of v±{0,1,2} , suffixes, orthography Table 1: Overview of baseline taggers and FLORS. vi : token, pi : POS tag. Positions included in the sets of token indices are relative to the position i of the word v0 to be tagged; e.g., p±{0,1,2} is short for {p−0 , p−1 , p−2 , p0 , p1 , p2 }. To represent tokens vi , models 1–4 use vocabulary indices and FLORS uses distributional representations. Models 2–4 use combinations of features (e.g., tag-word) as well. (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Miller et al. (2007)) we simply use all (lowercase) suffixes to avoid the need for selecting a subset of suffixes; and we treat all words equally as opposed to using suffix features for only a subset of words. For suffix s, we set the dimension corresponding to s in f suffix (w) to 1 if lowercased w ends in s and to 0 otherwise. Note that w is a suffix of itself.1 Shape features. We use the Berkeley parser word signatures (Petrov and Klein, 2007). Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., do"
Q14-1002,D12-1131,0,0.144227,"are them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as a form of constraint since feature weights will be shared among all words. Subramanya et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our ap24 proach does not seem to suffer from this"
Q14-1002,I13-1023,1,0.815222,"omains. This argues for using l = 2, i.e., a window size of 5. Results for left-to-right (L-to-R) tagging are given on line 14. Similar to SVMTool and C&P, each sentence is tagged from left to right and previous tagging decisions are used for the current classification. In this setting, we use the previous tag pi−1 as one additional feature in the feature vector of vi . The effect of left-to-right is similar to the effect of omitting suffixes: OOV accuracies go up in some domains, but ALL accuracies decrease (except for an increase of .02 for reviews). This is in line with the experiments in (Schnabel and Schütze, 2013) where sequential information in a CRF was not robust across domains. OOV tagging may benefit from correct previous tags because the larger left context that is indirectly made available by left-to-right tagging compensates partially for the lack of information about the OOV word. In contrast to standard approaches to POS tagging, the FLORS basic representation does not contain vocabulary indices. Line 15 shows what happens if we add them; the dimensionality of the feature vector is increased by 5|V |– where V is the training set vocabulary – and in training one binary feature is set to one fo"
Q14-1002,P93-1034,1,0.397436,"We represent each word w by four components: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f (w) = f left (w)⊕f right (w)⊕f suffix (w)⊕f shape (w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of f left (w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci , w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci , w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x). tf-weighting has been used by other researchers (Huang and Y"
Q14-1002,E95-1020,1,0.369462,"ach word w by four components: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f (w) = f left (w)⊕f right (w)⊕f suffix (w)⊕f shape (w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of f left (w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci , w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci , w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x). tf-weighting has been used by other researchers (Huang and Yates, 2009) and"
Q14-1002,D10-1017,0,0.0252218,"tractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as a form of constraint since feature weights will be shared among all words. Subramanya et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our ap24 proach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al"
Q14-1002,N03-1033,0,0.680958,",3} , v±{0,1,2,3} , affixes, orthography, word length p±{0,1,2,3} , v±{0,1,2,3} , affixes, orthography distributions of v±{0,1,2} , suffixes, orthography Table 1: Overview of baseline taggers and FLORS. vi : token, pi : POS tag. Positions included in the sets of token indices are relative to the position i of the word v0 to be tagged; e.g., p±{0,1,2} is short for {p−0 , p−1 , p−2 , p0 , p1 , p2 }. To represent tokens vi , models 1–4 use vocabulary indices and FLORS uses distributional representations. Models 2–4 use combinations of features (e.g., tag-word) as well. (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Miller et al. (2007)) we simply use all (lowercase) suffixes to avoid the need for selecting a subset of suffixes; and we treat all words equally as opposed to using suffix features for only a subset of words. For suffix s, we set the dimension corresponding to s in f suffix (w) to 1 if lowercased w ends in s and to 0 otherwise. Note that w is a suffix of itself.1 Shape features. We use the Berkeley parser word signatures (Petrov and Klein, 2007). Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., does the word contain a dig"
Q14-1002,H05-1059,0,0.0652473,"Màrquez (2004) and C&P. However, we do not use a left-to-right approach when tagging sentences. Moreover, SVMTool trains two separate models, one for OOVs and one for known words. FLORS only has a single model. In addition, we do not make use of ambiguity classes, token-tag dictionaries and rare feature thresholds. Instead, we rely only on three types of features: distributional representations, suffixes and word shapes. The local-context-only approach of SVMTool, C&P and FLORS is different from standard sequence classification such as MEMMs (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Tsuruoka and Tsujii (2005)) and CRFs (e.g., Collins (2002)). Sequence models are more powerful in theory, but this may not be an advantage in DA because the subtle dependencies they exploit may not hold across domains. 7 Conclusion We have presented FLORS, a new POS tagger for DA. FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags. FLORS is simpler and faster than previous DA methods, yet we were able to demonstrate that it has significantly better accuracy than several baselines. Acknowledgments. This work was supported by DFG (Deutsche Forschungsgemeinsc"
Q14-1002,P10-1040,0,0.447056,"ed by other researchers (Huang and Yates, 2009) and showed good performance in our own previous work. f right (w) is defined analogously. We restrict the set of indicator words to the n = 500 most frequent words in the corpus. To avoid zero vectors, we add an entry xn+1 to each vector that counts omitted contexts:   X xn+1 = tf  freq (bigram(cj , w)) j:j>n We compute distributional vectors on the joint corpus DALL of all labeled and unlabeled text of source domain and TD. The text is preprocessed by lowercasing everything – which is often done when computing word representations, e.g., by Turian et al. (2010) – and by padding sentences with hBOUNDARYi tokens. Suffix features. Suffixes are promising for DA because basic morphology rules are the same in different domains. In contrast to other work on tagging 1 2 3 4 5 model TnT Stanford SVMTool C&P FLORS classifier HMM bidir. MEMM SVM SVM SVM features p−{0,1,2} , v0 , suffixes (for OOVs) p±{0,1,2} , v±{0,1} , affixes, orthography p±{0,1,2,3} , v±{0,1,2,3} , affixes, orthography, word length p±{0,1,2,3} , v±{0,1,2,3} , affixes, orthography distributions of v±{0,1,2} , suffixes, orthography Table 1: Overview of baseline taggers and FLORS. vi : token,"
Q14-1002,C10-2146,0,0.0171612,"∗ 92.54∗ 72.48∗ 92.23∗ 71.47∗ weblogs ALL OOV 94.71 83.64 93.40∗ 72.55∗ 94.51 80.58∗ 94.45 81.76 answers ALL OOV 90.30 62.15 89.47∗ 55.82∗ 90.23 60.99 89.71∗ 56.28∗ emails ALL OOV 89.44 62.61 88.21∗ 57.83∗ 89.44 63.13 89.02∗ 63.20 wsj ALL OOV 96.59 90.37 96.29∗ 85.55∗ 96.72 90.48 96.48∗ 87.50 Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n: number of indicator words. A column’s best result is bold. Moreover, FLORS representations consist of simple counts whereas SCL solves a separate optimization problem for each pivot feature. Umansky-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010"
Q14-1002,P08-1086,0,0.00515171,"ds: (i) Brown clusters (Brown et al., 1992) and (ii) C&W embeddings, the word embeddings of Collobert et al. (2011). We use f dist (w) = f left (w) ⊕ f right (w) to refer to our own distributional word representations (see Section 2). The perhaps oldest and most frequently used lowdimensional representation of words is based on Brown clusters. Typically, prefixes of Brown clusters (Brown et al., 1992) are added to increase the robustness of POS taggers (e.g., Toutanova et al. (2003)). Computational costs are high (quadratic in the vocabulary size) although the computation can be parallelized (Uszkoreit and Brants, 2008). More recently, general word representations (Collobert et al., 2011; Turian et al., 2010) have been used for robust POS tagging. These word representations are typically trained on a large amount of unlabeled text and fine-tuned for specific NLP tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, either alone or in combination with other features. To compare f dist (w) (our distributional representations) with Brown clusters, we induced 1000 Brown clusters on the joint corpus data DALL (see Section 2) using the publicly available implemen"
Q14-1002,P11-2009,0,\N,Missing
Q16-1019,D12-1050,0,0.00810263,"tworks to model sentence pairs for AS, PI and TE. For AS, Yu et al. (2014) present a bigram CNN to model question and answer candidates. Yang et al. (2015) extend this method and get state-of-the-art performance on the WikiQA dataset (Section 5.1). Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2016) explore bidirectional LSTMs on the same dataset. Our approach is different because we do not model the sentences by two independent neural networks in parallel, but instead as an interdependent sentence pair, using attention. For PI, Blacoe and Lapata (2012) form sentence representations by summing up word embeddings. Socher et al. (2011) use recursive autoencoders (RAEs) to model representations of local phrases in sentences, then pool similarity values of phrases from the two sentences as features for binary classification. Yin and Sch¨utze (2015a) similarly replace an RAE with a CNN. In all three papers, the representation of one sentence is not influenced by the other – in contrast to our attention-based model. For TE, Bowman et al. (2015b) use recursive neural networks to encode entailment on SICK (Marelli et al., 2014b). Rockt¨aschel et al."
Q16-1019,D15-1075,0,0.242137,"when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture tha"
Q16-1019,W15-4002,0,0.0821956,"when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture tha"
Q16-1019,N10-1066,0,0.0141601,"is on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such approaches often require more computational resources. In addition, employing syntactic or semantic parsers – which produce errors on many sentences – to find the best match between the structured representations of two sentences is not trivial. DL on Sentence Pair Modeling. To address some of the challenges of non-DL work, mu"
Q16-1019,C04-1051,0,0.18671,"Missing"
Q16-1019,D15-1181,0,0.40273,"Missing"
Q16-1019,N10-1145,0,0.0633371,"attracted lots of attention in the past decades. Many tasks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phra"
Q16-1019,D13-1090,0,0.0125067,"Missing"
Q16-1019,S14-2131,0,0.0262344,"Missing"
Q16-1019,P14-1062,0,0.0950702,"nize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture that computes different representations of si for different s1−i (i ∈ {0, 1}). 259 Transactions of the Association for Computational Linguistics, vol. 4, pp. 259–272, 2016. Action Editor: Brian Roark. Submission batch: 12/2015; Revision batch: 3/2016; Published 6/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are widely used to model sentences (Kalchbrenner et al., 2014; Kim, 2014) and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying atte"
Q16-1019,D14-1181,0,0.0350541,"and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture that computes different representations of si for different s1−i (i ∈ {0, 1}). 259 Transactions of the Association for Computational Linguistics, vol. 4, pp. 259–272, 2016. Action Editor: Brian Roark. Submission batch: 12/2015; Revision batch: 3/2016; Published 6/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are widely used to model sentences (Kalchbrenner et al., 2014; Kim, 2014) and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g.,"
Q16-1019,S14-2055,0,0.00838841,"ctic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such approaches often require more computational resources. In addition, employing syntactic or semantic parsers – which produce errors on many sentences – to find the best match between the structured representations of two sentences is not trivial. DL on Sentence Pair Modeling. To address some of the challenges of non-DL work, much recent work uses neural ne"
Q16-1019,P15-1107,0,0.199045,"Missing"
Q16-1019,W04-1013,0,0.0122377,"Missing"
Q16-1019,D15-1166,0,0.137021,"only processing the selected regions at high resolution. Gregor et al. (2015) combine a spatial attention mechanism with RNNs for image generation. Ba et al. (2015) investigate attention-based RNNs for recognizing multiple objects in images. Chorowski et al. (2014) and Chorowski et al. (2015) use attention in RNNs for speech recognition. Attention-Based DL in NLP. Attention-based DL systems have been applied to NLP after their success in computer vision and speech recognition. They mainly rely on RNNs and end-to-end encoderdecoders for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). Our work takes the lead in exploring attention mechanisms in CNNs for NLP tasks. 3 BCNN: Basic Bi-CNN We now introduce our basic (non-attention) CNN that is based on the Siamese architecture (Bromley et al., 1993), i.e., it consists of two weightsharing CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair task. See Figure 2. We refer to this architecture as the BCNN. The next section will then introduce the ABCNN, an attention architecture that extends the BCNN. Table 1 gives our notationa"
Q16-1019,N12-1019,0,0.00988376,"al influence of the two sentences in the context of the task. It also contradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors"
Q16-1019,S14-2001,0,0.275524,"tradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need f"
Q16-1019,marelli-etal-2014-sick,0,0.153021,"tradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need f"
Q16-1019,D15-1044,0,0.195677,"r et al. (2015) combine a spatial attention mechanism with RNNs for image generation. Ba et al. (2015) investigate attention-based RNNs for recognizing multiple objects in images. Chorowski et al. (2014) and Chorowski et al. (2015) use attention in RNNs for speech recognition. Attention-Based DL in NLP. Attention-based DL systems have been applied to NLP after their success in computer vision and speech recognition. They mainly rely on RNNs and end-to-end encoderdecoders for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). Our work takes the lead in exploring attention mechanisms in CNNs for NLP tasks. 3 BCNN: Basic Bi-CNN We now introduce our basic (non-attention) CNN that is based on the Siamese architecture (Bromley et al., 1993), i.e., it consists of two weightsharing CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair task. See Figure 2. We refer to this architecture as the BCNN. The next section will then introduce the ABCNN, an attention architecture that extends the BCNN. Table 1 gives our notational conventions. In our implementation and also in the mathemat"
Q16-1019,D07-1002,0,0.00848433,"atures are used. 2 Related Work Non-DL on Sentence Pair Modeling. Sentence pair modeling has attracted lots of attention in the past decades. Many tasks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, s"
Q16-1019,D07-1003,0,0.344451,"sks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such ap"
Q16-1019,D15-1237,0,0.717817,"Missing"
Q16-1019,D13-1056,0,0.00563054,"Missing"
Q16-1019,P13-2029,0,0.00630854,"Missing"
Q16-1019,P13-1171,0,0.0672204,"and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g., Yih et al. (2013) employ word alignment to match related parts of the two sentences. In contrast, our attention scheme based on CNNs models relatedness between two parts fully automatically. Moreover, attention at multiple levels of granularity, not only at word level, is achieved as we stack multiple convolution layers that increase abstraction. Prior work on attention in deep learning (DL) mostly addresses long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997). LSTMs achieve attention usually in a word-to-word scheme, and word representations mostly encode the whole context within the sen"
Q16-1019,N15-1091,1,0.587059,"Missing"
Q16-1019,P15-1007,1,0.520976,"Missing"
Q16-1019,S14-2044,0,0.0779176,"Missing"
Q18-1003,D15-1041,0,0.0188175,"latedness, which is a proxy for semantic coherence. HR (highrelatedness) words were judged to be more compositional than LR (low-relatedness) words. Character-Level Neural Retrofitting. As a further strong baseline, we consider a retrofitting (Faruqui et al., 2015) approach based on characterlevel recurrent neural networks. Recently, running a recurrent net over the character stream has become a popular way of incorporating subword information into a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016). To the best of our knowledge, character-level retrofitting is a novel approach. Given a vector v for a word form w, we seek a function to minimize the following objective 1 ||v − hN ||22 , 2 (10) where hN is the final hidden state of a recurrent neural architecture, i.e., hi = σ(Ahi−1 + Bwi ), (11) where σ is a non-linearity and wi is the ith character in w, hi−1 is the previous hidden state and A and B are matrices. While we have defined the architecture for a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al."
Q18-1003,D14-1179,0,0.013185,"Missing"
Q18-1003,P11-1004,0,0.0806345,"Missing"
Q18-1003,P14-2102,1,0.89922,"ix as σ 2 I, which is positive definite. representation (SR) w, the character string observed in raw text, and an underlying representation (UR), a character string with orthographic processes reversed. The aim of this factor is to place high weight on good pairs, e.g., the pair (w=questionably,u=questionablely), so we can accurately restore character-level changes. We encode this portion of the model as a weighted finite-state machine for ease of computation. This factor generalizes probabilistic edit distance (Ristad and Yianilos, 1998) by looking at additional input and output context; see Cotterell et al. (2014) for details. As mentioned above and in contrast to Cotterell et al. (2014), we bound the insertion limit in the edit distance model.8 Computing the score between two strings u and w requires a dynamic program that runs in O(|u|·|w|). This is a generalization of the forward algorithm for Hidden Markov Models (HMMs) (Rabiner, 1989). We employ standard feature templates for the task that look at features of edit operations, e.g., substitute i for y, in varying context granularities. See Cotterell et al. (2016b) for details. Recent work has also explored weighting of WFST arcs with scores compute"
Q18-1003,K15-1017,1,0.927378,"Missing"
Q18-1003,D16-1256,1,0.883949,"Missing"
Q18-1003,N16-1080,1,0.916571,"Missing"
Q18-1003,D11-1057,0,0.0361717,"Missing"
Q18-1003,N15-1184,0,0.0858587,"Missing"
Q18-1003,Q16-1001,0,0.0133792,"it is un∇θ log p(v, s, l, u |w) = f (s, l, u)&gt; + g(u, w)&gt; supervised and uses vectors as features, rather than 1 explicitly treating vector composition. All of the − 2 (v − Cβ (s, l))∇θ Cβ (s, l) σ above work focuses on surface segmentation and not − ∇θ log Zθ (w), (9) canonical segmentation, as we do. A related line of work that has different goals conwhere we use the importance sampling algorithm cerns morphological generation. Two recent papers described in §4.1 to approximate the gradient of that address this problem using deep learning are the log-partition function, following Bengio and Faruqui et al. (2016a) and Faruqui et al. (2016b). Senecal (2003). Note that ∇θ Cβ (s, l) depends on In an older line of work, Yarowsky and Wicenthe composition function used. In the most com- towski (2000) and Wicentowski (2002) exploit log plicated case when Cβ is a RNN, we can com- frequency ratios of inflectionally related forms to pute ∇β Cβ (s, l) efficiently with backpropagation tease apart that, e.g., the past tense of sing is not through time (Werbos, 1990). We take M = 10 im- singed, but instead sang. Related work by Dreyer portance samples; using so few samples can lead to a and Eisner (2011) uses a Di"
Q18-1003,N16-1077,0,0.0137907,"it is un∇θ log p(v, s, l, u |w) = f (s, l, u)&gt; + g(u, w)&gt; supervised and uses vectors as features, rather than 1 explicitly treating vector composition. All of the − 2 (v − Cβ (s, l))∇θ Cβ (s, l) σ above work focuses on surface segmentation and not − ∇θ log Zθ (w), (9) canonical segmentation, as we do. A related line of work that has different goals conwhere we use the importance sampling algorithm cerns morphological generation. Two recent papers described in §4.1 to approximate the gradient of that address this problem using deep learning are the log-partition function, following Bengio and Faruqui et al. (2016a) and Faruqui et al. (2016b). Senecal (2003). Note that ∇θ Cβ (s, l) depends on In an older line of work, Yarowsky and Wicenthe composition function used. In the most com- towski (2000) and Wicentowski (2002) exploit log plicated case when Cβ is a RNN, we can com- frequency ratios of inflectionally related forms to pute ∇β Cβ (s, l) efficiently with backpropagation tease apart that, e.g., the past tense of sing is not through time (Werbos, 1990). We take M = 10 im- singed, but instead sang. Related work by Dreyer portance samples; using so few samples can lead to a and Eisner (2011) uses a Di"
Q18-1003,N16-1101,0,0.0235884,"Missing"
Q18-1003,N16-1155,0,0.0140959,"a novel approach. Given a vector v for a word form w, we seek a function to minimize the following objective 1 ||v − hN ||22 , 2 (10) where hN is the final hidden state of a recurrent neural architecture, i.e., hi = σ(Ahi−1 + Bwi ), (11) where σ is a non-linearity and wi is the ith character in w, hi−1 is the previous hidden state and A and B are matrices. While we have defined the architecture for a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al., 2014b) and LSTMs (Hochreiter and Schmidhuber, 1997) as well as deep variants (Sutskever et al., 2014; Gillick et al., 2016; Firat et al., 2016). Importantly, this model has no knowledge of morphology—it can only rely on representations it extracts from the characters. This gives us a clear ablation on the benefit of adding structured morphological knowledge. We optimize the depth and the size of the hidden units on development data using a coarse-grained grid search. We found a depth of 2 and hidden units of size 100 (in both LSTM and GRU) performed best. We trained all models for 100 iterations of Adam (Kingma and Ba, 2015) with L2 regularization with regularization coefficient 0.01. Table 4 shows that the two c"
Q18-1003,P16-1020,0,0.0509962,"Missing"
Q18-1003,D16-1097,1,0.868472,"Missing"
Q18-1003,J94-3001,0,0.135854,"deviation is given in parentheses. We compare against two baselines that do not make use of semantic vectors: (i) “Semi-CRF (baseline)”, a semi-CRF that cannot account for orthographic changes and (ii) “Joint (Baseline)”, a version of our joint model without vectors. We also compare against an oracle version with access to gold URs (“Joint + UR (Oracle)”, “Joint + UR + Vec (Oracle)”), revealing that the toughest part of the canonical segmentation task is reversing the orthographic changes. calization as an orthographic analogue to phonology. On this interpretation, the finite-state systems of Kaplan and Kay (1994), which computationally apply SPE-style phonological rules (Chomsky and Halle, 1968), may be run backwards to get canonical underlying forms. 6 Experiments and Results We conduct experiments on English and German derivational morphology. We analyze our joint model’s ability to segment words into their canonical morphemes as well as its ability to compositionally derive vectors for new words. Finally, we explore the relationship between distributional semantics and morphological productivity. For English, we use the pretrained vectors of Levy and Goldberg (2014a) for all experiments. For German"
Q18-1003,W15-0108,0,0.171782,"Missing"
Q18-1003,W10-2211,0,0.0323643,"del, i.e., how much could we benefit from a richer model. Our hyperparameters are (i) the regularization coefficient λ and (ii) σ 2 , the variance of the Gaussian factor. We use grid search to tune them: λ ∈ {0.0, 101 , 102 , 103 , 104 , 105 }, σ 2 ∈ {0.25, 0.5, 0.75, 1.0}. Metrics. We use three metrics to evaluate segmentation accuracy. Note that the evaluation of canonical segmentation is hard since a system may return a sequence of morphemes whose concatenation is not the same length as the concatenation of the gold morphemes. This rules out metrics for surface segmentation like border F1 (Kurimo et al., 2010), which require the strings to be of the same length. We now define the metrics. (i) Segmentation accuracy measures whether every single canonical morpheme in the returned sequence is correct. It is inflexible: closer answers are penalized the same as 13 i.e., a model without the Gaussian factor that scores vectors. 41 more distant answers. (ii) Morpheme F1 (van den Bosch and Daelemans, 1999) takes the predicted sequence of canonical morphemes, turns it into a set, computes precision and recall in the standard way and based on that then computes F1 . This metric gives credit if some of the can"
Q18-1003,P13-1149,0,0.317863,"erived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness. In the computational literature, derivational morphology has received less attention than inflectional. There are, however, two bodies of work on derivation in computational linguistics. First, there is a series of papers that explore the relation between lexical semantics and derivation (Lazaridou et al., 2013; Zeller et al., 2014; Pad´o et al., 2015; Kisselew et al., 2015). All of these assume a gold morphological analysis and primarily focus on the effect of derivation on distributional semantics. The second body of work, e.g., the unsupervised morphological segmenter M ORFESSOR (Creutz and Lagus, 2007), does not deal with semantics and makes no distinction between inflectional and derivational morphology.3 Even though the boundary between inflectional and derivational morphology is a continuum rather than a rigid divide (Haspelmath and Sims, 2013), there is still the clear distinction that deriv"
Q18-1003,P14-2050,0,0.530231,"antics of morphemes and words allows us to improve morphological analysis. On the English portion of CELEX (Baayen et al., 1993), we achieve a 5 point improvement in segmentation accuracy and a 3 point improvement in morpheme F1 . On the German DErivBase dataset we achieve a 3 point improvement in segmentation accuracy and a 3 point improvement in morpheme F1 . • Second, we explore improved models of vector composition for synthesizing word meaning. We find a recurrent neural network improves over previously proposed additive models. Moreover, we find that more syntactically oriented vectors (Levy and Goldberg, 2014a) are better suited for morphology than bag-ofword (BOW) models. • Finally, we explore the productivity of English derivational affixes in the context of distributional semantics. 2 Derivational Morphology Two important goals of morphology, the linguistic study of the internal structure of words, are to describe the relation between different words in the lexicon and to decompose them into morphemes, the smallest linguistic unit bearing meaning. Morphology can be divided into two types: inflectional and derivational. Inflectional morphology is the set of processes through which the word form"
Q18-1003,P96-1004,0,0.455302,". Inflectional morphology is the set of processes through which the word form outwardly 34 displays syntactic information, e.g., verb tense. It follows that an inflectional affix typically neither changes the part-of-speech (POS) nor the semantics of the word. For example, the English verb to run takes various forms: run, runs, ran and running, all of which convey “moving by foot quickly”, but appear in complementary syntactic contexts. Derivation deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996). Consider the example of the English noun discontentedness, which is derived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness. In the computational literature, derivational morphology has received less attention than inflectional. There are, however, two bodies of work on derivation in computational linguistics. First, there is a series of pape"
Q18-1003,D15-1176,0,0.0357222,"vectors are annotated for relatedness, which is a proxy for semantic coherence. HR (highrelatedness) words were judged to be more compositional than LR (low-relatedness) words. Character-Level Neural Retrofitting. As a further strong baseline, we consider a retrofitting (Faruqui et al., 2015) approach based on characterlevel recurrent neural networks. Recently, running a recurrent net over the character stream has become a popular way of incorporating subword information into a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016). To the best of our knowledge, character-level retrofitting is a novel approach. Given a vector v for a word form w, we seek a function to minimize the following objective 1 ||v − hN ||22 , 2 (10) where hN is the final hidden state of a recurrent neural architecture, i.e., hi = σ(Ahi−1 + Bwi ), (11) where σ is a non-linearity and wi is the ith character in w, hi−1 is the previous hidden state and A and B are matrices. While we have defined the architecture for a vanilla RNN, we experiment with two more advanced recur"
Q18-1003,W13-3512,0,0.100424,"zation. for the semi-supervised incorporation of distributional semantics into a structured model of inflec4.3 Decoding tional paradigm completion. Decoding the model is also intractable. To approxiOur work is also related to recent attempts to inmate the solution, we again employ importance sam- tegrate morphological knowledge into general empling. We take M =10,000 importance samples and bedding models. For example, Botha and Blunselect the highest weighted sample. som (2014) train a log-bilinear language model that models the composition of morphological structure. 5 Related Work Likewise, Luong et al. (2013) train a recursive neural The idea that vector semantics is useful for mor- network (Goller and K¨uchler, 1996) over a heuristiphological segmentation is not new. Count vectors cally derived tree structure to learn morphological (Salton, 1971; Turney and Pantel, 2010) have been composition over continuous vectors. Our work is shown to be beneficial in the unsupervised induction different in that we learn a joint model of segmenof morphology (Schone and Jurafsky, 2000; Schone tation and composition. Moreover, supervised morand Jurafsky, 2001). Embeddings were shown to phological analysis can dr"
Q18-1003,P08-1028,0,0.0631841,".79 .78 .77 .57 .80 .81 .80 .74 .72 .67 .80 .81 .81 .75 .73 BOW2 Lazaridou all HR LR Table 4: Vector approximation (measured by mean cosine similarity) with gold morphology on the train/test split of Lazaridou et al. (2013). HR/LR = high/low-relatedness words. See Lazaridou et al. (2013) for details. rent neural networks are currently the most widely used nonlinear sequence model and simple RNNs are the simplest such models. Part of the motivation for considering a richer class of models lies in our removal of the twomorpheme assumption. Indeed, it is unclear that the wadd and fulladd models (Mitchell and Lapata, 2008) are useful models in the general case of multimorphemic words—the weights are tied by position, i.e., the first morpheme’s vector (be it a prefix or stem) is always multiplied by the same matrix. Comparison with Lazaridou et al. To compare with Lazaridou et al. (2013), we use their exact train/test split. Those results are reported in Table 4. This dataset enforces that all words are composed of 42 exactly two morphemes. Thus, a word like unquestionably is segmented as un+questionably, without further decomposition. The vectors employed by Lazaridou et al. (2013) are high-dimensional count ve"
Q18-1003,D14-1095,0,0.0566642,"Missing"
Q18-1003,Q15-1012,0,0.0837571,"makes no distinction between inflectional and derivational morphology.3 Even though the boundary between inflectional and derivational morphology is a continuum rather than a rigid divide (Haspelmath and Sims, 2013), there is still the clear distinction that derivation changes meaning whereas inflection does not. Our goal in this paper is to develop an account of how the meaning of a word form can be computed jointly, combining these two lines of work. Productivity and Semantic Coherence. We highlight two related issues in derivation that motivated the development of our model: productivity 3 Narasimhan et al. (2015) also make no distinction between inflectional and derivational morphology, but their model is an exception in that it includes vector similarity as a semantic feature. See §5 for discussion. and semantic coherence. Roughly, a productive affix is one that can still actively be employed to form new words in a language. For example, the English nominalizing affix ness (red7→red+ness) can be attached to just about any adjective, including novel forms. In contrast, the archaic English nominalizing affix th (dear7→dear+th, heal7→heal+th, steal7→steal+th) does not allow us to form new words such as"
Q18-1003,N16-1076,1,0.81264,"the edit distance model.8 Computing the score between two strings u and w requires a dynamic program that runs in O(|u|·|w|). This is a generalization of the forward algorithm for Hidden Markov Models (HMMs) (Rabiner, 1989). We employ standard feature templates for the task that look at features of edit operations, e.g., substitute i for y, in varying context granularities. See Cotterell et al. (2016b) for details. Recent work has also explored weighting of WFST arcs with scores computed by LSTMs (Hochreiter and Schmidhuber, 1997), obviating the need for human selection of feature templates (Rastogi et al., 2016). 3.2 Segmentation Factor The second factor  is the segmentation factor: &gt; exp f (s, l, u) η . The goal of this factor is to score a segmentation s of a UR u. In our example, it scores the input-output pair (u=questionablely, s=question+able+ly). It additionally scores a labeling of the segmentation. Our label set in this work is L = {stem, prefix, suffix}. The proper labeling of the segmentation above is l=question:stem+able:suffix+ly:suffix. The labeling is critical for our composition functions Cβ (Cotterell et al., 2015): which vectors are used depends on the label given to the segment; e"
Q18-1003,W13-3504,0,0.0197535,"d K¨uchler, 1996) over a heuristiphological segmentation is not new. Count vectors cally derived tree structure to learn morphological (Salton, 1971; Turney and Pantel, 2010) have been composition over continuous vectors. Our work is shown to be beneficial in the unsupervised induction different in that we learn a joint model of segmenof morphology (Schone and Jurafsky, 2000; Schone tation and composition. Moreover, supervised morand Jurafsky, 2001). Embeddings were shown to phological analysis can drastically outperform unsuact similarly (Soricut and Och, 2015). Our method pervised analysis (Ruokolainen et al., 2013). Early work by Kay (1977) can be interpreted as differs from this line of research in two key ways. (i) We present a probabilistic model of the pro- finite-state canonical segmentation, but it neither adcess of synthesizing the word’s meaning from the dresses nor experimentally evaluates the question of meaning of its morphemes. Prior work was ei- joint modeling of morphological analysis and sether not probabilistic or did not explicitly model mantic synthesis. Moreover, we may view canonito the joint model. Each of these distributions is tractable—we can compute the marginals with dynamic pr"
Q18-1003,W00-0712,0,0.571451,"aining locally normalized distributions for the individual components. Concretely, we train two proposal distributions q1 (u |w) and q2 (l, s |u) that take the form of a WFST and a semi-CRF, respectively, using features identical 11 The subvector β is responsible for computing only the mean of the Gaussian factor and thus has no impact on its normalization coefficient (Murphy, 2012). 12 Informally, the indirect importance sampling estimate converges to the true expectation as M → ∞ (the definition of statistical consistency). morphemes. (ii) Our method is supervised and focuses on derivation. Schone and Jurafsky (2000) and Soricut and Och (2015), being fully unsupervised, do not distinguish between inflection and derivation and Schone and Jurafsky (2001) focus on inflection. More recently, Narasimhan et al. (2015) 4.2 Learning look at the unsupervised induction of “morphologWe optimize the log-likelihood of the model using ical chains” with semantic vectors as a crucial feaA DAG RAD (Duchi et al., 2011), which is SGD with ture. Their goal is to jointly figure out an ordering a special per-parameter learning rate. The full gra- of word formation and a morphological segmentadient of the objective for one trai"
Q18-1003,N01-1024,0,0.0833015,", s |u) that take the form of a WFST and a semi-CRF, respectively, using features identical 11 The subvector β is responsible for computing only the mean of the Gaussian factor and thus has no impact on its normalization coefficient (Murphy, 2012). 12 Informally, the indirect importance sampling estimate converges to the true expectation as M → ∞ (the definition of statistical consistency). morphemes. (ii) Our method is supervised and focuses on derivation. Schone and Jurafsky (2000) and Soricut and Och (2015), being fully unsupervised, do not distinguish between inflection and derivation and Schone and Jurafsky (2001) focus on inflection. More recently, Narasimhan et al. (2015) 4.2 Learning look at the unsupervised induction of “morphologWe optimize the log-likelihood of the model using ical chains” with semantic vectors as a crucial feaA DAG RAD (Duchi et al., 2011), which is SGD with ture. Their goal is to jointly figure out an ordering a special per-parameter learning rate. The full gra- of word formation and a morphological segmentadient of the objective for one training example is: tion, e.g., play7→playful7→playfulness. While it is a rich model like ours, theirs differs in that it is un∇θ log p(v, s,"
Q18-1003,Q15-1026,0,0.0702613,"Missing"
Q18-1003,N15-1186,0,0.214251,"ibutions for the individual components. Concretely, we train two proposal distributions q1 (u |w) and q2 (l, s |u) that take the form of a WFST and a semi-CRF, respectively, using features identical 11 The subvector β is responsible for computing only the mean of the Gaussian factor and thus has no impact on its normalization coefficient (Murphy, 2012). 12 Informally, the indirect importance sampling estimate converges to the true expectation as M → ∞ (the definition of statistical consistency). morphemes. (ii) Our method is supervised and focuses on derivation. Schone and Jurafsky (2000) and Soricut and Och (2015), being fully unsupervised, do not distinguish between inflection and derivation and Schone and Jurafsky (2001) focus on inflection. More recently, Narasimhan et al. (2015) 4.2 Learning look at the unsupervised induction of “morphologWe optimize the log-likelihood of the model using ical chains” with semantic vectors as a crucial feaA DAG RAD (Duchi et al., 2011), which is SGD with ture. Their goal is to jointly figure out an ordering a special per-parameter learning rate. The full gra- of word formation and a morphological segmentadient of the objective for one training example is: tion, e.g."
Q18-1003,P99-1037,0,0.123687,"Missing"
Q18-1003,D14-1167,0,0.0335737,"is semantically compositional. Implicit in such a treatment is the desire to only segment a word if the segmentation is derived from a productive process. While most prior work on morphological segmentation has not explicitly modeled productivity,5 we believe, from a computational modeling perspective, segmenting only productive affixes is preferable. This is analogous to the modeling of phrase compositionality in embedding models, where it can be better to not further decompose noncompositional multiword units like named entities and idiomatic expressions; see, e.g., Mikolov et al. (2013b), Wang et al. (2014), Yin and Sch¨utze (2015), Yaghoobzadeh and Sch¨utze (2015), and Hashimoto and Tsuruoka (2016).6 In this paper, we refer to the semantic aspect of the model either as semantic synthesis or as coherence. These are two ways of looking at semantics that are related as follows. If the synthesis (i.e., composition) of the meaning of the derived form from the meaning of its parts is a regular application of the linguistic rules of derivation, then the meaning so constructed is coherent. These are the cases where a joint model is expected to be beneficial for both segmentation and interpretation. 3 A"
Q18-1003,D15-1083,1,0.784005,"Missing"
Q18-1003,P00-1027,0,0.370285,"Missing"
Q18-1003,N15-1091,1,0.843313,"Missing"
Q18-1003,P13-1118,0,0.0946051,"Missing"
Q18-1003,C14-1163,0,0.316639,"Missing"
Q18-1047,E17-1003,1,0.928732,"ramlevel hidden state, and the gate gi sets a trade-off between the unigram-level input ui and the temporary output oi at the phrase-level. We elaborate these modules in the remainder of this subsection. Attention Source. First, we present a general instance of generating source of attention by function fmgran (H), learning word representations in multi-granular context. In our system, we consider granularities 1 and 3, corresponding to unigram hidden state and trigram hidden state. For the uni-hidden state case, it is a gated convolution layer: hxuni,i = fgconv (hxi ) (10) Advanced ATT C ONV Adel and Schütze (2017) distinguish between focus and source of attention. The focus of atten691 tion and a similar process of computing context vectors, but differs in three ways. (i) The discrimination of attention source, focus, and beneficiary improves expressivity. (ii) In CNNs, the surrounding hidden states for a concrete position are available, so the attention matching is able to encode the left context as well as the right context. In RNNs, however, we need bidirectional RNNs to yield both left and right context representations. (iii) As attentive convolution can be implemented by summing up two separate co"
Q18-1047,N18-2017,0,0.0174559,"ourcing. Accuracy is reported. Table 1 shows examples and Table 4 gives statistics. By this construction, a substantial performance improvement on S CI TAIL is equivalent to a better QA performance (Khot et al., 2018). The hypothesis tx is the target sentence, and the premise ty acts as its context. ATT C ONV on SNLI. Table 7 shows the comparison. We observe that: (i) classifying hypotheses without looking at premises, that is, “w/o context” baseline, results in a large improvement over the “majority baseline.” This verifies the strong bias in the hypothesis construction of the SNLI data set (Gururangan et al., 2018; Poliak et al., 2018). (ii) ATT C ONV (advanced) surpasses Baselines. Apart from the common baselines (see Section 4.1), we include systems covered by Khot et al. (2018): (i) n-gram Overlap: An overlap baseline, considering lexical granularity 695 # (Premise ty , Hypothesis tx ) Pair (t ) These insects have 4 life stages, the egg, larva, pupa, and adult. (tx ) The sequence egg −&gt; larva −&gt; pupa −&gt; adult shows the life cycle of some insects. (ty ) . . . the notochord forms the backbone (or vertebral column). (tx ) Backbone is another name for the vertebral column. (ty ) Water lawns early in the"
Q18-1047,D15-1075,0,0.0550758,"Missing"
Q18-1047,P17-1171,0,0.0772014,"m in RNNs. inter-hidden-state match (Bahdanau et al., 2015; Luong et al., 2015; Kim et al., 2017; Libovický and Helcl, 2017), response generation in social media (Shang et al., 2015), document reconstruction (Li et al., 2015), and document summarization (Nallapati et al., 2016); machine comprehension (Hermann et al., 2015; Kumar et al., 2016; Xiong et al., 2016; Seo et al., 2017; Wang and Jiang, 2017; Xiong et al., 2017; Wang et al., 2017a); and sentence relation classification, such as textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; Chen et al., 2017b) and answer sentence selection (Miao et al., 2016). We try to explore the RNN-style attention mechanisms in CNNs—more specifically, in convolution. 2.2 hidden states layer X convolution Y convolution word embedding layer x sentence t y sentence t Figure 3: Attentive pooling, summarized from ABCNN (Yin et al., 2016) and APCNN (dos Santos et al., 2016). hidden states generated by a convolution layer; then, each sentence will learn a weight for every hidden state by comparing this hidden state with all hidden states in the other sentence; finally, each input sentence obtains a representation by"
Q18-1047,P14-1062,0,0.018502,"current input state, (ii) a representation of local context (computed unidirectionally or bidirectionally; Rocktäschel et al. [2016]), and (iii) the attention-weighted sum of hidden states corresponding to nonlocal context (e.g., the hidden states of the encoder in neural machine translation; Bahdanau et al. [2015]). An important question, therefore, is whether CNNs can benefit from such an attention mechanism as well, and how. This is our technical motivation. Our second motivation is natural language understanding. In generic sentence modeling without extra context (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014), CNNs learn sentence representations by composing word representations that are conditioned on a local context window. We believe that attentive convolution is needed 687 Transactions of the Association for Computational Linguistics, vol. 6, pp. 687–702, 2018. Action Editor: Slav Petrov. Submission batch: 6/2018; Revision batch: 10/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. premise, modeled as context ty Plant cells have structures that animal cells lack. Animal cells do not have cell walls. The cell wall is no"
Q18-1047,D14-1181,0,0.00537862,"a representation of local context (computed unidirectionally or bidirectionally; Rocktäschel et al. [2016]), and (iii) the attention-weighted sum of hidden states corresponding to nonlocal context (e.g., the hidden states of the encoder in neural machine translation; Bahdanau et al. [2015]). An important question, therefore, is whether CNNs can benefit from such an attention mechanism as well, and how. This is our technical motivation. Our second motivation is natural language understanding. In generic sentence modeling without extra context (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014), CNNs learn sentence representations by composing word representations that are conditioned on a local context window. We believe that attentive convolution is needed 687 Transactions of the Association for Computational Linguistics, vol. 6, pp. 687–702, 2018. Action Editor: Slav Petrov. Submission batch: 6/2018; Revision batch: 10/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. premise, modeled as context ty Plant cells have structures that animal cells lack. Animal cells do not have cell walls. The cell wall is not a freestan"
Q18-1047,P16-2022,0,0.0445596,"Missing"
Q18-1047,E17-1038,0,0.0525947,"Missing"
Q18-1047,D16-1244,0,0.0523478,"Missing"
Q18-1047,P15-1107,0,0.0279115,"s idea has been broadly explored in RNNs, shown in Figure 2, to deal with text generation, such as neural machine translation 688 weighted sum attentive context x t y representation : X ⋅ softmax( t ) representation : Y ⋅ softmax( column-wise compose ) row-wise compose hidden states matching scores y sentence t x (4 × 6) sentence t Figure 2: A simplified illustration of attention mechanism in RNNs. inter-hidden-state match (Bahdanau et al., 2015; Luong et al., 2015; Kim et al., 2017; Libovický and Helcl, 2017), response generation in social media (Shang et al., 2015), document reconstruction (Li et al., 2015), and document summarization (Nallapati et al., 2016); machine comprehension (Hermann et al., 2015; Kumar et al., 2016; Xiong et al., 2016; Seo et al., 2017; Wang and Jiang, 2017; Xiong et al., 2017; Wang et al., 2017a); and sentence relation classification, such as textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; Chen et al., 2017b) and answer sentence selection (Miao et al., 2016). We try to explore the RNN-style attention mechanisms in CNNs—more specifically, in convolution. 2.2 hidden states layer X convolution Y convolution word e"
Q18-1047,D14-1162,0,0.0809972,"olve attentive pooling (dos Santos et al., 2016; Yin et al., 2016); namely, the weights of the post-convolution pooling layer are determined by attention. These weights come from the matching process between hidden states of two text pieces. However, a weight value is not informative enough to tell the relationships between aligned terms. Consider a textual entailment sentence pair for which we need to determine whether “inside −→ outside” holds. The matching degree (take cosine similarity as example) of these two words is high: for example, ≈ 0.7 in Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). On the other hand, the matching score between “inside” and “in” is lower: 0.31 in Word2Vec, 0.46 in GloVe. Apparently, the higher number 0.7 does not mean that “outside” is more likely than “in” to be entailed by “inside.” Instead, joint representations for aligned phrases [hinside , houtside ], [hinside , hin ] are more informative and enable finer-grained reasoning than a mechanism that can only transmit information downstream by matching scores. We modify the conventional CNN filters so that “inside” can make the entailment decision by looking at the representation of the counterpart term"
Q18-1047,P17-2031,0,0.030482,"introduced a differentiable attention mechanism that allows RNNs to focus on different parts of the input. This idea has been broadly explored in RNNs, shown in Figure 2, to deal with text generation, such as neural machine translation 688 weighted sum attentive context x t y representation : X ⋅ softmax( t ) representation : Y ⋅ softmax( column-wise compose ) row-wise compose hidden states matching scores y sentence t x (4 × 6) sentence t Figure 2: A simplified illustration of attention mechanism in RNNs. inter-hidden-state match (Bahdanau et al., 2015; Luong et al., 2015; Kim et al., 2017; Libovický and Helcl, 2017), response generation in social media (Shang et al., 2015), document reconstruction (Li et al., 2015), and document summarization (Nallapati et al., 2016); machine comprehension (Hermann et al., 2015; Kumar et al., 2016; Xiong et al., 2016; Seo et al., 2017; Wang and Jiang, 2017; Xiong et al., 2017; Wang et al., 2017a); and sentence relation classification, such as textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; Chen et al., 2017b) and answer sentence selection (Miao et al., 2016). We try to explore the RNN-style attention mechanisms"
Q18-1047,N18-1202,0,0.0518198,"Missing"
Q18-1047,N16-1170,0,0.0466326,"plified illustration of attention mechanism in RNNs. inter-hidden-state match (Bahdanau et al., 2015; Luong et al., 2015; Kim et al., 2017; Libovický and Helcl, 2017), response generation in social media (Shang et al., 2015), document reconstruction (Li et al., 2015), and document summarization (Nallapati et al., 2016); machine comprehension (Hermann et al., 2015; Kumar et al., 2016; Xiong et al., 2016; Seo et al., 2017; Wang and Jiang, 2017; Xiong et al., 2017; Wang et al., 2017a); and sentence relation classification, such as textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; Chen et al., 2017b) and answer sentence selection (Miao et al., 2016). We try to explore the RNN-style attention mechanisms in CNNs—more specifically, in convolution. 2.2 hidden states layer X convolution Y convolution word embedding layer x sentence t y sentence t Figure 3: Attentive pooling, summarized from ABCNN (Yin et al., 2016) and APCNN (dos Santos et al., 2016). hidden states generated by a convolution layer; then, each sentence will learn a weight for every hidden state by comparing this hidden state with all hidden states in the other sentence; finally, each inp"
Q18-1047,P17-1018,0,0.01242,": Y ⋅ softmax( column-wise compose ) row-wise compose hidden states matching scores y sentence t x (4 × 6) sentence t Figure 2: A simplified illustration of attention mechanism in RNNs. inter-hidden-state match (Bahdanau et al., 2015; Luong et al., 2015; Kim et al., 2017; Libovický and Helcl, 2017), response generation in social media (Shang et al., 2015), document reconstruction (Li et al., 2015), and document summarization (Nallapati et al., 2016); machine comprehension (Hermann et al., 2015; Kumar et al., 2016; Xiong et al., 2016; Seo et al., 2017; Wang and Jiang, 2017; Xiong et al., 2017; Wang et al., 2017a); and sentence relation classification, such as textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; Chen et al., 2017b) and answer sentence selection (Miao et al., 2016). We try to explore the RNN-style attention mechanisms in CNNs—more specifically, in convolution. 2.2 hidden states layer X convolution Y convolution word embedding layer x sentence t y sentence t Figure 3: Attentive pooling, summarized from ABCNN (Yin et al., 2016) and APCNN (dos Santos et al., 2016). hidden states generated by a convolution layer; then, each sentence w"
Q18-1047,P15-1152,0,0.0319574,"to focus on different parts of the input. This idea has been broadly explored in RNNs, shown in Figure 2, to deal with text generation, such as neural machine translation 688 weighted sum attentive context x t y representation : X ⋅ softmax( t ) representation : Y ⋅ softmax( column-wise compose ) row-wise compose hidden states matching scores y sentence t x (4 × 6) sentence t Figure 2: A simplified illustration of attention mechanism in RNNs. inter-hidden-state match (Bahdanau et al., 2015; Luong et al., 2015; Kim et al., 2017; Libovický and Helcl, 2017), response generation in social media (Shang et al., 2015), document reconstruction (Li et al., 2015), and document summarization (Nallapati et al., 2016); machine comprehension (Hermann et al., 2015; Kumar et al., 2016; Xiong et al., 2016; Seo et al., 2017; Wang and Jiang, 2017; Xiong et al., 2017; Wang et al., 2017a); and sentence relation classification, such as textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; Chen et al., 2017b) and answer sentence selection (Miao et al., 2016). We try to explore the RNN-style attention mechanisms in CNNs—more specifically, in convolution. 2.2 hidden stat"
Q18-1047,N18-1074,0,0.129289,") context. (ii) For extra-context, wordnew is generated to represent word, together with its cross-text alignment attcontext , in the context leftcontext and rightcontext . In other words, the decision for the word is made based on the connected hidden states of cross-text aligned terms, with local context. We apply ATT C ONV to three sentence modeling tasks with variable-size context: a largescale Yelp sentiment classification task (Lin et al., 2017) (intra-context, i.e., no additional context), S CI TAIL textual entailment (Khot et al., 2018) (single extra-context), and claim verification (Thorne et al., 2018) (multiple extra-contexts). ATT C ONV outperforms competitive DNNs with and without attention and achieves state-of-the-art on the three tasks. Overall, we make the following contributions: • This is the first work that equips convolution filters with the attention mechanism commonly used in RNNs. • We distinguish and build flexible modules— attention source, attention focus, and attention beneficiary—to greatly advance the expressivity of attention mechanisms in CNNs. • ATT C ONV provides a new way to broaden the originally constrained scope of filters in conventional CNNs. Broader and richer"
Q18-1047,Q16-1019,1,0.927953,"2015; Kumar et al., 2016; Xiong et al., 2016; Seo et al., 2017; Wang and Jiang, 2017; Xiong et al., 2017; Wang et al., 2017a); and sentence relation classification, such as textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; Chen et al., 2017b) and answer sentence selection (Miao et al., 2016). We try to explore the RNN-style attention mechanisms in CNNs—more specifically, in convolution. 2.2 hidden states layer X convolution Y convolution word embedding layer x sentence t y sentence t Figure 3: Attentive pooling, summarized from ABCNN (Yin et al., 2016) and APCNN (dos Santos et al., 2016). hidden states generated by a convolution layer; then, each sentence will learn a weight for every hidden state by comparing this hidden state with all hidden states in the other sentence; finally, each input sentence obtains a representation by a weighted mean pooling over all its hidden states. The core component—weighted mean pooling— was referred to as “attentive pooling,” aiming to yield the sentence representation. In contrast to attentive convolution, attentive pooling does not connect directly the hidden states of cross-text aligned phrases in a fin"
S10-1018,H05-1004,0,0.0911077,"ifiers, the best results, which are reported in Section 3, were achieved with the Decision-Tree. 2.5 Decoding In decoding, the coreference chains are created. SUCRE uses best-first clustering for this purpose. It searches for the best predicted antecedent from right-to-left starting from the end of the document. 3 Results Table 2 shows the results of SUCRE and the best competitor system on the test portions of the six languages from SemEval-2010 Task 1. Four different evaluation metrics were used to rank the participating systems: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, in prep). SUCRE has the best results in regular closed annotation track of English and German (for all metrics). Its results for gold closed annotation track of both English and German are the best in MUC and BLANC scoring metrics (MUC: English +27.1 German +32.5, BLANC: English +9.5 German +9.0) and for CEAF and B3 (CEAF: English -1.3 German -4.8, B3 : English -2.1 German -4.8); in comparison to the second ranked system, the performance is clearly better in the first case and slightly better in the second. This result shows that SUCRE has been optimized in a way"
S10-1018,P02-1014,0,0.062473,"publicly available systems that perform coreference resolution, such as BART (Versley et al., 2008) and GUITAR (Steinberger et al., 2007). A considerable engineering effort is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. Thus, a system which is able to extract the features based on a feature definition language can help the researcher reduce the implementation effort needed for feature extraction. Most methods of coreference resolution, if providing a baseline, usually use a feature set similar to (Soon et al., 2001) or (Ng and Cardie, 2002) 2 Architecture The architecture of SUCRE has two main parts: preprocessing and coreference resolution. In preprocessing the text corpus is converted to a relational database model. These are the main functionalities in this stage: 1. Preliminary text conversion 2. Extracting atomic word features 3. Markable detection 92 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 92–95, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Column Characteristic optional (the system can also work without them). It is obvious that the raw te"
S10-1018,W09-2411,0,0.315644,"Missing"
S10-1018,J01-4004,0,0.885531,"rent. There are various publicly available systems that perform coreference resolution, such as BART (Versley et al., 2008) and GUITAR (Steinberger et al., 2007). A considerable engineering effort is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. Thus, a system which is able to extract the features based on a feature definition language can help the researcher reduce the implementation effort needed for feature extraction. Most methods of coreference resolution, if providing a baseline, usually use a feature set similar to (Soon et al., 2001) or (Ng and Cardie, 2002) 2 Architecture The architecture of SUCRE has two main parts: preprocessing and coreference resolution. In preprocessing the text corpus is converted to a relational database model. These are the main functionalities in this stage: 1. Preliminary text conversion 2. Extracting atomic word features 3. Markable detection 92 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 92–95, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Column Characteristic optional (the system can also work without them). It i"
S10-1018,P08-4003,0,0.022495,"btained the best results in several categories, including the regular closed annotation tracks of English and German. 1 Introduction In this paper, we introduce a new software tool for coreference resolution. Coreference resolution is the process of finding discourse entities (markables) referring to the same real-world entity or concept. In other words, this process groups the markables of a document into equivalence classes (coreference entities) so that all markables in an entity are coreferent. There are various publicly available systems that perform coreference resolution, such as BART (Versley et al., 2008) and GUITAR (Steinberger et al., 2007). A considerable engineering effort is needed for the full coreference resolution task, and a significant part of this effort concerns feature engineering. Thus, a system which is able to extract the features based on a feature definition language can help the researcher reduce the implementation effort needed for feature extraction. Most methods of coreference resolution, if providing a baseline, usually use a feature set similar to (Soon et al., 2001) or (Ng and Cardie, 2002) 2 Architecture The architecture of SUCRE has two main parts: preprocessing and"
S10-1018,M95-1005,0,0.460304,"mum-Entropy (Tsuruoka, 2006). When we compared these classifiers, the best results, which are reported in Section 3, were achieved with the Decision-Tree. 2.5 Decoding In decoding, the coreference chains are created. SUCRE uses best-first clustering for this purpose. It searches for the best predicted antecedent from right-to-left starting from the end of the document. 3 Results Table 2 shows the results of SUCRE and the best competitor system on the test portions of the six languages from SemEval-2010 Task 1. Four different evaluation metrics were used to rank the participating systems: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, in prep). SUCRE has the best results in regular closed annotation track of English and German (for all metrics). Its results for gold closed annotation track of both English and German are the best in MUC and BLANC scoring metrics (MUC: English +27.1 German +32.5, BLANC: English +9.5 German +9.0) and for CEAF and B3 (CEAF: English -1.3 German -4.8, B3 : English -2.1 German -4.8); in comparison to the second ranked system, the performance is clearly better in the first case and slightly better in the second. This res"
S10-1018,S10-1001,0,\N,Missing
S15-2088,D11-1052,0,0.0226662,"sheer amount of possible sequences increases the OOV rate dramatically. Therefore, all sequences of punctuations are replaced by a list of distinct punctuations in this sequence (e.g., “!?!?” is replaced by “[!?]”). That reduces the OOV rate and still keeps most of the information. Mohammad et al. (2013) showed that sentiment lexicons are crucial for achieving good polarity classification. Unfortunately, miss-spellings and elongated surface forms of sentiment-bearing tokens, such as “cooooolllll”, lead to lower coverage of all sentiment lexicons. Since elongated words often convey sentiment (Brody and Diakopoulos, 2011), we carefully normalize them in the following way. First, all elongated words are identified by searching for tokens that contain a sequence of at least three equal characters. Afterwards, for each elongated word a candidate set is created by removing the repeated character one by one until only one occurrence is left. If a word contains several repeated character sequences, all combinations are taken as candidates. For instance, the candidate set of the word “cooolll” will be {coolll, colll, cooll, coll, cool, col}. We then search every candidate in a sentiment lexicon to find the correct ca"
S15-2088,P14-1062,0,0.0294958,"om 2013 and 2014, but worse on 2015. Even worse results are achieved on the sarcasm data. However, the results should be taken with care, because this sub set is very small. 5 F1,neutral Related Work One early work that used CNNs to model sentences was published by Collobert et al. (2011). They used one convolution layer followed by a max pooling layer to create a sentence representation. We extend their method by incorporating additional features focused on the polarity classification task. In contrast 531 to their approach, we do not embed our external features, but make direct use of them. Kalchbrenner et al. (2014) show that a CNN for modeling sentences can achieve competitive results in polarity classification. Among others, they introduce dynamic k-max pooling, a method that adapts max pooling to the length of an input sentence. Compared to their work we use a simpler architecture of the CNN without max-pooling, because this technique did not show any improvements in our experiments. Furthermore, we use the same filter for each dimension to reduce the number of parameters, whereas their model uses a different filter per dimension. Finally, our CNN model is combined with another classifier to produce t"
S15-2088,S13-2053,0,0.284029,"entions are replaced by “<user&gt;” and all urls by “<web&gt;”, because they do not provide any cues of polarity. We do not replace hashtags, because they often contain valuable information such as topics or even sentiment. Punctuation sequences like “!?!?” can act as exaggeration or other polarity modifier. However, the sheer amount of possible sequences increases the OOV rate dramatically. Therefore, all sequences of punctuations are replaced by a list of distinct punctuations in this sequence (e.g., “!?!?” is replaced by “[!?]”). That reduces the OOV rate and still keeps most of the information. Mohammad et al. (2013) showed that sentiment lexicons are crucial for achieving good polarity classification. Unfortunately, miss-spellings and elongated surface forms of sentiment-bearing tokens, such as “cooooolllll”, lead to lower coverage of all sentiment lexicons. Since elongated words often convey sentiment (Brody and Diakopoulos, 2011), we carefully normalize them in the following way. First, all elongated words are identified by searching for tokens that contain a sequence of at least three equal characters. Afterwards, for each elongated word a candidate set is created by removing the repeated character on"
S15-2088,N13-1039,0,0.0627824,"Missing"
S15-2088,W02-1011,0,0.0186085,"e. Compared to their work we use a simpler architecture of the CNN without max-pooling, because this technique did not show any improvements in our experiments. Furthermore, we use the same filter for each dimension to reduce the number of parameters, whereas their model uses a different filter per dimension. Finally, our CNN model is combined with another classifier to produce the final polarity label. Using an SVM for polarity classification is a common approach. One of the first polarity classification systems used bag-of-words features and an SVM to classify the polarity of movie reviews (Pang et al., 2002). The winning system of SemEval 2013 and SemEval 2014 also used an SVM with many different features (Mohammad et al., 2013). We implemented their most helpful features, which is bagof-words and lexicon features and added the CNN output as an additional feature to improve the final performance. 6 Conclusion This paper summarizes the features of our automatic sentiment analysis system – CIS-positive – for the SemEval 2015 shared task - Task 10, subtask B. We carefully normalize the Twitter data and integrate the output of convolutional neural networks into support vector machines for the polarit"
S15-2088,S15-2078,0,0.0230011,"g due to many different factors, such as ambiguous word senses, context dependency, sarcasm, etc. Specific properties of Twitter text make this task even more challenging. The limit of 140 character per message leads to countless acronyms and abbreviations. Moreover, the vast majority of tweets is of informal character and contains intentional miss-spellings and wrong use of grammar. Hence, the out-of-vocabulary (OOV) rate of Twitter text is rather high, which leads to information loss. One of the SemEval 2015 shared tasks – Task 10: Sentiment Analysis in Twitter – addresses these challenges (Rosenthal et al., 2015). We participated in Subtask B the “Message Polarity Classification” task. The goal is to predict the polarity of a given tweet into positive, negative, or neutral. The task organizers provided tweet IDs and corresponding labels to have a common ground for training polarity classification systems. More information about the task, its other subtasks as well as information about how the data was selected can be found in (Rosenthal et al., 2015). In this paper, we present our sentiment analysis system for SemEval 2015 - Task 10. Our system addresses the above mentioned challenges in two ways. Fir"
S15-2088,D13-1140,0,0.0227879,"ow many elements 529 the filter spans. Applying this concept to a two dimensional input leads to a convolution matrix where the elements are computed by Ci,j = mT Si,j:j+m−1 , where i is the ith row in S and j is the start index of the convolution. A, the output of the convolution layer is computed by an element-wise addition of a bias term (one bias per row) and an element-wise non-linearity: A = f (C + b). As non-linear function we use a rectified linear unit: f (x) = max(0, x). This non-linearity proved to be a crucial part in object recognition (Jarrett et al., 2009), machine translation (Vaswani et al., 2013), and speech recognition (Zeiler et al., 2013). Our model uses two layers of convolution. The concatenation of all rows of the second convolution layer output is the input to a sequence of three fully connected hidden layers. A hidden layer transforms the input vector x into z = f (W x + b), where W is a weight matrix that is learned during training and b is a bias. In order to convert the final hidden layer output z into a probability distribution over polarity labels o ∈ R3 , the softmax function is used: oi = Pexp(zi ) . exp(z ) j j Pretraining of Word Embeddings The standard way of initial"
S15-2088,H05-1044,0,0.23017,"olution layers SVM hidden layers confidence softmax probabilities SVM final label Figure 1: System architecture the shortest match is taken. Since several sentiment lexicons with different qualities exist, we apply a sequential approach. We search the canonical form of the elongated word in one lexicon. If it does not exist, the next lexicon in the sequence is searched. The sequence of sentiment lexicons is sorted based on the reliability of the lexicon. Manually created lexicons precede automatically created lexicons. In this paper, the ordering is as follows: MPQA subjectivity cues lexicon (Wilson et al., 2005), Opinion lexicon (Hu and Liu, 2004), NRCC Emotion lexicon (Mohammad and Turney, 2013), sentiment 140, and Hashtag lexicon (both in (Mohammad et al., 2013)). As a result, a mapping from elongated words to their canonical form is found and used to normalize the corpus. Lowercasing finalizes the preprocessing step. 3 Model The system architecture consists of three main components and is depicted in Figure 1. The first component is a CNN (left part in the figure), which makes use of the sequence of all words in a tweet. The second component is an SVM classifier which uses several linguistic featu"
scheible-schutze-2012-bootstrapping,N03-5008,0,\N,Missing
scheible-schutze-2012-bootstrapping,H05-1044,0,\N,Missing
scheible-schutze-2012-bootstrapping,J11-2001,0,\N,Missing
scheible-schutze-2012-bootstrapping,C10-2127,1,\N,Missing
scheible-schutze-2012-bootstrapping,P02-1053,0,\N,Missing
scheible-schutze-2012-bootstrapping,P10-1041,0,\N,Missing
scheible-schutze-2012-bootstrapping,P07-1056,0,\N,Missing
schwarz-etal-2010-identification,H91-1025,0,\N,Missing
W06-2913,J93-2004,0,\N,Missing
W06-2913,J04-4004,0,\N,Missing
W06-2913,P03-1054,0,\N,Missing
W06-2913,P91-1030,0,\N,Missing
W06-2913,J05-1003,0,\N,Missing
W06-2913,P05-1022,0,\N,Missing
W09-0203,W02-0908,0,0.0994771,"d in the following two subsections. Thus, an occurrence of any path, irrespective of length or grammatical relations that are involved, increases the count of the respective basis element by one. We implemented three different association functions, A, to transform the raw frequency counts and weight the influence of the different cooccurrences. We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+ ) are kept following the results of Curran and Moens (2002). We also experimented with different frequency cutoffs removing dimensions that occur very frequently or very rarely. 3.1 Results for 214 nouns from Almuhareb and Poesio (2004) The first set we worked with was introduced by Almuhareb and Poesio (2004) and consists of 214 nouns from 13 different categories in WordNet. In the original paper the best results were achieved with vector representations built from concept attributes and their values as identified by simple patterns. For the identification of attribute values of a concept C the following pattern was used “[a|an|the] * C [is|was]” It"
W09-0203,J93-1003,0,0.0518264,"illustrative purposes and have no impact on the word 19 of sentences found in ukWaC. vectors. The examples are slightly simplified versions sets we used are described in the following two subsections. Thus, an occurrence of any path, irrespective of length or grammatical relations that are involved, increases the count of the respective basis element by one. We implemented three different association functions, A, to transform the raw frequency counts and weight the influence of the different cooccurrences. We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+ ) are kept following the results of Curran and Moens (2002). We also experimented with different frequency cutoffs removing dimensions that occur very frequently or very rarely. 3.1 Results for 214 nouns from Almuhareb and Poesio (2004) The first set we worked with was introduced by Almuhareb and Poesio (2004) and consists of 214 nouns from 13 different categories in WordNet. In the original paper the best results were achieved with vector representations built from concept attri"
W09-0203,P90-1034,0,0.448611,"instay in the automatic acquisition of lexical semantic knowledge. The computation of semantic relatedness of two words in such models is based on their distributional similarity. The most crucial way in which such models differ is the definition of distributional similarity: In a regular word space model the observed distribution concerns the immediate neighbours of a word within a predefined window to the left and right (Schütze, 1992; Sahlgren, 2006). Early on in the development as an alternative models were proposed that relied on the similarity of the distribution of syntactic relations (Hindle, 1990; Padó and Lapata, 2007). More recently the distribution of the occurrence within simple patterns defined in the form of regular expressions that are supposed to capture explicit semantic relations was explored as the basis of distributional similarity (Almuhareb and Poesio, 2004). Whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems (Padó and Lapata, 2007; Lin, 1998), for the task of categorisation simple pattern based spaces have been shown to Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language S"
W09-0203,P98-2127,0,0.824829,"he same time we want to show that such a system can be built with freely available components and without the need to rely on the index of a proprietary search engine vendor. We propose to use the web acquired data of the ukWaC (Ferraresi et al., 2008), which is huge but still manageable and comes in a pre-cleaned version with HTML markup removed. It can easily be fed into a parser like MiniPar which allows for the subsequent extraction of dependency relations of different types and complexity. In particular we work with dependency paths that can reach beyond direct dependencies as opposed to Lin (1998) but in the line of Pado and Lapata (2007). In contrast to the latter, however, different paths that end in the same word are not generally mapped to the same dimension in our model. A path in a dependency graph can pass through several nodes and encompass different relations. We experimented with two sets of nouns previously used in the literature for word clustering. The nouns in both sets are taken from a number of different WordNet categories. Hence, the task consists in clustering together the words from the same category. By keeping the clustering algorithm constant, differences in perfo"
W09-0203,J07-2002,0,0.710386,"automatic acquisition of lexical semantic knowledge. The computation of semantic relatedness of two words in such models is based on their distributional similarity. The most crucial way in which such models differ is the definition of distributional similarity: In a regular word space model the observed distribution concerns the immediate neighbours of a word within a predefined window to the left and right (Schütze, 1992; Sahlgren, 2006). Early on in the development as an alternative models were proposed that relied on the similarity of the distribution of syntactic relations (Hindle, 1990; Padó and Lapata, 2007). More recently the distribution of the occurrence within simple patterns defined in the form of regular expressions that are supposed to capture explicit semantic relations was explored as the basis of distributional similarity (Almuhareb and Poesio, 2004). Whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems (Padó and Lapata, 2007; Lin, 1998), for the task of categorisation simple pattern based spaces have been shown to Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 17–24, c"
W09-0203,W05-1003,0,0.317593,"Missing"
W09-0203,W04-3221,0,0.308413,"imilarity: In a regular word space model the observed distribution concerns the immediate neighbours of a word within a predefined window to the left and right (Schütze, 1992; Sahlgren, 2006). Early on in the development as an alternative models were proposed that relied on the similarity of the distribution of syntactic relations (Hindle, 1990; Padó and Lapata, 2007). More recently the distribution of the occurrence within simple patterns defined in the form of regular expressions that are supposed to capture explicit semantic relations was explored as the basis of distributional similarity (Almuhareb and Poesio, 2004). Whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems (Padó and Lapata, 2007; Lin, 1998), for the task of categorisation simple pattern based spaces have been shown to Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 17–24, c Athens, Greece, 31 March 2009. 2009 Association for Computational Linguistics 17 modifies (appearing as relation IX in Figure 1) as a close approximation of the pattern used by (Almuhareb and Poesio, 2004) to identify attributes of a concept as detailed in"
W09-1902,W05-0619,0,0.0305953,"ally bad seed set. MIN serves to demonstrate the opposite case. For each type of seed set, we sampled ten independent versions to calculate averages over several AL runs. 13 Cost measure The success of AL is usually measured as reduction of annotation effort according to some cost measure. Traditionally, the most common cost measure considers a unit cost per annotated token, which favors AL systems that select individual tokens. In a real annotation setting, however, it is unnatural, and therefore hard for humans to annotate single, possibly isolated tokens, leading to bad annotation quality (Hachey et al., 2005; Ringger et al., 2007). When providing context, the question arises whether the annotator can label several tokens present in the context (e.g., an entire multi-token entity or even the whole sentence) at little more cost than annotating a single token. Thus, assigning a linear cost of n to a sentence where n is the sentence’s length in tokens seems to unfairly disadvantage sentenceselection AL setups. However, more work is needed to find a more realistic cost measure. At present there is no other generally accepted cost measure than unit cost per token, so we report costs using the token mea"
W09-1902,W04-3111,0,0.017281,"f neighboring tokens to the left and right of the current token. 3.2 Data sets We used three data sets in our experiments. Two of them (ACE and PB IO) are standard data sets. The third (S YN) is a synthetic set constructed to have specific characteristics. For simplicity, we consider only scenarios with two entity classes, a majority class (MAJ) and a minority class (MIN). We discarded all other entity annotations originally contained in the corpus assigning the OUTSIDE class.2 The first data set (PB IO) is based on the annotations of the P ENN B IO IE corpus for biomedical entity extraction (Kulick et al., 2004). As P ENN B IO IE makes fine-grained and subtle distinctions between various subtypes of classes irrelevant for this study, we combined several of the original classes into two entity classes: The majority class consists of the three original classes ‘gene-protein’, ‘gene-generic’, and ‘gene-rna’. The minority class consists of the original and similar classes ‘variation-type’ and 2 The OUTSIDE class marks that a token is not part of an named entity. 12 ’variation-event’. All other entity labels were replaced by the OUTSIDE class. The second data set (ACE) is based on the newswire section of"
W09-1902,W07-1516,0,0.274433,"equence learning problems including, e.g., POS tagging, and named entity recognition. Sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences. Algorithms for sequence learning obviously work on sequence data, so respective AL approaches need to select complete sequences instead of single text tokens (Settles and Craven, 2008). Furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (Ringger et al., 2007; Tomanek et al., 2007). Within such sequences, instances of different classes often co-occur. Thus, an active learner that selects uncertain examples of one class gets examples of a second class as an unintended, yet positive side effect. We call this the co-selection effect. As a result, AL for sequence labeling is not “pure” exploitative AL, but implicitly comprises an exploratory aspect which can substantially reduce the missed class problem. In scenarios where we cannot hope for such a co-selection, we are much more likely to have decreased AL performance due to missed clusters or classes"
W09-1902,D08-1112,0,0.0611269,"ce, it may mistake the instances of Xi and Xj before it has acquired enough information to discriminate between them. So, under certain situations similarity of classes can mitigate the missed class effect. 11 The co-selection effect Many NLP tasks are sequence learning problems including, e.g., POS tagging, and named entity recognition. Sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences. Algorithms for sequence learning obviously work on sequence data, so respective AL approaches need to select complete sequences instead of single text tokens (Settles and Craven, 2008). Furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (Ringger et al., 2007; Tomanek et al., 2007). Within such sequences, instances of different classes often co-occur. Thus, an active learner that selects uncertain examples of one class gets examples of a second class as an unintended, yet positive side effect. We call this the co-selection effect. As a result, AL for sequence labeling is not “pure” exploitative AL, but"
W09-1902,D07-1051,1,0.869461,"ems including, e.g., POS tagging, and named entity recognition. Sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences. Algorithms for sequence learning obviously work on sequence data, so respective AL approaches need to select complete sequences instead of single text tokens (Settles and Craven, 2008). Furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (Ringger et al., 2007; Tomanek et al., 2007). Within such sequences, instances of different classes often co-occur. Thus, an active learner that selects uncertain examples of one class gets examples of a second class as an unintended, yet positive side effect. We call this the co-selection effect. As a result, AL for sequence labeling is not “pure” exploitative AL, but implicitly comprises an exploratory aspect which can substantially reduce the missed class problem. In scenarios where we cannot hope for such a co-selection, we are much more likely to have decreased AL performance due to missed clusters or classes. 3 Experiments We ran"
W15-2915,S15-2097,0,0.0338013,"Nakov et al., 2013; Rosenthal et al., 2015). The final evaluation is done on the SemEval 2015 test set. Table 1 lists all data set sizes in detail. To test the generality of our findings we additionally report results on the manually labeled test set of the Sentiment140 corpus (Sent140) (Go et 2 http://sentiment.christopherpotts.net/lingstruc.html The list of emoticons was taken from SentiStrengh: http://sentistrength.wlv.ac.uk/ 3 4 111 objective instances were considered to be neutral For reference we add the first and second best systems of the SemEval 2015 tweet level polarity task: Webis (Hagen et al., 2015) and UNITN (Severyn and Moschitti, 2015). Webis is an ensemble based on four systems, which participated in the same task of SemEval 2014. The UNITN system trains a CNN similar to ours. They rely on pretraining the entire model on a large distant supervised training corpus (10M labeled tweets). This approach is orthogonal to ours and can easily be combined with our idea if linguistic feature integration. This combination is likely to increase the performance further. features SemEval Sent140 SVM bow ling. bow + ling. Webis UNITN 50.51 57.28 59.28 67.34 66.90 70.21 64.84 64.59 - 57.83 59.24 62."
W15-2915,P14-1062,0,0.36019,"d of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge training data in a semi-supervised fashion (Severyn and Moschitti, 201"
W15-2915,D14-1181,0,0.196423,"N), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge training data"
W15-2915,S13-2053,0,0.239287,"formed Convolutional Neural Network (lingCNN), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this is"
W15-2915,N13-1039,0,0.0966121,"Missing"
W15-2915,S15-2078,0,0.0329058,"(SVM) for classification. According to their analysis, bag-of-word features ({1, 2, 3}-grams for words and {3, 4, 5}-grams for characters) and linguistic features are the most important ones. Therefore, we implement both of them. There are three feature settings: (i) only bag-of-words features (for both, word and characters), (ii) only linguistic features, and (iii) the combination of bag-of-words and linguistic features. We use LIBLINEAR (Fan et al., 2008) to train the model and optimized the C parameter on the development set. Data To evaluate the lingCNN, we use the SemEval 2015 data set (Rosenthal et al., 2015). We train the model on the SemEval 2013 training and development set and use the SemEval 2013 test set as development set (Nakov et al., 2013; Rosenthal et al., 2015). The final evaluation is done on the SemEval 2015 test set. Table 1 lists all data set sizes in detail. To test the generality of our findings we additionally report results on the manually labeled test set of the Sentiment140 corpus (Sent140) (Go et 2 http://sentiment.christopherpotts.net/lingstruc.html The list of emoticons was taken from SentiStrengh: http://sentistrength.wlv.ac.uk/ 3 4 111 objective instances were considered"
W15-2915,S15-2079,0,0.192968,"ncorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge training data in a semi-supervised fashion"
W15-2915,D13-1170,0,0.00342617,"es. Especially, where only limited training data is available, the performance difference is large. Even with only 1000 training samples, the CNN with linguistic features yields a reasonable result of 60.89. The CNN that does not have access to this source of information reaches only 49.89. Although, the performance 112 1000 3000 powerful for unbounded dependencies, but tweets are short; the sentiment of a tweet is usually determined by one part of it and unlike RNN/LSTM, convolution plus max pooling can learn to focus on that. Recursive architectures like the Recursive Neural Tensor Network (Socher et al., 2013). assume some kind of hierarchical sentence structure. This structure does not exist or is hard to recognize for many noisy tweets. As mentioned before, we use the SemEval 2013 and SemEval 2014 winning system (Mohammad et al., 2013) as baseline. Moreover, we include several features of their system to improve the CNN. all emb. 49.89 58.10 62.72 emb. + word + sent. 60.89 62.51 64.46 Table 3: Different training set sizes. of the CNN without linguistic features increases much for 3000 training examples, this model is still more than 4 points behind the linguistically informed model. 6 Related Wor"
W15-2915,S14-2033,0,0.145095,"ural Network (lingCNN), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge t"
W15-2915,P14-1146,0,0.269857,"ural Network (lingCNN), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge t"
W15-2915,H05-1044,0,0.0278602,"ft and right side (i.e., the sentence length becomes n + 2 ∗ (m − 1)). This makes sure that every column of the filter reaches every column of the input. The model uses 2d convolution, because a filter that span all d dimensions in height has the advantage that it can find features that interact with multiple dimensions. Binary Sentiment Indicators Binary features that indicate a word’s prior polarity. We create two such features per word per lexicon. The first feature indicates positive and the second negative polarity of that word in the lexicon. The lexicons for this feature type are MPQA (Wilson et al., 2005), Opinion lexicon (Hu and Liu, 2004), and NRCC Emotion lexicon (Mohammad and Turney, 2013). Max Pooling To keep only the most salient features, the lingCNN selects the largest value of each convolution output. This way we hope to find the most important polarity indicators, independently of their position. To the remaining values we add a bias b(2) and apply a rectified linear unit nonlinearity: a(2) = max(0, a(1) + b(2) ) (Nair and Hinton, 2010). Sentiment Scores The Sentiment140 lexicon and the Hashtag lexicon (Mohammad et al., 2013) both provide a score for each word instead of just a label"
W15-2915,C14-1220,0,0.0405243,"ve the CNN. all emb. 49.89 58.10 62.72 emb. + word + sent. 60.89 62.51 64.46 Table 3: Different training set sizes. of the CNN without linguistic features increases much for 3000 training examples, this model is still more than 4 points behind the linguistically informed model. 6 Related Work Collobert et al. (2011) published the first CNN architecture for a range of natural language processing tasks. We adopt their idea of using look-up tables to incorporate linguistic features at the wordlevel into the CNN. Since then CNNs have been used for a variety of sentence classification tasks (e.g., Zeng et al. (2014)), including polarity classification (e.g., Kim (2014)). Kalchbrenner et al. (2014) showed that their DCNN for modeling sentences can achieve competitive results in this field. Our CNN architecture is simpler than theirs. There are alternative approaches of integrating linguistic features into model training. By adding more labeled data, implicit knowledge is given to the model. This approach usually requires manual labeling effort. A different approach is to incorporate linguistic knowledge into the objective function to guide the model training. For instance Tang et al. (2014b) incorporate t"
W15-2915,S13-2052,0,\N,Missing
W16-0103,D14-1070,0,0.0854777,"Missing"
W16-0103,P15-1121,0,0.0418894,"deep learning (DL) based system for this MCTest task. 16 Figure 2: Illustrations of HABCNN-QAP (top), HABCHH-QP (middle) and HABCNN-TE (bottom). Q, A, S: question, answer, statement; D: document (ii) Prior document modeling systems based on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our HABCNN systems outperform other DL competitors by big margins. 2 Related Work Existing systems for MCTest are mostly based on manually engineered features, e.g., (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015; Smith et al., 2015). In these works, a common route is first to define a loss function based on feature vectors, then the effort focuses on designing effective features based on various rules. Even though this research is groundbreaking for this task, its flexibility and capacity for generalization is limited. DL approaches appeal to increasing interest in analogous tasks. Weston et al., (2015b) introduce memory networks for factoid QA. Memory network framework is extended in (Weston et al., 2015a; Kumar et al., 2016) for Facebook bAbI dataset. Peng et"
W16-0103,D14-1162,0,0.0811221,"question is better recognized and answer identification is more accurate. To be specific, we classify questions into 12 classes: “how”, “how much”, “how many”, “what”, “who”, “where”, “which”, “when”, “whose”, “why”, “will” and “other”. The question label is created by querying for the label keyword in the question. If more than one keyword appears in a question, we adopt the one appearing earlier and the more specific one (e.g., “how much”, not “how”). In case there is no match, the class “other” is assigned. We train with AdaGrad (Duchi et al., 2011) and use 50-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word representations,2 kept fixed during training. Table 1 gives hyperparameter values, tuned on dev. We consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG4 (J¨arvelin and Kek¨al¨ainen, 2002). Unlike accuracy which evaluates if the question is correctly answered or not, NDCG4 , being a measure of ranking quality, evaluates the position of the correct answer in our predicted ranking. 4.3 Baseline Systems (i) Addition. Directly compare question and answers without considering the document. Sentence representations are computed by elemen"
W16-0103,D13-1020,0,0.148864,"t, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin. 1 Figure 1: One example with 2 out of 4 questions in the MCTest. “*” marks correct answer. Introduction Machine comprehension is an open-domain question-answering problem which contains factoid questions, but the answers can be derived by extraction or induction of key clues. Figure 1 shows one example in MCTest (Richardson et al., 2013). Each example consists of one document, four associated questions; each question is followed by four answer candidates of which only one is correct. Questions in MCTest have two categories; “one” questions can be answered based on a single sentence from document where “multiple” questions require several sentences. To correctly answer the first question in the example, the two blue sentences are required; for the second question instead, we only need the red sentence. The following observations hold for the whole MCTest. (i) Most of the sentences in the document are irrelevant for a given que"
W16-0103,P15-1024,0,0.444971,"m for this MCTest task. 16 Figure 2: Illustrations of HABCNN-QAP (top), HABCHH-QP (middle) and HABCNN-TE (bottom). Q, A, S: question, answer, statement; D: document (ii) Prior document modeling systems based on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our HABCNN systems outperform other DL competitors by big margins. 2 Related Work Existing systems for MCTest are mostly based on manually engineered features, e.g., (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015; Smith et al., 2015). In these works, a common route is first to define a loss function based on feature vectors, then the effort focuses on designing effective features based on various rules. Even though this research is groundbreaking for this task, its flexibility and capacity for generalization is limited. DL approaches appeal to increasing interest in analogous tasks. Weston et al., (2015b) introduce memory networks for factoid QA. Memory network framework is extended in (Weston et al., 2015a; Kumar et al., 2016) for Facebook bAbI dataset. Peng et al. (2015)’s Neural"
W16-0103,D15-1197,0,0.0774347,"ustrations of HABCNN-QAP (top), HABCHH-QP (middle) and HABCNN-TE (bottom). Q, A, S: question, answer, statement; D: document (ii) Prior document modeling systems based on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our HABCNN systems outperform other DL competitors by big margins. 2 Related Work Existing systems for MCTest are mostly based on manually engineered features, e.g., (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015; Smith et al., 2015). In these works, a common route is first to define a loss function based on feature vectors, then the effort focuses on designing effective features based on various rules. Even though this research is groundbreaking for this task, its flexibility and capacity for generalization is limited. DL approaches appeal to increasing interest in analogous tasks. Weston et al., (2015b) introduce memory networks for factoid QA. Memory network framework is extended in (Weston et al., 2015a; Kumar et al., 2016) for Facebook bAbI dataset. Peng et al. (2015)’s Neural Reasoner infers over multiple supporting"
W16-0103,P16-1041,0,0.0849513,"2016) for Facebook bAbI dataset. Peng et al. (2015)’s Neural Reasoner infers over multiple supporting facts to generate an entity answer for a given question and it is also tested on bAbI. All of this work deals with short texts with simple grammar, aiming to generate an answer that is restricted to be one word denoting a location, a person etc. There is also work on other kinds of QA, e.g., (Iyyer et al., 2014; Hermann et al., 2015). Overall, for open-domain MCTest machine comprehension task, we are the first to use deep neural networks. HABCNN shares similarities with the model published by Trischler et al. (2016) six weeks after our submission on arxiv. It considers multiple levels of granularity in a way that is similar to our approach. Figure 3: HABCNN. Feature maps for phrase representations pi and the max pooling steps that create sentence representations out of phrase representations are omitted for simplification. Each snippet covers three sentences in snippet-CNN. Symbols ◦ mean cosine similarity calculation. Trischler et al. (2016) achieve better performance than HABCNN, but they still use linguistically engineered features like Stanford dependencies whereas our approach is more truly end-to-e"
W16-0103,P15-2115,0,0.537141,"k. 16 Figure 2: Illustrations of HABCNN-QAP (top), HABCHH-QP (middle) and HABCNN-TE (bottom). Q, A, S: question, answer, statement; D: document (ii) Prior document modeling systems based on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our HABCNN systems outperform other DL competitors by big margins. 2 Related Work Existing systems for MCTest are mostly based on manually engineered features, e.g., (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015; Smith et al., 2015). In these works, a common route is first to define a loss function based on feature vectors, then the effort focuses on designing effective features based on various rules. Even though this research is groundbreaking for this task, its flexibility and capacity for generalization is limited. DL approaches appeal to increasing interest in analogous tasks. Weston et al., (2015b) introduce memory networks for factoid QA. Memory network framework is extended in (Weston et al., 2015a; Kumar et al., 2016) for Facebook bAbI dataset. Peng et al. (2015)’s Neural Reasoner infers ove"
W16-0103,P10-1125,0,\N,Missing
W16-0103,D15-1237,0,\N,Missing
W16-2010,E14-1060,0,0.0258653,"owever, the difference is not big. This might actually be different for languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The mos"
W16-2010,N15-1107,0,0.0817768,"y analyze and generate different morphological forms, including previously unseen forms. Two examples are machine translation and question answering, where errors in the understanding of morphological forms can seriously harm performance. Accordingly, learning morphological inflection patterns from labeled data is an important challenge. The task of morphological inflection (MI) consists of generating an inflected form for a given lemma and target tag. Several approaches have been developed for this, including machine learning models and models that exploit the paradigm structure of language (Ahlberg et al., 2015; 62 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 62–70, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Decoder. It will be described in detail in this Section. for Hungarian and Maltese were only released at evaluation time. Tasks. The Shared Task consists of 3 separate tasks with increasing difficulty: task 1 is supposed to be the easiest and task 3 the hardest. The first task consists of mapping a given lemma and target tag to a target form. Task 2 requires the mapping of a giv"
W16-2010,W14-4012,0,0.0234538,"Missing"
W16-2010,W98-1239,0,0.0618057,"Missing"
W16-2010,N13-1138,0,0.0187163,"is not big. This might actually be different for languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The most similar work to ours was"
W16-2010,D13-1105,0,0.0622397,"tually be different for languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The most similar work to ours was probably the one by Fa"
W16-2010,P16-2090,1,0.36865,"Missing"
W16-2010,D15-1272,1,0.451281,"Missing"
W16-2010,N15-1093,0,0.0515452,"languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The most similar work to ours was probably the one by Faruqui et al. (2015). In"
W16-2010,N16-1077,0,\N,Missing
W16-2010,W16-2002,0,\N,Missing
W17-4111,K17-2001,0,0.0636864,"Missing"
W17-4111,N13-1138,0,0.150344,"Missing"
W17-4111,N16-1077,0,0.0664057,"Missing"
W17-4111,P17-1182,1,0.822714,"Missing"
W17-4111,P16-1185,0,0.0259796,"Missing"
W17-4111,N15-1093,0,0.106512,"Missing"
W17-4111,W14-4012,0,0.0196779,"Missing"
W17-4111,W16-2005,0,0.0338841,"Missing"
W17-4111,D14-1179,0,0.0200405,"Missing"
W17-4111,W16-2003,0,0.0354377,"Missing"
W17-4111,N09-1024,0,0.0487814,"Missing"
W17-4111,P08-1084,0,0.0941985,"Missing"
W17-4111,D11-1014,0,0.107625,"Missing"
W18-3013,W14-1611,0,0.0231012,"in a classification setting and different subspaces of embeddings are analyzed. Extrinsic evaluations are also used (Li and Jurafsky, 2015; K¨ohn, 2015; Lai et al., 2015). In most tasks, embeddings are used in context/sentence representations with composition involved. In this work, we evaluate embeddings in isolation, on their ability to represent multiple senses. Related tasks and datasets. Our proposed task is fine-grained name typing (FNT). A related task is entity set expansion (ESE): given a set of a few seed entities of a particular class, find other entities (Thelen and Riloff, 2002; Gupta and Manning, 2014). We can formulate FNT as ESE, however, there is a difference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and"
W18-3013,D15-1246,0,0.0193011,"Missing"
W18-3013,D15-1200,0,0.148229,"ind all types that a name can refer to based on the name embedding. Given the scale of entities in knowledge bases, we can build datasets for this task that are complementary to the current embedding evaluation datasets in: they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context. 1 Figure 1: Types (ellipses; green) of the entities (rectangles; red), to which the name “Washington” can refer. Ideally, the embedding for “Washington” should represent all these types. Extrinsic evaluations are also used, cf. Li and Jurafsky (2015). In these tasks, embeddings are used in context/sentence representations with composition involved. In this paper, we propose a new evaluation method. In contrast to the prior work on intrinsic evaluation, our method is supervised, largescale, fine-grained, automatically built, and evaluates embeddings in a classification setting where different subspaces of embeddings need to be analyzed. In contrast to the prior work on extrinsic evaluation, we evaluate embeddings in isolation, without confounding factors like sentence contexts or composition functions. Introduction Distributed representati"
W18-3013,N15-1142,0,0.0410841,"Missing"
W18-3013,P14-1023,0,0.138775,"r work on intrinsic evaluation, our method is supervised, largescale, fine-grained, automatically built, and evaluates embeddings in a classification setting where different subspaces of embeddings need to be analyzed. In contrast to the prior work on extrinsic evaluation, we evaluate embeddings in isolation, without confounding factors like sentence contexts or composition functions. Introduction Distributed representation of words, aka word embedding, is an important element of many natural language processing applications. The quality of word embeddings is assessed using different methods. Baroni et al. (2014) evaluate word embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Different concept categorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about"
W18-3013,D16-1144,0,0.0252587,"er, there is a difference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and Weld, 2012; Yogatama et al., 2015; Ren et al., 2016; Shimaoka et al., 2016). One way of solving our task is to collect every mention of a name, use NER to predict the context-dependent types of mentions, and then take all predictions as the global types of the name. However, our focus in this paper is on how embedding models perform and propose this task as a good evaluation method. We leave the comparison to an NER-based approach for future work. Corpus-level fine-grained entity typing is the 1 Our dataset is available at: https://github.com/ yyaghoobzadeh/name_typing 102 1 holds. For example, for the name “Hamilton”, we should find all of th"
W18-3013,E17-1055,1,0.887038,"Missing"
W18-3013,N13-1008,0,0.0214632,"ategorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about entities from text, either locally (i.e., at the context-level), or globally (i.e., at the corpus-level). For example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 2015) in entity typing. In most global tasks, each entity is indexed with an identifier (ID) that usually comes from knowledge bases such as 101 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 101–106 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics like sentence context1 . Freebase. Exceptions are tasks in lexicon generation or population like entity set expansion (ESE) (Thelen and Riloff, 2002), which are global but without entity IDs."
W18-3013,P15-2048,0,0.0150152,"ulate FNT as ESE, however, there is a difference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and Weld, 2012; Yogatama et al., 2015; Ren et al., 2016; Shimaoka et al., 2016). One way of solving our task is to collect every mention of a name, use NER to predict the context-dependent types of mentions, and then take all predictions as the global types of the name. However, our focus in this paper is on how embedding models perform and propose this task as a good evaluation method. We leave the comparison to an NER-based approach for future work. Corpus-level fine-grained entity typing is the 1 Our dataset is available at: https://github.com/ yyaghoobzadeh/name_typing 102 1 holds. For example, for the name “Hamilton”, we sho"
W18-3013,P15-2119,0,0.0245732,"n, we evaluate embeddings in isolation, without confounding factors like sentence contexts or composition functions. Introduction Distributed representation of words, aka word embedding, is an important element of many natural language processing applications. The quality of word embeddings is assessed using different methods. Baroni et al. (2014) evaluate word embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Different concept categorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about entities from text, either locally (i.e., at the context-level), or globally (i.e., at the corpus-level). For example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 20"
W18-3013,C14-1220,0,0.0486649,"preference. Different concept categorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about entities from text, either locally (i.e., at the context-level), or globally (i.e., at the corpus-level). For example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 2015) in entity typing. In most global tasks, each entity is indexed with an identifier (ID) that usually comes from knowledge bases such as 101 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 101–106 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics like sentence context1 . Freebase. Exceptions are tasks in lexicon generation or population like entity set expansion (ESE) (Thelen and Riloff, 2002), which ar"
W18-3013,D15-1036,0,0.0200014,"subspaces of embeddings. In summary, our contributions are (i) introducing a new evaluation method for word embeddings (ii) publishing a new dataset that is a good resource for evaluating word embeddings and is complementary to prior work: it is very large, contains more different classes than previous word categorization datasets, and allows the direct evaluation of embeddings without confounding factors 2 Related Work Embedding evaluation. Baroni et al. (2014) evaluate embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Schnabel et al. (2015) introduce tasks with more fine-grained datasets. The concept categorization datasets used for embedding evaluation are mostly small (<500) (Baroni et al., 2014) and therefore measure the goodness of embeddings by the quality of their clustering. In contrast, we test embeddings in a classification setting and different subspaces of embeddings are analyzed. Extrinsic evaluations are also used (Li and Jurafsky, 2015; K¨ohn, 2015; Lai et al., 2015). In most tasks, embeddings are used in context/sentence representations with composition involved. In this work, we evaluate embeddings in isolation,"
W18-3013,W16-1313,0,0.0143477,"ference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and Weld, 2012; Yogatama et al., 2015; Ren et al., 2016; Shimaoka et al., 2016). One way of solving our task is to collect every mention of a name, use NER to predict the context-dependent types of mentions, and then take all predictions as the global types of the name. However, our focus in this paper is on how embedding models perform and propose this task as a good evaluation method. We leave the comparison to an NER-based approach for future work. Corpus-level fine-grained entity typing is the 1 Our dataset is available at: https://github.com/ yyaghoobzadeh/name_typing 102 1 holds. For example, for the name “Hamilton”, we should find all of the following: LOCATION, O"
W18-3013,W02-1028,0,0.202606,"example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 2015) in entity typing. In most global tasks, each entity is indexed with an identifier (ID) that usually comes from knowledge bases such as 101 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 101–106 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics like sentence context1 . Freebase. Exceptions are tasks in lexicon generation or population like entity set expansion (ESE) (Thelen and Riloff, 2002), which are global but without entity IDs. ESE usually starts from a few seed entities per set and completes the set using pattern-based methods. Here, we address the task of fine-grained name typing (FNT), a global prediction task, operating on the surface names of entities. FNT and ESE share applications in name lexicon population. FNT is different from ESE because we assume to have sufficient training instances for each type to train supervised models. The challenging goal of FNT is to find the types of all entities a name can refer to. For example, ”Washington” might refer to several entit"
W18-3013,D15-1083,1,0.891522,"Missing"
W18-3013,P16-1023,1,0.878829,"Missing"
W18-4001,I11-1031,0,0.0349131,"he task to find out if the text pairs mean the same thing. The important tasks in Natural Language Processing (NLP), such as Information Retrieval (IR) and text understanding may be improved by modeling the underlying semantic similarity between texts. With recent progress in deep learning, the STS task has gained success using LSTM (Mueller and Thyagarajan, 2016) and CNN (Yin et al., 2016) based architectures; however, these approaches model the underlying semantic similarity between example pairs, each with a single sentence or phrase with term overlaps. In the domain of question retrieval (Cai et al., 2011; Zhang et al., 2014), users retrieve historical questions which precisely match their questions (single sentence) semantically equivalent or relevant. However, we investigate similarity learning between texts of asymmetric lengths, such as short (phrase) Vs longer (paragraph/documents) with significant term mismatch. The application of textual understanding in retrieval becomes more challenging when the relevant document-sized retrievals are stylistically distinct with the input short texts. Learning a similarity metric has gained much research interest, however due to limited availability of"
W18-4001,C16-1239,1,0.900482,"Missing"
W18-4001,N18-1098,1,0.876457,"Missing"
W18-4001,N16-1030,0,0.0891464,"non-parametric regression step to obtain bettercalibrated predictions P [1, 5], same as (Mueller and Thyagarajan, 2016). We then evaluate the trained model for IR task, where we retrieve the top 10 similar results (SUB2+DESC2+SOL2), ranked by their similarity scores, for each query (SUB1+DESC1) in the development set and compute MAP@K, MRR@K and Acc@K, where K=1, 5, and 10. We use 300-dimensional pre-trained word2vec3 embeddings for input words, however, to generalize beyond the limited vocabulary in word2vec due to industrial domain data with technical vocabulary, we also employ char-BLSTM (Lample et al., 2016) to generate additional embeddings (=50 dimension4 ).The resulting dimension for word embeddings is 350. We use 50-dimensional hidden vector, ht , memory cells, ct and Adadelta (Zeiler, 2012) with dropout and gradient clipping (Pascanu et al., 2013) for optimization. The topics vector (T) size is 100. We use python NLTK toolkit5 for sentence tokenization. See Table 2 for the hyperparameters in Replicated Siamese LSTM for experiment #No:22. 3.4 Results: State-of-the-art Comparisons Table 5 shows the similarity and retrieval scores for unsupervised and supervised baseline methods. The #9, #18 an"
W18-4001,D12-1110,0,0.146828,"Missing"
W18-4001,N16-1065,1,0.886461,"Missing"
W18-4001,Q16-1019,1,0.803172,"pervised and supervised baselines. We also show that the topic and distributed semantic features for short and long texts improved both similarity learning and retrieval. 1 Introduction Semantic Textual Similarity (STS) is the task to find out if the text pairs mean the same thing. The important tasks in Natural Language Processing (NLP), such as Information Retrieval (IR) and text understanding may be improved by modeling the underlying semantic similarity between texts. With recent progress in deep learning, the STS task has gained success using LSTM (Mueller and Thyagarajan, 2016) and CNN (Yin et al., 2016) based architectures; however, these approaches model the underlying semantic similarity between example pairs, each with a single sentence or phrase with term overlaps. In the domain of question retrieval (Cai et al., 2011; Zhang et al., 2014), users retrieve historical questions which precisely match their questions (single sentence) semantically equivalent or relevant. However, we investigate similarity learning between texts of asymmetric lengths, such as short (phrase) Vs longer (paragraph/documents) with significant term mismatch. The application of textual understanding in retrieval bec"
W18-4001,S14-2044,0,0.0493554,"Missing"
W18-5418,N16-1097,1,0.904683,"Missing"
W18-5418,N18-1098,1,0.871043,"Missing"
W18-5418,N18-1003,1,0.635502,"Missing"
W18-5418,C16-1239,1,0.855302,"Missing"
W18-5418,W14-3106,0,0.0259166,"ee to apply LISA and example2pattern on different tasks such as document categorization, sentiment analysis, language modeling, etc. Another interesting direction would be to analyze the bag-of-word neural topic models such as DocNADE (Larochelle and Lauly, 2012) and iDocNADE (Gupta et al., 2018b) to interpret their semantic accumulation during autoregressive computations in building document representation(s). We extract the saliency patterns for each category in the data that can be effectively used in instantiating pattern-based information extraction systems, such as bootstrapping entity (Gupta and Manning, 2014) and relation extractors (Gupta et al., 2018e). classification decisions. Journal of Machine Learning Research, 11(Jun):1803–1831. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Alexey Dosovitskiy and Thomas Brox. 2016. Inverting visual representations with convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4829– 4837. Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211. Dumitru Erhan, Yoshu"
W18-5418,W09-2415,0,0.0412475,"Missing"
W18-5418,P16-1105,0,0.0224692,"2010) and Bach et al. (2015); Montavon et al. (2017) explain neural network predictions by sensitivity analysis to different input features and decomposition of decision functions, respectively. Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (Vu et al., 2016a; Miwa and Bansal, 2016; Gupta et al., 2016, 2018c), language modeling (Mikolov et al., 2010; Peters et al., 2018), slot filling (Mesnil et al., 2015; Vu et al., 2016b), machine translation (Bahdanau et al., 2014), sentiment analysis (Wang et al., 2016; Tang et al., 2015), semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d). Past works (Zeiler and Fergus, 2014; Dosovitskiy and Brox, 2016) have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons. Recent s"
W18-5418,N16-1030,0,0.0454857,"(2014) interprets by generating adversarial examples. However, Baehrens et al. (2010) and Bach et al. (2015); Montavon et al. (2017) explain neural network predictions by sensitivity analysis to different input features and decomposition of decision functions, respectively. Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (Vu et al., 2016a; Miwa and Bansal, 2016; Gupta et al., 2016, 2018c), language modeling (Mikolov et al., 2010; Peters et al., 2018), slot filling (Mesnil et al., 2015; Vu et al., 2016b), machine translation (Bahdanau et al., 2014), sentiment analysis (Wang et al., 2016; Tang et al., 2015), semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d). Past works (Zeiler and Fergus, 2014; Dosovitskiy and Brox, 2016) have mostly analyzed deep neural network, especially CNN in the field o"
W18-5418,N16-1082,0,0.0295922,"al., 2015), semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d). Past works (Zeiler and Fergus, 2014; Dosovitskiy and Brox, 2016) have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons. Recent studies have investigated visualization of RNN and its variants. Tang et al. (2017) visualized the memory vectors to understand the behavior of LSTM and gated recurrent unit (GRU) in speech recognition task. For given words in a sentence, Li et al. (2016) employed heat maps to study sensitivity and meaning composition in recurrent networks. Ming et al. (2017) proposed a tool, RNNVis to visualize hidden states based on RNN’s expected response to Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining"
W18-5418,N18-1202,0,0.263351,"ensitivity analysis to different input features and decomposition of decision functions, respectively. Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (Vu et al., 2016a; Miwa and Bansal, 2016; Gupta et al., 2016, 2018c), language modeling (Mikolov et al., 2010; Peters et al., 2018), slot filling (Mesnil et al., 2015; Vu et al., 2016b), machine translation (Bahdanau et al., 2014), sentiment analysis (Wang et al., 2016; Tang et al., 2015), semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d). Past works (Zeiler and Fergus, 2014; Dosovitskiy and Brox, 2016) have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons. Recent studies have investigated visualization of RNN and its variants. Tang et al. (2017) visualiz"
W18-5418,P16-1101,0,0.0156369,"generating adversarial examples. However, Baehrens et al. (2010) and Bach et al. (2015); Montavon et al. (2017) explain neural network predictions by sensitivity analysis to different input features and decomposition of decision functions, respectively. Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (Vu et al., 2016a; Miwa and Bansal, 2016; Gupta et al., 2016, 2018c), language modeling (Mikolov et al., 2010; Peters et al., 2018), slot filling (Mesnil et al., 2015; Vu et al., 2016b), machine translation (Bahdanau et al., 2014), sentiment analysis (Wang et al., 2016; Tang et al., 2015), semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d). Past works (Zeiler and Fergus, 2014; Dosovitskiy and Brox, 2016) have mostly analyzed deep neural network, especially CNN in the field of computer vision to"
W18-5418,D15-1167,0,0.0209156,"al networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (Vu et al., 2016a; Miwa and Bansal, 2016; Gupta et al., 2016, 2018c), language modeling (Mikolov et al., 2010; Peters et al., 2018), slot filling (Mesnil et al., 2015; Vu et al., 2016b), machine translation (Bahdanau et al., 2014), sentiment analysis (Wang et al., 2016; Tang et al., 2015), semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d). Past works (Zeiler and Fergus, 2014; Dosovitskiy and Brox, 2016) have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons. Recent studies have investigated visualization of RNN and its variants. Tang et al. (2017) visualized the memory vectors to understand the behavior of LSTM and gated recurrent unit (GRU) in speech recognition task. For given words in a sentence, Li et al. ("
W18-5418,N16-1065,1,0.90239,"Missing"
W18-5418,D16-1058,0,0.0269255,"n, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016), relation extraction (Vu et al., 2016a; Miwa and Bansal, 2016; Gupta et al., 2016, 2018c), language modeling (Mikolov et al., 2010; Peters et al., 2018), slot filling (Mesnil et al., 2015; Vu et al., 2016b), machine translation (Bahdanau et al., 2014), sentiment analysis (Wang et al., 2016; Tang et al., 2015), semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d). Past works (Zeiler and Fergus, 2014; Dosovitskiy and Brox, 2016) have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons. Recent studies have investigated visualization of RNN and its variants. Tang et al. (2017) visualized the memory vectors to understand the behavior of LSTM and gated recurrent unit (GRU) in speech recognition task. For given words in a s"
W18-5437,D16-1216,0,0.0226417,"entations. For computer vision problems, Simonyan et al. (2014) propose to use gradient ascent to find an input image that maximizes the activation of a neuron of interest. Using these image representations, one can for instance show that lower level neurons in vision CNNs specialize in patterns such as stripes (Mordvintsev et al., 2015). Applying gradient ascent input optimization to NLP is not straightforward, as discrete symbols are not open to continuous manipulation. A common alternative approach is to search existing corpora for optimal documents or n-grams (e.g., K´ad´ar et al. (2017), Aubakirova and Bansal (2016)). As this strategy only covers the space of existing inputs, we assume that it may lead to incorrect assumptions. For instance, the representation of a given neuron may suggest that syntax was learned, when in reality this is due to a lack of ungrammatical inputs in the corpus. In the following, we propose and test methods for gradient ascent input optimization in NLP. Our Embedding optimization One straightforward approach to NLP input optimization is to treat E like Simonyan et al. (2014) treat images, i.e., to apply gradient ascent directly to the embedding vectors, while keeping  other"
W18-6538,P18-1063,0,0.0194833,"e, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first sentence or some key points are combined together to train the mo"
W18-6538,N16-1012,0,0.032176,"ation ranks among the oldest synthesis tasks of computer science, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first"
W18-6538,K16-1028,0,0.0457146,"Missing"
W18-6538,P17-1099,0,0.0441586,"of computer science, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first sentence or some key points are combined"
W18-6538,W17-4508,1,0.896057,"Missing"
W19-8666,W19-8665,0,0.0128685,"valuator script to compute ROUGE scores. Each software and evaluator run on the test set was manually reviewed by organizers for errors and data leakage. After a successful review, https://tac.nist.gov/ 524 5 Evaluating models on TIRA using ROUGE was allowed even after the submission deadline. Thus, a participant’s technical paper may have a variation of the same model with different ROUGE scores, but was not manually evaluated. the scores were shared on a public leaderboard.6 Two participants provided their system descriptions. We did not receive any description for the tldr-bottom-up model. Gehrmann et al. (2019) leveraged fine-tuned language models to generate abstractive summaries. They argue that excessive copying facilitated by the copy-attention mechanism hinders paraphrasing and information compression (abstraction). As part of the TL;DR challenge, they compared two summarization approaches (pseudo-self-attn and transf-seq2seq) demonstrating the effectiveness of transfer learning at generating abstractive summaries. Our manual evaluation confirms that these models generate concise and coherent summaries. Tackling the same problem of excessive copying in pointer-generator models, Choi et al. (201"
W19-8666,W10-0722,0,0.0151451,"ey U for pairwise comparison using Bonferroni correction. 2 unified-pgn 2 38 unified-vae-pgn 4 30 transf-seq2seq 4 27 pseudo-self-attn 12 35 tldr-bottom-up 2 25 seq2seq-baseline 79 14 ts .28 up Sufficiency 2 3 Avg. 60 66 69 53 73 7 Text quality 1 2 2.11 6 2.13 9 2.20 0 1.97 2 2.30 1 1.11 73 68 62 5 8 28 21 8 90 2.52 3 Avg. 26 29 95 90 71 6 1.78 1.78 2.70 2.67 2.29 1.11 0 15 85 2.57 Table 3: Sufficiency and text quality score distribution in the majority category. the models. Furthermore, it may help to identify if non-expert annotators can still produce reliable judgments without a guideline. Gillick and Liu (2010) cautioned that workers have difficulties distinguishing the content of a summary from its text quality. With that in mind, we devised two orthogonal three-level rating scales. With respect to sufficiency, workers could rate a summary as insufficient (incomplete and unrelated to the source text), as barely acceptable (missing the main point, but capturing relevant secondary information), or as sufficient (capturing the main point of the text). In terms of text quality, we distinguished the levels badly written (incoherent or major errors), needs improvement (minor errors breaking the flow, but"
W19-8666,hovy-etal-2006-automated,0,0.0683851,"summarization technology for social media, focusing on abstractive summarization. This paper reports the results of the challenge and describes our manual evaluation of the submissions. Finally, we discuss the expected properties of a good summary after analyzing the comments provided by human annotators. 1 c Hinrich Schützed 2 Related Work Shared tasks on automatic summarization were first introduced at the Document Understanding Conferences (DUC).3 In addition to new summarization technology, equal emphasis was given to formulating strong evaluation measures. Methods such as basic elements (Hovy et al., 2006), pyramid (Nenkova and Passonneau, 2004), and ROUGE (Lin, 2004) were introduced for automatically evaluating the content selection capabilities of the participating systems. Furthermore, Dang (2005) presented the first guideline for manually judging summary quality. In 2008, DUC became a summarization track at the Text Analysis Conference 2 TL;DR, short for “too long; didn’t read”, is a cliché reply bemoaning a post’s excessive length, and has given rise to a practice of adding a summary at the end of long posts, introduced by that same letter sequence or variants thereof. 3 https://www-nlpir."
W19-8666,D18-1208,0,0.0297649,"of news. This can be attributed to the ease of obtaining large amounts of news articles alongside suitable summary ground truth, greatly simplifying the corpus construction. However, the summaries found in the currently widely used corpora are either only highlights directly extracted from news articles, offering little abstraction and no coherent text, or headlines, which are short and not necessarily summaries, albeit occasionally abstractive. Furthermore, the common structure of news articles1 introduces bias, since the lead paragraph usually already captures the most relevant information (Kedzie et al., 2018). To foster the development of robust summarization technology, we need to venture off the beaten track and explore more diverse domains. In this regard, the recently published Webis-TLDR-17 corpus (Völske et al., 2017) provides for the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than"
W19-8666,N04-1019,0,0.0522061,"social media, focusing on abstractive summarization. This paper reports the results of the challenge and describes our manual evaluation of the submissions. Finally, we discuss the expected properties of a good summary after analyzing the comments provided by human annotators. 1 c Hinrich Schützed 2 Related Work Shared tasks on automatic summarization were first introduced at the Document Understanding Conferences (DUC).3 In addition to new summarization technology, equal emphasis was given to formulating strong evaluation measures. Methods such as basic elements (Hovy et al., 2006), pyramid (Nenkova and Passonneau, 2004), and ROUGE (Lin, 2004) were introduced for automatically evaluating the content selection capabilities of the participating systems. Furthermore, Dang (2005) presented the first guideline for manually judging summary quality. In 2008, DUC became a summarization track at the Text Analysis Conference 2 TL;DR, short for “too long; didn’t read”, is a cliché reply bemoaning a post’s excessive length, and has given rise to a practice of adding a summary at the end of long posts, introduced by that same letter sequence or variants thereof. 3 https://www-nlpir.nist.gov/projects/duc/index.html https:/"
W19-8666,D15-1044,0,0.0377217,"st, the example from the WebisTLDR-17 corpus exhibits higher abstraction, abbreviations and composition of multiple facts into single phrases. (TAC)4 with evaluation as an independent task (Automatically Evaluating Summaries of Peers, AESOP). Most of these efforts were limited to extractive summarization on comparably small datasets from specific domains, such as biomedical records, newswire articles, and opinions, since neural text generation had not yet become mainstream, rendering abstractive summarization much more difficult. The first attempt at abstractive summarization was presented by Rush et al. (2015), which resulted in a subsequent surge in neural summarization research yielding promising results—we refer to Shi et al. (2018) for a comprehensive review. However, as most recent models have been evaluated exclusively on news corpora, our knowledge of their full capabilities is still superficial. Through the TL;DR challenge, we hope to close this gap. 4 3 Survey of Submissions Out of 16 registered participants, we received 5 submissions from 3 participants (2 from industry). In addition, we provided a seq2seq-baseline model with 2 layers, bi-LSTM, 256 hidden units and no attention. Participa"
W19-8666,E17-2007,0,0.0375653,"Missing"
W19-8666,P17-1099,0,0.0391249,"e diversity, the unified-vae-pgn model uses a VAE for generating summaries of the extracted important sentences. This multi-stage architecture preserves a substantial amount of key information while generating acceptable summaries as revealed in our manual evaluation. We refer readers to the system description papers for further details. 4 Evaluation via crowdsourcing to evaluate both the sufficiency and the text quality of a generated abstractive summary. Below, after reviewing both approaches, we report on the results of the participating systems. 4.1 We begin with a novelty analysis as per See et al. (2017), calculating the fraction of n-grams in the summary that are absent from the text as its novelty (Table 2). The ground truth has the highest novelty, underlining the abstractive nature of selfauthored summaries. Next, we used ROUGE (Lin, 2004) for automatic evaluation and report the F1scores.7 From Table 2 it is difficult to draw any conclusions just by looking at ROUGE scores. Furthermore, a key issue of ROUGE is that it does not provide any upper bounds for the quality of a summarization system (Schluter, 2017), thus warranting an extensive manual evaluation of the systems. Model ROUGE 1 2"
W19-8666,W18-6538,1,0.848865,"r the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than generally found in news articles. Table 1 shows a comparison of the nature of ground truth summaries in the news and the social media domain. With permission from its creators, we used this corpus to organize the TL;DR challenge (Syed et al., 2018), inviting summarization researchers to test existing models as well as new ones. To ensure reproducibility as well as blind and semi-automatic evaluation, we adopted the cloud-based evaluation platform TIRA (Potthast et al., 2019). In addition to the automatic ROUGE metrics, we evaluate the submissions manually for summary effectiveness and text quality via crowdsourcing. In this paper, we report our findings, discuss what annotators consider when scoring summaries, and outline future directions for abstractive summarization research. With most summarization research focused on the news domai"
W19-8666,W17-4508,1,0.852321,"widely used corpora are either only highlights directly extracted from news articles, offering little abstraction and no coherent text, or headlines, which are short and not necessarily summaries, albeit occasionally abstractive. Furthermore, the common structure of news articles1 introduces bias, since the lead paragraph usually already captures the most relevant information (Kedzie et al., 2018). To foster the development of robust summarization technology, we need to venture off the beaten track and explore more diverse domains. In this regard, the recently published Webis-TLDR-17 corpus (Völske et al., 2017) provides for the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than generally found in news articles. Table 1 shows a comparison of the nature of ground truth summaries in the news and the social media domain. With permission from its creators, we used this corpus to organize the TL;DR"
