2007.mtsummit-ucnlg.21,W07-2307,1,0.863657,"Missing"
2007.mtsummit-ucnlg.21,J07-2004,1,0.878071,"Missing"
2020.ccl-1.81,P12-1063,0,0.217991,"choice between long and short forms during daily communication. Like human speakers and writers, the long and short form choice task also needs to be carefully resolved for various domain including Natural Language Generation(Inkpen and Hirst, 2004), Machine Translation(Nguyen and Chiang, 2017), and Style Transfer(Fu et al., 2018). In this work, we focus on long and short forms that share at least one same word meaning and one same morpheme, but compose of different number of syllables. The long and short form choice task is formulated as Fill-in-the-blank (FITB) task(Inkpen and Hirst, 2004; Zweig et al., 2012), whose goal is to select a missing word for a sentence from a set of candidates. A FITB example used in this work is shown in Table 1. Sentence 她去日本旅游时，必逛各种免税 ______。 When travels to Japan, she must go to duty free_____. Long Form (1) 商店 (1) shop Short Form (2) 店 (2) shop Table 1: A long and short form choice FITB question example. The lexical choice is difficult in the context of long and short forms for most language processing systems due to the identical word sense leading to their preceding and subsequent contexts are too similar to providing distinguishing information. To address this p"
2020.ccl-1.81,W19-8605,1,0.770537,"Missing"
2020.ccl-1.81,C10-1133,0,0.0798927,"Missing"
2020.ccl-1.81,P15-2084,0,0.103854,"ent performance in many language processing tasks, thus they have been also used to tackle the lexical choice task. A 5-gram language model(Islam and Inkpen, 2010) was trained from a large-scale Web corpus to choosing among English nearsynonyms, following which Yu et al.(2011) implemented n-gram language model to Chinese near-synonym choice. N-gram model shows a better accuracy than PMI in near-synonym choice which is similar to our task. Neural Language Models overcome the limitation of n-gram language model by its powerful capability of long-range dependency. Recurrent Neural Networks (RNN)(Mirowski and Vlachos, 2015) and it variation Long-short Term Memory (LSTM)(Tran et al., 2016). Zweig et al.(Zweig et al., 2012) tackled the sentence completion problem with various approaches like language models. NNLMs achieved a better performance in these work, whose improvement can be attributed to its capability of capturing global information. 3 Long and Short Form Choice via Language Models Language modeling is an effective approach to solve the task by computing occurrence probability of each candidate words. Given a context, the best long and short form can be chosen according to the probability acquired from l"
2020.ccl-1.81,N16-1036,0,0.0671195,"used to tackle the lexical choice task. A 5-gram language model(Islam and Inkpen, 2010) was trained from a large-scale Web corpus to choosing among English nearsynonyms, following which Yu et al.(2011) implemented n-gram language model to Chinese near-synonym choice. N-gram model shows a better accuracy than PMI in near-synonym choice which is similar to our task. Neural Language Models overcome the limitation of n-gram language model by its powerful capability of long-range dependency. Recurrent Neural Networks (RNN)(Mirowski and Vlachos, 2015) and it variation Long-short Term Memory (LSTM)(Tran et al., 2016). Zweig et al.(Zweig et al., 2012) tackled the sentence completion problem with various approaches like language models. NNLMs achieved a better performance in these work, whose improvement can be attributed to its capability of capturing global information. 3 Long and Short Form Choice via Language Models Language modeling is an effective approach to solve the task by computing occurrence probability of each candidate words. Given a context, the best long and short form can be chosen according to the probability acquired from language models. The state-of-the-art language modeling techniques"
2020.ccl-1.81,N19-1423,0,0.0135271,"Missing"
2020.codi-1.12,W10-4226,0,0.0718288,"cerned with measuring the distance in sentences (or clauses), while the third one goes beyond the sentential level, and focuses on paragraphs. Which of these interpretations does best in algorithms to predict referential choice in discourse contexts? 3 Recency in ML studies Within Natural Language Generation (Gatt and Krahmer, 2018), reference production is computationally modelled in an area known as Referring Expression Generation (REG) (Krahmer and van Deemter, 2019; van Deemter, 2016). REG models have various shapes and forms, with feature-based ML models playing a substantial role. GREC (Belz and Kow, 2010) was a series of Shared Task Evaluation tasks that is still regarded as a natural starting point when it comes to the generation of referring expressions in context. Different ML algorithms were submitted to these shared tasks, a number of which have exploited recency metrics. Some of the metrics used in these algorithms are pursuant to the interpretations mentioned in section 2. For example, the recency feature in Greenbacker and McCoy (2009) resembles the metric defined in McCoy and Strube (1999). Another example is a binary feature used by Bohnet (2008), which captures whether or not the an"
2020.codi-1.12,W08-1128,0,0.214599,"aying a substantial role. GREC (Belz and Kow, 2010) was a series of Shared Task Evaluation tasks that is still regarded as a natural starting point when it comes to the generation of referring expressions in context. Different ML algorithms were submitted to these shared tasks, a number of which have exploited recency metrics. Some of the metrics used in these algorithms are pursuant to the interpretations mentioned in section 2. For example, the recency feature in Greenbacker and McCoy (2009) resembles the metric defined in McCoy and Strube (1999). Another example is a binary feature used by Bohnet (2008), which captures whether or not the antecedent occurs in the same sentence. This metric is similar to the interpretation discussed above under the heading “Immediate Context”. Some of the other recency metrics used in these algorithms, however, are not in accordance with the interpretations introduced in section 2. For instance, Bohnet (2008) and Jamison and Mehay (2008) used distance metrics measuring number of words between the two mentions. In a more recent ML study, Kibrik et al. (2016) stated that referential choice belongs to a 114 large group of multifactorial processes. They used 7 dif"
2020.codi-1.12,W09-2819,0,0.0916841,"Missing"
2020.codi-1.12,W08-1129,0,0.0915579,"Missing"
2020.codi-1.12,C00-1045,0,0.454552,"Missing"
2020.codi-1.12,P98-1090,0,0.58316,"Missing"
2020.codi-1.12,N06-2015,0,0.0234398,"Missing"
2020.codi-1.12,P16-1054,0,0.132894,"dance with the interpretations introduced in section 2. For instance, Bohnet (2008) and Jamison and Mehay (2008) used distance metrics measuring number of words between the two mentions. In a more recent ML study, Kibrik et al. (2016) stated that referential choice belongs to a 114 large group of multifactorial processes. They used 7 different distance-related metrics in their study and concluded that these metrics are essential for successful prediction of referential choice, but there is no indication which metrics are the most relevant ones. Further studies that include recency metrics are Ferreira et al. (2016), Modi et al. (2017) and Saha et al. (2011), among others. We saw that the metrics used in the ML studies are based on different units of measurement (e.g. word distance versus sentence distance). Likewise, different strategies are used to encode these metrics. For instance, some distances are measured in natural numbers while others are categorized in a smaller class of broader “bins”. In the following example taken from the GREC-2.0 corpus (Belz et al., 2010), one could say that the distance between the expression “its” and its antecedent “Berlin” is 21 words (a natural number). Another solu"
2020.codi-1.12,W08-1130,0,0.234848,"algorithms are pursuant to the interpretations mentioned in section 2. For example, the recency feature in Greenbacker and McCoy (2009) resembles the metric defined in McCoy and Strube (1999). Another example is a binary feature used by Bohnet (2008), which captures whether or not the antecedent occurs in the same sentence. This metric is similar to the interpretation discussed above under the heading “Immediate Context”. Some of the other recency metrics used in these algorithms, however, are not in accordance with the interpretations introduced in section 2. For instance, Bohnet (2008) and Jamison and Mehay (2008) used distance metrics measuring number of words between the two mentions. In a more recent ML study, Kibrik et al. (2016) stated that referential choice belongs to a 114 large group of multifactorial processes. They used 7 different distance-related metrics in their study and concluded that these metrics are essential for successful prediction of referential choice, but there is no indication which metrics are the most relevant ones. Further studies that include recency metrics are Ferreira et al. (2016), Modi et al. (2017) and Saha et al. (2011), among others. We saw that the metrics used in"
2020.codi-1.12,W99-0108,0,0.635552,"argue that, sensitivity to discourse structure is important for recency metrics used in determining referring expression forms. 1 Introduction Speakers use various linguistic forms such as pronouns, proper names, and common nouns, to refer to entities in discourse. A great number of studies have addressed the issue of referring, and the factors that play a role in speakers’ choice of the form of referring expressions. These factors include grammatical function (Brennan, 1995), animacy (Fukumura and van Gompel, 2011), competition (Arnold and Griffin, 2007), frequency (Ariel, 1990) and recency (McCoy and Strube, 1999; Ariel, 2001), among others. The focus of this article is on recency. Broadly speaking, we understand recency to be the distance between the current mention of a referent and its antecedent. Therefore, in this work, we employ recency metrics to predict the form of subsequent mentions, and are not interested in the choice of “first-mention” expressions. Recency has received much attention in both linguistic and computational studies, but in many cases, the notion of recency itself has been left largely undefined even though, as we shall see, recency can be understood in different ways. This pa"
2020.codi-1.12,Q17-1003,0,0.163366,"ations introduced in section 2. For instance, Bohnet (2008) and Jamison and Mehay (2008) used distance metrics measuring number of words between the two mentions. In a more recent ML study, Kibrik et al. (2016) stated that referential choice belongs to a 114 large group of multifactorial processes. They used 7 different distance-related metrics in their study and concluded that these metrics are essential for successful prediction of referential choice, but there is no indication which metrics are the most relevant ones. Further studies that include recency metrics are Ferreira et al. (2016), Modi et al. (2017) and Saha et al. (2011), among others. We saw that the metrics used in the ML studies are based on different units of measurement (e.g. word distance versus sentence distance). Likewise, different strategies are used to encode these metrics. For instance, some distances are measured in natural numbers while others are categorized in a smaller class of broader “bins”. In the following example taken from the GREC-2.0 corpus (Belz et al., 2010), one could say that the distance between the expression “its” and its antecedent “Berlin” is 21 words (a natural number). Another solution would be, for i"
2020.codi-1.12,W09-2822,0,0.0603413,"Missing"
2020.codi-1.12,J04-3003,0,0.286895,"Missing"
2020.codi-1.12,W13-3516,0,0.0601457,"Missing"
2020.codi-1.12,I11-1011,0,0.372364,"ction 2. For instance, Bohnet (2008) and Jamison and Mehay (2008) used distance metrics measuring number of words between the two mentions. In a more recent ML study, Kibrik et al. (2016) stated that referential choice belongs to a 114 large group of multifactorial processes. They used 7 different distance-related metrics in their study and concluded that these metrics are essential for successful prediction of referential choice, but there is no indication which metrics are the most relevant ones. Further studies that include recency metrics are Ferreira et al. (2016), Modi et al. (2017) and Saha et al. (2011), among others. We saw that the metrics used in the ML studies are based on different units of measurement (e.g. word distance versus sentence distance). Likewise, different strategies are used to encode these metrics. For instance, some distances are measured in natural numbers while others are categorized in a smaller class of broader “bins”. In the following example taken from the GREC-2.0 corpus (Belz et al., 2010), one could say that the distance between the expression “its” and its antecedent “Berlin” is 21 words (a natural number). Another solution would be, for instance, to follow Ferr"
2020.coling-main.403,W08-1128,0,0.395585,"nd Kuhn, 2013; Siddharthan et al., 2011; Stent, 2011; Castro Ferreira and Paraboni, 2017). Based on the result of the manual search, we included feature sets from the papers by Castro Ferreira et al. (2016) and Kibrik et al. (2016), which together with the GREC feature sets form the seven sets we use in the feature selection experiments. As a naming convention, the GREC feature sets are called with their names from Belz et al. (2010); the other two feature sets are named after the last name of their first authors. Number 1 2 3 4 5 6 7 Dataset IS-G Ferreira OSU ICSI Kibrik U-Del CNTS Reference Bohnet (2008) Castro Ferreira et al. (2016) Jamison and Mehay (2008) Favre and Bohnet (2009) Kibrik et al. (2016) Greenbacker and McCoy (2009a) Hendrickx et al. (2008) Number of features 5 5 8 14 17 18 21 Table 2: The datasets used in this study. The first two columns show the number and the name with which the datasets will be referred to. To provide an overview, we grouped the features into 9 broad categories namely Grammatical role, Inherent features, Referential status, Recency, Competition, Antecedent form, Surrounding patterns, Position and Protagonism explained below. Throughout this article, REF re"
2020.coling-main.403,P87-1022,0,0.848092,"reated as a group, and the group is in focus, which makes it prominent (Patson and Warren, 2011). According to Gordon et al. (1999)’s repeated name penalty, referring to a prominent referent with a proper name instead of a pronoun increases the processing time. An implication of this is the possibility that including the plurality feature in a prediction task might facilitate the pronoun detection. • Grammatical role: Various studies, including centering-based research, tend to emphasize that referents in subject position have a higher tendency to be pronominalized in the subsequent sentence (Brennan et al., 1987; Brennan, 1995; Kaiser, 2010). The focus of previous research has usually been on the subjecthood of the antecedent, and less so on the subjecthood of the current mention. Our analysis suggests that the grammatical role of the current mention is more important than that of the antecedent in predicting the choice of referential form. • Antecedent form: In this case, our findings match those in the linguistic tradition (Gundel and Hedberg, 2008). This factor could be important either because people tend to avoid consecutive uses of the same expression (Bohnet, 2008), or because having a pronomi"
2020.coling-main.403,D19-1312,0,0.313982,"ternational License. creativecommons.org/licenses/by/4.0/. License details: http:// 4575 Proceedings of the 28th International Conference on Computational Linguistics, pages 4575–4586 Barcelona, Spain (Online), December 8-13, 2020 • We additionally selected two other feature sets from the papers archived in the ACL anthology. The selection method will be detailed in subsection 3.1. • We re-implemented the features that were obtained, following the method detailed in subsection 3.2. Note that our systematic evaluation does not include Deep Learning methods (e.g., Castro Ferreira et al. (2018); Cao and Cheung (2019)) since, at the current state of the art, these do not yet offer much opportunity for linguistic interpretation. Our focus is on interpretable linguistic features. To perform our evaluation, two further choices need to be explained and motivated: Since the SRF task as such can be defined in different ways, we had to define an exact task, and we had to specify a corpus. These two choices will be further explained in section 3. Section 4 details the feature selection experiments and propose a consensus feature set. The paper concludes with a discussion of the extent to which the features are lin"
2020.coling-main.403,W17-3536,0,0.0166293,"nthology BibTex file from https://www.aclweb.org/anthology/ and used regular expressions to search manually the expressions of Table 1 in the title and abstracts of the articles: [R|r]eferring [E|e]xpression.*[M|m]achine [L|l]earning [G|g]enerat[ion|ing].*[R|r]eferring [E|e]xpression.*discourse [D|d]ata-driven.*[E|e]xpression title =.*[R|r]eferring [E|e]xpression title =.*[R|r]efer.* [G|g]eneration Table 1: Terms used to search for SRF studies We excluded several results because they did not meet the criteria defined above (Zarrieß and Kuhn, 2013; Siddharthan et al., 2011; Stent, 2011; Castro Ferreira and Paraboni, 2017). Based on the result of the manual search, we included feature sets from the papers by Castro Ferreira et al. (2016) and Kibrik et al. (2016), which together with the GREC feature sets form the seven sets we use in the feature selection experiments. As a naming convention, the GREC feature sets are called with their names from Belz et al. (2010); the other two feature sets are named after the last name of their first authors. Number 1 2 3 4 5 6 7 Dataset IS-G Ferreira OSU ICSI Kibrik U-Del CNTS Reference Bohnet (2008) Castro Ferreira et al. (2016) Jamison and Mehay (2008) Favre and Bohnet (20"
2020.coling-main.403,P16-1054,0,0.554262,"ically interpretable. 2 Related Work Data-driven models employ different features to make choices similar to those made by humans. For instance, Greenbacker and McCoy (2009a) used linguistically informed features such as recency, subjecthood, parallelism and ambiguity. In a comprehensive study, Kibrik et al. (2016) argued that reference generation is a multifactorial process governed by various linguistically motivated features. Hendrickx et al. (2008), on the other hand, focused more on existing patterns in the text and less on the use of linguistic categories. In a more recent study, Castro Ferreira et al. (2016) used features marking the syntactic position, recency and referential status of the referents to model the referential choice variations. In one of the few studies targeting feature selection, Greenbacker and McCoy (2009b) surveyed the psycholinguistics literature to decide on feature sets for their SRF study. They implemented several linguistically informed rules in their prototyping system, and “examined the incorrect classifications which resulted in an attempt to discover which other factors suggested by psycholinguistic research could explain the patterns they observed.” Additionally, th"
2020.coling-main.403,P18-1182,0,0.194938,"Missing"
2020.coling-main.403,W09-2818,0,0.0882993,"Missing"
2020.coling-main.403,W09-2819,0,0.575003,"tic features. To perform our evaluation, two further choices need to be explained and motivated: Since the SRF task as such can be defined in different ways, we had to define an exact task, and we had to specify a corpus. These two choices will be further explained in section 3. Section 4 details the feature selection experiments and propose a consensus feature set. The paper concludes with a discussion of the extent to which the features are linguistically interpretable. 2 Related Work Data-driven models employ different features to make choices similar to those made by humans. For instance, Greenbacker and McCoy (2009a) used linguistically informed features such as recency, subjecthood, parallelism and ambiguity. In a comprehensive study, Kibrik et al. (2016) argued that reference generation is a multifactorial process governed by various linguistically motivated features. Hendrickx et al. (2008), on the other hand, focused more on existing patterns in the text and less on the use of linguistic categories. In a more recent study, Castro Ferreira et al. (2016) used features marking the syntactic position, recency and referential status of the referents to model the referential choice variations. In one of t"
2020.coling-main.403,W09-2820,0,0.0461863,"elect a collection of feature sets. Subsection 3.1 describes the criteria for the selection procedure, and provides an overview of the chosen sets. Afterwards, we explain how we applied the selected feature sets to the OntoNotes corpus (subsection 3.2). 3.1 Feature sets The criteria to select the feature sets were as follows: we looked for studies that (1) had SRF as their main objective , (2) used a ML method, (3) used an English dataset and (4) applied interpretable features. We first selected all the feature sets used by the SRF algorithms in the GREC challenges. We excluded the JUNLG set (Gupta and Bandopadhyay, 2009) because their study was rule-based, and the WLV feature set (Or˘asan and Dornescu, 2009) because we were not able to interpret all their features. 4576 Since the GREC challenges were conducted several years ago, we also examined the more recent literature to include later SRF feature sets satisfying our criteria. We downloaded the full ACL anthology BibTex file from https://www.aclweb.org/anthology/ and used regular expressions to search manually the expressions of Table 1 in the title and abstracts of the articles: [R|r]eferring [E|e]xpression.*[M|m]achine [L|l]earning [G|g]enerat[ion|ing].*"
2020.coling-main.403,W08-1129,0,0.508998,"Missing"
2020.coling-main.403,C00-1045,0,0.765779,"Missing"
2020.coling-main.403,W08-1130,0,0.106756,"Missing"
2020.coling-main.403,W99-0108,0,0.415813,"e choice of referring expressions. One of the main ideas in this tradition (henceforth, the linguistic tradition) is that there is a direct relationship between the “prominence” (in a broad sense) of a referent at a given point in the discourse, and the form used to refer to it. If a referent is prominent, a short anaphoric form (e.g. a pronoun) suffices; if it is less prominent, longer forms with more semantic content are used. Prominence has been argued to be influenced by various factors such as recency and frequency of mention (Ariel, 1990), grammatical function (Brennan, 1995), distance (McCoy and Strube, 1999), animacy (Fukumura and van Gompel, 2011), and competition between the referents (Arnold and Griffin, 2007). Reference production is also one of the most-studied topics in Natural Language Generation (Gatt and Krahmer, 2018), where it is known as Referring Expression Generation [REG] (Krahmer and van Deemter, 2019). A key part of the REG problem is deciding which form (e.g., proper name, definite description or pronoun) to employ to refer to a referent at a given point in the discourse. Henceforth, we call this task Selection of Referential Form (SRF). SRF models come in many shapes and forms,"
2020.coling-main.403,W09-2822,0,0.0839683,"Missing"
2020.coling-main.403,W13-3516,0,0.061907,"Missing"
2020.coling-main.403,J11-4007,0,0.479218,"ing our criteria. We downloaded the full ACL anthology BibTex file from https://www.aclweb.org/anthology/ and used regular expressions to search manually the expressions of Table 1 in the title and abstracts of the articles: [R|r]eferring [E|e]xpression.*[M|m]achine [L|l]earning [G|g]enerat[ion|ing].*[R|r]eferring [E|e]xpression.*discourse [D|d]ata-driven.*[E|e]xpression title =.*[R|r]eferring [E|e]xpression title =.*[R|r]efer.* [G|g]eneration Table 1: Terms used to search for SRF studies We excluded several results because they did not meet the criteria defined above (Zarrieß and Kuhn, 2013; Siddharthan et al., 2011; Stent, 2011; Castro Ferreira and Paraboni, 2017). Based on the result of the manual search, we included feature sets from the papers by Castro Ferreira et al. (2016) and Kibrik et al. (2016), which together with the GREC feature sets form the seven sets we use in the feature selection experiments. As a naming convention, the GREC feature sets are called with their names from Belz et al. (2010); the other two feature sets are named after the last name of their first authors. Number 1 2 3 4 5 6 7 Dataset IS-G Ferreira OSU ICSI Kibrik U-Del CNTS Reference Bohnet (2008) Castro Ferreira et al. (2"
2020.coling-main.403,P13-1152,0,0.0221142,"SRF feature sets satisfying our criteria. We downloaded the full ACL anthology BibTex file from https://www.aclweb.org/anthology/ and used regular expressions to search manually the expressions of Table 1 in the title and abstracts of the articles: [R|r]eferring [E|e]xpression.*[M|m]achine [L|l]earning [G|g]enerat[ion|ing].*[R|r]eferring [E|e]xpression.*discourse [D|d]ata-driven.*[E|e]xpression title =.*[R|r]eferring [E|e]xpression title =.*[R|r]efer.* [G|g]eneration Table 1: Terms used to search for SRF studies We excluded several results because they did not meet the criteria defined above (Zarrieß and Kuhn, 2013; Siddharthan et al., 2011; Stent, 2011; Castro Ferreira and Paraboni, 2017). Based on the result of the manual search, we included feature sets from the papers by Castro Ferreira et al. (2016) and Kibrik et al. (2016), which together with the GREC feature sets form the seven sets we use in the feature selection experiments. As a naming convention, the GREC feature sets are called with their names from Belz et al. (2010); the other two feature sets are named after the last name of their first authors. Number 1 2 3 4 5 6 7 Dataset IS-G Ferreira OSU ICSI Kibrik U-Del CNTS Reference Bohnet (2008)"
2020.inlg-1.33,D19-1312,0,0.014354,"l language generation systems (NLG, Reiter and Dale, 2000). The task is to generate expressions that help hearers to identify the referent that a speaker is thinking about. REG has important practical value in natural language generation (Gatt and Krahmer, 2018), computer vision (Mao et al., 2016), and robotics (Fang et al., 2015). Additionally, REG algorithms can be seen as models of human language use (van Deemter, 2016). In line with this second angle, and unlike REG studies which have started to use black-box Neural Network based models (e.g., Mao et al. (2016); Ferreira et al. (2018) and Cao and Cheung (2019)), we focus on two aspects (cf., Krahmer and van Deemter (2012)): 1) designing and conducting controlled elicitation experiments, yielding corpora which are then used for analysing and evaluating REG algorithms to gain insight into linguistic phenomena, e.g., GRE 3 D 3 (Dale and Viethen, 2009), (Gatt et al., 2007; van Deemter et al., 2012), (Jordan and Walker, 2005), and M AP TASK (Gupta and Stent, 2005). 2) designing algorithms that mimic certain behaviours used by human beings, for example the maximisation of discriminatory power (Dale, 1989) and/or the preferential use of cognitively “attra"
2020.inlg-1.33,W18-6519,1,0.887008,"Missing"
2020.inlg-1.33,P89-1009,0,0.561115,"al. (2016); Ferreira et al. (2018) and Cao and Cheung (2019)), we focus on two aspects (cf., Krahmer and van Deemter (2012)): 1) designing and conducting controlled elicitation experiments, yielding corpora which are then used for analysing and evaluating REG algorithms to gain insight into linguistic phenomena, e.g., GRE 3 D 3 (Dale and Viethen, 2009), (Gatt et al., 2007; van Deemter et al., 2012), (Jordan and Walker, 2005), and M AP TASK (Gupta and Stent, 2005). 2) designing algorithms that mimic certain behaviours used by human beings, for example the maximisation of discriminatory power (Dale, 1989) and/or the preferential use of cognitively “attractive” attributes (Dale and Reiter, 1995); see Gatt et al. (2013) for discussion. The focus of these studies was mostly on Indo-European languages, such as English, Dutch (Koolen and Krahmer, 2010) and German (Howcroft et al., 2017). Recently researchers have started to have a look at Mandarin Chinese (van Deemter et al., 2017), collecting a corpus of Mandarin REs, namely MTUNA. So far, only a preliminary analysis has been performed on MTUNA, and this analysis has focussed on issues of Linguistic Realisation (van Deemter et al., 2017): the REs"
2020.inlg-1.33,W09-0609,0,0.256087,"2016), and robotics (Fang et al., 2015). Additionally, REG algorithms can be seen as models of human language use (van Deemter, 2016). In line with this second angle, and unlike REG studies which have started to use black-box Neural Network based models (e.g., Mao et al. (2016); Ferreira et al. (2018) and Cao and Cheung (2019)), we focus on two aspects (cf., Krahmer and van Deemter (2012)): 1) designing and conducting controlled elicitation experiments, yielding corpora which are then used for analysing and evaluating REG algorithms to gain insight into linguistic phenomena, e.g., GRE 3 D 3 (Dale and Viethen, 2009), (Gatt et al., 2007; van Deemter et al., 2012), (Jordan and Walker, 2005), and M AP TASK (Gupta and Stent, 2005). 2) designing algorithms that mimic certain behaviours used by human beings, for example the maximisation of discriminatory power (Dale, 1989) and/or the preferential use of cognitively “attractive” attributes (Dale and Reiter, 1995); see Gatt et al. (2013) for discussion. The focus of these studies was mostly on Indo-European languages, such as English, Dutch (Koolen and Krahmer, 2010) and German (Howcroft et al., 2017). Recently researchers have started to have a look at Mandarin"
2020.inlg-1.33,W17-3532,1,0.846667,"Missing"
2020.inlg-1.33,P18-1182,0,0.0442734,"Missing"
2020.inlg-1.33,W07-2307,1,0.751839,"Missing"
2020.inlg-1.33,koolen-krahmer-2010-tuna,0,0.167023,"nalysing and evaluating REG algorithms to gain insight into linguistic phenomena, e.g., GRE 3 D 3 (Dale and Viethen, 2009), (Gatt et al., 2007; van Deemter et al., 2012), (Jordan and Walker, 2005), and M AP TASK (Gupta and Stent, 2005). 2) designing algorithms that mimic certain behaviours used by human beings, for example the maximisation of discriminatory power (Dale, 1989) and/or the preferential use of cognitively “attractive” attributes (Dale and Reiter, 1995); see Gatt et al. (2013) for discussion. The focus of these studies was mostly on Indo-European languages, such as English, Dutch (Koolen and Krahmer, 2010) and German (Howcroft et al., 2017). Recently researchers have started to have a look at Mandarin Chinese (van Deemter et al., 2017), collecting a corpus of Mandarin REs, namely MTUNA. So far, only a preliminary analysis has been performed on MTUNA, and this analysis has focussed on issues of Linguistic Realisation (van Deemter et al., 2017): the REs in the corpus have not yet been compared with those in other languages, and the performance of REG algorithms on the corpus has not been evaluated. To fill this gap, we provide a more detailed analysis of the use of Mandarin REs on the basis of th"
2020.inlg-1.33,J12-1006,1,0.820589,"Missing"
2020.inlg-1.33,W17-3522,0,0.0626569,"to gain insight into linguistic phenomena, e.g., GRE 3 D 3 (Dale and Viethen, 2009), (Gatt et al., 2007; van Deemter et al., 2012), (Jordan and Walker, 2005), and M AP TASK (Gupta and Stent, 2005). 2) designing algorithms that mimic certain behaviours used by human beings, for example the maximisation of discriminatory power (Dale, 1989) and/or the preferential use of cognitively “attractive” attributes (Dale and Reiter, 1995); see Gatt et al. (2013) for discussion. The focus of these studies was mostly on Indo-European languages, such as English, Dutch (Koolen and Krahmer, 2010) and German (Howcroft et al., 2017). Recently researchers have started to have a look at Mandarin Chinese (van Deemter et al., 2017), collecting a corpus of Mandarin REs, namely MTUNA. So far, only a preliminary analysis has been performed on MTUNA, and this analysis has focussed on issues of Linguistic Realisation (van Deemter et al., 2017): the REs in the corpus have not yet been compared with those in other languages, and the performance of REG algorithms on the corpus has not been evaluated. To fill this gap, we provide a more detailed analysis of the use of Mandarin REs on the basis of the MTUNA corpus. We annotated the MT"
2020.inlg-1.33,2020.acl-main.93,0,0.0146322,"rrors in the annotation of ETUNA (cf. section 4.1), the difference between IA and FB+TYPE is no longer significant in the people domain in terms of Tukey’s HSD (compare the conclusion in van Deemter et al. (2012)). In other words, in both languages there is no significant difference between the performance of these two algorithms on the people domain. We also checked the influence of language on the performance of FB and FB+TYPE: the influence of the former is significant (F (1, 349) = 23.63, p < .001) while 7 An analogous problem has been identified in the task of evaluating image capturing (Yi et al., 2020), where the collision of multiple references for a single image was considered. Figure 2: Change of the performance with respected to different probabilities of inserting superfluous TYPE for either the FB+TYPE and IA on either the people domain of MTUNA - OL and ETUNA. that of later is not (F (1, 349) = 0.36, p = .548). This suggest that, in fact, it is English speakers who show more brevity, except in terms of use of TYPE. This might also explain the differences in absolute scores for all algorithms in both ETUNA and MTUNA , especially in the people domain. Another possible reason for these"
2020.inlg-1.45,P02-1040,0,0.117646,"w that different kinds of errors elicit significantly different evaluation scores, even though all erroneous descriptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. Th"
2020.inlg-1.45,J18-3002,0,0.250965,"riptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. This paper aims to explore this hypothesis, Motivation Image description systems make different kinds of"
2020.inlg-1.45,2020.acl-main.704,0,0.0241616,"n as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. This paper aims to explore this hypothesis, Motivation Image description systems make different kinds of mistakes, and these mistakes are likely to be of different importance for a ‘correct’ interpretation of the relevant image. Consider Figure 1, which shows multiple human reference descriptions, and a description generated by Li et al.’s (2018) system (all in Chinese, with English glosses). This system makes three different mistakes, which are shown separatel"
2020.inlg-1.45,P17-1024,0,0.0222989,"hese studies show that while some errors may make users abandon a product, other errors may not be judged as harshly. In fact, Mirnig et al. found that people may even like a robot more if it occasionally makes a mistake. But, as Abdolrahmani et al. note: this all depends on the context of use. 399 Our study asks how we can systematically study the impact of different kinds of errors in automatic image descriptions. Several studies have proposed different categorizations of these errors. We will discuss those studies below. 2.2 Weaknesses in system competence Hodosh and Hockenmaier (2016) and Shekhar et al. (2017) both manipulate existing image descriptions to generate flawed descriptions, which they use to see if automatic image description systems can recognize those flaws. For example, given a sentence like (2), Hodosh and Hockenmaier swap the existing scene description for another one (2→2a), and ask systems to identify the correct description. Shekhar et al. change an entity with another entity falling under the same supercategory (e.g. VEHI CLE , 2→2b), and ask systems to identify the flaw in the description. (2) Ref: A man is riding a bicycle down the street. a. A man is riding a bicycle on the"
2020.nl4xai-1.9,C86-1136,0,0.728724,"Missing"
2020.nl4xai-1.9,T87-1042,0,0.552629,"d selecting the best one is left to future work. Kutlak and van Deemter (2015) apply transformations at the logic level with the aim of making the formula more suitable for generation. Studies with human participants to determine what output of NLG systems is preferable have been conducted in other domains. Eugenio et al. (2005) study the effect of aggregation in the output of an automated tutoring system on the learner and find that what they call functional aggregation, Introduction The task of generating natural language text from logical form has a long and diverse tradition (Wang (1980), Appelt (1987), Shieber et al. (1990), to name a few early examples). It has been approached from a variety of perspectives targeting different use cases, including providing feedback to students of logic (Flickinger (2016)), users of logistic software (Kutlak and van Deemter (2015)), and explaining the output of a reasoning engine (Coppock and Baxter (2009)). However, so far in this domain very little attention has been paid to generating output which is optimally helpful to the user, presumably a nonexpert with little knowledge of formal logic, We aim to build a system which, given a logical formula, will"
2020.nl4xai-1.9,J93-1008,0,0.615582,"uestions which need to be answered when developing an explanationproducing system aimed at making the explanations maximally helpful to the user. As Kutlak and van Deemter (2015) point out, it is not always the case that the inputted logical formula is in a form suitable for generation. Some formulas are unnecessarily complex and would therefore tend to produce unnecessarily complex text unless optimised first. To make matters trickier, for expressive logics it is often not decidable whether two logical formulas are equivalent to each other (termed “the problem of logical form equivalence” by Shieber (1993)), so heuristics need to be developed to decide what transformations to apply, and how many, and to determine how suitable a formula is for generation. Kutlak and van Deemter (2015) assume as a rule of thumb that transformations which will make a formula shorter are likely to also make it easier to comprehend. However, there are some cases where making the formula longer might be warranted, e.g. if that results in a clearer NL explanation. We believe that it would be beneficial to conduct empirical studies on comprehension and preference between text variants generated based on several equival"
2020.nl4xai-1.9,C80-1061,0,0.680749,"araphrases and selecting the best one is left to future work. Kutlak and van Deemter (2015) apply transformations at the logic level with the aim of making the formula more suitable for generation. Studies with human participants to determine what output of NLG systems is preferable have been conducted in other domains. Eugenio et al. (2005) study the effect of aggregation in the output of an automated tutoring system on the learner and find that what they call functional aggregation, Introduction The task of generating natural language text from logical form has a long and diverse tradition (Wang (1980), Appelt (1987), Shieber et al. (1990), to name a few early examples). It has been approached from a variety of perspectives targeting different use cases, including providing feedback to students of logic (Flickinger (2016)), users of logistic software (Kutlak and van Deemter (2015)), and explaining the output of a reasoning engine (Coppock and Baxter (2009)). However, so far in this domain very little attention has been paid to generating output which is optimally helpful to the user, presumably a nonexpert with little knowledge of formal logic, We aim to build a system which, given a logica"
2020.scil-1.35,S17-2001,0,0.0235151,"V. If we are to consider BERT as a DSM, we must do so at the cost of cross-sentence coherence. The analysis above suggests that embeddings for tokens drawn from first sentences live in a different semantic space than tokens drawn from second sentences, i.e. that BERT contains two DSM s rather than one. If so, the comparison between two sentence-representations from a single input would be meaningless, or at least less coherent than the comparison of two sentence representations drawn from the same sentence position. To test this conjecture, we use two compositional semantics benchmarks: STS (Cer et al., 2017) and SICK - R (Marelli et al., 2014). These datasets are structured as triplets, grouping a pair larity and relatedness ratings on the STS and SICK R benchmarks of sentences with a human-annotated relatedness score. The original presentation of BERT (Devlin et al., 2018) did include a downstream application to these datasets, but employed a learned classifier, which obfuscates results (Wieting and Kiela, 2019; Cover, 1965; Hewitt and Liang, 2019). Hence we simply reduce the sequence of tokens within each sentence into a single vector by summing them, a simplistic yet robust semantic compositio"
2020.scil-1.35,D19-1627,0,0.0240045,"rent semantic fields.1 Despite the importance of this characteristic, the question whether BERT contextual embeddings depict a coherent semantic space on their own has been left mostly untouched by papers focusing on analyzing BERT or Transformers (with some exceptions, e.g. Coenen et al., 2019). Moreover, many analyses of how meaning is represented in attention-based networks or contextual embeddings include “probes” (learned models such as classifiers) as part of their evaluation setup to ‘extract’ information from the embeddings (Peters et al., 2018; Tang et al., 2018; Coenen et al., 2019; Chang and Chen, 2019, e.g.). Yet this methodology has been criticized as potentially conflicting with the intended purpose of studying the representations themselves (Wieting and Kiela, 2019; Cover, 1965); cf. also Hewitt and 1 Vectors encoding contrasts between words are also expected to be coherent (Mikolov et al., 2013b), although this assumption has been subjected to criticism (Linzen, 2016). Liang (2019) for a discussion. We refrain from using learned probes in favor of a more direct assessment of the coherence of the semantic space. 3 Experiment 1: Word Type Cohesion The trait of distributional spaces that"
2020.scil-1.35,W19-4828,0,0.260144,"ing BERT representations has become in many cases a new standard approach: for instance, all submissions at the recent shared task on gendered pronoun resolution (Webster et al., 2019) were based on BERT. Furthermore, BERT serves both as a strong baseline and as a basis for a finetuned state-of-the-art word sense disambiguation pipeline (Wang et al., 2019a). Analyses aiming to understand the mechanical behavior of Transformers in general, and BERT in particular, have suggested that they compute word representations through implicitly learned syntactic operations (Raganato and Tiedemann, 2018; Clark et al., 2019; Coenen et al., 2019; Jawahar et al., 2019, a.o.): representations computed through the ‘attention’ mechanisms of Transformers can arguably be seen as weighted sums of intermediary representations from the previous layer, with many attention heads assigning higher weights to syntactically related tokens (however, contrast with Brunner et al., 2019; Serrano and Smith, 2019). Complementing these previous studies, in this paper we adopt a more theory-driven lexical semantic perspective. While a clear parallel was established between ‘traditional’ noncontextual embeddings and the theory of distri"
2020.scil-1.35,D17-1070,0,0.0307208,"sing two distinct inputs of a single sentence each. Results are reported in table 1; we also provide comparisons with three different sentence encoders and the aforementioned W 2 V model. As we had suspected, using sum vectors drawn from a two sentence input scheme single degrades performances below the W 2 V baseline. On the other hand, a one sentence input scheme seems to produce coherent sentence representations: in that scenario, BERT performs better than W 2 V and the older sentence encoder Skip-Thought (Kiros et al., 2015), but worse than the modern USE (Cer et al., 2018) and Infersent (Conneau et al., 2017). The comparison with W 2 V also shows that BERT representations over a coherent input are more likely to include some form of compositional knowledge than traditional DSMs; however it is difficult to decide whether some true form of compositionality is achieved by BERT or whether these performances are entirely a by-product of the positional encodings. In favor of the former, other research has suggested that Transformerbased architectures perform syntactic operations (Raganato and Tiedemann, 2018; Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Voita et al., 2019; Michel"
2020.scil-1.35,P19-1285,0,0.109342,", BERT (Devlin et al., 2018) stands at the crossroad of two key innovations that have brought about significant improvements over previous state-of-the-art results. On the one hand, BERT models are an instance of contextual embeddings (McCann et al., 2017; Peters et al., 2018), which have been shown to be subtle and accurate representations of words within sentences. On the other hand, BERT is a variant of the Transformer architecture (Vaswani et al., 2017) which has set a new state-of-the-art on a wide variety of tasks ranging from machine translation (Ott et al., 2018) to language modeling (Dai et al., 2019). BERT-based models have significantly increased state-of-the-art over the GLUE benchmark for natural language understanding (Wang et al., 2019b) and most of the best scoring models for this benchmark include or elaborate on BERT. Using BERT representations has become in many cases a new standard approach: for instance, all submissions at the recent shared task on gendered pronoun resolution (Webster et al., 2019) were based on BERT. Furthermore, BERT serves both as a strong baseline and as a basis for a finetuned state-of-the-art word sense disambiguation pipeline (Wang et al., 2019a). Analys"
2020.scil-1.35,P10-2017,0,0.0699855,"Missing"
2020.scil-1.35,D15-1003,0,0.0234235,"linguistic items other than words, e.g., word senses—based both on meaning inventories (Rothe and Sch¨utze, 2015) and word sense induction techniques (Bartunov et al., 2015)—or meaning exemplars (Reisinger and Mooney, 2010; Erk and Pad´o, 2010; Reddy et al., 2011). The default approach has however been to produce representations for word types. Word properties encoded by DSMs vary from morphological information (Marelli and Baroni, 2015; Bonami and Paperno, 2018) to geographic information (Louwerse and Zwaan, 2009), to social stereotypes (Bolukbasi et al., 2016) and to referential properties (Herbelot and Vecchi, 2015). A reason why contextualized embeddings have not been equated to distributional semantics may lie in that they are “functions of the entire input sentence” (Peters et al., 2018). Whereas traditional DSMs match word types with numeric vectors, contextualized embeddings produce distinct vectors per token. Ideally, the contextualized nature of these embeddings should reflect the semantic nuances that context induces in the meaning of a word—with varying degrees of subtlety, ranging from broad word-sense disambiguation (e.g. ‘bank’ as a river embankment or as a financial institution) to narrower"
2020.scil-1.35,D19-1275,0,0.0138214,"parison of two sentence representations drawn from the same sentence position. To test this conjecture, we use two compositional semantics benchmarks: STS (Cer et al., 2017) and SICK - R (Marelli et al., 2014). These datasets are structured as triplets, grouping a pair larity and relatedness ratings on the STS and SICK R benchmarks of sentences with a human-annotated relatedness score. The original presentation of BERT (Devlin et al., 2018) did include a downstream application to these datasets, but employed a learned classifier, which obfuscates results (Wieting and Kiela, 2019; Cover, 1965; Hewitt and Liang, 2019). Hence we simply reduce the sequence of tokens within each sentence into a single vector by summing them, a simplistic yet robust semantic composition method. We then compute the Spearman correlation between the cosines of the two sum vectors and the sentence pair’s relatedness score. We compare two setups: a “two sentences input” scheme (or 2 sent. ipt. for short)—where we use the sequences of vectors obtained by passing the two sentences as a single input—and a “one sentence input” scheme (1 sent. ipt.)—using two distinct inputs of a single sentence each. Results are reported in table 1; we"
2020.scil-1.35,N19-1419,0,0.135204,"ture leaves unanswered the question of what consequences there are to framing contextualized embeddings as DSMs. The analyses that contextual embeddings have been subjected to differ from most analyses of distributional semantics models. Peters et al. (2018) analyzed through an extensive ablation study of ELM o what information is captured by each layer of their architecture. Devlin et al. (2018) discussed what part of their architecture is critical to the performances of BERT, comparing pretraining objectives, number of layers and training duration. Other works (Raganato and Tiedemann, 2018; Hewitt and Manning, 2019; Clark et al., 2019; Voita et al., 2019; Michel et al., 2019) have introduced specific procedures to understand how attention-based architectures function on a mechanical level. Recent research has however questioned the pertinence of these attention-based analyses (Serrano and Smith, 2019; Brunner et al., 2019); moreover these works have focused more on the inner workings of the networks than on their adequacy with theories of meaning. One trait of DSMs that is very often encountered, discussed and exploited in the literature is the fact that the relative positions of embeddings are not rand"
2020.scil-1.35,P19-1356,0,0.115744,"ny cases a new standard approach: for instance, all submissions at the recent shared task on gendered pronoun resolution (Webster et al., 2019) were based on BERT. Furthermore, BERT serves both as a strong baseline and as a basis for a finetuned state-of-the-art word sense disambiguation pipeline (Wang et al., 2019a). Analyses aiming to understand the mechanical behavior of Transformers in general, and BERT in particular, have suggested that they compute word representations through implicitly learned syntactic operations (Raganato and Tiedemann, 2018; Clark et al., 2019; Coenen et al., 2019; Jawahar et al., 2019, a.o.): representations computed through the ‘attention’ mechanisms of Transformers can arguably be seen as weighted sums of intermediary representations from the previous layer, with many attention heads assigning higher weights to syntactically related tokens (however, contrast with Brunner et al., 2019; Serrano and Smith, 2019). Complementing these previous studies, in this paper we adopt a more theory-driven lexical semantic perspective. While a clear parallel was established between ‘traditional’ noncontextual embeddings and the theory of distributional semantics (a.o. Lenci, 2018; Boled"
2020.scil-1.35,2020.tacl-1.5,0,0.0447762,"Missing"
2020.scil-1.35,W14-1618,0,0.27265,"be made: the former are distributed representations of word meaning, the latter a theory stating that a word’s meaning is drawn from its distribution. In short, word embeddings could be understood as a vector-based implementation of the distributional hypothesis. This parallel is much less obvious for contextual embeddings: are constantly changing representations truly an apt description of the meaning of a word? More precisely, the literature on distributional semantics has put forth and discussed many mathematical properties of embeddings: embeddings are equivalent to count-based matrices (Levy and Goldberg, 2014b), expected to be linearly dependant (Arora et al., 2016), expressible as a unitary matrix (Smith et al., 2017) or as a perturbation of an identity matrix (Yin and Shen, 2018). All these properties have however been formalized for non-contextual embeddings: they were formulated using the tools of matrix algebra, under the assumption that embedding matrix rows correspond to word types. Hence they cannot be applied as such to contextual embeddings. This disconnect in the literature leaves unanswered the question of what consequences there are to framing contextualized embeddings as DSMs. The an"
2020.scil-1.35,W16-2503,0,0.0162358,"ntextual embeddings include “probes” (learned models such as classifiers) as part of their evaluation setup to ‘extract’ information from the embeddings (Peters et al., 2018; Tang et al., 2018; Coenen et al., 2019; Chang and Chen, 2019, e.g.). Yet this methodology has been criticized as potentially conflicting with the intended purpose of studying the representations themselves (Wieting and Kiela, 2019; Cover, 1965); cf. also Hewitt and 1 Vectors encoding contrasts between words are also expected to be coherent (Mikolov et al., 2013b), although this assumption has been subjected to criticism (Linzen, 2016). Liang (2019) for a discussion. We refrain from using learned probes in favor of a more direct assessment of the coherence of the semantic space. 3 Experiment 1: Word Type Cohesion The trait of distributional spaces that we focus on in this study is that similar words should lie in similar regions of the semantic space. This should hold all the more so for identical words, which ought to be be maximally similar. By design, contextualized embeddings like BERT exhibit variation within vectors corresponding to identical word types. Thus, if BERT is a DSM, we expect that word types form natural,"
2020.scil-1.35,2021.ccl-1.108,0,0.119947,"Missing"
2020.scil-1.35,marelli-etal-2014-sick,0,0.0298912,"s a DSM, we must do so at the cost of cross-sentence coherence. The analysis above suggests that embeddings for tokens drawn from first sentences live in a different semantic space than tokens drawn from second sentences, i.e. that BERT contains two DSM s rather than one. If so, the comparison between two sentence-representations from a single input would be meaningless, or at least less coherent than the comparison of two sentence representations drawn from the same sentence position. To test this conjecture, we use two compositional semantics benchmarks: STS (Cer et al., 2017) and SICK - R (Marelli et al., 2014). These datasets are structured as triplets, grouping a pair larity and relatedness ratings on the STS and SICK R benchmarks of sentences with a human-annotated relatedness score. The original presentation of BERT (Devlin et al., 2018) did include a downstream application to these datasets, but employed a learned classifier, which obfuscates results (Wieting and Kiela, 2019; Cover, 1965; Hewitt and Liang, 2019). Hence we simply reduce the sequence of tokens within each sentence into a single vector by summing them, a simplistic yet robust semantic composition method. We then compute the Spearm"
2020.scil-1.35,A94-1016,0,0.123081,"Missing"
2020.scil-1.35,W18-6301,0,0.0229081,"troduction A recent success story of NLP, BERT (Devlin et al., 2018) stands at the crossroad of two key innovations that have brought about significant improvements over previous state-of-the-art results. On the one hand, BERT models are an instance of contextual embeddings (McCann et al., 2017; Peters et al., 2018), which have been shown to be subtle and accurate representations of words within sentences. On the other hand, BERT is a variant of the Transformer architecture (Vaswani et al., 2017) which has set a new state-of-the-art on a wide variety of tasks ranging from machine translation (Ott et al., 2018) to language modeling (Dai et al., 2019). BERT-based models have significantly increased state-of-the-art over the GLUE benchmark for natural language understanding (Wang et al., 2019b) and most of the best scoring models for this benchmark include or elaborate on BERT. Using BERT representations has become in many cases a new standard approach: for instance, all submissions at the recent shared task on gendered pronoun resolution (Webster et al., 2019) were based on BERT. Furthermore, BERT serves both as a strong baseline and as a basis for a finetuned state-of-the-art word sense disambiguati"
2020.scil-1.35,N18-1202,0,0.737028,"herence, BERT does not fully live up to the natural expectations for a semantic vector space. In particular, we find that the position of the sentence in which a word occurs, while having no meaning correlates, leaves a noticeable trace on the word embeddings and disturbs similarity relationships. 1 Introduction A recent success story of NLP, BERT (Devlin et al., 2018) stands at the crossroad of two key innovations that have brought about significant improvements over previous state-of-the-art results. On the one hand, BERT models are an instance of contextual embeddings (McCann et al., 2017; Peters et al., 2018), which have been shown to be subtle and accurate representations of words within sentences. On the other hand, BERT is a variant of the Transformer architecture (Vaswani et al., 2017) which has set a new state-of-the-art on a wide variety of tasks ranging from machine translation (Ott et al., 2018) to language modeling (Dai et al., 2019). BERT-based models have significantly increased state-of-the-art over the GLUE benchmark for natural language understanding (Wang et al., 2019b) and most of the best scoring models for this benchmark include or elaborate on BERT. Using BERT representations ha"
2020.scil-1.35,W18-5431,0,0.361391,"clude or elaborate on BERT. Using BERT representations has become in many cases a new standard approach: for instance, all submissions at the recent shared task on gendered pronoun resolution (Webster et al., 2019) were based on BERT. Furthermore, BERT serves both as a strong baseline and as a basis for a finetuned state-of-the-art word sense disambiguation pipeline (Wang et al., 2019a). Analyses aiming to understand the mechanical behavior of Transformers in general, and BERT in particular, have suggested that they compute word representations through implicitly learned syntactic operations (Raganato and Tiedemann, 2018; Clark et al., 2019; Coenen et al., 2019; Jawahar et al., 2019, a.o.): representations computed through the ‘attention’ mechanisms of Transformers can arguably be seen as weighted sums of intermediary representations from the previous layer, with many attention heads assigning higher weights to syntactically related tokens (however, contrast with Brunner et al., 2019; Serrano and Smith, 2019). Complementing these previous studies, in this paper we adopt a more theory-driven lexical semantic perspective. While a clear parallel was established between ‘traditional’ noncontextual embeddings and"
2020.scil-1.35,I11-1079,0,0.0296061,"tics models (DSMs). DSM s assume that meaning is derived from use in context. DSMs are nowadays systematically represented using vector spaces (Lenci, 2018). They generally map each word in the domain of the model to a numeric vector on the basis of distributional criteria; vector components are inferred from text data. DSMs have also been computed for linguistic items other than words, e.g., word senses—based both on meaning inventories (Rothe and Sch¨utze, 2015) and word sense induction techniques (Bartunov et al., 2015)—or meaning exemplars (Reisinger and Mooney, 2010; Erk and Pad´o, 2010; Reddy et al., 2011). The default approach has however been to produce representations for word types. Word properties encoded by DSMs vary from morphological information (Marelli and Baroni, 2015; Bonami and Paperno, 2018) to geographic information (Louwerse and Zwaan, 2009), to social stereotypes (Bolukbasi et al., 2016) and to referential properties (Herbelot and Vecchi, 2015). A reason why contextualized embeddings have not been equated to distributional semantics may lie in that they are “functions of the entire input sentence” (Peters et al., 2018). Whereas traditional DSMs match word types with numeric vec"
2020.scil-1.35,N10-1013,0,0.0553145,"contextualized extension of distributional semantics models (DSMs). DSM s assume that meaning is derived from use in context. DSMs are nowadays systematically represented using vector spaces (Lenci, 2018). They generally map each word in the domain of the model to a numeric vector on the basis of distributional criteria; vector components are inferred from text data. DSMs have also been computed for linguistic items other than words, e.g., word senses—based both on meaning inventories (Rothe and Sch¨utze, 2015) and word sense induction techniques (Bartunov et al., 2015)—or meaning exemplars (Reisinger and Mooney, 2010; Erk and Pad´o, 2010; Reddy et al., 2011). The default approach has however been to produce representations for word types. Word properties encoded by DSMs vary from morphological information (Marelli and Baroni, 2015; Bonami and Paperno, 2018) to geographic information (Louwerse and Zwaan, 2009), to social stereotypes (Bolukbasi et al., 2016) and to referential properties (Herbelot and Vecchi, 2015). A reason why contextualized embeddings have not been equated to distributional semantics may lie in that they are “functions of the entire input sentence” (Peters et al., 2018). Whereas traditio"
2020.scil-1.35,P15-1173,0,0.0707347,"Missing"
2020.scil-1.35,P19-1282,0,0.141649,"aiming to understand the mechanical behavior of Transformers in general, and BERT in particular, have suggested that they compute word representations through implicitly learned syntactic operations (Raganato and Tiedemann, 2018; Clark et al., 2019; Coenen et al., 2019; Jawahar et al., 2019, a.o.): representations computed through the ‘attention’ mechanisms of Transformers can arguably be seen as weighted sums of intermediary representations from the previous layer, with many attention heads assigning higher weights to syntactically related tokens (however, contrast with Brunner et al., 2019; Serrano and Smith, 2019). Complementing these previous studies, in this paper we adopt a more theory-driven lexical semantic perspective. While a clear parallel was established between ‘traditional’ noncontextual embeddings and the theory of distributional semantics (a.o. Lenci, 2018; Boleda, 2019), this link is not automatically extended to contextual embeddings: some authors (Westera and Boleda, 2019) even explicitly consider only “context-invariant” representations as distributional semantics. Hence we study to what extent BERT, as a contextual embedding architecture, satisfies the properties expected from a natur"
2020.scil-1.35,N13-1090,0,0.74064,"ver, many analyses of how meaning is represented in attention-based networks or contextual embeddings include “probes” (learned models such as classifiers) as part of their evaluation setup to ‘extract’ information from the embeddings (Peters et al., 2018; Tang et al., 2018; Coenen et al., 2019; Chang and Chen, 2019, e.g.). Yet this methodology has been criticized as potentially conflicting with the intended purpose of studying the representations themselves (Wieting and Kiela, 2019; Cover, 1965); cf. also Hewitt and 1 Vectors encoding contrasts between words are also expected to be coherent (Mikolov et al., 2013b), although this assumption has been subjected to criticism (Linzen, 2016). Liang (2019) for a discussion. We refrain from using learned probes in favor of a more direct assessment of the coherence of the semantic space. 3 Experiment 1: Word Type Cohesion The trait of distributional spaces that we focus on in this study is that similar words should lie in similar regions of the semantic space. This should hold all the more so for identical words, which ought to be be maximally similar. By design, contextualized embeddings like BERT exhibit variation within vectors corresponding to identical w"
2020.scil-1.35,N18-2074,0,0.0260201,"al similarity. One way to overcome this violation of crosssentence coherence would be to consider first and second sentences representations as belonging to distinct distributional semantic spaces. The fact that first sentences were shown to have on average higher pairwise cosines than second sentences can be partially explained by the use of absolute positional encodings in BERT representations. Although positional encodings are required so that the model does not devolve into a bag-of-word system, absolute encodings are not: other works have proposed alternative relative position encodings (Shaw et al., 2018; Dai et al., 2019, e.g.); replacing the former with the latter may alleviate the gap in lexical contrasts. Other related questions that we must leave to future works encompass testing on other BERT models such as the wholewords model, or that of Liu et al. (2019) which differs only by its training objectives, as well as other contextual embeddings architectures. Our findings suggest that the formulation of the NSP objective of BERT obfuscates its relation to distributional semantics, by introducing a systematic distinction between first and second sentences which impacts the output embeddings"
2020.scil-1.35,W18-6304,0,0.0285563,"gions of the vectors space describe coherent semantic fields.1 Despite the importance of this characteristic, the question whether BERT contextual embeddings depict a coherent semantic space on their own has been left mostly untouched by papers focusing on analyzing BERT or Transformers (with some exceptions, e.g. Coenen et al., 2019). Moreover, many analyses of how meaning is represented in attention-based networks or contextual embeddings include “probes” (learned models such as classifiers) as part of their evaluation setup to ‘extract’ information from the embeddings (Peters et al., 2018; Tang et al., 2018; Coenen et al., 2019; Chang and Chen, 2019, e.g.). Yet this methodology has been criticized as potentially conflicting with the intended purpose of studying the representations themselves (Wieting and Kiela, 2019; Cover, 1965); cf. also Hewitt and 1 Vectors encoding contrasts between words are also expected to be coherent (Mikolov et al., 2013b), although this assumption has been subjected to criticism (Linzen, 2016). Liang (2019) for a discussion. We refrain from using learned probes in favor of a more direct assessment of the coherence of the semantic space. 3 Experiment 1: Word Type Cohesi"
2020.scil-1.35,2019.gwc-1.14,0,0.0566502,"Missing"
2020.scil-1.35,P19-1580,0,0.116937,"nsequences there are to framing contextualized embeddings as DSMs. The analyses that contextual embeddings have been subjected to differ from most analyses of distributional semantics models. Peters et al. (2018) analyzed through an extensive ablation study of ELM o what information is captured by each layer of their architecture. Devlin et al. (2018) discussed what part of their architecture is critical to the performances of BERT, comparing pretraining objectives, number of layers and training duration. Other works (Raganato and Tiedemann, 2018; Hewitt and Manning, 2019; Clark et al., 2019; Voita et al., 2019; Michel et al., 2019) have introduced specific procedures to understand how attention-based architectures function on a mechanical level. Recent research has however questioned the pertinence of these attention-based analyses (Serrano and Smith, 2019; Brunner et al., 2019); moreover these works have focused more on the inner workings of the networks than on their adequacy with theories of meaning. One trait of DSMs that is very often encountered, discussed and exploited in the literature is the fact that the relative positions of embeddings are not random. Early vector space models, by design"
2020.scil-1.35,W19-3801,0,0.0853885,"Missing"
2020.scil-1.35,W19-0410,0,0.0171641,"rguably be seen as weighted sums of intermediary representations from the previous layer, with many attention heads assigning higher weights to syntactically related tokens (however, contrast with Brunner et al., 2019; Serrano and Smith, 2019). Complementing these previous studies, in this paper we adopt a more theory-driven lexical semantic perspective. While a clear parallel was established between ‘traditional’ noncontextual embeddings and the theory of distributional semantics (a.o. Lenci, 2018; Boleda, 2019), this link is not automatically extended to contextual embeddings: some authors (Westera and Boleda, 2019) even explicitly consider only “context-invariant” representations as distributional semantics. Hence we study to what extent BERT, as a contextual embedding architecture, satisfies the properties expected from a natural contextualized extension of distributional semantics models (DSMs). DSM s assume that meaning is derived from use in context. DSMs are nowadays systematically represented using vector spaces (Lenci, 2018). They generally map each word in the domain of the model to a numeric vector on the basis of distributional criteria; vector components are inferred from text data. DSMs have"
2021.inlg-1.15,2020.acl-main.140,0,0.0344618,"Missing"
2021.inlg-1.15,P17-1080,0,0.0145362,"dia. Delexicialised Text: Pre-context: AWH Engineering College is in “Kuttikkattoor” , India in the state of Kerala . Target Entity: AWH Engineering College Pos-context: has 250 employees and Kerala is ruled by Kochi . The Ganges River is also found in India . Table 1: An example data from the WebNLG corpus. In the delexicalised text, every entity is underlined. series of probing tasks. Using probing tasks is a well-established method to analyse whether a model’s latent representation encodes specific information. This approach has been widely used for analysing models in machine translation (Belinkov et al., 2017), language modelling (Giulianelli et al., 2018), relation extraction (Alt et al., 2020), and so on. Additionally, there had been various works on coreference resolution and bridging anaphora (Sorodoc et al., 2020; Pandit and Hou, 2021) which, similar to this paper, target the understanding of reference. More precisely, for a probing task, a diagnostic classifier is trained on representations from the model. Its performance embodies how well those representations encode the information associated with the probing task. The aim of this paper is to understand what linguistic features neural model"
2021.inlg-1.15,W07-2302,0,0.0770288,"iscourse structure properties beyond the sentence level. 1 Introduction Referring Expression Generation (REG) is one of the main stages of classic Natural Language Generation (NLG) pipeline (Reiter and Dale, 2000; Krahmer and van Deemter, 2012; van Deemter, 2016). REG studies are concerned with two different tasks. The goal of the classic REG task (also called oneshot REG), is to find a set of attributes to single out a referent from a set of competing referents. The second REG task (henceforth discourse REG) is concerned with the generation of referring expressions (RE) in discourse context. Belz and Varges (2007) phrase it as follows: Given an intended referent and a discourse context, how do we generate appropriate referential expressions (REs) to refer to the referent at different points in the discourse? Classic discourse REG was usually understood as a two-step procedure. In the first step, the referential form (RF, i.e, the syntactic type) is determined. For instance, when referring to Joe Biden at a given point in a discourse, the first step is to decide whether to use a proper name (“Joe Biden”), a ∗ Equal contribution description (“the president of the USA”), a demonstrative (“this person”) or"
2021.inlg-1.15,D19-1052,0,0.0384623,"Missing"
2021.inlg-1.15,P18-1182,0,0.0336396,"Missing"
2021.inlg-1.15,W18-6521,0,0.160661,"linguistic features. For example, Henschel et al. (2000) investigated the impact of 3 linguistic features namely recency, subjecthood, and discourse status on pronominalization, i.e. deciding whether the RE should be realised as a pronoun. Using these features, they used the notion of local focus as a criterion for detecting the set of referents that can be pronominalised. The same holds for feature-based models (see Belz et al. (2010) for an overview) where models are trained on linguistically encoded data. More recently, a number of neural networkbased REG models have been presented (Castro Ferreira et al., 2018a; Cao and Cheung, 2019; Cunha et al., 2020), where they propose to generate REs in an End2End manner without any feature engineering. They all used a benchmark dataset called WebNLG. These models generally follow the sequence-to-sequence framework (Sutskever et al., 2014), where there is an encoder for encoding the given discourse, and a decoder responsible for generating REs using the encoded information. The evaluation results suggested that these neural methods perform well not only for selecting the proper RFs, but also for producing fluent REs. However, it was unclear to what extent thes"
2021.inlg-1.15,W18-6519,1,0.905184,"Missing"
2021.inlg-1.15,D19-1312,0,0.0605528,"example, Henschel et al. (2000) investigated the impact of 3 linguistic features namely recency, subjecthood, and discourse status on pronominalization, i.e. deciding whether the RE should be realised as a pronoun. Using these features, they used the notion of local focus as a criterion for detecting the set of referents that can be pronominalised. The same holds for feature-based models (see Belz et al. (2010) for an overview) where models are trained on linguistically encoded data. More recently, a number of neural networkbased REG models have been presented (Castro Ferreira et al., 2018a; Cao and Cheung, 2019; Cunha et al., 2020), where they propose to generate REs in an End2End manner without any feature engineering. They all used a benchmark dataset called WebNLG. These models generally follow the sequence-to-sequence framework (Sutskever et al., 2014), where there is an encoder for encoding the given discourse, and a decoder responsible for generating REs using the encoded information. The evaluation results suggested that these neural methods perform well not only for selecting the proper RFs, but also for producing fluent REs. However, it was unclear to what extent these neural models can enc"
2021.inlg-1.15,D14-1179,0,0.0392889,"Missing"
2021.inlg-1.15,P16-1054,0,0.0161119,"r. Concretely, for each input, we first use a model discussed in section 3 to obtain its representation R. As mentioned in section 3, we ran each model five times and reported their averaged scores. For the probing tasks, we use the representations of the models with the best RFS performance on the development set. 4.1 Probing Tasks Following our observations in section 2.2, we formulate the following probing tasks. Referential Status. The referential status of the target entity influences the choice of RF in both linguistic (Chafe, 1976; Gundel et al., 1993) and computational studies (Castro Ferreira et al., 2016). In this study, we define referential status on two levels: discourse-level and sentence-level. The former (DisStat) has two possible values: (a) discourseold (i.e., the entity has appeared in the previous discourse) and (b) discourse-new (i.e., the entity has not appeared in the previous discourse). Sentencelevel referential status (SenStat) also consists of two values: (a) sentence-new (i.e., the RE is the first mention of the entity in the sentence), and (b) sentence-old (i.e., the RE is not the first mention). Syntactic Position. Entities in subject position are more likely to be pronomin"
2021.inlg-1.15,N19-1423,0,0.0205283,"rget entity is in position i of the concatenated sequence, we extract the i-th representation from hi for obtaining R = ReLU(Wf hi ). After obtaining R, the rest of the procedure is the same as ConATT. Pre-training. As a secondary objective of this study, we want to see whether RFS can benefit from pre-trained word embeddings and language models, whose effectiveness has not yet been explored in REG1 . For both c-RNN and ConATT, we try the GloVe embeddings (Pennington et al., 2014) to see how pre-trained word embeddings contribute to the choice of RF. For c-RNN, we try to stake it on the BERT (Devlin et al., 2019) model. In order to let BERT better encode the delexicalised entity labels, we first re-train BERT as a masked language model on the training data of WebNLG. We then freeze the parameters of BERT and use the model to encode the input, which is then fed into c-RNN2 . where Wf is the weights in the feedforward layer. R is also used as the input of the probing classifiers (section 4). R is then fed for making the final prediction: P (f |x(pre) , x(r) , x(pos) ) = Softmax(Wc R), (5) where Wc is the weight in the output layer. c-RNN. In addition to ConATT, we also try a simpler yet effective struct"
2021.inlg-1.15,P17-1017,0,0.0171665,"sk is to build an algorithm that generates all these REs. So far, this task has attracted many research efforts (e.g., Hendrickx et al. (2008); Greenbacker and McCoy (2009)) and it has been used in the GREC shared tasks (Belz et al., 2010). More recently, this task was formulated into a format that goes together well with deep learning: Castro Ferreira et al. (2018a) introduced the End2End REG task, built a corresponding dataset based on WebNLG (Castro Ferreira et al., 2018b), and constructed NeuralREG models. The WebNLG corpus was originally designed to assess the performance of NLG systems (Gardent et al., 2017). Each sample in this corpus contains a knowledge base described by a Resource Description Framework (RDF) triple (Table 1). Castro Ferreira et al. (2018a) and Castro Ferreira et al. (2018b) enriched and delexicalised the corpus to fit the discourse REG task. Table 1 shows a text created from a RDF, and its corresponding delexicalised version. Taking the delexicalised text in Table 1 as an example, given the entity “AWH Engineering College”, REG chooses a RE based on that entity and its pre-context (“AWH Engineering College is in “Kuttikkattoor” , India in the state of Kerala . ”) and its pos-"
2021.inlg-1.15,W18-5426,0,0.0281347,"ngineering College is in “Kuttikkattoor” , India in the state of Kerala . Target Entity: AWH Engineering College Pos-context: has 250 employees and Kerala is ruled by Kochi . The Ganges River is also found in India . Table 1: An example data from the WebNLG corpus. In the delexicalised text, every entity is underlined. series of probing tasks. Using probing tasks is a well-established method to analyse whether a model’s latent representation encodes specific information. This approach has been widely used for analysing models in machine translation (Belinkov et al., 2017), language modelling (Giulianelli et al., 2018), relation extraction (Alt et al., 2020), and so on. Additionally, there had been various works on coreference resolution and bridging anaphora (Sorodoc et al., 2020; Pandit and Hou, 2021) which, similar to this paper, target the understanding of reference. More precisely, for a probing task, a diagnostic classifier is trained on representations from the model. Its performance embodies how well those representations encode the information associated with the probing task. The aim of this paper is to understand what linguistic features neural models encode when modelling REs. Our main focus is"
2021.inlg-1.15,W09-2819,0,0.106015,"Missing"
2021.inlg-1.15,J95-2003,0,0.912087,"d (or given) referents. Recency, another well-studied cue, is defined as the distance between the target referent and its antecedent. If a referent is not too far apart from its antecedent, then reduced forms are typically employed to refer to it. There are also intra-clausal cues such as grammatical role (Brennan, 1995) and thematic role (Arnold, 2001) which impact the prominence status of referents. For instance, the subject of a sentence is perceived to be more prominent than the object. Discourse-structural features affect the organisational aspects of discourse. Centering-based theories (Grosz et al., 1995) often use the notion of local focus to account for pronominalisation. Local focus takes the current and previous utterance into account. Global focus, on the other hand, situates a referent in a larger space, namely the whole text or a discourse segment (Hinterwimmer, 2019). Concepts such as the importance of a referent or familiarity are associated with the global prominence status of entities (Siddharthan et al., 2011). Type Classes 4-Way Demonstrative, Description, Proper Name, Pronoun Description, Proper Name, Pronoun Non-pronominal, Pronominal 3-Way 2-Way Table 2: 3 different types of RF"
2021.inlg-1.15,D19-1275,0,0.0184946,"well in the probing tasks. However, these results should still be taken with a pinch of salt: the variable importance has been conducted on the ML model and not on the neural models. We cannot be certain that the same features contribute to all the models similarly: a feature might be quite important in the machine learning model, but not as important in the neural models. On the other hand, some researches have questioned the validity of probing methods. They found out that it is difficult to distinguish between “learning the probing task” and “extracting the encoded linguistic information” (Hewitt and Liang, 2019; Kunz and Kuhlmann, 2020) for a probing classifier. This suggests that higher performance of a probing classifier does not necessarily mean more linguistic information has been encoded. This prevents us from directly quantifying how well the linguistic information has been learnt using the performance of probing classifiers and requires us to make conclusions more carefully. From our probing efforts, we conclude that: (1) All neural models have learnt some information about the features associated with the probing tasks, but how well they have learnt this information is yet to be assessed; (2"
2021.inlg-1.15,N06-2015,0,0.0510814,"from the text itself but from the wider context in which it is written and read. We believe that future models should take these lessons into consideration. In future, we plan to extend the current study from three angles. First, we plan to conduct experiments on different corpora. The WebNLG corpus used in this study consists predominantly of extremely short documents with an average length of only 1.4 sentences/document; consequently the majority of REs are first mentions. We hope to find a more representative distribution of uses of referring expressions in other corpora such as OntoNotes (Hovy et al., 2006), which contain longer texts. Secondly, we plan to conduct experiments on other languages than English, in particular ones that favour zero pronouns (e.g., Chinese (Chen et al., 2018)), because these pose new challenges for the task of RFS. Thirdly, we plan to design new probing tasks on the basis of other factors that could influence RFS, such as animacy, competition and positional attributes (see Same and van Deemter (2020) for an overview). Acknowledgements We thank the anonymous reviewers for their helpful comments. Guanyi Chen is supported by China Scholarship Council (No.201907720022). F"
2021.inlg-1.15,J12-1006,1,0.788031,"Missing"
2021.inlg-1.15,2020.coling-main.450,0,0.0322027,"s. However, these results should still be taken with a pinch of salt: the variable importance has been conducted on the ML model and not on the neural models. We cannot be certain that the same features contribute to all the models similarly: a feature might be quite important in the machine learning model, but not as important in the neural models. On the other hand, some researches have questioned the validity of probing methods. They found out that it is difficult to distinguish between “learning the probing task” and “extracting the encoded linguistic information” (Hewitt and Liang, 2019; Kunz and Kuhlmann, 2020) for a probing classifier. This suggests that higher performance of a probing classifier does not necessarily mean more linguistic information has been encoded. This prevents us from directly quantifying how well the linguistic information has been learnt using the performance of probing classifiers and requires us to make conclusions more carefully. From our probing efforts, we conclude that: (1) All neural models have learnt some information about the features associated with the probing tasks, but how well they have learnt this information is yet to be assessed; (2) The WebNLG corpus, which"
2021.inlg-1.15,2021.naacl-main.327,0,0.182061,"is also found in India . Table 1: An example data from the WebNLG corpus. In the delexicalised text, every entity is underlined. series of probing tasks. Using probing tasks is a well-established method to analyse whether a model’s latent representation encodes specific information. This approach has been widely used for analysing models in machine translation (Belinkov et al., 2017), language modelling (Giulianelli et al., 2018), relation extraction (Alt et al., 2020), and so on. Additionally, there had been various works on coreference resolution and bridging anaphora (Sorodoc et al., 2020; Pandit and Hou, 2021) which, similar to this paper, target the understanding of reference. More precisely, for a probing task, a diagnostic classifier is trained on representations from the model. Its performance embodies how well those representations encode the information associated with the probing task. The aim of this paper is to understand what linguistic features neural models encode when modelling REs. Our main focus is on the encoding of linguistic features in the representations. In the linguistic tradition, the majority of RE production studies focus on Referential Form Selection (RFS), rather than RE"
2021.inlg-1.15,W08-1129,0,0.12463,"Missing"
2021.inlg-1.15,D14-1162,0,0.0851618,"first concatenate x(pre) , x(r) , and x(pos) , and then encode them together: h = BiGRU([x(pre) , x(r) , x(pos) ]). (6) Suppose that the target entity is in position i of the concatenated sequence, we extract the i-th representation from hi for obtaining R = ReLU(Wf hi ). After obtaining R, the rest of the procedure is the same as ConATT. Pre-training. As a secondary objective of this study, we want to see whether RFS can benefit from pre-trained word embeddings and language models, whose effectiveness has not yet been explored in REG1 . For both c-RNN and ConATT, we try the GloVe embeddings (Pennington et al., 2014) to see how pre-trained word embeddings contribute to the choice of RF. For c-RNN, we try to stake it on the BERT (Devlin et al., 2019) model. In order to let BERT better encode the delexicalised entity labels, we first re-train BERT as a masked language model on the training data of WebNLG. We then freeze the parameters of BERT and use the model to encode the input, which is then fed into c-RNN2 . where Wf is the weights in the feedforward layer. R is also used as the input of the probing classifiers (section 4). R is then fed for making the final prediction: P (f |x(pre) , x(r) , x(pos) ) ="
2021.inlg-1.15,C00-1045,0,0.475194,"this person”) or a pronoun (“he”). The second step is to determine the RE content, that is, to choose between all the different ways in which a given form can be realised. For instance, to generate a description of Joe Biden, one needs to decide whether to only mention his job (e.g., The president entered the Oval Office.), or to mention the country as well (e.g., The president of the United states arrived in Cornwall for the G7 Summit.) In earlier works, computational linguists linked REG to linguistic theories and built discourse REG systems on the basis of linguistic features. For example, Henschel et al. (2000) investigated the impact of 3 linguistic features namely recency, subjecthood, and discourse status on pronominalization, i.e. deciding whether the RE should be realised as a pronoun. Using these features, they used the notion of local focus as a criterion for detecting the set of referents that can be pronominalised. The same holds for feature-based models (see Belz et al. (2010) for an overview) where models are trained on linguistically encoded data. More recently, a number of neural networkbased REG models have been presented (Castro Ferreira et al., 2018a; Cao and Cheung, 2019; Cunha et a"
2021.inlg-1.15,2020.coling-main.403,1,0.845732,"Missing"
2021.inlg-1.15,J11-4007,0,0.154034,"the subject of a sentence is perceived to be more prominent than the object. Discourse-structural features affect the organisational aspects of discourse. Centering-based theories (Grosz et al., 1995) often use the notion of local focus to account for pronominalisation. Local focus takes the current and previous utterance into account. Global focus, on the other hand, situates a referent in a larger space, namely the whole text or a discourse segment (Hinterwimmer, 2019). Concepts such as the importance of a referent or familiarity are associated with the global prominence status of entities (Siddharthan et al., 2011). Type Classes 4-Way Demonstrative, Description, Proper Name, Pronoun Description, Proper Name, Pronoun Non-pronominal, Pronominal 3-Way 2-Way Table 2: 3 different types of RF classification. 3 Neural Referential Form Selection In this section, we define the task of RFS built on the WebNLG dataset, and introduce a number of NeuralRFS models. 3.1 The RFS Task Akin to REG, given the previous context x(pre) = {w1 , w2 , ..., wi−1 } (where w is either a word or a delexicalised entity label), the target referent w(r) = {wi }, and the post context w(pos) = {wi , wi+1 , ..., wn }, a RFS algorithm aim"
2021.inlg-1.15,2020.acl-main.384,0,0.16971,"hi . The Ganges River is also found in India . Table 1: An example data from the WebNLG corpus. In the delexicalised text, every entity is underlined. series of probing tasks. Using probing tasks is a well-established method to analyse whether a model’s latent representation encodes specific information. This approach has been widely used for analysing models in machine translation (Belinkov et al., 2017), language modelling (Giulianelli et al., 2018), relation extraction (Alt et al., 2020), and so on. Additionally, there had been various works on coreference resolution and bridging anaphora (Sorodoc et al., 2020; Pandit and Hou, 2021) which, similar to this paper, target the understanding of reference. More precisely, for a probing task, a diagnostic classifier is trained on representations from the model. Its performance embodies how well those representations encode the information associated with the probing task. The aim of this paper is to understand what linguistic features neural models encode when modelling REs. Our main focus is on the encoding of linguistic features in the representations. In the linguistic tradition, the majority of RE production studies focus on Referential Form Selection"
2021.inlg-1.15,N16-1174,0,0.0127676,"51.98 51.55 71.27 69.24 68.34 86.64 82.76 84.57 c-RNN +GloVe +BERT ConATT +GloVe 68.79 69.10 62.63 67.42 65.98 62.95 63.90 61.80 62.39 62.49 64.96 65.40 62.15 64.07 63.67 84.49 84.29 83.02 85.04 83.62 82.52 82.55 81.44 82.21 81.41 83.63 83.30 82.15 83.53 82.45 90.31 89.33 90.98 89.30 89.60 88.01 88.02 88.00 89.19 88.06 89.09 88.63 89.42 89.23 88.80 Table 3: Evaluation results of our RFS systems on WEBNLG. Best results are boldfaced, whereas the second best results are underlined. ent from Castro Ferreira et al. (2018a), we encode h(k) into the context representation c(k) using selfattention (Yang et al., 2016). Concretely, given the total N steps in h(k) , we first calculate the attention (k) weight αj at each step j by: (k) ej (k) = va(k)T tanh(Wa(k) hj ), (1) (k) (k) αj exp(ej ) =P , (k) N exp(e ) n n=1 (2) where va is the attention vector and Wa is the weight in the attention layer. The context representation of x(k) is then the weighted sum of h(k) : c(k) = N X (k) αj h(k) . (3) j=1 After obtaining c(pre) and c(pos) , we concatenate them with the target entity embedding x(r) , and pass it through a feed forward network to obtain the final representation: R = ReLU(Wf [c(pre) , x(r) , c(pos) ]),"
2021.inlg-1.17,W18-6506,1,0.891796,"Missing"
2021.inlg-1.17,W19-8643,0,0.0234309,"Missing"
2021.inlg-1.17,P18-2023,0,0.0124,"sic use. To do this, we fine-tune BERT on the CCD as a multi-class classification task, where there are 172 classes (i.e., 172 classifier words) in total, and make a prediction with the help of the [CLS] symbol (see Figure 1 (right)). We refer to this model as BERT. 2 Since our experiments suggested that the head flag (i.e., hhi and h/hi) makes no contribution to classifier selection, we drop it to speed up the prediction. 1 github.com/wuningxi/ ChineseClassifierDataset 173 2.3 Research Questions hidden size to 300, and the learning rate to 2e5. We use pre-trained Chinese word embeddings from Li et al. (2018)4 . At the start of our research, we formulated the following hypotheses and research questions. 1. Since BERT models context closely and is pretrained on large scale corpora, we expect it to outperform other models; 2. How do the two BERT-based models compare? Although we expect BERT to outperform MLM, we were curious to see how well MLM performs. 3. We are curious how well BERT can handle classifiers that add information (concretely, in this paper: measure words, plurality, and politeness). 3 Experiments 3.1 Setup Dataset. In total, there are 681,102 sentences in the CCD dataset. We split th"
2021.inlg-1.17,I17-3011,0,0.10993,"mple, in terms of politeness ((b), neutral vs. polite), number ((c), singular vs. plural), or quantity ((d), a cup vs. a can of coffee). This is perhaps clearest in the case of (d), where “杯” (b¯ei; cup) and “听” (t¯ıng; can) indicate different containers, and consequently different quantities, of coffee; these classifiers are known as measure words, as opposed to the “pure” classifiers of (a)-(c). Researchers have asked what determines the choice of classifier, constructing algorithms that predict what classifier suits a given discourse context. The most sophisticated model we are aware of is Peinelt et al. (2017). Ambitiously, these authors decided to deal with classifiers of all different types, also including measure words for instance, which are difficult to predict because they add information. They approached the problem as follows: Given a sentence in which a classifier is yet to be realised, and the head noun is flagged, predict the missing classifier. For example, in the input: 一个 电脑/ 一台 电脑 y´ı g`e di`annˇao / y´ı t´ai di`annˇao ‘a computer’ 一个 老师/ 一位 老师 y´ı g`e lˇaosh¯ı / y´ı w`ei lˇaosh¯ı (1) 一hCLi 精彩的hhi球赛h/hi y`ı hCLi j¯ıngcˇai de hhiqi´us`aih/hi ‘a wonderful ball game’ hCLi indicates wher"
C00-1033,P98-2173,1,0.825795,"and the ability to construct natural language feedback texts to help the author understand the content and the form of the document while it is still under construction. The concluding section explains what needs to be done to ll the gap between the implemented system and the ideal one. 1 An exception is alfresco which takes natural language input, requiring the system to interpret unconstrained natural language (Stock 1991). Avoiding the need for doing this is an important design motivation for wysiwym-based systems. 2 A WYSIWYM-based System for the Authoring of Textual Documents Elsewhere (Power and Scott 1998, Scott et al. 1998, Scott 1999), a new knowledge-editing method called `wysiwym editing&apos; has been introduced and motivated. Wysiwym editing allows a domain expert to edit a knowledge base (kb) by interacting with a feedback text, generated by the system, which presents both the knowledge already de ned and the options for extending and modifying it. Knowledge is added or modi ed by menu-based choices which directly a ect the knowledge base; the result is displayed to the author by means of an automatically generated feedback text: thus `What You See Is What You Meant&apos;. Wysiwym instantiates a"
C00-1033,W98-1427,1,0.840751,"nstruct natural language feedback texts to help the author understand the content and the form of the document while it is still under construction. The concluding section explains what needs to be done to ll the gap between the implemented system and the ideal one. 1 An exception is alfresco which takes natural language input, requiring the system to interpret unconstrained natural language (Stock 1991). Avoiding the need for doing this is an important design motivation for wysiwym-based systems. 2 A WYSIWYM-based System for the Authoring of Textual Documents Elsewhere (Power and Scott 1998, Scott et al. 1998, Scott 1999), a new knowledge-editing method called `wysiwym editing&apos; has been introduced and motivated. Wysiwym editing allows a domain expert to edit a knowledge base (kb) by interacting with a feedback text, generated by the system, which presents both the knowledge already de ned and the options for extending and modifying it. Knowledge is added or modi ed by menu-based choices which directly a ect the knowledge base; the result is displayed to the author by means of an automatically generated feedback text: thus `What You See Is What You Meant&apos;. Wysiwym instantiates a general recent tren"
C00-1033,C98-2168,1,\N,Missing
C08-1055,W00-1416,0,0.129061,"nd to disjunctive plural references. Disjunction is required whenever there is no conjunction of atomic properties that sets the elements of a set of referents apart from all the other objects in the domain. Recall example 1 (from §1), where the aim is to single out the black sheep and black goats from the rest of the animals. This task cannot be performed by a simple conjunction (i.e., of the form ‘the X’, where X contains adjectives and nouns only), so disjunctions become unavoidable. Various proposals have been made for allowing gre algorithms to produce referring expressions of this kind (Stone, 2000; van Deemter, 2002; Gardent, 2002; Horacek, 2004). Here we take as our starting point the approach of (Gatt, 2007) (henceforth Gatt’s Algorithm with Partitioning or gap). gap is the only algorithm that produces a dd in Disjunctive Normal Form (dnf) while also guaranteeing that every “part” of the partition contains a noun. The dnf takes the form: S1 ∪ S2 ... ∪ Sn , where each Si itself expresses a conjunction of atomic properties. (For example, S1 might be Sheep ∩ Black, while S2 is Goat ∩ Black.) We sketch two extensions of this approach: the first, purely formal extension ensures that a set"
C08-1055,P04-1052,0,0.272256,"identify its referent uniquely (Dale, 1992). Such a description is called a Distinguishing Description (dd). In practice, however, most gre algorithms build sets of semantic properties available in a Knowledge Base (kb), rather than descriptions in natural language; surface issues are often ignored (exceptions are: (Stone and ∗ This work is supported by a University of Aberdeen Sixth Century Studentship, and EPSRC grant EP/E011764/1. ∗ c 2008. Licensed under the Creative Commons ° Attribution-Noncommercial-Share Alike 3.0 Unported. Some rights reserved. Webber, 1998; Krahmer and Theune, 2002; Siddharthan and Copestake, 2004)). This is an important limitation, for example because ambiguities can be introduced in the step from properties to language descriptions. Such “surface ambiguities” take centerstage in this paper. More specifically, we shall be investigating situations where they lead to referential ambiguity, that is, unclarity as to what the intended referent of a referring expression is. Example 1: Consider a scenario in which there are sheep and goats along with other animals, grazing in a meadow; some of the sheep and goats are black while others are either brown or yellow. Suppose our task is to single"
C08-1055,W98-1419,0,0.293553,"Missing"
C08-1055,J02-1003,1,0.945722,"Missing"
C08-1055,P02-1013,0,0.122556,"ces. Disjunction is required whenever there is no conjunction of atomic properties that sets the elements of a set of referents apart from all the other objects in the domain. Recall example 1 (from §1), where the aim is to single out the black sheep and black goats from the rest of the animals. This task cannot be performed by a simple conjunction (i.e., of the form ‘the X’, where X contains adjectives and nouns only), so disjunctions become unavoidable. Various proposals have been made for allowing gre algorithms to produce referring expressions of this kind (Stone, 2000; van Deemter, 2002; Gardent, 2002; Horacek, 2004). Here we take as our starting point the approach of (Gatt, 2007) (henceforth Gatt’s Algorithm with Partitioning or gap). gap is the only algorithm that produces a dd in Disjunctive Normal Form (dnf) while also guaranteeing that every “part” of the partition contains a noun. The dnf takes the form: S1 ∪ S2 ... ∪ Sn , where each Si itself expresses a conjunction of atomic properties. (For example, S1 might be Sheep ∩ Black, while S2 is Goat ∩ Black.) We sketch two extensions of this approach: the first, purely formal extension ensures that a set of such logical formulae is gener"
C08-1055,Y98-1026,0,0.144216,"s likely as the intended one. The problem is, how to determine the likelihood of different interpretations. 3 Getting likelihood from the bnc In scopally ambiguous referring expressions, there is a tension between wide- and narrowscope interpretations. This can be viewed in terms of two competing forces: a Coordination Force, whereby Noun1 and Noun2 attract each other to form a syntactic unit, and a Modification Force, whereby Adj and Noun1 attract each other to form a syntactic unit. Computational linguists have proposed using language corpora to estimate the likelihood of an interpretation (Wu and Furugori, 1998; Chantree et al., 2006). Chantree et al. used information from the Sketch Engine database (Kilgarriff, 2003) operating on the bnc to resolve coordination ambiguity. The Sketch Engine contains grammatical triples in the form of Word Sketches for each word, with each triple accompanied by a salience value indicating the likelihood of occurrence of the word with its argument in a grammatical relation. Word Sketches summarise the words’ grammatical and collocational behavior. Chantree et al. gathered a dataset of ambiguous phrases from a corpus of requirements specifications, and collected human"
C90-3016,C82-1028,0,0.0304615,"Missing"
C90-3016,P83-1009,0,0.481671,"Missing"
D07-1011,P89-1009,0,0.287637,"description: (1) hORIENTATION : backi ∧ hSIZE : smalli This description is overspecified, because ORI ENTATION is not strictly necessary to distinguish the referents (hSIZE : smalli suffices). Moreover, the description does not include TYPE, though it has been argued that this is always required, as it maps to the head noun of an NP (Dale and Reiter, 1995). We will adopt this assumption here, for reasons explained below. Due to its hillclimbing nature, the IA avoids combinatorial search, unlike some predecessors which searched exhaustively for the briefest possible description of a referent (Dale, 1989), based on a strict interpretation of the Gricean Maxim of Quantity (Grice, 1975). Given that, under the view proposed by Olson (1970) among others, the function of a referential NP is to identify, a strict Gricean interpretation holds that it should contain no more information than necessary to achieve this goal. The Incremental Algorithm constitutes a departure from this view given that it can overspecify through its use of a PO. This has been justified on psycholinguistic grounds. Speakers overspecify their descriptions because they begin their formulation of a reference without exhaustivel"
D07-1011,P02-1013,0,0.71859,"em to the description. This procedure has three consequences: 1. Efficiency: Searching through disjunctive combinations results in a combinatorial explosion (van Deemter, 2002). 2. Gestalts and content: The notion of a ‘preferred attribute’ is obscured, since it is difficult to apply the same reasoning that motivated the PO in the IA to combinations like (COLOUR ∨ SIZE). 1 Note that logical disjunction is usually rendered as linguistic coordination using and. Thus, the table and the desk is the union of things which are desks or tables. 3. Form: Descriptions can become logically very complex (Gardent, 2002; Horacek, 2004). Proposals to deal with (3) include Gardent’s (2002) non-incremental, constraint-based algorithm to generate the briefest available description of a set, an approach extended in Gardent et al. (2004). An alternative, by Horacek (2004), combines bestfirst search with optimisation to reduce logical complexity. Neither approach benefits from empirical grounding, and both leave open the question of whether previous psycholinguistic research on singular reference is applicable to plurals. This paper reports a corpus-based analysis of plural descriptions elicited in well-defined dom"
D07-1011,W07-2307,1,0.857995,"Missing"
D07-1011,J02-1003,1,0.938162,"Missing"
D07-1011,W06-1410,0,0.05089,"xtended in Gardent et al. (2004). An alternative, by Horacek (2004), combines bestfirst search with optimisation to reduce logical complexity. Neither approach benefits from empirical grounding, and both leave open the question of whether previous psycholinguistic research on singular reference is applicable to plurals. This paper reports a corpus-based analysis of plural descriptions elicited in well-defined domains, of which Table 1 is an example. This study falls within a recent trend in which empirical issues in GRE have begun to be tackled (Gupta and Stent, 2005; Jordan and Walker, 2005; Viethen and Dale, 2006). We then propose an efficient algorithm for the generation of references to arbitrary sets, which combines corpusderived heuristics and a partitioning-based procedure, comparing this to IAbool . Unlike van Deemter (2002), we only focus on disjunction, leaving negation aside. Our starting point is the assumption that plurals, like singulars, evince preferences for certain attributes as predicted by the Conceptual Gestalts Principle. Based on previous work in Gestalt perception (Wertheimer, 1938; Rock, 1983), we propose an extension of this to sets, whereby plural descriptions are preferred if"
D07-1011,W06-1420,1,\N,Missing
J00-4005,T87-1035,0,0.0606888,"tion of referring has its problems. For example, the speaker m a y be mistaken in her belief that Mr. X is the tenant of the house (Donnellan 1966). In such cases it is unclear w h o is being referred to. Such problems notwithstanding, w o r k on coreference annotation has usually taken the notion of reference for granted, on the assumption that clear cases, where the referent of an N P is clearly defined, o u t n u m b e r the problematic ones, at least in some important types of discourse. Let us, for now, b u y into the assumption that reference is a straightforward notion. Then, following Bach (1987) (especially Sections 3.2 and 12.2), for example, one thing that is clear about reference is that some NPs do not refer. W h e n someone says (1) a. No solution emerged from our discussions, or b. Whenever a solution emerged, we embraced it, the solution NPs do not refer to any single solution, nor to any definite set of solutions. Most theorists w o u l d agree that they do not have a referent. Nonreferring NPs can enter anaphoric relations. (For example, the N P a solution is the (bound) anaphoric antecedent to it in (lb).) But if they do not refer, the c0reference relation as defined in Sec"
J00-4005,W99-0106,0,0.0390721,"Missing"
J00-4005,kibble-van-deemter-2000-coreference,1,0.850509,"Missing"
J00-4005,poesio-2000-annotating,0,0.0397047,"W. I. Clinton) and a description (Hillary Rodham's husband) can corefer without either of the two depending on the other for its interpretation. Anaphoric and coreferential relations can coincide, of course, but not all coreferential relations are anaphoric, nor are all anaphoric relations coreferential. (An example of the latter is bound anaphora, see Section 2.1.) Coreference annotation has been a focus of the Sixth and Seventh Message Understanding Conferences (MUC-6, MUC-7) and various other annotation exercises (e.g., Passoneau 1997; Garside, Leech, and McEnery 1997; Davies et al. 1998; Poesio 2000). In this squib, we intend to point at some fundamental problems with many of these annotation exercises, which are caused by a failure to distinguish properly between coreference, anaphora, and other, related phenomena. Because the MUC project is the bestknown example of coreference annotation, on which much subsequent work is based, and because of the public availability of the MUC Task Definition (TD, Hirschman and Chinchor [1997]), we will focus on MUC. Four criteria are listed for the MUC TD, in order of their priority (Hirschman and Chinchor 1997): . The MUC information extraction tasks"
J00-4005,W99-0213,1,0.446325,"Missing"
J00-4005,W00-1705,0,\N,Missing
J02-1003,P99-1017,0,0.450364,"ecise way in which complexity is most relevant (e.g., “typical” or worst-case complexity, cf. footnote 3). Far from aiming to speak the last word on these issues, the material discussed here does shed some light on them. For example, even a fast algorithm can require a large number of calculations, in which case a solution may never be found; in the case of GRE, this happens when the set of distractors or the set of properties becomes extremely large (Section 3.2). Conversely, a complex algorithm can be safe to use if the domain is small (or if key calculations can be performed offline; e.g., Bateman 1999). This may be achieved by putting a bound on the size of the search space, and this may be justifiable on empirical grounds (see the discussion of D&RBoolean in Section 4.3). One might, on the other hand, argue that bounding does not eliminate the disadvantages of an otherwise intractable algorithm, because the true nature of an algorithm is best revealed “by considering how it operates on unlimited cases” (Barton, Berwick, and Ristad 1987, Section 1.4.1). Be this as it may, we believe that complexity theory can offer valuable insights into the structure of GRE algorithms and that the growing"
J02-1003,P89-1009,0,0.327708,"might be made for the selection of head nouns because, arguably, this has to involve realizational issues.1 1 Compare Dale and Reiter (1995), where head nouns are taken into account during content determination. Head nouns can also be selected during linguistic realization or by interleaving of content determination and realization (e.g., Horacek 1997; Stone and Webber 1998; Krahmer and Theune 1999). 38 van Deemter Generating Referring Expressions The Incremental Algorithm produces a set L of properties P1 , . . . , Pn such that their logical conjunction forms a “distinguishing description” (Dale 1989) of the target object r. In other words, writing [[Q]] for the extension of Q (i.e., the set of objects that have the property Q), the intersection [[P1 ]] ∩ · · · ∩ [[Pn ]] must equal the singleton set {r}. It is a “hillclimbing” algorithm, which finds better and better approximations of the target set {r} by accumulating more and more properties—hence the term Incremental. There is no backtracking. Consequently, if some property Pi in L is made redundant by later additions (i.e., when ([[P1 ]] ∩ · · · ∩ [[Pi − 1]] ∩ [[Pi + 1]] ∩ · · · ∩ [[Pn ]]) ⊆ [[Pi ]]), then Pi is retained as a member of"
J02-1003,E91-1028,0,0.961473,"ese generalizations, we will identify and confront a number of cases in which current GRE algorithms are incomplete even with respect to merely intersective descriptions. In this paper, we will deal with “first mention” descriptions only (unlike Dale 1992, Chapter 5; Mittal et al. 1998; Kibble 1999), assuming that the information used for generating the description is limited to a KB containing complete information about which properties are true of each object. Also, we focus on “one shot” descriptions, disregarding cases where an object is described through its relations with other objects (Dale and Haddock 1991; Horacek 1997; Krahmer, van Erk, and Verleg 2001). More crucially, we follow Dale and Reiter (1995) in focusing on the semantic content of a description (i.e., the problem of content determination, for short), assuming that any combination of properties can be expressed by the NLG module responsible for linguistic realization. This modular approach allows us to separate logical aspects of generation (which are largely language independent) from purely linguistic aspects, and it allows the realization module to base its decisions on complete information about which combination of properties is"
J02-1003,P97-1027,0,0.881717,"will identify and confront a number of cases in which current GRE algorithms are incomplete even with respect to merely intersective descriptions. In this paper, we will deal with “first mention” descriptions only (unlike Dale 1992, Chapter 5; Mittal et al. 1998; Kibble 1999), assuming that the information used for generating the description is limited to a KB containing complete information about which properties are true of each object. Also, we focus on “one shot” descriptions, disregarding cases where an object is described through its relations with other objects (Dale and Haddock 1991; Horacek 1997; Krahmer, van Erk, and Verleg 2001). More crucially, we follow Dale and Reiter (1995) in focusing on the semantic content of a description (i.e., the problem of content determination, for short), assuming that any combination of properties can be expressed by the NLG module responsible for linguistic realization. This modular approach allows us to separate logical aspects of generation (which are largely language independent) from purely linguistic aspects, and it allows the realization module to base its decisions on complete information about which combination of properties is to be realize"
J02-1003,W99-0109,0,0.0285331,"of Brighton, Lewes Road, Brighton BN2 4GJ, UK. E-mail: Kees.van.Deemter@itri.brighton.ac.uk. c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 algorithms are also generalized to generate references to sets, rather than individual objects. But, before we arrive at these generalizations, we will identify and confront a number of cases in which current GRE algorithms are incomplete even with respect to merely intersective descriptions. In this paper, we will deal with “first mention” descriptions only (unlike Dale 1992, Chapter 5; Mittal et al. 1998; Kibble 1999), assuming that the information used for generating the description is limited to a KB containing complete information about which properties are true of each object. Also, we focus on “one shot” descriptions, disregarding cases where an object is described through its relations with other objects (Dale and Haddock 1991; Horacek 1997; Krahmer, van Erk, and Verleg 2001). More crucially, we follow Dale and Reiter (1995) in focusing on the semantic content of a description (i.e., the problem of content determination, for short), assuming that any combination of properties can be expressed by the"
J02-1003,1993.eamt-1.1,0,0.0786216,"mited backtracking provided, of course, properties are properly aggregated (e.g., Dalianis and Hovy 1996). 3.2 Assumptions Concerning Infinite Sets To prove intersective completeness, certain assumptions concerning the cardinality of sets need to be made. To give an extreme example, suppose one wanted to refer to a real number that does not have a “proper name” (unlike, e.g., π); then the class of potentially useful properties is so vast that no GRE algorithm can take them all into consideration. As long as the number of properties (i.e., Attribute/Value combinations) is denumerably infinite (Kleene 1971), only termination becomes problematic: if a uniquely referring description [[P1 ]] ∩ · · · ∩ [[Pn ]] exists, then the algorithm will find one in finite time, since each of the n properties in the description will be found in finite time; if no distinguishing description exists, however, the algorithm never terminates. In the less likely case where the set of properties is nondenumerably infinite (i.e., it does not stand in a 1-1 relation to any set of natural numbers), completeness becomes problematic as well, since it is impossible for the algorithm to consider all properties; hence, success"
J02-1003,W01-0805,0,0.414865,"Missing"
J02-1003,J98-3004,0,0.0155515,"e (ITRI), University of Brighton, Lewes Road, Brighton BN2 4GJ, UK. E-mail: Kees.van.Deemter@itri.brighton.ac.uk. c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 algorithms are also generalized to generate references to sets, rather than individual objects. But, before we arrive at these generalizations, we will identify and confront a number of cases in which current GRE algorithms are incomplete even with respect to merely intersective descriptions. In this paper, we will deal with “first mention” descriptions only (unlike Dale 1992, Chapter 5; Mittal et al. 1998; Kibble 1999), assuming that the information used for generating the description is limited to a KB containing complete information about which properties are true of each object. Also, we focus on “one shot” descriptions, disregarding cases where an object is described through its relations with other objects (Dale and Haddock 1991; Horacek 1997; Krahmer, van Erk, and Verleg 2001). More crucially, we follow Dale and Reiter (1995) in focusing on the semantic content of a description (i.e., the problem of content determination, for short), assuming that any combination of properties can be exp"
J02-1003,W00-1416,0,0.265754,"Pi ]] then do L := L ∪ {Pi } C := C ∩ [[Pi ]] If C = S then Return L {Success} Return Failure {All properties in P have been tested, yet C 6= S} Note that S takes the place of the target object r in the earlier algorithms; the process of expanding L and contracting C continues until C = S. Because this is basically the same algorithm as D&R, it has the same computational complexity of O(na ), where na is the cardinality of P. D&RPlural characterizes a set by scrutinizing its elements. This does not work for properties like BEING OF THE SAME AGE, which crucially pertain to sets of objects (cf. Stone 2000). The algorithm can, however, be generalized to cover such cases if we initialize C not to D but to the powerset of D, after which the algorithm selects properties of sets, removing from P(D) all those sets for which the property is false. For example, selection of BEING OF THE SAME AGE removes all those sets whose elements are not of the same age as each other, selection of FORMING A FOOTBALL TEAM removes all sets that do not make up a football team, and so on. As a result, the algorithm generates descriptions of sets of collective entities (i.e., sets of sets). In this way, descriptions such"
J02-1003,W98-1419,0,0.468605,"n algorithm that only approximates Full Brevity, while being of only linear complexity. Our summary of the algorithm glosses over many details, yet still allows us to discuss completeness. In particular, we disregard any special provisions that might be made for the selection of head nouns because, arguably, this has to involve realizational issues.1 1 Compare Dale and Reiter (1995), where head nouns are taken into account during content determination. Head nouns can also be selected during linguistic realization or by interleaving of content determination and realization (e.g., Horacek 1997; Stone and Webber 1998; Krahmer and Theune 1999). 38 van Deemter Generating Referring Expressions The Incremental Algorithm produces a set L of properties P1 , . . . , Pn such that their logical conjunction forms a “distinguishing description” (Dale 1989) of the target object r. In other words, writing [[Q]] for the extension of Q (i.e., the set of objects that have the property Q), the intersection [[P1 ]] ∩ · · · ∩ [[Pn ]] must equal the singleton set {r}. It is a “hillclimbing” algorithm, which finds better and better approximations of the target set {r} by accumulating more and more properties—hence the term In"
J02-1003,W00-1424,1,0.772053,"Missing"
J02-1003,W01-0804,1,0.821053,"osed extensions of the Incremental Algorithm would raise further questions, stemming from the fact that our descriptions are structurally complex. For example, consider the treatment of relational properties. Which is better: adding a relational property to a given incomplete description (. . . in the wooden shed) or adding a negated property (. . . which is not a poodle)? Making informed decisions about such questions, with proper attention to their combined effects, is a difficult task that is perhaps best tackled using the graph-theoretical approach outlined by Krahmer, van Erk, and Verleg (2001). Their approach is specifically suitable for accommodating different GRE algorithms and treats relations in the same way as properties Brevity. We have assumed that, on the whole, descriptions ought to be as brief as they can, as long as they are uniquely identifying. But in fact, a description can contain much more than is logically necessary for identification, even beyond the redundancies allowed by the Incremental Algorithm. Logically superfluous properties can, for example, be motivated by “overloading” if they serve communicative purposes other than identification (Pollack 1991; Dale an"
J02-1003,J03-1003,0,\N,Missing
J02-1003,P90-1013,0,\N,Missing
J02-1003,J06-2002,1,\N,Missing
J05-1002,W98-1425,0,0.0589033,"particular, many systems will contain more intermediate representations. Template-based and standard NLG systems are said to be ‘‘Turing equivalent’’ (Reiter and Dale 1997); that is, each of them can generate all recursively enumerable languages. However, template-based systems have been claimed to be inferior with respect to maintainability, output quality and variation, and well-foundedness. Reiter and Dale (1997) state that template-based systems are more difficult to maintain and update (page 61) and that they produce poorer and less varied output (pages 60, 84) than standard NLG systems. Busemann and Horacek (1998) go even further by suggesting that template-based systems do not embody generic linguistic insights (page 238). Consistent with this view, template-based systems are sometimes overlooked. In fact, the only current textbook on NLG (Reiter and Dale 2000) does not pay any attention to template-based generation, except for a passing mention of the ECRAN system (Geldof and van de Velde 1997). Another example is a recent overview of NLG systems in the RAGS project (Cahill et al. 1999). The selection criteria employed by the authors were that the systems had to be fully implemented, complete (i.e.,"
J05-1002,A94-1047,0,0.0437963,"Missing"
J05-1002,P98-1116,0,0.0120019,"lly, template-based systems do not have to use shortcuts any more than standard NLG systems: Where linguistic rules are available, both types of systems can use them, as we have seen. Another response to the absence of linguistic rules is the use of statistical information derived from corpora, as is increasingly more common in realization, but also for instance in aggregation (e.g., Walker, Rambow, and Rogati 2002). The point we want to make here, however, is that ‘‘template-based’’ systems may profit from such corpus-based approaches just as much as ‘‘standard’’ NLG systems. The approach of Langkilde and Knight (1998), for example, in which corpus-derived n-grams are used for selecting the best ones from among a set of candidates produced by overgeneration, can also be applied to template-based systems (witness the mixed template/ stochastic system of Galley, Fosler-Lussier, and Potamianos [2001]). We have argued that systems that call themselves template based can, in principle, perform all NLG tasks in a linguistically well-founded way and that more and more actually implemented systems of this kind deviate dramatically from the stereotypical systems that are often associated with the term template. Conv"
J05-1002,E03-1019,0,0.0197257,"to be underspecified for number and person, while using attribute grammar rules to complete the specification: Returning to the example above, the number attribute of John and Mary is inferred to be plural (unlike, e.g., John and I); a subject-verb agreement rule makes the further inference that the verb must be realized as walk, rather than walks. 5. Templates: An Updated View A new generation of systems that call themselves template-based have blurred the line between template-based and standard NLG. This is not only because some systems combine standard NLG with templates and canned text (Piwek 2003), but also because modern template-based systems tend to use syntactically structured templates and allow the gaps in them to be filled recursively (i.e., by filling a gap, a new gap may result). Some ‘‘template-based’’ systems, finally, use grammars to aid linguistic realization. These developments call into question the very definition of ‘‘template based’’ (section 2), since the systems that call themselves template-based have come to express their nonlinguistic input with varying degrees of directness. ‘‘Template-based’’ systems vary in terms of linguistic coverage, the amount of syntactic"
J05-1002,W02-1713,0,0.130969,"types of systems have more in common than is generally thought and that it is counterproductive to treat them as distant cousins instead of close siblings. In fact, we argue that there is no crisp distinction between the two. 16 van Deemter, Krahmer, and Theune Real versus Template-Based NLG 3. Template-Based NLG Systems in Practice In recent years, a number of new template-based systems have seen the light, including TG/2 (Busemann and Horacek 1998), D2S (van Deemter and Odijk 1997; Theune et al. 2001), EXEMPLARS (White and Caldwell 1998), YAG (McRoy, Channarukul, and Ali 2003), and XTRAGEN (Stenzhorn 2002). Each of these systems represents a substantial research effort, achieving generative capabilities beyond what is usually expected from template-based systems, yet they call themselves template-based, and they clearly fall within the characterization of template-based systems offered above. In this article we draw on our own experiences with a data-to-speech method called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S consists of two modules: (1) a language generation module (LGM)"
J05-1002,W98-1428,0,0.0272105,"stems investigated were template-based. In what follows, we claim that the two types of systems have more in common than is generally thought and that it is counterproductive to treat them as distant cousins instead of close siblings. In fact, we argue that there is no crisp distinction between the two. 16 van Deemter, Krahmer, and Theune Real versus Template-Based NLG 3. Template-Based NLG Systems in Practice In recent years, a number of new template-based systems have seen the light, including TG/2 (Busemann and Horacek 1998), D2S (van Deemter and Odijk 1997; Theune et al. 2001), EXEMPLARS (White and Caldwell 1998), YAG (McRoy, Channarukul, and Ali 2003), and XTRAGEN (Stenzhorn 2002). Each of these systems represents a substantial research effort, achieving generative capabilities beyond what is usually expected from template-based systems, yet they call themselves template-based, and they clearly fall within the characterization of template-based systems offered above. In this article we draw on our own experiences with a data-to-speech method called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch"
J05-1002,C98-1112,0,\N,Missing
J05-1002,E03-1062,0,\N,Missing
J05-1002,W02-2211,0,\N,Missing
J06-2002,P89-1009,0,0.853343,"one off-line. One should bear in mind that worst-case theoretical complexity is not always a good measure of the time that a program takes in the kinds of cases that occur most commonly, let alone the difficulty for a person. For example, it seems likely that hearers and speakers will have most difficulty dealing with differences that are too small to be obvious (e.g., two mice that are very similar in size). 207 Computational Linguistics Volume 32, Number 2 5. Pragmatic Constraints NLG has to do more than select a distinguishing description (i.e., one that unambiguously denotes its referent; Dale 1989): The selected expression should also be felicitous. Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between “observationally indifferent” entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects th"
J06-2002,E91-1028,0,0.0625413,"t word this as the two heaviest brown ones among the smallest four mice. To avoid such awkward expressions, one can change the order of properties after CD (mirroring step 4 above), moving the inequalities to the end of the list before they are transformed into the appropriate superlatives. The effect would be to boost the number of occurrences of gradable properties in generated descriptions while keeping CD incremental. 9. Extensions of the Approach 9.1 Relational Descriptions Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. Note that this is felicitous even if the shed is not the largest one i"
J06-2002,C04-1181,0,0.0184287,". To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000; DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable size measurement, for examp"
J06-2002,W97-1413,0,0.0608909,"Missing"
J06-2002,W03-0603,0,0.0248027,"Missing"
J06-2002,W03-2307,0,0.0387074,"Missing"
J06-2002,P00-1012,0,0.0110637,"expressions generation from its language-dependent, linguistic aspect. Our algorithm suggests a distinction into three phases, the first two of which can be thought of as part of CD: proper, that is, the production of a distinguishing list of properties L; 1. CD 2. An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives’ relative position. Interestingly, vague properties tend to be realized before others. Quirk et al. (1985), for example, report that “adjectives denoting size, length, and height normally precede other nonderived adjectives” (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for example, the words"
J06-2002,W02-2113,0,0.0265244,"element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible). 1.2 Vagueness in NLG Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). F OG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by Reiter and Sripada (2002), where users can specify boundary values for attributes like rainfall, specifying, for example, rain counts as moderate above 7 mm/h, as heavy above 20 mm/h, and so on. A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart’s piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number"
J06-2002,P99-1018,0,0.00976485,"ent, logical aspect of referring expressions generation from its language-dependent, linguistic aspect. Our algorithm suggests a distinction into three phases, the first two of which can be thought of as part of CD: proper, that is, the production of a distinguishing list of properties L; 1. CD 2. An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP (e.g., Shaw and Hatzivassiloglou 1999; Malouf 2000). Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives’ relative position. Interestingly, vague properties tend to be realized before others. Quirk et al. (1985), for example, report that “adjectives denoting size, length, and height normally precede other nonderived adjectives” (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small(-est) [N], for examp"
J06-2002,W00-1416,0,0.0274138,"Missing"
J06-2002,W98-1419,0,0.206465,"Missing"
J06-2002,W00-1424,1,0.934847,"Missing"
J06-2002,J02-1003,1,0.815976,"Missing"
J07-2004,P89-1009,0,0.0918163,"rtant aspect of GRE is to find combinations of properties that allow the generator to refer uniquely to an entity, called the target. Crucially, GRE algorithms only use properties whose denotations are part of the common knowledge of writer and reader.2 These algorithms are typically designed in such a way that generation is performed quickly (e.g., their worst-case running time tends to be linear [Dale and Reiter 1995; van Deemter 2002]) but the processing effort of the reader is not taken into account. Some algorithms do make a point of generating descriptions that are as brief as possible (Dale 1989), and this can be argued to make interpretation easier. As we have seen, however, in relation to Examples (1a–c), brevity can make resolution difficult. For concreteness, let us focus on one of the best-known algorithms in this area. The Incremental Algorithm (Dale and Reiter 1995) starts by arranging attributes in a list, after which they are considered one by one, to see if any of their values contributes something to the description, by removing “distractors” (i.e., objects other than the referent); if an attribute (e.g., COLOR) can contribute something, then a suitable value (e.g., RED) fo"
J07-2004,E91-1028,0,0.0720446,"Missing"
J07-2004,C94-2182,0,0.128001,"Missing"
J07-2004,W05-1606,0,0.0396477,"ple of a description failing this requirement occurs in Get off one stop before I do, in an exchange between two people who have just met, as a description of where the hearer should get off the bus (Appelt 1985, cited in Dale and Reiter 1995). 231 Computational Linguistics Volume 33, Number 2 Suppose a referring expression identifies its referent uniquely. Then at least two things can stand in the way of finding its referent: the “difficulty” of the individual properties used in the description (i.e., the fact that it may be difficult to ascertain which objects have the property in question [Horacek 2005]), or the size and structure of the search space. To exemplify the first factor, suppose you are queuing up for a concert and want to explain to a friend that a girl further ahead in the queue has his ticket. Color is an attribute that speakers like to use, even if it leads to logical redundancy (Pechmann 1989). This might be done by describing the referent as the girl in a yellow dress, or as the girl with green eyes, for example. But arguably, the first property contributes more towards your friend’s search, because the color of a person’s eyes may not leap out at him from afar. In the Incr"
J07-2004,P00-1019,0,0.0269298,"r being interested in the referent) or to highlight the 1 Dale and Reiter (1995, Section 5) also mention the use of “navigational” (or “attention-directing”) information in referring expressions, which they distinguish from “discrimination information,” and whose function appears to be to move the attention of the reader/hearer towards an object. The concept is not defined precisely and it is not clear how navigational information should be used in GRE. 230 Paraboni, van Deemter, and Masthoff Making Referents Easy to Identify speaker’s awareness that the referent has the property in question (Jordan 2000, 2002). Implementations of such findings in NLG are not difficult to envisage. The present article takes this reader-oriented perspective on the redundancy of referring expressions a step further, by asking how a generator can use logically redundant information to reduce the search space within which a reader has to “find” a referent; this will be specifically useful when referents need to be found in situations where the extensions of some of the properties are not known to the reader/hearer in advance (cf., Edmonds [1994] for a related set of problems) and where some effort may be needed t"
J07-2004,W06-1409,1,0.695898,"Missing"
J07-2004,W02-2115,1,0.565138,"Missing"
J07-2004,P04-1052,0,0.0951487,"ni 2000, 2003; Paraboni and van Deemter 2002a, 2002b) but their relevance extends to many other situations. Our findings will also shed light on the egocentricity debate among psycholinguists about the extent to which speakers take hearer’s knowledge into account when they speak (Keysar, Lin, and Barr 2003). Throughout the article, we shall focus on issues of Content Determination (as opposed to, for example Lexical Choice), and on the situations in which individuals are first mentioned (as opposed to ones in which linguistic context allows them to be shortened [e.g., Krahmer and Theune 2002; Siddharthan and Copestake 2004]). 2. Ease of Resolution in the Incremental Algorithm Generation of referring expressions (GRE) is a key task of NLG systems (e.g., Reiter and Dale 2000, Section 5.4). An important aspect of GRE is to find combinations of properties that allow the generator to refer uniquely to an entity, called the target. Crucially, GRE algorithms only use properties whose denotations are part of the common knowledge of writer and reader.2 These algorithms are typically designed in such a way that generation is performed quickly (e.g., their worst-case running time tends to be linear [Dale and Reiter 1995;"
J07-2004,J02-1003,1,0.679526,"Missing"
J07-2004,P00-1012,0,\N,Missing
J12-1006,W08-1107,0,0.322048,"Missing"
J12-1006,W09-0612,0,0.0272606,"Missing"
J12-1006,P02-1012,0,0.321477,"and even harder to choose between the different types of demonstratives (Piwek 2008). Concerning pronouns, Krahmer and Theune suggested that “he” abbreviates “the (most salient) 187 Computational Linguistics Volume 38, Number 1 man,” and “she” “the (most salient) woman.” In this way, algorithms for generating distinguishing descriptions might also become algorithms for pronoun generation. Such an approach to pronoun generation is too simple, however, because additional factors are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004). Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example, emphasized the role of topics and discourse structure for pronoun generation, and pointed out that the changes in time scale are a reliable cue for this. In particular, they found that in certain places a deﬁnite description was used where a pronoun would have been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as can be signaled, for example, by a change in tense or a cue"
J12-1006,P02-1013,0,0.668288,"xample, that Step 2 selects the properties P ∪ S and P ∪ R, ruling out all distractors. L now takes the form (P ∪ S) ∩ (P ∪ R) (e.g., “the things that are both (women or men) and (women or wearing suits)”). The second phase uses logic optimization techniques, originally designed for the minimization of digital circuits (McCluskey 1965), to simplify this to P ∪ (S ∩ R) (“the women, and the men wearing suits”). Figure 3 Outline of the ﬁrst stage of van Deemter’s (2002) Boolean REG algorithm. 182 Krahmer and van Deemter Computational Generation of Referring Expressions Variations and Extensions. Gardent (2002) drew attention to situations where this proposal produces unacceptably lengthy descriptions; suppose, for example, the algorithm accumulates numerous properties during Steps 1 and 2, before ﬁnding one complex property (a union of three properties) during Step 3 which, on its own would have sufﬁced to identify the referent. This will make the description generated much lengthier than necessary, because the properties from Steps 1 and 2 are now superﬂuous. Gardent’s take on this problem amounts to a reinstatement of Full Brevity embedded in a reformulation of REG as a constraint satisfaction pr"
J12-1006,W11-2815,0,0.0233073,"thms. But, of course, human-likeness is not the only yardstick that can be used. In NLG systems whose main aim is to be practically useful, for example, it may be more important for referring expressions to be clear than to be human-like in all respects. The difference is important because psycholinguists have shown that human speakers have only limited capabilities for taking the addressee into account, frequently producing expressions that cannot be interpreted correctly by an addressee—for example, when they are under time pressure (Horton and Keysar 1996). If usefulness or successfulness (Garouﬁ and Koller 2011), rather than human-likeness, is the yardstick for success then a different type of evaluation test needs to be used. Possible tests include, for example, speed and accuracy of task completion (i.e., how often and how fast do readers ﬁnd the referent?). A variety of hearer-oriented tests is starting to be used in recent REG research (Paraboni, van Deemter, and Masthoff 2007; Khan, van Deemter, and Ritchie 2008), but evaluation of REG algorithms (and of NLG in general) remains difﬁcult (see, e.g., Oberlander 1998; Belz 2009; and Gatt and Belz 2010). Arguably, a central problem is that many diff"
J12-1006,W08-1131,0,0.324755,"Missing"
J12-1006,W09-0629,0,0.211729,"Missing"
J12-1006,W07-2307,1,0.934054,"Missing"
J12-1006,W10-4207,0,0.0217055,"ally all of them, regardless of their purpose, contain an REG module of some sort (Mellish et al. 2006). This is hardly surprising in view of the central role that reference plays in communication. A system providing advice about air travel (White, Clark, and Moore 2010) needs to refer to ﬂights (“the cheapest ﬂight,” “the KLM direct ﬂight”), a pollen forecast system (Turner et al. 2008) needs to generate spatial descriptions for areas with low or high pollen levels (“the central belt and further North”), and a robot dialogue system that assembles construction toys together with a human user (Giuliani et al. 2010) needs to refer to the components (“insert the green bolt through the end of this red cube”). REG “is concerned with how we produce a description of an entity that enables the hearer to identify that entity in a given context” (Reiter and Dale 2000, page 55). Because this can often be done in many different ways, a REG algorithm needs to make a number of choices. According to Reiter and Dale (2000), the ﬁrst choice concerns what form of referring expression is to be used; should the target be referred to, for instance, using its proper name, a pronoun (“he”), or a description (“the man with th"
J12-1006,P10-2011,1,0.850491,"owing feedback from the addressee. Others have argued that conversation partners automatically “align” with each other during interaction (Pickering and Garrod 2004). For instance, Branigan et al. (2010) report on a study showing that if a computer uses the word “seat” instead of the more common “bench” in a referring expression, the user is subsequently more likely to use “seat” instead of “bench” as well. This kind of lexical alignment takes place at the level of linguistic realization, and there is at least one NLG realizer that can mimic this process (Buschmeier, Bergmann, and Kopp 2009). Goudbeek and Krahmer (2010) found that speakers in an interactive setting also align at the level of content selection; they present experimental data showing that human speakers may opt for a “dispreferred” attribute (even when a preferred attribute would be distinguishing) when these were salient in a preceding interaction. The reader may want to consult Arnold (2008) for an overview of studies on reference choice in context, Clark and Bangerter (2004) for a discussion of studies on collaborative references, or Krahmer (2010) for a confrontation of some recent psycholinguistic ﬁndings with REG algorithms. Psycholingui"
J12-1006,J95-2003,0,0.583506,"Missing"
J12-1006,J86-3001,0,0.285574,"r and van Deemter Computational Generation of Referring Expressions think of salience—just like height or age—as coming in degrees. Existing theories of linguistic salience do not merely separate what is salient from what is not. They assign referents to different salience bands, based on factors such as recency of mention and syntactic structure (Gundel, Hedberg, and Zacharski 1993; Haji˘cov´a 1993; Grosz, Joshi, and Weinstein 1995). Salience and Context-Sensitive REG. Early REG algorithms (Kronfeld 1990; Dale and Reiter 1995) assumed that salience could be modeled by means of a focus stack (Grosz and Sidner 1986): A referring expression is taken to refer to the highest element on the stack that matches its description (see also DeVault, Rich, and Sidner 2004). Krahmer and Theune (2002) argue that the focus stack approach is not ﬂexible enough for context-sensitive generation of descriptions. They propose to assign individual salience weights (sws) to the objects in the domain, and to reinterpret referring expressions like “the man” as referring to the currently most salient man. Once such a gradable notion of salience is adopted, we are back in the territory of Section 3.3. One simple way to generate"
J12-1006,J95-3003,0,0.637708,"contains the seeds of much later work in REG, given its skepticism about the naturalness of minimal descriptions, its use of Rosch (1978)–style basic categories, and its acknowledgment of the role of computational complexity. Broadly speaking, it suggests an incremental generation strategy, compatible with the ones described subsequently, although it is uncertain what exactly was implemented. In recent years, the Appelt–Kronfeld line of research has largely given way to a new research tradition which focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Da"
J12-1006,W08-1129,0,0.0958485,"Missing"
J12-1006,P97-1027,0,0.0460623,"t Determination, in the style of a pipeline, with most of the actual research focusing predominantly on Content Determination. One might have thought that good results are easy to achieve by sending the output of the Content Determination module to 188 Krahmer and van Deemter Computational Generation of Referring Expressions a generic realizer (i.e., a program converting meaning representations into natural language). With hindsight, any such expectations must probably count as naive. Some REG studies have taken a different approach, interleaving Content Determination and Surface Realization (Horacek 1997; Stone and Webber 1998; Krahmer and Theune 2002; Siddharthan and Copestake 2004), running counter to the pipeline architecture (Mellish et al. 2006). In this type of approach, syntactic structures are built up in tandem with semantic descriptions: when type, man has been added to the semantic description, a partial syntactic tree is constructed for a noun phrase, whose head noun is man. As more properties are added to the semantic description, appropriate modiﬁers are slotted into the syntax tree; ﬁnally, the noun phrase is completed by choosing an appropriate determiner. Even in these inte"
J12-1006,W05-1606,0,0.0754419,"he greatest height of all men in this KB, the set of properties can be converted into {man, height = maximum}, where the exact height has been pruned away. The new description can be realized as “the tallest man” or simply as “the tall man” (provided the referent’s height exceeds a certain minimum value). The algorithm becomes more complicated when sets are referred to (because the elements of the target set may not all have the same heights), or when two or more gradable properties are combined (as in “the strong, tall man in the expensive car”) (van Deemter 2006). Variations and Extensions. Horacek (2005) integrates vagueness with other types of uncertainty. Horacek could be said to depict an REG algorithm as essentially a gambler who wants to maximize the chance of the referent being identiﬁed on the basis of 185 Computational Linguistics Volume 38, Number 1 the generated expression. Other things being equal, for example, it may be safer to identify a dog as being “owned by John,” than as being “tall,” because the latter involves borderline cases. A similar approach can be applied to perceptual uncertainty (as when it is uncertain whether the hearer will be able to observe a certain property)"
J12-1006,C08-1055,1,0.929481,"Missing"
J12-1006,J04-4001,0,0.0233588,"between the different types of demonstratives (Piwek 2008). Concerning pronouns, Krahmer and Theune suggested that “he” abbreviates “the (most salient) 187 Computational Linguistics Volume 38, Number 1 man,” and “she” “the (most salient) woman.” In this way, algorithms for generating distinguishing descriptions might also become algorithms for pronoun generation. Such an approach to pronoun generation is too simple, however, because additional factors are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004). Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example, emphasized the role of topics and discourse structure for pronoun generation, and pointed out that the changes in time scale are a reliable cue for this. In particular, they found that in certain places a deﬁnite description was used where a pronoun would have been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as can be signaled, for example, by a change in tense or a cue-phrase such as “several"
J12-1006,P07-1043,0,0.0461562,"minimal descriptions, its use of Rosch (1978)–style basic categories, and its acknowledgment of the role of computational complexity. Broadly speaking, it suggests an incremental generation strategy, compatible with the ones described subsequently, although it is uncertain what exactly was implemented. In recent years, the Appelt–Kronfeld line of research has largely given way to a new research tradition which focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concerned with the link between the Gricean maxims and the generation of r"
J12-1006,J10-2007,1,0.854011,"k from the addressee. Others have argued that conversation partners automatically “align” with each other during interaction (Pickering and Garrod 2004). For instance, Branigan et al. (2010) report on a study showing that if a computer uses the word “seat” instead of the more common “bench” in a referring expression, the user is subsequently more likely to use “seat” instead of “bench” as well. This kind of lexical alignment takes place at the level of linguistic realization, and there is at least one NLG realizer that can mimic this process (Buschmeier, Bergmann, and Kopp 2009). Goudbeek and Krahmer (2010) found that speakers in an interactive setting also align at the level of content selection; they present experimental data showing that human speakers may opt for a “dispreferred” attribute (even when a preferred attribute would be distinguishing) when these were salient in a preceding interaction. The reader may want to consult Arnold (2008) for an overview of studies on reference choice in context, Clark and Bangerter (2004) for a discussion of studies on collaborative references, or Krahmer (2010) for a confrontation of some recent psycholinguistic ﬁndings with REG algorithms. Psycholingui"
J12-1006,W08-1138,1,0.918692,"Alternatively, one could assign costs in accordance with the list of preferred attributes in the IA, making more preferred properties cheaper than less preferred ones. A third possibility is to compute the costs of an edge e in terms of the probability P(e) that e occurs in a distinguishing description (which can be estimated by counting occurrences in a corpus), making frequent properties cheap and rare ones expensive: cost(e) = −log2 (P(e)) Experiments with stochastic cost functions have shown that these enable the graphbased algorithm to capture a lot of the ﬂexibility of human references (Krahmer et al. 2008; Viethen et al. 2008). In the graph-based perspective, relations are treated in the same way as individual properties, and there is no risk of running into inﬁnite loops (“the cup to the left of the saucer to the right of the cup . . . ”). Unlike Dale and Haddock (1991) and Kelleher and Kruijff (2006), no special measures are required, because a relational edge is either included in a referring graph or not: including it twice is not possible. Van Deemter and Krahmer (2007) show that many of the proposals discussed in Section 3 can be recast in terms of graphs. They argue, however, that the g"
J12-1006,J03-1003,1,0.952883,"Missing"
J12-1006,N03-1020,0,0.0387736,"e, Rambow, and Whittaker 2000). The BLEU (Papineni et al. 2002) and NIST (Doddington 2002) metrics, which have their origin in machine translation evaluation, have also been proposed for REG evaluation. BLEU measures n-gram overlap between strings; for machine translation n is often set to 4, but given that referring expressions tend to be short, n = 3 seems a better option for REG evaluation (Gatt, Belz, and Kow 2009). NIST is a BLEU variant giving more importance to less frequent (and hence more informative) n-grams. Finally, Belz and Gatt (2008) also use the rouge-2 and rouge-su4 measures (Lin and Hovy 2003), originally proposed for evaluating automatically generated summaries. An obvious beneﬁt of these string metrics is that they are easy to compute automatically, whereas property-based evaluation measures such as Dice require an extensive manual annotation of selected properties. However, the added value of string-based metrics for REG is relatively unclear. It is not obvious, for instance, that a smaller Levenshtein distance is always to be preferred over a longer one; the expressions “the man wearing a t-shirt” and “the woman wearing a t-shirt” are at a mere Levenshtein distance of 2 from ea"
J12-1006,P00-1012,0,0.0587648,"he performance of an REG algorithm on a given data set. We shall see that although much work has been done in recent years, there are still signiﬁcant open questions, particularly regarding the relation between automatic metrics and human judgments. 195 Computational Linguistics Volume 38, Number 1 5.1 Corpora for REG Evaluation Text corpora are full of referring expressions. For evaluating the realization of referring expressions, such corpora are very suitable, and various researchers have used them, for instance, to evaluate algorithms for modiﬁer orderings (Shaw and Hatzivassiloglou 1999; Malouf 2000; Mitchell 2009). Text corpora are also important for the study of anaphoric links between referring expressions. The texts that make up the GNOME corpus (Poesio et al. 2004), for instance, contain descriptions of museum objects and medical patient information leaﬂets, with each of the two subcorpora containing some 6,000 NPs. Much information is marked up, including anaphoric links. Yet, text corpora of this kind are of limited value for evaluating the content selection part of REG algorithms. For that, one needs a corpus that is fully “semantically transparent” (van Deemter, van der Sluis, a"
J12-1006,W99-0108,0,0.0305107,"remarkably difﬁcult to decide when these should be used, and even harder to choose between the different types of demonstratives (Piwek 2008). Concerning pronouns, Krahmer and Theune suggested that “he” abbreviates “the (most salient) 187 Computational Linguistics Volume 38, Number 1 man,” and “she” “the (most salient) woman.” In this way, algorithms for generating distinguishing descriptions might also become algorithms for pronoun generation. Such an approach to pronoun generation is too simple, however, because additional factors are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004). Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example, emphasized the role of topics and discourse structure for pronoun generation, and pointed out that the changes in time scale are a reliable cue for this. In particular, they found that in certain places a deﬁnite description was used where a pronoun would have been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as"
J12-1006,W09-0608,0,0.021117,"e of an REG algorithm on a given data set. We shall see that although much work has been done in recent years, there are still signiﬁcant open questions, particularly regarding the relation between automatic metrics and human judgments. 195 Computational Linguistics Volume 38, Number 1 5.1 Corpora for REG Evaluation Text corpora are full of referring expressions. For evaluating the realization of referring expressions, such corpora are very suitable, and various researchers have used them, for instance, to evaluate algorithms for modiﬁer orderings (Shaw and Hatzivassiloglou 1999; Malouf 2000; Mitchell 2009). Text corpora are also important for the study of anaphoric links between referring expressions. The texts that make up the GNOME corpus (Poesio et al. 2004), for instance, contain descriptions of museum objects and medical patient information leaﬂets, with each of the two subcorpora containing some 6,000 NPs. Much information is marked up, including anaphoric links. Yet, text corpora of this kind are of limited value for evaluating the content selection part of REG algorithms. For that, one needs a corpus that is fully “semantically transparent” (van Deemter, van der Sluis, and Gatt 2006): A"
J12-1006,N03-2024,0,0.134978,"Missing"
J12-1006,W98-0607,0,0.151953,"Missing"
J12-1006,P02-1040,0,0.088798,"2008). The measures discussed so far do not take the actual linguistic realization of the referring expressions into account. For these, string distance metrics are obvious candidates, because these have proven their worth in various other areas of computational linguistics. One well-known string distance metric, which has also been proposed for REG evaluation, is the Levenshtein (1966) distance: The minimal number of insertions, deletions, and substitutions needed to convert one string into another, possibly normalized with respect to length (Bangalore, Rambow, and Whittaker 2000). The BLEU (Papineni et al. 2002) and NIST (Doddington 2002) metrics, which have their origin in machine translation evaluation, have also been proposed for REG evaluation. BLEU measures n-gram overlap between strings; for machine translation n is often set to 4, but given that referring expressions tend to be short, n = 3 seems a better option for REG evaluation (Gatt, Belz, and Kow 2009). NIST is a BLEU variant giving more importance to less frequent (and hence more informative) n-grams. Finally, Belz and Gatt (2008) also use the rouge-2 and rouge-su4 measures (Lin and Hovy 2003), originally proposed for evaluating automati"
J12-1006,J07-2004,1,0.939287,"Missing"
J12-1006,passonneau-2006-measuring,0,0.0973052,"Missing"
J12-1006,J04-3003,0,0.073445,"Missing"
J12-1006,J98-2001,0,0.516966,"been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as can be signaled, for example, by a change in tense or a cue-phrase such as “several months ago.” Kibble and Power (2004), in an alternative approach, use Centering Theory as their starting point in a constraint-based text generation framework, taking into account constraints such as salience, cohesion, and continuity for the choice of referring expressions. Many studies on contextual reference take text as their starting point (Poesio and Vieira 1998; Belz et al. 2010, among others), unlike the majority of REG research discussed so far, which uses standard knowledge representations of the kind exempliﬁed in Table 1 (or some more sophisticated frameworks, see Section 4). An interesting variant is presented by Siddharthan and Copestake (2004), who set themselves the task of generating a referring expression at a speciﬁc point in a discourse, without assuming that a knowledge base (in the normal sense of the word) is available: All their algorithm has to go by is text. For example, a text might start saying “The new president applauded the o"
J12-1006,P90-1013,0,0.5053,"Missing"
J12-1006,C92-1038,0,0.781158,"hich focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concerned with the link between the Gricean maxims and the generation of referring expressions. They discuss the following pair of examples: (1) Sit by the table. (2) Sit by the brown wooden table. In a situation where there is only one table, which happens to be brown and wooden, both the descriptions in (1) and (2) would successfully refer to their target. However, if you hear (2) you might make the additional inference that it is signiﬁcant to know that the table is brown a"
J12-1006,W10-4212,1,0.906985,"Missing"
J12-1006,P88-1003,0,0.520454,"ctive Plurals. Reference to sets is a rich topic, where many issues on the borderline between theoretical, computational, and experimental linguistics are waiting to be explored. Most computational proposals, so far, use properties that apply to individual objects. To refer to a set, in this view, is to say things that are true of each member of the set. Such references may be contrasted with collective ones (e.g., “the lines that run parallel to each other,” “the group of four people”) which are more complicated from a 183 Computational Linguistics Volume 38, Number 1 semantic point of view (Scha and Stallard 1988; Lønning 1997, among others). For initial ideas about the generation of collective plurals, we refer to Stone (2000). 3.2 Relational Descriptions Another important limitation of most early REG algorithms is that they are restricted to one-place predicates (e.g., “being a man”), instead of relations involving two or more arguments. Even a property like “wearing a suit” is modeled as if it were simply a one-place predicate without internal structure (instead of a relation between a person and a piece of clothing). This means that the algorithms in question are unable to identify one object via"
J12-1006,P04-1052,0,0.150792,"4), in an alternative approach, use Centering Theory as their starting point in a constraint-based text generation framework, taking into account constraints such as salience, cohesion, and continuity for the choice of referring expressions. Many studies on contextual reference take text as their starting point (Poesio and Vieira 1998; Belz et al. 2010, among others), unlike the majority of REG research discussed so far, which uses standard knowledge representations of the kind exempliﬁed in Table 1 (or some more sophisticated frameworks, see Section 4). An interesting variant is presented by Siddharthan and Copestake (2004), who set themselves the task of generating a referring expression at a speciﬁc point in a discourse, without assuming that a knowledge base (in the normal sense of the word) is available: All their algorithm has to go by is text. For example, a text might start saying “The new president applauded the old president.” From this alone, the algorithm has to ﬁgure out whether, in the next sentence, it can talk about “the old president” (or some other suitable noun phrase) without risk of misinterpretation by the reader. The authors argue that standard REG methods can achieve reasonable results in"
J12-1006,J11-4007,0,0.0688613,"Missing"
J12-1006,W06-1412,0,0.0869462,"Walker (2005) and Gupta and Stent (2005), who studied references in dialogue corpora discussed in Section 5. They found that in these data sets, traditional algorithms are outperformed by simple strategies that pay attention to the referring expressions produced earlier in the dialogue. A more recent machine learning experiment on a larger scale, using data from the iMap corpus, conﬁrmed the importance of features related to the process of alignment (Viethen, Dale, and Guhe 2011). Other researchers have started exploring the generation of referring expressions in interactive settings as well. Stoia et al. (2006), for example, presented a system that generates references in situated dialogues, taking into account both dialogue history and spatial visual context, deﬁned in terms of which distractors are in the current ﬁeld of vision of the speakers and how distant they are from the target. Janarthanam and Lemon (2009) present a method which automatically adapts to the expertise level of the intended addressee (using “the router” when communicating with an expert user, and “the black block with the lights” while interacting with a novice). This line of research ﬁts in well with another, more general, st"
J12-1006,W00-1416,0,0.136275,"xperimental linguistics are waiting to be explored. Most computational proposals, so far, use properties that apply to individual objects. To refer to a set, in this view, is to say things that are true of each member of the set. Such references may be contrasted with collective ones (e.g., “the lines that run parallel to each other,” “the group of four people”) which are more complicated from a 183 Computational Linguistics Volume 38, Number 1 semantic point of view (Scha and Stallard 1988; Lønning 1997, among others). For initial ideas about the generation of collective plurals, we refer to Stone (2000). 3.2 Relational Descriptions Another important limitation of most early REG algorithms is that they are restricted to one-place predicates (e.g., “being a man”), instead of relations involving two or more arguments. Even a property like “wearing a suit” is modeled as if it were simply a one-place predicate without internal structure (instead of a relation between a person and a piece of clothing). This means that the algorithms in question are unable to identify one object via another, as when we say “the woman next to the man who wears a suit,” and so on. One early paper does discuss relatio"
J12-1006,W98-1419,0,0.337931,"h later work in REG, given its skepticism about the naturalness of minimal descriptions, its use of Rosch (1978)–style basic categories, and its acknowledgment of the role of computational complexity. Broadly speaking, it suggests an incremental generation strategy, compatible with the ones described subsequently, although it is uncertain what exactly was implemented. In recent years, the Appelt–Kronfeld line of research has largely given way to a new research tradition which focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concern"
J12-1006,P11-2116,1,0.841007,"the ﬁrst case, and a tie between two properties that have the same discriminatory power in the second. To resolve such ties frequency data would clearly be helpful. Similar questions apply to other generation algorithms. For instance, the graph-based algorithm as described by Krahmer et al. (2008) assigns one of three different costs to properties (they can be free, cheap, or somewhat expensive), and frequency data is used to determine which costs should be assigned to which properties (properties that are almost always used in a particular domain can be for free, etc.). A recent experiment (Theune et al. 2011) suggests that training the graph-based algorithm on a corpus with a few dozen items may already lead to a good performance. In general, knowing how much data is required for a new domain to reach a good level of performance is an important open problem for many REG algorithms. 6.2 How Do We Move beyond the “Paradigms” of Reference? A substantial amount of REG research focuses on what we referred to in the Introduction as the “paradigms” of reference: “ﬁrst-mention” distinguishing descriptions consisting of a noun phrase starting with “the” that serve to identify some target, and that do so wi"
J12-1006,W09-0607,0,0.100674,"Missing"
J12-1006,W08-1104,0,0.0894647,"cal information (Reiter and Dale 2000). Of all the subtasks of NLG, Referring Expression Generation (REG) is among those that have received most scholarly attention. A survey of implemented, practical NLG systems shows that virtually all of them, regardless of their purpose, contain an REG module of some sort (Mellish et al. 2006). This is hardly surprising in view of the central role that reference plays in communication. A system providing advice about air travel (White, Clark, and Moore 2010) needs to refer to ﬂights (“the cheapest ﬂight,” “the KLM direct ﬂight”), a pollen forecast system (Turner et al. 2008) needs to generate spatial descriptions for areas with low or high pollen levels (“the central belt and further North”), and a robot dialogue system that assembles construction toys together with a human user (Giuliani et al. 2010) needs to refer to the components (“insert the green bolt through the end of this red cube”). REG “is concerned with how we produce a description of an entity that enables the hearer to identify that entity in a given context” (Reiter and Dale 2000, page 55). Because this can often be done in many different ways, a REG algorithm needs to make a number of choices. Acc"
J12-1006,J02-1003,1,0.886397,"Missing"
J12-1006,J06-2002,1,0.925449,"Missing"
J12-1006,W06-1420,1,0.92113,"Missing"
J12-1006,W06-1410,0,0.414977,"or this data set, participants were asked to describe objects in various computer generated scenes. Each of these scenes contained up to 30 objects (“cones”) randomly positioned on a virtual surface. All objects had the same shape and size, and hence targets could only be distinguished using their color (either green or purple) and their location on the surface (“the green cone at the left bottom”). Each participant was asked to identify targets in one shot, and for the beneﬁt of an addressee who was physically present but did not interact with the participant. The Drawer corpus, collected by Viethen and Dale (2006), has a similar objective, but here targets are real, being one of 16 colored drawers in a ﬁling cabinet. On different occasions, participants were given a random number between 1 and 16 and asked to refer to the corresponding drawer for an onlooker. Naturally, they were asked not to use the number; instead they could refer to the target drawers using color, row, and column, or some combination of these. In this corpus, referring expressions (“the pink drawer in the ﬁrst row, third column”) once again solely serve an identiﬁcation purpose. Viethen and Dale (2008) also collected another corpus"
J12-1006,D11-1107,0,0.117858,"Missing"
J12-1006,viethen-etal-2008-controlling,1,0.891802,"uld assign costs in accordance with the list of preferred attributes in the IA, making more preferred properties cheaper than less preferred ones. A third possibility is to compute the costs of an edge e in terms of the probability P(e) that e occurs in a distinguishing description (which can be estimated by counting occurrences in a corpus), making frequent properties cheap and rare ones expensive: cost(e) = −log2 (P(e)) Experiments with stochastic cost functions have shown that these enable the graphbased algorithm to capture a lot of the ﬂexibility of human references (Krahmer et al. 2008; Viethen et al. 2008). In the graph-based perspective, relations are treated in the same way as individual properties, and there is no risk of running into inﬁnite loops (“the cup to the left of the saucer to the right of the cup . . . ”). Unlike Dale and Haddock (1991) and Kelleher and Kruijff (2006), no special measures are required, because a relational edge is either included in a referring graph or not: including it twice is not possible. Van Deemter and Krahmer (2007) show that many of the proposals discussed in Section 3 can be recast in terms of graphs. They argue, however, that the graph-based approach is"
J12-1006,viethen-etal-2010-dialogue,0,0.0261134,"ess suitable for studying content determination. To facilitate the study of reference, the iMap corpus was created (Guhe and Bard 2008), a modiﬁed version of the Map Task corpus where landmarks are not labelled, and systematically differ along a number of dimensions, including type (owl, penguin, etc.), 196 Krahmer and van Deemter Computational Generation of Referring Expressions number (singular, plural) and color; a target may thus be referred to as “the two purple owls.” Because participants may refer to targets more than once, it becomes possible to study initial and subsequent reference (Viethen et al. 2010). Yet another example is the Coconut corpus (Di Eugenio et al. 2000), a set of taskoriented dialogues in which participants negotiate which furniture items they want to buy on a ﬁxed, shared budget. Referring expressions in this corpus (“a yellow rug for 150 dollars”) do not only contain information to identify a particular piece of furniture, but also include properties which directly refer to the task at hand (e.g., how much money is still available for a particular furniture item and what the state of agreement between the negotiators is). An attractive aspect of these corpora is that they"
J12-1006,J10-2001,0,0.0303998,"Missing"
J12-1006,H89-1033,0,0.222391,"eworks, such as Graph Theory and Description Logic (Section 4), and collection of data and evaluation of REG algorithms (Section 5). Section 6 highlights open questions and avenues for future work. Section 7 summarizes our ﬁndings. 2. A Very Short History of Pre-2000 REG Research The current survey focuses primarily on the progress in REG research in the 21st century, but it is important to have a basic insight into pre-2000 REG research and how it laid the foundation for much of the current work. 2.1 First Beginnings REG can be traced back to the earliest days of Natural Language Processing; Winograd (1972) (Section 8.3.3, Naming Objects and Events), for example, sketches a primitive “incremental” REG algorithm, used in his SHRDLU program. In the 1980s, researchers such as Appelt and Kronfeld set themselves the ambitious task of modeling the human capacity for producing and understanding referring expressions in programs such as KAMP and BERTRAND (Appelt 1985; Appelt and Kronfeld 1987; Kronfeld 1990). They argued that referring expressions should be studied as part of a larger speech act. KAMP (Appelt 1985), for example, was conceived as a general utterance planning system, building on Cohen and"
kibble-van-deemter-2000-coreference,M98-1022,0,\N,Missing
kibble-van-deemter-2000-coreference,M95-1005,0,\N,Missing
kibble-van-deemter-2000-coreference,J00-4005,1,\N,Missing
kibble-van-deemter-2000-coreference,W99-0213,1,\N,Missing
N13-1137,W08-1107,0,0.163416,"n method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Motivation & Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on"
N13-1137,P08-2050,0,0.0193456,"he training data. The ordered attribute lists for the algorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5 We remove location from evaluation in this corpus. Location is not annotated directly, but split such that only xdimension or y-dimension may be marked for a reference. 1180 descending frequency. For these corpora, there are not absolute properties beyond color, so AP is empty. 6 Evaluation Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice (Belz and Gatt, 2008), and Accuracy (Gatt et al., 2009; Reiter and Belz, 2009). Uniqueness is the proportion of outputs that identify the referent uniquely, and Minimality is the proportion of outputs that are both minimal and unique. As our goal is to mimic human reference, these metrics are not as useful for the evaluations as the others. The Dice metric provides a value for the similarity between a generated description and a humanproduced description, and therefore serves as a reasonable objective measure for how human-like the produced sets are. Given the generated property set (DS ) and the human-produced pr"
N13-1137,P89-1009,0,0.334524,"l input; saying an object is “tall” requires further reasoning). 4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Motivation & Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains"
N13-1137,W08-1108,0,0.143947,"is represented as a vertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during search. This has 3 https://github.com/nltk/nltk contrib/blob/master/ nltk contrib/referring.py retrieved 1.Aug.2012. been one of the best-performing systems in recent generation challenges (Gatt and Belz, 2008; Gatt et al., 2009). An important commonality between these algorithms, and much of the work on REG that they have influenced, is the focus on unique identification and operating deterministically. Both produce one property set (and only one), and stop once a target item has been uniquely identified (or else fail). Their driving goal is to rule out distractor objects. In the approach introduced here, the algorithm produces a distribution over several possible outputs, and the initial driving mechanism is based on likelihood estimates for each attribute independent of the other objects in the"
N13-1137,W09-0629,0,0.0194427,"ertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during search. This has 3 https://github.com/nltk/nltk contrib/blob/master/ nltk contrib/referring.py retrieved 1.Aug.2012. been one of the best-performing systems in recent generation challenges (Gatt and Belz, 2008; Gatt et al., 2009). An important commonality between these algorithms, and much of the work on REG that they have influenced, is the focus on unique identification and operating deterministically. Both produce one property set (and only one), and stop once a target item has been uniquely identified (or else fail). Their driving goal is to rule out distractor objects. In the approach introduced here, the algorithm produces a distribution over several possible outputs, and the initial driving mechanism is based on likelihood estimates for each attribute independent of the other objects in the scene, rather than r"
N13-1137,J09-2005,0,0.021447,"a way similar to human visual processing, the generated expression may be overspecified or underspecified. We are limited by available REG corpora to reliably assess methods for generating more complex absolute properties like shape and material, but adding such properties would help advance the generation of human-like reference in visual scenes and offers further points of connection between the generation process and computer vision property detection. Models for generating more complex spatial relations are currently available, and are a natural extension to this framework (e.g., those of Kelleher and Costello (2009)) as object detection becomes more robust. We may also be able to build more sophisticated graphical models as larger corpora become available. For example, modeling the conditional probability of generating reference for a property vn given the previously generated context p(vn |v1 . . . vn−1 ) may bring us closer to human-like output. There are several additional issues that do not arise in this evaluation, but we expect must be accounted for when referring to naturalistic objects in improves performance. 1182 visual domains. These include: • The interconnected nature of properties, where so"
N13-1137,W12-1503,0,0.0289095,"Missing"
N13-1137,J12-1006,1,0.849312,"Missing"
N13-1137,J03-1003,0,0.0294139,"Missing"
N13-1137,W11-2808,1,0.858487,"Missing"
N13-1137,J09-4008,1,0.840197,"lgorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5 We remove location from evaluation in this corpus. Location is not annotated directly, but split such that only xdimension or y-dimension may be marked for a reference. 1180 descending frequency. For these corpora, there are not absolute properties beyond color, so AP is empty. 6 Evaluation Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice (Belz and Gatt, 2008), and Accuracy (Gatt et al., 2009; Reiter and Belz, 2009). Uniqueness is the proportion of outputs that identify the referent uniquely, and Minimality is the proportion of outputs that are both minimal and unique. As our goal is to mimic human reference, these metrics are not as useful for the evaluations as the others. The Dice metric provides a value for the similarity between a generated description and a humanproduced description, and therefore serves as a reasonable objective measure for how human-like the produced sets are. Given the generated property set (DS ) and the human-produced property set (DH ), Dice is calculated as: 2 × |DS ∩ DH | |"
N13-1137,C92-1038,1,0.627793,"ing an object is “tall” requires further reasoning). 4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Motivation & Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975;"
N13-1137,P11-2116,0,0.0738951,"precedes size in the preference orders, in line with recent research showing that this allows the algorithm to perform optimally on the TUNA corpus (van Deemter et al., 2012a). In development, we find that IA performs best with type as the last attribute in the PO, and report on numbers with this approach. 5.2.2 The Graph-Based Algorithm The version of the Graph-Based Algorithm that we use is available from Viethen et al. (2008). This algorithm requires (1) a set of cost functions for each edge, and (2) a PO for deciding between properties in the case of a tie. For (1), we use the method from Theune et al. (2011) to assign two costs (0, 1) to the edges. We first determine the relative frequency with which each property is mentioned for a target object, and then create costs for each property using k-means clustering (k=2) in the Weka toolkit (Hall et al., 2009). We refer interested readers to the Theune et al. paper for further details. For (2), we follow the same method as for the Incremental Algorithm. 5.2.3 The Visual Objects Algorithm The proposed algorithm requires αatt , which we estimate as the relative frequency of each attribute att in the training data. The ordered attribute lists for the al"
N13-1137,W06-1420,1,0.52943,"Missing"
N13-1137,W08-1109,0,0.418851,"colour:green size:(254,254) type:fan loc:(2,2) ori:left Figure 6: Example input scene: TUNA corpus. For IA And GB, gold-standard size values are provided rather than measurements (small, large). As such, although hcolor:grey, type:deski would sufficiently distinguish the intended referent, we instead produce a variety of sets, overspecifying in some instances (e.g., hcolor:grey, ori:front, type:deski), and with a small chance of underspecifying in others (e.g., hsize:large, type:deski). 5 Evaluation Algorithms & Corpora 5.1 Corpora We evaluate on two well-known REG corpora, the GRE3D3 corpus (Viethen and Dale, 2008) and the singular furniture section of the TUNA corpus (van Deemter et al., 2006). Both corpora contain expressions elicited to computer-generated objects, and so provide a reasonable starting point for evaluating reference to visible objects. For all algorithms, we evaluate on the selection of referent attributes. Lexical choice and word order are not taken into account. Example images from GRE3D3 and TUNA are shown in Figure 4, and example algorithm input from these corpora are shown in Figures 5 and 6. In GRE3D3, we evaluate on the selection of type, color, size, and location, but leave asi"
N13-1137,U10-1013,0,0.10772,"respond to general visual attributes and may generate forms for visual properties (attribute:value pairs). That is, a property such as color:red is generated from the attribute node color and a property such as size:tall is generated from the attribute node size. We are limited by existing REG corpora in which properties we can evaluate; in this paper, we examine the effect of the independent selection of color and size, followed by location and orientation.2 Generating human-like expressions in this setting begins to be possible by adopting recent proposals that REG handle speaker variation (Viethen and Dale, 2010) and the non-deterministic nature of reference (van Gompel et al., 2012; van Deemter et al., 2012b). We can capture such variation simply by estimating αatt , the likelihood that an attribute att generates a corresponding visual property. During generation, the algorithm passes through each attribute node, and uses this estimate to stochastically add each property to the output property set. Such a non-deterministic process means that the algorithm will not return the same output every time, which offers new challenges for evaluation. If we run the algorithm 1,000 times, we have a distribution"
N13-1137,viethen-etal-2008-controlling,0,0.376005,"ce a novel algorithm for generating referring expressions, informed by human and computer vision and designed to refer to visible objects. Our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs. Expressions generated using this method are often overspecified and may be underspecified, akin to expressions produced by people. We call such expressions identifying descriptions. The algorithm outperforms the well-known Incremental Algorithm (Dale and Reiter, 1995) and the GraphBased Algorithm (Krahmer et al., 2003; Viethen et al., 2008) across a variety of images in two domains. We additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm’s non-determinism into account. 1 Introduction Referring expression generation (REG) is the task of generating an expression that can identify a referent to a listener. These expressions generally take the form of a definite noun phrase such as “the large orange plate” or “the furry running dog”. Research in REG primarily focuses on the subtask of selecting a set of properties that may be used to construct the final surface expression,"
N13-1137,J02-4007,1,\N,Missing
P01-1015,W00-1410,1,0.816566,"which all the data levels can be represented in a common framework consisting of networks of typed ‘objects’ connected by typed ‘arrows’. This lingua franca allows NLG modules to manipulate data flexibly and consistently. It also facilitates modular design of NLG systems, and reusability of modules and data sets. However, it does not in itself say anything about how modules in such a system might interact. This paper describes a concrete realisation of the RAGS object and arrows model, OASYS, as applied to a simple but flexible NLG system called RICHES. This is not the first such realisation: Cahill et al., (2000) describes a partial re-implementation of the ‘Caption Generation System’ (Mittal et al., 1999) which includes an objects and arrows ‘whiteboard’. The OASYS system includes more specific proposals for processing and inter-module communication, and RICHES demonstrates how this can be used to support a modular architecture based on small scale functionally-motivated units. 3 OASYS Quote representations These are used to represent literal unanalysed content used by a generator, such as canned text, pictures or tables. OASYS (Objects and Arrows SYStem) is a software library which provides: The rep"
P01-1015,copestake-flickinger-2000-open,0,0.0196598,"7) and FUF/SURGE (Elhadad et al., 1997). Renderer (REND) The Renderer is the module that puts the concrete document together. Guided by the document structure, it produces HTML formatting for the text and positions and references the pictures. Individual sentences are produced for it by LinGO, via the FLO interface. FLO actually processes sentences independently of REND, so when REND makes a request, either the sentence is there already, or the request is queued, and serviced when it becomes available. LinGO The LinGO realiser uses a widecoverage grammar of English in the LKB HPSG framework, (Copestake and Flickinger, 2000). The tactical generation component accepts input in the Minimal Recursion Semantics formalism and produces the target text using a chartdriven algorithm with an optimised treatment of modification (Carroll et al., 1999). No domainspecific tuning of the grammar was required for the RICHES system, only a few additions to the lexicon were necessary. 5 An example: generation in RICHES In this section we show how RICHES generates the first sentence of the example text, Blow your nose so that it is clear and the picture that accompanies the text. The system starts with a rhetorical representation ("
P01-1015,J97-2001,0,0.0233148,"clauses. In addition, SF decides whether a sentence should be imperative, depending on who the reader of the document is (an input parameter to the system). Finalise Lexical Output (FLO) RICHES uses an external sentence realiser component with its own non-RAGS input specification. FLO provides the interface to this realiser, extracting (mostly syntactic) information from OASYS and converting it to the appropriate form for the realiser. Currently, FLO supports the LinGO realiser (Carroll et al., 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al., 1997). Renderer (REND) The Renderer is the module that puts the concrete document together. Guided by the document structure, it produces HTML formatting for the text and positions and references the pictures. Individual sentences are produced for it by LinGO, via the FLO interface. FLO actually processes sentences independently of REND, so when REND makes a request, either the sentence is there already, or the request is queued, and serviced when it becomes available. LinGO The LinGO realiser uses a widecoverage grammar of English in the LKB HPSG framework, (Copestake and Flickinger, 2000). The ta"
P01-1015,A97-1039,0,0.0462862,"example, combining main and subordinate clauses. In addition, SF decides whether a sentence should be imperative, depending on who the reader of the document is (an input parameter to the system). Finalise Lexical Output (FLO) RICHES uses an external sentence realiser component with its own non-RAGS input specification. FLO provides the interface to this realiser, extracting (mostly syntactic) information from OASYS and converting it to the appropriate form for the realiser. Currently, FLO supports the LinGO realiser (Carroll et al., 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al., 1997). Renderer (REND) The Renderer is the module that puts the concrete document together. Guided by the document structure, it produces HTML formatting for the text and positions and references the pictures. Individual sentences are produced for it by LinGO, via the FLO interface. FLO actually processes sentences independently of REND, so when REND makes a request, either the sentence is there already, or the request is queued, and serviced when it becomes available. LinGO The LinGO realiser uses a widecoverage grammar of English in the LKB HPSG framework, (Co"
P01-1015,A00-1017,0,0.0710182,"sity of Sussex Brighton, BN1 9QH, UK johnca@cogs.susx.ac.uk Abstract The RAGS proposals for generic specification of NLG systems includes a detailed account of data representation, but only an outline view of processing aspects. In this paper we introduce a modular processing architecture with a concrete implementation which aims to meet the RAGS goals of transparency and reusability. We illustrate the model with the RICHES system – a generation system built from simple linguisticallymotivated modules. 1 Introduction As part of the RAGS (Reference Architecture for Generation Systems) project, Mellish et al (2000) introduces a framework for the representation of data in NLG systems, the RAGS ‘data model’. This model offers a formally well-defined declarative representation language, which supports the complex and dynamic data requirements of generation systems, e.g. different levels of representation (conceptual to syntax), mixed representations that cut across levels, partial and shared structures and ‘canned’ representations. However  We would like to acknowledge the financial support of the EPSRC (RAGS – Reference Architecture for Generation Systems: grant GR/L77102 to Donia Scott), as well as the"
P01-1015,C00-2093,1,0.828569,"picture should illustrate it. Pic2 The dashed lines indicate flow of information, solid arrows indicate approximately flow of control between modules, double boxes indicate a completely reused module (from another system), while a double box with a dashed outer indicates a module partially reused. Ellipses indicate information sources, as opposed to processing modules. tures, annotated with their SemReps, are part of the picture library, and Media Selection builds small pieces of DocRep referencing the pictures. Document Planner (DP) The Document Planner, based on the ICONOCLAST text planner (Power, 2000) takes the input RhetRep and produces a document structure (DocRep). This specifies aspects such as the text-level (e.g., paragraph, sentence) and the relative ordering of propositions in the DocRep. Its leaves refer to SynReps corresponding to syntactic phrases. This module is pipelined after MS, to make sure that it takes account of any pictures that have been included in the document. Lexical Choice (LC) Lexical choice happens in two stages. In the first stage, LC chooses the lexical items for the predicate of each SynRep. This fixes the basic syntactic structure of the proposition, and the"
P01-1015,W94-0319,0,0.0336301,"ther colleagues at the ITRI, especially Nedjet BouayadAgha. We would also like to acknowledge the contribution of colleagues who worked on the RICHES system previously: Neil Tipper and Rodger Kibble. We are grateful to our anonymous referees for their helpful comments. RAGS, as described in that paper, says very little about the functional structure of an NLG system, or the issues arising from more complex processing regimes (see for example Robin (1994), Inuie et al., (1992) for further discussion). NLG systems, especially end-to-end, applied NLG systems, have many functionalities in common. Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline. More recently Cahill et al (1999) attempted to repeat the analysis, but found that while most systems did implement a pipeline, they did not implement the same pipeline – different functionalities occurred in different ways and different orders in different systems. But this survey did identify a number of core functionalities which seem to occur during the execution of most systems. In order to accommodate this result, a ‘process model’ was sketched which aimed to support both pipelines and more complex control r"
P01-1015,J98-3004,0,\N,Missing
P06-2033,E06-1041,1,0.789342,"y by assuming a one-to-one mapping between properties and words. Another requirement is to distinguish between type properties (the set T ), and non-types (M )3 . The Thesaurus is used to find pairwise similarity of types in order to group them into related clusters. Word Sketches are used to find, for each type, the modifiers in the KB that are appropriate to the type, on the basis of the associated salience values. For example, in Table 3, e3 has plump as the value for girth, which combines more felicitously with man, than with biologist. Types are clustered using the algorithm described in Gatt (2006). For each type t, the algorithm finds its nearest neighbour nt in semantic space. Clusters are then found by recursively grouping elements with their nearest neighbours. If t, t′ have a common nearest neighbour n, then {t, t′ , n} is a cluster. Clearly, the resulting sets are convex in the sense of Definition 1. Each modifier is assigned to a cluster by finding in its Word Sketch the type with which it co-occurs with the greatest salience value. Thus, a cluster is a pair Definition 2. Maximal coherence A description D is maximally coherent iff there is no description D ′ coextensive with D su"
P06-2033,P00-1019,0,0.0485914,"Missing"
P06-2033,P89-1008,0,0.115753,"the top row, IA would yield the postgraduate and the chef, which is fine in case occupation is the relevant attribute in the discourse, but otherwise is arguably worse than an alternative like the italian and the maltese, because it is more difficult to see what a postgraduate and a chef have in common. Related issues have been raised in the formal semantics literature. Aloni (2002) argues that an appropriate answer to a question of the form ‘Wh x?’ must conceptualise the different instantiations of x using a perspective which is relevant given the hearer’s information state and the context. Kronfeld (1989) distinguishes a description’s functional relevance – i.e. its success in distinguishing a referent – from its conversational relevance, which arises in part from implicatures. In our example, describing e1 as the postgraduate carries the implicature that the entity’s academic role is relevant. When two entities are described using contrasting properties, say the student and the italian, the contrast may be misleading for the listener. Any attempt to port these observations to the GRE scenario must do so without sacrificing logical completeness. While a GRE algorithm should attempt to find the"
P06-2033,E99-1005,0,0.0128217,"r’ = 1,2,3; ‘dissimilar’ = 4) showed a significant difference in proportions for the nominal category (χ2 = 4.78, p = .03), but not the modifier category. Pairwise comparisons showed a significantly larger proportion of 1 (Z = 2.7, p = .007) and 2 responses (Z = 2.54, p = .01) in the nominal compared to the modifier condition. id base type occupation specialisation girth e1 woman professor physicist plump e2 woman lecturer geologist thin e3 man lecturer biologist plump e4 man chemist thin Table 2: An example knowledge base restrictions on the plausibility of adjective-noun combinations exist (Lapata et al., 1999), and that using unlikely combinations (e.g. the immaculate kitchen rather than the spotless kitchen) impacts processing in online tasks (Murphy, 1984). Unlike types, which have a categorisation function, modifiers have the role of adding information about an element of a category. This would partially explain the experimental results: When elements of a plurality have identical types (as in the modifier version of our experiment), the CC is already satisfied, and selection of modifiers would presumably depend on respecting adjective-noun combination restrictions. Further research is required"
P06-2033,J90-1003,0,0.0661115,"Missing"
P06-2033,P89-1009,0,0.614083,"en the avoidance of redundancy and achieving coherence. It is to an investigation of this tension that we now turn. Definition 3. Local coherence A description D is locally coherent iff: a. either D is maximally coherent or b. there is no D ′ coextensive with D, obtained by replacing types from some perspective in PD with types from another perspective such that w(D) &gt; w(D ′ ). 4 Evaluation It has been known at least since Dale and Reiter (1995) that the best distinguishing description is not always the shortest one. Yet, brevity plays a part in all GRE algorithms, sometimes in a strict form (Dale, 1989), or by letting the algorithm approximate the shortest description (for example, in the Dale and Reiter’s IA). This is also true of references to sets, the clearest example being Gardent’s constraint based approach, which always finds the description with the smallest number of logical operators. Such proposals do not take coherence (in our sense of the word) into account. This raises obvious questions about the relative importance of brevity and coherence in reference to sets. The evaluation took the form of an experiment to compare the output of our Coherence Model with the family of algorit"
P06-2033,P02-1013,0,0.439447,"ence Constraint (CC): As far as possible, describe objects using related properties. 1 Introduction Algorithms for the Generation of Referring Expressions (GRE) seek a set of properties that distinguish an intended referent from its distractors in a knowledge base. Much of the GRE literature has focused on developing efficient content determination strategies that output the best available description according to some interpretation of the Gricean maxims (Dale and Reiter, 1995), especially Brevity. Work on reference to sets has also proceeded within this general framework (van Deemter, 2002; Gardent, 2002; Horacek, 2004). One problem that has not received much attention is that of conceptual coherence in the generation of plural references, i.e. the ascription of related properties to elements of a set, so that the resulting description constitutes a coherent cover for the plurality. As an example, consider a reference to {e1 , e3 } in Table 1 using the Incremental Algorithm (IA) (Dale and Reiter, 1995). IA searches along an ordered list of attributes, selecting properties of the intended referents that remove some distractors. Assuming the ordering in the top row, IA would yield the postgradu"
P06-2033,P04-1052,0,0.0505963,"Missing"
P06-2033,J02-1003,1,0.918351,"Missing"
W00-1424,P98-2173,0,0.0626268,"Missing"
W00-1424,P99-1018,0,0.0761568,"Missing"
W00-1424,C98-2168,0,\N,Missing
W01-0804,P83-1009,0,0.258416,"Missing"
W01-0804,C00-2093,0,0.0450034,"Missing"
W01-0804,P90-1013,0,0.403563,"Missing"
W01-0804,J93-1008,0,0.0432879,"Missing"
W01-0804,T87-1042,0,\N,Missing
W01-0804,W00-1416,0,\N,Missing
W01-0804,W00-1424,1,\N,Missing
W01-0804,P99-1017,0,\N,Missing
W01-0804,C90-3016,1,\N,Missing
W02-2115,P89-1009,0,0.096073,"e the term ‘description’ loosely to refer to either the combination of properties or it linguistic realization. G RE algorithms take as their input a knowledge base that is shared between writer and reader, generating unique descriptions of entities whenever the knowledge base allows it (van Deemter 2002). These algorithms are designed in such a way that generation is made easy (i.e., quick). The implications for the reader, for example in terms of the difficulty of finding the referent, are never taken into account. Note that some algorithms do generate descriptions that are optimally brief (Dale 1989), while others approximate optimal brevity (Dale and Reiter 1995), and this can be argued to make interpretation easier. As we have seen, however, a short description can sometimes make search difficult. To illustrate, the Incremental Algorithm works roughly as follows. (See Dale and Reiter 1995 for details). Attributes represented in the shared Knowledge base are ordered in a linear preference ordering. The Attributes in the ordering are considered one by one, to see if any of their Values contributes something to the description, typically by removing ‘distractors’ (i.e., objects other than"
W02-2115,E91-1028,0,0.892772,"ally ordered (in sections, subsections, etc.), allowing us to picture it as a tree. Documents and the hierarchical relations that hold them together can be modelled in different ways. For present purposes, we stick with a simple Attribute/Value model.2 One Attribute, TYPE, says 2 At first sight, a Dale and Haddock-style modeling based on relational properties looks promising for the modeling of a hierarchy, but appearances are deceptive. Consider, for example, a description like ‘the vase on the table’, which can be said when there are many tables, as long as only one of them supports a vase (Dale and Haddock 1991). By comparison, if a document contains only two pictures, one of which occurs in the only Appendix, while the other occurs in one of the many what kind of entity it is (picture, table, section, subsection, part, etc.). Given its TYPE, the identity of the referent within its parent node can usually be determined by means of one other Attribute. For example, a picture might be identified by the property T YPE : P ICTURE and the property P ICTURE N UM BER : 3, the combination of which may be realized as ‘picture (iii)’. Often, of course, this will not be enough to identify the referent within th"
W02-2115,P00-1019,0,0.143147,"an entity is mentioned, disregarding anaphoric references (e.g., ‘it’). 2 Generating references that are easy to resolve Generation of Referring Expressions (GRE) is a key task of NLG systems (e.g., Reiter and Dale 2000, section 5.4). The task of a GRE algorithm is to find combinations of properties that allow the generator to refer uniquely to an entity, called the target of the algorithm, and to express these properties in a linguistic description. We will focus on the first part of the problem, which involves determining the semantic content of a description, paying no attention 1 Compare Jordan 2000, Jordan (forthcoming), where other factors triggering logically redundant material in referring expressions are explored. to linguistic realization (e.g., Malouf 2000). When there is no risk of confusion, we will use the term ‘description’ loosely to refer to either the combination of properties or it linguistic realization. G RE algorithms take as their input a knowledge base that is shared between writer and reader, generating unique descriptions of entities whenever the knowledge base allows it (van Deemter 2002). These algorithms are designed in such a way that generation is made easy (i."
W02-2115,P00-1012,0,0.0581529,"a key task of NLG systems (e.g., Reiter and Dale 2000, section 5.4). The task of a GRE algorithm is to find combinations of properties that allow the generator to refer uniquely to an entity, called the target of the algorithm, and to express these properties in a linguistic description. We will focus on the first part of the problem, which involves determining the semantic content of a description, paying no attention 1 Compare Jordan 2000, Jordan (forthcoming), where other factors triggering logically redundant material in referring expressions are explored. to linguistic realization (e.g., Malouf 2000). When there is no risk of confusion, we will use the term ‘description’ loosely to refer to either the combination of properties or it linguistic realization. G RE algorithms take as their input a knowledge base that is shared between writer and reader, generating unique descriptions of entities whenever the knowledge base allows it (van Deemter 2002). These algorithms are designed in such a way that generation is made easy (i.e., quick). The implications for the reader, for example in terms of the difficulty of finding the referent, are never taken into account. Note that some algorithms do"
W02-2115,J02-1003,1,\N,Missing
W06-1409,E91-1028,0,0.658701,"r can achieve the same knowledge. Many of our examples will be drawn from a simple model of a University campus, structured into buildings and rooms; the intended referent will often be a library located in one of the rooms. The location of the library is not known to the hearer, but it is known to the speaker. Each domain entity r will be University of Brighton Watts building Cockcroft building 2 Hierarchical domains Existing work on GRE tends to focus on fairly simple domains, dominated by one-place properties. When relations (i.e., two-place properties) are taken into account at all (e.g., Dale and Haddock 1991, Krahmer and Theune 2002), the motivating examples are kept so small that it is reasonable to assume that speaker and hearer know all the relevant facts in advance. Consequently, search is not much of an issue (i.e., resolution is easy): the hearer can identify the referent by simply intersecting the denotations of the properties in the description. While such simplifications permit the study of many aspects of reference, other aspects come to the fore when larger domains are considered. Interesting questions arise, for example, when a large domain is hierarchically ordered. We consider a dom"
W06-1409,C94-2182,0,0.316554,"xtually required. Uncertainty can arise, for example, if the hearer does not know about a property, or if she does not know whether it applies to the target referent. Our own work explores the need for overspecification in situations where each of the properties in question is unproblematic (i.e., certain) in principle, but where the reader has to make an effort to discover their extension (i.e., what objects are truthfully described by the property). We ask how a generator can use logically redundant information to reduce the search space within which a reader has to ‘find’ a referent. (Cf., Edmonds 1994 for a related set of problems.) A crucial question, in all such cases, is what knowledge is shared between speaker and hearer at utterance time. It will be convenient to start by focussing on the extreme case where, before the start of resolution, knows nothing about the domain. When the utterance is made, the hearer’s blindfold is removed, so to speak, and resolution can start. No similar assumption about the speaker is made: we assume that the speaker knows everything about the domain, and that he knows that the hearer can achieve the same knowledge. Many of our examples will be drawn from"
W06-1409,W05-1606,0,0.0450047,"s tend to be included when they fulfill one of a number of pragmatic functions, such as to indicate that a property is of particular importance to the speaker, or to highlight the speaker’s awareness that the referent has the property in question (Jordan 2000). However, redundancy has been built into GRE algorithms 55 Proceedings of the Fourth International Natural Language Generation Conference, pages 55–62, c Sydney, July 2006. 2006 Association for Computational Linguistics only to a very limited extent. Perhaps the most interesting account of overspecification so far is the one proposed by Horacek (2005), where logically redundant properties enter the descriptions generated when the combined certainty of other properties falls short of what is contextually required. Uncertainty can arise, for example, if the hearer does not know about a property, or if she does not know whether it applies to the target referent. Our own work explores the need for overspecification in situations where each of the properties in question is unproblematic (i.e., certain) in principle, but where the reader has to make an effort to discover their extension (i.e., what objects are truthfully described by the propert"
W06-1409,P00-1019,0,0.132789,"et) than (1b), but the additional material in (1a) makes resolution easier once interpretation is successfully completed. We explore how an GRE program should make use of logically redundant properties so as to simplify resolution (i.e., the identification of the referent). In corpus-based studies, it has been shown that logically redundant properties tend to be included when they fulfill one of a number of pragmatic functions, such as to indicate that a property is of particular importance to the speaker, or to highlight the speaker’s awareness that the referent has the property in question (Jordan 2000). However, redundancy has been built into GRE algorithms 55 Proceedings of the Fourth International Natural Language Generation Conference, pages 55–62, c Sydney, July 2006. 2006 Association for Computational Linguistics only to a very limited extent. Perhaps the most interesting account of overspecification so far is the one proposed by Horacek (2005), where logically redundant properties enter the descriptions generated when the combined certainty of other properties falls short of what is contextually required. Uncertainty can arise, for example, if the hearer does not know about a property"
W06-1409,W02-2115,1,0.93619,"Missing"
W06-1413,W05-1606,0,0.0203749,"pproach will ultimately shed light not only on the effect of the discourse situation, but also some aspects of generating indefinite descriptions. 3.4 Anja Arts. 2004. Overspecification in Instructive Text. Ph.D. thesis, Tilburg University, The Netherlands. References Over-specification Recently, interest has been growing in ‘overspecified’ referring expressions, which contain more information than is required to identify their intended referent. Some of this work is mainly or exclusively experimental (Jordan and Walker, 2000; Arts, 2004), but algorithmic consequences are also being explored (Horacek, 2005; Paraboni and van Deemter, 2002; van der Sluis and Krahmer, 2005). Over-specification could also arise in a dialogue situation (comparable to that in Section 3.3) if a speaker is unclear about the hearer’s knowledge, and so over-specifies (relative to his own knowledge) to increase the chances of success. This goes beyond the classical algorithms, where the main goal is total clarity, with no reason for the algorithm to add further properties to an already unambiguous expression. That is, such algorithms assume that every description S for which |[[ S ]] |= 1 has the same level of clarity (fC"
W06-1413,P00-1024,0,0.030211,"re there are just a few distractors, with fB making a large contribution to the cost. this approach will ultimately shed light not only on the effect of the discourse situation, but also some aspects of generating indefinite descriptions. 3.4 Anja Arts. 2004. Overspecification in Instructive Text. Ph.D. thesis, Tilburg University, The Netherlands. References Over-specification Recently, interest has been growing in ‘overspecified’ referring expressions, which contain more information than is required to identify their intended referent. Some of this work is mainly or exclusively experimental (Jordan and Walker, 2000; Arts, 2004), but algorithmic consequences are also being explored (Horacek, 2005; Paraboni and van Deemter, 2002; van der Sluis and Krahmer, 2005). Over-specification could also arise in a dialogue situation (comparable to that in Section 3.3) if a speaker is unclear about the hearer’s knowledge, and so over-specifies (relative to his own knowledge) to increase the chances of success. This goes beyond the classical algorithms, where the main goal is total clarity, with no reason for the algorithm to add further properties to an already unambiguous expression. That is, such algorithms assume"
W06-1413,J03-1003,0,0.256092,"Missing"
W06-1413,W02-2115,1,0.898424,"Missing"
W06-1413,P02-1013,0,\N,Missing
W06-1420,J02-1003,1,0.640807,"Missing"
W06-1420,P00-1024,0,0.140798,"Missing"
W06-1420,P90-1013,0,0.431905,"Missing"
W07-2307,P02-1013,0,0.716767,"Department of Computing Science University of Aberdeen {agatt,ivdsluis,kvdeemte}@csd.abdn.ac.uk ter match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and Reiter’s (1995) contribution, adapted to also deal with pluralities and gradable properties. A"
W07-2307,P97-1027,0,0.0435983,"the Generation of Referring Expressions using a balanced corpus Albert Gatt and Ielka van der Sluis and Kees van Deemter Department of Computing Science University of Aberdeen {agatt,ivdsluis,kvdeemte}@csd.abdn.ac.uk ter match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that fo"
W07-2307,P06-1131,0,0.148513,"of Referring Expressions using a balanced corpus Albert Gatt and Ielka van der Sluis and Kees van Deemter Department of Computing Science University of Aberdeen {agatt,ivdsluis,kvdeemte}@csd.abdn.ac.uk ter match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and R"
W07-2307,P04-1052,0,0.0140927,"ub-corpus described in §3 was conducted, using the same overall setup as the present study. In this section, we briefly discuss our main findings in this sub-corpus. A more detailed comparison of the evaluation results on the furniture domain with parallel results on the people domain is reported elsewhere. The targets in the people corpus differ from their distractors only in whether they had a beard (HAS - Conclusions In recent years, GRE has extended considerably beyond what was seen as its remit a decade ago, for example by taking linguistic context into account (Krahmer and Theune, 2002; Siddharthan and Copestake, 2004). We have been conservative by focusing on three classic algorithms discussed in Dale and Reiter (1995), with straightforward extensions to plurals and gradables. We tested the Incremental Algorithm’s match against speaker behaviour compared to other models using a a balanced, semantically and pragmatically transparent corpus. It turns out that performance depends on the preference order of the attributes that are used by the IA. Preliminary indications from a study on a more complex sub-corpus 55 support this view. This evaluation took a speakeroriented perspective. A reader-oriented perspect"
W07-2307,W00-1416,0,0.0698807,"erents, which had identical values on the MD attributes. For example, both targets might be blue in a domain where the minimally distinguishing description consisted of COLOUR. Plural/Dissimilar (PD): In the remaining 7 Plural trials, the targets had different values of the minimally distinguishing attributes. Plural referents were taken into account because plurality is pervasive in NL discourse. The literature (e.g. Gardent (2002)) suggests that they can be treated adequately by minor variations of the classic GRE algorithms (as long as the descriptions in question refer distributively, cf. Stone (2000)), which is something we considered worth testing. 3.2 Corpus annotation The XML annotation scheme (van der Sluis et al., 2006) pairs each corpus description with a representation of the domain in which it was produced. The domain representation, exemplified in Figure 1(a)), indicates which entities are target referents or distractors, and what combination of the attributes and values in Table 1 they have, as well as their numeric X - DIM and Y- DIM values (row and column numbers). 3 This was manipulated as a second, between-subjects factor. Participants were randomly placed in groups which va"
W07-2307,W06-1420,1,0.577592,"Missing"
W07-2307,J02-1003,1,0.838349,"Missing"
W07-2307,W06-1410,0,0.447421,"Missing"
W07-2307,P89-1009,0,0.411171,"lgorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict. 1 Introduction The current state of the art in the Generation of Referring Expressions (GRE) is dominated by versions of the Incremental Algorithm (IA) of Dale and Reiter (1995). Focusing on the generation of “first-mention” definite descriptions, Dale and Reiter compared the IA to a number of its predecessors, including a Full Brevity (FB) algorithm, which generates descriptions of minimal length, and a Greedy algorithm (GR), which approximates Full Brevity (Dale, 1989). In doing so, the authors focused on Content Determination (CD, which is the purely semantic part of GRE), and on a description’s ability to identify a referent for a hearer. Under this problem definition, GRE algorithms take as input a Knowledge Base (KB), which lists domain entities and their properties (often represented as attribute-value pairs), together with a set of intended referents, R. The output of CD is a distinguishing description of R, that is, a logical form which distinguishes this set from its distractors. Dale and Reiter argued that the IA was a superior model, and predicted"
W07-2307,J06-2002,1,\N,Missing
W07-2307,W02-2113,0,\N,Missing
W09-0615,C08-1055,1,0.891826,"Missing"
W09-0615,W06-1419,0,0.020107,"roduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our"
W09-0615,P04-1052,0,0.0190968,"and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is In order to study specific data, we have focussed on the construction illustrated in Section 1 above: potentially ambiguous Noun Phrases of the general form the Adj Nouni and Nounj . For such phrases, there are potentially two interpretations: wide scope (Adj modifies both Nouni and Nounj"
W09-0615,W06-1421,0,0.0195059,"describe an ongoing study with human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2"
W09-0615,W98-1419,0,0.0281273,"mter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is In order to study specific data, we have focussed on the construction illustrated in Section 1 above: potentially ambiguous Noun Phrases of the general form the Adj Nouni and Nounj . For such phrases, there are potentially two in"
W09-0615,E06-1040,0,0.027936,"ith human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Co"
W09-0615,W06-1420,1,0.826323,"Missing"
W09-0615,W08-1108,1,0.810867,"as seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce ref"
W09-0615,U06-1017,0,0.0608209,"ears, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algor"
W09-0615,W06-1409,1,\N,Missing
W09-0626,W07-2315,0,0.0430172,"Missing"
W09-0626,W09-0615,1,0.881848,"Missing"
W09-0626,W09-0616,0,0.0606708,"Missing"
W09-0626,W08-1104,0,0.0327512,"Missing"
W09-0626,J06-2002,1,0.89101,"Missing"
W10-4210,W08-1107,0,0.0536568,"gly little research on how people refer to objects in a real-world setting. This paper addresses this issue, and we begin formulating the requirements for an REG algorithm that refers to visible three-dimensional objects in the real world. Reference to objects in a visual domain provides a straightforward extension of the sorts of reference REG research already tends to consider. Toy examples outline reference to objects, people, and animals that are perceptually available before the speaker begins generating an utterance (Dale and Reiter, 1995; Krahmer et al., 2003; van Deemter et al., 2006; Areces et al., 2008). Example referents may be referred to by their color, size, type (“dog” or “cup”), whether or not they have a beard, etc. Typically, the reference process proceeds by comparing the properties of the referent with the properties of all the other items in the set. The final expression roughly conforms to the Gricean maxims (Grice, 1975). However, when the goal is to generate natural reference, this framework is too simple. The form reference takes is profoundly affected by modality, task, and audience (Chapanis et al., 1977; Cohen, 1984; Clark and Wilkes-Gibbs, 1986), and even when these aspect"
W10-4210,E06-1041,0,0.0178676,"of object prototypes. 4.1 Spatial and Visual Properties It is perhaps unsurprising to find reference that exhibits spatial knowledge in a study where objects are presented in three-dimensional space. Human behavior is anchored in space, and spatial information is essential for our ability to navigate the world we live in. However, referring expression generation algorithms geared towards spatial representations have oversimplified this tendency, keeping objects within the realm of twodimensions and only looking at the spatial relations between objects. For example, Funakoshi et al. (2004) and Gatt (2006) focus on how objects should be clustered together to form groups. This utilizes some of the spatial information between objects, but does not address the spatial, three-dimensional nature of objects themselves. Rather, objects exist as entities that may be grouped with other entities in a set or singled out as individual objects; they do not have their own spatial characteristics. Similarly, one of the strengths of the Graph-Based Algorithm (Krahmer et al., 2003) is its ability to generate expressions that involve relations between objects, and these include spatial ones (“next to”, “on top o"
W10-4210,J95-3003,0,0.072875,"hin language, and language is often a joint activity. However, most research on referring expression generation supposes a solitary generating agent.1 This tacitly assumes that reference will be taking place in a monologue setting, rather than a dialogue or group setting. Indeed, the goal of most REG algorithms is to produce uniquely distinguishing, one-shot referring expressions. Studies on natural reference usually use a two person (speaker-listener) communication task (e.g., Flavell et al., 1968; Krauss and Glucksberg, 1969; Ford and Olson, 1975). This research has 1 A notable exception is Heeman and Hirst (1995). shown that reference is more accurate and efficient when it incorporates things like gesture and gaze (Clark and Krych, 2004). There is a trade-off in effort between initiating a noun phrase and refashioning it so that both speakers understand the referent (Clark and Wilkes-Gibbs, 1986), and speakers communicate to form lexical pacts on how to refer to an object (Sacks and Schegloff, 1979; Brennan and Clark, 1996). Mutual understanding of referents is achieved in part by referring within a subset of potential referents (Clark et al., 1983; Beun and Cremers, 1998). A few studies have compared"
W10-4210,J84-2002,0,0.436048,"; Krahmer et al., 2003; van Deemter et al., 2006; Areces et al., 2008). Example referents may be referred to by their color, size, type (“dog” or “cup”), whether or not they have a beard, etc. Typically, the reference process proceeds by comparing the properties of the referent with the properties of all the other items in the set. The final expression roughly conforms to the Gricean maxims (Grice, 1975). However, when the goal is to generate natural reference, this framework is too simple. The form reference takes is profoundly affected by modality, task, and audience (Chapanis et al., 1977; Cohen, 1984; Clark and Wilkes-Gibbs, 1986), and even when these aspects are controlled, different people will refer differently to the same object (Mitchell, 2008). In light of this, we isolate one kind of natural reference and begin building the algorithmic framework necessary to generate the observed language. Psycholinguistic research has examined reference in a variety of settings, which may inform research on natural REG, but it is not always clear how to extend this work to a computational model. This is true in part because these studies favor an analysis of reference in the context of collaborati"
W10-4210,J03-1003,0,0.0598886,"Missing"
W10-4210,W06-1420,1,0.41927,"Missing"
W10-4210,W08-1109,0,\N,Missing
W10-4212,W08-1107,0,0.301189,"ted work mentioned above, issues of human-likeness are temporarily put on the backburner. These and other empirical issues will be brought to bear once it is better understood what types of KR system are best suitable for GRE, and what is the best way to pursue GRE in them. A few proposals have started to combine GRE with KR. Following on from work based on labelled directed graphs (cf. (Krahmer et al., 2003)) – a well-understood mathematical formalism that offers no reasoning support – (Croitoru and van Deemter, 2007) analysed GRE as a projection problem in Conceptual Graphs. More recently, (Areces et al., 2008) analysed GRE as a problem in Description Logic (DL), a formalism which, like Conceptual Graphs, is specifically designed for representing and reasoning with potentially complex information. The idea is to produce a formula such as Dog u ∃love.Cat (the set of dogs intersected with the set of objects that love at least one cat); this is, of course, a successful reference if there exists exactly one dog who loves at least one cat. This approach forms the starting point for the present paper, which aims to show that when a principled, logic based approach is chosen, it becomes possible to refer t"
W10-4212,P08-2050,0,0.0239322,"issue in theoretical linguistics and psycholinguistics, and GRE is a crucial component of almost every practical NLG system. In the years following seminal publications such as (Dale and Reiter, 1995), GRE has become one of the most intensively studied areas of NLG, with links to many other areas of Cognitive Science. After plan-based contributions (e.g., (Appelt, 1985)), recent work increasingly stresses the human-likeness of the expressions generated in simple situations, culminating in two evaluation campaigns in which dozens of GRE algorithms were compared to human-generated expressions (Belz and Gatt, 2008; Gatt et al., 2009). Figure 1: An example in which edges from women to dogs denote f eed relations, from dogs to cats denote love relations. Traditional GRE algorithms are usually based on very elementary, custom-made, forms of Knowledge Representation (KR), which allow little else than atomic facts (with negation of atomic facts left implicit), often using a simple hAttribute : V aluei format, e.g hT ype : Dogi. This is justifiable as long as the properties expressed by these algorithms are simple one-place predicates (e.g., being a dog), but when logically more complex descriptions are invo"
W10-4212,W09-0629,0,0.0796664,"linguistics and psycholinguistics, and GRE is a crucial component of almost every practical NLG system. In the years following seminal publications such as (Dale and Reiter, 1995), GRE has become one of the most intensively studied areas of NLG, with links to many other areas of Cognitive Science. After plan-based contributions (e.g., (Appelt, 1985)), recent work increasingly stresses the human-likeness of the expressions generated in simple situations, culminating in two evaluation campaigns in which dozens of GRE algorithms were compared to human-generated expressions (Belz and Gatt, 2008; Gatt et al., 2009). Figure 1: An example in which edges from women to dogs denote f eed relations, from dogs to cats denote love relations. Traditional GRE algorithms are usually based on very elementary, custom-made, forms of Knowledge Representation (KR), which allow little else than atomic facts (with negation of atomic facts left implicit), often using a simple hAttribute : V aluei format, e.g hT ype : Dogi. This is justifiable as long as the properties expressed by these algorithms are simple one-place predicates (e.g., being a dog), but when logically more complex descriptions are involved, the potential"
W10-4212,J03-1003,0,0.400354,"Missing"
W11-2808,W08-1108,0,0.25444,"put and output of these systems are a novel characterization of the factors that affect referring expression generation, and the methods described here may serve as one building block in future work connecting vision to language. 1 Introduction The task of referring expression generation (REG) has often been contextualized as a problem of generating uniquely identifying reference to visible items. Properties such as COLOR, SIZE, LOCATION, and ORIENTATION have been treated as exemplars of attributes used to distinguish a referent (Dale and Reiter, 1995; Krahmer et al., 2003; van Deemter, 2006; Gatt and Belz, 2008). This paper is no exception. However, we approach the task of REG by examining in depth what it means to uniquely identify something that is visible. We specifically address the attribute of size and explore ways to connect the dimensional properties of real-world objects to surface forms used by people to pick out a referent. This work contributes to recent research examining naturalistic reference in visual domains explicitly (Kelleher et al., 2005; Viethen and Dale, 2010; Koolen et al., 2011). Traditionally, to create an algorithm for the generation of reference, one considers a set of dif"
W11-2808,J03-1003,0,0.358267,"Missing"
W11-2808,W10-4210,1,0.891997,"Missing"
W11-2808,J06-2002,1,0.917165,"Missing"
W11-2808,U10-1013,0,0.1766,"as exemplars of attributes used to distinguish a referent (Dale and Reiter, 1995; Krahmer et al., 2003; van Deemter, 2006; Gatt and Belz, 2008). This paper is no exception. However, we approach the task of REG by examining in depth what it means to uniquely identify something that is visible. We specifically address the attribute of size and explore ways to connect the dimensional properties of real-world objects to surface forms used by people to pick out a referent. This work contributes to recent research examining naturalistic reference in visual domains explicitly (Kelleher et al., 2005; Viethen and Dale, 2010; Koolen et al., 2011). Traditionally, to create an algorithm for the generation of reference, one considers a set of different properties and develops ways to decide which properties to include in a final surface string. This may be considered a breadth-based methodology, where many properties are considered, but the details of how those properties are input to the algorithm is left unspecified. Here, we begin creating an algorithm for the generation of naturalistic reference by considering a single property – size – and tracing how it is realized based on a variety of different inputs and ou"
W11-2846,W10-4203,0,0.0672219,"Missing"
W11-2846,P10-2011,0,0.0563364,"Missing"
W11-2846,J95-3003,0,0.0734109,"ttons. 2.3 Referring Expressions A quick survey of the systems submitted to GIVE 2009 suggested to us that generation of referring expressions was generally a weak point. A good example is Denis (2009), which appears to rely on a strategy whereby the system indicates an underspecified referring expression (e.g., “(push) a red button”); if the user pushes the wrong button, the system proceeds to say that the wrong button was pushed, and another one needs to be attempted. While it is interesting to have a referential strategy that allows a degree of collaboration between speaker and hearer (cf. Heeman and Hirst 1995), this particular strategy seems error prone (particularly given the existence of alarmed buttons), and problematic in the presence of a large domain. (What if there are 10 red buttons, for example?) Our initial plan was to use the algorithm of van Deemter (2006), originally designed to generate vague descriptions such as “The tall man”. In a configuration of buttons on a wall, for example, this algorithm is able to identify any single button, by generating a sequence of gradable properties. Imagine a sequence of three buttons, for example, numbered 1,2,3 from left to right. Button 2 may be id"
W11-2846,J06-2002,1,0.893042,"Missing"
W12-1520,W10-1301,0,0.0308918,"ges 120–124, c Utica, May 2012. 2012 Association for Computational Linguistics (a) (b) Figure 1: Plot of (a) distance from nest as a function of time, and (b) clusters of visited locations. ically been used to generate summaries of technical data for professionals, such as engineers, nurses and oil rig workers. There is some work on the use of data-to-text for lay audiences; e.g., generating narratives from sensor data for automotive (Reddington et al., 2011) and environmental (Molina et al., 2011) applications, generating personal narratives to help children with complex communication needs (Black et al., 2010), and summarising neonatal intensive care data for parents (Mahamood et al., 2008). Our application differs from the above-mentioned data-to-text applications, in that we aim to generate inspiring as well as informative texts. It bears some resemblance to NLG systems that offer “infotainment”, such as Dial Your Disc (Van Deemter and Odijk, 1997) and Ilex (O’Donnell et al., 2001). In fact, Dial Your Disc, which generates spoken monologues about classical music, focused emphatically on generating engaging texts, and achieved linguistic variation through the use of recursive, syntactically struct"
W13-0212,bunt-etal-2010-towards,0,0.0308784,"den Acts, which do not correspond with actual Prot´eg´e actions, but which turn out to play an important role in the interaction, as evidenced by subjects’ spoken utterances. These include, most prominently, various types of goal handling, such as the adoption, revision, satisfaction, and abandonment of goals. At the dialogue level, these correspond to the expression of desires (when a goal is set aside for later) and intentions (when a goal is starting to be pursued). How does our annotation scheme compare to schemes for the analysis of natural language dialogue, such as the ISO-standard of (Bunt et al., 2010)? Similarities with our own scheme are easy to find: the distinction between information seeking and informative providing is an obvious example. Important differences exist as well. The ISO standard has been developed for annotating a wide class of dialogues. Our own scheme targets a specific kind of “asymmetric” dialogue, between a person and a machine. Goals can only be expressed by the person; what-if questions can only be posed by the person (the system responds, for example, with an Expressive Statement). Furthermore, the person cannot, at present, make Expressive Statements (given that"
W13-0212,W09-0602,0,0.0307978,"ion of a knowledge authoring dialogue compare to the annotation of real spoken dialogue? ∗ This work was supported by EPSRC grants EP/J014354/1 and EP/J014176/1. University of Aberdeen, UK ‡ University of Manchester, UK 1 http://protege.stanford.edu † Ontology authoring aided by CNL has been addressed from various points. ACE (Fuchs et al., 1999) and PENG (Schwitter, 2002) allow users to express specifications that can be translated into an unambiguous logical language, understandable to machines. CLCE (Sowa, 2004) resembles ACE but, being closer to English, its output is more readable. SWAT (Power, 2009) uses natural language generation to enable users to produce description logic statements. Similar to CLCE, (Bernardi et al., 2012)’s work is another example of a novel controlled language capable of assisting ontology authoring. Whereas this previous work has addressed the problem of producing isolated utterances relevant to knowledge authoring using CNL, this project attempts to further develop knowledge authoring by adopting new interactive methods inspired by human dialogue. For example, we hypothesise that by allowing an author to pose what-if questions prior to authoring an axiom in an o"
W13-2133,W03-1016,0,\N,Missing
W13-2133,W12-1527,1,\N,Missing
W15-0402,W13-0212,1,0.831337,"Missing"
W16-6605,P89-1009,0,0.66778,"ter’s Preference Order (Dale and Reiter, 1996), making NAMES the most highly preferred attribute in an Incremental Algorithm. Alternatively, a new type of brevity-based algorithm might be used that generates the RE that contains the smallest number of syllables.3 Assuming that PNs are brief (as they often are), this type of approach would favour PNs, and it would favour shorter PNs over longer ones (e.g., “Klein” over “Joe Klein”). It would also predict that PNs are avoided 3 Note that this approach would measure brevity as a surface property of a string, unlike the Full Brevity algorithm of (Dale, 1989), which sees brevity as a semantic property, letting REG choose the RE composed by the smallest number of properties. where large sets are enumerated (compare the RE “the citizens of China” with an enumeration of all the elements of this set). To see how REG could work in an Incremental Algorithm, consider a simple KB, where each individual has 1 name: T YPE: woman {w1 , w2 , w3 }, man {m1}, dog {d1 , d2 } NAMES: mary {w1 }, shona {w2 , w3 }, rover {d1 }, max {m1 , d2 } ACTION: feed {(w1 , d1 ), (w2 , d2 ), (w2 , d1 )} A FFECTION: love {(w1 , d1 ), (w3 , d1 )} This approach generates REs such"
W16-6605,W15-4723,0,0.112787,"Missing"
W16-6605,J12-1006,1,0.876192,"Missing"
W16-6605,H89-1033,0,0.212997,", REG would become trivial – so the presumed argument goes. We argue that this line of reasoning misses some important points and that PNs deserve more attention from researchers in REG. 2 Generating REs that contain a PN Observe that: – Name are often ambiguous. “Obama”, for instance (not to mention “Smith”) could refer to many different people. – A referent can have many names (“Barack”, “Obama”, “Barack Obama”, etc.) or none. – A name can combine with other properties and epithets, as in “Mr Barack Obama, America’s current president”. 2 An early exception is the ad hoc treatment of PNs in (Winograd, 1972)’s SRDLU; recently the possibility of a systematic treatment was suggested as part of (van Deemter, 2014); an exploratory experimental study is (de Oliveira et al., 2015). 31 Proceedings of The 9th International Natural Language Generation conference, pages 31–35, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics – A name can be part of an expression that refers to another referent. The process is recursive, e.g., “The height of the income of Obama’s Secretary of State”. So how might PNs be given a place in REG? 2.1 Incorporating Proper Names into REG Received"
W16-6618,P09-1011,0,0.0151773,"t al., 2003). However, manual coding is time consuming, it can be argued to be theoretically unsatisfactory, and it is error prone even when performed by domain experts. The process is complicated in the fact that words like pink (Roy, 2002) and evening (Reiter et al., 2005) have different meanings for individual speakers. Recent NLG approaches learn the use of words through statistical analysis of data-text corpora. For example, Belz’s semi-automatic system for weather forecasting automatically learns a grammar based on a pre-existing (i.e., manually coded) set of grammar rules (Belz, 2008). Liang et al. (2009) developed a fully statistical alignment-based algorithm that automatically acquires a mapping from quantitative information to English words by adopting a hierarchical hidden semi-Markov model trained by Expectation Maximization. Konstas and Lapata (2013) introduced a generation model based on Liang’s algo104 rithm . However, these existing approaches have difficulty handling situations in which a word expresses a combination of data dimensions, for example as when the word “mild¨expresses a combination of warm temperatures and low wind speed. In this paper, we discuss a new approach to the p"
W18-6506,W11-2817,0,0.615419,"troduce SimpleNLG-ZH, a realisation engine for Mandarin that follows the software design paradigm of SimpleNLG (Gatt and Reiter, 2009). We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of humanauthored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences. 1 To date, the original English SimpleNLG has been adapted to German (Bollmann, 2011), French (Vaudry and Lapalme, 2013), Portuguese (De Oliveira and Sripada, 2014), Italian (Mazzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation a"
W18-6506,W17-3532,1,0.893297,"Missing"
W18-6506,W09-0613,0,0.891579,"ge corpora. For example, OpenCCG (White et al., 2007) built a grammar bank based on Combinatorial Categorial Grammar, extracted from the Penn Treebank (Marcus et al., 1993). When realising, OpenCCG applies a chart-based algorithm to generate all possible surface forms, which are then re-ranked by language models. Such an approach tends to have broader coverage, but less controllability and extendibility, which may explain why SimpleNLG is more popular in practical applications. We introduce SimpleNLG-ZH, a realisation engine for Mandarin that follows the software design paradigm of SimpleNLG (Gatt and Reiter, 2009). We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of humanauthored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences. 1 To date, the original English SimpleNLG has been adapted to German (Bollmann, 2011), French (Vaudry and Lapalme, 2013), Portuguese (De Oliveira and Sripada, 2014), Italian (Mazzei et al., 2016), Spanish"
W18-6506,P09-1091,0,0.0175848,"azzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation and development, supports limited sentence structures in Mandarin (Yang and Bateman, 2009). He et al. (2009) introduced a data-driven generator, with dependency trees as input. They used divide-and-conquer to break the dependency tree into sub-trees, realising each sub-tree using a log-linear model recursively. However, their system needs a large amount of fully inflected dependency trees as training data. Introduction A classic natural language generation (NLG) system (Reiter and Dale, 2000) is a pipeline consisting of document planning, sentence planning and surface realisation (in that order). Surface realisation maps information produced by earlier components to well-formed output strings in the"
W18-6506,W17-3521,0,0.23212,"We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of humanauthored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences. 1 To date, the original English SimpleNLG has been adapted to German (Bollmann, 2011), French (Vaudry and Lapalme, 2013), Portuguese (De Oliveira and Sripada, 2014), Italian (Mazzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation and development, supports limited sentence structures in Mandarin (Yang and Bateman, 2009). He et al. (2009) introduced a data-driven genera"
W18-6506,W13-2125,0,0.66723,"ealisation engine for Mandarin that follows the software design paradigm of SimpleNLG (Gatt and Reiter, 2009). We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of humanauthored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences. 1 To date, the original English SimpleNLG has been adapted to German (Bollmann, 2011), French (Vaudry and Lapalme, 2013), Portuguese (De Oliveira and Sripada, 2014), Italian (Mazzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation and development, supports limited se"
W18-6506,A00-2023,0,0.239838,"Missing"
W18-6506,2007.mtsummit-ucnlg.4,0,0.073932,"troduction A classic natural language generation (NLG) system (Reiter and Dale, 2000) is a pipeline consisting of document planning, sentence planning and surface realisation (in that order). Surface realisation maps information produced by earlier components to well-formed output strings in the target language. A (surface) realiser employs languagespecific morpho-syntactic constraints to achieve proper word ordering, inflection, and selection of function words. Different types of realisers exist (Gatt and Krahmer, 2018). Unlike approaches that aim primarily for linguistic depth and coverage (White et al., 2007), realisers in the SimpleNLG tradition aim primarily for ease of use and extendibility (Gatt and Reiter, 2009), and have become the realisation method of choice in many practical NLG applications, such as BabyTalk (Portet et al., 2009) and Absum (Lapalme, 2013). SimpleNLG, as a human-crafted grammarbased realisation engine, performs linearisation This paper describes a realisation engine fol57 Proceedings of The 11th International Natural Language Generation Conference, pages 57–66, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics lowing the design"
W18-6506,W13-2110,0,0.0935991,"engine for Mandarin that follows the software design paradigm of SimpleNLG (Gatt and Reiter, 2009). We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of humanauthored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences. 1 To date, the original English SimpleNLG has been adapted to German (Bollmann, 2011), French (Vaudry and Lapalme, 2013), Portuguese (De Oliveira and Sripada, 2014), Italian (Mazzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation and development, supports limited se"
W18-6506,W17-0408,0,0.0148299,"identical verbatim to the original MTuna NPs. 35 noun phrases did not match completely (i.e., verbatim) with the original noun phrases. Table 2 lists some typical examples, showing differences in word ordering, punctuation, and so on. We ran a human evaluation to find out whether the realised sentences were acceptable (i.e., are they fluent and do they have the same meaning as their inputs). Two native speakLexicon Unlike SimpleNLG-EN, we did not have a readyto-use elaborate lexicon for SimpleNLG-ZH. Instead, we extracted a primary lexicon from the Chinese as a Foreign Language (CFL) corpus5 (Lee et al., 2017), which is a sub-corpus of the Universal Dependencies corpus. The CFL corpus has 451 human tagged dependency trees and 7,256 tokens in total. Each word in CFL was primarily mapped to one of the lexical categories in SimpleNLG-ZH based on the relations in Table 1 as well as the following rules: 1. The tag <proper/> is appended for PROPNs; 2. The tag <nonpredicate/> is appended for non-predicate adjectives manually, which is based on the non-predicate adjective list in Liu et al. (2001); 3. The tag <locative/> is appended for localisers manually; 4. The words that serve as a dependent of a clf ("
W18-6506,P09-1071,0,0.113794,"ripada, 2014), Italian (Mazzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation and development, supports limited sentence structures in Mandarin (Yang and Bateman, 2009). He et al. (2009) introduced a data-driven generator, with dependency trees as input. They used divide-and-conquer to break the dependency tree into sub-trees, realising each sub-tree using a log-linear model recursively. However, their system needs a large amount of fully inflected dependency trees as training data. Introduction A classic natural language generation (NLG) system (Reiter and Dale, 2000) is a pipeline consisting of document planning, sentence planning and surface realisation (in that order). Surface realisation maps information produced by earlier components to well-formed out"
W18-6506,J93-2004,0,0.0613347,"Missing"
W18-6506,W16-6630,0,0.669718,"leNLG (Gatt and Reiter, 2009). We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of humanauthored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences. 1 To date, the original English SimpleNLG has been adapted to German (Bollmann, 2011), French (Vaudry and Lapalme, 2013), Portuguese (De Oliveira and Sripada, 2014), Italian (Mazzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation and development, supports limited sentence structures in Mandarin (Yang and Bateman, 2009). He et al. (2009) in"
W18-6506,Y11-1006,0,0.0215302,"orphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of humanauthored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences. 1 To date, the original English SimpleNLG has been adapted to German (Bollmann, 2011), French (Vaudry and Lapalme, 2013), Portuguese (De Oliveira and Sripada, 2014), Italian (Mazzei et al., 2016), Spanish (Soto et al., 2017), Filipino (Ong et al., 2011) and Telugu (Dokkara et al., 2015). There is no such adaptation work yet for Sino-Tibetan languages, whose morphosyntactic structure is very different from the above languages. Mandarin, a Sino-Tibetan language with nearly 1 billion first-language speakers, offers huge opportunities for natural language generation, yet only a limited amount of work has focused on Mandarin realisation. KPML, a largescale multilingual generation and development, supports limited sentence structures in Mandarin (Yang and Bateman, 2009). He et al. (2009) introduced a data-driven generator, with dependency trees as"
W18-6519,P15-2053,0,0.0483819,"Missing"
W18-6519,P16-1074,0,0.0193803,"ution, and REG. When translating from a pro-drop language, recovering the dropped pronouns of the source language can improve the overall performance of MT (Wang et al., 2016, 159 Proceedings of The 11th International Natural Language Generation Conference, pages 159–164, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics 2018). Co-reference resolution of ZPs has been widely explored with a variety of techniques including the centring theory (Rao et al., 2015), statistical machine learning (Zhao and Ng, 2007; Chen and Ng, 2014, 2015), deep learning (Chen and Ng, 2016; Yin et al., 2016, 2017) and reinforcement learning (Yin et al., 2018). REG of ZPs for “cool” languages has been addressed through rule-based methods (Yeh and Mellish, 1997) including centring theory (Yamura-Takei et al., 2001) (for Japanese), but we are not aware of any testable computational account.1 We offer such an account, along probabilistic lines. Some discourse theories suggest that speakers choose referring expressions (REs) by considering discourse salience (Giv´on, 1983), i.e., speakers tend to choose pronouns if they believe the referent is highly salient. The intuition behind is"
W18-6519,J95-2003,0,0.905872,"Missing"
W18-6519,N06-2015,0,0.0607999,"at has previously been referred to and d is the number of sentences between two REs. Instead of taking the direct raw count 1, Count(ri , rj ) decays exponentially with respect to how far it is from the predicting RE. The RE that has larger distance contributes less to the overall count of that referent. For NZREs (z = 0), we assume that the number of times that the referent has been referred to is equal to the total number of referents referred to by that NZRE. Thus, the speaker believes that the listener can always resolve the reference by giving 2 Our use of the term salience is similar to Hovy et al. (2006)’s use of “recoverability”. 161 them a NZRE. In other words, their informativeness equals 1. 4 4.1 not know which referent (speaker or listener) a deictic pronoun in the corpus refers to. For example, in the corpus, both the speaker and listener will use “I” to refer to themselves, so we don’t know whether “I” refers to the speaker or the listener. This setting will lead to over-estimation of the informativeness of pronouns. On the other hand, computing cost by average length (as we do) overestimates the costs of pronouns, whose lengths are generally shorter than proper names. The baseline mod"
W18-6519,N16-1113,0,0.0181355,"pronouns (only) where they are appropriate is a difficult challenge for Referring Expression Generation (REG) (Van Deemter, (1) ∅ 有 二十三 项 高新技术 ∅ has 23 CLASSFIER high-tech 项目 进区 开发 projects in.the.zone under.development ‘there are 23 high-tech projects under development in the zone’ in which the ∅ cannot be recovered. 2 Related Work Pro-drop raises challenges for a number of NLP tasks including, machine translation (MT), coreference resolution, and REG. When translating from a pro-drop language, recovering the dropped pronouns of the source language can improve the overall performance of MT (Wang et al., 2016, 159 Proceedings of The 11th International Natural Language Generation Conference, pages 159–164, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics 2018). Co-reference resolution of ZPs has been widely explored with a variety of techniques including the centring theory (Rao et al., 2015), statistical machine learning (Zhao and Ng, 2007; Chen and Ng, 2014, 2015), deep learning (Chen and Ng, 2016; Yin et al., 2016, 2017) and reinforcement learning (Yin et al., 2018). REG of ZPs for “cool” languages has been addressed through rule-based methods (Yeh a"
W18-6519,J97-1007,0,0.68534,"ces, Utrecht University 2 Department of Computing Science, University of Aberdeen {g.chen, c.j.vandeemter}@uu.nl, chenghua.lin@abdn.ac.uk 1 Abstract 2016), and more specifically for the task of choosing referential form, a key step in the classic Natural Language Generation (NLG) architecture (Reiter and Dale, 2000). Traditionally, choosing referential form is framed as modelling speakers’ behaviour of deciding whether entities are referred to using a pronoun, a proper name, or a description. However, for “cool” languages, an extra option, namely of choosing a zero pronoun, needs to be added (Yeh and Mellish, 1997) for fully simulating speakers’ behaviour. In this paper, we model the use of zero pronouns in Chinese with the Rational Speech Acts (RSA) model (Frank and Goodman, 2012) by assuming that speakers tend to choose a ZP if it is salient enough for successful communication (see §2). For computing discourse salience, we focus on ZPs that are recoverable, meaning that they either refer anaphorically to an entity mentioned earlier in the text (i.e., anaphoric ZPs, or AZPs for short), or to the speaker or hearer (i.e., deictic nonanaphoric ZPs or DNZPs for short) (Zhao and Ng, 2007); a ZP is unrecover"
W18-6519,Q17-1023,0,0.0321807,"stener models (Bard et al., 2004), meaning the models that how speakers model listeners’ interpretation of the utterance, also have impact on language production. It suggests to us that the salience of the referent may not be enough for modelling speakers’ choice. The RSA model (see §3) used in this paper is possible to take all these factors into consideration. of tasks including modelling speakers’ referential choice between pronouns and proper names (Orita et al., 2015), the selection of attributes for referring expressions (Monroe and Potts, 2015), and the generation of colour references (Monroe et al., 2017, 2018). The key idea of RSA is to model human communication by assuming that a rational listener PL uses Bayesian inference to recover a speaker’s intended referent rs for word w under context C. In this way, RSA claims to offer not only accurate models, but highly explanatory ones as well. Formally, PL is defined as 3 We model the decision of whether to use a ZPbased on the formulation expressed in Eq. 3. The speaker model is PS (z|rs ), which is the probability that the speaker uses ZP (i.e., drops the RE). We assume that the speaker makes a binary choice (i.e., z = {1, 0}), with z = 1 indi"
W18-6519,N18-1196,0,0.0297417,"Missing"
W18-6519,D17-1135,0,0.0225031,"Missing"
W18-6519,P18-1053,0,0.0131514,"he dropped pronouns of the source language can improve the overall performance of MT (Wang et al., 2016, 159 Proceedings of The 11th International Natural Language Generation Conference, pages 159–164, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics 2018). Co-reference resolution of ZPs has been widely explored with a variety of techniques including the centring theory (Rao et al., 2015), statistical machine learning (Zhao and Ng, 2007; Chen and Ng, 2014, 2015), deep learning (Chen and Ng, 2016; Yin et al., 2016, 2017) and reinforcement learning (Yin et al., 2018). REG of ZPs for “cool” languages has been addressed through rule-based methods (Yeh and Mellish, 1997) including centring theory (Yamura-Takei et al., 2001) (for Japanese), but we are not aware of any testable computational account.1 We offer such an account, along probabilistic lines. Some discourse theories suggest that speakers choose referring expressions (REs) by considering discourse salience (Giv´on, 1983), i.e., speakers tend to choose pronouns if they believe the referent is highly salient. The intuition behind is that a highly salient referent tends to be highly prominent in the min"
W18-6519,P15-1158,0,0.0388431,"Missing"
W18-6519,D07-1057,0,0.245987,"s to be added (Yeh and Mellish, 1997) for fully simulating speakers’ behaviour. In this paper, we model the use of zero pronouns in Chinese with the Rational Speech Acts (RSA) model (Frank and Goodman, 2012) by assuming that speakers tend to choose a ZP if it is salient enough for successful communication (see §2). For computing discourse salience, we focus on ZPs that are recoverable, meaning that they either refer anaphorically to an entity mentioned earlier in the text (i.e., anaphoric ZPs, or AZPs for short), or to the speaker or hearer (i.e., deictic nonanaphoric ZPs or DNZPs for short) (Zhao and Ng, 2007); a ZP is unrecoverable if it cannot be linked to any referent, for example: We extend the classic Referring Expressions Generation task by considering zero pronouns in “pro-drop” languages such as Chinese, modelling their use by means of the Bayesian Rational Speech Acts model (Frank and Goodman, 2012). By assuming that highly salient referents are most likely to be referred to by zero pronouns (i.e., pro-drop is more likely for salient referents than the less salient ones), the model offers an attractive explanation of a phenomenon not previously addressed probabilistically. 1 Introduction L"
W18-6519,N15-1052,0,0.0343157,"Missing"
W18-6519,U03-1004,0,0.316936,"Missing"
W18-6548,W17-3535,1,0.890675,"Missing"
W18-6548,W17-3525,0,0.0277796,"n, 1996) of “Overview first, zoom and filter, then details-ondemand”. One of the main ideas presented there is that it is beneficial for a reader to be exposed to an overview of the information before diving into specific details of interest. There have been related NLG research about sets of objects, although with different goals or focuses. For example, to refer to or identify a set of objects within a larger set (Van Deemter, 2002), to perform a data-to-text analysis of tabularized data by records1 , to generate a page title for set items with shared characteristics from existing metadata (Mathur et al., 2017), or to address the issue of missing data encountered in summarisation (Inglis et al., 2017). In contrast, our work explores summaries that describe commonalities and differences within a set in order to help a user make informed decisions in selecting an object from the set. Our work focuses particularly on Content Determination step in the NLG pipe-line (Reiter and Dale, 2000), including selecting features and values to be presented. We explored the task of creating a textual summary describing a large set of objects characterised by a small number of features using an e-commerce dataset. Wh"
W18-6548,J02-1003,1,0.573446,"Missing"
W18-6551,W08-1104,1,0.845298,"Missing"
W18-6551,W17-5525,0,0.0638373,"Missing"
W18-6551,W15-4723,1,0.903448,"Missing"
W18-6551,W07-2315,1,0.567353,"of 7 descriptors (including cardinal points, coast, inland, and a proper name). Figure 2 shows a representation of the answers given by the students for “Northern Galicia” and a contour map that illustrates the percentages of overlapping answers. The second survey was addressed to meteorologists in the Galician Weather Agency (MeteoGalicia, 2018). Its purpose was to gather data to create Language grounding, i.e., understanding how words and expressions are anchored in data, is one of the initial tasks that are essential for the conception of a data-to-text (D2T) system (Roy and Reiter, 2005; Reiter, 2007). This can be achieved through different means, such as using heuristics or machine learning algorithms on an available parallel corpora of text and data (Novikova et al., 2017) to obtain a mapping between the expressions of interest and the underlying data (Reiter et al., 2005), getting experts to provide these mappings, or running surveys on writers or readers that provide enough data for the application of mapping algorithms (Ramos-Soto et al., 2017). Performing language grounding ensures that generated texts include words whose meaning is aligned with what writers understand or what reader"
W18-6551,W02-2113,1,0.63836,"Missing"
W18-6551,W09-0607,1,\N,Missing
W18-6561,P89-1009,0,0.19655,"expression of these attributes in words (e.g., “the red sofa”) (Krahmer and Van Deemter, 2012). The term REG is sometimes restricted to “one-shot” references, where it is the task of one single NP to identify the referent. We will use the term REG in this restricted sense. Consequently, linguistic context is irrelevant, so pronouns and other anaphoric NPs (as in the GREC challenge, for instance (Gatt et al., 2009)) will not be taken into account. Early REG approaches sought to find a minimum set of attributes that jointly single out the referent, this is called the Full Brevity (FB) approach (Dale, 1989), or to “greedily” add maximally discriminatory attributes one by one, that is, adding attributes one at a time, choosing always the one that removes the largest number of distractors, until the referent has been uniquely identified; this is 482 Proceedings of The 11th International Natural Language Generation Conference, pages 482–491, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics called the Greedy algorithm (GR). The approach that is often thought to be most suitable for relatively simple referential situations (cf. §6 below), known as the Inc"
W18-6561,W08-1133,0,0.0911086,"Missing"
W18-6561,D13-1197,0,0.0298143,"Missing"
W18-6561,W09-0629,0,0.0368276,"sk. Related Work The main REG algorithms have often focussed on the semantic core of the REG task, which is to select semantic attributes for a referring expression (e.g., sofa, red), disregarding the expression of these attributes in words (e.g., “the red sofa”) (Krahmer and Van Deemter, 2012). The term REG is sometimes restricted to “one-shot” references, where it is the task of one single NP to identify the referent. We will use the term REG in this restricted sense. Consequently, linguistic context is irrelevant, so pronouns and other anaphoric NPs (as in the GREC challenge, for instance (Gatt et al., 2009)) will not be taken into account. Early REG approaches sought to find a minimum set of attributes that jointly single out the referent, this is called the Full Brevity (FB) approach (Dale, 1989), or to “greedily” add maximally discriminatory attributes one by one, that is, adding attributes one at a time, choosing always the one that removes the largest number of distractors, until the referent has been uniquely identified; this is 482 Proceedings of The 11th International Natural Language Generation Conference, pages 482–491, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association fo"
W18-6561,W07-2307,1,0.78886,"Missing"
W18-6561,D14-1086,0,0.153347,"in two conditions: the participants were either allowed to use location descriptions (e.g. “in the top left”) or not (van der Sluis et al., 2006). Here we focus on those corpus records which have one singular target object (rather than a set of two) and where the locational descriptions were not allowed to use. There are 210 corpus records in Furniture domain, and 180 records in People domain. We also discarded the records in which the locational descriptions were still used. Our evaluation assumes a type of Knowledge Representation based on an attribute-value schema. Corpora such as ReferIt (Kazemzadeh et al., 2014), which do not use this type of KR, are not directly amenable to this approach. For each placeholder class, we first build up a small training corpus that consists of all the words (or multi-word phrases) corresponding to the placeholder, with the features that the word (or the multi-word phrase) expresses. Looking through the Tuna corpus, from the expressions, for example, “the red sofa” and “the blue chair”, we find that the placeholder [colour] can be described by words “red” and “blue”. So the extracted small corpus for [colour] contains the two words, which is shown in Table 3 Table 3: Th"
W18-6561,D15-1199,0,0.0838958,"Missing"
W18-6561,J12-1006,1,0.928877,"Missing"
W18-6561,W16-6618,1,0.872135,"Missing"
W18-6561,J07-2004,1,0.818767,"Missing"
W18-6561,W07-2315,0,0.0130439,"re Noun Phrases produced by a new set of human speakers. 1 Introduction 2 Referring Expressions Generation (REG) is a subtask of Natural Language Generation (NLG) that decides how to distinguish a target referent from its distractors, as when we say “the sofa”, “the red sofa”, and so on, to distinguish the referent from other furniture. Most current REG algorithms are rule-based (Gatt and Krahmer, 2018), though Machine Learning is also starting to be used (e.g. Di Fabbrizio et al., 2008). REG is usually treated as an independent stage or component of NLG pipelines (e.g. Reiter and Dale, 2000; Reiter, 2007; Gatt and Krahmer, 2018; Di Fabbrizio et al., 2008). The present paper changes the relationship between NLG and REG: it regards REG as a special case of usual NLG, and proposes a vector-based algorithm to transform REG tasks into a generic NLG tasks. The paper adopts the NLG algorithm of our previous work (which is also vector-based; Li, 2019), but certain adaptations needed to be made to allow the algorithm to perform the traditional REG attribute selection task. Related Work The main REG algorithms have often focussed on the semantic core of the REG task, which is to select semantic attribu"
W19-8616,W13-2120,1,0.56448,"Missing"
W19-8616,W19-8667,1,0.640807,"Missing"
W19-8616,W07-2307,1,0.877974,"Missing"
W19-8616,S13-1001,0,0.57285,"Sanford, 1993), and work that investigates the links between quantifiers’ logical types and human processing of quantified expressions (Szymanik and Zajenkowski, 2010; Szymanik et al., 2016, QEs). Yet there is a dearth of knowledge about human usage of QEs. For instance, what QEs, and what combinations of QEs, are uttered by a speaker in a given situation, to accomplish a given task? And, if a given NP is uttered, what information does it convey? Some questions are starting to be addressed, for example, Sorodoc et al. (2016) looked at speakers’ choice between “all”, “some”, and “no” (see also Grefenstette (2013) and Herbelot and Vecchi (2015)). Yildirim et al. (2013) studied speakers’ use (and hearers’ interpretation) of the quantifiers “some” and “many”, as in “Many of the candies are green”. Barr et al. (2013) investigated referring expressions in which a quantifier is embedded (e.g., “the square with 11 black dots”, “the square with lots of dark dashes”). Yet there has been few attempts to chart how the wider class of generalised quantifiers are used by human speakers. The present paper lies the basis for such a study, with the ultimate aim of modelling the human production of quantifiers computat"
W19-8616,D15-1003,0,0.203672,"that investigates the links between quantifiers’ logical types and human processing of quantified expressions (Szymanik and Zajenkowski, 2010; Szymanik et al., 2016, QEs). Yet there is a dearth of knowledge about human usage of QEs. For instance, what QEs, and what combinations of QEs, are uttered by a speaker in a given situation, to accomplish a given task? And, if a given NP is uttered, what information does it convey? Some questions are starting to be addressed, for example, Sorodoc et al. (2016) looked at speakers’ choice between “all”, “some”, and “no” (see also Grefenstette (2013) and Herbelot and Vecchi (2015)). Yildirim et al. (2013) studied speakers’ use (and hearers’ interpretation) of the quantifiers “some” and “many”, as in “Many of the candies are green”. Barr et al. (2013) investigated referring expressions in which a quantifier is embedded (e.g., “the square with 11 black dots”, “the square with lots of dark dashes”). Yet there has been few attempts to chart how the wider class of generalised quantifiers are used by human speakers. The present paper lies the basis for such a study, with the ultimate aim of modelling the human production of quantifiers computationally. In the computational m"
W19-8616,J12-1006,1,0.904593,"Missing"
W19-8616,W16-3211,0,0.565402,"uk Abstract there is psycholinguistic work on people’s use of vague quantifiers (Moxey and Sanford, 1993), and work that investigates the links between quantifiers’ logical types and human processing of quantified expressions (Szymanik and Zajenkowski, 2010; Szymanik et al., 2016, QEs). Yet there is a dearth of knowledge about human usage of QEs. For instance, what QEs, and what combinations of QEs, are uttered by a speaker in a given situation, to accomplish a given task? And, if a given NP is uttered, what information does it convey? Some questions are starting to be addressed, for example, Sorodoc et al. (2016) looked at speakers’ choice between “all”, “some”, and “no” (see also Grefenstette (2013) and Herbelot and Vecchi (2015)). Yildirim et al. (2013) studied speakers’ use (and hearers’ interpretation) of the quantifiers “some” and “many”, as in “Many of the candies are green”. Barr et al. (2013) investigated referring expressions in which a quantifier is embedded (e.g., “the square with 11 black dots”, “the square with lots of dark dashes”). Yet there has been few attempts to chart how the wider class of generalised quantifiers are used by human speakers. The present paper lies the basis for such"
W19-8667,W13-2120,1,0.565502,"Missing"
W19-8667,W07-2307,1,0.898725,"Missing"
W19-8667,W19-5301,0,0.0544635,"Missing"
W19-8667,S13-1001,0,0.839736,"(1981) and further elaborated in Keenan and Moss (1985); Van Benthem et al. (1986); Peters et al. (2006). Though the main focus of this work has been on mathematical logic, there is a growing body of empirical work. For example, there is some psycholinguistic work on the human use of vague quantifiers (Moxey and Sanford, 1993), and work that investigates the links between quantifiers’ logical types and the human processing of quantified expressions (Szymanik and Zajenkowski, 2010; Szymanik et al., 2016, QEs). Sorodoc et al. (2016) looked at speakers’ choice between all, some, and no (see also Grefenstette (2013) and Herbelot and Vecchi (2015)). However, there has been no attempt to find out how the wider class of all (“generalised”) quantifiers are used by human speakers. In a simple version of the problem, consider a table with two black tea cups and four coffee cups, three of which are red while the remaining one is white. Each of (a)-(d) describes the scene truthfully (though not necessarily optimally): Quantified expressions have always taken up a central position in formal theories of meaning and language use. Yet quantified expressions have so far attracted far less attention from the Natural L"
W19-8667,W06-2920,0,0.0875052,"conditions (e.g., the TUNA corpus (Gatt et al., 2007; van Deemter et al., 2012); such corpora were used in evaluation campaigns where computer-generated referring expressions were compared with the corpus gold standard (Gatt and Belz, 2010). This comparison allowed researchers to know which algorithms worked best, and to develop new algorithms that match human language production even more closely. These evaluation campaigns created a tradition of NLG “generation challenges”, consistent with the broader tradition of evaluation campaigns in Machine Translation (Barrault et al., 2019), parsing (Buchholz and Marsi, 2006), and other areas of Computational Linguistics. Focusing on a far wider class of NPs, a series of elicitation experiments was recently conducted, called here the QTUNA experiments (Q stands for quantification). We will summarise the main findings from these experiments before proposing and experimentally comparing algorithms that seek to mimic the aforementioned corpus. More details can be found in Chen et al. (2019) 1 . The QTUNA experiment was set up in order to study how speakers use sequences of QEs to describe a visual scene. Participants were asked to describe a series of abstract visual"
W19-8667,D15-1003,0,0.546624,"rated in Keenan and Moss (1985); Van Benthem et al. (1986); Peters et al. (2006). Though the main focus of this work has been on mathematical logic, there is a growing body of empirical work. For example, there is some psycholinguistic work on the human use of vague quantifiers (Moxey and Sanford, 1993), and work that investigates the links between quantifiers’ logical types and the human processing of quantified expressions (Szymanik and Zajenkowski, 2010; Szymanik et al., 2016, QEs). Sorodoc et al. (2016) looked at speakers’ choice between all, some, and no (see also Grefenstette (2013) and Herbelot and Vecchi (2015)). However, there has been no attempt to find out how the wider class of all (“generalised”) quantifiers are used by human speakers. In a simple version of the problem, consider a table with two black tea cups and four coffee cups, three of which are red while the remaining one is white. Each of (a)-(d) describes the scene truthfully (though not necessarily optimally): Quantified expressions have always taken up a central position in formal theories of meaning and language use. Yet quantified expressions have so far attracted far less attention from the Natural Language Generation community th"
W19-8667,W19-8616,1,0.640807,"Missing"
W19-8667,J12-1006,1,0.917901,"Missing"
W19-8667,W96-0413,0,0.416688,"show up some surprising commonalities between the task of the present paper – Quantified Description Generation (QDG) – and the Referring Expressions Generation (REG). Section 4 details our evaluation experiment and section 5 discusses implications and future work. Code for the proposed QDG algorithms can be found at: https://github.com/a-quei/ quantified-description-generation. 2 Background An early investigation of the choice between patterns (a)-(d) above rested on the idea that the generator should always choose the logically strongest statement that holds true in the situation described (Creaney, 1996). Thus, statement (b) was always preferred over (a). However, this attractive idea ran into difficulties over pairs of statements that are logically independent of each other, such as (b) and (c), or also (b) and (d), where each of the two statements conveys some information that the other one does not. Clearly, other computational mechanisms are called for. The present paper proposes and evaluates a family of such mechanisms. A body of work that is indirectly relevant concerns the generation of referring NPs (Krahmer and van Deemter, 2012; van Deemter, 2016). One strand of this work focusses"
W19-8667,W18-6561,1,0.893665,"Missing"
W19-8667,W09-0609,0,0.0238786,"s in each case: (e.g., red, blue, circle, square). Furthermore, our algorithms have had to find a way to take both the semantics and the pragmatics of a quantifier pattern into account. Finally, the QTUNA experiments have taught us that, except in very small domains (such as the QTUNA one where n=4), vague quantifiers (such as many, most) are highly frequent, and hence need to be taken into account by any QDG algorithm. By contrast, REG has been able to disregard vague properties for a long time, because in domains consisting of simple objects (such as the TUNA furniture domains, and those of Dale and Viethen (2009), where all properties were crisp and well defined,) they did not play an important role. A related difference is that, for larger domains, it is typically not feasible to produce a description that identifies the target scene completely. This different in REG, where except in very complicated situations (e.g., van Deemter (2016, Chapters 11-15)), producing a distinguishing description (i.e., one that singles out its referent) is par for the course. 4 • Q1 (Naturalness): On a scale of 1-5, how likely do you think it might be that this description was uttered by a human? [1=very unlikely, 5=ver"
W19-8667,W16-3211,0,0.682178,"); the link with natural language was more fully established with Barwise and Cooper (1981) and further elaborated in Keenan and Moss (1985); Van Benthem et al. (1986); Peters et al. (2006). Though the main focus of this work has been on mathematical logic, there is a growing body of empirical work. For example, there is some psycholinguistic work on the human use of vague quantifiers (Moxey and Sanford, 1993), and work that investigates the links between quantifiers’ logical types and the human processing of quantified expressions (Szymanik and Zajenkowski, 2010; Szymanik et al., 2016, QEs). Sorodoc et al. (2016) looked at speakers’ choice between all, some, and no (see also Grefenstette (2013) and Herbelot and Vecchi (2015)). However, there has been no attempt to find out how the wider class of all (“generalised”) quantifiers are used by human speakers. In a simple version of the problem, consider a table with two black tea cups and four coffee cups, three of which are red while the remaining one is white. Each of (a)-(d) describes the scene truthfully (though not necessarily optimally): Quantified expressions have always taken up a central position in formal theories of meaning and language use. Yet"
W97-0610,C92-3128,0,0.0314132,"d as an actual sentence. If not, a different candidate sentence is subjected to examination, etc. We will see that very similar rules, which are also based on the information in the Discourse Model, are used to determine which words in binary trees, in which one daughter of each node is marked as &apos;strong&apos; and the other as &apos;weak&apos;. Metrical structure determines which leaves of the tree are most suitable to carry an accent on syntactic grounds. Roughly, these are the leaves that can be reached through a path that starts from an expression that is &apos;in focus&apos;, and that does not contain weak nodes (Dirksen, 1992). More exactly, if a given major phrase is &apos;in focus&apos;, it is also marked as accented, and so is each strong node that is the daughter of a node that is marked as accented. Accent is realized on those leaves that are marked as accented. However, several obstacles may prevent this from happening. For example, the sentence are to be accented. 4 Prosody and speech Generating acceptable speech requires syntactic and semantic information that is hard to extract from unazmotated text. In the present setting, however, speech generation is helped by the availability of syntactic and semantic informatio"
W97-0610,J86-3001,0,\N,Missing
W97-0610,C90-2017,0,\N,Missing
W98-0608,P98-2173,1,0.868426,"Missing"
