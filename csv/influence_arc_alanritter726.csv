2020.acl-main.19,D17-1219,0,0.0339127,"Missing"
2020.acl-main.19,P18-1177,0,0.0299992,"Missing"
2020.acl-main.19,P17-1123,0,0.0374965,"Missing"
2020.acl-main.19,D18-1134,0,0.0140061,"pairwise ranking by maximizing the multiclass margin between correct and incorrect answers (Joachims, 2002; Burges et al., 2005; K¨oppel et al., 2019). This is an important distinction from TREC-style answer selection as our ST-generated candidate responses have lower semantic, syntactic, and lexical variance, making pointwise methods less effective. Question Answering Using crowd-sourcing methods to create QA datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Rajpurkar et al., 2018), conversational datasets (Dinan et al., 2018), and ConvQA datasets (Choi et al., 2018; Reddy et al., 2019; Elgohary et al., 2018; Saha et al., 2018) has largely driven recent methodological advances. However, models trained on these ConvQA datasets typically select exact answer spans instead of generating them (Yatskar, 2019b). Instead of creating another crowd-sourced dataset for our task, we augment existing QA datasets to include such conversational answer responses using the STs + BERT trained with softmax loss. Related Work Question Generation (QG) is a well studied problem in the NLP community with many machine learning based solutions (Rus et al., 2010; Heilman 198 5 Conclusion In this work, we study the problem"
2020.acl-main.19,N18-1017,0,0.0260451,"he times (option e). The weighted average Cohen’s kappa (Cohen, 1960) score for all annotators in this evaluation is 0.750 (substantial agreement). This result demonstrates ability of our model to generalize over different domains and generate good conversational responses for questions when provided with correct answer spans. 4 and Smith, 2010; Yao et al., 2012; Labutov et al., 2015; Serban et al., 2016; Reddy et al., 2017; Du et al., 2017; Du and Cardie, 2017, 2018). In comparison, our work explores the opposite direction, i.e. (generating conversational humanlike answers given a question). Fu and Feng (2018) also try to solve fluent answer response generation task but in a restricted setting of movie related questions with 115 question patterns. In contrast, our generation models can deal with human generated questions from any domain. Learning to Rank formulations for answer selection in QA systems is common practice, most frequently relying on pointwise ranking models (Severyn and Moschitti, 2015; Garg et al., 2019). Our use of discriminative re-ranking (Collins and Koo, 2005) with softmax loss is closer to learning a pairwise ranking by maximizing the multiclass margin between correct and inco"
2020.acl-main.19,W09-0613,0,0.0311663,"e answer responses for QA pairs using STs in §2.1. We then rank these responses from best to worst using the response classification models described in §2.2. Later in §3, we describe how we augment existing QA datasets with fluent answer responses using STs and a best response classifier. This augmented QA dataset is used for training the PGN and Transformer models. response. The remaining questions are processed via the following transformations to over-generate a list of candidate answers: (1) Verb modification: change the tense of the main verb based on the auxiliary verb using SimpleNLG (Gatt and Reiter, 2009); (2) Pronoun replacement: substitute the noun phrase with pronouns from a fixed list; (3) Fixing Preposition and Determiner: find the preposition and determiner in the question’s parse tree that connects to the answer phrase and add all possible prepositions and determiners if missing. (4) Response Generation: Using Tregex and Tsurgeon (Levy and Andrew, 2006), compile responses by combining components of all previous steps and the answer phrase. In cases where there are multiple options in steps (2) and (3), the number of options can explode and we use the best response classifier (described"
2020.acl-main.19,N10-1086,0,0.202512,"uses this pipeline to augment training data for training a S EQ 2S EQ networks PGN or D-GPT (§3.1). The final S EQ 2S EQ model is end-to-end, scalable, easier to train, and performs better than the first method exclusively. strate that our proposed NLG models are capable of generating fluent, abstractive answers on both SQuAD 2.0 and CoQA. In this section, we describe our approach for constructing a corpus of questions and answers that supports fluent answer generation (top half of Figure 1). We use the framework of overgenerate and rank previously used in the context of question generation (Heilman and Smith, 2010). We first overgenerate answer responses for QA pairs using STs in §2.1. We then rank these responses from best to worst using the response classification models described in §2.2. Later in §3, we describe how we augment existing QA datasets with fluent answer responses using STs and a best response classifier. This augmented QA dataset is used for training the PGN and Transformer models. response. The remaining questions are processed via the following transformations to over-generate a list of candidate answers: (1) Verb modification: change the tense of the main verb based on the auxiliary"
2020.acl-main.19,D16-1244,0,0.105118,"Missing"
2020.acl-main.19,D14-1162,0,0.0943998,"is the question and the answer phrase hq, ai and the response r is the corresponding generation target. PGN: PGNs are widely used S EQ 2S EQ models equipped with a copy-attention mechanism capable of copying any word from the input directly into the generated output, making them well equipped to handle rare words and named entities present in questions and answer phrases. We train a 2-layer stacked bi-LSTM PGN using the OpenNMT toolkit (Klein et al., 2017) on the SS and SS+ data. We additionally explore PGNs with pre-training information by initializing the embedding layer with GloVe vectors (Pennington et al., 2014) and pretraining it with hq, ri pairs from the questions-only subset of the OpenSubtitles corpus9 (Tiedemann, 2009). This corpus contains about 14M questionresponse pairs in the training set and 10K pairs in the validation set. We name the pre-trained PGN model as PGN-Pre. We also fine-tune PGN-Pre on the SS and SS+ data to generate two additional variants. D-GPT: DialoGPT (i.e. dialogue generative pretrained transformer) (Zhang et al., 2019) is a recently released large tunable automatic conversation model trained on 147M Reddit conversationlike exchanges using the GPT-2 model architecture (R"
2020.acl-main.19,P06-1063,0,0.104083,"Missing"
2020.acl-main.19,P18-2124,0,0.197009,"rpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a S EQ 2S EQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model’s scalability by conducting tests on the CoQA dataset.1 1 Introduction Factoid question answering (QA) has recently enjoyed rapid progress due to the increased availability of large crowdsourced datasets (e.g., SQuAD (Rajpurkar et al., 2016), MS MARCO (Bajaj et al., 2016), Natural Questions (Kwiatkowski et al., 2019)) for training neural models and the significant advances in pre-training contextualized representations using massive text corpora (e.g."
2020.acl-main.19,D16-1264,0,0.340733,"ecifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model’s scalability by conducting tests on the CoQA dataset.1 1 Introduction Factoid question answering (QA) has recently enjoyed rapid progress due to the increased availability of large crowdsourced datasets (e.g., SQuAD (Rajpurkar et al., 2016), MS MARCO (Bajaj et al., 2016), Natural Questions (Kwiatkowski et al., 2019)) for training neural models and the significant advances in pre-training contextualized representations using massive text corpora (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019)). Building on these successes, recent work examines conversational QA (ConvQA) systems capable of interacting with users over multiple turns. 1 The code and data are available https://github.com/abaheti95/QADialogSystem. at Kevin Small Amazon Alexa smakevin@amazon.com Large crowdsourced ConvQA datasets (e.g., CoQA (Reddy et al."
2020.acl-main.19,E17-1036,0,0.0604615,"Missing"
2020.acl-main.19,Q19-1016,0,0.192377,"t al., 2016), MS MARCO (Bajaj et al., 2016), Natural Questions (Kwiatkowski et al., 2019)) for training neural models and the significant advances in pre-training contextualized representations using massive text corpora (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019)). Building on these successes, recent work examines conversational QA (ConvQA) systems capable of interacting with users over multiple turns. 1 The code and data are available https://github.com/abaheti95/QADialogSystem. at Kevin Small Amazon Alexa smakevin@amazon.com Large crowdsourced ConvQA datasets (e.g., CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018)) consist of dialogues between crowd workers who are prompted to ask and answer a sequence of questions regarding a source document. Although these ConvQA datasets support multi-turn QA interactions, the responses have mostly been limited to extracting text spans from the source document and do not readily support abstractive answers (Yatskar, 2019a). While responses copied directly from a Wikipedia article can provide a correct answer to a user question, they do not sound natural in a conversational setting. To address this challenge, we develop S EQ 2S EQ models tha"
2020.acl-main.19,W10-4234,0,0.0910218,"Missing"
2020.acl-main.19,P17-1099,0,0.0317918,"rmations (STs). These STs over-generate a large set of candidate responses from which a BERT-based classifier selects the best response as shown in the top half of Figure 1. While over-generation and selection generates fluent responses in many cases, the brittleness of the off-the-shelf parsers and the syntatic transformation rules prevent direct use in cases that are not well-covered. To mitigate this limitation, we generate a new augmented training dataset using the best response classifier that is used to train end-toend response generation models based on PointerGenerator Networks (PGN) (See et al., 2017) and pre-trained Transformers using large amounts of dialogue data, DialoGPT (D-GPT) (Zhang et al., 2019). In §3.2 and §3.3, we empirically demon191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 191–207 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Over-generate and select the best response: r1: he failed to egypt to pull off a bloodless coup q: where did Hizb utparser + r2: they failed to pull off a bloodless coup in Tahrir fail to pull off a 1974 Egypt syntactic bloodless coup in … rules 1974 ? a: egypt rm: he failed to p"
2020.acl-main.19,N19-1241,0,0.0723429,"nteracting with users over multiple turns. 1 The code and data are available https://github.com/abaheti95/QADialogSystem. at Kevin Small Amazon Alexa smakevin@amazon.com Large crowdsourced ConvQA datasets (e.g., CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018)) consist of dialogues between crowd workers who are prompted to ask and answer a sequence of questions regarding a source document. Although these ConvQA datasets support multi-turn QA interactions, the responses have mostly been limited to extracting text spans from the source document and do not readily support abstractive answers (Yatskar, 2019a). While responses copied directly from a Wikipedia article can provide a correct answer to a user question, they do not sound natural in a conversational setting. To address this challenge, we develop S EQ 2S EQ models that generate fluent and informative answer responses to conversational questions. To obtain data needed to train these models, rather than constructing yet-another crowdsourced QA dataset, we transform the answers from an existing QA dataset into fluent responses via data augmentation. Specifically, we synthetically generate supervised training data by converting questions an"
2020.acl-main.19,levy-andrew-2006-tregex,0,\N,Missing
2020.acl-main.19,J05-1003,0,\N,Missing
2020.acl-main.19,U06-1019,0,\N,Missing
2020.acl-main.19,P15-1086,0,\N,Missing
2020.acl-main.19,P17-4012,0,\N,Missing
2020.acl-main.19,W18-2501,0,\N,Missing
2020.acl-main.19,D18-1241,0,\N,Missing
2020.acl-main.19,Q19-1026,0,\N,Missing
2020.acl-main.19,N19-1423,0,\N,Missing
2020.acl-main.443,P18-1009,0,0.0186997,"for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity types. We also develop a novel attention based neural NER model to ext"
2020.acl-main.443,W04-1213,0,0.0311238,"nnotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow"
2020.acl-main.443,P11-2008,0,0.191018,"Missing"
2020.acl-main.443,L18-1708,0,0.0384899,"rior programming knowledge can easily recognize that ‘list()’ is code, ‘list’ can be either code or a common English word, whereas ‘listing’ is more likely a non-code natural language token. We thus introduce a code recognition module to capture such prior probability of how 4 https://archive.org/details/ stackexchange likely a word can be a code token without considering any contextual information. It is worth noting that this standalone code recognition model is also useful for language-and-code research, such as retrieving code snippets based on natural language queries (Iyer et al., 2016; Giorgi and Bader, 2018; Yao et al., 2019) Our code recognition model (Code Recognizer) is a binary classifier. It utilizes language model features and spelling patterns to predict whether a word is a code entity. The input features include unigram word and 6-gram character probabilities from two language models (LMs) that are trained on the Gigaword corpus (Napoles et al., 2012) and all the code-snippets in the StackOverflow 10-year archive respectively. We also pre-trained FastText (Joulin et al., 2016) word embeddings using these code-snippets, where a word vector is represented as a sum of its character ngrams."
2020.acl-main.443,D18-1306,0,0.0181373,"from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity t"
2020.acl-main.443,P16-1195,0,0.0663524,"sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and techniques for identif"
2020.acl-main.443,N18-2016,1,0.919977,"pecific auxiliary tasks. 4917 P Test set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) Dev set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) R F1 71.77 73.03 78.22 77.02 68.77 78.42 39.70 64.82 78.59 45.92 67.47 79.79 51.12 68.68 78.41 57.54 68.12 79.10 66.85 74.44 79.43 79.57 72.11 78.81 46.19 68.71 80.00 46.42 70.51 81.72 54.64 71.46 79.72 58.64 71.30 80.24 models in various domains (Lample et al., 2016; Kulkarni et al., 2018; Dai et al., 2019). • An Attentive BiLSTM-CRF model with in-domain ELMo embeddings as well as domain-specific embeddings from the code recognizer and the entity segmenter. This model combines these three word embeddings using an attention network and then utilizes a BiLSTM-CRF layer to predict the entity type of each input word (details in Appendix B). Table 2: Evaluation on the dev and test sets of the StackOverflow NER corpus. Our SoftNER model outperforms the existing approaches. 4.1 Data We train and evaluate our SoftNER model on the StackOverflow NER corpus of 9,352 train, 2,942 developm"
2020.acl-main.443,N16-1030,0,0.462484,"ings and two domain-specific auxiliary tasks. 4917 P Test set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) Dev set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) R F1 71.77 73.03 78.22 77.02 68.77 78.42 39.70 64.82 78.59 45.92 67.47 79.79 51.12 68.68 78.41 57.54 68.12 79.10 66.85 74.44 79.43 79.57 72.11 78.81 46.19 68.71 80.00 46.42 70.51 81.72 54.64 71.46 79.72 58.64 71.30 80.24 models in various domains (Lample et al., 2016; Kulkarni et al., 2018; Dai et al., 2019). • An Attentive BiLSTM-CRF model with in-domain ELMo embeddings as well as domain-specific embeddings from the code recognizer and the entity segmenter. This model combines these three word embeddings using an attention network and then utilizes a BiLSTM-CRF layer to predict the entity type of each input word (details in Appendix B). Table 2: Evaluation on the dev and test sets of the StackOverflow NER corpus. Our SoftNER model outperforms the existing approaches. 4.1 Data We train and evaluate our SoftNER model on the StackOverflow NER corpus of 9,35"
2020.acl-main.443,W16-3920,0,0.0149243,"s a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain"
2020.acl-main.443,D17-1086,0,0.0177935,"2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity"
2020.acl-main.443,P16-1101,0,0.0382171,"chieves sufficient performance to be useful for applications on GitHub.7 We leave investigation of semi-supervised learning and other domain adaptation approaches for future work. 7 As a reference, the state-of-the-art performance for 10class Twitter NER is 70.69 F1 (Zhang et al., 2018). 4920 6 Related Work The CoNLL 2003 dataset (Sang and De Meulder, 2003) is a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball"
2020.acl-main.443,D14-1162,0,0.0948039,"code recognition and entity segmentation. The input to the SoftNER model include 850-dimensional vectors extracted from both the code recognizer and the entity segmenter. We pre-trained BERTbase , ELMo and GloVe vectors on 152 million sentences from the StackOverflow, excluding sentences from the 1,237 posts in our annotated corpus. The pretraining of the 768-dimensional BERTbase model with 64,000 WordPiece vocabulary took 7 days on a Google TPU. The pre-training of 1024dimensional ELMo vectors took 46 days on 3 NVIDIA Titan X Pascal GPUs. The pre-training of 300-dimensional GloVe embeddings (Pennington et al., 2014) with a frequency cut-off of 5 took 8 hours on a server with 32 CPU cores and 386 GB memory. We train the SoftNER model and the two auxiliary models separately. Our segmentation model follows the simple BERT fine-tuning architecture except for the input, where BERT embeddings are concatenated with 100-dimensional code markdown and 10-dimensional word frequency features. We set the number of bins k to 10 for Gaussian vectorization. Our code recognition model is a feedforward network with two hidden layers and a single output node with sigmoid activation. 4 Evaluation In this section, we show th"
2020.acl-main.443,N18-1202,0,0.720905,"ey contributions are the following: 1 Our code and data are available at: https:// github.com/jeniyat/StackOverflowNER/ • A new StackOverflow NER corpus manually annotated with 20 types of named en4913 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4913–4926 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tities, including all in-line code within natural language sentences (§2). We demonstrate that NER in the software domain is an ideal benchmark task for testing effectiveness of contextual word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), due to its inherent polysemy and salient reliance on context. • An in-domain trained neural SoftNER tagger for StackOveflow (§3) that can recognize 20 fine-grained named entity types related to software developing. We also tested its performance on GitHub data of readme files and issue reports. • A code token recognizer (§3.1) that utilizes StackOveflow code snippets to capture the spelling patterns of code-related tokens, and consistently improves the NER tagger. • In-domain pretrained ELMo and BERT representations (§3.3) on 152 million sentences from StackOve"
2020.acl-main.443,E14-1078,0,0.0202013,"LL 2003 dataset (Sang and De Meulder, 2003) is a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on nam"
2020.acl-main.443,P15-1085,0,0.0326894,"low) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and tec"
2020.acl-main.443,D18-1410,1,0.927645,"ilizes language model features and spelling patterns to predict whether a word is a code entity. The input features include unigram word and 6-gram character probabilities from two language models (LMs) that are trained on the Gigaword corpus (Napoles et al., 2012) and all the code-snippets in the StackOverflow 10-year archive respectively. We also pre-trained FastText (Joulin et al., 2016) word embeddings using these code-snippets, where a word vector is represented as a sum of its character ngrams. We first transform each ngram probability into a k-dimensional vector using Gaussian binning (Maddela and Xu, 2018), which has shown to improve the performance of neural models using numeric features (Sil et al., 2017; Liu et al., 2016; Maddela and Xu, 2018). We then feed the vectorized features into a linear layer, concatenate the output with FastText character-level embeddings, and pass them through another hidden layer with sigmoid activation. We predict the token as a codeentity if the output probability is greater than 0.5. This binary prediction is then converted into a vector and used as an input to the SoftNER model. 3.1.3 Entity Segmentation The segmentation task refers to identifying entity spans"
2020.acl-main.443,P14-5010,0,0.00242252,"readme files collected from these 143 repositories. The resulting GitHub NER dataset consists of 6,510 sentences and 10,963 entities of 20 types labeled by two inhouse annotators. The inter-annotator agreement of this dataset is 0.68, measured by span-level Cohen’s Kappa. 2.4 common features of web texts, including abbreviations, emoticons, URLs, ungrammatical sentences and spelling errors. We found that tokenization is non-trivial as many code-related tokens are mistakenly split by the existing web-text tokenizers, including the CMU Twokenizer (Gimpel et al., 2011), Stanford TweetTokenizer (Manning et al., 2014), and NLTK Twitter Tokenizer (Bird et al., 2009): txScope.Complete() std::condition variable math.h hspani a==b Therefore, we implemented a new tokenizer, using Twokenizer3 as the starting point and added additional regular expression rules to avoid splitting code-related tokens. 3 Named Entity Recognition Models The extraction of software-related named entities imposes significant challenges as it requires resolving a significant amount of unseen tokens, inherent polysemy, and salient reliance on context. Unlike news or biomedical data, spelling patterns and long-distance dependencies are mor"
2020.acl-main.443,D11-1141,1,0.492578,"Related Work The CoNLL 2003 dataset (Sang and De Meulder, 2003) is a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively litt"
2020.acl-main.443,P15-1140,0,0.0316407,"tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and techniques for identifying software-related named entities (e.g., variable names or application names) within natural language texts. In this paper, we present a comprehensive study that investigates the unique challenges of named entity recognition in the social computer programming domain. These named entities are often ambiguous and have implicit reliance on the accompanied code snippets. For example,"
2020.acl-main.443,W12-3018,0,0.0368501,"Missing"
2020.acl-main.443,N19-1250,0,0.0158917,"s social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity types. We also develop a novel attention based neural NER model to extract those finegrained entities. 7 long-dist"
2020.acl-main.443,W03-0419,0,0.151891,"Missing"
2020.acl-main.443,W18-5618,0,0.0385168,"Missing"
2020.acl-main.443,E12-2021,0,0.0329356,"Missing"
2020.acl-main.443,D18-1310,0,0.0139321,"s a feature, converting the scalar value into a k-dimensional vector by Gaussian binning (Maddela and Xu, 2018). • Code Markdown indicates whether the given token appears inside a hcodei markdown tag in the StackOverflow post. It is worth noting that hcodei tags are noisy as users do not always enclose inline code in a hcodei tag or sometimes use the tag to highlight non-code texts (details in §2.1). Nevertheless, we find it helpful to include the markdown information as a feature as it improves the performance of our segmentation model. The inclusion of hand-crafted features is influenced by Wu et al. (2018), where word-shapes and POS tags were shown to improve the performance of sequence tagging models. 3.2 Embedding-Level Attention For each input word wi in the input sentence, we have three embeddings: BERT (wi1 ), Code Recognizer (wi2 ), and Entity Segmenter (wi3 ). We introduce the embedding-level attention αit (t ∈ {1, 2, 3}), which captures each embedding’s contribution towards the meaning of the word, to combine them together. To compute αit , we pass the input embeddings through a bidirectional GRU and generate their corresponding hidden repre←−−→ sentations hit = GRU (wit ). These vector"
2020.acl-main.443,D18-1034,0,0.012837,"2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standar"
2020.acl-main.443,C18-1183,0,0.0143257,"d languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity types. We also develop a novel attention based neural NER model to extract those finegrai"
2020.acl-main.443,N16-1174,0,0.156375,"Missing"
2020.acl-main.443,D18-2002,0,0.063399,"kOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and techniques for identifying software-related n"
2020.acl-main.443,D08-1030,0,\N,Missing
2020.acl-main.443,N03-1031,0,\N,Missing
2020.acl-main.443,W17-4419,0,\N,Missing
2020.acl-main.443,W18-3219,0,\N,Missing
2020.acl-main.443,D18-1347,0,\N,Missing
2020.acl-main.443,D19-1539,0,\N,Missing
2020.acl-main.443,N19-1423,0,\N,Missing
2020.acl-main.443,W10-0713,0,\N,Missing
2020.acl-main.473,E17-1003,0,0.0435235,"Missing"
2020.acl-main.473,W18-5042,0,0.0198791,"Missing"
2020.acl-main.473,W10-3001,0,0.126177,"Missing"
2020.acl-main.473,D13-1181,0,0.119392,"tween people’s language and their forecasting skill. To the best of our knowledge, this is the first work that presents a computational way of exploring this direction. Our work is also closely related to prior research on predicting various phenomenon from users’ language. For example Tan et al. (2014) study the effect of wording on message propagation, Gillick and Bamman (2018) examine the connection between language used by politicians in campaign speeches and applause and P´erez-Rosas and Mihalcea (2015) explored linguistic differences between truthful and deceptive statements. Ganjigunte Ashok et al. (2013) show linguistic cues drawn from authors’ language are strong indicators of the success of their books and Tsur and Rappoport (2009) presented an unsupervised model to analyze the helpfulness of book reviews by analyzing their text. There have been several studies using data from Good Judgment Open or Good Judgment Project (Mellers et al., 2015b). One recent study examining the language side of this data is Schwartz et al. (2017). Their main goal is to suggest objective metrics as alternatives for subjective ratings when evaluating the quality of recommendations. To achieve this, justification"
2020.acl-main.473,N09-1031,0,0.0383996,"rk has also studied persuasive language on crowdfunding platforms (Yang et al., 2019). In contrast, our work focuses on directly measuring forecasting skill based on text justifications. Finally we note that there is a long history of research on financial analysts’ forecasting ability (Crichfield et al., 1978; Chopra, 1998; Loh and Mian, 2006). Most work relies on regression models to test if pre-identified factors are correlated with forecasting skill (e.g., Loh and Mian (2006); Call et al. (2009)). Some work has also explored the use of textual information in financial domain. For example, Kogan et al. (2009) present a study of predicting companies’ risk by using financial reports. We also note a recent paper on studying financial analysts’ decision making process by using text-based features from earning calls (Keith and Stent, 2019). As far as we aware, our work is the first to evaluate analysts’ forecasting skill based on their language. 6 Limitations and Future Work Our experiments demonstrated it is possible to analyze language to estimate people’s skill at making predictions about the future. In this section we 5324 highlight several limitations of our study and ethical issues that should be"
2020.acl-main.473,P14-1018,0,0.0791703,"Missing"
2020.acl-main.473,D18-1004,0,0.0273447,"in addition to rates of heart disease. Demszky et al. (2019) analyzed political polarization in social media and Voigt et al. (2017) examined the connections between police officers’ politeness and race by analyzing language. A number of studies (De Choudhury et al., 2014; Eichstaedt et al., 2018; Benton et al., 2017; Park et al., 2017) have examined the connection between users’ language on social media and depression and alcohol use (Kiciman et al., 2018). Other work has analyzed users’ language to study the effect of attributes, such as gender, in online communication (Bamman et al., 2014; Wang and Jurgens, 2018; Voigt et al., 2018). In this work we study the relationship between people’s language and their forecasting skill. To the best of our knowledge, this is the first work that presents a computational way of exploring this direction. Our work is also closely related to prior research on predicting various phenomenon from users’ language. For example Tan et al. (2014) study the effect of wording on message propagation, Gillick and Bamman (2018) examine the connection between language used by politicians in campaign speeches and applause and P´erez-Rosas and Mihalcea (2015) explored linguistic di"
2020.acl-main.473,H05-1044,0,0.0106245,"language. In this paper, we present the first systematic study of the connection between language and forecasting ability. To do so, we analyze texts written by top forecasters (ranked by accuracy against ground truth) in two domains: geopolitical forecasts from an online prediction forum, and company earnings forecasts made by financial analysts. To shed light on the differences in approach employed by skilled and unskilled forecasters, we investigate a variety of linguistic metrics. These metrics are computed using natural language processing methods to analyze sentiment (Pang et al., 2002; Wilson et al., 2005), uncertainty (de Marneffe et al., 2012; Saur´ı and Pustejovsky, 2012), readability, etc. In addition we make use of word lists taken from the Linguistic Inquiry and Word Count (LIWC) software (Tausczik and Pennebaker, 2010), which is widely used in psychological research. By analyzing forecasters’ texts, we are able to provide evidence to support or refute hypotheses about factors that may influence forecasting skill. For example, we show forecasters whose justifications contain a higher proportion of uncertain statements tend to make more accurate predictions. This supports the hypothesis th"
2020.acl-main.473,N19-1364,1,0.794096,"dgment Project (Mellers et al., 2015b). One recent study examining the language side of this data is Schwartz et al. (2017). Their main goal is to suggest objective metrics as alternatives for subjective ratings when evaluating the quality of recommendations. To achieve this, justifications written by one group are provided as tips to another group. These justifications are then evaluated on their ability to persuade people to update their predictions, leading to real benefits that can be measured by objective metrics. Prior work has also studied persuasive language on crowdfunding platforms (Yang et al., 2019). In contrast, our work focuses on directly measuring forecasting skill based on text justifications. Finally we note that there is a long history of research on financial analysts’ forecasting ability (Crichfield et al., 1978; Chopra, 1998; Loh and Mian, 2006). Most work relies on regression models to test if pre-identified factors are correlated with forecasting skill (e.g., Loh and Mian (2006); Call et al. (2009)). Some work has also explored the use of textual information in financial domain. For example, Kogan et al. (2009) present a study of predicting companies’ risk by using financial"
2020.emnlp-main.382,2020.acl-main.709,1,0.873797,"Missing"
2020.emnlp-main.382,kamholz-etal-2014-panlex,0,0.0191955,"Missing"
2020.emnlp-main.382,W19-5407,0,0.0284384,"ki, Oscar (+ code-switch) 10.4B/6.1B/4.3B WordPiece 50k/21k/26k no base 125M Table 1: Configuration comparisons for AraBERT (Antoun et al., 2020), mBERT (Devlin et al., 2019), XLMRoBERTa (Conneau et al., 2020a), and GigaBERT (this work). trained language model designed specifically for English-Arabic. K et al. (2020) pre-trained smallscale (e.g., 1GB data and 2M training steps) bilingual BERT for English-Hindi, English-Spanish, and English-Russian to study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives on crosslingual transfer. Kim et al. (2019) presented a bilingual BERT using multi-task learning for translation quality estimation with regards to English-Russian and English-German. Conneau et al. (2020b) focused on the bilingual XLM for English-French, English-Russian, and English-Chinese to analyze the cross-lingual transfer ability with domain similarity, anchor points, parameter sharing, and language similarity. 3 GigaBERT We present five versions of GigaBERT pre-trained using the Transformer encoder (Vaswani et al., 2017) with BERTbase configurations: 12 attention layers, each has 12 attention heads and 768 hidden dimensions, wh"
2020.emnlp-main.382,D19-1077,0,0.135851,"20) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at https://github.com/ lanwuwei/GigaBERT. 1 Introduction Fine-tuning pre-trained Transformer models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) has recently achieved state-of-the-art results on a wide range of NLP tasks where supervised training data is available. When trained on multilingual corpora, BERT-based models have demonstrated the ability to learn multilingual representations that support zero-shot cross-lingual transfer learning surprisingly effectively (Wu and Dredze, 2019; Pires et al., 2019; Lample and Conneau, 2019). Without access to any parallel text or target language annotations, multilingual BERT (mBERT; Devlin et al., 2019) even supports cross-lingual transfer for language pairs that are written in different scripts, for example, English-to-Arabic. However, the transfer learning performance still lags far behind where supervised data is available in the target language. In this paper, we explore to what extent it is possible to improve performance in the zero-shot scenario by building a customized bilingual BERT for English and Arabic, a particularly c"
2020.wnut-1.33,C18-1139,0,0.0617004,"We provided the participants baseline model for both of the subtasks. The baseline model for named 261 entity recognition task utilized a feature-based CRF tagger developed using the CRF-Suite3 with a standard set of contextual, lexical and gazetteer features. The baseline relation extraction system employed a feature-based logistic regression model developed using the Scikit-Learn4 with a standard set of contextual, lexical and gazetteer features. DSC-IITISM (Gupta et al., 2020) developed a BiLSTM-CRF model that utilized a concatenation of CamemBERTbase (Martin et al., 2020), Flair(PubMed) (Akbik et al., 2018), and GloVe(en) (Pennington et al., 2014) word representations. 2.3 IBS (Sikdar et al., 2020) utilized an ensemble classifier with 4 feature based on CRF taggers. Fancy Man (Zeng et al., 2020) fine-tuned the BERTbase (Devlin et al., 2019) model with an additional linear layer. NER Systems Thirteen teams (Table 3) participated in the named entity recognition sub-task. A wide variety of approaches were taken to tackle this task. Table 2 summarizes the word representations, features and the machine learning approaches taken by each team. Majority of the teams (11 out of 13) utilized contextual wo"
2020.wnut-1.33,N19-1423,0,0.247952,"and gazetteer features. The baseline relation extraction system employed a feature-based logistic regression model developed using the Scikit-Learn4 with a standard set of contextual, lexical and gazetteer features. DSC-IITISM (Gupta et al., 2020) developed a BiLSTM-CRF model that utilized a concatenation of CamemBERTbase (Martin et al., 2020), Flair(PubMed) (Akbik et al., 2018), and GloVe(en) (Pennington et al., 2014) word representations. 2.3 IBS (Sikdar et al., 2020) utilized an ensemble classifier with 4 feature based on CRF taggers. Fancy Man (Zeng et al., 2020) fine-tuned the BERTbase (Devlin et al., 2019) model with an additional linear layer. NER Systems Thirteen teams (Table 3) participated in the named entity recognition sub-task. A wide variety of approaches were taken to tackle this task. Table 2 summarizes the word representations, features and the machine learning approaches taken by each team. Majority of the teams (11 out of 13) utilized contextual word representations. Four teams combined the contextual word representations with global word vectors. Only two teams did not use any type of word representations and relied entirely on hand-engineered features and a CRF taggers. The best"
2020.wnut-1.33,doddington-etal-2004-automatic,0,0.139539,"ticipant teams, evaluated on the Test-20 corpus. Both of the teams utilized the gold entities and then predict the relations among these entities by fine-tuning con848 16161 8000 6000 Baseline 3015 587 4558 Error Count Figure 2: Summary of incorrectly classified entity tokens by each submitted systems. 4000 2000 Figure 3: Summary of incorrectly predicted relations in each submitted systems. 4 Related Work The task of information extraction from wet lab protocols is closely related to the event trigger extraction task. The event trigger task has been studied extensively, mostly using ACE data (Doddington et al., 2004) and the BioNLP data (N´edellec et al., 2013). Broadly, there are two ways to classify various event trigger detection models: (1) Rule-based methods using pattern matching and regular expression to identify triggers (Vlachos et al., 2009) and (2) Machine Learning based methods focusing on generation of high-end hand-crafted features to be used in classification models like SVMs or maxent classifiers (Pyysalo et al., 2012). Kernel based learning methods have also been utilized with embedded features from the syntactic and semantic contexts to identify and extract the biomed264 ical event entit"
2020.wnut-1.33,2020.acl-main.740,0,0.0383653,"Missing"
2020.wnut-1.33,2020.acl-main.192,1,0.820751,"chnologies to extract structured representations of procedures from natural language protocols (Kuniyoshi et al., 2020; Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010). Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions. However, performance of state-of-the-art tools for extracting named entity and relations from wet lab protocols still lags behind well edited text genres (Jiang et al., 2020). This motivates the need for continued research, in addition to new datasets and tools adapted to this noisy text genre. 1 https://autoprotocol.org/ 2 Wet Lab Protocols Wet lab protocols consist of the guidelines from different lab procedures which involve chemicals, drugs, or other materials in liquid solutions or volatile phases. The protocols contain a sequence of steps that are followed to perform a desired task. These protocols also include general guidelines or warnings about the materials being used. The publicly available archive of protocol.io contains such guidelines of wet lab expe"
2020.wnut-1.33,2020.wnut-1.34,0,0.0265821,"Missing"
2020.wnut-1.33,W04-1213,0,0.041156,"also been utilized with embedded features from the syntactic and semantic contexts to identify and extract the biomed264 ical event entities (Zhou et al., 2014). In order to counteract highly sparse representations, different neural models were proposed. These neural models utilized the dependency based word embeddings with feed forward neural networks (Wang et al., 2016b), CNNs (Wang et al., 2016a) and Bidirectional RNNs (Rahul et al., 2017). Previous work has experimented on datasets of well-edited biomedical publications with a small number of entity types. For example, the JNLPBA corpus (Kim et al., 2004) with 5 entity types (CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN) and the BC2GM corpus (Hirschman et al., 2005) with a single entity class for genes/proteins. In contrast, our dataset addresses the challenges of recognizing 18 finegrained named entities along with 15 types of relations from the user-created wet lab protocols. 5 Summary In this paper, we presented a shared task for consisting of two sub-tasks: named entity recognition and relation extraction from the wet lab protocols. We described the task setup and datasets details, and also outlined the approach taken by the participating s"
2020.wnut-1.33,2020.wnut-1.40,0,0.56796,"near CRF with hand-crafted features. mahab (Pour and Farinnia, 2020) fine-tuned the BERTbase (Devlin et al., 2019) sequence tagging model. mgsohrab (Sohrab et al., 2020) fine-tuned the SciBERT (Beltagy et al., 2019) model. PublishInCovid19 (Singh and Wadhawan, 2020) employed a structured ensemble classifier (Nguyen and Guo, 2007) consisting of 11 BiLSTMCRF taggers, that utilized the PubMedBERT (Gu et al., 2020) word representation. SudeshnaTCS (Jana, 2020) fine-tuned XLNet (Yang et al., 2019) model. IITKGP (Kaushal and Vaidhya, 2020) finetuned the Bio-BERT (Lee et al., 2020) model. 2.4 BiTeM (Knafou et al., 2020) developed a voting based ensemble classifier containing 14 transformer models, and utilized 7 different word representations including BERT (Devlin et al., 2019), ClinicalBERT (Huang et al., 2019), PubMedBERTbase (Gu et al., 2020), BioBERT (Lee et al., 2020), RoBERTa (Liu et al., 2019), BiomedRoBERTabase (Gururangan et al., 2020) and XLNet (Yang et al., 2019). 3 Kabir (Khan, 2020) employed an RNN-CRF model that utilized concatenation of Flair(PubMed) (Akbik et al., 2018) and ELMo(PubMed) (Peters et al., 2018) word representations. RE Systems Two teams (Table 3) participated in the relation ex"
2020.wnut-1.33,N18-2016,1,0.301656,"e, see Figure 1). While there have been efforts to develop domain-specific formal languages in order to support robotic automation1 of experimental procedures (Bates et al., 2017), the vast majority of knowledge about how to carry out biological experiments or chemical synthesis procedures is only documented in natural language texts, including in scientific papers, electronic lab notebooks, and so on. Recent research has begun to apply human language technologies to extract structured representations of procedures from natural language protocols (Kuniyoshi et al., 2020; Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010). Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions. However, performance of state-of-the-art tools for extracting named entity and relations from wet lab protocols still lags behind well edited text genres (Jiang et al., 2020). This motivates the need for continued research, in addition to new datasets and tools adapted to this noisy text genre. 1 https://autoprotocol"
2020.wnut-1.33,2020.lrec-1.239,0,0.0116175,"hemistry or biology experiments (for an example, see Figure 1). While there have been efforts to develop domain-specific formal languages in order to support robotic automation1 of experimental procedures (Bates et al., 2017), the vast majority of knowledge about how to carry out biological experiments or chemical synthesis procedures is only documented in natural language texts, including in scientific papers, electronic lab notebooks, and so on. Recent research has begun to apply human language technologies to extract structured representations of procedures from natural language protocols (Kuniyoshi et al., 2020; Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010). Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions. However, performance of state-of-the-art tools for extracting named entity and relations from wet lab protocols still lags behind well edited text genres (Jiang et al., 2020). This motivates the need for continued research, in addition to new datasets and tools adapted to"
2020.wnut-1.33,2021.ccl-1.108,0,0.0224884,"Missing"
2020.wnut-1.33,N19-1308,0,0.0327409,"tence 24.32 318.77 255.25 171.90 13.11 10.49 7.07 Table 1: Statistics of the Wet Lab Protocol corpus. noisy style of user created protocols imposed crucial challenges for the entity and relation extraction systems. Hence, off-the-shelf named entity recognition and relation extraction tools, tuned for well edited texts, suffer a severe performance degradation when applied to noisy protocol texts (Kulkarni et al., 2018). To address these challenges, there has been an increasing body of work on adapting entity and relation extraction recognition tools for noisy wet lab texts (Jiang et al., 2020; Luan et al., 2019; Kulkarni et al., 2018). However, different research groups have used different evaluation setups (e.g., training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation, we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user generated wet-lab text genres. T YPE -L INK, C O - REFERENCE -L INK, M OD -L INK, C OUNT, M ERONYM, U SING, M EASURE, C OM MANDS, O F -T YPE, O R, P RODUCT, and ACTS - ON. The training and development dataset for our t"
2020.wnut-1.33,2020.wnut-1.36,0,0.0133602,"and XLNet (Yang et al., 2019). 3 Kabir (Khan, 2020) employed an RNN-CRF model that utilized concatenation of Flair(PubMed) (Akbik et al., 2018) and ELMo(PubMed) (Peters et al., 2018) word representations. RE Systems Two teams (Table 3) participated in the relation extraction sub-task. Both of the teams followed fine-tuning of contextual word representation and did not use any hand-crafted features. Table 5 summarizes the word representations and the machine learning approaches followed by each team. Below we provide a brief description of the model developed by taken by each team. Big Green (Miller and Vosoughi, 2020) considered the protocols as a knowledge graph, in which relationships between entities are edges in the knowledge graph. They trained a BERT (Devlin et al., 2019) based system to classify edge presence and type between two entities, given entity text, label, and local context. 262 Team BiTeM PublishInCovid19 Fancy Man mahab mgsohrab SudeshnaTCS IITKGP B-NLP BIO-BIO DSC-IITISM Kabir IBS KaushikAcharya Baseline Word Representation BERT, BioBERT, RoBERTa, XLNet PubMedBERT BERT BERT SciBERT XLNet BioBERT SciBERT, word2vec BioBERT GLoVe, CamemBERT, Flair GLoVe, ELMo, BERT, Flair - Features Lexical"
2020.wnut-1.33,W13-2001,0,0.086491,"Missing"
2020.wnut-1.33,D14-1162,0,0.0824004,"Missing"
2020.wnut-1.33,N18-1202,0,0.00904826,"ushal and Vaidhya, 2020) finetuned the Bio-BERT (Lee et al., 2020) model. 2.4 BiTeM (Knafou et al., 2020) developed a voting based ensemble classifier containing 14 transformer models, and utilized 7 different word representations including BERT (Devlin et al., 2019), ClinicalBERT (Huang et al., 2019), PubMedBERTbase (Gu et al., 2020), BioBERT (Lee et al., 2020), RoBERTa (Liu et al., 2019), BiomedRoBERTabase (Gururangan et al., 2020) and XLNet (Yang et al., 2019). 3 Kabir (Khan, 2020) employed an RNN-CRF model that utilized concatenation of Flair(PubMed) (Akbik et al., 2018) and ELMo(PubMed) (Peters et al., 2018) word representations. RE Systems Two teams (Table 3) participated in the relation extraction sub-task. Both of the teams followed fine-tuning of contextual word representation and did not use any hand-crafted features. Table 5 summarizes the word representations and the machine learning approaches followed by each team. Below we provide a brief description of the model developed by taken by each team. Big Green (Miller and Vosoughi, 2020) considered the protocols as a knowledge graph, in which relationships between entities are edges in the knowledge graph. They trained a BERT (Devlin et al.,"
2020.wnut-1.33,W17-2340,0,0.0124734,"generation of high-end hand-crafted features to be used in classification models like SVMs or maxent classifiers (Pyysalo et al., 2012). Kernel based learning methods have also been utilized with embedded features from the syntactic and semantic contexts to identify and extract the biomed264 ical event entities (Zhou et al., 2014). In order to counteract highly sparse representations, different neural models were proposed. These neural models utilized the dependency based word embeddings with feed forward neural networks (Wang et al., 2016b), CNNs (Wang et al., 2016a) and Bidirectional RNNs (Rahul et al., 2017). Previous work has experimented on datasets of well-edited biomedical publications with a small number of entity types. For example, the JNLPBA corpus (Kim et al., 2004) with 5 entity types (CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN) and the BC2GM corpus (Hirschman et al., 2005) with a single entity class for genes/proteins. In contrast, our dataset addresses the challenges of recognizing 18 finegrained named entities along with 15 types of relations from the user-created wet lab protocols. 5 Summary In this paper, we presented a shared task for consisting of two sub-tasks: named entity rec"
2020.wnut-1.33,2020.wnut-1.35,0,0.0682165,"Missing"
2020.wnut-1.33,2020.wnut-1.38,0,0.480082,"Missing"
2020.wnut-1.33,E12-2021,0,0.0592604,"Missing"
2020.wnut-1.33,W09-1405,0,0.0756553,"Missing"
2020.wnut-1.33,2020.wnut-1.39,0,0.0145229,"3 with a standard set of contextual, lexical and gazetteer features. The baseline relation extraction system employed a feature-based logistic regression model developed using the Scikit-Learn4 with a standard set of contextual, lexical and gazetteer features. DSC-IITISM (Gupta et al., 2020) developed a BiLSTM-CRF model that utilized a concatenation of CamemBERTbase (Martin et al., 2020), Flair(PubMed) (Akbik et al., 2018), and GloVe(en) (Pennington et al., 2014) word representations. 2.3 IBS (Sikdar et al., 2020) utilized an ensemble classifier with 4 feature based on CRF taggers. Fancy Man (Zeng et al., 2020) fine-tuned the BERTbase (Devlin et al., 2019) model with an additional linear layer. NER Systems Thirteen teams (Table 3) participated in the named entity recognition sub-task. A wide variety of approaches were taken to tackle this task. Table 2 summarizes the word representations, features and the machine learning approaches taken by each team. Majority of the teams (11 out of 13) utilized contextual word representations. Four teams combined the contextual word representations with global word vectors. Only two teams did not use any type of word representations and relied entirely on hand-en"
2021.eacl-main.187,E17-1051,0,0.0280162,"turn to the literature on abstract meaning representation (AMR; Banarescu et al., 2013) for established graph agreement metrics, which we adapt to our setting. Similarly to our PEG representation, the AMR formalism has predicate and argument nodes (lab operations and entities in our notation) and directed labeled edges which can form undirected cycles through reentrancies (nodes with multiple incoming edges).4 In Table 5 we report a graph Smatch score (Cai and Knight, 2013) widely used to quantify AMR’s graph structure agreement, as well as finer grained graph agreement metrics, adapted from Damonte et al. (2017). Smatch values are comparable to those obtained for AMR, where reported gold agreement 4 2194 Unfortunately, we cannot follow this analogy to train AMR models on our graphs, since, to the best of our knowledge, they are currently limited to single sentences, notwithstanding a promising recent initial exploration into multi-sentence AMR annotation (O’Gorman et al., 2018). Agreement Metric Smatch Argument identification Predicate identification Core roles Re-entrancies F1 Dataset Avg. #args/op #Ops. w/o core arg. #Ops. Pct. 84.99 89.72 86.68 80.52 73.12 WLP X-WLP Table 5: X-WLP inter-annotator"
2021.eacl-main.187,D19-1371,0,0.0540651,"ing. Our annotator pay was 13 USD / hour. The overall annotation budget for X-WLP was roughly 3,200 USD. 5 Models We present two approaches for PEG prediction. First, in §5.1 we design models for separate graph sub-component prediction, which are chained to form a pipeline PEG prediction model. Second, in §5.2 we present a model which directly predicts the entire PEG using a span-graph prediction approach. 5.1 Pipeline Model (P IPELINE) A full PEG representation as defined in §3 can be obtained by chaining the following models which predict its sub-components. In all of these, we use SciBERT (Beltagy et al., 2019) which was trained on scientific texts similar to our domain. 2195 Mention identification. Given a scientific protocol written in natural language, we begin by identifying all experiment-involved text spans mentioning lab operations (predicates) or entities and their traits (arguments), which are the building blocks for PEGs. We model this problem of mention identification as a sequence tagging problem. Specifically, we transfer span-level mention labels, which are annotated in the WLP corpus into token-level labels using the BIO tagging scheme, then fine-tune the SciBERT model for token class"
2021.eacl-main.187,D15-1114,0,0.0180507,"tions; linking our approach with their framework is a promising future direction. Structurally, PEGs are similar to abstract meaning representation (AMR; Banarescu et al. 2013), allowing us to use agreement and performance metrics developed for AMR. In contrast with the sentence-level AMR, a major challenge in this work is annotating and predicting procedure-level representations.8 Another line of research focuses on procedural text understanding for more general domains: simple scientific processes (Dalvi et al., 2018), open domain procedural texts (Tandon et al., 2020), and cooking recipes (Kiddon et al., 2015; Bosselut et al., 2018). These works represent process-level information and entity state changes, but typically feature shorter processes, simpler language and an open ontology, compared with our domain-specific terminology and grounded ontology. Our framework also provides a link to text-based game approaches to procedural text understanding. Tamari et al. (2019) modelled scientific procedures with text-based games but used only synthetic data. Our simulator enables leveraging recent advances on text-based games agents (e.g., (Adhikari et al., 2020)) towards natural language understanding."
2021.eacl-main.187,N18-2016,1,0.925152,"re 1 does not specify the swirling (mixing) speed or its duration. Our process execution graph (PEG) captures the predicate-argument structure of the protocol, allowing it to be more lenient than a programming language (for example, capturing that gently modifies swirl). Better suited to represent underspecified natural language, PEGs can serve as a convenient scaffold to support downstream tasks such as text-to-code assistants (Mehr et al., 2020). For example, by asking researchers to fill in missing required arguments for swirl. To annotate PEGs, we leverage the sentencelevel annotations of Kulkarni et al. (2018) (WLP henceforth). WLP, exemplified at the top of 2 2191 https://autoprotocol.org/specification Figure 1, collected sentence-level structures using the BRAT annotation tool (Stenetorp et al., 2012). For example, capturing that cells, culture tubes are arguments for add. However, WLP does not capture cross-sentence implicit relations such that culture tubes are an argument for incubate. These are abundant in lab protocols, require tracking entities across many sentences, and are not easy to annotate using BRAT (see discussion in §4). We vastly extend upon WLP annotations, aiming to capture the"
2021.eacl-main.187,N18-1144,0,0.127216,"identification. In conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough for wet-lab protocol annotation by non-experts. • We release X-WLP, a challenging corpus of 279 PEGs representing document-level lab protocols. This size is on par with similar corpora of procedural text (Dalvi et al., 2018; Mysore et al., 2019; Vaucher et al., 2020). • We develop two graph parsers: a pipeline model which chains predictions for graph subcomponents, and a joint-model of mention and relation detectors. 2 Background and Motivation Several formalisms for programmatic lab controller interfaces were developed in recent years (Yachie and Natsume, 2017; Lee and Miles, 2018). For instance, Autoprotocol defines 35 lab commands, including spin, incubate, and mix.2 While these define wet-lab experiments in a precise and unambiguous manner, they do not readily replace their natural language description in sc"
2021.eacl-main.187,N19-1308,0,0.0218954,"ps by issuing textual commands to the simulator. The commands are deterministically converted to our PEG representation. This interface takes much of the burden off annotators by keeping track of object traits and commonsense constraints. For example, when the annotator issues a transfer command for a container, the simulator moves all its contents as well. We find that in-house annotators were able to effectively use this interface on complex protocols, achieving good agreement. Finally, we use this data to explore several models, building upon recent advances in graph prediction algorithms (Luan et al., 2019; Wadden et al., 2019). We thoroughly analyze model performance and find that our data introduces interesting new challenges, such as complex coreference resolution and long-range, cross-sentence relation identification. In conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough f"
2021.eacl-main.187,W19-4007,0,0.21867,"conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough for wet-lab protocol annotation by non-experts. • We release X-WLP, a challenging corpus of 279 PEGs representing document-level lab protocols. This size is on par with similar corpora of procedural text (Dalvi et al., 2018; Mysore et al., 2019; Vaucher et al., 2020). • We develop two graph parsers: a pipeline model which chains predictions for graph subcomponents, and a joint-model of mention and relation detectors. 2 Background and Motivation Several formalisms for programmatic lab controller interfaces were developed in recent years (Yachie and Natsume, 2017; Lee and Miles, 2018). For instance, Autoprotocol defines 35 lab commands, including spin, incubate, and mix.2 While these define wet-lab experiments in a precise and unambiguous manner, they do not readily replace their natural language description in scientific publications"
2021.eacl-main.187,C18-1313,0,0.065185,"Missing"
2021.eacl-main.187,W19-2609,1,0.827688,"representations.8 Another line of research focuses on procedural text understanding for more general domains: simple scientific processes (Dalvi et al., 2018), open domain procedural texts (Tandon et al., 2020), and cooking recipes (Kiddon et al., 2015; Bosselut et al., 2018). These works represent process-level information and entity state changes, but typically feature shorter processes, simpler language and an open ontology, compared with our domain-specific terminology and grounded ontology. Our framework also provides a link to text-based game approaches to procedural text understanding. Tamari et al. (2019) modelled scientific procedures with text-based games but used only synthetic data. Our simulator enables leveraging recent advances on text-based games agents (e.g., (Adhikari et al., 2020)) towards natural language understanding. 8 Conclusion We developed a novel meaning representation and simulation-based annotation interface, enabling the collection of process-level annotations of experimental procedures, as well as two parsers (pipeline and joint modelling) trained on this data. Our dataset and experiments present several directions for future work, including the modelling of challenging"
2021.eacl-main.187,2020.emnlp-main.520,0,0.0178966,"g, relying on a human-inthe-loop for corrections; linking our approach with their framework is a promising future direction. Structurally, PEGs are similar to abstract meaning representation (AMR; Banarescu et al. 2013), allowing us to use agreement and performance metrics developed for AMR. In contrast with the sentence-level AMR, a major challenge in this work is annotating and predicting procedure-level representations.8 Another line of research focuses on procedural text understanding for more general domains: simple scientific processes (Dalvi et al., 2018), open domain procedural texts (Tandon et al., 2020), and cooking recipes (Kiddon et al., 2015; Bosselut et al., 2018). These works represent process-level information and entity state changes, but typically feature shorter processes, simpler language and an open ontology, compared with our domain-specific terminology and grounded ontology. Our framework also provides a link to text-based game approaches to procedural text understanding. Tamari et al. (2019) modelled scientific procedures with text-based games but used only synthetic data. Our simulator enables leveraging recent advances on text-based games agents (e.g., (Adhikari et al., 2020)"
2021.eacl-main.187,D19-1585,0,0.30608,"al commands to the simulator. The commands are deterministically converted to our PEG representation. This interface takes much of the burden off annotators by keeping track of object traits and commonsense constraints. For example, when the annotator issues a transfer command for a container, the simulator moves all its contents as well. We find that in-house annotators were able to effectively use this interface on complex protocols, achieving good agreement. Finally, we use this data to explore several models, building upon recent advances in graph prediction algorithms (Luan et al., 2019; Wadden et al., 2019). We thoroughly analyze model performance and find that our data introduces interesting new challenges, such as complex coreference resolution and long-range, cross-sentence relation identification. In conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough for wet-lab protocol an"
2021.eacl-main.187,E12-2021,0,0.146098,"Missing"
2021.emnlp-main.397,D15-1075,0,0.0225748,"g doesn’t eradicate the model’s offensive behavior. 5 Offensive Language and Stance Classification We now investigate the predictability of Offensive Language (Offensive) and Stance (Stance) in conversations that include generated responses. Given a thread, T = (u1 , u2 , ..., uk ), we predict Offensive labels oi ∈ {0, 1} for each utterance, ui , i ≤ k and Stance labels si←j ∈{Neutral, Agree, Disagree} for every pair of utterances (ui , uj ), i &lt; j ≤ k. 5.1 Model Architectures In both classification tasks, we experiment with the following three model architectures: NBOW - Neural-Bag-Of-Words (Bowman et al., 2015) model converts input sentences into latent representations by taking weighted average of their word embeddings. Then, the sentence representations are concatenated and processed through a 3-layer perceptron with ReLU activations and softmax layer to get classification output. BERT - We fine-tune BERTLARGE model (340M parameters, Devlin et al., 2019) based classifiers. BERT computes latent token representations of input “[CLS] ui [SEP]” for the Offensive 10 https://github.com/XuhuiZhou/Toxic_ Debias/blob/main/data/word_based_bias_ list.csv task and “[CLS] ui [SEP] uj [SEP]” for the Stance task"
2021.emnlp-main.397,D19-1176,0,0.0265152,"t effective technique in reducing offensive behavior in chatbots, but it is still far from perfect. 7 Related Work 14 The test threads used to evaluate dialogue models didn’t have a follow-up Reddit user response. Hence, we collect a different set of 500 offensive threads with a final user response. Identifying Toxicity - Most works on identifying toxic language looked at isolated social media posts 4853 or comments while ignoring the context (Davidson et al., 2017; Xu et al., 2012; Zampieri et al., 2019; Rosenthal et al., 2020; Kumar et al., 2018; Garibo i Orts, 2019; Ousidhoum et al., 2019; Breitfeller et al., 2019; Sap et al., 2020; Hada et al., 2021; Barikeri et al., 2021). These methods are ill-equipped in conversational settings where responses can be contextually offensive. Recently, Dinan et al. (2019a); Xu et al. (2020) studied contextual offensive language using adversarial humanbot conversations, where a human intentionally tries to trick the chatbot into saying something inappropriate. On the other hand, Pavlopoulos et al. (2020); Xenos et al. (2021) created labeled datasets for toxicity detection in single turn conversations and studied context-sensitivity in detection models. In contrast, we"
2021.emnlp-main.397,P19-1271,0,0.027033,"s in offensive contexts. 8 Conclusion media users to form echo-chambers (Cinelli et al., 2021; Soliman et al., 2019). Consequently, dialogue models learn to mimic this behavior and agree more frequently in offensive contexts. However, fine-tuning dialogue models on cleaner training data with desirable conversational properties (safe and neutral responses with DAPT) can mitigate this issue to some extent. To further strengthen dialogue safety, future research on detection of offensive context (Dinan et al., 2019a; Zhang et al., 2018a) and subsequent generation of nonprovocative counter-speech (Chung et al., 2019) is crucial. 9 Societal and Ethical Considerations This paper tackles issues of safety of neural models, and specifically it attempts to understand how dialogue systems can help combat social biases and help make conversations more civil (Dinan et al., 2019a; Xu et al., 2020). For this purpose, we crowdannotate a dataset of offensive conversations from publicly available Reddit conversations enriched with automatically generated responses. This study was conducted under the approval of the Institutional Review Board (IRB) of Georgia Institute of Technology. We paid crowd workers on Amazon’s Me"
2021.emnlp-main.397,S19-2081,0,0.0132927,"onversations i.e. DAPT to be the most effective technique in reducing offensive behavior in chatbots, but it is still far from perfect. 7 Related Work 14 The test threads used to evaluate dialogue models didn’t have a follow-up Reddit user response. Hence, we collect a different set of 500 offensive threads with a final user response. Identifying Toxicity - Most works on identifying toxic language looked at isolated social media posts 4853 or comments while ignoring the context (Davidson et al., 2017; Xu et al., 2012; Zampieri et al., 2019; Rosenthal et al., 2020; Kumar et al., 2018; Garibo i Orts, 2019; Ousidhoum et al., 2019; Breitfeller et al., 2019; Sap et al., 2020; Hada et al., 2021; Barikeri et al., 2021). These methods are ill-equipped in conversational settings where responses can be contextually offensive. Recently, Dinan et al. (2019a); Xu et al. (2020) studied contextual offensive language using adversarial humanbot conversations, where a human intentionally tries to trick the chatbot into saying something inappropriate. On the other hand, Pavlopoulos et al. (2020); Xenos et al. (2021) created labeled datasets for toxicity detection in single turn conversations and studied contex"
2021.emnlp-main.397,2020.findings-emnlp.301,1,0.87986,"el-controlled data). Here xi is the thread without the last utterance, cti is the classifier labeled control token and yi is the last utterance or response to xi . We discard ‘Disagree’ stance responses, as we only found about 10, 000 high-precision disagreeing responses. Our final sample contains about 100, 000 offensive responses and 75, 000 agreeing responses. We further divide into each control dataset of size L into a 95-5 ratio to get train and dev split. 6.1 Modeling, Training and Testing Details We use CTG techniques that were found effective in reducing toxicity in language models by Gehman et al. (2020). This includes (1) Domain-Adaptive PreTraining (DAPT) - fine-tuning a pretrained dialogue model on threads with fixed control tokens (Gururangan et al., 2020). (2) Attribute Conditioning (AT C ON) - In this method, special control to11 Only threads with all safe comments were considered for Stance control attribute. 12 We selected thresholds for all labels such that we get .75 and higher precision. kens encapsulate different response attributes. For example, [OFF] and [SAFE] tokens indicate offensive control attributes. During training, these tokens are prepended to responses and at inference"
2021.emnlp-main.397,2020.acl-main.740,0,0.0440623,"Missing"
2021.emnlp-main.397,2021.acl-long.210,0,0.034851,"behavior in chatbots, but it is still far from perfect. 7 Related Work 14 The test threads used to evaluate dialogue models didn’t have a follow-up Reddit user response. Hence, we collect a different set of 500 offensive threads with a final user response. Identifying Toxicity - Most works on identifying toxic language looked at isolated social media posts 4853 or comments while ignoring the context (Davidson et al., 2017; Xu et al., 2012; Zampieri et al., 2019; Rosenthal et al., 2020; Kumar et al., 2018; Garibo i Orts, 2019; Ousidhoum et al., 2019; Breitfeller et al., 2019; Sap et al., 2020; Hada et al., 2021; Barikeri et al., 2021). These methods are ill-equipped in conversational settings where responses can be contextually offensive. Recently, Dinan et al. (2019a); Xu et al. (2020) studied contextual offensive language using adversarial humanbot conversations, where a human intentionally tries to trick the chatbot into saying something inappropriate. On the other hand, Pavlopoulos et al. (2020); Xenos et al. (2021) created labeled datasets for toxicity detection in single turn conversations and studied context-sensitivity in detection models. In contrast, we study the stance dynamics of dialogu"
2021.emnlp-main.397,J18-4007,0,0.0194506,"rget groups from a predefined list comprising identity-based groups of people (e.g., people of various sexuality/sexualorientation/gender, people with disabilities, people from a specific race, political ideologies, etc.) and specific individuals e.g., (public figures, Reddit users, etc.) We present the list of selected target groups in Figure 7 in the Appendix. 2) Stance - We annotate the stance of ui towards each previous comment, uj , ∀j &lt; i. Stance is viewed as a linguistically articulated form of social action, in the context of the entire thread and sociocultural setting (Du Bois, 2007; Kiesling et al., 2018). Stance alignment between a pair of utterances is annotated as Agree, Disagree or Neutral. Our primary interest is in analyzing the stance taken towards offensive statements. We assume that a user or a chatbot can become offensive by aligning themselves with an offensive statement made by another user (see Figure 1).4 Additionally, for dialogue model responses uk , we also annotate their grammatical and contextual plausibility given the context. A screenshot of our annotation interface is shown in Figure 8 in the Appendix. Our main contributions include: (1) We release T OXI C HAT, a corpus o"
2021.emnlp-main.397,W18-4401,0,0.0448398,"Missing"
2021.emnlp-main.397,Q18-1027,0,0.0195448,"del on threads with fixed control tokens (Gururangan et al., 2020). (2) Attribute Conditioning (AT C ON) - In this method, special control to11 Only threads with all safe comments were considered for Stance control attribute. 12 We selected thresholds for all labels such that we get .75 and higher precision. kens encapsulate different response attributes. For example, [OFF] and [SAFE] tokens indicate offensive control attributes. During training, these tokens are prepended to responses and at inference time, they are manually frozen to steer the model’s response towards the desired attribute (Niu and Bansal, 2018; See et al., 2019; Xu et al., 2020). For each CTG experiment, we fine-tune DialoGPTmedium on the train split for 3 epochs and tune hyperparameters using dev set perplexity. Our goal is to test the conversation models in offensive contexts, where they have a propensity to agree with offensive comments, hence, we sample a test set of 500 threads where the last utterance is offensive. Using this test set, our CTG models are compared against DGPT-medium, GPT-3, and Blender in both automatic and human evaluations. 6.2 Automatic Evaluation An ideal dialogue model should have diverse, engaging and s"
2021.emnlp-main.397,D19-1474,0,0.0148938,"i.e. DAPT to be the most effective technique in reducing offensive behavior in chatbots, but it is still far from perfect. 7 Related Work 14 The test threads used to evaluate dialogue models didn’t have a follow-up Reddit user response. Hence, we collect a different set of 500 offensive threads with a final user response. Identifying Toxicity - Most works on identifying toxic language looked at isolated social media posts 4853 or comments while ignoring the context (Davidson et al., 2017; Xu et al., 2012; Zampieri et al., 2019; Rosenthal et al., 2020; Kumar et al., 2018; Garibo i Orts, 2019; Ousidhoum et al., 2019; Breitfeller et al., 2019; Sap et al., 2020; Hada et al., 2021; Barikeri et al., 2021). These methods are ill-equipped in conversational settings where responses can be contextually offensive. Recently, Dinan et al. (2019a); Xu et al. (2020) studied contextual offensive language using adversarial humanbot conversations, where a human intentionally tries to trick the chatbot into saying something inappropriate. On the other hand, Pavlopoulos et al. (2020); Xenos et al. (2021) created labeled datasets for toxicity detection in single turn conversations and studied context-sensitivity in detecti"
2021.emnlp-main.397,2020.acl-main.396,0,0.0349942,"ng the context (Davidson et al., 2017; Xu et al., 2012; Zampieri et al., 2019; Rosenthal et al., 2020; Kumar et al., 2018; Garibo i Orts, 2019; Ousidhoum et al., 2019; Breitfeller et al., 2019; Sap et al., 2020; Hada et al., 2021; Barikeri et al., 2021). These methods are ill-equipped in conversational settings where responses can be contextually offensive. Recently, Dinan et al. (2019a); Xu et al. (2020) studied contextual offensive language using adversarial humanbot conversations, where a human intentionally tries to trick the chatbot into saying something inappropriate. On the other hand, Pavlopoulos et al. (2020); Xenos et al. (2021) created labeled datasets for toxicity detection in single turn conversations and studied context-sensitivity in detection models. In contrast, we study the stance dynamics of dialogue model responses to offensive Reddit conversations with more than one turns. Inappropriate Language Mitigation - Sheng et al. (2020) manipulate training objectives and use adversarial triggers (Wallace et al., 2019) to reduce biases across demographics and generate less negatively biased text overall. Liu et al. (2020) propose adversarial training to reduce gender bias. Dinan et al. (2020a) t"
2021.emnlp-main.397,D14-1162,0,0.0863086,"Missing"
2021.emnlp-main.397,P19-1534,0,0.0227944,"/ AgainstHateSubReddits/ 6 GPT-3 model, ‘davinci’ with 175B parameters, in our data construction. Blender - More recently, Facebook released Blender Bot; a 2.7B parameter dialogue model (Roller et al., 2021). Blender bot is first pretrained on 1.5B Reddit comment threads (Baumgartner et al., 2020) and later finetuned on Blended Skill Talk (BST) dataset (Smith et al., 2020). The BST dataset contains 5K polite conversations between crowdworkers which aims to blend 3 conversational skills into one dataset 1) engaging personality (Zhang et al., 2018b; Dinan et al., 2020b), 2) empathetic dialogue (Rashkin et al., 2019) and 3) knowledge incorporation (Dinan et al., 2019b). We only include the first two models during annotation but compare our controlled text generation models against all three dialogue models in §6.1. Responses for DGPT and GPT-3 are generated on the comments part of the threads7 using nucleus sampling (p = 0.9) (Holtzman et al., 2019). Blender bot uses beam search with beam size = 10 and min. beam sequence length = 20 to generate responses. 3.2 T OXI C HAT Corpus Statistics We recruited crowd-workers from the Amazon Mechanical Turk platform to annotate the 2000 threads from our corpus, with"
2021.emnlp-main.397,2021.eacl-main.24,0,0.176693,"network presents new risks, as it is difficult to predict when the model might say something toxic, or otherwise harmful. A key challenge for conversational AI is that toxic language is often context-dependent (Dinan et al., 2019a), making it notoriously difficult to detect; text that seems innocuous in isolation may be offensive when considered in the broader context of a conversation. For example, neural chatbots will often agree with offensive statements, which is undesirable (see examples in Figure 1). The solution employed by current systems, such as GPT-3 or Facebook’s Blender chatbot (Roller et al., 2021), is to stop producing output when offensive inputs are detected (Xu et al., 2020). This is problematic, because today’s toxic language classifiers are far 1 Our code and corpus are available at https:// github.com/abaheti95/ToxiChat 2 https://bit.ly/3BKQNSF 4846 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4846–4862 c November 7–11, 2021. 2021 Association for Computational Linguistics from perfect, often generating false positive predictions. Rather than completely shutting down, for some applications, it may be preferable to simply avoid agree"
2021.emnlp-main.397,2020.acl-main.486,1,0.921657,"responses in neural conversation requires both understanding whether a response is offensive and whether it agrees with previous offensive utterances. We develop an interface to annotate these two concepts in conversations that are enriched with dialogue model responses. Formally, a thread consists of k utterances = {u1 , u2 , ..., uk }, where the last comment, uk , is generated by a dialogue model. For each ui , we collect annotations of: 1) Offensiveness - We consider ui offensive if it is intentionally or unintentionally toxic, rude or disrespectful towards a group or individual following Sap et al. (2020). This is a binary choice, where ui is either Offensive or Safe.3 For offensive comments, we further annotate target groups from a predefined list comprising identity-based groups of people (e.g., people of various sexuality/sexualorientation/gender, people with disabilities, people from a specific race, political ideologies, etc.) and specific individuals e.g., (public figures, Reddit users, etc.) We present the list of selected target groups in Figure 7 in the Appendix. 2) Stance - We annotate the stance of ui towards each previous comment, uj , ∀j &lt; i. Stance is viewed as a linguistically a"
2021.emnlp-main.397,N19-1170,0,0.0209143,"xed control tokens (Gururangan et al., 2020). (2) Attribute Conditioning (AT C ON) - In this method, special control to11 Only threads with all safe comments were considered for Stance control attribute. 12 We selected thresholds for all labels such that we get .75 and higher precision. kens encapsulate different response attributes. For example, [OFF] and [SAFE] tokens indicate offensive control attributes. During training, these tokens are prepended to responses and at inference time, they are manually frozen to steer the model’s response towards the desired attribute (Niu and Bansal, 2018; See et al., 2019; Xu et al., 2020). For each CTG experiment, we fine-tune DialoGPTmedium on the train split for 3 epochs and tune hyperparameters using dev set perplexity. Our goal is to test the conversation models in offensive contexts, where they have a propensity to agree with offensive comments, hence, we sample a test set of 500 threads where the last utterance is offensive. Using this test set, our CTG models are compared against DGPT-medium, GPT-3, and Blender in both automatic and human evaluations. 6.2 Automatic Evaluation An ideal dialogue model should have diverse, engaging and safe responses. Thu"
2021.emnlp-main.397,2020.findings-emnlp.291,0,0.120869,"r Safe ?BOT 3: Everyone in the world&apos;s the same Stance Labels Figure 1: Example of an offensive comment by a Reddit user followed by three Dialogue model’s responses. We also show the stance labels for the responses with respect to the preceding offensive comment. Introduction Despite significant progress toward data-driven conversational agents (Ritter et al., 2011; Li et al., 2016), dialogue models still suffer from issues surrounding safety and offensive language. Previous research has shown that dialogue models can produce utterances that are gender and racially biased (Wolf et al., 2017; Sheng et al., 2020; Dinan et al., 2020a). For example, OpenAI’s GPT-3 (Brown et al., 2020), a 175 billion parameter neural network, has been shown to generate dangerous advice, such as recommending a hypothetical patient to kill themselves.2 Presenting users with content generated by a neural network presents new risks, as it is difficult to predict when the model might say something toxic, or otherwise harmful. A key challenge for conversational AI is that toxic language is often context-dependent (Dinan et al., 2019a), making it notoriously difficult to detect; text that seems innocuous in isolation may be of"
2021.emnlp-main.397,2020.acl-main.183,0,0.0169283,"between multiple people on Reddit. U1:u1 U2:u2 ... ”, where u1 , u2 , ... are the user comments. The model then predicts the next turn in the conversation. We select the largest 5 The data was acquired from pushshift.io https://www.reddit.com/r/ AgainstHateSubReddits/ 6 GPT-3 model, ‘davinci’ with 175B parameters, in our data construction. Blender - More recently, Facebook released Blender Bot; a 2.7B parameter dialogue model (Roller et al., 2021). Blender bot is first pretrained on 1.5B Reddit comment threads (Baumgartner et al., 2020) and later finetuned on Blended Skill Talk (BST) dataset (Smith et al., 2020). The BST dataset contains 5K polite conversations between crowdworkers which aims to blend 3 conversational skills into one dataset 1) engaging personality (Zhang et al., 2018b; Dinan et al., 2020b), 2) empathetic dialogue (Rashkin et al., 2019) and 3) knowledge incorporation (Dinan et al., 2019b). We only include the first two models during annotation but compare our controlled text generation models against all three dialogue models in §6.1. Responses for DGPT and GPT-3 are generated on the comments part of the threads7 using nucleus sampling (p = 0.9) (Holtzman et al., 2019). Blender bot u"
2021.emnlp-main.397,D19-1221,0,0.0156866,"ual offensive language using adversarial humanbot conversations, where a human intentionally tries to trick the chatbot into saying something inappropriate. On the other hand, Pavlopoulos et al. (2020); Xenos et al. (2021) created labeled datasets for toxicity detection in single turn conversations and studied context-sensitivity in detection models. In contrast, we study the stance dynamics of dialogue model responses to offensive Reddit conversations with more than one turns. Inappropriate Language Mitigation - Sheng et al. (2020) manipulate training objectives and use adversarial triggers (Wallace et al., 2019) to reduce biases across demographics and generate less negatively biased text overall. Liu et al. (2020) propose adversarial training to reduce gender bias. Dinan et al. (2020a) trains dialogue models with attribute conditioning to mitigate bias by producing genderneutral responses. Saleh et al. (2020) proposes a toxicity classifier-based reinforcement learning objective to discourage the dialogue model from generating inappropriate responses. To enhance safety, Xu et al. (2020) train chatbots to avoid sensitive discussions by changing the topic of the conversation. In contrast, we tackle con"
2021.emnlp-main.409,D19-1371,0,0.314419,"f model. 5 https://github.com/nlplab/brat The average time to annotate each sentence varies across datasets based on the complexity of the text, average length of sentences, etc. P ROCEDURE Corpus Collection. To pre-train our models, we create a novel collection of procedural texts from the same domains as the annotated data in §3, hereinafter referred to as the P ROCE DURE corpus. Specially trained classifiers were used to identify paragraphs describing experimental procedures. For PubMed, a classifier was used to identify paragraphs describing experimental procedures by fine-tuning SciBERT (Beltagy et al., 2019) on the SciSeg dataset (Dasigi et al., 2017), which is annotated with scientific discourse structure, to extract procedures from the Materials and Methods section of 680k articles. For the chemical synthesis domain, the chemical reaction extractor developed by Lowe (2012) was applied to the Description section of 303k patents (174k U.S. and 129k European) we collected from USPTO7 and EPO8 . More details of our data collection process can be found in Appendix B. Cooking recipes are also an important domain for research on procedural text understanding, therefore we include the text component of"
2021.emnlp-main.409,W06-1615,0,0.226306,"urrett (2019). The state of an ingredient in each cooking step is correct if it matches with the gold labels, as either present or absent. Results. Test set results of eight pre-trained language models on six procedural text datasets are presented in Table 7.20 ProcBERT, performs best in most tasks and even achieves the state-off-the-art performance on operational argument role labeling (""Core"" and ""Non-Core"") of XWLP, showing the effectiveness of in-domain pre-training. 7 Conclusion In this paper, we address a number of questions related to the costs of adapting an NLP model to a new domain (Blitzer et al., 2006; Han and Eisenstein, 2019), an important and well-studied problem in NLP. We frame domain adaptation under a constrained budget as a problem of consumer choice. Experiments are conducted using several pre-trained models in three procedural text domains to determine when it is economical to pre-train indomain transformers (Gururangan et al., 2020), and when it is better to spend available resources on annotation. Our results suggest that when a small number of NLP models need to be adapted to a new domain, pre-training, by itself, is not an economical solution. For C H EMU, gold arguments are"
2021.emnlp-main.409,2020.acl-main.194,0,0.0155624,"TPUs to pre-train learning and unsupervised domain adaptation is large in-domain language models. In this paper, we that labeled data is expensive; therefore, train- empirically study the best strategy for adapting to ing on a combination of labeled and unlabeled a new domain given a fixed budget. data is an economical approach to improve perWe view the NLP practitioner’s dilemma of how formance when adapting to a new domain (Blum to adapt to a new domain as a problem of consumer and Mitchell, 1998; Daume III and Marcu, 2006; choice, a classical problem in microeconomics Hoffman et al., 2018; Chen et al., 2020). Recent (Becker, 1965; Lancaster, 1966). As illustrated in work has shown that pre-training in-domain Trans- Figure 1, the NLP practitioner (consumer) can obformers is an effective method for unsupervised tain Xa annotated documents (by hiring annotators) adaptation (Han and Eisenstein, 2019; Wright and at a cost of Ca each, and Xp hours of pre-training Augenstein, 2020) and even boosts performance (by renting GPUs or TPUs) at a cost of Cp per hour. 1 Given a fixed budget B, the consumer may choose Our code and data are publicly available on Github: https://github.com/bflashcp3f/ProcBERT. any"
2021.emnlp-main.409,P07-1033,0,0.3889,"Missing"
2021.emnlp-main.409,N19-1423,0,0.540544,"producible scientific experiments, yet few annotated datasets currently exist in these domains. Furthermore, annotation of scientific procedures is not easily amenable to crowdsourcing, making this an ideal testbed for pretraining-based domain adaptation. We measure the cost of in-domain pre-training on a large collection of unlabeled procedural texts using Google’s Cloud TPUs.2 Model performance is then evaluated under varying budget constraints in six source and target domain combinations. Our analysis suggests that given current costs of pre-training large Transformer models, such as BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019), in-domain data annotation should always be part of an economical strategy when adapting a single NLP system to a new domain. For small budgets (e.g. less than $800 USD), spending all funds on annotation is the best policy; however, as more funding becomes available, a combination of pre-training and annotation is the best choice. This paper addresses a specific question that is often faced by NLP practitioners working on applications: what is the most economical approach to adapt an NLP system to a new domain when no pre-trained models or task-annotated datase"
2021.emnlp-main.409,D19-1070,0,0.012125,"410.1 84.540.8 81.750.4 82.920.3 WLP NER RE BERTbase BERTlarge RoBERTabase RoBERTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including"
2021.emnlp-main.409,2020.acl-main.740,0,0.0420387,"Missing"
2021.emnlp-main.409,D19-1433,0,0.121017,"xed budget. data is an economical approach to improve perWe view the NLP practitioner’s dilemma of how formance when adapting to a new domain (Blum to adapt to a new domain as a problem of consumer and Mitchell, 1998; Daume III and Marcu, 2006; choice, a classical problem in microeconomics Hoffman et al., 2018; Chen et al., 2020). Recent (Becker, 1965; Lancaster, 1966). As illustrated in work has shown that pre-training in-domain Trans- Figure 1, the NLP practitioner (consumer) can obformers is an effective method for unsupervised tain Xa annotated documents (by hiring annotators) adaptation (Han and Eisenstein, 2019; Wright and at a cost of Ca each, and Xp hours of pre-training Augenstein, 2020) and even boosts performance (by renting GPUs or TPUs) at a cost of Cp per hour. 1 Given a fixed budget B, the consumer may choose Our code and data are publicly available on Github: https://github.com/bflashcp3f/ProcBERT. any combination that fits within the budget con5002 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5002–5015 c November 7–11, 2021. 2021 Association for Computational Linguistics straint Xa Ca + Xp CP ≤ B. The goal is to choose a combination that ma"
2021.emnlp-main.409,2021.emnlp-main.831,0,0.0327949,"IsnL0O7WnEvK9Xbi3KtnsdRBMfgBJwBF1yBGrgBTdACGDyCZ/AK3qwn68V6tz7mowUr3zkEf2R9/gAacpTB</latexit&gt; Number of Pretraining Hours (TPUs) Figure 1: We view domain adaptation as a consumer choice problem (Becker, 1965; Lancaster, 1966). The NLP practitioner (consumer) is faced with the problem of choosing an optimal combination of annotation and pre-training under a constrained budget. This figure is purely for illustration and is not based on experimental data. when large quantities of in-domain data are available (Gururangan et al., 2020). However, modern pre-training methods incur substantial costs (Izsak et al., 2021), and generate carbon emissions (Strubell et al., 2019; Schwartz et al., 2020; Bender et al., 2021). This raises an important question: given a fixed budget to improve a model’s performance, what steps should an NLP practitioner take? On one hand, they could hire annotators to label 1 Introduction in-domain task-specific data, while on the other, The conventional wisdom on semi-supervised they could buy or rent GPUs or TPUs to pre-train learning and unsupervised domain adaptation is large in-domain language models. In this paper, we that labeled data is expensive; therefore, train- empirically"
2021.emnlp-main.409,D16-1032,0,0.0151893,"an benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including trigger detection and argument role labeling. The XWLP corpus (Tamari et al., 2021) provides the Process Event Graphs (PEG) of 279 wet-lab biochemistry protocols. The PEG is a document-level graph-based representation specifying the involved experimental objects. The R ECIPE corpus (Kiddon et al., 2016) includes annotation of entity states for 866 cooking recipes. It supports Entity Tracking (ET) task which predicts whether or not a specific ingredient is involved in each step of the recipe. 6.2 Experiments on Ancillary Tasks chitecture of Gupta and Durrett (2019). The state of an ingredient in each cooking step is correct if it matches with the gold labels, as either present or absent. Results. Test set results of eight pre-trained language models on six procedural text datasets are presented in Table 7.20 ProcBERT, performs best in most tasks and even achieves the state-off-the-art perform"
2021.emnlp-main.409,C16-1038,0,0.0184099,"ts the target, and the third is domain-independent. These contextualized vectors are then concatenated and fed into a linear layer that is 3 times as large as the base model’s. When encoding data from a specific domain (e.g. C HEM S YN), the other domain’s representations are zeroed out (1/3 of the new representations will always be 0.0). This enables the domain-specific block of the linear layer to encode information specific to that domain, while the domain-independent parameters can learn to represent information that transfers across domains. This is similar to prior work using EasyAdapt (Kim et al., 2016) for LSTMs. As we have three procedural text datasets (§3) annotated with entities and relations, we can experiment with six source ⇒ target adaptation settings. For each domain pair, we compare five different 5.4 Experimental Results and Analysis pre-trained language models when adapted to the procedural text domain under varying budgets. We present the test set NER and RE results with five Based on the estimations of the annotation costs annotation and pre-training combination strategies Ca (§3) and pre-training costs Cp (§4), we con- under six domain adaptation settings in Table 4 and duct"
2021.emnlp-main.409,2020.wnut-1.40,0,0.0153524,"ER RE BERTbase BERTlarge RoBERTabase RoBERTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including trigger detection and argument ro"
2021.emnlp-main.409,P19-1355,0,0.124699,"qwn68V6tz7mowUr3zkEf2R9/gAacpTB</latexit&gt; Number of Pretraining Hours (TPUs) Figure 1: We view domain adaptation as a consumer choice problem (Becker, 1965; Lancaster, 1966). The NLP practitioner (consumer) is faced with the problem of choosing an optimal combination of annotation and pre-training under a constrained budget. This figure is purely for illustration and is not based on experimental data. when large quantities of in-domain data are available (Gururangan et al., 2020). However, modern pre-training methods incur substantial costs (Izsak et al., 2021), and generate carbon emissions (Strubell et al., 2019; Schwartz et al., 2020; Bender et al., 2021). This raises an important question: given a fixed budget to improve a model’s performance, what steps should an NLP practitioner take? On one hand, they could hire annotators to label 1 Introduction in-domain task-specific data, while on the other, The conventional wisdom on semi-supervised they could buy or rent GPUs or TPUs to pre-train learning and unsupervised domain adaptation is large in-domain language models. In this paper, we that labeled data is expensive; therefore, train- empirically study the best strategy for adapting to ing on a comb"
2021.emnlp-main.409,2020.wnut-1.33,1,0.80909,"Missing"
2021.emnlp-main.409,2021.eacl-main.187,1,0.774105,"75.040.8 73.771.6 75.480.7 74.890.6 95.70 80.620.7 81.530.5 83.410.1 84.540.8 81.750.4 82.920.3 WLP NER RE BERTbase BERTlarge RoBERTabase RoBERTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extrac"
2021.emnlp-main.409,2020.emnlp-main.639,0,0.04382,"Missing"
2021.emnlp-main.409,D19-1005,0,0.0220881,"Missing"
2021.emnlp-main.409,2020.wnut-1.38,0,0.0210938,"RTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including trigger detection and argument role labeling. The XWLP corpus (Tamari et a"
2021.emnlp-main.459,2020.acl-main.560,0,0.04747,"election that uses the finetuned model’s own internal representations to predict its cross-lingual capabilities. In extensive experiments we find that this method consistently selects better models than English validation data across twenty five languages (including eight low-resource languages), and often achieves results that are comparable to model selection using target language development data.1 1 Introduction Pre-trained Transformers (Vaswani et al., 2017; Devlin et al., 2019) have achieved state-of-the-art results on a range of NLP tasks, often approaching human inter-rater agreement (Joshi et al., 2020a). These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons (Wu and Dredze, 2019; Pires et al., 2019). 1 Our code and data is available at: https://github. com/edchengg/model_selection In the zero-shot transfer learning, training and development data are only assumed in a high resource source language (e.g. English), and performance is evaluated on another target language. Because no target language annotations are assumed, source language data is typically used to select among models that are fine-tu"
2021.emnlp-main.459,2020.emnlp-main.40,0,0.0372264,"Pires et al., 2019). 1 Our code and data is available at: https://github. com/edchengg/model_selection In the zero-shot transfer learning, training and development data are only assumed in a high resource source language (e.g. English), and performance is evaluated on another target language. Because no target language annotations are assumed, source language data is typically used to select among models that are fine-tuned with different hyperparameters and random seeds. However, recent work has shown that English dev accuracy does not always correlate well with target language performance (Keung et al., 2020). In this paper, we propose an alternative strategy for model selection in zero-shot transfer. Our approach, dubbed Learned Model Selection (LMS), learns a function that scores the compatibility between a fine-tuned multilingual Transformer, and a target language. The compatibility score is calculated based on features of the multilingual model’s learned representations. This is done by aggregating representations over an unlabeled target language text corpus after fine-tuning on source language data. We show that these modelspecific features effectively capture information about how the cross"
2021.emnlp-main.459,D12-1091,0,0.0759363,"hest F1 of English dev set / Pivot language dev set (pivot language in bracket) / 100 target language dev set examples / target language dev set. LMS: model selection based on the highest scores for the target language: arg maxm s(m, ltarget ); “# All-Target” is the number of labeled target-language sentences used for model selection in the All-Target oracle. Bold / underlined indicates the best / second best. AVG En-Dev∆: average differences with En-Dev baseline. Significance compared to the En-Dev is indicated with ∗∗ (p < 0.05) – all tests are computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012). better on Arabic (ar). We use (Lewis et al., 2020) as references for zero-shot cross-lingual transfer with mBERT. ARL and RE In Table 3, our method selects models with higher F1 scores compared to En-Dev. It also outperforms 100-Target on Arabic. We hypoth5679 esize this is because 100 target-language examples is not sufficient for effective model selection, as the dataset contains a large proportion of negative examples (no relation). Also, RE and ARL have large label sizes (18 and 35) so a random sample of 100 instances might not cover every label. In contrast, the full dev set contains th"
2021.emnlp-main.459,Q17-1010,0,0.0166835,"Missing"
2021.emnlp-main.459,N18-1202,0,0.0253278,"glang2vec (l) is the lang2vec representation of language l. The model and language features are each passed through a feed-forward neural network and then combined using a bilinear layer to calculate a final score as follows: The zero-shot setting considered in this paper works as follows. A Transformer model is first pretrained using a standard masked language model s(m, l) = f (gmBERT (m), glang2vec (l)) objective. The only difference from the mono= FFNN(gmBERT (m))T Wbi FFNN(glang2vec (l)) lingual approach to contextual word representaUsing the above score, we can represent the probtions (Peters et al., 2018; Devlin et al., 2019) is ability that model mi performs better than mj on the pre-training corpus, which contains text writlanguage l: ten in multiple languages; for example, mBERT is trained on Wikipedia data from 104 languages. P (mi .l mj ) = σ(s(mi , l) − s(mj , l)) 5676 where σ(·) is the sigmoid function. To tune the parameters of the scoring function, which include the feed-forward and bilinear layers, we minimize cross-entropy loss: X X X C= −Cmi ,mj ,l (1) l∈L{ltarget } mi ∈M mj ∈M where Cmi ,mj ,l = 1[mi .l mj ] log P (mi .l mj ) +1[mj .l mi ] log P (mj .l mi ) Here 1[mj .l mi ] is"
2021.emnlp-main.459,2020.emnlp-main.617,0,0.0592229,"Missing"
2021.emnlp-main.459,P19-1493,0,0.101347,"els than English validation data across twenty five languages (including eight low-resource languages), and often achieves results that are comparable to model selection using target language development data.1 1 Introduction Pre-trained Transformers (Vaswani et al., 2017; Devlin et al., 2019) have achieved state-of-the-art results on a range of NLP tasks, often approaching human inter-rater agreement (Joshi et al., 2020a). These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons (Wu and Dredze, 2019; Pires et al., 2019). 1 Our code and data is available at: https://github. com/edchengg/model_selection In the zero-shot transfer learning, training and development data are only assumed in a high resource source language (e.g. English), and performance is evaluated on another target language. Because no target language annotations are assumed, source language data is typically used to select among models that are fine-tuned with different hyperparameters and random seeds. However, recent work has shown that English dev accuracy does not always correlate well with target language performance (Keung et al., 2020)."
2021.emnlp-main.459,P19-1015,0,0.0180427,", eventargument role labeling (ARL), and named entity recognition (NER). In total, we cover 25 target languages including 8 low-resource languages in our experiments following prior work (shown in Table 1). We adopt the best performing model from Soares et al. (2019), [E NTITY M ARKERS - E N TITY S TART ], for RE and ARL. For other tasks, we use established task-specific layers and evaluation protocols, following the references in Table 1. Labeled training data for each task is assumed in English and trained models are evaluated on each target language. 4.1 Low-resource Languages partition of Rahimi et al. (2019) is used following the XTREME benchmark’s NER setup (Hu et al., 2020). The related language used for the PivotDev baseline is chosen following Xia et al. (2021), which is based on LangRank (Lin et al., 2019). 5 Experimental Design For a multilingual NLP task with n languages L : {l1 , ..., ln }, our goal is to select the model that performs best on a new target language, ltarget 6∈ L. We assume the available resources are English training and development data, in addition to a small development set for each of the pivot languages, L. First, a set of mBERT models, M , are finetuned on an Englis"
2021.emnlp-main.459,W02-2024,0,0.568998,"Missing"
2021.emnlp-main.459,P19-1279,0,0.0162597,"guages, models are ranked based on their scores for the target language, and the highest scoring model, m ˆ = arg maxm s(m, ltarget ), is selected. 4 Tasks and Datasets We perform model selection experiments on five well-studied NLP tasks in the zero-shot transfer setting: part-of-speech (POS) tagging, question answering (QA), relation extraction (RE), eventargument role labeling (ARL), and named entity recognition (NER). In total, we cover 25 target languages including 8 low-resource languages in our experiments following prior work (shown in Table 1). We adopt the best performing model from Soares et al. (2019), [E NTITY M ARKERS - E N TITY S TART ], for RE and ARL. For other tasks, we use established task-specific layers and evaluation protocols, following the references in Table 1. Labeled training data for each task is assumed in English and trained models are evaluated on each target language. 4.1 Low-resource Languages partition of Rahimi et al. (2019) is used following the XTREME benchmark’s NER setup (Hu et al., 2020). The related language used for the PivotDev baseline is chosen following Xia et al. (2021), which is based on LangRank (Lin et al., 2019). 5 Experimental Design For a multilingu"
2021.emnlp-main.459,D19-1030,0,0.0314633,"Missing"
2021.emnlp-main.459,D19-1077,0,0.0815976,"ly selects better models than English validation data across twenty five languages (including eight low-resource languages), and often achieves results that are comparable to model selection using target language development data.1 1 Introduction Pre-trained Transformers (Vaswani et al., 2017; Devlin et al., 2019) have achieved state-of-the-art results on a range of NLP tasks, often approaching human inter-rater agreement (Joshi et al., 2020a). These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons (Wu and Dredze, 2019; Pires et al., 2019). 1 Our code and data is available at: https://github. com/edchengg/model_selection In the zero-shot transfer learning, training and development data are only assumed in a high resource source language (e.g. English), and performance is evaluated on another target language. Because no target language annotations are assumed, source language data is typically used to select among models that are fine-tuned with different hyperparameters and random seeds. However, recent work has shown that English dev accuracy does not always correlate well with target language performance"
2021.emnlp-main.459,2021.naacl-main.42,0,0.43803,"ts following prior work (shown in Table 1). We adopt the best performing model from Soares et al. (2019), [E NTITY M ARKERS - E N TITY S TART ], for RE and ARL. For other tasks, we use established task-specific layers and evaluation protocols, following the references in Table 1. Labeled training data for each task is assumed in English and trained models are evaluated on each target language. 4.1 Low-resource Languages partition of Rahimi et al. (2019) is used following the XTREME benchmark’s NER setup (Hu et al., 2020). The related language used for the PivotDev baseline is chosen following Xia et al. (2021), which is based on LangRank (Lin et al., 2019). 5 Experimental Design For a multilingual NLP task with n languages L : {l1 , ..., ln }, our goal is to select the model that performs best on a new target language, ltarget 6∈ L. We assume the available resources are English training and development data, in addition to a small development set for each of the pivot languages, L. First, a set of mBERT models, M , are finetuned on an English training set using different hyperparameters and random seeds and shuffled into meta-train/dev/test sets. We then evaluate each model, mi , on the pivot langu"
2021.emnlp-main.459,D18-1034,0,0.018948,"Data 0.6 0.7 (a) English Development Selection mBERT [CLS] Scores 0.5 Scoring Function Unlabeled Target Language Data - 0.2 - 0.1 0.6 After pre-training, the resulting network encodes language-independent representations that support surprisingly effective cross-lingual transfer, simply by fine-tuning with English data. For example, after fine-tuning mBERT using the English portion of the CoNLL Named Entity Recognition dataset, the resulting model can perform inference directly on Spanish text, achieving an F1 score around 75, and outperforming prior work using cross-lingual word embeddings (Xie et al., 2018; Mikolov et al., 2013). A challenge, however, is the relatively high variance across multiple training runs. Although mean F1 on Spanish is 75, the performance of 60 fine-tuned models with different learning rates and random seeds ranges from around 70 F1 to 78. In zero-shot learning, no validation/development data is available in the target language, motivating the need for a machine learning approach to model selection. 3 (b) Learned Model Selection (LMS) Figure 1: An illustration of our approach to select the best model for zero-shot cross-lingual transfer. (a) Prior work selects the best"
2021.emnlp-main.459,P19-1130,0,0.0349999,"Missing"
C12-1177,P05-1074,0,0.0667073,"Missing"
C12-1177,N03-1003,0,0.0552004,"Missing"
C12-1177,D08-1021,0,0.0816547,"Missing"
C12-1177,C08-1013,0,0.0540476,"Missing"
C12-1177,C96-2183,0,0.047339,"Missing"
C12-1177,P11-1020,0,0.164662,"Missing"
C12-1177,P09-1053,0,0.0276086,"Missing"
C12-1177,C04-1051,1,0.720944,"Missing"
C12-1177,C04-1088,0,0.0176159,"Missing"
C12-1177,W07-1401,1,0.326763,"Missing"
C12-1177,D10-1051,0,0.0161918,"Missing"
C12-1177,P07-2045,0,0.0606319,"Missing"
C12-1177,N10-1017,0,0.0920108,"Missing"
C12-1177,D10-1090,0,0.0393625,"Missing"
C12-1177,J10-3003,0,0.0626954,"Missing"
C12-1177,moore-2002-fast,0,0.0606931,"Missing"
C12-1177,J03-1002,0,0.0531454,"Missing"
C12-1177,P02-1040,0,0.110879,"Missing"
C12-1177,W04-3219,0,0.0160521,"Missing"
C12-1177,P10-2008,0,0.0191379,"Missing"
C12-1177,W03-1609,0,0.0609861,"Missing"
C12-1177,D08-1027,0,0.0349755,"Missing"
C12-1177,N10-1056,0,0.0714576,"Missing"
C12-1177,P03-1021,0,\N,Missing
D08-1002,P08-1004,1,0.273221,"ons We report on the AU C ONTRAIRE CD system, which addresses each of the above challenges. First, AU C ONTRAIRE identifies “functional phrases” statistically (Section 3). Second, AU C ONTRAIRE uses these phrases to automatically create a large corpus of apparent contradictions (Section 4.2). Finally, AU C ONTRAIRE sifts through this corpus to find genuine contradictions using knowledge about synonymy, meronymy, argument types, and ambiguity (Section 4.3). Instead of analyzing sentences directly, AU C ON TRAIRE relies on the T EXT RUNNER Open Information Extraction system (Banko et al., 2007; Banko and Etzioni, 2008) to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them (e.g., was born in(Mozart,Salzburg)). Using extracted tuples greatly simplifies the CD task, because numerous syntactic problems (e.g., anaphora, relative clauses) and semantic challenges (e.g., quantification, counterfactuals, temporal qualification) are 1 Although we focus on function-based CD in our case study, we believe that our observations apply to other types of CD as well. 12 delegated to T EXT RUNNER or simply ignored. Nevertheless, extracted tuples are a conven"
D08-1002,W03-0906,0,0.0194348,"Missing"
D08-1002,P08-1118,0,0.329103,"Missing"
D08-1002,W07-1431,0,0.0150601,"Etzioni, 2007)— a system that identifies synonyms from T EXT RUNNER extractions. Additionally, AU C ONTRAIRE uses edit-distance and tokenbased string similarity (Cohen et al., 2003) between apparently contradictory values of y to identify synonyms. Meronyms: For some relations, there is no contradiction when y1 and y2 share a meronym, i.e. “part of” relation. For example, in the set born in(Mozart,·) there is no contradiction between the y values “Salzburg” and “Austria”, but “Salzburg” conflicts with “Vienna”. Although this is only true in cases where y occurs in an upward monotone context (MacCartney and Manning, 2007), in practice genuine contradictions between 16 y-values sharing a meronym relationship are extremely rare. We therefore simply assigned contradictions between meronyms a probability close to zero. We used the Tipster Gazetteer4 and WordNet to identify meronyms, both of which have high precision but low coverage. Argument Typing: Two y values are not contradictory if they are of different argument types. For example, the relation born in can take a date or a location for the y value. While a person can be born in only one year and in only one city, a person can be born in both a year and a cit"
D08-1002,P08-1008,0,0.13176,"003) first proposed contradiction detection as an important NLP task, and Harabagiu et al. (2006) were the first to report results on contradiction detection using negation, although their evaluation corpus was a balanced data set built by manually negating entailments in a data set from the Recognizing Textual Entailment conferences (RTE) (Dagan et al., 2005). De Marneffe et al. (2008) reported experimental results on a contradiction corpus created by annotating the RTE data sets. RTE-3 included an optional task, requiring systems to make a 3-way distinction: {entails, contradicts, neither} (Voorhees, 2008). The average performance for contradictions on the RTE-3 was precision 0.11 at recall 0.12, and the best system had precision 0.23 at recall 0.19. We did not run AU C ON TRAIRE on the RTE data sets because they contained 19 Conclusions and Future Work We have described a case study of contradiction detection (CD) based on functional relations. In this context, we introduced and evaluated the AU C ON TRAIRE system and its novel EM-style algorithm for determining whether an arbitrary phrase is functional. We also created a unique “natural” data set of seeming contradictions based on sentences d"
D08-1002,N07-1016,1,0.819609,"ion, in order to estimate the probability that a given pair, {R(x, y1 ), R(x, y2 )} is a genuine contradiction. Synonyms: The set of potential contradictions died from(Mozart,·) may contain assertions that Mozart died from renal failure and that he died from kidney failure. These are distinct values of y, but do not contradict each other, as the two terms are synonyms. AU C ONTRAIRE uses a variety of knowledge sources to handle synonyms. WordNet is a reliable source of synonyms, particularly for common nouns, but has limited recall. AU C ONTRAIRE also utilizes synonyms generated by R ESOLVER (Yates and Etzioni, 2007)— a system that identifies synonyms from T EXT RUNNER extractions. Additionally, AU C ONTRAIRE uses edit-distance and tokenbased string similarity (Cohen et al., 2003) between apparently contradictory values of y to identify synonyms. Meronyms: For some relations, there is no contradiction when y1 and y2 share a meronym, i.e. “part of” relation. For example, in the set born in(Mozart,·) there is no contradiction between the y values “Salzburg” and “Austria”, but “Salzburg” conflicts with “Vienna”. Although this is only true in cases where y occurs in an upward monotone context (MacCartney and"
D11-1054,P05-1018,0,0.0133253,"nerating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004)"
D11-1054,W10-0733,0,0.0758945,"Missing"
D11-1054,J90-2002,0,0.0683599,"ether in the training data. Because there is a wide range of acceptable responses to any status, these identical pairs have the strongest associations in the data, and therefore dominate the phrase table. In order to discourage lexically similar translations, we filter out all phrase-pairs where one phrase is a substring of the other, and introduce a novel feature to penalize lexical similarity: φlex (s, t) = J(s, t) Where J(s, t) is the Jaccard similarity between the set of words in s and t. 4.2 Challenge: Word Alignment Alignment is more difficult in conversational data than bilingual data (Brown et al., 1990), or textual entailment data (Brockett, 2006; MacCartney et al., 2008). In conversational data, there are some cases in which there is a decomposable alignment between . . . . . . . .         . . . . . . . . .         . . . . . . . . .         . question . . . . . . . . . . . . . . . .  easier please . if anyones still awake lets play a game. name 3 kevin costner movies that dont suck . Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult c"
D11-1054,W09-0401,0,0.0484964,"Missing"
D11-1054,W04-2302,0,0.0128491,"plate rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including"
D11-1054,C04-1051,0,0.0105841,"t discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, howev"
D11-1054,P03-1003,0,0.00509224,"vestigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million"
D11-1054,P08-1095,0,0.00653504,"ering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a status message). For the purposes of this paper, we limit the data set to only the first two utterances from each conversation. As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts. 4 Response Generation as Translation When applied to conversations, SMT models the probability of a response r given the input statuspost s using a log-linear combination of feat"
D11-1054,W03-2506,0,0.0113476,"s posts. Note that we make no mention of context, intent or dialogue state; our goal is to generate any response that fits the provided stimulus; however, we do so without employing rules or templates, with the hope of creating a system that is both flexible and extensible when operating in an open domain. Success in open domain response generation could be immediately useful to social media platforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the followi"
D11-1054,D07-1103,0,0.0466526,"pared to a dataset of aligned French-English sentence pairs (the WMT 08 news commentary data) where the average number of intersection alignments is 14. Direct Phrase Pair Extraction C(s, t) C(¬s, t) C(t) C(s, ¬t) C(¬s, ¬t) N − C(t) C(s) N − C(s) N Figure 3: Contingency table for phrase pair (s,t). Fisher’s Exact Test estimates the probability of seeing this event, or one more extreme assuming s and t are independent. ing whether its phrases form a valid mapping. We consider all possible phrase-pairs in the training data,1 then use Fisher’s Exact Test to filter out pairs with low correlation (Johnson et al., 2007). Given a source and target phrase s and t, we consider the contingency table illustrated in figure 3, which includes co-occurrence counts for s and t, the number of sentence-pairs containing s, but not t and vice versa, in addition to the number of pairs containing neither s nor t. Fisher’s Exact Test provides us with an estimate of the probability of observing this table, or one more extreme, assuming s and t are independent; in other words it gives us a measure of how strongly associated they are. In contrast to statistical tests such as χ2 , or the G2 Log Likelihood Ratio, Fisher’s Exact T"
D11-1054,P95-1034,0,0.012965,"ponses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ord"
D11-1054,D08-1084,0,0.0102901,"Missing"
D11-1054,W04-3243,0,0.0261509,"to statistical tests such as χ2 , or the G2 Log Likelihood Ratio, Fisher’s Exact Test produces accurate p-values even when the expected counts are small (as is extremely common in our case). In Fisher’s Exact Test, the hypergeometric probability distribution is used to compute the exact probability of a particular joint frequency assuming a model of independence: C(s)!C(¬s)!C(t)!C(¬t)! N !C(s, t)!C(¬s, t)!C(s, ¬t)!C(¬s, ¬t)! The statistic is computed by summing the probability for the joint frequency in Table 3, and every more extreme joint frequency consistent with the marginal frequencies. Moore (2004) illustrates several tricks which make this computation feasible in practice. We found that this approach generates phrasetable entries which appear quite reasonable upon manual inspection. The top 20 phrase-pairs (after filtering out identical source/target phrases, substrings, Because word alignment in status/response pairs is 1 We define a possible phrase-pair as any pair of phrases a difficult problem, instead of relying on local align- found in a sentence-pair from our training corpus, where both ments for extracting phrase pairs, we exploit infor- phrases consist of 4 tokens or fewer. Th"
D11-1054,J03-1002,0,0.0503241,". . . . . . . . . . . .  easier please . if anyones still awake lets play a game. name 3 kevin costner movies that dont suck . Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult cases where alignment between large phrases is required, for example figure 2. These difficult sentence pairs confuse the IBM word alignment models which have no way to distinguish between the easy and hard cases. We aligned words in our parallel data using the widely used tool GIZA++ (Och and Ney, 2003); however, the standard growing heuristic resulted in very noisy alignments. Precision could be improved considerably by using the intersection of GIZA++ trained in two directions (s → r, and r → s), but the alignment also became extremely sparse. The average number of alignments-per status/response pair in our data was only 1.7, as compared to a dataset of aligned French-English sentence pairs (the WMT 08 news commentary data) where the average number of intersection alignments is 14. Direct Phrase Pair Extraction C(s, t) C(¬s, t) C(t) C(s, ¬t) C(¬s, ¬t) N − C(t) C(s) N − C(s) N Figure 3: Con"
D11-1054,P03-1021,0,0.0439363,"arry cream you ? morning norris movie miss you too i ’m happy birthday good luck it was i miss flu love you too are you ? i did michael i ’m good mj We do not use any form of SMT reordering model, as the position of the phrase in the response does not seem to be very correlated with the corresponding position in the status. Instead we let the language model drive reordering. We used the default feature weights provided by Moses.4 Because automatic evaluation of response generation is an open problem, we avoided the use of discriminative training algorithms such as Minimum Error-Rate Training (Och, 2003). 5 One straightforward data-driven approach to response generation is nearest neighbour, or information retrieval. This general approach has been applied previously by several authors (Isbell et al., 2000; Swanson and Gordon, 2008; Jafarpour and Burges, 2010), and is used as a point of comparison in our experiments. Given a novel status s and a training corpus of status/response pairs, two retrieval strategies can be used to return a best response r0 : Table 1: Top 20 Phrase Pairs ranked by the Fisher Exact Test statistic. Slight variations (substrings or symmetric pairs) were removed to show"
D11-1054,P02-1040,0,0.11072,"ul in this conversational setting. Finally, as an additional baseline, we compared M T-C HAT’s output to random responses selected from those observed 2 or more times in the training data. One might argue that short, common responses are very general, and that a reply like “lol” could be considered a good response to almost any status. However, the human evaluation shows a clear preference for M T-C HAT’s output: raters favour responses that are tailored to the stimulus. 6.3 Automatic Evaluation The field of SMT has benefited greatly from the existence of an automatic evaluation metric, BLEU (Papineni et al., 2002), which grades an output candidate according to n-gram matches to one or more reference outputs. To evaluate whether BLEU is an appropriate automatic evaluation measure for response generation, we attempted to measure its agreement with the human judgments. We calculate BLEU using a single reference derived from our parallel corpus. We show the smoothed BLEU 1-4 scores for each system on each dataset evaluated in Table 4. Although these scores are extremely low, the overall BLEU scores agree with overall annotator judgments in all cases except when comparing M T-C HAT and I R -R ESPONSE. It wo"
D11-1054,W04-3219,1,0.21248,"Missing"
D11-1054,H01-1055,0,0.0615326,"which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to"
D11-1054,A00-2026,0,0.0223351,"ikh et al., 2010), which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might a"
D11-1054,P07-1059,0,0.00589638,"on based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API"
D11-1054,N10-1020,1,0.362331,"ong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a stat"
D11-1054,W10-2708,0,0.0332719,"Missing"
D11-1054,D08-1027,0,0.0254844,"Missing"
D11-1054,P06-2103,0,0.00920762,"knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so u"
D11-1054,P10-1028,0,0.0462903,"der to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twit"
D11-1054,N09-1023,0,0.0165476,"same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a status message). For the purposes of this paper, we limit the data set to only the first two utterances from each conversation. As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts. 4 Response Generation as Translation When applied to conversations, SMT models the probability of a response r given the input statuspost s using a log-linear combination of feature functions. Most pr"
D11-1054,N06-1056,0,0.00410199,"tate into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010;"
D11-1054,N07-1022,0,0.0183228,"latforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances (Hobbs, 1985). For example, consider the stimulusresponse pair from the data: Stimulus: I’m slowly making this soup ...... and it smells gorgeous! Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583–593, c Edinburgh, Scot"
D11-1054,P07-2045,0,\N,Missing
D11-1054,J08-1001,0,\N,Missing
D11-1054,J08-4004,0,\N,Missing
D11-1054,J05-4004,0,\N,Missing
D11-1054,D08-1076,0,\N,Missing
D11-1141,P11-1040,0,0.697784,"Missing"
D11-1141,J92-4003,0,0.0910564,"Missing"
D11-1141,W99-0613,0,0.0450456,"l release the [Nintendo]ORG 3DS in north [America]LOC march 27 for $250 7 http://www.chasen.org/˜taku/software/ TinySVM/ The OOV word ‘Yess’ is mistaken as a named entity. In addition, although the first occurrence of ‘Nintendo’ is correctly segmented, it is misclassified, whereas the second occurrence is improperly segmented – it should be the product “Nintendo 3DS”. Finally “north America” should be segmented as a LOCATION, rather than just ‘America’. In general, news-trained Named Entity Recognizers seem to rely heavily on capitalization, which we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most"
D11-1141,N09-1019,0,0.0205718,"C march 27 for $250 7 http://www.chasen.org/˜taku/software/ TinySVM/ The OOV word ‘Yess’ is mistaken as a named entity. In addition, although the first occurrence of ‘Nintendo’ is correctly segmented, it is misclassified, whereas the second occurrence is improperly segmented – it should be the product “Nintendo 3DS”. Finally “north America” should be segmented as a LOCATION, rather than just ‘America’. In general, news-trained Named Entity Recognizers seem to rely heavily on capitalization, which we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most words found in tweets are not part of an entit"
D11-1141,W10-0713,0,0.605816,"Missing"
D11-1141,P05-1045,0,0.09483,"ue to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’s distribution of possible types is shared across its mentions. 5 Conclusions We have demonstrated that existing tools for POS tagging, Chunking and Named Entity Recognition perform quite poorly when applied to Tweets. To address this challenge we have annotated tweets and built tools trained on unlabeled, in-domain and outof-domain data, showing substantial improvement"
D11-1141,W02-2010,0,0.0537015,"Missing"
D11-1141,P11-2008,0,0.750872,"Missing"
D11-1141,W11-0704,0,0.045433,"yles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Amazon’s Mechanical Turk for annotating Named Entities in Twitter, Minkov et. al. (2005) investigate person name recognizers in email, and Singh et. al. (2010) apply a minimally supervised approach to extracting entities from text advertisements. In contrast to previous work, we have demonstrated the utility of features based on Twitterspecific POS taggers and Shallow Parsers in segmenting Named Entiti"
D11-1141,P11-1038,0,0.693265,"tter or similar text styles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Amazon’s Mechanical Turk for annotating Named Entities in Twitter, Minkov et. al. (2005) investigate person name recognizers in email, and Singh et. al. (2010) apply a minimally supervised approach to extracting entities from text advertisements. In contrast to previous work, we have demonstrated the utility of features based on Twitterspecific POS taggers and Shallow Parsers in se"
D11-1141,C08-1056,0,0.0175354,"Missing"
D11-1141,N10-1087,0,0.0117909,"y annotated data to be very beneficial for named entity segmentation, we were motivated to explore approaches that don’t rely on manual labels for classification due to Twitter’s wide range of named entity types. Additionally, unlike previous work on NER in informal text, our approach allows the sharing of information across an entity’s mentions which is quite beneficial due to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’"
D11-1141,P11-1037,0,0.762242,"End to End System: Finally we present the end to end performance on segmentation and classification (T- NER) in Table 12. We observe that T- NER again outperforms co-training. Moreover, comparing against the Stanford Named Entity Recognizer on the 3 MUC types, T- NER doubles F1 score. 4 Related Work There has been relatively little previous work on building NLP tools for Twitter or similar text styles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Ama"
D11-1141,D10-1035,0,0.0451038,"ty segmentation, we were motivated to explore approaches that don’t rely on manual labels for classification due to Twitter’s wide range of named entity types. Additionally, unlike previous work on NER in informal text, our approach allows the sharing of information across an entity’s mentions which is quite beneficial due to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’s distribution of possible types is shared ac"
D11-1141,H05-1056,0,0.0177837,"Missing"
D11-1141,P09-1113,0,0.173591,"Missing"
D11-1141,D09-1026,0,0.642196,"Missing"
D11-1141,D10-1037,0,0.0260391,"ich we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most words found in tweets are not part of an entity, we need a larger annotated dataset to effectively learn a model of named entities. We therefore use a randomly sampled set of 2,400 tweets for NER. All experiments (Tables 6, 8-10) report results using 4-fold cross validation. 3.1 Segmenting Named Entities Because capitalization in Twitter is less informative than news, in-domain data is needed to train models which rely less heavily on capitalization, and also are able to utilize features provided by T- CAP. We exhaustively annotated our s"
D11-1141,N03-1028,0,0.0319403,"Missing"
D11-1141,N10-1009,0,0.0418606,"Missing"
D11-1141,P10-1149,0,0.0787574,"Missing"
D11-1141,W00-0726,0,0.037791,"Missing"
D11-1141,N03-1033,0,0.0846633,"Missing"
D11-1141,P10-1040,0,0.504898,"Missing"
D11-1141,P95-1026,0,0.0853228,"Missing"
D11-1141,J93-2004,0,\N,Missing
D14-1214,P11-2000,0,0.166133,"Missing"
D14-1214,P11-1040,0,0.0160923,"sonal topic needs to be adequately discussed by the user and their followers in order to be detected16 . Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structu"
D14-1214,D13-1114,0,0.00534075,"roughly constant, but recall increases as more life events and C ONGRATULA TIONS and C ONDOLENCES are discovered. 8 Related Work Our work is related to three lines of NLP researches. (1) user-level information extraction on social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitl"
D14-1214,P07-1030,0,0.0127633,"disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major li"
D14-1214,D13-1192,0,0.0182098,"social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoid"
D14-1214,P12-1056,0,0.222329,"important life events on which algorithms can rely for extraction or classification. Introduction Social networking websites such as Facebook and Twitter have recently challenged mainstream media as the freshest source of information on important news events. In addition to an important source for breaking news, social media presents a unique source of information on private events, for example a friend’s engagement or college graduation (examples are presented in Figure 1). While a significant amount of previous work has investigated event extraction from Twitter (e.g., (Ritter et al., 2012; Diao et al., 2012)), existing approaches mostly focus on public bursty event extraction, and little progress has been made towards the problem of automatically extracting the major life events of ordinary users. A system which can automatically extract major life events and generate fine-grained descriptions as in Figure 1 will not only help Twitter Challenge 2: Noisiness of Twitter Data: The user-generated text found in social media websites such as Twitter is extremely noisy. The language used to describe life events is highly varied and ambiguous and social media users frequently discuss public news and mund"
D14-1214,P11-1055,0,0.00961363,"of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying on minimum efforts of human supervision, and s"
D14-1214,D14-1108,0,0.00709329,"e Stanford PragBank10 , 8 Most tweets in the bootstrapping output are positive. The majority of results returned by Twitter Search are negative examples. 10 http://compprag.christopherpotts.net/ factbank.html 2002 9 an extension of FactBank (Saur´ı and Pustejovsky, 2009) which contains a list of modal words such as “might”, “will”, “want to” etc11 . • I: Whether the subject of the tweet is first person singular. • Dependency: If the subject is first person singular and the u is a verb, the dependency path between the subject and u (or nondependency). Tweet dependency paths were obtained from (Kong et al., 2014). As the tweet parser we use only supports one-to-one dependency path identification but no dependency properties, Dependency is a binary feature. The subject of each tweet is determined by the dependency link to the root of the tweet from the parser. Among the features we explore, Word encodes the general information within the tweet. Window addresses the information around topic key word. The rest of the features specifically address each of the negative situations described in Challenge 2, Section 1: Tense captures past event description, Factuality filters out wishes or imagination, I and"
D14-1214,P10-1150,1,0.666806,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,N10-1087,1,0.304045,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,P08-1119,1,0.214259,"potting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a rel"
D14-1214,P14-1016,1,0.167411,"ic event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoids the nuisance of"
D14-1214,D11-1024,0,0.00393037,"by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics"
D14-1214,P09-1113,0,0.0027458,"on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying o"
D14-1214,D08-1027,0,0.0347829,"Missing"
D14-1214,N10-1012,0,0.00569359,"2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D ="
D14-1214,N13-1039,0,0.0248148,"Missing"
D14-1214,D11-1135,0,0.0127003,"lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not sto"
D14-1214,N10-1020,1,0.423031,"nd C ONDOLENCES, including the phrases: ”Congratulations”, ”Congrats”, ”Sorry to hear that”, ”Awesome”, and gather tweets that were observed with seed responses. Next, an LDA (Blei et al., 2003)2 based topic model is used to cluster the gathered 2 Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twi"
D14-1214,D11-1141,1,0.538137,"Missing"
D14-1214,D11-1091,0,0.0235403,"Missing"
D14-1214,W09-1703,0,\N,Missing
D16-1030,P13-1009,0,0.371269,"e (Chang and Manning, 2012), outperforming other state-of-the-art time expression resolvers HeidelTime (Str¨otgen and Gertz, 2013), TempEX (Mani and Wilson, 2000) and UWTime (Lee et al., 2014) as well. Our approach also produces a confidence score that allows us to trade recall for precision. To the best of our knowledge, TweeTIME is the first time resolver designed specifically for social media data.3 This is also the first time that distant supervision is successfully applied for end-to-end temporal recognition and normalization. Previous distant supervision approaches (Angeli et al., 2012; Angeli and Uszkoreit, 2013) only address the normalization problem, assuming gold time mentions are available at test time. 2 System Overview Our TweeTIME system consists of two major components as shown in Figure 3: 1. A Temporal Recognizer which identifies time expressions (e.g. Monday) in English text and outputs 5 different temporal types (described in Table 1) indicating timeline direction, month of year, date of month, day of week or no temporal information (NA). It is realized as a multipleinstance learning model, and in an enhanced version, as a missing data model. Based on this assumption, tweets that contain t"
D16-1030,N12-1049,0,0.221093,"ns. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news arti"
D16-1030,D14-1164,0,0.0140877,") Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision"
D16-1030,S16-1165,0,0.191973,"blicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical texts (UzZaman et al., 2013; Bethard and Savova, 2016). Resolving time expressions in social media is a non-trivial problem. Besides many spelling variations, time expressions are more likely to refer to future dates than in newswire. For the example in 307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 307–318, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Figure 1, we need to recognize that Monday refers to the upcoming Monday and not the previous one to resolve to its correct normalized date (5/9/2016). We also need to identify that the word Sun is not referri"
D16-1030,S13-2002,0,0.189164,"-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal re"
D16-1030,D13-1078,0,0.0946954,"-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal re"
D16-1030,P07-1073,0,0.0459083,"e (SUTime, HeidelTime) None (SUTime, HeidelTime) 2002-03-12 2015-03-12 (TweeTime) None 2015-12-08 (TweeTime) Table 7: Representative Examples of System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2"
D16-1030,S13-2012,0,0.0222482,"ion extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and theref"
D16-1030,chang-manning-2012-sutime,0,0.0332083,"ction: Unlike rule-based systems, TweeTIME has a tendency to over-predict when there is no explicit time expression in the tweets, possibly because of the presence of present tense verbs. Such mistakes could also happen in some past tense verbs. Because TweeTIME resolves time expressions using a very different approach compared to traditional methods, its distribution of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova,"
D16-1030,N16-1045,0,0.0564644,"Missing"
D16-1030,P13-2114,0,0.0228848,"ished on Friday 5/6/2016 that contains the temporal expression Monday referring to the date of the event (5/9/2016), which a generic temporal tagger failed to resolve correctly. Figure 2: A tweet that contains a simple explicit time mention and an event (Mercury, 5/9/2016) that can be identified by an Introduction open-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requi"
D16-1030,D12-1062,0,0.0301003,"tains the temporal expression Monday referring to the date of the event (5/9/2016), which a generic temporal tagger failed to resolve correctly. Figure 2: A tweet that contains a simple explicit time mention and an event (Mercury, 5/9/2016) that can be identified by an Introduction open-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temp"
D16-1030,S13-2009,0,0.107116,"ferent approach compared to traditional methods, its distribution of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova, 2016) offers an evaluation in the clinical domain besides newswire, most participants used the provided annotated corpus to train supervised models in addition to employing hand-coded rules. Previous work on adapting temporal taggers primarily focus on scaling up to more languages. HeidelTime was exten"
D16-1030,P11-1055,0,0.240785,"future (TL=f uture). The multiple instance learning assumption implies that at least one word must be tagged with each of these present temporal tags. For example, ideally after training, the model will learn to assign z8 to tag a and z1 to tag b. lem, we never directly observe the words’ tags (z = z1 , . . . , zn ) during learning. Instead, they are latent and we only observe the date of an event mentioned in the text, from which we derive sentencelevel binary variables t = t1 , . . . , tk corresponding to temporal tags for the sentence. Following previous work on multiple-instance learning (Hoffmann et al., 2011a; Xu et al., 2014), we model the connection between sentence-level labels and word-level tags using a set of deterministic-OR factors φsent . The overall conditional probability of our model is defined as: P (t, z|w; θ r ) k n Y 1 Y sent = φ (ti , z) × φword (zj , wj ) Z = 1 Z i=1 j=1 k Y n Y i=1 φsent (ti , z) × eθ r ·f(z (1) j ,wj ) j=1 where f(zj , wj ) is a feature vector and   1 if ti = true ∧ ∃j : zj = i sent φ (ti , z) = 1 if ti = f alse ∧ ∀j : zj 6= i   0 otherwise (2) We include a standard set of tagging features that 310 X X t,z z P (z|w, t; θ r ) · f(z, w) P (t, z|w; θ r ) · f"
D16-1030,S10-1072,0,0.027782,"E resolves time expressions using a very different approach compared to traditional methods, its distribution of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova, 2016) offers an evaluation in the clinical domain besides newswire, most participants used the provided annotated corpus to train supervised models in addition to employing hand-coded rules. Previous work on adapting temporal taggers primarily focus on scaling u"
D16-1030,P14-1135,0,0.353564,"pressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical t"
D16-1030,P00-1010,0,0.664434,"by an Introduction open-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses gene"
D16-1030,E12-1062,0,0.0812064,"They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision. We presented a method for learning a wordlevel temporal tagging models from tweets that are heuristically labeled with only sentence-level labels. This approach was further extended to account for 3"
D16-1030,P09-1113,0,0.532674,"from the list that are also written within ±7 days of the event. These tweets and the dates of the known events serve as labeled examples that are likely to mention a known date. We also include a set of pseudo-negative examples, that are unlikely to refer to any event, by gathering a random sample of tweets that do not mention any of the top 10, 000 events and where TempEx does not extract any date. 5.2 Large-Scale Heuristic Evaluation We first evaluate our tagging model, by testing how well it can predict the heuristically generated labels. As noted in previous work on distant supervision (Mintz et al., 2009a), this type of evaluation usually under-estimates precision, however it provides us with a useful intrinsic measure of performance. In order to provide even coverage of months in the training and test set, we divide the twitter corpus into 3 subsets based on the mod-5 week of each tweet’s creation date. To train system we use tweets that are created in 1st, 2nd or 3rd weeks. To tune parameters of the MiDaT model we used tweets from 5th weeks, and to evaluate the performance of the trained model we used tweets from 4th weeks. MultiT MiDaT Precision 0.61 0.67 Recall 0.21 0.31 F-value 0.32 0.42"
D16-1030,P11-2048,0,0.0165397,"12-08 (TweeTime) Table 7: Representative Examples of System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large"
D16-1030,C14-1168,0,0.0239454,"013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision. We presented a method for learning a wordlevel temporal tagging models from tweets that are heuristically labe"
D16-1030,E12-1049,0,0.0297372,"y, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision. We presented a method for learning a wordlevel temporal tagging models from tweets that are heuristically labeled with only sentence-level labels. Th"
D16-1030,D11-1141,1,0.930792,"f model training. Temporal Types Timeline (TL) Day of Week (DOW) Day of Month (DOM) Month of Year (MOY) None (NA) Possible Values (tags) past, present, f uture M on, T ue, . . . , Sun 1, 2, 3, . . . , 31 Jan, F eb, . . . , Dec NA Table 1: Our Temporal Recognizer can extract five different temporal types and assign one of their values to each word of a tweet. proposed by Ritter et al. (2012). Each event consists of one or more named entities, in addition to the date on which the event takes place, for example [Mercury, 5/9/2016]. Tweets are first processed by a Twitter named entity recognizer (Ritter et al., 2011), and a generic date resolver (Mani and Wilson, 2000). Events are then extracted based on the strength of association between each named entity and calendar date, as measured by a G2 test on their co-occurrence counts. More details of the Event Extractor can be found in Section 5.1. The following two sections describe the details of our Temporal Recognizer and Temporal Normalizer separately. 3 Distant Supervision for Recognizing Time Expressions The goal of the recognizer is to predict the temporal tag of each word, given a sentence (or a tweet) w = w1 , . . . , wn . We propose a multiple-inst"
D16-1030,Q13-1030,1,0.841866,"HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events"
D16-1030,N15-1044,0,0.0215173,"ous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical texts (UzZaman et al., 2013; Bethard and Savova, 2016). Resolving time expressions in social media is a non-trivial problem. Besides many spelling variations, time expressions are more likely to refer to future dates than in newswire. For the example in 307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 307–318, c Austin, Texas, November 1-5"
D16-1030,strotgen-gertz-2012-temporal,0,0.0735469,"Missing"
D16-1030,D15-1063,0,0.105871,"Missing"
D16-1030,D12-1042,0,0.022492,"presentative Examples of System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled t"
D16-1030,S10-1062,0,0.0994182,"ion of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova, 2016) offers an evaluation in the clinical domain besides newswire, most participants used the provided annotated corpus to train supervised models in addition to employing hand-coded rules. Previous work on adapting temporal taggers primarily focus on scaling up to more languages. HeidelTime was extended to multilingual (Str¨otgen and Gertz, 2015), colloquial (S"
D16-1030,S13-2001,0,0.415971,"r code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical texts (UzZaman et al., 2013; Bethard and Savova, 2016). Resolving time expressions in social media is a non-trivial problem. Besides many spelling variations, time expressions are more likely to refer to future dates than in newswire. For the example in 307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 307–318, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Figure 1, we need to recognize that Monday refers to the upcoming Monday and not the previous one to resolve to its correct normalized date (5/9/2016). We also need to identify that"
D16-1030,P13-2117,1,0.82717,"f System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a data"
D16-1030,Q14-1034,1,0.944486,"multiple instance learning assumption implies that at least one word must be tagged with each of these present temporal tags. For example, ideally after training, the model will learn to assign z8 to tag a and z1 to tag b. lem, we never directly observe the words’ tags (z = z1 , . . . , zn ) during learning. Instead, they are latent and we only observe the date of an event mentioned in the text, from which we derive sentencelevel binary variables t = t1 , . . . , tk corresponding to temporal tags for the sentence. Following previous work on multiple-instance learning (Hoffmann et al., 2011a; Xu et al., 2014), we model the connection between sentence-level labels and word-level tags using a set of deterministic-OR factors φsent . The overall conditional probability of our model is defined as: P (t, z|w; θ r ) k n Y 1 Y sent = φ (ti , z) × φword (zj , wj ) Z = 1 Z i=1 j=1 k Y n Y i=1 φsent (ti , z) × eθ r ·f(z (1) j ,wj ) j=1 where f(zj , wj ) is a feature vector and   1 if ti = true ∧ ∃j : zj = i sent φ (ti , z) = 1 if ti = f alse ∧ ∀j : zj 6= i   0 otherwise (2) We include a standard set of tagging features that 310 X X t,z z P (z|w, t; θ r ) · f(z, w) P (t, z|w; θ r ) · f(z, w) (3) This gra"
D16-1127,D11-1054,1,\N,Missing
D16-1127,W00-0306,0,\N,Missing
D16-1127,P02-1040,0,\N,Missing
D16-1127,P10-1083,0,\N,Missing
D16-1127,P15-1152,0,\N,Missing
D16-1127,P16-1094,1,\N,Missing
D16-1127,D16-1230,0,\N,Missing
D16-1127,P11-1028,0,\N,Missing
D16-1127,P16-1153,1,\N,Missing
D17-1166,W15-1201,0,0.059731,"Missing"
D17-1166,P11-2102,0,0.113136,"Missing"
D17-1166,N13-1132,0,0.0335281,"nt Leonardo to win at the Oscars!” asserts the author’s desire toward Leonardo winning, but remains agnostic about the likelihood of this outcome, whereas “Leonardo DiCaprio will win the Oscars” is predicting with confidence that the event will happen. Figure 1 shows the annotation interface presented to Turkers. Each HIT contained 10 tweets to be annotated. We gathered annotations for 1, 841 tweets for winners and 1, 702 tweets for losers, giving us a total of 3, 543 tweets. We paid $0.30 per HIT. The total cost for our dataset was $1,000. Each tweet was annotated by 7 Turkers. We used MACE (Hovy et al., 2013) to resolve differences between annotators and produce a single gold label for each tweet. Figures 2a and 2c show heatmaps of the distribution of annotations for the winners for the Oscars in addition to all categories. In both instances, most of the data is annotated with “Definitely Yes” and “Probably Yes” labels for veridicality. Figures 2b and 2d show that the distribution is more diverse for the losers. Such distributions indicate that the veridicality of crowds’ statements could indeed be predictive of outcomes. We provide additional evidence for this hypothesis using automatic veridical"
D17-1166,N10-1038,0,0.0345519,"mes that were not expected according to popular belief (Section 4.5). We also show how TwiVer can be used to measure the number of correct and incorrect predictions made by individual accounts. This provides an intuitive measurement of the reliability of an information source (Section 4.6). 2 Related Work In this section we summarize related work on textdriven forecasting and computational models of veridicality. Text-driven forecasting models (Smith, 2010) predict future response variables using text written in the present: e.g., forecasting films’ box-office revenues using critics’ reviews (Joshi et al., 2010), predicting citation counts of scientific articles (Yogatama et al., 2011) and success of literary works (Ashok et al., 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al., 2014), predicting betrayal in online strategy games (Niculae et al., 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al., 2017). These methods typically require historical data for fitting model parameters, and may be sensitive to issues such as concept drift (Fung, 2014). In contrast"
D17-1166,D14-1108,0,0.0286733,"o entities: e.g., t1 will win t2 . Distance to keyword. We also compute the distance of TARGET and OPPONENT entities to the keyword. Punctuation. We introduce two binary features for the presence of exclamation marks and question marks in the tweet. We also have features which check whether a tweet ends with an exclamation mark, a question mark or a period. Punctuation, especially question marks, could indicate how certain authors are of their claims. Dependency paths. We retrieve dependency paths between the two TARGET entities and between the TARGET and keyword (win) using the TweeboParser (Kong et al., 2014) after applying rules to normalize paths in the tree (e.g., “doesn’t” → “does not”). Negated keyword. We check whether the keyword is negated (e.g., “not win”, “never win”), using the normalized dependency paths. We randomly divided the annotated tweets into a training set of 2,480 tweets, a development set of 354 tweets and a test set of 709 tweets. MAP parameters were fit using LBFGS-B (Zhu et al., 1997). Table 6 provides examples of high-weight features for positive and negative veridicality. 3.4 Evaluation We evaluated TwiVer’s precision and recall on our held-out test set of 709 tweets. F"
D17-1166,P12-3005,0,0.0382806,"r the Eurovision contest,5 52 for Tennis Grand Slams,6 6 for the Rugby World Cup,7 18 for the Cricket World Cup,8 12 for the Football World Cup,9 76 for the 2016 US presidential elections,10 and 68 queries for the 2014 Indian general elections.11 We added an event prefix (e.g., “Oscars” or the state for presidential primaries), a keyword (“win”), and the relevant date range for the event. For example, “Oscars Leonardo DiCaprio win since:2016-2-22 until:2016-2-28” would be the query generated for the first entry in Table 2. We restricted the data to English tweets only, as tagged by langid.py (Lui and Baldwin, 2012). Jaccard similarity was computed between messages to identify and remove duplicates.12 We removed URLs and preserved only tweets that mention contenders in the text. This automatic postprocessing left us with 57,711 tweets for all winners and 55,558 tweets for losers (contenders who did not win) across all events. Table 4 gives the data distribution across event categories. Event 2016 US Presidential primaries Oscars (2009 – 2016) Tennis Grand Slams (2011 – 2016) Ballon d’Or Award (2010 – 2016) Eurovision (2010 – 2016) 2016 US Presidential elections 2014 Indian general elections Rugby World C"
D17-1166,J12-2003,1,0.894104,"Missing"
D17-1166,S13-2053,0,0.0314073,"ries 2016 US presidential elections Indian elections Table 9: F1 scores for each event when training on all events vs. holding out that event from training. |Tt |is the number of tweets of that event category present in the test dataset. where |Tc |is the set of tweets mentioning positive veridicality predictions toward candidate c, and |TO |is the set of all tweets predicting any opponent will win. For each contest, we simply predict as winner the contender whose score is highest. 4.2 Sentiment Baseline We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013). Prior work on social media analysis used sentiment to make predictions about real-world outcomes. For instance, O’Connor et al. (2010) correlated sentiment with public opinion polls and Tumasjan et al. (2010) use political sentiment to make predictions about outcomes in German elections. We use a re-implementation of (Mohammad et al., 2013)’s system14 to estimate sentiment for tweets in our corpus. We run the tweets obtained for every contender through the sentiment analysis system to obtain a count of positive labels. Sentiment scores are computed analogously to veridicality using Equation"
D17-1166,P15-1159,0,0.0282846,"elated work on textdriven forecasting and computational models of veridicality. Text-driven forecasting models (Smith, 2010) predict future response variables using text written in the present: e.g., forecasting films’ box-office revenues using critics’ reviews (Joshi et al., 2010), predicting citation counts of scientific articles (Yogatama et al., 2011) and success of literary works (Ashok et al., 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al., 2014), predicting betrayal in online strategy games (Niculae et al., 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al., 2017). These methods typically require historical data for fitting model parameters, and may be sensitive to issues such as concept drift (Fung, 2014). In contrast, our approach does not rely on historical data for training; instead we forecast outcomes of future events by directly extracting users’ explicit predictions from text. Prior work has also demonstrated that user sentiment online directly correlates with various real-world time series, including polling data (O’Connor et al., 2010) and m"
D17-1166,P14-2068,0,0.0239631,"for training; instead we forecast outcomes of future events by directly extracting users’ explicit predictions from text. Prior work has also demonstrated that user sentiment online directly correlates with various real-world time series, including polling data (O’Connor et al., 2010) and movie revenues (Mishne and Glance, 2006). In this paper, we empirically demonstrate that veridicality can often be more predictive than sentiment (Section 4.1). Also related is prior work on detecting veridicality (de Marneffe et al., 2012; Søgaard et al., 2015) and sarcasm (Gonz´alez-Ib´anez et al., 2011). Soni et al. (2014) investigate how journalists frame quoted content on Twitter using predicates such as think, claim or admit. In contrast, our system TwiVer, focuses on the author’s belief toward a claim and direct predictions of future events as opposed to quoted content. Our approach, which aggregates predictions extracted from user-generated text is related to prior work that leverages explicit, positive veridicality, statements to make inferences about users’ demographics. For example, Coppersmith et al. (2014; 2015) exploit users’ self-reported statements of diagnosis on Twitter. 3 Measuring the Veridical"
D17-1166,D11-1055,0,0.060319,"Missing"
D17-1166,D11-1141,1,0.738866,"els for veridicality into three: positive veridicality (“Definitely Yes” and “Probably Yes”), neutral (“Uncertain about the outcome”) and negative veridicality (“Definitely No” and “Probably No”). We model the conditional distribution over a tweet’s veridicality toward a candidate c winning a contest against a set of opponents, O, using a log-linear model: P (y = v|c, tweet) ∝ exp (θv · f (c, O, tweet)) where v is the veridicality (positive, negative or neutral). To extract features f (c, O, tweet), we first preprocessed tweets retrieved for a specific event to identify named entities, using (Ritter et al., 2011)’s Twitter NER system. Candidate (c) and opponent entities were identified in the tweet as follows: - TARGET (t). A target is a named entity that matches a contender name from our queries. - OPPONENT (O). For every event, along with the current TARGET entity, we also keep track of other contenders for the same event. If a named entity in the tweet matches with one of other contenders, it is labeled as opponent. - ENTITY (e): Any named entity which does not match the list of contenders. Figure 3 illustrates the named entity labeling for a tweet obtained from the query “Oscars Leonardo DiCaprio"
D17-1166,D13-1181,0,\N,Missing
D17-1230,N16-1014,1,0.34709,"Jurafsky 1 1 Stanford University, Stanford, CA, USA 2 New York University, NY, USA 3 Ohio State University, OH, USA jiweil,wmonroe4,tianlins,jurafsky@stanford.edu sebastien@cs.nyu.edu ritter.1492@osu.edu Abstract 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to problems: responses are dull, generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive, and short-sighted (Li et al., 2016d). In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. Th"
D17-1230,P16-1094,1,0.386843,"Jurafsky 1 1 Stanford University, Stanford, CA, USA 2 New York University, NY, USA 3 Ohio State University, OH, USA jiweil,wmonroe4,tianlins,jurafsky@stanford.edu sebastien@cs.nyu.edu ritter.1492@osu.edu Abstract 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to problems: responses are dull, generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive, and short-sighted (Li et al., 2016d). In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. Th"
D17-1230,K16-1002,0,0.642309,"mpirical Methods in Natural Language Processing, pages 2157–2169 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics human-generated one. The output from the discriminator is used as a reward to the generator, pushing it to generate utterances indistinguishable from human-generated dialogues. The idea of a Turing test—employing an evaluator to distinguish machine-generated texts from human-generated ones—can be applied not only to training but also testing, where it goes by the name of adversarial evaluation. Adversarial evaluation was first employed in Bowman et al. (2016) to evaluate sentence generation quality, and preliminarily studied for dialogue generation by Kannan and Vinyals (2016). In this paper, we discuss potential pitfalls of adversarial evaluations and necessary steps to avoid them and make evaluation reliable. Experimental results demonstrate that our approach produces more interactive, interesting, and non-repetitive responses than standard S EQ 2S EQ models trained using the MLE objective function. 2 Related Work Dialogue generation Response generation for dialogue can be viewed as a source-to-target transduction problem. Ritter et al. (2011) f"
D17-1230,D16-1137,0,0.00721876,"sequence generation, Chen et al. (2016b) apply the idea of adversarial training to sentiment analysis and Zhang et al. (2017) apply the idea to domain adaptation tasks. Our work is distantly related to recent work that formalizes sequence generation as an action-taking problem in reinforcement learning. Ranzato et al. (2016) train RNN decoders in a S EQ 2S EQ model using policy gradient to obtain competitive machine translation results. Bahdanau et al. (2017) take this a step further by training an actor-critic RL model for machine translation. Also related is recent work (Shen et al., 2016; Wiseman and Rush, 2016) to address the issues of exposure bias and loss-evaluation mismatch in neural translation. 3 Adversarial Training for Dialogue Generation In this section, we describe in detail the components of the proposed adversarial reinforcement 2158 learning model. The problem can be framed as follows: given a dialogue history x consisting of a sequence of dialogue utterances,1 the model needs to generate a response y = {y1 , y2 , ..., yT }. We view the process of sentence generation as a sequence of actions that are taken according to a policy defined by an encoder-decoder recurrent neural network. 3.1"
D17-1230,P15-1152,0,0.321537,"Missing"
D18-1431,H05-1042,0,0.0508253,"to a semantic vector representation, ∇ is a function that computes similarity of the two embeddings and β is a tunable parameter. Both of the constraint terms from Eq 2 and Eq 3 are additive in nature and thus can be combined in a straightforward fashion. This formulation allows us to systematically combine information from three different models to produce better responses in terms of topic and semantic relevance. Conceptually, the likelihood term governs the grammatical structure of the response while the topic and semantic constraints drive content selection (Nenkova and Passonneau, 2004; Barzilay and Lapata, 2005). 4 Decoding with Distributional Constraints In Section 3, we defined two constraints (one topic constraint and one semantic) for use in the decoding objective. Incorporating these constraints during decoding requires that they factorize in a way that is compatible with left-to-right beam search over words in the response. The standard approach to computing posterior distributions in topic models requires a probabilistic inference procedure over the entire source and target. Furthermore, computing semantic representations can involve the use of complex neural architectures. Both of these proce"
D18-1431,D12-1091,0,0.0140613,"information to the conversation? We asked the evaluators to respond on a 5-point scale to the questions above (Strongly Agree, Agree, Unsure, Disagree, Strongly Disagree). These were later collapsed to 3 categories (Agree, Unsure, Disagree). The results for plausibility and content richness of our model in addition to the MMI and TA-Seq2Seq baselines and human responses are presented in Table 5. We observe that MMI200 and TA-10 models Statistical Significance of Results To verify the statistical significance of our findings, we conducted a pairwise bootstrap test (Efron and Tibshirani, 1994; Berg-Kirkpatrick et al., 2012) comparing the difference between percentage of Agree annotations (Yes column in the Table 5). We computed p-values for each pair of models: MMI200 vs DC-MMI200 and TA vs DC-MMI200. For plausibility, we did not find a significant difference in either comparison (pvalue ≈ 0.25) while for content richness, both differences were found to be significant (p-value &lt;10−4 ). To summarize: our model significantly beats both baselines in terms of content richness while the difference in plausibility was not found to be statistically significant. 3976 7.2.2 Pairwise Evaluation of Interestingness To furth"
D18-1431,N18-1016,0,0.0657807,"Missing"
D18-1431,D18-1241,0,0.0303352,"Missing"
D18-1431,W11-0609,0,0.0366974,"data by assuming each line corresponds to a full speaker turn. Although this assumption is often violated, prior work has successfully trained 3973 and evaluated neural conversation models using this corpus. In our experiments we used a preprocessed version of this dataset distributed by Li et. al. (2016a).6 The dataset contains large number of two turn dialogues out of which we sampled 23M to use as our training set and 10k as a validation set. Due to the noisy nature of the OpenSubtitles conversations we do not use them for evaluation. Instead, we leverage the Cornell Movie Dialogue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) which is much smaller but contains accurate speaker annotations. We extracted all two turn conversations (source target pair) from this corpus and removed those with less than three and more than 25 words. After this, we divided the remaining conversations into three buckets based on source length. The numbers can be found in Table 2. From each bucket we randomly sampled ≈333 dialogues for a total of 1000 dialogues in our test set. We evaluate all models on this test set. Since automatic metrics do not correlate with human judgment, we manually tuned the hyperparameters (α and β) on a small d"
D18-1431,D13-1111,0,0.0744224,"Missing"
D18-1431,P17-4012,0,0.0513069,"I-bidi reranking with a beam size of 200 (DC-MMI200). We test all configurations on the 1000 conversations test set described in Section 5 and compare them on automatic metrics and also in a crowdsourced human evaluation. We do not consider TA-200 (TA-Seq2Seq, Beam=200), DC-200 and MMI-10 for human evaluation as they appear to perform worse than other model variants in automatic metrics and also on our set of development sentences. Sample responses for all the remaining models are presented in Table 3. 7.1 http://nlp.stanford.edu/data/OpenSubData.tar 7 OpenNMT is used for training our models (Klein et al., 2017). Results and Analysis Automatic Metrics Following Li et. al. (2016a), we report distinct-1 and distinct-2, which measure the diversity of re3974 Source Target (ground truth) in there , sir . MMI here ’s your jacket ! uh , thanks ... i don ’t want it ! what ’s so damn funny ? been to any good &lt;unk&gt;lately ? what are you laughing at ? what kind of suit is this ? what ’s the matter with you ? well , what exactly does our platoon do ? serve &lt;unk&gt;? process paperwork ? left us here to rot . that ’s what they ’ve done . heroes of the newspapers ! that ’s it . you ’re not setting foot off this ship un"
D18-1431,D17-1259,0,0.0405372,"Missing"
D18-1431,N16-1014,1,0.743863,"ents, without relying on hand-written rules or manual annotation. Such response generation models could be combined with traditional dialogue systems to enable more natural and adaptive conversation, in addition to new applications such as predictive response suggestion (Kannan et al., 2016), however many challenges remain. A major drawback of neural conversation generation is that it tends to produce too many “safe” or generic responses, for example: “I don’t know” or “What are you talking about ?”. This is a pervasive problem that has been independently reported by multiple research groups (Li et al., 2016a; Serban et al., 2016; Li et al., 2016c).1 The effect is due to the use of conditional likelihood as a decoding objective – maximizing conditional likelihood is a suitable choice for text-to-text generation tasks such as machine translation, where the source and target are semantically equivalent, however, in conversation there are many acceptable ways to respond. Simply choosing most predictable reply often leads to very dull conversation. Figure 1 illustrates the problem with conditional likelihood using an example. After encoding the source message using a bidirectional LSTM with attention"
D18-1431,D16-1127,1,0.753379,"ents, without relying on hand-written rules or manual annotation. Such response generation models could be combined with traditional dialogue systems to enable more natural and adaptive conversation, in addition to new applications such as predictive response suggestion (Kannan et al., 2016), however many challenges remain. A major drawback of neural conversation generation is that it tends to produce too many “safe” or generic responses, for example: “I don’t know” or “What are you talking about ?”. This is a pervasive problem that has been independently reported by multiple research groups (Li et al., 2016a; Serban et al., 2016; Li et al., 2016c).1 The effect is due to the use of conditional likelihood as a decoding objective – maximizing conditional likelihood is a suitable choice for text-to-text generation tasks such as machine translation, where the source and target are semantically equivalent, however, in conversation there are many acceptable ways to respond. Simply choosing most predictable reply often leads to very dull conversation. Figure 1 illustrates the problem with conditional likelihood using an example. After encoding the source message using a bidirectional LSTM with attention"
D18-1431,D17-1230,1,0.868792,"l. (2018) develop models which converse while assuming a persona defined by a short description of attributes. Wang et. al. (2017) suggested decoding methods that influence the style and topic of the generated response. Bosselutet al. (2018) develop discourse-aware rewards with reinforcement learning (RL) to generate long and coherent texts. Li et. al. (2016c) applied deep reinforcement learning to dialogue generation to maximize long-term reward of the conversation, as opposed to directly maximizing likelihood of the response. This line of work was further extended with adversarial learning (Li et al., 2017) that rewards generated conversations that are indistinguishable from real conversations in the data. Lewis et. al. (2017) applied reinforcement learning with dialogue rollouts to generate replies that maximize expected reward, while learning to generate responses from a crowdsourced dataset of negotiation dialogues. Choi et. al. (2018) used crowd-workers to gather a corpus of 100K information-seeking QA dialogues that are answerable using text spans from Wikipedia. Niu and Bansal (2018) designed a number of weakly-supervised models that generate polite, neutral or rude responses. Their fusion"
D18-1431,D16-1230,0,0.209707,"Missing"
D18-1431,P17-1103,0,0.0466107,"d model is Polite-RL which assigns a reward based on a politeness classifier. Gimpel et. al. (2013) explored methods for increasing the diversity of N-best lists in machine translation by in3977 troducing a pairwise dissimilarity function. Similar ideas have been explored in the context of neural generation models. (Vijayakumar et al., 2016; Li and Jurafsky, 2016; Li et al., 2016b) Following previous work we evaluated our approach using a combination of automatic metrics and human judgments. Some recent work has explored the possibility of adversarial evaluation of neural conversation models (Lowe et al., 2017; Li et al., 2017). 9 Conclusions We presented an approach to generate more interesting responses in neural conversation models by incorporating side information in the form of distributional constraints. When using maximum likelihood decoding objectives, neural conversation models tend to generate safe responses, such as “I don’t know” for most inputs. Our proposed approach provides a flexible method of incorporating a broad range of distributional constraints into the decoding objective. We proposed and empirically evaluated two constraints that factorize over words, and therefore naturally"
D18-1431,P08-1099,0,0.0303836,"017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a). Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization (Mann and McCallum, 2008; Ganchev et al., 2010; Zhu et al., 2014). Posterior regularization introduces similar distributional constraints on expectations computed over unlabeled data using a model’s parameters. These are typically added to the learning objective for semi-supervised scenarios where available labeled data is limited. In contrast, our approach introduces distributional constraints into the decoding objective as a way to combine neural conversation models trained on large quantities of conversational data with separately trained models of topics and semantic similarity that can drive content selection. T"
D18-1431,N04-1019,0,0.136084,"unction that maps an utterance to a semantic vector representation, ∇ is a function that computes similarity of the two embeddings and β is a tunable parameter. Both of the constraint terms from Eq 2 and Eq 3 are additive in nature and thus can be combined in a straightforward fashion. This formulation allows us to systematically combine information from three different models to produce better responses in terms of topic and semantic relevance. Conceptually, the likelihood term governs the grammatical structure of the response while the topic and semantic constraints drive content selection (Nenkova and Passonneau, 2004; Barzilay and Lapata, 2005). 4 Decoding with Distributional Constraints In Section 3, we defined two constraints (one topic constraint and one semantic) for use in the decoding objective. Incorporating these constraints during decoding requires that they factorize in a way that is compatible with left-to-right beam search over words in the response. The standard approach to computing posterior distributions in topic models requires a probabilistic inference procedure over the entire source and target. Furthermore, computing semantic representations can involve the use of complex neural archit"
D18-1431,Q18-1027,0,0.0130513,"irectly maximizing likelihood of the response. This line of work was further extended with adversarial learning (Li et al., 2017) that rewards generated conversations that are indistinguishable from real conversations in the data. Lewis et. al. (2017) applied reinforcement learning with dialogue rollouts to generate replies that maximize expected reward, while learning to generate responses from a crowdsourced dataset of negotiation dialogues. Choi et. al. (2018) used crowd-workers to gather a corpus of 100K information-seeking QA dialogues that are answerable using text spans from Wikipedia. Niu and Bansal (2018) designed a number of weakly-supervised models that generate polite, neutral or rude responses. Their fusion model combines a language model trained on polite utterances with the decoder. In the second method they prepend the utterance with a politeness label and scale its embedding to vary politeness. The third model is Polite-RL which assigns a reward based on a politeness classifier. Gimpel et. al. (2013) explored methods for increasing the diversity of N-best lists in machine translation by in3977 troducing a pairwise dissimilarity function. Similar ideas have been explored in the context"
D18-1431,N10-1020,1,0.744556,"DC-MMI10, both models generate the same candidates, but MMI is able to re-rank the results and thus improves plausibility. 8 Related Work Conversational agents primarily fall into two categories: task oriented dialogue systems (Williams et al., 2013; Wen et al., 2015) and chatbots (Weizenbaum, 1966), although there have been some efforts to integrate the two (Dodge et al., 2015; Yu et al., 2017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a). Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization (Mann and McCallum, 2008; Ganchev et al., 2010; Zhu et al., 2014). Posterior regularization introduces similar distributional constraints on expectations computed over unlabeled data using a model’s parameters. These are typica"
D18-1431,D11-1054,1,0.821319,"uccessfully inject content words into candidate hypotheses and that MMI is able to effectively choose plausible candidates. In the case of DC-10 and DC-MMI10, both models generate the same candidates, but MMI is able to re-rank the results and thus improves plausibility. 8 Related Work Conversational agents primarily fall into two categories: task oriented dialogue systems (Williams et al., 2013; Wen et al., 2015) and chatbots (Weizenbaum, 1966), although there have been some efforts to integrate the two (Dodge et al., 2015; Yu et al., 2017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a). Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization (Mann and McCallum, 2008; Ganchev et al., 2010; Zhu et al., 2014). Posterior r"
D18-1431,P15-1152,0,0.0778861,"Missing"
D18-1431,D17-1235,0,0.0611142,"d P (T |Y ), where T is a random variable defined over k topics. Then we can modify the decoding objective from Eq 1: Y https://www.ranks.nl/stopwords The top 10 topic words were taken from each of the 50 topics inferred by an HMM-LDA model (after removing stop words). (1) log P (wi |w1 , . . . wi−1 , X)} Yˆ T = arg max{ log P (Y |X)+ As a starting point for our approach we leverage the Seq2Seq model (Sutskever et al., 2014; Bahdanau et al., 2014) which has been used as a basis for a broad range of recent work on neural conversation (Kannan et al., 2016; Li et al., 2016a; Serban et al., 2016; Shao et al., 2017). This model consists of two parts, an encoder and a decoder both of which are typically stacked LSTM layers. The encoder reads the input sequence and creates 3 = arg max{log P (Y |X)} Y Neural Conversation Generation 2 Distributional Topic and Semantic Similarity Constraints (2) α × ∆(P (T |X), P (T |Y ))} Here, ∆ is a similarity function between the two probability distributions and α is a tunable hyperparameter to adjust impact of this constraint. Much recent work has investigated how to encode the semantic meaning of a sentence into a fixed high dimensional embedding space (Kiros et al., 2"
D18-1431,N15-1020,1,0.872914,"Missing"
D18-1431,D17-1228,0,0.032418,"Missing"
D18-1431,D15-1199,0,0.08673,"Missing"
D18-1431,P17-1190,0,0.0234424,"s model consists of two parts, an encoder and a decoder both of which are typically stacked LSTM layers. The encoder reads the input sequence and creates 3 = arg max{log P (Y |X)} Y Neural Conversation Generation 2 Distributional Topic and Semantic Similarity Constraints (2) α × ∆(P (T |X), P (T |Y ))} Here, ∆ is a similarity function between the two probability distributions and α is a tunable hyperparameter to adjust impact of this constraint. Much recent work has investigated how to encode the semantic meaning of a sentence into a fixed high dimensional embedding space (Kiros et al., 2015; Wieting and Gimpel, 2017). Given such an embedding representation of X and Y , one can find the semantic similarity between the two and similar to Eq 2 we can add a semantic similarity constraint to the likelihood objective as 3971 follows: Yˆ Emb = arg max{ log(P (Y |X))+ Y (3) β × ∇(Emb(X), Emb(Y ))} where, Emb() is a function that maps an utterance to a semantic vector representation, ∇ is a function that computes similarity of the two embeddings and β is a tunable parameter. Both of the constraint terms from Eq 2 and Eq 3 are additive in nature and thus can be combined in a straightforward fashion. This formulatio"
D18-1431,W13-4065,0,0.0112187,"in Table 6. We observe that with a beam size of 10 our model is able to generate content rich responses, but suffers in terms of plausibility. The values in the table suggests the decoding constraints defined in this work successfully inject content words into candidate hypotheses and that MMI is able to effectively choose plausible candidates. In the case of DC-10 and DC-MMI10, both models generate the same candidates, but MMI is able to re-rank the results and thus improves plausibility. 8 Related Work Conversational agents primarily fall into two categories: task oriented dialogue systems (Williams et al., 2013; Wen et al., 2015) and chatbots (Weizenbaum, 1966), although there have been some efforts to integrate the two (Dodge et al., 2015; Yu et al., 2017). Some of the earliest work on data-driven chatbots (Ritter et al., 2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015; Sordoni et al., 2015; Li et al., 2016a)"
D18-1431,P18-1205,0,0.121134,"Missing"
N10-1020,N04-1015,0,0.0371749,"Twitter through mobile devices. Posts are often highly ungrammatical, and filled with spelling errors. In order to illustrate the spelling variation found on Twitter, we ran the Jcluster word clustering algorithm (Goodman, 2001) on our cor4 http://twitter.com/public_timeline vides the 20 most recent posts on Twitter procoming comming enough enought enuff enuf be4 b4 befor before yuhr yur your yor ur youur yhur msgs messages couldnt culdnt cldnt cannae cudnt couldent about bou abt abour abut bowt Ck Ck 174 πk θk W0 s0,j w0,j Our base model structure is inspired by the content model proposed by Barzilay and Lee (2004) for multi-document summarization. Their sentencelevel HMM discovers the sequence of topics used to describe a particular type of news event, such as earthquakes. A news story is modeled by first generating a sequence of hidden topics according to a Markov model, with each topic generating an observed sentence according to a topic-specific language model. These models capture the sequential structure of news stories, and can be used for summarization tasks such as sentence extraction and ordering. Our goals are not so different: we wish to discover the sequential dialogue structure of conversa"
N10-1020,W04-3240,0,0.126967,"Missing"
N10-1020,W09-3951,0,0.364465,"sations are carried out by replying to specific posts. The Twitter API provides a link from each reply to the post it is responding to, allowing 2 The Crook et al. model should be able to be combined with the models we present here. 3 Will be available at http://www.cs.washington. edu/homes/aritter/twitter_chat/ 173 10 12 14 8 6 log frequency 4 2 0 There is surprisingly little work in unsupervised dialogue act tagging. Woszczyna and Waibel (1994) propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level. Crook et al. (2009) use Dirichlet process mixture models to cluster utterances into a flexible number of acts in a travel-planning domain, but do not examine the sequential structure of dialogue.2 In contrast to previous work, we address the problem of discovering dialogue acts in an informal, open-topic domain, where an unsupervised learner may be distracted by strong topic clusters. We also train and test our models in a new medium: Twitter. Rather than test against existing dialogue inventories, we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the"
N10-1020,P06-1039,0,0.0465747,"Missing"
N10-1020,P08-1095,0,0.00778227,"edium. 172 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172–180, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out"
N10-1020,P07-1094,0,0.0183454,"we first sample its act, and then sample a source for each word in the post. The hidden act and source variables are sampled according to the following transition distributions: 6 This figure omits hyperparameters as well as act transition and emission multinomials to reduce clutter. Dirichlet priors are placed over all multinomials. Ptrans (ai |a−i , s, w) ∝ P (ai |a−i ) Wi Y P (wi,j |a, s, w−(i,j) ) j=1 Ptrans (si,j |a, s−(i,j) , w) ∝ P (si,j |s−(i,j) )P (wi,j |a, s, w−(i,j) ) These probabilities can be computed analogously to the calculations used in the collapsed sampler for a bigram HMM (Goldwater and Griffiths, 2007), and those used for LDA (Griffiths and Steyvers, 2004). Note that our model contains five hyperparameters. Rather than attempt to set them using an expensive grid search, we treat the concentration parameters as additional hidden variables and sample each in turn, conditioned on the current assignment to all other variables. Because these variables are continuous, we apply slice sampling (Neal, 2003). Slice sampling is a general technique for drawing samples from a distribution by sampling uniformly from the area under its density function. 3.3 Estimating Likelihood on Held-Out Data In Sectio"
N10-1020,N09-1041,0,0.075662,"om nom! Table 2: Example of a topical cluster discovered by the EM Conversation Model. similar to previous HMMs for supervised dialogue act recognition (Stolcke et al., 2000), but our model is trained unsupervised. 3.2 generated word clusters, but we found that these approaches degrade model performance. Another approach to filtering out topic information leaves the data intact, but modifies the model to account for topic. To that end, we adopt a Latent Dirichlet Allocation, or LDA, framework (Blei et al., 2003) similar to approaches used recently in summarization (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). The goal of this extended model is to separate content words from dialogue indicators. Each word in a conversation is generated from one of three sources: • The current post’s dialogue act Conversation + Topic model Our conversations are not restricted to any particular topic: Twitter users can and will talk about anything. Therefore, there is no guarantee that our model, charged with discovering clusters of posts that aid in the prediction of the next cluster, will necessarily discover dialogue acts. The sequence model could instead partition entire conversations into topics, such as food,"
N10-1020,D09-1130,0,0.095308,"ire not only new annotations, but new annotation guidelines and new dialogue acts. This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs, Facebook, Twitter, and whatever is on the horizon. Previous work has taken a variety of approaches to dialogue act tagging in new media. Cohen et al. (2004) develop an inventory of dialogue acts specific to e-mail in an office domain. They design their inventory by inspecting a large corpus of e-mail, and refine it during the manual tagging process. Jeong et al. (2009) use semi-supervised learning to transfer dialogue acts from labeled speech corpora to the Internet media of forums and e-mail. They manually restructure the source act inventories in an attempt to create coarse, domain-independent acts. Each approach relies on a human designer to inject knowledge into the system through the inventory of available acts. As an alternative solution for new media, we propose a series of unsupervised conversation models, where the discovery of acts amounts to clustering utterances with similar conversational roles. This avoids manual construction of an act invento"
N10-1020,N06-1047,0,0.00439271,"mount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the ∗ 1 This work was conducted at Microsoft Research. Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for tr"
N10-1020,D09-1035,0,0.00897471,"urs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the ∗ 1 This work was conducted at Microsoft Research. Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training. The expense is compounded as we consider ne"
N10-1020,J00-3003,0,0.517721,"Missing"
N10-1020,N09-1054,0,0.0160001,"North American Chapter of the ACL, pages 172–180, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out by replying to specific posts. The Twitter API provides a link fr"
N18-2016,H05-1059,0,\N,Missing
N18-2016,P05-1053,0,\N,Missing
N18-2016,P14-1038,0,\N,Missing
N18-2016,J02-3001,0,\N,Missing
N18-2016,Q13-1005,0,\N,Missing
N18-2016,D14-1082,0,\N,Missing
N18-2016,P15-2072,0,\N,Missing
N18-2016,D15-1090,0,\N,Missing
N18-2016,N16-1030,0,\N,Missing
N18-2016,D16-1264,0,\N,Missing
N18-2016,P16-1101,0,\N,Missing
N18-2016,P09-1010,0,\N,Missing
N18-2016,P10-1129,0,\N,Missing
N19-1140,N18-1147,0,0.0223178,"cores) found in the NVD. Finally, we show that our approach can provide an early indication of vulnerabilities that result in real exploits in the wild as measured by the existence of Symantec virus signatures associated with CVEs; we also show how our approach can be used to retrospectively identify Twitter accounts that provide reliable warnings about severe vulnerabilities. Recently there has been increasing interest in developing NLP tools to identify cybersecurity events reported online, including denial of service attacks, data breaches and more (Ritter et al., 2015; Chang et al., 2016; Chambers et al., 2018). Our proposed approach in this paper builds on this line of work by evaluating users opinions toward the severity of cybersecurity threats. Prior work has also explored forecasting software vulnerabilities that will be exploited in the wild (Sabottke et al., 2015). Features included structured data sources (e.g., NVD), in addition to the volume of tweets mentioning a list of 31 keywords. Rather than relying on a fixed set of keywords, we analyze message content to determine whether the author believes a vulnerability is severe. As discussed by Sabottke et al. (2015), methods that rely on trac"
N19-1140,N16-1045,0,0.015497,"tics ratings (CVSS scores) found in the NVD. Finally, we show that our approach can provide an early indication of vulnerabilities that result in real exploits in the wild as measured by the existence of Symantec virus signatures associated with CVEs; we also show how our approach can be used to retrospectively identify Twitter accounts that provide reliable warnings about severe vulnerabilities. Recently there has been increasing interest in developing NLP tools to identify cybersecurity events reported online, including denial of service attacks, data breaches and more (Ritter et al., 2015; Chang et al., 2016; Chambers et al., 2018). Our proposed approach in this paper builds on this line of work by evaluating users opinions toward the severity of cybersecurity threats. Prior work has also explored forecasting software vulnerabilities that will be exploited in the wild (Sabottke et al., 2015). Features included structured data sources (e.g., NVD), in addition to the volume of tweets mentioning a list of 31 keywords. Rather than relying on a fixed set of keywords, we analyze message content to determine whether the author believes a vulnerability is severe. As discussed by Sabottke et al. (2015), m"
N19-1140,D14-1181,0,0.0041112,"Missing"
N19-1140,W02-1011,0,0.0276953,"ted in tweets to Common Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD). Using our predicted severity scores, we show that it is possible to achieve a Precision@50 of 0.86 when forecasting high severity vulnerabilities, significantly outperforming a baseline that is based on tweet volume. Finally we showed how reports of severe vulnerabilities online are predictive of real-world exploits. Acknowledgments There is a long history of prior work on analyzing users’ opinions online (Wiebe et al., 2004), a large body of prior work has focused on sentiment analysis (Pang et al., 2002; Rosenthal et al., 2015), e.g., determining whether a message is positive or negative. In this paper we developed annotated corpora and classifiers to analyze users’ opinions toward the severity of cybersecurity threats reported online, as far as we are aware this is the first work to explore this direction. Forecasting real-world exploits is a topic of interest in the security community. For example, Bozorgi et al. (2010) train SVM classifiers to rank We thank our anonymous reviewers for their valuable feedback. We also thank Tudor Dumitras¸ for helpful discussion on identifying real exploit"
N19-1140,D14-1162,0,0.081159,"Missing"
N19-1140,D11-1141,1,0.666642,"Missing"
N19-1140,S15-2078,1,0.819102,"mmon Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD). Using our predicted severity scores, we show that it is possible to achieve a Precision@50 of 0.86 when forecasting high severity vulnerabilities, significantly outperforming a baseline that is based on tweet volume. Finally we showed how reports of severe vulnerabilities online are predictive of real-world exploits. Acknowledgments There is a long history of prior work on analyzing users’ opinions online (Wiebe et al., 2004), a large body of prior work has focused on sentiment analysis (Pang et al., 2002; Rosenthal et al., 2015), e.g., determining whether a message is positive or negative. In this paper we developed annotated corpora and classifiers to analyze users’ opinions toward the severity of cybersecurity threats reported online, as far as we are aware this is the first work to explore this direction. Forecasting real-world exploits is a topic of interest in the security community. For example, Bozorgi et al. (2010) train SVM classifiers to rank We thank our anonymous reviewers for their valuable feedback. We also thank Tudor Dumitras¸ for helpful discussion on identifying real exploited threats. Funding was p"
N19-1140,W13-1107,0,0.0392052,"y of cybersecurity threats. Prior work has also explored forecasting software vulnerabilities that will be exploited in the wild (Sabottke et al., 2015). Features included structured data sources (e.g., NVD), in addition to the volume of tweets mentioning a list of 31 keywords. Rather than relying on a fixed set of keywords, we analyze message content to determine whether the author believes a vulnerability is severe. As discussed by Sabottke et al. (2015), methods that rely on tracking keywords and message volume are vulnerable to adversarial attacks from Twitter bots or sockpuppet accounts (Solorio et al., 2013). In contrast, our method is somewhat less prone to such attacks; by extracting users’ opinions expressed in individual tweets, we can track the provenance of information associated with our forecasts for display to an analyst, who can then determine whether or not they trust the source of information. 2 2.1 Data Collection To collect tweets describing cybersecurity events for annotation, we tracked the keywords “ddos” and “vulnerability” from Dec 2017 to July 2018 using the Twitter API. We then used the Twitter tagging tool described by Ritter et. al. (2011) to extract named entities,5 retain"
N19-1140,D17-1166,1,0.898461,"Missing"
N19-1140,P14-1017,0,0.0207736,"CVSS scores in NVD. This model is only for reference and can not be used in real practice, as we do not know true CVSS scores when forecasting. the exploitability of threats. Several studies have also predicted CVSS scores from various sources including text descriptions in NVD (Han et al., 2017; Bullough et al., 2017). Prior work has also explored a variety of forecasting methods that incorporate textual evidence (Smith, 2010), including the use of Twitter message content to forecast influenza rates (Paul et al., 2014), predicting the propagation of social media posts based on their content (Tan et al., 2014) and forecasting election outcomes (O’Connor et al., 2010; Swamy et al., 2017). 5 3.4 Identifying Accounts that Post Reliable Warnings Finally we perform an analysis of the reliability of individual Twitter accounts. We evaluate all accounts with more than 5 tweets exceeding 0.5 confidence score from our severity classifier. Table 10 presents our results. Accounts in our data whose warnings were found to have highest precision when compared against CVSS include “@securityaffairs” and “@EduardKovacs”, which are known to post security related information, and both have more than 10k followers. A"
N19-1140,H05-1044,0,0.0819708,"Missing"
N19-1140,P14-2114,0,0.0219111,"er or not the author expresses an opinion that the threat towards the target entity is severe. 2.3.2 Results Threat existence classifier: The logistic regression baseline has good performance at identifying threats, which we found to be a relatively easy task; area under the precision-recall curve (AUC) on the development and test set presented in Table 5. This enables accurate detection of trending threats online by tracking cybersecurity keywords using the Twitter streaming API, following an approach that is similar to prior work on entity-based Twitter event detection (Ritter et al., 2012; Zhou et al., 2014; Wei et al., 2015). Table 3 presents an example of threats detected using this procedure on Nov. 22, 2018.8 Threat severity classifier: Figure 3 shows precision recall curves for the threat severity classifiers. Logistic regression with bag-of-ngram features provides a strong baseline for this task. Table 4 presents examples of high-weight features from the logistic regression model. These features often intuitively indicate severe threats, e.g. “critical vulnerability”, “a massive”, “million”, etc. Without much hyperparameter tuning on the development set, the convolutional neural network co"
N19-1310,doddington-etal-2004-automatic,0,0.0430255,"es state-of-the-art results on minimally supervised sentential relation extraction, outperforming a number of baselines, including a competitive approach that uses the attention layer of a purely neural model.1 1 Introduction Recent years have seen significant progress on tasks such as object detection, automatic speech recognition and machine translation. These performance advances are largely driven by the application of neural network methods on large, highquality datasets. In contrast, traditional datasets for relation extraction are based on expensive and time-consuming human annotation (Doddington et al., 2004) and are therefore relatively small. Distant supervision (Mintz et al., 2009), a technique which uses existing knowledge bases such as Freebase or Wikipedia as a source of weak supervision, enables learning from large quantities of unlabeled text and is a promising approach for scaling up. Recent work has shown promising results from large-scale training of neural networks for relation extraction (Toutanova et al., 2015; Zeng et al., 2015). There are, however, significant challenges due to the inherent noise in distant supervision. For 1 Our code and data are publicly available on Github: http"
N19-1310,N16-1030,0,0.0593765,"on techniques to multirelational data (Nickel et al., 2011; Bordes et al., 2013; Chang et al., 2014). Rows of the matrix typically model pairs of entities, and columns represent relations or syntactic patterns (i.e., syntactic dependency paths observed between the entities). Structured Learning with Neural Representations: Prior work has investigated the combination of structured learning with learned representations for a number of NLP tasks, including parsing (Weiss et al., 2015; Durrett and Klein, 2015; Andor et al., 2016), named entity recognition (Cherry and Guo, 2015; Ma and Hovy, 2016; Lample et al., 2016) and stance detection (Li et al., 2018). We are not aware of any previous work that has explored this direction on the task of minimally supervised relation extraction; we believe structured learning is particularly crucial when learning from minimal supervision to help address the issues of missing data and overlapping relations. 3064 5 Conclusions In this paper we presented a hybrid approach to minimally supervised relation extraction that combines the benefits of structured learning and learned representations. Extensive experiments show that by performing inference during the learning proc"
N19-1310,P16-1200,0,0.0945796,"n for Computational Linguistics tions, including Structured Prediction Energy Networks (SPENs) (Belanger and McCallum, 2016); the key differences are the application to minimally supervised relation extraction and the inclusion of latent variables with deterministic factors, which we demonstrate enables effective learning in the presence of missing data in distant supervision. Our proposed method achieves state-of-theart results on minimally supervised sentential relation extraction, outperforming a number of baselines including one that leverages the attention layer of a purely neural model (Lin et al., 2016). 2 E⇥E R &lt;latexit sha1_base64=&quot;9n6P/Psi0wW6toZoaWyZIG9f3WQ=&quot;&gt;AAACG3icbVBNS8NAEN3Urxq/qh69LBbBU0lEUG9FKXisYG2xDWWz2bRLN5uwOxFK6L/wan+NJ/HqwR8juGlzsK0DA483M7w3z08E1+A431ZpbX1jc6u8be/s7u0fVA6PnnScKspaNBax6vhEM8ElawEHwTqJYiTyBWv7o7t83n5hSvNYPsI4YV5EBpKHnBIw1HMD94BHTONGv1J1as6s8CpwC1BFRTX7lZ9eENM0YhKoIFp3XScBLyMKOBVsYvdSzRJCR2TAugZKYmS8bOZ4gs8ME+AwVqYl4Bn79yIjkdbjyDebEYGhXp7l5H+zbgrhtZdxmaTAJJ0LhanAEOP8fRxwxSiIsQGEKm68YjokilAwIdkLMoHOvS08kiWD0Jie2LZt8nKX01kFrYvaTc15uKzWb4vgyugEnaJz5KIrVEf3qIlaiCKJXtEbmlpT6936sD7nqyWruDlGC2V9/QLnIKEZ&lt;/latexit&gt; sha1_base64=&quot;FViP8FQW0xxf/P1rHASRXeO3OD4=&quot;&gt;AAACA3icbVBN"
N19-1310,P16-1101,0,0.0291597,"matrix factorization techniques to multirelational data (Nickel et al., 2011; Bordes et al., 2013; Chang et al., 2014). Rows of the matrix typically model pairs of entities, and columns represent relations or syntactic patterns (i.e., syntactic dependency paths observed between the entities). Structured Learning with Neural Representations: Prior work has investigated the combination of structured learning with learned representations for a number of NLP tasks, including parsing (Weiss et al., 2015; Durrett and Klein, 2015; Andor et al., 2016), named entity recognition (Cherry and Guo, 2015; Ma and Hovy, 2016; Lample et al., 2016) and stance detection (Li et al., 2018). We are not aware of any previous work that has explored this direction on the task of minimally supervised relation extraction; we believe structured learning is particularly crucial when learning from minimal supervision to help address the issues of missing data and overlapping relations. 3064 5 Conclusions In this paper we presented a hybrid approach to minimally supervised relation extraction that combines the benefits of structured learning and learned representations. Extensive experiments show that by performing inference du"
N19-1310,P09-1113,0,0.783018,", outperforming a number of baselines, including a competitive approach that uses the attention layer of a purely neural model.1 1 Introduction Recent years have seen significant progress on tasks such as object detection, automatic speech recognition and machine translation. These performance advances are largely driven by the application of neural network methods on large, highquality datasets. In contrast, traditional datasets for relation extraction are based on expensive and time-consuming human annotation (Doddington et al., 2004) and are therefore relatively small. Distant supervision (Mintz et al., 2009), a technique which uses existing knowledge bases such as Freebase or Wikipedia as a source of weak supervision, enables learning from large quantities of unlabeled text and is a promising approach for scaling up. Recent work has shown promising results from large-scale training of neural networks for relation extraction (Toutanova et al., 2015; Zeng et al., 2015). There are, however, significant challenges due to the inherent noise in distant supervision. For 1 Our code and data are publicly available on Github: https://github.com/bflashcp3f/PCNN-NMAR example, Riedel et al. (2010) showed that"
N19-1310,P18-1046,0,0.0170496,"17; Yu et al., 2015), experimenting with alternative sentence representations in our framework is an interesting direction for future work. Recent work has also shown improved performance by incorporating supervised training data on the sentence level (Angeli et al., 2014; Beltagy et al., 2018), in contrast our approach does not make use of any sentence-level labels during learning and therefore relies on less human supervision. Finally, prior work has explored a variety of methods to address the issue of noise introduced during distant supervision (Wu et al., 2017; Yaghoobzadeh et al., 2017; Qin et al., 2018). Another line of work has explored open-domain and unsupervised methods for IE (Yao et al., 2011; Ritter et al., 2012; Stanovsky et al., 2015; Huang et al., 2016; Weber et al., 2017). Universal schemas (Riedel et al., 2013) combine aspects of minimally supervised and unsupervised approaches to knowledge-base completion by applying matrix factorization techniques to multirelational data (Nickel et al., 2011; Bordes et al., 2013; Chang et al., 2014). Rows of the matrix typically model pairs of entities, and columns represent relations or syntactic patterns (i.e., syntactic dependency paths obse"
N19-1310,N13-1008,0,0.0353358,"ing data on the sentence level (Angeli et al., 2014; Beltagy et al., 2018), in contrast our approach does not make use of any sentence-level labels during learning and therefore relies on less human supervision. Finally, prior work has explored a variety of methods to address the issue of noise introduced during distant supervision (Wu et al., 2017; Yaghoobzadeh et al., 2017; Qin et al., 2018). Another line of work has explored open-domain and unsupervised methods for IE (Yao et al., 2011; Ritter et al., 2012; Stanovsky et al., 2015; Huang et al., 2016; Weber et al., 2017). Universal schemas (Riedel et al., 2013) combine aspects of minimally supervised and unsupervised approaches to knowledge-base completion by applying matrix factorization techniques to multirelational data (Nickel et al., 2011; Bordes et al., 2013; Chang et al., 2014). Rows of the matrix typically model pairs of entities, and columns represent relations or syntactic patterns (i.e., syntactic dependency paths observed between the entities). Structured Learning with Neural Representations: Prior work has investigated the combination of structured learning with learned representations for a number of NLP tasks, including parsing (Weiss"
N19-1310,Q13-1030,1,0.92555,"l relation mention variables (i.e. one that has a large score and also a large Hamming loss) and z∗g corresponds to the best configuration that is consistent with the observed relations in the KB. This objective can be minimized using stochastic subgradient descent. Fixing z∗g and z∗e to their maximum values in Equation 2, subgradients with respect to the parameters can be computed as follows: ∇θ LSH (θ) = = variables. In preliminary experiments on development data, we found that local-search (Eisner and Tromble, 2006) using both relation type and mention search operators (Liang et al., 2010; Ritter et al., 2013) usually finds an optimal solution and also scales up to large training datasets; we use local search with 30 random restarts to compute argmax assignments for the hidden variables, z∗g and z∗e , in all our experiments. Bag-Size Adaptive Learning Rate: Since the search space of the MAP inference problem increases exponentially as the number of hidden variables goes up, it becomes more difficult to find the exact argmax solution using local search, leading to increased noise in the computed gradients. To mitigate the search-error problem in large bags of sentences, we dynamically modify the lea"
N19-1310,P15-2050,0,0.024355,"Missing"
N19-1310,P16-1123,0,0.0586119,"Missing"
N19-1310,P15-1032,0,0.0205512,"2013) combine aspects of minimally supervised and unsupervised approaches to knowledge-base completion by applying matrix factorization techniques to multirelational data (Nickel et al., 2011; Bordes et al., 2013; Chang et al., 2014). Rows of the matrix typically model pairs of entities, and columns represent relations or syntactic patterns (i.e., syntactic dependency paths observed between the entities). Structured Learning with Neural Representations: Prior work has investigated the combination of structured learning with learned representations for a number of NLP tasks, including parsing (Weiss et al., 2015; Durrett and Klein, 2015; Andor et al., 2016), named entity recognition (Cherry and Guo, 2015; Ma and Hovy, 2016; Lample et al., 2016) and stance detection (Li et al., 2018). We are not aware of any previous work that has explored this direction on the task of minimally supervised relation extraction; we believe structured learning is particularly crucial when learning from minimal supervision to help address the issues of missing data and overlapping relations. 3064 5 Conclusions In this paper we presented a hybrid approach to minimally supervised relation extraction that combines the benefi"
N19-1310,D17-1187,0,0.0475607,"raction (Wang et al., 2016; Zhang et al., 2017; Yu et al., 2015), experimenting with alternative sentence representations in our framework is an interesting direction for future work. Recent work has also shown improved performance by incorporating supervised training data on the sentence level (Angeli et al., 2014; Beltagy et al., 2018), in contrast our approach does not make use of any sentence-level labels during learning and therefore relies on less human supervision. Finally, prior work has explored a variety of methods to address the issue of noise introduced during distant supervision (Wu et al., 2017; Yaghoobzadeh et al., 2017; Qin et al., 2018). Another line of work has explored open-domain and unsupervised methods for IE (Yao et al., 2011; Ritter et al., 2012; Stanovsky et al., 2015; Huang et al., 2016; Weber et al., 2017). Universal schemas (Riedel et al., 2013) combine aspects of minimally supervised and unsupervised approaches to knowledge-base completion by applying matrix factorization techniques to multirelational data (Nickel et al., 2011; Bordes et al., 2013; Chang et al., 2014). Rows of the matrix typically model pairs of entities, and columns represent relations or syntactic p"
N19-1310,P13-2117,0,0.0556113,"Missing"
N19-1310,D12-1042,0,0.11572,"Missing"
N19-1310,E17-1111,0,0.0342807,"Missing"
N19-1310,D15-1174,0,0.0560622,"Missing"
N19-1310,N16-1103,0,0.542529,"the method described by Glorot and Bengio (2010). The Hoffmann et. al. sentential evaluation dataset is split into a development and test set and grid search on the development set was used to determine optimal values for the learning rate λ among {0.001, 0.01}, KB disagreement penalty scalar µ among {100, 200, · · · , 2000} and β1 /β2 bag size threshold for the adaptive learning rate among {10, 15, · · · , 40}. Other hyperparameters with fixed values are presented in Table 2. Neural Baselines: To demonstrate the effectiveness of the our approach, we compare against colless universal schema (Verga et al., 2016) in addition to the PCNN+ATT model of Lin et. al. (2016). After training the Lin et. al. model to predict observed facts in the KB, we use its attention layer to make mention-level predictions as follows: exp(rj · xi ) p(rj |xi ) = Pnr k=1 exp(rk · xi ) Where rj indicates the vector representation of the jth relation. Structured Baselines: In addition to initializing convolutional filters used in the φPCNN (·) factors randomly and performing structured learning of representations as in Equation 4, we also experimented with variants of MultiR and DNMAR, which are based on the structured percept"
N19-1310,D11-1135,0,0.0217828,"an interesting direction for future work. Recent work has also shown improved performance by incorporating supervised training data on the sentence level (Angeli et al., 2014; Beltagy et al., 2018), in contrast our approach does not make use of any sentence-level labels during learning and therefore relies on less human supervision. Finally, prior work has explored a variety of methods to address the issue of noise introduced during distant supervision (Wu et al., 2017; Yaghoobzadeh et al., 2017; Qin et al., 2018). Another line of work has explored open-domain and unsupervised methods for IE (Yao et al., 2011; Ritter et al., 2012; Stanovsky et al., 2015; Huang et al., 2016; Weber et al., 2017). Universal schemas (Riedel et al., 2013) combine aspects of minimally supervised and unsupervised approaches to knowledge-base completion by applying matrix factorization techniques to multirelational data (Nickel et al., 2011; Bordes et al., 2013; Chang et al., 2014). Rows of the matrix typically model pairs of entities, and columns represent relations or syntactic patterns (i.e., syntactic dependency paths observed between the entities). Structured Learning with Neural Representations: Prior work has inves"
N19-1310,N15-1155,0,0.0732284,"Missing"
P10-1044,J03-3005,0,0.292451,"Missing"
P10-1044,P08-1004,1,0.0945607,"2 in LinkLDA) are sampled sequentially conditioned on a fullassignment to all others, integrating out the parameters (Griffiths and Steyvers, 2004). This produces robust parameter estimates, as it allows computation of expectations over the posterior distribution LinkLDA Figure 2 illustrates the LinkLDA model in the plate notation, which is analogous to the model in (Erosheva et al., 2004). In particular note that each ai is drawn from a different hidden topic zi , however the zi ’s are drawn from the same distribution θr for a given relation r. To facilitate learn427 tracted by T EXT RUNNER (Banko and Etzioni, 2008) from 500 million Web pages. To create a generalization corpus from this large dataset. We first selected 3,000 relations from the middle of the tail (we used the 2,0005,000 most frequent ones)3 and collected all instances. To reduce sparsity, we discarded all tuples containing an NP that occurred fewer than 50 times in the data. This resulted in a vocabulary of about 32,000 noun phrases, and a set of about 2.4 million tuples in our generalization corpus. We inferred topic-argument and relation-topic multinomials (β, γ, and θ) on the generalization corpus by taking 5 samples at a lag of 50 aft"
P10-1044,P08-1119,0,0.0281168,"Missing"
P10-1044,D08-1007,0,0.732714,"ing multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to map arguments to a class (poor lexical co"
P10-1044,J98-2002,0,0.720098,"s used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce"
P10-1044,E09-1013,0,0.0251596,"e. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z. LinkLDA (Figure 2) lies between these two extremes, and as demonstrated in Section 4, it is the best model for our relation da"
P10-1044,N09-1042,0,0.010883,".washington.edu/research/ ldasp. 425 these relations. 2 Our task is to compute, for each argument ai of each relation r, a set of usual argument values (noun phrases) that it takes. For example, for the relation is headquartered in the first argument set will include companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is"
P10-1044,D09-1092,0,0.0188142,"vor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z. LinkLDA (Figure 2) lies between these two extremes, and as demonstrated in"
P10-1044,J02-2003,0,0.533616,"e parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable"
P10-1044,N10-1012,0,0.0218463,"Missing"
P10-1044,P06-1039,0,0.0314514,"Missing"
P10-1044,P10-1045,0,0.631386,"Missing"
P10-1044,N07-1071,0,0.755472,"del. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suf"
P10-1044,P07-1028,0,0.746694,"rd pseudodisambiguation task. Additionally, because L DA - SP is based on a formal probabilistic model, it has the advantage that it can naturally be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Erk (2007) showed the advantages of this approach over Resnik’s information-theoretic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbase"
P10-1044,P09-1070,0,0.0561091,"k is to compute, for each argument ai of each relation r, a set of usual argument values (noun phrases) that it takes. For example, for the relation is headquartered in the first argument set will include companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generate"
P10-1044,J02-3001,0,0.173292,"Missing"
P10-1044,W97-0209,0,0.907626,"Missing"
P10-1044,P99-1014,0,0.952895,"retic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbased approaches. Probably the closest to our work is a model proposed by Rooth et al. (1999), in which each class corresponds to a multinomial over relations and arguments and EM is used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectiona"
P10-1044,W03-0902,0,0.0148719,"dels to similar tasks. O poses a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on a set of human-plausibility judgments obtaining impressive results against Keller and Lapata’s (2003) Web hit-count based system. Van Durme and Gildea (2009) proposed applying LDA to general knowledge templates extracted using the K NEXT system (Schubert and Tong, 2003). In contrast, our work uses LinkLDA and focuses on modeling multiple arguments of a relation (e.g., the subject and direct object of a verb). 3 Topic Models for Selectional Prefs. 426 3.2 JointLDA α As a more tightly coupled alternative, we first propose JointLDA, whose graphical model is depicted in Figure 1. The key difference in JointLDA (versus LDA) is that instead of one, it maintains two sets of topics (latent distributions over words) denoted by β and γ, one for classes of each argument. A topic id k represents a pair of topics, βk and γk , that co-occur in the arguments of extracted r"
P10-1044,N09-1054,0,0.00743456,"companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z"
P14-1016,P11-1055,0,0.0115094,"consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Sec"
P14-1016,P11-1040,0,0.0339493,"emainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of our model are presented in Section 4. We present experimental results in Section 5 and conclude in Section 6. 166 Figure 2: Example of fetching tweets containing entity USC mention from Miranda Cosgrove (an American actress and singer-songwriter)’s twitter stream. Figure 1: Illustration of Goolge Plus “knowledge base”. fields such as protein relation extraction (Craven et al., 1999; Ravikumar et al., 2012), event extraction from Twitter (Benson et al., 2011), sentiment analysis (Go et al., 2009) and Wikipedia infobox generation (Wu and Weld, 2007). Education/Job We first used the Google Plus API5 (shown in Figure 1) to obtain a seed set of users whose profiles contain both their education/job status and a link to their twitter account.6 Then, we fetched tweets containing the mention of the education/job entity from each correspondent user’s twitter stream using Twitter’s search API7 (shown in Figure 2) and used them to construct positive bags of tweets expressing the associated attribute, namely E DUCATION(Useri , Entityj ), or E MPLOYER(Useri ,"
P14-1016,P09-1113,0,0.688937,"annotations, which are expensive and time consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The"
P14-1016,D13-1114,0,0.0432055,"N , HOMETOWN , LIVING LOCA TION , FAMILY MEMBERS and so on, where training data can be obtained by matching ground truth retrieved from multiple types of online social media such as Facebook, Google Plus, or LinkedIn. Our contributions are as follows: Related Work While user profile inference from social media has received considerable attention (Al Zamal et al., 2012; Rao and Yarowsky, 2010; Rao et al., 2010; Rao et al., 2011), most previous work has treated this as a classification task where the goal is to predict unary predicates describing attributes of the user. Examples include gender (Ciot et al., 2013; Liu and Ruths, 2013; Liu et al., 2012), age (Rao et al., 2010), or political polarity (Pennacchiotti and Popescu, 2011; Conover et al., 2011). A significant challenge that has limited previous efforts in this area is the lack of available training data. For example, researchers obtain training data by employing workers from Amazon Mechanical Turk to manually identify users’ gender from profile pictures (Ciot et al., 2013). This approach is appropriate for attributes such as gender with a small numbers of possible values (e.g., male or female), for which the values can be directly identified."
P14-1016,N13-1039,0,0.0107717,"s using Equ.6. k zi,e = argmax Ψ(z 0 , Xi , Fik ) z0 AFFINITY Job 14.5 Table 3: Affinity values for Education and Job. 5.1 Preprocessing and Experiment Setup Each tweet posting is tokenized using Twitter NLP tool introduced by Noah’s Ark14 with # and @ separated following tokens. We assume that attribute values should be either name entities or terms following @ and #. Name entities are extracted using Ritter et al.’s NER system (2011). Consecutive tokens with the same named entity tag are chunked (Mintz et al., 2009). Part-ofspeech tags are assigned based on Owoputi et al’s tweet POS system (Owoputi et al., 2013). Data is divided in halves. The first is used as training data and the other as testing data. (6) For NEIGH - LATENT setting, attributes for each node along the network are treated latent and user attribute prediction depends on attributes of his neighbors. The objective function for joint inference would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to network of the size we consider. Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al., 2010). Att"
P14-1016,D10-1048,0,0.0124034,"system (Owoputi et al., 2013). Data is divided in halves. The first is used as training data and the other as testing data. (6) For NEIGH - LATENT setting, attributes for each node along the network are treated latent and user attribute prediction depends on attributes of his neighbors. The objective function for joint inference would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to network of the size we consider. Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al., 2010). Attributes are initialized using only text features, maximizing Ψtext (e, Xi ), and ignoring network information. Then for each user we iteratively reestimate their profile given both their text features and network features (computed based on the current predictions made for their friends) which provide additional evidence. In this way, highly confident predictions will be made strictly from text in the first round, then the network can either support or contradict low confidence predictions as more decisions are made. This process continues until no changes are made at which point the algo"
P14-1016,D11-1141,1,0.441201,"Missing"
P14-1016,Q13-1030,1,0.223646,"re expensive and time consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our data"
P14-1016,D12-1042,0,0.00990537,"distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of"
P14-1016,P12-1076,0,0.011214,"verages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of our model are presented i"
Q13-1030,P11-1040,0,0.046149,"Missing"
Q13-1030,P07-1073,0,0.0926997,"Missing"
Q13-1030,W99-0613,0,0.16261,"all curve (from 0.16 to 0.34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, s"
Q13-1030,W02-1001,0,0.0880796,"the sentences in our text corpus: θ∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this app"
Q13-1030,P11-1055,1,0.951987,"e. To address this challenge, we propose a joint model of extraction from text and the process by which propositions are observed or missing in both the database and text. Our approach provides a natural way to incorporate side information in the form of a missing data model. For instance, popular entities such as Barack Obama already have good coverage in Freebase, so new extractions are more likely to be errors than those involving rare entities with poor coverage. Our approach to missing data is general and can be combined with various IE solutions. As a proof of concept, we extend MultiR (Hoffmann et al., 2011), a recent model for distantly supervised information extraction, to explicitly model missing data. These extensions complicate the MAP inference problem which is used as a subroutine in learning. This motivated us to explore a variety of approaches to inference in the joint extraction and missing data model. We explore both exact inference based on A* search and efficient approximate inference using local search. Our experiments demonstrate that with a carefully designed set of search operators, local search produces optimal solutions in most cases. Experimental results demonstrate large perf"
Q13-1030,P06-1096,0,0.0137234,"Missing"
Q13-1030,N10-1082,0,0.0108074,"penalty for extracting a fact not in Freebase, and produce an overall higher score. To avoid the problem of getting stuck in local optima, we propose an additional search operator which considers changing all variables, zi , which are currently assigned to a specific relation r, to a new relation r0 , resulting in an additional (k − 1)2 possible neighbors, in addition to the n × (k − 1) neighbors which come from the standard search operator. This aggregate-level search operator allows for more global moves which help to avoid local optima, similar to the type-level sampling approach for MCMC (Liang et al., 2010). At each iteration, we consider all n × (k − 1) + (k−1)2 possible neighboring solutions generated by both search operators, and pick the one with biggest overall improvement, or terminate the algorithm if no improvements can be made over the current solution. 20 random restarts were used for each infer372 ence problem. We found this approach to almost always find an optimal solution. In over 100,000 problems with 200 or fewer variables from the New York Times dataset used in Section 7, an optimal solution was missed in only 3 cases which was verified by comparing against optimal solutions fou"
Q13-1030,N13-1095,0,0.0522778,"on, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approach assuming a fixed proportion of true positives for each entity pair. The Min et al. (2013) approach is perhaps the most closely related of the recent approaches for distant supervision. However, there are a number of key differences: (1) They impose a hard constraint on the proportion of true positive examples for each entity pair, whereas we jointly model relation extraction and missing data in the text and KB. (2) They only handle the case of missing information in the database and not in the text. (3) Their mo"
Q13-1030,P09-1113,0,0.938168,"Missing"
Q13-1030,W04-2407,0,0.0215227,"Times text, features and Freebase relations developed by Riedel et. al. (2010) which was also used by Hoffmann et. al. (2011). This dataset is constructed by extracting named entities from 1.8 million New York Times articles, which are then match against entities in Freebase. Sentences which contain pairs of entities participating in one or more relations are then used as training examples for those relations. The sentencelevel features include word sequences appearing in context with the pair of entities, in addition to part 373 of speech sequences, and dependency paths from the Malt parser (Nivre et al., 2004). 7.1.1 Baseline To evaluate the effect of modeling missing data in distant supervision, we compare against the MultiR model for distant supervision (Hoffmann et al., 2011), a state of the art approach for binary relation extraction which is the most similar previous work, and models facts in Freebase as hard constraints disallowing the possibility of missing information in either the text or the database. To make our experiment as controlled as possible and ruleout the possibility of differences in performance due to implementation details, we compare against our own re-implementation of Mult"
Q13-1030,N13-1008,0,0.0576422,"Missing"
Q13-1030,D11-1141,1,0.107054,"Missing"
Q13-1030,D12-1042,0,0.677887,"Missing"
Q13-1030,P12-1076,0,0.336988,"distant supervision to train event extractors from Twitter. Mintz et. al. (2009) used a set of relations from Freebase as a distant source of supervision to learn to extract information from Wikipedia. Ridel et. al. (2010), Hoffmann et. al. (2011), and Surdeanu et. al. (2012) presented a series of models casting distant supervision as a multiple-instance learning problem (Dietterich et al., 1997). Recent work has begun to address the challenge of noise in heuristically labeled training data generated by distant supervision, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approa"
Q13-1030,P10-1149,0,0.0213479,"34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, so we presented an approach ba"
Q13-1030,P13-2117,0,0.593462,"Missing"
Q13-1030,D07-1071,1,0.364681,"∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this approximation to the gradient requires solving two infe"
Q14-1034,S12-1051,0,0.0148988,"f Green Ryu The Clippers Candice Robert Woods Amber Reggie Miller filtered random 0.0 0.2 0.4 0.6 0.8 Percentage of Positive Judgements Figure 5: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield. sio, 2008) with other annotators. We also compute inter-annotator agreement with an expert annotator on 971 sentence pairs. In the expert annotation, we adopt a 5-point Likert scale to measure the degree of semantic similarity between sentences, which is defined by Agirre et al. (2012) as follows: 5: Completely equivalent, as they mean the same thing; 4: Mostly equivalent, but some unimportant details differ; 3: Roughly equivalent, but some important information differs/missing. 2: Not equivalent, but share some details; 1: Not equivalent, but are on the same topic; 0: On different topics. method is inspired by a typical problem in extractive summarization, that the salient sentences are likely redundant (paraphrases) and need to be removed in the output summaries. We employ the scoring method used in SumBasic (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007), a sim"
Q14-1034,J08-4004,0,0.0498405,"Missing"
Q14-1034,R13-1026,1,0.754437,"otation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the pair, specifying whether the two words have same or different POS tags and what the specific tags are. We use the Twitter Part-Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3” in tweets about basketball game, “RIP” in tweets about a celebrity’s death. We use G2 log-likelihoodratio statistic, which has been frequently used in NLP, as a measure of word associations (Dunning, 1993; Moore, 2004). The significant scores are computed for"
Q14-1034,P12-1056,0,0.0569989,"Missing"
Q14-1034,C04-1051,0,0.816209,"emantic similarity systems. We make this dataset available to the research community.2 2 Joint Word-Sentence Paraphrase Model We present a new latent variable model that jointly captures paraphrase relations between sentence pairs and word pairs. It is very different from previous approaches in that its primary design goal and motivation is targeted towards short, lexically diverse text on the social web. 2.1 At-least-one-anchor Assumption Much previous work on paraphrase identification has been developed and evaluated on a specific benchmark dataset, the Microsoft Research Paraphrase Corpus (Dolan et al., 2004), which is de2 The dataset and code are made available at: SemEval-2015 shared task http://alt.qcri.org/semeval2015/ task1/ and https://github.com/cocoxu/ twitterparaphrase/ Corpus News (Dolan and Brockett, 2005) Twitter (This Work) Examples ◦ Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier. ◦ With the scandal hanging over Stewart’s company, revenue in the first quarter of the year dropped 15 percent from the same period a year earlier. ◦ The Senate Select Committee on Intelligence is preparing a blistering report on prewar intelligence on Iraq."
Q14-1034,W10-0735,0,0.0293585,"Missing"
Q14-1034,I05-5002,1,0.535018,"Missing"
Q14-1034,P11-1020,1,0.373917,"2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking Mechanical Turk workers to caption the action in short video segments. Similarly, Burrows et al. (2012) asked crowdsourcing workers to rewrite selected excerpts from books. Ling et al. (2014) crowdsourced bilingual parallel text using Twitter as the source of data. In contrast, we design a simple crowdsourcing task requiring only binary judgements on sentences collected from Twitter. There are several advantages as compared to existing work: a) the corpus also covers a very diverse range of topics and linguistic expressions, especially colloq"
Q14-1034,W02-1001,0,0.012317,"-or function; that is, if there exists at least one j such that zj = 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ"
Q14-1034,P09-1053,0,0.877987,"ng to Twitter, trends are determined by an algorithm which 6 More information about Twitter’s APIs: https://dev. twitter.com/docs/api/1.1/overview 442 =4 turk =5 turk turk =3 =2 =1 turk turk turk =0 The resulting system M ULTI P-PE provides consistently better precision and recall over the LEXLATENT model, as shown on the right in Figure 3. The M ULTI P-PE system outperforms LEXLATENT significantly according to a paired ttest with ρ less than 0.05. Our proposed M UL TI P takes advantage of Twitter’s specific properties and provides complementary information to previous approaches. Previously, Das and Smith (2009) has also used a product of experts to combine a lexical and a syntax-based model together. expert=0 Figure 4: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are"
Q14-1034,H93-1035,0,0.218833,"Missing"
Q14-1034,W04-3208,0,0.0872468,"Missing"
Q14-1034,C04-1151,0,0.0139898,"Missing"
Q14-1034,P12-1091,0,0.20052,"al Linguistics. (a) (b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined system shows significa"
Q14-1034,P13-1024,0,0.0363338,"on of a strong baseline used by Das and Smith (2009). competitive performance on the MSR corpus. The second baseline is a state-of-the-art unsupervised method, Weighted Textual Matrix Factorization (WTMF),4 which is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The original model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). We enhance the model with 1.6 million sentences from Twitter as suggested by Guo et al. (2013). Ji and Eisenstein (2013) presented a state-ofthe-art ensemble system, which we call LEXDISCRIM.5 It directly combines both discriminativelytuned latent features and surface lexical features into a SVM classifier. Specifically, the latent representation of a pair of sentences v~1 and v~2 is converted into a feature vector, [v~1 + v~2 , |v~1 − v~2 |], by concatenating the element-wise sum v~1 + v~2 and absolute different |v~1 − v~2 |. We also introduce a new baseline, LEXLATENT, which is a simplified version of LEXDISCRIM and easy to reproduce. It uses the same method to combine latent feature"
Q14-1034,D12-1039,0,0.0269215,"r to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the pair, specifying whether the two words have same or different POS tags and what the specific tags are. We use the Twitter Part-Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3"
Q14-1034,P11-1055,0,0.709502,"= 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ ← 0 2: for i ← 1 to n do 3: extract all possible word pairs wi exp(θ · f (z"
Q14-1034,N06-2015,0,0.015456,"rent paraphrase identification approaches on Twitter data. *An enhanced version that uses additional 1.6 million sentences from Twitter. ** Reimplementation of a strong baseline used by Das and Smith (2009). competitive performance on the MSR corpus. The second baseline is a state-of-the-art unsupervised method, Weighted Textual Matrix Factorization (WTMF),4 which is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The original model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). We enhance the model with 1.6 million sentences from Twitter as suggested by Guo et al. (2013). Ji and Eisenstein (2013) presented a state-ofthe-art ensemble system, which we call LEXDISCRIM.5 It directly combines both discriminativelytuned latent features and surface lexical features into a SVM classifier. Specifically, the latent representation of a pair of sentences v~1 and v~2 is converted into a feature vector, [v~1 + v~2 , |v~1 − v~2 |], by concatenating the element-wise sum v~1 + v~2 and absolute different |v~1 − v~2 |. We also"
Q14-1034,D13-1090,1,0.286981,"(b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined system shows significant improvement over the s"
Q14-1034,P06-1096,0,0.0226525,"Missing"
Q14-1034,W14-3356,0,0.0220602,"at at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking Mechanical Turk workers to caption the action in short video segments. Similarly, Burrows et al. (2012) asked crowdsourcing workers to rewrite selected excerpts from books. Ling et al. (2014) crowdsourced bilingual parallel text using Twitter as the source of data. In contrast, we design a simple crowdsourcing task requiring only binary judgements on sentences collected from Twitter. There are several advantages as compared to existing work: a) the corpus also covers a very diverse range of topics and linguistic expressions, especially colloquial language, which is different from and thus complements previous paraphrase corpora; b) the paraphrase corpus collected contains a representative proportion of both negative and positive instances, while lack of good negative examples was"
Q14-1034,D08-1084,0,0.0999499,"set zτ∗ = 1 and zj∗ = arg maxx∈0,1 φ(x, wj ; θ) for all j 6= τ ; for a negative example, we set z∗i = 0. The time complexity of both inferences for one sentence pair is O(|W (s)|2 ), where |W (s)|2 is the number of word pairs. In practice, we use online learning instead of optimizing the full objective. The detailed learning algorithm is presented in Figure 2. Following Hoffmann et al. (2011), we use 50 iterations in the experiments. 2.4 Feature Design 3 At the word-level, our discriminative model allows use of arbitrary features that are similar to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tag"
Q14-1034,J10-3003,0,0.0844497,"l−m e arms that have the highest estimated reward until reaching the maximum l = 10 annotations for any topic to insure data diversity. We tune the parameters m to be 1 and  to be between 0.35 ∼ 0.55 through simulation experiments, by artificially duplicating a small amount of real annotation data. We then apply this MAB algorithm in the real-world. We explore 500 random topics and then exploited 100 of them. The yield of paraphrases rises to 688 out of 2000 sentence pairs by 444 Related Work Automatic Paraphrase Identification has been widely studied (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of various techniques.8 Many recent high-performance approaches use system combination (Das and Smith, 2009; Madnani et al., 2012; Ji and Eisenstein, 2013). For example, Madnani et al. (2012) combines multiple sophisticated machine translation metrics using a metaclassifier. An earlier attempt on Twitter data is that of Xu et al. (2013b). They limited the search space to only the tweets that explicitly mention a same date and a same named entity, however there remain a considerable amount of mislabels in their data.9 Zanzotto et al. (2011) also experim"
Q14-1034,N12-1019,0,0.30475,"to be between 0.35 ∼ 0.55 through simulation experiments, by artificially duplicating a small amount of real annotation data. We then apply this MAB algorithm in the real-world. We explore 500 random topics and then exploited 100 of them. The yield of paraphrases rises to 688 out of 2000 sentence pairs by 444 Related Work Automatic Paraphrase Identification has been widely studied (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of various techniques.8 Many recent high-performance approaches use system combination (Das and Smith, 2009; Madnani et al., 2012; Ji and Eisenstein, 2013). For example, Madnani et al. (2012) combines multiple sophisticated machine translation metrics using a metaclassifier. An earlier attempt on Twitter data is that of Xu et al. (2013b). They limited the search space to only the tweets that explicitly mention a same date and a same named entity, however there remain a considerable amount of mislabels in their data.9 Zanzotto et al. (2011) also experimented with SVM tree kernel methods on Twitter data. Departing from the previous work, we propose a latent variable model to jointly infer the correspondence between words"
Q14-1034,W04-3243,0,0.0161665,"Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3” in tweets about basketball game, “RIP” in tweets about a celebrity’s death. We use G2 log-likelihoodratio statistic, which has been frequently used in NLP, as a measure of word associations (Dunning, 1993; Moore, 2004). The significant scores are computed for each trend on an average of about 1500 sentences and converted to binary features for every word pair, indicating whether the two words are both significant or not. Our topical features are novel and were not used in previous work. Following Riedel et al. (2010) and Hoffmann et al. (2011), we also incorporate conjunction features into our system for better accuracy, namely Word+POS, Word+Topical and Word+POS+Topical features. 3 https://github.com/knowitall/morpha 439 Experiments Data It is nontrivial to gather a gold-standard dataset of naturally occur"
Q14-1034,N12-1034,0,0.334376,"Missing"
Q14-1034,Q13-1030,1,0.648609,"ata is released by Xu et al. (2013b) at: https:// github.com/cocoxu/twitterparaphrase/ 2011; Yao et al., 2013a,b), but different in that the paraphrase task requires additional sentence alignment modeling with no word alignment data. Our approach is also inspired by Fung and Cheung’s (2004a; 2004b) work on bootstrapping bilingual parallel sentence and word translations from comparable corpora. Multiple Instance Learning (Dietterich et al., 1997) has been used by different research groups in the field of information extraction (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking"
Q14-1034,I05-5011,0,0.0125198,"and Yangfeng Ji4 1 University of Pennsylvania, Philadelphia, PA, USA {xwe, ccb}@cis.upenn.edu 2 The Ohio State University, Columbus, OH, USA ritter.1492@osu.edu 3 Microsoft Research, Redmond, WA, USA billdol@microsoft.com 4 Georgia Institute of Technology, Atlanta, GA, USA jiyfeng@gatech.edu Abstract (e.g. oscar nom’d doc ↔ Oscar-nominated documentary). In this paper, we investigate the task of determining whether two tweets are paraphrases. Previous work has exploited a pair of shared named entities to locate semantically equivalent patterns from related news articles (Shinyama et al., 2002; Sekine, 2005; Zhang and Weld, 2013). But short sentences in Twitter do not often mention two named entities (Ritter et al., 2012) and require nontrivial generalization from named entities to other words. For example, consider the following two sentences about basketball player Brook Lopez from Twitter: We present M ULTI P (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent vari"
Q14-1034,D12-1042,0,0.031759,"te_of_ the_art) 9 The data is released by Xu et al. (2013b) at: https:// github.com/cocoxu/twitterparaphrase/ 2011; Yao et al., 2013a,b), but different in that the paraphrase task requires additional sentence alignment modeling with no word alignment data. Our approach is also inspired by Fung and Cheung’s (2004a; 2004b) work on bootstrapping bilingual parallel sentence and word translations from comparable corpora. Multiple Instance Learning (Dietterich et al., 1997) has been used by different research groups in the field of information extraction (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraph"
Q14-1034,P11-2044,0,0.0133162,"maxx∈0,1 φ(x, wj ; θ) for all j 6= τ ; for a negative example, we set z∗i = 0. The time complexity of both inferences for one sentence pair is O(|W (s)|2 ), where |W (s)|2 is the number of word pairs. In practice, we use online learning instead of optimizing the full objective. The detailed learning algorithm is presented in Figure 2. Following Hoffmann et al. (2011), we use 50 iterations in the experiments. 2.4 Feature Design 3 At the word-level, our discriminative model allows use of arbitrary features that are similar to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the p"
Q14-1034,U06-1019,0,0.813586,"on for Computational Linguistics. (a) (b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined sy"
Q14-1034,D13-1008,0,0.0679257,"a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one word pair (we call it"
Q14-1034,P13-2117,1,0.909424,"us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one"
Q14-1034,W13-2515,1,0.806798,"us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one"
Q14-1034,P13-2123,1,0.512784,"Missing"
Q14-1034,D13-1056,1,0.747986,"Missing"
Q14-1034,D11-1061,0,0.281688,"3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one word pair (we call it an anchor and highlight with underscores; the words in the anchor pair need not be identical) that is indicative of sentential paraphra"
Q14-1034,D07-1071,0,0.0109552,"sts at least one j such that zj = 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ ← 0 2: for i ← 1 to n do 3: extract all possible wo"
Q14-1034,D13-1183,0,0.0609342,"i4 1 University of Pennsylvania, Philadelphia, PA, USA {xwe, ccb}@cis.upenn.edu 2 The Ohio State University, Columbus, OH, USA ritter.1492@osu.edu 3 Microsoft Research, Redmond, WA, USA billdol@microsoft.com 4 Georgia Institute of Technology, Atlanta, GA, USA jiyfeng@gatech.edu Abstract (e.g. oscar nom’d doc ↔ Oscar-nominated documentary). In this paper, we investigate the task of determining whether two tweets are paraphrases. Previous work has exploited a pair of shared named entities to locate semantically equivalent patterns from related news articles (Shinyama et al., 2002; Sekine, 2005; Zhang and Weld, 2013). But short sentences in Twitter do not often mention two named entities (Ritter et al., 2012) and require nontrivial generalization from named entities to other words. For example, consider the following two sentences about basketball player Brook Lopez from Twitter: We present M ULTI P (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we ac"
Q14-1034,N03-1031,0,\N,Missing
Q14-1034,W10-0711,0,\N,Missing
Q14-1034,J13-3001,0,\N,Missing
R13-1026,R13-1011,1,0.223646,"Missing"
R13-1026,A00-1031,0,0.0624805,"work on bootstrapped PoS tagging is that of Clark et al. (2003), who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpu"
R13-1026,J95-4004,0,0.451368,", who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagse"
R13-1026,gimenez-marquez-2004-svmtool,0,0.0991477,"Missing"
R13-1026,W03-0407,0,0.0502941,"Missing"
R13-1026,P11-2008,0,0.690806,"Missing"
R13-1026,E03-1009,0,0.0208576,"Missing"
R13-1026,W02-2006,0,0.249811,"Missing"
R13-1026,P11-1038,0,0.117493,"Missing"
R13-1026,P02-1022,1,0.755704,"Missing"
R13-1026,P12-1109,0,0.017024,"Missing"
R13-1026,J93-2004,0,0.0447464,"downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagset based on the Penn Treebank tagset,plus four new tags for URLs (URL), hashtags (HT), username mentions (USR) and retweet signifiers (RT). The DCU dataset of 14K tokens (Foster et al., 2011) is also based on the Penn Treebank (PTB) set, but does not have the same new tags as TPos, and uses slightly differe"
R13-1026,D11-1141,1,0.750123,"arning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagset based on the Penn Treebank tagset,plus four new tags for URLs (URL), hashtags (HT), username mentions (USR) and retweet signifiers (RT). The DCU dataset of 14K tokens (Foster et al., 2011) is also based on the Penn Treebank (PTB) set, but does not have the same new tags as TPos, and uses slightly different tokenisation. The ARK corpus of 39K tokens (Gimpel et al., 2011) uses a novel tagset, which, while suitable for the microblog genre, is somewhat less descriptive than the PTB sets on many points. For example, its V tag corresponds to any verb, conflating PTB’s VB, VBD, VBG, VB"
R13-1026,N03-1033,0,0.114204,"t al., 2011). Finally, classic work on bootstrapped PoS tagging is that of Clark et al. (2003), who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently availa"
R13-1026,W12-0601,0,\N,Missing
S13-2052,baccianella-etal-2010-sentiwordnet,0,0.68035,",131 471 648 430 57 2,734 1,541 160 1,071 1,104 159 Vocabulary Size 20,012 4,426 11,736 3,562 Table 2: Statistics for Subtask A. We then identified popular topics as those named entities that are frequently mentioned in association with a specific date (Ritter et al., 2012). Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from training and spanned later periods. To identify messages that express sentiment towards these topics, we filtered the tweets using SentiWordNet (Baccianella et al., 2010). We removed messages that contained no sentimentbearing words, keeping only those with at least one word with positive or negative sentiment score that is greater than 0.3 in SentiWordNet for at least one sense of the words. Without filtering, we found class imbalance to be too high.3 Twitter messages are rich in social media features, including out-of-vocabulary (OOV) words, emoticons, and acronyms; see Table 1. A large portion of the OOV words are hashtags (e.g., #sheenroast) and mentions (e.g., @tash jade). Corpus Twitter - Training Twitter - Dev Twitter - Test SMS - Test Filtering based o"
S13-2052,C10-2005,0,0.783288,"sing beyond those encountered when working with more traditional text genres such as newswire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is c"
S13-2052,W10-2914,0,0.0363732,"ith more traditional text genres such as newswire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is conveyed, e.g., we can study the properties"
S13-2052,S10-1097,0,0.598998,"wire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is conveyed, e.g., we can study the properties of persuasive language or those associated wit"
S13-2052,D11-1141,1,0.104433,"ity Classification Given a message, decide whether it is of positive, negative, or neutral sentiment. For messages conveying both a positive and a negative sentiment, whichever is the stronger one was to be chosen. http://www.daedalus.es/TASS/corpus.php 313 Dataset Creation In the following sections we describe the collection and annotation of the Twitter and SMS datasets. 3.1 Data Collection Twitter is the most common micro-blogging site on the Web, and we used it to gather tweets that express sentiment about popular topics. We first extracted named entities using a Twitter-tuned NER system (Ritter et al., 2011) from millions of tweets, which we collected over a one-year period spanning from January 2012 to January 2013; we used the public streaming Twitter API to download tweets. Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective, positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark the position of its start and end in the text boxes below. The number above each word indicates its position. The word/phrase will be generated in the adjacent textbox so that you can confirm"
S14-2009,baccianella-etal-2010-sentiwordnet,0,0.393694,"luding the test data) was used to develop a sentiment lexicon, and this lexicon was used to automatically label additional Tweet/SMS messages and then used with the original data to train the classifier, then such a system would be considered unconstrained. 3 Positive Positive Negative 3,662 575 1,572 492 982 33 427 1,466 340 601 394 202 40 304 Objective / Neutral 4,600 739 1,640 1,207 669 13 411 Table 2: Dataset statistics for Subtask B. 3.2 Annotation We annotated the new tweets as in 2013: by identifying tweets from popular topics that contain sentiment-bearing words by using SentiWordNet (Baccianella et al., 2010) as a filter. We altered the annotation task for the sarcastic tweets, displaying them to the Mechanical Turk annotators without the #sarcasm hashtag; the Turkers had to determine whether the tweet is sarcastic on their own. Moreover, we asked Turkers to indicate the degree of sarcasm as (a) definitely sarcastic, (b) probably sarcastic, and (c) not sarcastic. As in 2013, we combined the annotations using intersection, where a word had to appear in 2/3 of the annotations to be accepted. An annotated example from each source is shown in Table 3. Datasets In this section, we describe the process"
S14-2009,S13-2053,0,0.592137,"sometimes it was worse, sometimes it performed the same. Thus, we decided to produce a single ranking, including both constrained and unconstrained systems, where we mark the latter accordingly. 5.1 The features used were quite varied, including word-based (e.g., word and character ngrams, word shapes, and lemmata), syntactic, and Twitter-specific such as emoticons and abbreviations. The participants still relied heavily on lexicons of opinion words, the most popular ones being the same as in 2013: MPQA, SentiWordNet and Bing Liu’s opinion lexicon. Popular this year was also the NRC lexicon (Mohammad et al., 2013), created by the best-performing team in 2013, which is top-performing this year as well. Subtask A Table 4 shows the results for subtask A, which attracted 27 submissions from 21 teams. There were seven unconstrained submissions: five teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only. The best systems were constrained. All participating systems outperformed the majority class baseline by a sizable margin. 5.2 Preprocessing of tweets was still a popular technique. In addition to standard NLP steps such as tokenization, stemming, lemm"
S14-2009,C10-2005,0,0.376895,"Missing"
S14-2009,S13-2052,1,0.554296,"resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment. Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media. Toward that goal, we created the SemEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013). It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity. This year, we extended the corpus by adding new tweets and LiveJournal sentences. Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). In fact, sarcasm indicates that the message polarity should be flipped. With this in mind, this year, we also evaluate on sarcastic tweets. We describe the Sentiment Analysi"
S14-2009,W10-2914,0,0.0317872,"14 Task 9: Sentiment Analysis in Twitter Sara Rosenthal Columbia University Alan Ritter Carnegie Mellon University sara@cs.columbia.edu rittera@cs.cmu.edu Preslav Nakov Qatar Computing Research Institute Veselin Stoyanov Johns Hopkins University pnakov@qf.org.qa ves@cs.jhu.edu Abstract Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document. How to handle such challenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created"
S14-2009,S10-1097,0,0.139604,"Rosenthal Columbia University Alan Ritter Carnegie Mellon University sara@cs.columbia.edu rittera@cs.cmu.edu Preslav Nakov Qatar Computing Research Institute Veselin Stoyanov Johns Hopkins University pnakov@qf.org.qa ves@cs.jhu.edu Abstract Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document. How to handle such challenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small a"
S14-2009,P11-2008,0,0.0644631,"Missing"
S14-2009,W02-1011,0,0.0331504,"er, tweets and SMS messages are short: a sentence or a headline rather than a document. How to handle such challenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment. Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conve"
S14-2009,S14-2004,0,0.0177631,"llenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment. Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media. Toward that goal, we created the SemEval Tweet corpus as part of our inaugural Sentiment Ana"
S14-2009,P11-2102,0,0.0688453,"understanding of how sentiment is conveyed in Social Media. Toward that goal, we created the SemEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013). It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity. This year, we extended the corpus by adding new tweets and LiveJournal sentences. Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). In fact, sarcasm indicates that the message polarity should be flipped. With this in mind, this year, we also evaluate on sarcastic tweets. We describe the Sentiment Analysis in Twitter task, ran as part of SemEval-2014. It is a continuation of the last year’s task that ran successfully as part of SemEval2013. As in 2013, this was the most popular SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21 teams) and 50 submissions for subtask B (44 teams). This year, we introduced three new test sets: (i) regular tweets, (ii) sarcastic tweets, and"
S14-2009,D11-1141,1,0.0497869,"were constrained. All participating systems outperformed the majority class baseline by a sizable margin. 5.2 Preprocessing of tweets was still a popular technique. In addition to standard NLP steps such as tokenization, stemming, lemmatization, stopword removal and POS tagging, most teams applied some kind of Twitter-specific processing such as substitution/removal of URLs, substitution of emoticons, word normalization, abbreviation lookup, and punctuation removal. Finally, several of the teams used Twitter-tuned NLP tools such as part of speech and named entity taggers (Gimpel et al., 2011; Ritter et al., 2011). Subtask B The results for subtask B are shown in Table 5. The subtask attracted 50 submissions from 44 teams. There were eight unconstrained submissions: six teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only. As for subtask A, the best systems were constrained. Again, all participating systems outperformed the majority class baseline; however, some systems were very close to it. 6 The similarity of preprocessing techniques, NLP tools, classifiers and features used in 2013 and this year is probably partially due to many teams partic"
S14-2009,W13-1605,0,0.0809238,"Missing"
S15-2078,baccianella-etal-2010-sentiwordnet,0,0.572805,"rs to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other"
S15-2078,C10-2005,0,0.0833505,"ion are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we dis"
S15-2078,W10-2914,0,0.0273404,"iu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of"
S15-2078,P11-2102,0,0.0395462,"dia such as Weblogs, microblogs, and discussion forums are used daily to express personal thoughts, which allows researchers to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, speci"
S15-2078,N03-2012,0,0.0438636,"of prior polarity of a phrase. 1 Svetlana Kiritchenko Introduction Social media such as Weblogs, microblogs, and discussion forums are used daily to express personal thoughts, which allows researchers to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et"
S15-2078,S12-1047,1,0.517008,"ator agreement is low, and annotators struggle even to remain self-consistent. In contrast, it is much easier to make relative judgments, e.g., to say whether one word is more positive than another. Moreover, it is possible to derive an absolute score from pairwise judgments, but this requires a much larger number of annotations. Fortunately, there are schemes that allow to infer more pairwise annotations from less judgments. 455 One such annotation scheme is MaxDiff (Louviere, 1991), which is widely used in market surveys (Almquist and Lee, 2009); it was also used in a previous SemEval task (Jurgens et al., 2012). In MaxDiff, the annotator is presented with four terms and asked which term is most positive and which is least positive. By answering just these two questions, five out of six pairwise rankings become known. Consider a set in which a judge evaluates A, B, C, and D. If she says that A and D are the most and the least positive, we can infer the following: A &gt; B, A &gt; C, A &gt; D, B &gt; D, C &gt; D. The responses to the MaxDiff questions can then be easily translated into a ranking for all the terms and also into a real-valued score for each term. We crowdsourced the MaxDiff questions on CrowdFlower, r"
S15-2078,W13-1605,0,0.0482134,"Missing"
S15-2078,S13-2053,1,0.407706,"of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five subtasks, with most teams participating in more than one subtask. This year the task included reruns of two legacy subtasks, which asked to detect the sentiment expressed in a tweet or by a particular phrase in a tweet. The task further added three new subtasks. The first two focused on the sentiment towards a given topic in a single tweet or in a set of tweets, respect"
S15-2078,S13-2052,1,0.689604,"ing sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five sub"
S15-2078,S10-1097,0,0.0524882,"ion (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter ha"
S15-2078,W02-1011,0,0.0472385,"t into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled en"
S15-2078,D11-1141,1,0.0366066,"ive. • Subtask E. Determining Strength of Association of Twitter Terms with Positive Sentiment (Degree of Prior Polarity): Given a word/phrase, propose a score between 0 (lowest) and 1 (highest) that is indicative of the strength of association of that word/phrase with positive sentiment. If a word/phrase is more positive than another one, it should be assigned a relatively higher score. 452 Data Collection Subtasks A–D First, we gathered tweets that express sentiment about popular topics. For this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned NER system (Ritter et al., 2011). Our initial training set was collected over a one-year period spanning from January 2012 to January 2013. Each subsequent Twitter test set was collected a few months prior to the corresponding evaluation. We used the public streaming Twitter API to download the tweets. We then identified popular topics as those named entities that are frequently mentioned in association with a specific date (Ritter et al., 2012). Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from tr"
S15-2078,S14-2009,1,0.518377,"d on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five subtasks, with most teams pa"
S15-2078,H05-1044,0,0.415097,"s much smaller: 30.3 for B vs. 26.7 for C. Finally, the last column in the table reports the results for the 75 sarcastic 2015 tweets. The winner here is KLUEless with an F1 of 39.26, followed by TwitterHawk with F1 =31.30, and then by UMDuluth-CS8761 with F1 =29.91. 460 Subtask D: Trend Towards a Topic Subtask E: Degree of Prior Polarity Ten teams participated in subtask E. Many chose an unsupervised approach and leveraged newlycreated and pre-existing sentiment lexicons such as the Hashtag Sentiment Lexicon, the Sentiment140 Lexicon (Kiritchenko et al., 2014), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and SentiWordNet (Baccianella et al., 2010), among others. Several participants further automatically created their own sentiment lexicons from large collections of tweets. Three teams, including the winner INESC-ID, adopted a supervised approach and used word embeddings (supplemented with lexicon features) to train a regression model. The results are presented in Table 14. The last row shows the performance of a lexicon-based baseline. For this baseline, we chose the two most frequently used existing, publicly available, and automatically generated sentiment lexicons: Hashtag Sentiment Lexi"
S15-2078,P14-1029,1,0.129095,"tag Sentiment tweet corpora (Kiritchenko et al., 2014). In order to reduce the skewness towards the neutral class, we selected terms from different ranges of automatically determined sentiment values as provided by the corresponding Sentiment140 and Hashtag Sentiment lexicons. The term set comprised regular English words, hashtagged words (e.g., #loveumom), misspelled or creatively spelled words (e.g., parlament or happeeee), abbreviations, shortenings, and slang. Some terms were negated expressions such as no fun. (It is known that negation impacts the sentiment of its scope in complex ways (Zhu et al., 2014).) We annotated these terms for degree of sentiment manually. Further details about the data collection and the annotation process can be found in Section 3.2.2 as well as in (Kiritchenko et al., 2014). The trial dataset consisted of 200 instances, and no training dataset was provided. Note, however, that the trial data was large enough to be used as a development set, or even as a training set. Moreover, the participants were free to use any additional manually or automatically generated resources when building their systems for subtask E. The testset included 1,315 instances. 453 Annotation"
S16-1001,S16-1024,0,0.0330359,"Missing"
S16-1001,S16-1036,0,0.0182689,"Missing"
S16-1001,S16-1033,0,0.0180935,"Missing"
S16-1001,S16-1010,0,0.0389367,"Missing"
S16-1001,S16-1032,0,0.0327633,"Missing"
S16-1001,S16-1015,0,0.0357065,"Missing"
S16-1001,S16-1019,0,0.0273999,"Missing"
S16-1001,S16-1173,0,0.130145,"Missing"
S16-1001,S16-1017,0,0.0167507,"Missing"
S16-1001,S16-1011,0,0.0306781,"Missing"
S16-1001,S16-1037,0,0.0231103,"Missing"
S16-1001,S16-1034,0,0.0319013,"Missing"
S16-1001,S16-1020,0,0.0329578,"Missing"
S16-1001,S16-1021,0,0.0169336,"Missing"
S16-1001,S16-1028,0,0.0313488,"Missing"
S16-1001,S16-1039,0,0.0328294,"Missing"
S16-1001,S16-1014,0,0.0235931,"Missing"
S16-1001,S16-1018,0,0.0272029,"Missing"
S16-1001,E12-1062,0,0.0190165,"16, pages 1–18, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 1.2 2 Quantification We replace classification with quantification, i.e., supervised class prevalence estimation. With regard to Twitter, hardly anyone is interested in whether a specific person has a positive or a negative view of the topic. Rather, applications look at estimating the prevalence of positive and negative tweets about a given topic. Most (if not all) tweet sentiment classification studies conducted within political science (Borge-Holthoefer et al., 2015; Kaya et al., 2013; Marchetti-Bowick and Chambers, 2012), economics (Bollen et al., 2011; O’Connor et al., 2010), social science (Dodds et al., 2011), and market research (Burton and Soboleva, 2011; Qureshi et al., 2013), use Twitter with an interest in aggregate data and not in individual classifications. Estimating prevalences (more generally, estimating the distribution of the classes in a set of unlabelled items) by leveraging training data is called quantification in data mining and related fields. Previous work has argued that quantification is not a mere byproduct of classification, since (a) a good classifier is not necessarily a good quant"
S16-1001,N13-1090,0,0.00624088,"t frequently used by the participants are Theano and Keras. Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is scarce. The use of distant supervision is ubiquitous; this is natural, since there is an abundance of freely available tweets labelled according to sentiment (possibly with silver labels only, e.g., emoticons), and it is intuitive that their use as additional training data could be helpful. Another ubiquitous technique is the use of word embeddings, usually generated via either word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014); most authors seem to use general-purpose, pre-trained embeddings, while some authors also use customized word embeddings, trained either on the Tweet 2016 dataset or on tweet datasets of some sort. Nothing radically new seems to have emerged with respect to text preprocessing; as in previous editions of this task, participants use a mix of by now obvious techniques, such as negation scope detection, elongation normalization, detection of amplifiers and diminishers, plus the usual extraction of word n-grams, character n-grams, and POS ngrams. The use of sent"
S16-1001,S16-1029,0,0.0228175,"Missing"
S16-1001,S16-1005,0,0.0353255,"Missing"
S16-1001,S13-2052,1,0.676646,"s. Subtask E is similar to SemEval-2015 Task 10 Subtask D, which consisted of the following problem: Given a set of messages on a given topic from the same period of time, classify the overall sentiment towards the topic in these messages as strongly positive, weakly positive, neutral, weakly negative, or strongly negative. Note that in SemEval-2015 Task 10 Subtask D, exactly one of the five classes had to be chosen, while in our Subtask E, a distribution across the five classes has to be estimated. 2 Note that we retired the expression-level subtask A, which was present in SemEval 2013–2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b). Goal As per the above discussion, Subtasks B to E are new. Conceptually, they form a 2×2 matrix, as shown in Table 1, where the rows indicate the goal of the task (classification vs. quantification) and the columns indicate the granularity of the task (twovs. five-point scale). Granularity Two-point Five-point (binary) (ordinal) Subtask B Subtask C Subtask D Subtask E Classification Quantification Table 1: A 2×2 matrix summarizing the similarities and the differences between Subtasks B-E. 3 Datasets In this section, we des"
S16-1001,S16-1023,0,0.035765,"Missing"
S16-1001,D14-1162,0,0.101908,"Missing"
S16-1001,S16-1007,0,0.0359431,"Missing"
S16-1001,D11-1141,1,0.122667,"ask (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b) for training and development. In addition we created new training and testing datasets. 3,662 575 1,572 492 982 33 427 1,040 1,466 340 601 394 202 40 304 365 4,600 739 1,640 1,207 669 13 411 987 9,728 1,654 3,813 2,093 1,853 86 1,142 2,392 We employed the following annotation procedure. As in previous years, we first gathered tweets that express sentiment about popular topics. For this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned named entity recognition system (Ritter et al., 2011). The collected tweets were greatly skewed towards the neutral class. In order to reduce the class imbalance, we removed those that contained no sentiment-bearing words. We used SentiWordNet 3.0 (Baccianella et al., 2010) as a repository of sentiment words. Any word listed in SentiWordNet 3.0 with at least one sense having a positive or a negative sentiment score greater than 0.3 was considered sentiment-bearing.4 The training and development tweets were collected from July to October 2015. The test tweets were collected from October to December 2015. We used the public streaming Twitter API t"
S16-1001,S14-2009,1,0.830416,"Eval-2016 Task 4: Sentiment Analysis in Twitter Preslav Nakov♣ , Alan Ritter♦ , Sara Rosenthal♥ , Fabrizio Sebastiani♣∗, Veselin Stoyanov♠ ♣ Qatar Computing Research Institute, Hamad bin Khalifa University, Qatar ♦ Department of Computer Science and Engineering, The Ohio State University, USA ♥ IBM Watson Health Research, USA ♠ Johns Hopkins University, USA Abstract As a testament to the prominence of research on sentiment analysis in Twitter, the tweet sentiment classification (TSC) task has attracted the highest number of participants in the last three SemEval campaigns (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b). Previous editions of the SemEval task involved binary (P OSITIVE vs. N EGATIVE) or single-label multi-class classification (SLMC) when a N EU TRAL 1 class is added (P OSITIVE vs. N EGATIVE vs. N EUTRAL). SemEval-2016 Task 4 represents a significant departure from these previous editions. Although two of the subtasks (Subtasks A and B) are reincarnations of previous editions (SLMC classification for Subtask A, binary classification for Subtask B), SemEval-2016 Task 4 introduces two completely new problems, taken individually (Subtasks C and D) and"
S16-1001,S15-2078,1,0.882833,"ent Analysis in Twitter Preslav Nakov♣ , Alan Ritter♦ , Sara Rosenthal♥ , Fabrizio Sebastiani♣∗, Veselin Stoyanov♠ ♣ Qatar Computing Research Institute, Hamad bin Khalifa University, Qatar ♦ Department of Computer Science and Engineering, The Ohio State University, USA ♥ IBM Watson Health Research, USA ♠ Johns Hopkins University, USA Abstract As a testament to the prominence of research on sentiment analysis in Twitter, the tweet sentiment classification (TSC) task has attracted the highest number of participants in the last three SemEval campaigns (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b). Previous editions of the SemEval task involved binary (P OSITIVE vs. N EGATIVE) or single-label multi-class classification (SLMC) when a N EU TRAL 1 class is added (P OSITIVE vs. N EGATIVE vs. N EUTRAL). SemEval-2016 Task 4 represents a significant departure from these previous editions. Although two of the subtasks (Subtasks A and B) are reincarnations of previous editions (SLMC classification for Subtask A, binary classification for Subtask B), SemEval-2016 Task 4 introduces two completely new problems, taken individually (Subtasks C and D) and in combination (Subtask"
S16-1001,S16-1030,0,0.0918744,"Missing"
S16-1001,S16-1026,0,0.030281,"Missing"
S16-1001,S16-1031,0,0.0361636,"Missing"
S16-1001,S16-1035,0,0.0311617,"Missing"
S16-1001,S16-1022,0,0.0439887,"Missing"
S16-1001,S16-1009,0,0.0306813,"Missing"
S16-1001,S16-1027,0,0.0572505,"Missing"
S16-1001,S16-1013,0,0.043193,"Missing"
S16-1001,S16-1008,0,0.0255744,"Missing"
S16-1001,S16-1040,0,0.0464329,"Missing"
S16-1001,baccianella-etal-2010-sentiwordnet,1,\N,Missing
S16-1001,S16-1016,0,\N,Missing
S16-1001,S16-1041,0,\N,Missing
S16-1001,S16-1012,0,\N,Missing
S16-1001,S16-1006,1,\N,Missing
W10-0911,W10-0907,1,0.72391,"wledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage b"
W10-0911,P07-1088,1,0.752966,"leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KO"
W10-0911,P10-1030,1,0.76682,"omingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned inference patterns (Holmes, Sherlock), and cotraining using relation-specific and relation-independent (open) extraction to reinforce semantic coherence (Wu et al., 2008)"
W10-0911,D08-1068,1,0.90344,"radictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausi"
W10-0911,D09-1001,1,0.413926,"edge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Scho"
W10-0911,H05-1043,1,0.0833929,"ic knowledge (Doorenbos et al., 1997); WIEN induces wrappers for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContrair"
W10-0911,D08-1002,1,0.769247,"nd Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via rel"
W10-0911,P10-1044,1,0.578046,"ustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and Text"
W10-0911,D08-1009,1,0.92382,"usters (e.g., in USP, LOFT). Similarly, a variety forms of joint inference have been used, ranging from simple voting to heuristic rules to sophisticated probabilistic models. All these can be compactly encoded in Markov logic (Domingos and Lowd, 2009), which provides a unifying framework for knowledge representation and joint inference. Past work at Washington has shown that in supervised learning, joint inference can substantially improve predictive performance on tasks related to 90 machine reading (e.g., citation information extraction (Poon and Domingos, 2007), ontology induction (Wu and Weld, 2008), temporal information extraction (Ling and Weld, 2010)). In addition, it has demonstrated that sophisticated joint inference can enable effective learning without any labeled information (UCR, USP, LOFT), and that joint inference can scale to millions of Web documents by leveraging sparsity in naturally occurring relations (Holmes, Sherlock), showing the promise of our unifying approach. Simpler representations limit the expressiveness in representing knowledge and the degree of sophistication in joint inference, but they currently scale much better than more expressive ones. A key direction"
W10-0911,D10-1106,1,0.437126,"009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and ofte"
W10-0911,P10-1013,1,0.458065,"n and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned"
W10-0911,N07-1016,1,0.79964,"ioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs s"
W10-0911,H05-2017,1,\N,Missing
W10-0911,P10-1031,1,\N,Missing
W13-1103,J93-1003,0,0.217259,"another one with a flexible number (vary from 1 to 4) of tweets. Both ↵ and are set to 0.1 in our implementation. All parameters are set experimentally over a small development dataset consisting of 10 events in Twitter data of September 2012. EventRank−Flexible 0 a Twitter specific name entity tagger1 ) and a reference to the same unique calendar date (resolved using a temporal expression processor (Mani and Wilson, 2000)). Tweets published during the whole period are aggregated together to find top events that happen on each calendar day. We applied the G2 test for statistical significance (Dunning, 1993) to rank the event clusters, considering the corpus frequency of the named entity, the number of times the date has been mentioned, and the number of tweets which mention both together. We randomly picked the events of one day for human evaluation, that is the day of January 16, 2013 with 38 events and an average of 465 tweets per event cluster. 4 5 compactness completeness overall EventRank−Flexible EventRank−Fixed SumBasic Figure 5: human judgments evaluating tweet summarization systems Event System EventRank (Flexible) Google 1/16/2013 SumBasic EventRank (Flexible) Instagram 1/16/2013 SumBa"
W13-1103,P11-2008,0,0.0183447,"Missing"
W13-1103,P06-1047,1,0.898837,"(e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explici"
W13-1103,W11-0709,0,0.423753,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P11-1037,0,0.32789,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P12-3003,0,0.028684,"generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects of the initial event to be summarized to create comprehen20 Proceedings of the W"
W13-1103,P00-1010,0,0.136012,"epresent the relationship between them. We then rank and partition the events using PageRank-like algorithms, and create summaries of variable length for different topics. 3.1 Event Extraction from Tweets As a first step towards summarizing popular events discussed on Twitter, we need a way to identify events from Tweets. We utilize several natural language processing tools that specially developed for noisy text to extract text phrases that bear essential event information, including named entities (Ritter et al., 2011), event-referring phrases (Ritter et al., 2012) and temporal expressions (Mani and Wilson, 2000). Both the named entity and event taggers utilize Conditional Random Fields models (Lafferty, 2001) trained on annotated data, while the temporal expression resolver uses a mix of hand-crafted and machine-learned rules. Example event information extracted from Tweets are presented in Table 2. The self-contained nature of tweets allows efficient extraction of event information without deep analysis (e.g. co-reference resolution). On the other hand, individual tweets are also very terse, often lacking sufficient context to access the importance of events. It is crucial to exploit the highly redu"
W13-1103,radev-etal-2004-mead,0,0.0270509,"event cluster with a single but complex focus 25 4.2 Figure 4: Event graph of ’West Ham - 1/16/2013’, an example of event cluster with a single focus Baseline SumBasic (Vanderwende et al., 2007) is a simple and effective summarization approach based on term frequency, which we use as our baseline. It uses word probabilities with an update function to avoid redundancy to select sentences or posts in a social media setting. It is shown to outperform three other well-known multi-document summarization methods, namely LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004) and MEAD (Radev et al., 2004) on tweets in (Inouye and Kalita, 2011), possibly because that the relationship between tweets is much simpler than between sentences in news articles and can be well captured by simple frequency methods. The improvement over the LexRank model on tweets is gained by considering the number of retweets and influential users is another side-proof (Wei et al., 2012) of the effectiveness of frequency. Annotator 1 1 https://github.com/aritter/twitter_nlp 26 3 2 1 0 EventRank−Fixed SumBasic Annotator 2 2 3 4 5 compactness completeness overall 1 For each cluster, our systems produce two versions of su"
W13-1103,D11-1141,1,0.233403,"ted advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects"
W13-1103,C12-1047,0,0.0630086,"quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to ext"
W13-1103,W04-3252,0,\N,Missing
W13-2515,N03-1003,0,0.0737541,"This paper presents the first investigation into automatically collecting a large paraphrase corpus of tweets, which can be used for building paraphrase systems adapted to Twitter using techniques from statistical machine translation (SMT). We show experimental results demonstrating the benefits of an in-domain parallel corpus when paraphrasing tweets. In addition, our paraphrase models can be applied to the task of normalizing noisy text where we show improvements over the state-of-the-art. Relevant previous work has extracted sentencelevel paraphrases from news corpora (Dolan et al., 2004; Barzilay and Lee, 2003; Quirk et al., 2004). Paraphrases gathered from noisy usergenerated text on Twitter have unique characteristics which make this comparable corpus a valuable new resource for mining sentence-level paraphrases. Twitter also has much less context than news articles and much more diverse content, thus posing new challenges to control the noise in mining paraphrases while retaining the desired superficial dissimilarity. We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a"
W13-2515,2011.iwslt-evaluation.18,0,0.00622054,"Missing"
W13-2515,P11-1020,0,0.229551,"ine at https://github.com/cocoxu/ twitterparaphrase/ 121 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121–128, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics we apply a typical phrase-based statistical MT pipeline, performing word alignment on the parallel data using GIZA++ (Och and Ney, 2003), then extracting phrase pairs and performing decoding uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task"
W13-2515,C04-1051,0,0.644793,"Missing"
W13-2515,J10-3003,0,0.141397,"ty of this new resource on the task of paraphrasing and normalizing noisy text, showing improvement over several state-of-the-art paraphrase and normalization systems 1 . 1 Introduction Social media services provide a massive amount of valuable information and demand NLP tools specifically developed to accommodate their noisy style. So far not much success has been reported on a key NLP technology on social media data: paraphrasing. Paraphrases are alternative ways to express the same meaning in the same language and commonly employed to improve the performance of many other NLP applications (Madnani and Dorr, 2010). In the case of Twitter, Petrovi´c et al. (2012) showed improvements on first story detection by using paraphrases extracted from WordNet. Learning paraphrases from tweets could be especially beneficial. First, the high level of information redundancy in Twitter provides a good opportunity to collect many different expressions. Second, tweets contain many kinds of paraphrases not available elsewhere including typos, abbreviations, ungrammatical expressions and slang, 2 Related Work There are several key strands of related work, including previous work on gathering parallel monolingual text fr"
W13-2515,I08-1059,0,0.015274,"Missing"
W13-2515,P00-1010,0,0.0216211,"se pair in our system, in addition to the four basic components (translation model, distortion model, language model and word penalty) in SMT. Paraphrasing Tweets 5.1.1 Data Our paraphrase dataset is distilled from a large corpus of tweets gathered over a one-year period spanning November 2011 to October 2012 using the Twitter Streaming API. Following Ritter et al. (2012), we grouped together all tweets which mention the same named entity (recognized using a Twitter specific name entity tagger3 ) and a reference to the same unique calendar date (resolved using a temporal expression processor (Mani and Wilson, 2000)). Then we applied a statistical significance test (the G test) to rank the events, which considers the corpus frequency of the named entity, the number of times the date has been mentioned, and the number of tweets which mention both together. Altogether we collected more than 3 million tweets from the 50 top events of each day according to the p-value from the statistical test, with an average of 229 tweets per event cluster. Each of these tweets was passed through a Twitter tokenizer4 and a simple sentence splitter, which also removes emoticons, URLs and most of the hashtags and usernames."
W13-2515,W11-2210,0,0.0157816,"ecause Twitter contains both normal and noisy language, with appropriate tuning, our models have the capability to translate between these two styles, e.g. paraphrasing into noisy style or normalizing into standard language. Here we demonstrate its capability to normalize tweets at the sentence-level. 5.2.1 Baselines Much effort has been devoted recently for developing normalization dictionaries for Microblogs. One of the most competitive dictionaries available today is HB-dict+GHM-dict+S-dict used by Han et al. (2012), which combines a manuallyconstructed Internet slang dictionary , a small (Gouws et al., 2011) and a large automaticallyTable 4: Example paraphrases of a given sentence “who want to get a beer” 126 derived dictionary based on distributional and string similarity. We evaluate two baselines using this large dictionary consisting of 41181 words; following Han et. al. (2012), one is a simple dictionary look up. The other baseline uses the machinery of statistical machine translation using this dictionary as a phrase table in combination with Twitter and NYT language models. 5.2.2 No-Change SMT+TwitterLM SMT+TwitterNYTLM Dictionary Dicionary+TwitterNYTLM SMT+Dictionary+TwitterNYTLM System D"
W13-2515,moore-2002-fast,0,0.019796,"and less agreement. 123 In addition, because no previous work has evaluated these metrics in the context of noisy Twitter data, we perform a human evaluation in which annotators are asked to choose which system generates the best paraphrase. Finally we evaluate our phrase-based normalization system against a state-of-the-art word-based normalizer developed for Twitter (Han et al., 2012). 5.1 velopment data and the exact configuration are released together with the phrase table for system replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure ever"
W13-2515,P11-1038,0,0.387701,"ng uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent"
W13-2515,W04-3243,0,0.0749033,"tem replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure every phrase has an option to remain unchanged during paraphrasing and normalization. Without these noise reduction steps, our system will produce paraphrases with serious errors (e.g. change a person’s last name) for 100 out of 200 test tweets in the evaluation in §5.1.5. At the same time, it is also important to promote lexical dissimilarity in the paraphrase task. Following Ritter et. al. (2011) we add a lexical similarity penalty to each phrase pair in our system, in addition to the fo"
W13-2515,D12-1039,0,0.361981,"al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying"
W13-2515,J03-1002,0,0.00357666,"d Work There are several key strands of related work, including previous work on gathering parallel monolingual text from topically clustered news articles, normalizing noisy Twitter text using word-based 1 Our Twitter paraphrase models are available online at https://github.com/cocoxu/ twitterparaphrase/ 121 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121–128, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics we apply a typical phrase-based statistical MT pipeline, performing word alignment on the parallel data using GIZA++ (Och and Ney, 2003), then extracting phrase pairs and performing decoding uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related"
W13-2515,P02-1040,0,0.104655,"cal phrase-based statistical MT pipeline on the parallel data, which uses GIZA++ for word alignment and Moses for extracting phrase pairs, training and decoding. We use a language model trained on the 3 million collected tweets in the decoding process. The parameters are tuned over de5.1.2 Evaluation Details The beauty of lexical similarity penalty is that it gives control over the degree of paraphrasing by adjusting its weight versus the other components. Thus we can plot a BLEU-PINC curve to express the tradeoff between semantic adequacy and lexical dissimilarity with the input, where BLUE (Papineni et al., 2002) and PINC (Chen and Dolan, 2011) are previously proposed automatic evaluation metrics to measure respectively the two criteria of paraphrase quality. To compute these automatic evaluation metrics, we manually prepared a dataset of gold paraphrases by tracking the trending topics on Twitter5 and gathering groups of paraphrases in November 2012. In total 20 sets of sentences were collected and each set contains 5 different sentences that express the same meaning. Each sentence is used 3 https://github.com/aritter/twitter_ nlp 4 https://github.com/brendano/ tweetmotif 5 https://support.twitter.co"
W13-2515,W12-3153,0,0.0127172,"ses tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain paraphrase systems to improve performance at first story detection in Twitter (Petrovi´c et al., 2012). By building better paraphrase models adapted to Twitter, it should be possible to improve performance at such tasks, which benefit from paraphrasing Tweets. 3 3.1 Extracting Events from Tweets As a first step towards extracting paraphrases from popular events discussed on Twitter, we need a way to identify Tweets which mentio"
W13-2515,N12-1034,0,0.367404,"Missing"
W13-2515,W04-3219,0,0.0109492,"first investigation into automatically collecting a large paraphrase corpus of tweets, which can be used for building paraphrase systems adapted to Twitter using techniques from statistical machine translation (SMT). We show experimental results demonstrating the benefits of an in-domain parallel corpus when paraphrasing tweets. In addition, our paraphrase models can be applied to the task of normalizing noisy text where we show improvements over the state-of-the-art. Relevant previous work has extracted sentencelevel paraphrases from news corpora (Dolan et al., 2004; Barzilay and Lee, 2003; Quirk et al., 2004). Paraphrases gathered from noisy usergenerated text on Twitter have unique characteristics which make this comparable corpus a valuable new resource for mining sentence-level paraphrases. Twitter also has much less context than news articles and much more diverse content, thus posing new challenges to control the noise in mining paraphrases while retaining the desired superficial dissimilarity. We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a comparable corpus of"
W13-2515,D07-1103,0,0.0134095,"ormalizer developed for Twitter (Han et al., 2012). 5.1 velopment data and the exact configuration are released together with the phrase table for system replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure every phrase has an option to remain unchanged during paraphrasing and normalization. Without these noise reduction steps, our system will produce paraphrases with serious errors (e.g. change a person’s last name) for 100 out of 200 test tweets in the evaluation in §5.1.5. At the same time, it is also important to promote lexical dissimilarity in the"
W13-2515,D11-1054,1,0.557611,"Missing"
W13-2515,D08-1027,0,0.0184776,"Missing"
W13-2515,P13-1018,0,0.0118995,"much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain paraphrase systems to improve performance at first story detection in Twitter (Petrovi´c et al., 2012). By building better paraphrase models adapted to Twitter, it should be possible to improve performance at such tasks, which benefit from paraphrasing Tweets. 3 3.1 Extracting Events from Tweets As a first step towards extracting paraphrases from popular events discussed on Twitter, we need a way to identify Tweets which mention the same event. To"
W13-2515,P12-1109,0,0.0224913,"s, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain parap"
W13-2515,P07-2045,0,\N,Missing
W15-4319,W15-4308,0,0.196632,"et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supra"
W15-4319,P06-2005,0,0.224961,"Missing"
W15-4319,W15-4312,0,0.040755,"d using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the basis of n-grams tatistics. BEKLI (Beckley, 2015) A substitution dictionary is constructed in which keys are non-standard words and values are lists of potential normalizations. Frequent morphology errors are captured by hand-crafted rules. Finally, the Viterbi algorithm is applied to bigram sequences to decode the normalized sentence with maximum probability. LYSGROUP (Mosquera et al., 2015) A system originally developed for Spanish text normalization was adapted to English text normalization. The method consists of a cascaded pipeline of several data adaptors and processors, such as a Twitter POS tagger and a spell checker. 3 Named Entity"
W15-4319,W15-4318,0,0.105136,"Missing"
W15-4319,N15-1075,0,0.0319326,"Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Twitter NER (Ritter et al., 2011), which disti"
W15-4319,W15-4307,0,0.270391,"tain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-competition analysis of the effect of training on development sets is presented in the NRC system description paper (Cherry et al., 2015). 131 Figure 2: Annotation interface. POS Orthographic Gazetteers Brown clustering – X X X – – – X X X – X X – X – X X X – X – – X X X X – X – X – X X – X BASELINE Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Word embedding ML – correlation analysis – – word2vec word2vec & GloVe word2vec X – CRFsuite CRFsuite CRF++ CRF wapiti FFNN CRF++ semi-Markov MIRA entity linking CRF L-BFGS Table 7: Features and machine learning approach taken by each team. Precision Recall F ousia NLANGP nrc multimedialab USFD iitp Hallym lattice 57.66 63.62 53.24 49.52 45.72 60.68 39.59 55.17 55.22 43.12 38.5"
W15-4319,P14-2111,0,0.0445576,"r data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods,"
W15-4319,W09-2010,0,0.0437694,"guistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-"
W15-4319,W15-4306,0,0.105059,"hallenge is concept drift (Dredze et al., 2010; Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Tw"
W15-4319,W15-4322,0,0.0251441,"Missing"
W15-4319,P11-1038,1,0.912118,"Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can b"
W15-4319,D12-1039,1,0.757294,"zation examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words tha"
W15-4319,P13-1155,0,0.0178022,"rds can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task"
W15-4319,W15-4313,0,0.0798376,"shown in Tables 3 and 4. Overall, common approaches were lexicon-based methods, CRFs, and neural network-based approaches. Among the constrained systems, neural networks achieved strong results, even without off-the-shelf tools. In contrast, CRF- and lexicon-based approaches were shown to be effective in the unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et"
W15-4319,D14-1108,0,0.0271234,"new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited t"
W15-4319,W15-4323,0,0.0427639,"ING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015)"
W15-4319,D10-1057,0,0.157913,"Missing"
W15-4319,D13-1008,0,0.00982812,"level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on Engli"
W15-4319,P11-2013,0,0.247801,"dard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined paralle"
W15-4319,W15-4321,0,0.0536682,"s first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as activation function in combination with dropout to prevent overfitting. A context window of 5 words was used As input (2 words left and right). The output is a single tag of the middle word. Afterwards, a rule-based post-processing step was executed to ensure every I-tag has a B-tag in front of it and that all tags within a single span are of the same type. Train and dev were used as training data and used dev 2015 as validation set. NLANGP (Toh et al., 2015) The NLANGP team modeled the problem as a sequential labeling task and used Conditional Random Fields. Several post-processing steps (e.g. rulebased matching) were applied to refine the system output. Besides Brown clusters, Kmeans clusters were also used; the K-means clusters were generated based on word embeddings. nrc (Cherry et al., 2015) NRC applied a MIRAtrained semi-Markov tagger with Gazetteer, Brown cluster and Word Embedding features. The Word Embeddings were built over phrases using Word2Vec’s phrase finder tool, and were modified using an auto-encoder to be predictive of Gazetteer"
W15-4319,P12-1109,0,0.0482015,"es. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical no"
W15-4319,P10-1040,0,0.00514115,"d resources. There were 6 official submissions in the constrained category, and 5 official submissions in the unconstrained category. Overall, deep learning methods and methods based on lexicon-augmented conditional random fields (CRFs) achieved the best results. The winning team achieved a precision of 0.9061 precision, recall of 0.7865, and F1 of 0.8421. The named entity recognition task attracted 8 participants. The majority of teams built their systems using linear-chain conditional random fields (Lafferty et al., 2001), and many teams also used brown clusters and word embedding features (Turian et al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al."
W15-4319,P12-3005,1,0.690086,"e their system categories: • Constrained: participants could not use any data other than the provided training data to perform the text normalization task. They were allowed to use pre-trained tools (e.g., Twitter POS taggers), but no normalization lexicons or extra tweet data. • Unconstrained: participants could use any publicly accessible data or tools to perform the text normalization task. Evaluation was based on token-level precision, recall and F-score. 2.2.1 Preprocessing We first collected tweets using the Twitter Streaming API over the period 23–29 May, 2014, and then used langid.py (Lui and Baldwin, 2012)1 to remove all non-English tweets. Tokenization was performed with CMU-ARK tokeniser.2 To ensure that tweets had a high likelihood of requiring lexical normalization, we filtered out tweets with less than 2 non-standard words (i.e. words not occurring in our dictionary — see Section 2.2.3). While this biases the sample of tweets, the decision was made at a pragmatic level to ensure a reasonable level of lexical normalization and “annotation density”. This was based on a pilot study over a random sample of English tweets, in which we found that many non-standard words were actually unknown nam"
W15-4319,W15-4314,0,0.0181512,"Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction mo"
W15-4319,W15-4317,0,0.0288445,"unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al."
W15-4319,W15-4315,0,0.0571066,"Missing"
W15-4319,C14-1168,0,0.0359301,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,E14-1078,0,0.0294125,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,D11-1141,1,0.828298,"Missing"
W15-4319,N13-1050,0,0.00844789,"(e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One ma"
W15-4319,W15-4320,0,0.0994733,"all 56.64 57.52 57.07 =1 Table 5: Precision and recall comparing one annotator against the other. Cohen’s kappa between the annotators was 0.607. Disagreements between the annotators resolved by a 3rd adjudicator for the final datasets. Team ID Affiliation Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Hallym University Indian Institute of Technology Patna University Paris 3 UGent - iMinds Institute for Infocomm Research National Research Council Canada Studio Ousia University of Sheffield Table 6: Team ID and affiliation of the named entity recognition shared task participants. sia (Yamada et al., 2015). All the other teams used CRFs. On top of a CRF, the iitp team used a differential evolution based technique to obtain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-co"
W15-4319,D13-1007,0,0.160415,"anslation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words that happen to coincide in spel"
W15-4319,W15-4311,0,0.0298261,"2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the ba"
W15-4319,W15-4309,0,0.0208179,".14 54.31 56.28 55.22 54.61 51.44 48.5 25.72 70.63 60.29 59.81 58.82 58.13 56.81 53.01 35.71 BASELINE 53.86 46.44 49.88 =1 Table 8: Results segmenting and categorizing entities into 10 types. Hallym (Yang and Kim, 2015) The Hallym team used an approach based on CRFs using both Brown clusters and word embeddings trained using Canonical Correlation Analysis as features. iitp (Akhtar et al., 2015a) The iitp team pro=1 Table 9: Results on segmentation only (no types). posed a multi-objective differential evolution based technique for feature selection in twitter named entity recognition. lattice (Tian, 2015) Lattice employed a CRF model using Wapiti. The feature templates consisted of standard features used in stateof-the-art. They trained first a model with 132 dev 2015 and evaluated this model on train and dev. multimedialab (Godin et al., 2015) The goal of the multimedia lab system was to only use neural networks and word embeddings to show the power of automatic feature learning and semi-supervised methods. A FeedForward Neural Network was first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as act"
W15-4319,W03-0419,0,0.581711,"Missing"
W15-4319,fromreide-etal-2014-crowdsourcing,0,\N,Missing
W15-4319,W15-4316,0,\N,Missing
W16-3919,W15-4319,1,0.519066,"Missing"
W16-3919,D10-1057,0,0.0761634,"n when applied to noisy Twitter data. But tweets often contain more up-to-date information than news, in addition the increased volume of text offers opportunities to exploit redundancy of information which is very beneficial for information extraction (Downey et al., 2005). To exploit the opportunities for information extraction on top of social media, there is a crucial need for in-domain annotated data to train and evaluate named entity recognition systems on this noisy style of text. Twitter processing has the additional challenge that the language people use on Twitter changes over time (Dredze et al., 2010; Fromreide et al., 2014). The previous edition of this task (Baldwin et al., 2015) addressed this issue by evaluating on a test set collected from a later time period then the training and development data. This year we take a similar approach, providing a new test dataset of tweets gathered from 2016. In addition to enabling research on adapting named entity recognition to new language over time, we hope this new dataset will be useful for adapting future Twitter named entity recognition systems, improving their performance on up-to-date data. Additionally, this year we address the issue of"
W16-3919,W16-3924,0,0.0132806,"features based on lexicon-type information such as stop word matching, word frequencies, and entries in the shared task lexicon and Babelfy (Moro et al., 2014). Talos (Ioannis Partalas and Kalitvianski, 2016) The system uses three types of features: lexical and morpho-syntactic features, contextual enrichment features using Linked Open Data, and features based on distributed representation of words. The system also exploits words clustering to enhance performance. The learning algorithm was solved by using Learning to search (L2S) that resembles a reinforcement learning algorithm. DeepNNNER (Dugas and Nichols, 2016) The system uses a bidirectional LSTM-CNN model with word embedding trained on a large scale Web corpus. Additionally, the system uses automatically constructed lexicons with a partial matching algorithm and text normalization to handle the large vocabulary problem in Web texts. 141 CambridgeLTL Talos akora NTNU ASU DeepNNNER DeepER hjpwhu UQAM-NTL LIOX Precision Recall F1 60.77 58.51 51.70 53.19 40.58 54.97 45.40 48.90 40.73 40.15 46.07 38.12 39.48 32.13 37.58 28.16 31.15 28.76 23.52 12.69 52.41 46.16 44.77 40.06 39.02 37.24 36.95 36.22 29.82 19.26 CambridgeLTL NTNU Talos akora ASU DeepER Dee"
W16-3919,P05-1045,0,0.0488289,"the location of the shooting event. The shooting domain contains 8,963 tokens with 751 phrases. Computer hacking events were found by searching for tweets including the keyword “breach”. The breach domain contains 5,537 tokens with 603 phrases. The additional data annotated this year was completed by a single annotator instructed to follow the annotation guidelines of the prior annotations. The annotator was presented with a set of simple guidelines2 that cover common ambiguous cases and was also instructed to refer to the September 2010 data 1 2 For example, the Stanford named entity tagger (Finkel et al., 2005) achieves an F1 score of 0.86 on the CoNLL data set. http://bit.ly/1FSP6i2 139 for reference (Ritter et al., 2011). The BRAT tool3 was used for annotation. Figure 1 is a screenshot of the interface presented to the annotators. To ensure that the new annotations were consistent with the earlier annotations, 100 tweets were annotated in both tasks to calculate agreement. The new annotator proved a high agreement with the old data set with a F1 score of 67.67. Table 1 presents the count of each of the 10 named entity types labeled by the annotators in the training, development and test sets creat"
W16-3919,W16-3923,0,0.0295917,"Missing"
W16-3919,W16-3920,0,0.153678,"ML – – – – GloVe Multiple – – CRFsuite LSTM LSTM CRF L2S LSTM-CNN LSTM CRF Table 3: Features and machine learning approach taken by each team. 2.3 System Descriptions This section briefly describes the approach taken by each team. Overall we noticed different trends between the types of systems submitted this year and last year. The most notable change is the use of LSTM-based systems. Four of the seven submissions were LSTM-based as opposed to zero submissions last year. The previous year Conditional Random Fields was the most popular ML technique for extracting named entities. CambridgeLTL (Limsopatham and Collier, 2016) The system uses bidirectional LSTM to automatically induce and leverage orthographic features for performing Named Entity Recognition in Twitter messages. akora (Kurt Junshean Espinosa and Ananiadou, 2016) This system uses bidirectional LSTM networks and exploits weakly annotated data to bootstrap sparse entity types. NTNU (Sikdar and Gamb¨ack, 2016) This system is based on classification using Conditional Random Fields, a supervised machine learning approach. The system utilizes a large feature set developed specifically for the task, with eight types of features based on actual characters a"
W16-3919,Q14-1019,0,0.00717094,"and exploits weakly annotated data to bootstrap sparse entity types. NTNU (Sikdar and Gamb¨ack, 2016) This system is based on classification using Conditional Random Fields, a supervised machine learning approach. The system utilizes a large feature set developed specifically for the task, with eight types of features based on actual characters and token internal data, five types of features built through context and chunk information, and five types of features based on lexicon-type information such as stop word matching, word frequencies, and entries in the shared task lexicon and Babelfy (Moro et al., 2014). Talos (Ioannis Partalas and Kalitvianski, 2016) The system uses three types of features: lexical and morpho-syntactic features, contextual enrichment features using Linked Open Data, and features based on distributed representation of words. The system also exploits words clustering to enhance performance. The learning algorithm was solved by using Learning to search (L2S) that resembles a reinforcement learning algorithm. DeepNNNER (Dugas and Nichols, 2016) The system uses a bidirectional LSTM-CNN model with word embedding trained on a large scale Web corpus. Additionally, the system uses a"
W16-3919,I11-1108,0,0.0110506,"presents the results of the Twitter Named Entity Recognition shared task associated with W-NUT 2016: a named entity tagging task with 10 teams participating. We outline the shared task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task. 1 Introduction The increasing flood of user-generated text on social media has created an enormous opportunity for new data analysis techniques to extract and aggregate information about breaking news (Ritter et al., 2012), disease outbreaks (Paul and Dredze, 2011), natural disasters (Neubig et al., 2011), cyber-attacks (Ritter et al., 2015) and more. Named entity recognition is an important first step in most information extraction pipelines. However, performance of state-of-the-art NER on social media still lags behind well edited text genres. This motivates the need for continued research, in addition to new datasets and tools adapted to this noisy text genre. In this paper, we present the development and evaluation of a shared task on named entity recognition in Twitter, which was held at the 2nd Workshop on Noisy User-generated Text (W-NUT 2016) and attracted 10 participating teams, 7 of"
W16-3919,W16-3926,0,0.0197598,"n segmenting and categorizing entities into 10 types. ASU (Michel Naim Gerguis and Gerguis, 2016) The system shows an experimental study on using word embeddings, Brown clusters, part-of-speech tags, shape features, gazetteers, and local context to create a feature representation along with a set of experiments for the network design. A Wikipedia-based classifier framework was adopted to extract lists of fine-grained entities out of few input examples to be used as gazetteers. The model uses the LSTM algorithm to learn a NE classifier from the feature representation. UQAM-NTL (Ngoc Tan LE and Sadat, 2016) The system is based on supervised machine learning and trained with a sequential labeling algorithm, using Conditional Random Fields to learn a classifier for Twitter NE extraction. The model uses 6 different categories of features including (1) orthographic, (2) lexical and (3) syntactic features as well as (4) part-of-speech tags, (5) polysemy count and (6) longest n-gram length in order to create a feature representation. 3 Summary In this paper, we presented a shared task for Named Entity Recognition in Twitter data. We detailed the task setup and datasets used in the respective shared ta"
W16-3919,D11-1141,1,0.92682,"with general domain data. Both the time period and topic selection of the evaluation data were not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. Teams had 7 days to submit their results on the test data, which were subsequently scored and gold annotations were released to participants. Evaluating the NER systems on these domains specific Twitter data provides information about possible system weakness. 2.1 Training and Development Data The training and development data for our task was taken from prior work on Twitter NER (Ritter et al., 2011; Baldwin et al., 2015), which distinguishes 10 different named entity types (see Table 1 for the set of types). The training data was created from the union of the training and development data from the 2015 task (Baldwin et al., 2015). The data was split into 2,394 annotated tweets for training and 1,000 as a development set. We also provided an additional 425 annotated tweets from the 2015 development data set (Baldwin et al., 2015). 2.2 Test Data Annotation The data set we created for testing is new for this shared task. We collected general Twitter data and domain specific Twitter data. I"
W16-3919,W16-3922,0,0.121077,"Missing"
W16-3919,W03-0419,0,0.420022,"Missing"
