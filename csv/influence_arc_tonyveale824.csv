1996.amta-1.1,J93-1004,0,0.0952855,"ility to adapt examples retrieved to suit the new problem translation. Here we describe a technique whereby reusability is a function of the abstract &apos;adaptability&apos; information stored in the cases. This information is exploited during both the adaptation and retrieval stages. 1. Introduction Example Based Machine Translation (EBMT) is the marriage of MT and Case Based Reasoning (CBR) techniques (see Nagao 1984; Sumita 1995). It involves translating the source language (SL) into the target language (TL) via remindings from previous translation cases. Because of the sparse data problem in EBMT (Gale and Church, 1993; Moonjoo 1995) an EBMT system must have a sophisticated adaptation mechanism so that the previous translation may be suitably adapted to suit the new sentence at hand. However, adaptation is complicated by translation divergences (Dorr 1993) which cause the translation example to be brittle in reuse. This means that a translation example involving a target language segment that is structurally different to the source language segment can only be adapted with great care. What is required is a retrieval policy whereby the extent to which a previous translation may be adapted is made explicit an"
1996.amta-1.1,E95-1008,0,0.0316652,"Missing"
1996.amta-1.1,C90-3044,0,0.116501,"Missing"
1996.amta-1.1,1988.tmi-1.13,0,0.132933,"Missing"
1996.amta-1.1,E95-1022,0,0.105987,"Missing"
1996.amta-1.17,1991.mtsummit-papers.9,0,0.0313334,"rogram. A DCL program, when executed, manipulates an on-screen animated doll, causing the correct gesture sequence to be articulated graphically to the user by (ix) a DCL animator (see Conway & Veale 1994). 4. Schematization and Interlingual Representation To ensure maximal decoupling of the input languages (e.g., English, Japanese) from the output sign variants (ISL, ASL, or JSL), ZARDOZ eschews the Transfer approach (originated in Yngve (1957) and more recently advocated by Lee & Kunii 1992) in favour of the Interlingua methodology (originated in Weaver (1955), and more recently employed by Mitamura et al 1991), which places a language-independent interface between source and target. An interlingua may capture the generic fact-stating capacity of language, broadly speaking, using two different strategies: the first attempts to construct a universal grammar which generalises over the syntactic forms of many languages; the second side-steps form and attempts to model the world directly. This second strategy is knowledge intensive, but allows for the natural incorporation of heterogeneous common-sense inference processes into the translation process. The English-to-ASL translation system of Patten & Ha"
1996.amta-1.17,W94-0333,1,0.71451,"(iv) composed into an interlingual frame format; however, before this representation can be considered truly language-independent, metaphoric and metonymic structures specific to the source language are removed by a process of (v) schematization (described in the next section). The interlingua representation proper provides grist for the (vi) discourse tracking agency (anaphoric resolution is an issue even in translation—see sections (ix)), before being passed to the generation panels of the system, (vii) the sign syntax agency, which employs a robust scheme of spatial dependency graphs (see Veale & Conway 1994), and (viii) the sign mapping agency, which employs direct lookup or a variety of heuristic measures to assign concept-to-sign correspondences to the tokens that comprise the interlingua structure. The syntax and mapping agencies are responsible for transducing the interlingua structure into a flat output stream of sign tokens, which eventually forms the compilation basis for a Doll Control Language (DCL) program. A DCL program, when executed, manipulates an on-screen animated doll, causing the correct gesture sequence to be articulated graphically to the user by (ix) a DCL animator (see Conwa"
C04-1195,J03-3005,0,0.039111,"Missing"
C08-1011,S07-1003,0,0.221351,"ng of the problem as a search for a missing relation suggests two broad strategies for the interpretation of compounds. In the first, the top-down strategy, we assume that there are only so many ways of combining two concepts; by enumerating these ways, we can view the problem of interpretation as a problem of classification, in which compounds are placed into separate classes that each correspond to a single manner of concept connection (Kim and Baldwin, 2006), (Nastase and Szpakowicz, 2003). This strategy explicitly shaped the SemEval task on classifying semantic relations between nominals (Girju et al., 2007) and so is employed by all of the systems that participated in that task. In the second, the bottom-up strategy, we assume that it is futile to try and enumerate the many ways in which concepts can relationally combine, but look instead to large corpora to discover the ways in which different word combinations are explicitly framed by language (Nakov, 2006), (Turney, 2006a). In this paper we describe an approach that employs the bottom-up strategy with an openrather than closed-inventory of inter-concept relations. These relations are acquired from the analysis of large corpora, such as the We"
C08-1011,W96-0309,0,0.42147,", or are facilitated by, these nouns. We present a model of noun-compound interpretation that first learns the relational possibilities for individual nouns from corpora, and which then uses these to hypothesize about the most likely relationship that underpins a noun compound. 1 Tony Veale School of Computer Science and Informatics University College Dublin Belfield, Dublin 4 Tony.Veale@ucd.ie Introduction Noun compounds hide a remarkable depth of conceptual machinery behind a simple syntactic form, Noun-Noun, and thus pose a considerable problem for the computational processing of language (Johnston and Busa, 1996). It is not just that compounds are commonplace in language, or that their interpretation requires a synthesis of lexical, semantic, and pragmatic information sources (Finin, 1980); compounds provide a highly compressed picture of the workings of concept combination, so there are as many ways of interpreting a noun compound as there are ways of combining the underlying concepts (Gagné, 2002). Linguists have thus attempted to © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights rese"
C08-1011,P06-2064,0,0.227177,"an audience to reconstruct the decompressed proposition, there must be some systematic means by which the missing relation can easily be inferred. This framing of the problem as a search for a missing relation suggests two broad strategies for the interpretation of compounds. In the first, the top-down strategy, we assume that there are only so many ways of combining two concepts; by enumerating these ways, we can view the problem of interpretation as a problem of classification, in which compounds are placed into separate classes that each correspond to a single manner of concept connection (Kim and Baldwin, 2006), (Nastase and Szpakowicz, 2003). This strategy explicitly shaped the SemEval task on classifying semantic relations between nominals (Girju et al., 2007) and so is employed by all of the systems that participated in that task. In the second, the bottom-up strategy, we assume that it is futile to try and enumerate the many ways in which concepts can relationally combine, but look instead to large corpora to discover the ways in which different word combinations are explicitly framed by language (Nakov, 2006), (Turney, 2006a). In this paper we describe an approach that employs the bottom-up str"
C08-1011,I05-1082,0,0.280711,"Missing"
C08-1011,S07-1083,1,0.813107,"alidation, to interpret specific nouncompounds. We present an evaluation of this approach in section 5 and conclude the paper with some final remarks in section 6. 2 Related Work Machine-learning and example-based approaches to noun-compounds generally favor the top-down strategy for defining relations, since it allows training data and exemplars/cases to be labeled using a fixed inventory of relational classes. As noted earlier, this strategy is characteristic of the systems that participated in the SemEval task on classifying semantic relations between nominals (Girju et al., 2007), such as Butnariu and Veale (2007). Though the inventory is fixed in size, it can be defined using varying levels of abstraction; for instance, Nastase and Szpakowicz (2003) use an inventory of 35 relations, 5 of which are top level relations with the remaining 30 at the lower level. The topdown strategy pre-dates these computational approaches, and is a key aspect of the 82 Google’s web index. Table 1 summarizes the linguistic relational possibilities that can be derived from specific n-gram patterns. corpus analysis to learn about the relational behavior of individual nouns rather than pairs of nouns. Like many other authors"
C08-1011,P07-1112,0,0.0238527,"Missing"
C08-1011,P06-1040,0,0.0281881,"hat each correspond to a single manner of concept connection (Kim and Baldwin, 2006), (Nastase and Szpakowicz, 2003). This strategy explicitly shaped the SemEval task on classifying semantic relations between nominals (Girju et al., 2007) and so is employed by all of the systems that participated in that task. In the second, the bottom-up strategy, we assume that it is futile to try and enumerate the many ways in which concepts can relationally combine, but look instead to large corpora to discover the ways in which different word combinations are explicitly framed by language (Nakov, 2006), (Turney, 2006a). In this paper we describe an approach that employs the bottom-up strategy with an openrather than closed-inventory of inter-concept relations. These relations are acquired from the analysis of large corpora, such as the Web IT corpus of Google n-grams (Brants and Franz, 2006). We argue that an understanding of nouncompounds requires an understanding of lexicalized concept combination, which in turn requires an understanding of how lexical concepts can be used and connected to others. As such, we do not use corpora as a means of Abstract A noun-compound is a compressed proposition that requ"
C08-1011,J06-3003,0,0.100539,"hat each correspond to a single manner of concept connection (Kim and Baldwin, 2006), (Nastase and Szpakowicz, 2003). This strategy explicitly shaped the SemEval task on classifying semantic relations between nominals (Girju et al., 2007) and so is employed by all of the systems that participated in that task. In the second, the bottom-up strategy, we assume that it is futile to try and enumerate the many ways in which concepts can relationally combine, but look instead to large corpora to discover the ways in which different word combinations are explicitly framed by language (Nakov, 2006), (Turney, 2006a). In this paper we describe an approach that employs the bottom-up strategy with an openrather than closed-inventory of inter-concept relations. These relations are acquired from the analysis of large corpora, such as the Web IT corpus of Google n-grams (Brants and Franz, 2006). We argue that an understanding of nouncompounds requires an understanding of lexicalized concept combination, which in turn requires an understanding of how lexical concepts can be used and connected to others. As such, we do not use corpora as a means of Abstract A noun-compound is a compressed proposition that requ"
C08-1119,W04-3221,0,0.024817,"Missing"
C08-1119,J91-1003,0,0.81901,"interpretation of metaphor crucially hinges on a systems ability to recognize these conventions and accommodate the exceptional meaning conveyed by each figurative expression. Indeed, most computational approaches embody a sense of what it means to be literal, and accommodate metaphoric meanings within this conventional scheme through a form of relaxation, mapping or translation. Wilks (1978) advocates that the typically hard constraints that define a literal semantics should instead be modeled as soft preferences that can accommodate the violations that arise in metaphoric utterances, while Fass (1991) builds on this view to show how these violations can be repaired to thus capture the literal intent behind each metaphor. This repair process in turn relies on the availability of a concept taxonomy through which metaphoric uses can be mapped onto their literal counterparts; a car that “drinks gasoline” would thus be understood as a car that “consumes gasoline”. Way (1991) emphasizes the importance of this taxonomy by positing a central role for a dynamic type hierarchy (DTH) in metaphor, one that can create new and complex taxonyms on the fly. For instance, Way’s DTH would understand the “ma"
C08-1119,P88-1027,0,\N,Missing
D17-1050,K16-1017,0,0.46835,"ures from context, the author, and the audience. Khattri et al. (2015) identified sarcasm by seeking a strong contrast in affect toward named entities in current vs. historical tweets, while Rajadesingan et al. (2015) also exploited a contrast in statistically-derived author traits across current and historical tweets. Zhang et al. (2016) use similar sources of contextual information to show the effectiveness of a neural network over more traditional approaches involving manually-selected, discrete features, claiming that automatic feature induction can uncover more subtle markers of sarcasm. Amir et al. (2016) argue that sarcasm detection hinges on speaker modeling, and exploited user embedding to quantify incongruity between utterances and the behavioral traits of their authors. These methods measure the disparity between an utterance and expectations arising from knowledge of context or speaker or both together. Speaker Utterance: @MSNBC of course all of those jobs will be in China In reply to @realDonaldTrump: I will be the greatest jobsproducing president that God ever created. The speaker’s sarcastic intent cannot be grasped without knowledge of the larger context. This issue provides our seco"
D17-1050,P11-2102,0,0.0517383,"Missing"
D17-1050,W15-2905,0,0.0353813,"d Walker, 2013) showed that phrases such as “no way”, “Oh really?” and “not so much” serve to flag a sarcastic intent when used with specific linguistic patterns. Capelli et al. (1990); Woodland and Voyer (2011) suggest that contextual awareness is a necessary precursor to identifying sarcasm. Sarcasm is a response to a motivating context that appears to force a rueful incongruity between a text and its context. Exploiting the principle of inferability (see Kreuz (1996)), Bamman and Smith (2015) modeled shared common knowledge by extracting features from context, the author, and the audience. Khattri et al. (2015) identified sarcasm by seeking a strong contrast in affect toward named entities in current vs. historical tweets, while Rajadesingan et al. (2015) also exploited a contrast in statistically-derived author traits across current and historical tweets. Zhang et al. (2016) use similar sources of contextual information to show the effectiveness of a neural network over more traditional approaches involving manually-selected, discrete features, claiming that automatic feature induction can uncover more subtle markers of sarcasm. Amir et al. (2016) argue that sarcasm detection hinges on speaker mode"
D17-1050,W10-2914,0,0.633949,"ly dependent not just on the context of an utterance but on the state-of-mind and personality of the speaker, as well as on facial expressions and prosody (Shamay-Tsoory et al., 2005). Without the latter markers, purely textual detection must depend largely on the content and context of an utterance, though speaker personality and state-of-mind can also be approximated via text-analytic means. Probabilistic classification models that exploit textual cues – such as the juxtaposition of positive sentiment and negative situations (Riloff et al., 2013), discriminative words and punctuation marks (Davidov et al., 2010), and emoticon usage (Gonz´alez-Ib´anez et al., 2011) – have achieved good performance across domains, yet these models typically suffer from an absence of psychological insight into a speaker and topical insight into the context of utterance production. Kreuz and Link (2002) argue that the likelihood of sarcasm is proportional to the amount of knowledge shared by speaker and audience, which includes knowledge of the world and knowledge of the speaker and audience. Personality is defined Sarcasm is a pervasive phenomenon in social media, permitting the concise communication of meaning, affect"
D17-1050,W13-1605,0,0.0254535,"Missing"
D17-1050,W16-0425,1,0.79781,"chosen tweet (si ) to its author, appending a yes/no question (qi ) as a comment to elicit a reply. At the time of retweeting (si ), the 11 AnalyzeWords.com dimensions (awi ) of the tweet’s author (ui ) are saved, along with the context tweet (sj ) by author (uj ) that provoked (si ). Authors respond to the bot by favoriting/retweeting the bot’s request or via a reply (rei ) containing #Yes or #No. Author responses often contain more than a simple #Yes or #No response, and so, after observing a series of responses the following linguistic rules were used to extract the training annotations: 5 Ghosh and Veale (2016) described an Artificial Neural Network (ANN) model built around layers of CNNs (Convolutional Neural Networks) and LSTMs (Long Short Term Memory) for sarcasm detection to efficiently capture contrasting text signals of sarcasm within a tweet. We build here on this model as shown in Fig.1, adding input features for the psychological profile of the author and the context of the tweet to those for the tweet itself. The LSTM layer (Hochreiter and Schmidhuber, 1997) captures dependencies amongst nonadjacent contrasting signals for sarcasm within each si . We extend this architecture to include a c"
D17-1050,W13-1104,0,0.0541867,"pragmatic pretense that is designed to be seen through (Campbell and Katz, 2012), so cues such as interjections, intensifiers, punctuation and markers of non-veridicality and hyperbole play a crucial role in recognizing sarcastic intent. Likewise, stock plaudits such as “yay!” or “great!” are common in sarcastic product reviews (Tsur et al., 2010), while hashtags such as #sarcasm, as compressed vehicles for user intent, are often used to self-annotate sarcastic texts (Davidov et al., 2010). Liebrecht et al. (2013) used topic-specific information and n-grams as discriminative features, while (Lukin and Walker, 2013) showed that phrases such as “no way”, “Oh really?” and “not so much” serve to flag a sarcastic intent when used with specific linguistic patterns. Capelli et al. (1990); Woodland and Voyer (2011) suggest that contextual awareness is a necessary precursor to identifying sarcasm. Sarcasm is a response to a motivating context that appears to force a rueful incongruity between a text and its context. Exploiting the principle of inferability (see Kreuz (1996)), Bamman and Smith (2015) modeled shared common knowledge by extracting features from context, the author, and the audience. Khattri et al."
D17-1050,C16-1231,0,0.175304,"o identifying sarcasm. Sarcasm is a response to a motivating context that appears to force a rueful incongruity between a text and its context. Exploiting the principle of inferability (see Kreuz (1996)), Bamman and Smith (2015) modeled shared common knowledge by extracting features from context, the author, and the audience. Khattri et al. (2015) identified sarcasm by seeking a strong contrast in affect toward named entities in current vs. historical tweets, while Rajadesingan et al. (2015) also exploited a contrast in statistically-derived author traits across current and historical tweets. Zhang et al. (2016) use similar sources of contextual information to show the effectiveness of a neural network over more traditional approaches involving manually-selected, discrete features, claiming that automatic feature induction can uncover more subtle markers of sarcasm. Amir et al. (2016) argue that sarcasm detection hinges on speaker modeling, and exploited user embedding to quantify incongruity between utterances and the behavioral traits of their authors. These methods measure the disparity between an utterance and expectations arising from knowledge of context or speaker or both together. Speaker Utt"
D17-1050,C14-1022,0,0.308649,"Missing"
D17-1050,D13-1066,0,0.484093,"Missing"
E09-1095,P88-1027,0,0.110584,"Missing"
E09-1095,J06-1003,0,0.0602834,"n computer science, from common-sense ontologies like Cyc (Lenat and Guha, 1990) and SUMO (Niles and Pease, 2001) to lexical ontologies like WordNet (Miller et al., 1990). Each of these uses is based on the same root-branch-leaf metaphor: the broadest terms with the widest scope occupy the highest positions of a taxonomy, near the root, while specific terms with the most local concerns are located lower in the hierarchy, nearest the leaves. The more interior nodes that a taxonomy possesses, the finer the conceptual distinctions and the more gradated the similarity judgments it can make (e.g., Budanitsky and Hirst, 2006). General-purpose computational taxonomies are called upon to perform both coarse-grained and fine-grained judgments. In NLP, for instance, the semantics of “eat” requires just enough knowledge to discriminate foods like Yanfen Hao School of Computer Science University College Dublin Ireland yanfen.hao@ucd.ie tofu and cheese from non-foods like wool and steel, while specific applications in the domain of cooking and recipes (e.g., Hammond’s (1986) CHEF) require enough discrimination to know that tofu can be replaced with clotted cheese in many recipes because each is a soft, white and bland fo"
E09-1095,P07-1112,0,0.0227823,"stem fishes in a large text for particular word sequences that strongly suggest a semantic relationship such as hypernymy or, in the case of Charniak and Berland (1999), the part-whole relation. Such efforts offer high precision but can exhibit low recall on moderate-sized corpora, and extract just a tiny (but very useful) subset of the semantic content of a text. The KnowItAll system of Etzioni et al. (2004) employs the same generic patterns as Hearst (e.g., “NPs such as NP1, NP2, …”), and more besides, to extract a whole range of facts that can be exploited for web-based question-answering. Cimiano and Wenderoth (2007) also use a range of Hearst-like patterns to find text sequences in web-text that are indicative of the lexico-semantic properties of words; in particular, these authors use phrases like “to * a new NOUN” and “the purpose of NOUN is to *” to identify the formal (isa), agentive (made by) and telic (used for) roles of nouns. Snow, Jurafsky and Ng (2004) use supervised learning techniques to acquire those syntagmatic patterns that prove most useful for extracting hypernym relations from text. They train their system using pairs of WordNet terms that exemplify the hypernym relation; these are used"
E09-1095,P99-1008,0,0.0422678,"tstrapping is completed. The paper then concludes with some final remarks in section 6. 2 Related Work Simple pattern-matching techniques can be surprisingly effective for the extraction of lexico-semantic relations from text when those relations are expressed using relatively stable and unambiguous syntagmatic patterns (Ahlswede and Evens, 1988). For instance, the work of Hearst (1992) typifies this surgical approach to relation extraction, in which a system fishes in a large text for particular word sequences that strongly suggest a semantic relationship such as hypernymy or, in the case of Charniak and Berland (1999), the part-whole relation. Such efforts offer high precision but can exhibit low recall on moderate-sized corpora, and extract just a tiny (but very useful) subset of the semantic content of a text. The KnowItAll system of Etzioni et al. (2004) employs the same generic patterns as Hearst (e.g., “NPs such as NP1, NP2, …”), and more besides, to extract a whole range of facts that can be exploited for web-based question-answering. Cimiano and Wenderoth (2007) also use a range of Hearst-like patterns to find text sequences in web-text that are indicative of the lexico-semantic properties of words;"
E09-1095,C92-2082,0,0.473758,"how the acquired categorizations are validated and filtered after each bootstrapping cycle. An evaluation of the key ideas is then presented in section 5, to determine which seed yields the highest quality taxonomy once bootstrapping is completed. The paper then concludes with some final remarks in section 6. 2 Related Work Simple pattern-matching techniques can be surprisingly effective for the extraction of lexico-semantic relations from text when those relations are expressed using relatively stable and unambiguous syntagmatic patterns (Ahlswede and Evens, 1988). For instance, the work of Hearst (1992) typifies this surgical approach to relation extraction, in which a system fishes in a large text for particular word sequences that strongly suggest a semantic relationship such as hypernymy or, in the case of Charniak and Berland (1999), the part-whole relation. Such efforts offer high precision but can exhibit low recall on moderate-sized corpora, and extract just a tiny (but very useful) subset of the semantic content of a text. The KnowItAll system of Etzioni et al. (2004) employs the same generic patterns as Hearst (e.g., “NPs such as NP1, NP2, …”), and more besides, to extract a whole r"
E09-1095,P08-1119,0,0.537823,"o identify specific sentences in corpora that are most likely to express the relation in lexical terms. A binary classifier is then trained on lexico-syntactic features that are extracted from a dependency-structure parse of these sentences. Kashyap et al., (2005) experiment with a bootstrapping approach to growing concept taxonomies in the medical domain. A gold standard taxonomy provides terms that are used to retrieve documents which are then hierarchically clustered; cohesiveness measures are used to yield a taxonomy of terms that can then further drive the retrieval and clustering cycle. Kozareva et al. (2008) use a bootstrapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. First, they use a doubly-anchored retrieval pattern of the form “NOUNcat such as NOUNexample and *” to ground the retrieval relative to a known example of hypernymy, so that any values extracted for the wildcard * are likely to be coordinate terms of NOUNexample and even more likely to be good examples of NOUNcat. Secondly, they construct a graph of terms that co-occur within this pattern to determine which terms are supported by others, and by how much. These authors also use two kin"
E09-1095,P07-1008,1,0.85682,"explicitly annotated with the properties (ADJcat) that they bequeath to their members. These categories have an obvious descriptive and organizational utility, but of a kind that one is unlikely to find in conventional resources like WordNet and Wikipedia. Kozareva et al. (2008) test their approach on relatively simple and objective categories like states, countries (both 836 closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007). By validating membership in these complex categories using WordNet-based heuristics, we can hang these categories and members on specific WordNet senses, and thus enrich WordNet with this additional taxonomic structure. 3 Seeds for Taxonomic Growth A fine-grained taxonomy can be viewed as a set of triples Tijk = &lt;Ci, Dj, Pk&gt;, where Ci denotes a child of the parent term Pk that possesses the discriminating property Dj; in effect, each such triple expresses that Ci is a specialization of the complex taxonym Dj-Pk. Thus, the belief that cola is a carbonated-drink is expressed by the triple &lt;col"
E09-1095,C08-1119,1,0.897527,"Missing"
hayes-etal-2004-enriching,peters-peters-2000-lexicalised,0,\N,Missing
hayes-etal-2004-enriching,J03-3003,0,\N,Missing
P06-2021,P02-1032,0,0.0611193,"Missing"
P07-1008,J06-1003,0,0.0643994,"umshisky, 57 Yanfen Hao Computer Science and Informatics University College Dublin Ireland yanfen.hao@ucd.ie 2004) but to the underlying ontological structure itself (see Cimiano, Hotho and Staab, 2005). The most revealing variations are syntagmatic in nature, which is to say, they look beyond individual word forms to larger patterns of contiguous usage (Hanks, 2004). In most contexts, the similarity between chocolate, say, and a narcotic like heroin will meagerly reflect the simple ontological fact that both are kinds of substances; certainly, taxonomic measures of similarity as discussed in Budanitsky and Hirst (2006) will capture little more than this commonality. However, in a context in which the addictive properties of chocolate are very salient (e.g., an online dieting forum), chocolate is more likely to be categorized as a drug and thus be considered more similar to heroin. Look, for instance, at the similar ways in which these words can be used: one can be ”chocolate-crazed” or ”chocolate-addicted” and suffer ”chocolate-induced” symptoms (e.g., each of these uses can be found in the pages of Wikipedia). In a context that gives rise to these expressions, it is unsurprising that chocolate should appea"
P07-1008,W04-1908,0,0.0232693,"tion is not binary but gradable. Furthermore, the gradability of category membership is clearly influenced by context: in a corpus describing the exploits of Vikings, an axe will most likely be seen as a kind of weapon, but in a corpus dedicated to forestry, it will likely describe a tool. A resource like WordNet, in which is-a links are reserved for category relationships that are always true, in any context, is going to be inherently limited when dealing with real text. We have described an approach that can be seen as a functional equivalent to the CPA (Corpus Pattern Analysis) approach of Pustejovsky et al. (2004), in which our goal is not that of automated induction of word senses in context (as it is in CPA) but the automated induction of flexible, context-sensitive category structures. As such, our goal is primarily ontological rather than lexicographic, though both approaches are complementary since each views syntagmatic evidence as the key to understanding the use of lexical concepts in context. By defining category membership in terms of syntagmatic expectations, we establish a functional and gradable basis for determining whether one lexical concept (or synset) in WordNet deserves to be seen as"
P07-1008,C04-1133,0,\N,Missing
P08-1060,W04-3221,0,0.0875246,"eb, by using corresponding translations of the commonplace stereotype-establishing pattern “as ADJ as a NOUN”; and in section 4, we describe how these English and Chinese data-sets can be unified using the bilingual ontology HowNet (Dong and Dong, 2006). This mapping allows us to determine the meaning overlap in both data sets, the amount of noise in each data set, and the degree to which this noise is reduced when parallel translations can be identified. In section 5 we demonstrate the overall usefulness of stereotype-based knowledgerepresentation by replicating the clustering experiments of Almuhareb and Poesio (2004, 2005) and showing that stereotype-based representations are both compact and predictive of ontological classification. We conclude the paper with some final remarks in section 6. 2 Related Work Text-based approaches to knowledge acquisition range from the ambitiously comprehensive, in which an entire text or resource is fully parsed and analyzed in depth, to the surgically precise, in which highly-specific text patterns are used to eke out correspondingly specific relationships from a large corpus. Endeavors such as that of Harabagiu et al. (1999), in which each of the textual glosses in Wor"
P08-1060,P99-1008,0,0.0404278,"y precise, in which highly-specific text patterns are used to eke out correspondingly specific relationships from a large corpus. Endeavors such as that of Harabagiu et al. (1999), in which each of the textual glosses in WordNet (Fellbaum, 1998) is linguistically analyzed to yield a sense-tagged logical form, is an example of the former approach. In contrast, foundational efforts such as that of Hearst (1992) typify the latter surgical approach, in which one fishes in a large text for word sequences that strongly suggest a particular semantic relationship, such as hypernymy or, in the case of Charniak and Berland (1999), the partwhole relation. Such efforts offer high precision but low recall, and extract just a tiny (but very useful) subset of the semantic content of a text. The KnowItAll system of Etzioni et al. (2004) employs the same generic patterns as Hearst ( e.g., “NPs such as N P1 , N P2 , ...”), and more besides, to extract a whole range of facts that can be exploited for webbased question-answering. Cimiano and Wenderoth (2007) also use a range of Hearst-like patterns to find text sequences in web-text that are indicative of the lexico-semantic properties of words; in particular, these authors use"
P08-1060,W02-1030,0,0.0323995,"puter Science University College Dublin Belfield, Dublin 4, Ireland tony.veale@ucd.ie Yanfen Hao School of Computer Science University College Dublin Belfield, Dublin 4, Ireland yanfen.hao@ucd.ie Abstract Since computers have limited means of human-like perception, the latter approach is also very much suited to the automatic acquisition of world knowledge by a computer (see Hearst, 1992; Charniak and Berland, 1999; Etzioni et al., 2004; V¨olker et al., 2005; Almuhareb and Poesio, 2005; Cimiano and Wenderoth, 2007; Veale and Hao, 2007). Thus, by using the web as a distributed text corpus (see Keller et al., 2002), a multitude of facts and beliefs can be extracted, for purposes ranging from questionanswering to ontology population. People rarely articulate explicitly what a native speaker of a language is already assumed to know. So to acquire the stereotypical knowledge that underpins much of what is said in a given culture, one must look to what is implied by language rather than what is overtly stated. Similes are a convenient vehicle for this kind of knowledge, insofar as they mark out the most salient aspects of the most frequently evoked concepts. In this paper we perform a multilingual explorati"
P08-1060,J91-4003,0,0.158404,"Missing"
P08-1060,C92-2082,0,0.102391,"on 6. 2 Related Work Text-based approaches to knowledge acquisition range from the ambitiously comprehensive, in which an entire text or resource is fully parsed and analyzed in depth, to the surgically precise, in which highly-specific text patterns are used to eke out correspondingly specific relationships from a large corpus. Endeavors such as that of Harabagiu et al. (1999), in which each of the textual glosses in WordNet (Fellbaum, 1998) is linguistically analyzed to yield a sense-tagged logical form, is an example of the former approach. In contrast, foundational efforts such as that of Hearst (1992) typify the latter surgical approach, in which one fishes in a large text for word sequences that strongly suggest a particular semantic relationship, such as hypernymy or, in the case of Charniak and Berland (1999), the partwhole relation. Such efforts offer high precision but low recall, and extract just a tiny (but very useful) subset of the semantic content of a text. The KnowItAll system of Etzioni et al. (2004) employs the same generic patterns as Hearst ( e.g., “NPs such as N P1 , N P2 , ...”), and more besides, to extract a whole range of facts that can be exploited for webbased questi"
P08-1060,P07-1112,0,0.0846391,"tter surgical approach, in which one fishes in a large text for word sequences that strongly suggest a particular semantic relationship, such as hypernymy or, in the case of Charniak and Berland (1999), the partwhole relation. Such efforts offer high precision but low recall, and extract just a tiny (but very useful) subset of the semantic content of a text. The KnowItAll system of Etzioni et al. (2004) employs the same generic patterns as Hearst ( e.g., “NPs such as N P1 , N P2 , ...”), and more besides, to extract a whole range of facts that can be exploited for webbased question-answering. Cimiano and Wenderoth (2007) also use a range of Hearst-like patterns to find text sequences in web-text that are indicative of the lexico-semantic properties of words; in particular, these authors use phrases like “to * a new NOUN” and “the purpose of NOUN is to *” to identify the agentive and telic roles of given nouns, thereby fleshing out the noun’s qualia structure as posited by Pustejovsky’s (1990) theory of the generative lexicon. The basic Hearst approach has even proven useful for identifying the meta-properties of concepts in a formal ontology. V¨olker et al. (2005) show that patterns like “is no longer a|an NO"
P08-1060,J03-3002,0,0.0780482,"Missing"
P08-1060,W99-0501,0,0.0207081,"replicating the clustering experiments of Almuhareb and Poesio (2004, 2005) and showing that stereotype-based representations are both compact and predictive of ontological classification. We conclude the paper with some final remarks in section 6. 2 Related Work Text-based approaches to knowledge acquisition range from the ambitiously comprehensive, in which an entire text or resource is fully parsed and analyzed in depth, to the surgically precise, in which highly-specific text patterns are used to eke out correspondingly specific relationships from a large corpus. Endeavors such as that of Harabagiu et al. (1999), in which each of the textual glosses in WordNet (Fellbaum, 1998) is linguistically analyzed to yield a sense-tagged logical form, is an example of the former approach. In contrast, foundational efforts such as that of Hearst (1992) typify the latter surgical approach, in which one fishes in a large text for word sequences that strongly suggest a particular semantic relationship, such as hypernymy or, in the case of Charniak and Berland (1999), the partwhole relation. Such efforts offer high precision but low recall, and extract just a tiny (but very useful) subset of the semantic content of"
P08-1060,P07-1008,1,0.916398,"itly what a native speaker of a language is already assumed to know. So to acquire the stereotypical knowledge that underpins much of what is said in a given culture, one must look to what is implied by language rather than what is overtly stated. Similes are a convenient vehicle for this kind of knowledge, insofar as they mark out the most salient aspects of the most frequently evoked concepts. In this paper we perform a multilingual exploration of the space of common-place similes, by mining a large body of Chinese similes from the web and comparing these to the English similes harvested by Veale and Hao (2007). We demonstrate that while the simile-frame is inherently leaky in both languages, a multilingual analysis allows us to filter much of the noise that otherwise hinders the knowledge extraction process. In doing so, we can also identify a core set of stereotypical descriptions that exist in both languages and accurately map these descriptions onto a multilingual lexical ontology like HowNet. Finally, we demonstrate that conceptual descriptions that are derived from common-place similes are extremely compact and predictive of ontological structure. 1 Guofu Li School of Computer Science Universi"
P11-1029,W04-3221,0,0.0478272,"Missing"
P11-1029,C92-2082,0,0.0162693,"cially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of met"
P11-1029,P98-2127,0,0.118899,"essions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable co"
P11-1029,J06-1003,0,0.0382247,"t two ideas are related at a generic level, as in “priests and imams” or “mosques and synagogues”. More generally, consider the pattern “X and Y”, where X and Y are proper-names (e.g., “Zeus and Hera”), or X and Y are inflected nouns or verbs with the same inflection (e.g., the plurals “cats and dogs” or the verb forms “kicking and screaming”). Millions of matches for this pattern can be found in the Google 3-grams (Brants and Franz, 2006), allowing us to build a map of comparable terms by linking the root-forms of X and Y with a similarity score obtained via a WordNet-based measure (e.g. see Budanitsky and Hirst (2006) for a good selection). The pragmatic neighborhood of a term X can be defined as {X, X1, X2, …, Xn}, so that for each Xi, the Google 3-grams contain “X+inf and Xi+inf” or “X+inf and Xi+inf”. The boundaries of neighborhoods are thus set by usage patterns: if ?X denotes the neighborhood of X, then ?artist matches not just artist, composer and poet, but studio, portfolio and gallery, and many other terms that are semantically dissimilar but pragmatically linked to artist. Since each Xi ∈ ?X is ranked by similarity to X, query matches can also be ranked by similarity. When X is an adjective, then"
P11-1029,J91-1003,0,0.57383,"h also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distin"
P11-1029,H92-1086,0,0.0186814,"ansparent, immutable, beautiful, tough, expensive, valuable, shiny, bright, lasting, desirable, strong, …, hard} . If A is an adjective, then @ A matches any element of the set {N1, N2, …, Nn}, where each Ni is a noun denoting a stereotype for which A is a culturally established property. For example, @tall matches any element of {giraffe, skyscraper, tree, redwood, tower, sunflower, lighthouse, beanstalk, rocket, …, supermodel}. Stereotypes crystallize in a language as clichés, so one can argue that stereotypes and clichés are little or no use to a creative IR system. Yet, as demonstrated in Fishlov (1992), creative language 281 is replete with stereotypes, not in their clichéd guises, but in novel and often incongruous combinations. The creative value of a stereotype lies in how it is used, as we’ll show later in section 4. 3.3 The Ad-Hoc Category Wildcard ^X Barsalou (1983) introduced the notion of an adhoc category, a cross-cutting collection of often disparate elements that cohere in the context of a specific task or goal. The ad-hoc nature of these categories is reflected in the difficulty we have in naming them concisely: the cumbersome “things to take on a camping trip” is Barsalou’s mos"
P11-1029,P07-1008,1,0.946501,"o construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of metaphoricity based on word-sense distributions in corpora. Veale and Hao (2007) exploit the simile frame “as X as Y” to harvest a great many common similes and their underlying stereotypes from the web (e.g. “as hot as an oven”), while Veale and Hao (2010) show that the pattern “about as X as Y” retrieves an equally large collection of creative (if mostly ironic) comparisons. These authors demonstrate that a large vocabulary of stereotypical ideas (over 4000 nouns) and their salient properties (over 2000 adjectives) can be harvested from the web. We now build on these results to develop a set of new semantic operators, that use corpus-derived knowledge to support finely"
P11-1029,J05-4002,0,0.0203001,"lexicalized in a language, they are unlikely to identify relatively novel expressions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc"
P11-1029,J04-1002,0,\N,Missing
P11-1029,C98-2122,0,\N,Missing
P11-4003,P07-1008,1,0.917041,"ion of cultural knowledge: they point to, and exploit, the shared cultural touchstones that speakers and listeners alike can use to construct and intuit meanings. Taylor (1954) catalogued thousands of proverbial comparisons and similes from California, identifying just as many building blocks in the construction of new phrases and figurative meanings. Only the most common similes can be found in dictionaries, as shown by Norrick (1986), while Moon (2008) demonstrates that large-scale corpus analysis is needed to identify folk similes with a breadth approaching that of Taylor’s study. However, Veale and Hao (2007) show that the World-Wide Web is the ultimate resource for harvesting similes. Veale and Hao use the Google API to find many instances of the pattern “as ADJ as a|an *” on the web, where ADJ is an adjectival property and * is the Google wildcard. WordNet (Fellbaum, 1998) is used to provide a set of over 2,000 different values for ADJ, and the text snippets returned by Google are parsed to extract the basic simile bindings. Once the bindings are annotated to remove noise, as well as frequent uses of irony, this Web harvest produces over 12,000 cultural bindings between a noun (such as fish, or"
P12-2015,esuli-sebastiani-2006-sentiwordnet,0,0.0423197,"Missing"
P12-2015,P07-1054,0,0.0593375,"Missing"
P12-2015,W10-0204,0,0.015107,"hissell’s (1989) Dictionary of Affect (or DoA) assigns a trio of scores to each of its 8000+ words to describe three psycholinguistic dimensions: pleasantness, activation and imagery. In the DoA, the lowest pleasantness score of 1.0 is assigned to words like abnormal and ugly, while the highest, 3.0, is assigned to words like wedding and winning. Though Whissell’s DoA is based on human ratings, Turney (2002) shows how affective valence can be derived from measures of word association in web texts. Human intuitions are prized in matters of lexical affect. For reliable results on a large-scale, Mohammad & Turney (2010) and Mohammad & Yang (2011) thus used the Mechanical Turk to elicit human ratings of the emotional content of words. Ratings were sought along the eight dimensions identified in Plutchik (1980) as primary emotions: trust , anger, anticipation, disgust, fear, joy, sadness and surprise. Automated tests were used to exclude unsuitable raters. In all, 24,000+ wordsense pairs were annotated by five different raters. 75 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 75–79, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Li"
P12-2015,W11-1709,0,0.0173443,"f Affect (or DoA) assigns a trio of scores to each of its 8000+ words to describe three psycholinguistic dimensions: pleasantness, activation and imagery. In the DoA, the lowest pleasantness score of 1.0 is assigned to words like abnormal and ugly, while the highest, 3.0, is assigned to words like wedding and winning. Though Whissell’s DoA is based on human ratings, Turney (2002) shows how affective valence can be derived from measures of word association in web texts. Human intuitions are prized in matters of lexical affect. For reliable results on a large-scale, Mohammad & Turney (2010) and Mohammad & Yang (2011) thus used the Mechanical Turk to elicit human ratings of the emotional content of words. Ratings were sought along the eight dimensions identified in Plutchik (1980) as primary emotions: trust , anger, anticipation, disgust, fear, joy, sadness and surprise. Automated tests were used to exclude unsuitable raters. In all, 24,000+ wordsense pairs were annotated by five different raters. 75 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 75–79, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Liu et al. (2003)"
P12-2015,strapparava-valitutti-2004-wordnet,0,0.164957,"Missing"
P12-2015,P05-1017,0,0.0365701,"build a reference set -R of typically negative words, and a set +R of typically positive words. Given a few seed members of -R (such as sad, evil, etc.) and a few seed members of +R (such as happy, wonderful, etc.), we find many other candidates to add to +R and -R by considering neighbors of these seeds in N. After just three iterations, +R and -R contain ~2000 words each. For a property p, we define N+(p) and N-(p) as (1) N+(p) = N(p) ∩ +R (2) N-(p) = N(p) ∩ -R We assign pos/neg valence scores to each property p by interpolating from reference values to their neighbors in N. Unlike that of Takamura et al. (2005), the approach is non-iterative and involves 77 no feedback between the nodes of N, and thus, no inter-dependence between adjacent affect scores: |N+(p)| (3) pos(p) = |N+(p) ∪ N-(p)| (4) neg(p) = 1 - pos(p) If a term S denotes a stereotypical idea and is described via a set of typical properties and behaviors typical(S) in the lexicon, then: (5) pos(S) = Σp∈typical(S) pos(p) |typical(S)| (6) neg(S) = 1 - pos(S) Thus, (5) and (6) calculate the mean affect of the properties and behaviors of S, as represented via typical(S). We can now use (3) and (4) to separate typical(S) into those elements th"
P12-2015,P02-1053,0,0.00474252,"ereotypical qualities are highlighted or Related Work and Ideas In its simplest form, an affect lexicon assigns an affective score – along one or more dimensions – to each word or sense. For instance, Whissell’s (1989) Dictionary of Affect (or DoA) assigns a trio of scores to each of its 8000+ words to describe three psycholinguistic dimensions: pleasantness, activation and imagery. In the DoA, the lowest pleasantness score of 1.0 is assigned to words like abnormal and ugly, while the highest, 3.0, is assigned to words like wedding and winning. Though Whissell’s DoA is based on human ratings, Turney (2002) shows how affective valence can be derived from measures of word association in web texts. Human intuitions are prized in matters of lexical affect. For reliable results on a large-scale, Mohammad & Turney (2010) and Mohammad & Yang (2011) thus used the Mechanical Turk to elicit human ratings of the emotional content of words. Ratings were sought along the eight dimensions identified in Plutchik (1980) as primary emotions: trust , anger, anticipation, disgust, fear, joy, sadness and surprise. Automated tests were used to exclude unsuitable raters. In all, 24,000+ wordsense pairs were annotate"
P12-2015,P07-1008,1,0.889496,"Missing"
P12-2015,P11-1029,1,0.824595,"property of NOUN. Veale & Hao harvested tens of thousands of instances of this pattern from the Web, to extract sets of adjectival properties for thousands of commonplace nouns. They show that if one estimates the pleasantness of a term like snake or artist as a weighted average of the pleasantness of its properties (like sneaky or creative) in 76 a resource like Whissell’s DoA, then the estimated scores show a reliable correlation with the DoA’s own scores. It thus makes computational sense to calculate the affect of a word-concept as a function of the affect of its most salient properties. Veale (2011) later built on this work to show how a property-rich stereotypical representation could be used for non-literal matching and retrieval of creative texts, such as metaphors and analogies. Both Liu et al. (2003) and Veale & Hao (2010) argue for the importance of common-sense knowledge in the determination of affect. We incorporate ideas from both here, while choosing to build mainly on the latter, to construct a nuanced, two-level model of the affective lexicon. 3 An Affective Lexicon of Stereotypes We construct the stereotype-based lexicon in two stages. For the first layer, a large collection"
P12-2015,W04-3252,0,\N,Missing
P12-3002,C10-1113,0,0.0472573,"ility of expression with machines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a tyrant or a god, or that a certain company is a dinosaur while another is a cult. Those that agree might respond by elaborating the metaphor and providing substantiating evidence, while those that disagree might refute the metaphor and switch to another of their own choosing. A well-chosen metaphor can provide the talking points for an i"
P12-3002,J04-1002,0,\N,Missing
P12-3002,P11-1029,1,\N,Missing
P13-1065,N09-1003,0,0.0634241,"Missing"
P13-1065,W04-3221,0,0.0785586,"Missing"
P13-1065,J06-1003,0,0.0801584,". 1 Seeing is Believing (and Creating) Similarity is a cognitive phenomenon that is both complex and subjective, yet for practical reasons it is often modeled as if it were simple and objective. This makes sense for the many situations where we want to align our similarity judgments with those of others, and thus focus on the same conventional properties that others are also likely to focus upon. This reliance on the consensus viewpoint explains why WordNet (Fellbaum, 1998) has proven so useful as a basis for computational measures of lexico-semantic similarity (e.g. see Pederson et al. 2004, Budanitsky & Hirst, 2006; Seco et al. 2006). These measures reduce the similarity of two lexical concepts to a single number, by viewing similarity as an objective estimate of the overlap in their salient qualities. This convenient perspective is poorly suited to creative or insightful comparisons, but it is sufficient for the many mundane comparisons we often perform in daily life, such as when we organize books or look for items in a supermarket. So if we do not know in which aisle to locate a given item (such as oatmeal), we may tacitly know how to locate a similar product (such as cornflakes) and orient ourselves"
P13-1065,O97-1002,0,0.141166,"Missing"
P13-1065,P08-1119,0,0.0275036,"our similes. Once C is seen to be an exemplary member of the category P-S, such as cola in fizzy-drink, a targeted web search is used to find other members of P-S, via the anchored query “P S such as * and C”. For example, “fizzy drinks such as * and cola” will retrieve web texts in which * is matched to soda or lemonade. Each new member can then be used to instantiate a further query, as in “fizzy drinks such as * and soda”, to retrieve other members of P-S, such as champagne and root beer. This bootstrapping process runs in successive cycles, using doubly-anchored patterns that – following Kozareva et al. (2008) and Veale et al. (2009) – explicitly mention both the category to be populated (P-S) and a recently acquired member of this category (C). As cautioned by Kozareva et al., it is reckless to bootstrap from members to categories to members again if each enfilade of queries is likely to return noisy results. A reliable filter must be applied at each stage, to ensure that any member C that is placed in a category P-S is a sensible member of the category S. Only by filtering in this way can we stop the rapid accumulation of noise. For instance, a WordNet-based filter discards any categorization “P"
P13-1065,N04-3012,0,0.380186,"ty measure for WordNet. 1 Seeing is Believing (and Creating) Similarity is a cognitive phenomenon that is both complex and subjective, yet for practical reasons it is often modeled as if it were simple and objective. This makes sense for the many situations where we want to align our similarity judgments with those of others, and thus focus on the same conventional properties that others are also likely to focus upon. This reliance on the consensus viewpoint explains why WordNet (Fellbaum, 1998) has proven so useful as a basis for computational measures of lexico-semantic similarity (e.g. see Pederson et al. 2004, Budanitsky & Hirst, 2006; Seco et al. 2006). These measures reduce the similarity of two lexical concepts to a single number, by viewing similarity as an objective estimate of the overlap in their salient qualities. This convenient perspective is poorly suited to creative or insightful comparisons, but it is sufficient for the many mundane comparisons we often perform in daily life, such as when we organize books or look for items in a supermarket. So if we do not know in which aisle to locate a given item (such as oatmeal), we may tacitly know how to locate a similar product (such as cornfl"
P13-1065,E09-1095,1,0.853194,"n to be an exemplary member of the category P-S, such as cola in fizzy-drink, a targeted web search is used to find other members of P-S, via the anchored query “P S such as * and C”. For example, “fizzy drinks such as * and cola” will retrieve web texts in which * is matched to soda or lemonade. Each new member can then be used to instantiate a further query, as in “fizzy drinks such as * and soda”, to retrieve other members of P-S, such as champagne and root beer. This bootstrapping process runs in successive cycles, using doubly-anchored patterns that – following Kozareva et al. (2008) and Veale et al. (2009) – explicitly mention both the category to be populated (P-S) and a recently acquired member of this category (C). As cautioned by Kozareva et al., it is reckless to bootstrap from members to categories to members again if each enfilade of queries is likely to return noisy results. A reliable filter must be applied at each stage, to ensure that any member C that is placed in a category P-S is a sensible member of the category S. Only by filtering in this way can we stop the rapid accumulation of noise. For instance, a WordNet-based filter discards any categorization “P S such as X and C” where"
P13-1065,P11-1029,1,0.87436,"cowboy” or “as grizzled as a cowboy”. So for each property P suggested by Google n-grams for a lexical concept C, we generate a like-simile for verbal behaviors such as swaggering and an as-as-simile for adjectives such as lonesome. Each is then dispatched to Google as a phrasal query. We value quality over size, as these similes will later be used to find diverse viewpoints on the web via bootstrapping. We thus manually filter each web simile, to weed out any that are ill-formed, and those intended to be seen as ironic by their authors. This gives us a body of 12,000+ valid web similes. 663 Veale (2011, 2012, 2013) notes that web uses of the pattern “as P as C” are rife with irony. In contrast, web instances of “P S such as C” – where S denotes a superordinate of C – are rarely ironic. Hao & Veale (2010) exploit this fact to filter ironic comparisons from web similes, by re-expressing each “as P as C” simile as “P * such as C” (using a wildcard * to match any values for S) and looking for attested uses of this new form on the web. Since each hit will also yield a value for S via the wildcard *, and a finegrained category P-S for C, we use this approach here to harvest fine-grained categorie"
P13-1065,P94-1019,0,0.0442815,"gauge the overlap in information content, and thus of meaning, of two lexical concepts. We need only identify the deepest point in the taxonomy at which this content starts to diverge. This point of divergence is often called the LCS, or least common subsumer, of two concepts (Pederson et al., 2004). Since sub-categories add new properties to those they inherit from their parents – Aristotle called these properties the differentia that stop a category system from trivially collapsing into itself – the depth of a lexical concept in a taxonomy is an intuitive proxy for its information content. Wu & Palmer (1994) use the depth of a lexical concept in the WordNet hierarchy as such a proxy, and thereby estimate the similarity of two lexical concepts as twice the depth of their LCS divided by the sum of their individual depths. Leacock and Chodorow (1998) instead use the length of the shortest path between two concepts as a proxy for the conceptual distance between them. To connect any two ideas in a hierarchical system, one must vertically ascend the hierarchy from one concept, change direction at a potential LCS, and then descend the hierarchy to reach the second concept. (Aristotle was also first to s"
S10-1007,P07-1072,0,0.0565426,"ited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose a"
S10-1007,I05-1082,1,0.151111,"a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected dire"
S10-1007,P06-2064,1,0.272527,"(MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This pa"
S10-1007,E03-1073,0,0.0115181,"ample, malaria mosquito is a ‘mosquito that carries malaria’. Evaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems. 1 Introduction Noun compounds (NCs) are sequences of two or more nouns that act as a single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Bald´ S´eaghdha, 2008). As a win and Tanaka, 2004;"
S10-1007,C94-2125,0,0.271982,"Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edit"
S10-1007,W04-2609,0,0.051487,"ing, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popular"
S10-1007,P08-1052,1,0.415224,"e, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This paper gives a bird’s-eye view of the task. Section 2"
S10-1007,W06-3813,1,0.819134,"lex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-bas"
S10-1007,W07-1108,1,0.889705,"Missing"
S10-1007,D08-1027,0,0.00993176,"Missing"
S10-1007,C08-1011,1,\N,Missing
S10-1007,W03-1803,0,\N,Missing
S10-1007,W04-0404,0,\N,Missing
S10-1007,P84-1109,0,\N,Missing
S10-1007,W01-0511,0,\N,Missing
S10-1051,C08-1011,1,0.836547,"presents the results of our empirical evaluation of the UCD-Goggle system. Introduction 2 Noun compounds (NC) are sequences of nouns acting as a single noun (Downing, 1977). Research on noun compounds involves two main tasks: NC detection and NC interpretation. The latter has been studied in the context of many natural language applications, including questionanswering, machine translation, information retrieval, and information extraction. The use of multiple paraphrases as a semantic intepretation of noun compounds has recently become popular (Kim and Baldwin, 2006; Nakov and Hearst, 2006; Butnariu and Veale, 2008; Nakov, 2008). The best paraphrases are those which most aptly characterize the relationship between the modifier noun and the head noun. The aim of this current work is to provide a ranking for a list of paraphrases that best approximates human rankings for the same paraphrases. We have created a system called UCD-Goggle, which uses semantic knowledge acquired from Google n-grams together with human-preferences mined from training data. Three major components are involved in our system: B-score, produced by a Bayesian algorithm using semantic knowledge from the n-grams corpus with a smoothin"
S10-1051,S10-1007,1,0.837417,"Missing"
S10-1051,P06-2064,0,\N,Missing
S10-1051,W09-2416,1,\N,Missing
S13-2025,W04-0404,0,0.0532624,"Missing"
S13-2025,C08-1011,1,0.662917,"ages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings."
S13-2025,W09-2416,1,0.899683,"Missing"
S13-2025,P07-1072,0,0.203668,"Missing"
S13-2025,P06-2064,0,0.395087,"Missing"
S13-2025,W04-2609,0,0.0835979,"located in Geneva. 138 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE)"
S13-2025,P08-1052,1,0.483189,"uation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve r"
S13-2025,E09-1071,1,0.900906,"Missing"
S13-2025,C08-1082,1,0.890902,"Missing"
S13-2025,W03-1803,0,0.161511,"Missing"
S13-2025,P10-1070,0,0.829879,"Missing"
S15-2080,C10-1113,1,0.0610957,"Missing"
S17-2011,P15-1070,0,0.186587,"Missing"
S17-2011,S17-2005,0,0.0578134,"isement catchphrases (Tanaka, 1992), puns have been widely used as a humorous and rhetorical device. For a polysemous word, the non-literal meaning is addressed when contextual information has low accordance with it’s primary or most prominent meaning (Giora, 1997). A pun can be seen as a democratic form of literal and non-literal meaning. In using puns, the author alternates an idiomatic expression to a certain extent or provides enough context for a polysemous word to evoke non-literal meaning without attenuating literal meaning completely (Giora, 2002). Task 7 of the 2017 SemEval workshop (Miller et al., 2017) involves three subtasks. The first subtask requires the system to classify a given context into two binary categories: puns and non-puns. The second subtask concerns itself with finding the word producing the punning effect in a given context. The third and final subtask involves annotating puns with the dual senses with which the 2 General Approach We argue that the detection of heterographic puns rests on two assumptions. Firstly, the word being used to introduce the punning effect is phonetically similar to the intended word, so that the reader can infer the desired meaning behind the pun."
S17-2011,P06-4018,0,0.00713836,"tegy, similarity score were calculated between gi and gj , the gloss of wj ∈ ci . In most of the cases, pun words and their grounding words in the context do not share the same part-of-speech (POS) tags. In the latter strategy, we added a POS damping factor, noted as pij of 0.2 if the POS tags of wi and wj are equal. Following Optimal Innovation hypothesis, the similarity of a punning word and its grounding word should neither be too high or too low in order to evoke the non-literal meaning. We applied following correction on computed similarities. ( 0 x &lt; 0.01 fws (x) = 1 − x x &gt;= 0.01 NLTK (Bird, 2006) and also analyses the suffixes of the last word in the context (for example, words ending in “ly”). With relation to tasks 1, an amalgamation of this approach and the original is performed. If the highest score does not exceed the threshold, we check to see if the pun is of type Tom Swifty. If this is the case, then we mark the context as a pun. Task 2 operates similarly - if the pun is flagged as a Tom Swifty, then the last adverb is returned as a candidate. For task 3 however, we need to transform the adverb into the intended word in order to get the appropriate sense entry in WordNet. To d"
S18-1093,W10-2914,0,0.192861,"t a solid understanding of irony, one can still recognize and produce 1 Tony Veale University College Dublin Dublin, Ireland tony.veale@ucd.ie 2 https://nlp.stanford.edu/projects/glove/ www.Twitter.com 570 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 570–575 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 2.1 negative situations (Riloff et al., 2013); discriminative N-grams like ‘yay!’ or ‘great!’ or “oh really” or “yeah right” (Tsur et al., 2010; Lukin and Walker, 2013); social media markers like hashtags (Davidov et al., 2010); emoticon usage (Gonz´alez-Ib´anez et al., 2011); and topics associated with irony (e.g. schools, dentists, church life, public transport, the weather). Carvalho et al. (2009) exploited text patterns in comments on articles of online newspapers to detect ironic statements, while Van Hee et al. (2016) developed a irony detection model using support vector machine (SVM) with a combination of lexical, syntactic, sentiment, and semantic (Word2Vec embedding) features. In recent times, multiple research attempts, founded on variants of the deep neural network built on top of word embeddings, showed"
S18-1093,W16-0425,1,0.947143,"al, syntactic, sentiment, and semantic (Word2Vec embedding) features. In recent times, multiple research attempts, founded on variants of the deep neural network built on top of word embeddings, showed a significant improvement over traditional methods over several natural language processing (NLP) tasks. A few representative works in this direction for detecting sarcasm, a demeaning variant of irony, especially in the colloquial form, are based on Convolutional Neural Networks(CNN) (Mishra et al., 2017), Recurrent Neural Networks (RNN) (Zhang et al., 2016) and a combination of CNNs and RNNs (Ghosh and Veale, 2016). Siamese networks (Bromley et al., 1994), widely used in image classification, have displayed a good performance over sentence similarity or document similarity tasks. A Siamese architecture contains two identical sub-networks, which are trained with two different inputs to distinguish the difference among them. Since in irony, different parts of a sentence can be incongruous with each other, we adopted the Siamese architecture to detect such incongruity between different sections of a sentence. In this implementation, each of the subnetworks consists of an embedding layer and a LSTM layer. W"
S18-1093,W07-0101,0,0.0799913,"ing in a completely unexpected output (“shortly after I started losing friends”). Due to the limited scope of expression in social media such as Twitter2 , authors often end up lacing their statements with ironic cue words (‘Yay’) or social media specific features such as hashtags (‘#irony’) to make their ironic intent more obvious for the reader. Following this intuition, most of the attempts were made using probabilistic classification models which relied on textual cues such as lexical indicators like punctuation symbols (e.g., ‘?’), interjections (e.g., “gee” or “gosh”), and intensifiers (Kreuz and Caucci, 2007); the juxtaposition of positive sentiment and This paper describes our system, entitled IronyMagnet, for the 3rd Task of the SemEval 2018 workshop, “Irony Detection in English Tweets”. In Task 1, irony classification task has been considered as a binary classification task. Now for the first time, finer categories of irony are considered as part of a shared task. In task 2, three types of irony are considered; “Irony by contrast” - ironic instances where evaluative expression portrays inverse polarity (positive, negative) of the literal proposition; “Situational irony” - ironic instances where"
S18-1093,P17-1035,0,0.0149643,"et al. (2016) developed a irony detection model using support vector machine (SVM) with a combination of lexical, syntactic, sentiment, and semantic (Word2Vec embedding) features. In recent times, multiple research attempts, founded on variants of the deep neural network built on top of word embeddings, showed a significant improvement over traditional methods over several natural language processing (NLP) tasks. A few representative works in this direction for detecting sarcasm, a demeaning variant of irony, especially in the colloquial form, are based on Convolutional Neural Networks(CNN) (Mishra et al., 2017), Recurrent Neural Networks (RNN) (Zhang et al., 2016) and a combination of CNNs and RNNs (Ghosh and Veale, 2016). Siamese networks (Bromley et al., 1994), widely used in image classification, have displayed a good performance over sentence similarity or document similarity tasks. A Siamese architecture contains two identical sub-networks, which are trained with two different inputs to distinguish the difference among them. Since in irony, different parts of a sentence can be incongruous with each other, we adopted the Siamese architecture to detect such incongruity between different sections"
S18-1093,D13-1066,0,0.103591,"Missing"
S18-1093,C16-1257,0,0.333638,"Missing"
S18-1093,C16-1231,0,0.0122541,"support vector machine (SVM) with a combination of lexical, syntactic, sentiment, and semantic (Word2Vec embedding) features. In recent times, multiple research attempts, founded on variants of the deep neural network built on top of word embeddings, showed a significant improvement over traditional methods over several natural language processing (NLP) tasks. A few representative works in this direction for detecting sarcasm, a demeaning variant of irony, especially in the colloquial form, are based on Convolutional Neural Networks(CNN) (Mishra et al., 2017), Recurrent Neural Networks (RNN) (Zhang et al., 2016) and a combination of CNNs and RNNs (Ghosh and Veale, 2016). Siamese networks (Bromley et al., 1994), widely used in image classification, have displayed a good performance over sentence similarity or document similarity tasks. A Siamese architecture contains two identical sub-networks, which are trained with two different inputs to distinguish the difference among them. Since in irony, different parts of a sentence can be incongruous with each other, we adopted the Siamese architecture to detect such incongruity between different sections of a sentence. In this implementation, each of the sub"
seco-etal-2004-concept,J03-3005,0,\N,Missing
veale-hao-2008-acquiring,C92-2082,0,\N,Missing
veale-hao-2008-acquiring,P99-1008,0,\N,Missing
veale-hao-2008-acquiring,W04-3221,0,\N,Missing
veale-hao-2008-acquiring,W99-0501,0,\N,Missing
W03-1404,J91-4003,0,0.0472092,"th existing metaphors for thinking about the target concept, so that when viewed collectively, they together suggest the operation of a common underlying schema. This view of systematicity is external to the concepts involved since it predicates their aptness to each other on the existence of other structures (metaphor schemas) into which they can be coherently connected. In this paper we argue that the lexicon is central to the determination of both kinds of systematicity, internal and external, especially if one is an adherent of the generative lexicon view of word meaning as championed by (Pustejovsky, 1991). In such a lexicon we can expect to find precisely the kind of relational structure needed to perform structure mapping and thereby measure the internal systematicity of a metaphor like “ a passport is a travel diary” . In addition, we can expect to find the lexicalized metaphor structures that represent the surface manifestations of existing modes of thought, and it is against these structures that the external systematicity of an interpretation can be measured. This research is conducted in the context of WordNet (Miller, 1995; Fellbaum, 1998), a comprehensive lexical knowledge-base of Engl"
W03-1404,W99-0501,0,0.0146229,"glish. The structure of WordNet makes explicit some of the relationships needed to construct a generative lexicon, most obviously the formal (taxonomic) and constitutive (meronymic) aspects of word meaning. But to truly test a model of metaphoric interpretation on a large-scale, it is necessary to augment these relationships with the telic and agentive components that are not encoded directly but merely alluded to in the textual glosses associated with each sense entry. In the sections to follow we describe a mechanism for automating the extraction of these relationships (in the same vein as (Harabagiu et al. 1999), and for using them to generative apt interpretations for metaphors involving WordNet entries. 2 Qualia Extraction from Glosses In a generative lexicon, the core elements of word meaning are represented by a nexus of relations called a qualia structure, which ties together the formal (i.e., hierarchical relations), constitutive (i.e., meronymic), telic (i.e., functional) and agentive (i.e., construction/creation) aspects of a word. For instance, a diary is formally a kind of ?book’ that constitutes a ?collection of personal writings’ whose telic purpose is to ?record’ the observations of the"
W09-2416,W04-0404,0,0.485888,"Missing"
W09-2416,C08-1011,1,0.300292,"he problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability distribution over a range of candidates. For example, highly probable paraphrases for chocolate bar are bar made of chocolate and bar that tastes like chocolate, while bar that eats chocolate is very unlikely. As described in Section 3.3, a set of goldstandard paraphrase distributions can be constructed by collating responses from a large number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we"
W09-2416,P07-1072,0,0.421183,"Missing"
W09-2416,P84-1109,0,0.450496,"araphrasing models, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a gi"
W09-2416,W96-0309,0,0.0699356,"ls, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability d"
W09-2416,P06-2064,1,0.371927,"rge number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for detai"
W09-2416,E03-1073,0,0.0568415,"Missing"
W09-2416,P08-1052,1,0.883976,"jects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for details.3 The most popular par"
W09-2416,W07-1108,1,0.886065,"Missing"
W09-2416,W01-0511,0,0.105028,"ONSTITUTE into SOURCE-RESULT, RESULT-SOURCE and COPULA; COPULA is then further subdivided at two additional levels. 101 In computational linguistics, popular inventories of semantic relations have been proposed by Nastase and Szpakowicz (2003) and Girju et al. (2005), among others. The former groups 30 finegrained relations into five coarse-grained supercategories, while the latter is a flat list of 21 relations. Both schemes are intended to be suitable for broad-coverage analysis of text. For specialized applications, however, it is often useful to use domain-specific relations. For example, Rosario and Hearst (2001) propose 18 abstract relations for interpreting NCs in biomedical text, e.g., DEFECT, MATERIAL, PERSON AFFILIATED, ATTRIBUTE OF CLINICAL STUDY. Inventory-based analyses offer significant advantages. Abstract relations such as ‘location’ and ‘possession’ capture valuable generalizations about NC semantics in a parsimonious framework. Unlike paraphrase-based analyses (Section 2.2), they are not tied to specific lexical items, which may themselves be semantically ambiguous. They also lend themselves particularly well to automatic interpretation methods based on multi-class classification. On the"
W09-2416,D08-1027,0,0.0137166,"Missing"
W09-2416,W03-1803,0,0.0693929,"Missing"
W14-2307,C10-1113,0,0.503744,"y of creative computational tasks, we present here a view of metaphor as a public Web service that can be freely called on demand. 1 Introduction Metaphor is a knowledge-hungry phenomenon. Fortunately, much of the knowledge needed for the processing of metaphor is already implicit in the large body of metaphors that are active in a language community (e.g. Martin, 1990; Mason, 2004). For existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (see Veale, 2011; Shutova, 2010). Thus, though linguistic metaphors are most naturally viewed as the output of a language generation process, and as the input to a language understanding process, it is just as meaningful to view the conceptual metaphors that underpin these linguistic forms as an input to the generation process and an output of the understanding process. A rich source of existing linguistic metaphors, such as a text corpus or a database of Web 52 Proceedings of the Second Workshop on Metaphor in NLP, pages 52–60, c Baltimore, MD, USA, 26 June 2014. 2014 Association for Computational Linguistics presented, to"
W14-2307,J91-1003,0,0.874986,"using on conventional metaphors (e.g., Martin, 1990; Mason, 2004), or on specific domains of usage, such as figurative descriptions of mental states (e.g., Barnden, 2006). From the earliest computational forays, it has been recognized that metaphor is fundamentally a problem of knowledge representation. Semantic representations are, by and large, designed for well-behaved mappings of words to meanings – what Hanks (2006) calls norms – but metaphor requires a system of soft preferences rather than hard (and brittle) constraints. Wilks (1978) thus proposed a preference semantics approach, which Fass (1991,1997) extended into a collative semantics. In contrast, Way (1990) argued that metaphor requires a dynamic concept hierarchy that can stretch to meet the norm-bending demands of figurative ideation, though her approach lacked specific computational substance. More recently, some success has been obtained with statistical approaches that side-step the problems of knowledge representation, by working instead with implied or latent representations that are derived from word distributions. Turney and Littman (2005) show how a statistical model of relational similarity that is constructed from Web"
W14-2307,H92-1086,0,0.203897,"Web service, it reliably retrieves, with remarkable speed, scores of fascinating similes that have already been indexed for a property. The Jigsaw Bard service can be accessed online at: www.educatedinsolence.com/jigsaw/ The Jigsaw Bard Similes and stereotypes enjoy a mutually beneficial relationship. Stereotypes anchor our similes in familiar concepts with obvious features, while similes, for their part, further popularize these stereotypes and entrench them in a culture. Since the core of any good simile is an evocative stereotype that embodies just the qualities we want to communicate (see Fishelov, 1992), simile generation is essentially a problem of apt stereotype retrieval. However, we can also turn this view on its head by asking: what kinds of simile might be generated from a given stereotype, or a linguistic combination or two or more lexicalized stereotypes? For instance, were we to consider the many phrases in the Google n-grams that combine a lexicalized stereotype with an affective modifier (such as “cold fish”), or that combine multiple stereotypes with shared qualities (such as “chocolate espresso” (brown) or “robot fish” (cold and emotionless)), we might imagine repurposing these"
W14-2307,P07-1008,1,0.7619,"lusters can also be analyzed to find literal paraphrases for a given metaphor (e.g. “to provoke excitement” or “suppress anger”). Shutova’s approach is noteworthy for operating with Lakoff and Johnson’s inventory of conceptual metaphors without using an explicit knowledge representation of the knowledge domains involved. Hanks (2006) argues that metaphors exploit distributional norms: to understand a metaphor, one must first recognize the norm that is exploited. Common norms in language are the preferred semantic arguments of verbs, as well as idioms, clichés and other multi-word expressions. Veale and Hao (2007a) suggest that stereotypes are conceptual norms that are found in many figurative expressions, and note that stereotypes and similes enjoy a symbiotic relationship that has obvious computational advantages. Similes rely on stereotypes to illustrate the qualities ascribed to a topic, while stereotypes are often promulgated via proverbial similes (Taylor, 1954). Veale and Hao (2007a) show how stereotypical knowledge can be acquired by harvesting “Hearst” patterns (Hearst, 1992) of the form “as P as C” (e.g. “as smooth as silk”) from the Web. They go on to show in (2007b) how this body of stereo"
W14-2307,C92-2082,0,0.0453428,"uage are the preferred semantic arguments of verbs, as well as idioms, clichés and other multi-word expressions. Veale and Hao (2007a) suggest that stereotypes are conceptual norms that are found in many figurative expressions, and note that stereotypes and similes enjoy a symbiotic relationship that has obvious computational advantages. Similes rely on stereotypes to illustrate the qualities ascribed to a topic, while stereotypes are often promulgated via proverbial similes (Taylor, 1954). Veale and Hao (2007a) show how stereotypical knowledge can be acquired by harvesting “Hearst” patterns (Hearst, 1992) of the form “as P as C” (e.g. “as smooth as silk”) from the Web. They go on to show in (2007b) how this body of stereotypes can be used in a Web-based model of metaphor generation and comprehension. Veale (2011) employs stereotypes as the basis of the Creative Information Retrieval paradigm, by introducing a variety of non-literal-matching wildcards in the vein of Mihalcea (2002). In this paradigm, @Noun matches any adjective that denotes a stereotypical property of Noun (so e.g. @knife matches sharp, pointy, etc.) while @Adj matches any noun for which Adj is stereotypical (e.g. @sharp matche"
W14-2307,P11-1029,1,0.930578,"n a wide array of creative computational tasks, we present here a view of metaphor as a public Web service that can be freely called on demand. 1 Introduction Metaphor is a knowledge-hungry phenomenon. Fortunately, much of the knowledge needed for the processing of metaphor is already implicit in the large body of metaphors that are active in a language community (e.g. Martin, 1990; Mason, 2004). For existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (see Veale, 2011; Shutova, 2010). Thus, though linguistic metaphors are most naturally viewed as the output of a language generation process, and as the input to a language understanding process, it is just as meaningful to view the conceptual metaphors that underpin these linguistic forms as an input to the generation process and an output of the understanding process. A rich source of existing linguistic metaphors, such as a text corpus or a database of Web 52 Proceedings of the Second Workshop on Metaphor in NLP, pages 52–60, c Baltimore, MD, USA, 26 June 2014. 2014 Association for Computational Linguistic"
W14-2307,P13-1065,1,0.843826,"ystem that wishes to take a divergent view of conceptual structure, whether for purposes of literal similarity assessment or for nonliteral metaphoric reasoning. Rex can be used as a browsing tool by Web users in search of insights or apt comparisons – for instance, one can go from Leadership to Creativity via the categories soft skill, valuable skill or transferable skill – or as a flexible similarity service that supports 3rd-party metaphor processing systems. It should be noted that while Rex relies on the Web for its divergent view of the world, it does not sacrifice quality for quantity. Veale & Li (2013) show that a combination of Thesaurus Rex and WordNet produces similarity scores for the standard Miller & Charles (1991) test-set that achieve a 0.93 correlation with human judgments. This is as good as the best machine-learning systems (which do not explain their ratings the way that Rex can) and far superior to any WordNet-only approach. The Thesaurus Rex service can be accessed here: http://boundinanutshell.com/therex2 5 Metaphor Magnet In many ways, a metaphor resembles a query in information retrieval (IR). Metaphors, like queries, allow us to simultaneously express what we believe and t"
W14-2307,J04-1002,0,\N,Missing
W15-1410,P13-1065,1,0.830933,"tation requires us to find a non-trivial category – one a good deal more specific than event – to embrace these very differentseeming concepts (Glucksberg, 1998). To see how people categorize, we need only see how they speak. On the Web, we see descriptions of both war and of divorce, in separate texts, as traumatic events, serious conflicts, immoral acts, and as bad things in general. Such descriptions often come in standardized linguistic containers, such as the “A_Bs such as Cs” pattern of Hearst (1992), instances of which are easily harvested from the Web. The Thesaurus Rex Web service of Veale & Li (2013) offers up its resulting system of Webharvested categorizations as a public service that can be exploited by 3rd-party metaphor systems. Thesaurus Rex can be used for the interpretation of metaphors by permitting another system to explore specific unifying categories for distant ideas, such as divorce & war, but it can also be used in the generation of metaphors. So if looking for a metaphor for creativity, Thesaurus Rex suggests the category special ability, leading a metaphor generator to consider other members of this category as possible vehicles, such as x-ray vision, superior strength, m"
W15-1410,C92-2082,0,0.293218,"rn, central to the act of figurative description. Consider the metaphor divorce is war, whose interpretation requires us to find a non-trivial category – one a good deal more specific than event – to embrace these very differentseeming concepts (Glucksberg, 1998). To see how people categorize, we need only see how they speak. On the Web, we see descriptions of both war and of divorce, in separate texts, as traumatic events, serious conflicts, immoral acts, and as bad things in general. Such descriptions often come in standardized linguistic containers, such as the “A_Bs such as Cs” pattern of Hearst (1992), instances of which are easily harvested from the Web. The Thesaurus Rex Web service of Veale & Li (2013) offers up its resulting system of Webharvested categorizations as a public service that can be exploited by 3rd-party metaphor systems. Thesaurus Rex can be used for the interpretation of metaphors by permitting another system to explore specific unifying categories for distant ideas, such as divorce & war, but it can also be used in the generation of metaphors. So if looking for a metaphor for creativity, Thesaurus Rex suggests the category special ability, leading a metaphor generator t"
W15-1410,C10-1113,0,0.0289724,"ource, Path, Goal and Vehicle. A CM such as Life is a Journey thus allows us to impose the schematic structure of a Journey onto our mental structure of a Life, to understand Life as something with a starting point, a destination, a path to follow and a means of conveyance. Carbonell (1981), Martin (1990) and Barnden (2008) each build and exploit an explicit representation of conceptual metaphors, while Mason (2004) uses statistical methods to extract conventional metaphors – CMs that are so entrenched in the way we speak that their uses in language can often seem literal – from text corpora. Shutova (2010) uses statistical clustering to identify possible target ideas – such as Democracy and Marriage – for a given source idea such as Mechanism. This allows her system to recognize “fix a marriage” and “the functioning of democracy” (or vice versa) as figurative uses of a Mechanism schema because they each use verbs that typically take mechanisms as their objects. But whether one views CMs as real cognitive structures or as useful statistical generalizations, CMs serve as script-like bundles of norms and roles that shape the generation and interpretation of metaphors. In any case, CMs are so often"
W16-0425,D10-1115,0,0.0119839,"sentence’s parts. (Nakagawa et al., 2010) describes a Tree-CRF classifier which uses a data-driven dependency parser, maltparser2 , to obtain a parse tree for a sentence, and whose composition function uses the head-modifier relations of the parse tree. (Mitchell and Lapata, 2010) and (Mitchell and Lapata, 2008) defined the composition function of a sentence by algebraic operations over word meaning vectors to obtain sentence meaning vectors. (Guevara, 2010) and (Malakasiotis, 2011) formulated their composition function using a set of specific syntactic relations or specific word categories (Baroni and Zamparelli, 2010). (Socher et al., 2011) proposed a structured recursive neural network based on the convolutional operation, while (Kalchbrenner et al., 2014) proposed a convolution neural network (CNN) with dynamic k-max pooling, considering max pooling as function of input length. For sarcasm detection, due to the complexity of the task and the somewhat poorer accuracy of start-of-the-art constituency parsers on tweets, researchers have considered surface level lexical and syntactic cues as legitimate features. Kreuz and Caucci (Kreuz and Caucci, 2007) explored the role of lexical indicators, such as interj"
W16-0425,W10-2914,0,0.486496,"ewhat poorer accuracy of start-of-the-art constituency parsers on tweets, researchers have considered surface level lexical and syntactic cues as legitimate features. Kreuz and Caucci (Kreuz and Caucci, 2007) explored the role of lexical indicators, such as interjections (e.g., “gee” or “gosh”), punctuation symbols (e.g., ‘?’), intensifiers, and other linguistic markers for e.g. non-veridicality and hyperbole, in recognizing sarcasm in narratives. Tsur (Tsur et al., 2010) noted the occurrence of “yay!” or “great!” as a recurring aspect of sarcastic patterns in Amazon product reviews. Davidov (Davidov et al., 2010) examined the effectiveness of social media indicators such as hashtags to identify sarcasm. Lukin (Lukin and Walker, 2013) proposed a potential bootstrapping method for sarcasm classification in social dialogue to expand lexical N-gram cues related to sarcasm (e.g. “oh really”, “no way”, etc.) as well as lexico-syntactic patterns. Riloff (Riloff et al., 2013) and Liebrecht (Liebrecht et al., 2013) applied N-grams features to a classifier for English and Dutch tweets and observed that some topics recur frequently in sarcastic tweets, such as schools, dentists, church life, public transport, th"
W16-0425,P15-1061,0,0.00480166,"rom vanishing or exploding gradients while per164 Figure 2: Sentence modelling with CNN forming back propagation through time. LSTM has the capability to remember long distance temporal dependencies. Moreover, as they performs temporal text modelling over input features, higher level modelling can distinguish factors of linguistic variation within the input. CNNs can also capture temporal text sequence through convolutional filters. CNNs reduce frequency variation and convolutional filters connect a subset of the feature space that is shared across the entire input (Chan and Lane, 2015). (Dos Santos et al., 2015) have shown that CNNs can directly capture temporal text patterns for shorter texts, yet in longer texts, where temporal text patterns may span across 15 to 20 words, CNNs must rely on higher-level fully connected layers to model long distance dependencies as the maximum convolutional filter width for a text is 5 (Figure 2). Another major limitation of CNNs is the fixed convolutional filter width, which is not suitable for different lengths of temporal text patterns and cannot always resolve dependencies properly. Obtaining the optimal filter size is expensive and corpusdependent, while LSTM o"
W16-0425,N13-1037,0,0.0104681,"difficult challenge to realize. Accurate semantic modelling of context becomes obligatory for automatic sarcasm detection if social cues and extended meaning are to be grasped. Encouraging an immediate and very social use of language, social media platforms such as Twitter1 are rich sources of texts for Natural Language Processing (NLP). Social micro-texts are dense in figurative language, and are useful for figurative analysis because of their topicality, ease of access, and the use of self-annotation via hashtag. In Twitter, language is distorted, often plumbing the depths of bad language (Eisenstein, 2013). Yet due to the presence of grammatical errors liberally mixed with social media markers (hashtags, emoticons, profiles), abbreviations, and code switching, these micro-texts are harder to parse, and parsing is the most commonly used method to obtain a semantic representation of a sentence. The accuracy of state-of-theart constituency parsers over tweets can be significantly lower than that for normal texts, so social media researchers still largely rely on surface level features. With the recent move to artificial neural networks in NLP, ANNs provide an alternative basis for semantic modelli"
W16-0425,P11-2102,0,0.782155,"Missing"
W16-0425,W10-2805,0,0.00750172,"sentiment analysis, researchers have used syntac1 tic structure to compose a total representation as a function of the word-vector representation of a sentence’s parts. (Nakagawa et al., 2010) describes a Tree-CRF classifier which uses a data-driven dependency parser, maltparser2 , to obtain a parse tree for a sentence, and whose composition function uses the head-modifier relations of the parse tree. (Mitchell and Lapata, 2010) and (Mitchell and Lapata, 2008) defined the composition function of a sentence by algebraic operations over word meaning vectors to obtain sentence meaning vectors. (Guevara, 2010) and (Malakasiotis, 2011) formulated their composition function using a set of specific syntactic relations or specific word categories (Baroni and Zamparelli, 2010). (Socher et al., 2011) proposed a structured recursive neural network based on the convolutional operation, while (Kalchbrenner et al., 2014) proposed a convolution neural network (CNN) with dynamic k-max pooling, considering max pooling as function of input length. For sarcasm detection, due to the complexity of the task and the somewhat poorer accuracy of start-of-the-art constituency parsers on tweets, researchers have consider"
W16-0425,P14-1062,0,0.0180346,"parse tree for a sentence, and whose composition function uses the head-modifier relations of the parse tree. (Mitchell and Lapata, 2010) and (Mitchell and Lapata, 2008) defined the composition function of a sentence by algebraic operations over word meaning vectors to obtain sentence meaning vectors. (Guevara, 2010) and (Malakasiotis, 2011) formulated their composition function using a set of specific syntactic relations or specific word categories (Baroni and Zamparelli, 2010). (Socher et al., 2011) proposed a structured recursive neural network based on the convolutional operation, while (Kalchbrenner et al., 2014) proposed a convolution neural network (CNN) with dynamic k-max pooling, considering max pooling as function of input length. For sarcasm detection, due to the complexity of the task and the somewhat poorer accuracy of start-of-the-art constituency parsers on tweets, researchers have considered surface level lexical and syntactic cues as legitimate features. Kreuz and Caucci (Kreuz and Caucci, 2007) explored the role of lexical indicators, such as interjections (e.g., “gee” or “gosh”), punctuation symbols (e.g., ‘?’), intensifiers, and other linguistic markers for e.g. non-veridicality and hyp"
W16-0425,W07-0101,0,0.577209,"c syntactic relations or specific word categories (Baroni and Zamparelli, 2010). (Socher et al., 2011) proposed a structured recursive neural network based on the convolutional operation, while (Kalchbrenner et al., 2014) proposed a convolution neural network (CNN) with dynamic k-max pooling, considering max pooling as function of input length. For sarcasm detection, due to the complexity of the task and the somewhat poorer accuracy of start-of-the-art constituency parsers on tweets, researchers have considered surface level lexical and syntactic cues as legitimate features. Kreuz and Caucci (Kreuz and Caucci, 2007) explored the role of lexical indicators, such as interjections (e.g., “gee” or “gosh”), punctuation symbols (e.g., ‘?’), intensifiers, and other linguistic markers for e.g. non-veridicality and hyperbole, in recognizing sarcasm in narratives. Tsur (Tsur et al., 2010) noted the occurrence of “yay!” or “great!” as a recurring aspect of sarcastic patterns in Amazon product reviews. Davidov (Davidov et al., 2010) examined the effectiveness of social media indicators such as hashtags to identify sarcasm. Lukin (Lukin and Walker, 2013) proposed a potential bootstrapping method for sarcasm classific"
W16-0425,W13-1605,0,0.327214,"Missing"
W16-0425,W13-1104,0,0.0115159,"al and syntactic cues as legitimate features. Kreuz and Caucci (Kreuz and Caucci, 2007) explored the role of lexical indicators, such as interjections (e.g., “gee” or “gosh”), punctuation symbols (e.g., ‘?’), intensifiers, and other linguistic markers for e.g. non-veridicality and hyperbole, in recognizing sarcasm in narratives. Tsur (Tsur et al., 2010) noted the occurrence of “yay!” or “great!” as a recurring aspect of sarcastic patterns in Amazon product reviews. Davidov (Davidov et al., 2010) examined the effectiveness of social media indicators such as hashtags to identify sarcasm. Lukin (Lukin and Walker, 2013) proposed a potential bootstrapping method for sarcasm classification in social dialogue to expand lexical N-gram cues related to sarcasm (e.g. “oh really”, “no way”, etc.) as well as lexico-syntactic patterns. Riloff (Riloff et al., 2013) and Liebrecht (Liebrecht et al., 2013) applied N-grams features to a classifier for English and Dutch tweets and observed that some topics recur frequently in sarcastic tweets, such as schools, dentists, church life, public transport, the weather and so on. 2 https://twitter.com 162 http://www.maltparser.org/ In this paper, we investigate the usefulness of n"
W16-0425,P08-1028,0,0.0168299,"tweets, the semantic modelling of tweets has captured the attention of researchers. To build a semantic representation of a sentence in various NLP tasks such as sentiment analysis, researchers have used syntac1 tic structure to compose a total representation as a function of the word-vector representation of a sentence’s parts. (Nakagawa et al., 2010) describes a Tree-CRF classifier which uses a data-driven dependency parser, maltparser2 , to obtain a parse tree for a sentence, and whose composition function uses the head-modifier relations of the parse tree. (Mitchell and Lapata, 2010) and (Mitchell and Lapata, 2008) defined the composition function of a sentence by algebraic operations over word meaning vectors to obtain sentence meaning vectors. (Guevara, 2010) and (Malakasiotis, 2011) formulated their composition function using a set of specific syntactic relations or specific word categories (Baroni and Zamparelli, 2010). (Socher et al., 2011) proposed a structured recursive neural network based on the convolutional operation, while (Kalchbrenner et al., 2014) proposed a convolution neural network (CNN) with dynamic k-max pooling, considering max pooling as function of input length. For sarcasm detect"
W16-0425,N10-1120,0,0.0096829,"Finally, section 9 concludes with a short discussion of future work. 2 Related work Semantic modelling of sentence meaning is a wellresearched topic in NLP. Due to ’bad language’ in Twitter and a noticeable drop of accuracy for startof-the-art constituency parsers on tweets, the semantic modelling of tweets has captured the attention of researchers. To build a semantic representation of a sentence in various NLP tasks such as sentiment analysis, researchers have used syntac1 tic structure to compose a total representation as a function of the word-vector representation of a sentence’s parts. (Nakagawa et al., 2010) describes a Tree-CRF classifier which uses a data-driven dependency parser, maltparser2 , to obtain a parse tree for a sentence, and whose composition function uses the head-modifier relations of the parse tree. (Mitchell and Lapata, 2010) and (Mitchell and Lapata, 2008) defined the composition function of a sentence by algebraic operations over word meaning vectors to obtain sentence meaning vectors. (Guevara, 2010) and (Malakasiotis, 2011) formulated their composition function using a set of specific syntactic relations or specific word categories (Baroni and Zamparelli, 2010). (Socher et a"
W16-0425,D13-1066,0,0.862412,"Missing"
W16-0425,D11-1014,0,0.00876189,"al., 2010) describes a Tree-CRF classifier which uses a data-driven dependency parser, maltparser2 , to obtain a parse tree for a sentence, and whose composition function uses the head-modifier relations of the parse tree. (Mitchell and Lapata, 2010) and (Mitchell and Lapata, 2008) defined the composition function of a sentence by algebraic operations over word meaning vectors to obtain sentence meaning vectors. (Guevara, 2010) and (Malakasiotis, 2011) formulated their composition function using a set of specific syntactic relations or specific word categories (Baroni and Zamparelli, 2010). (Socher et al., 2011) proposed a structured recursive neural network based on the convolutional operation, while (Kalchbrenner et al., 2014) proposed a convolution neural network (CNN) with dynamic k-max pooling, considering max pooling as function of input length. For sarcasm detection, due to the complexity of the task and the somewhat poorer accuracy of start-of-the-art constituency parsers on tweets, researchers have considered surface level lexical and syntactic cues as legitimate features. Kreuz and Caucci (Kreuz and Caucci, 2007) explored the role of lexical indicators, such as interjections (e.g., “gee” or"
W16-1105,J91-1003,0,0.586631,"ntational form, for it is this overlap that allows an NLP system to find its way from one to the other. Many systems assume that this overlap is taxonomic in nature: simply, that the concepts used in the surface meaning share a general categorization with their figurative counterparts. Thus, gasoline and beer are both liquids, to drink and to run on are both modes of consumption, while job and jail can each denote a confining, oppressive situation. The taxonomic approach actually dates all the way back to Aristotle, but is exemplified by AI models such as those of Wilks (1978), Way (1991) and Fass (1991) and by the psychological theories of Glucksberg (1998). In approaches based on analogy, this shared representation comes in the form of isomorphic aspects of the source and target that are built from identical semantic primitives. Winston (1980), for example, as well as Carbonell (1981), Gentner (1983), Gentner et al. (1989), Holyoak & Thagard (1989) and Veale & Keane (1997) all propose AI models that identify a common core of structure shared by the source (what Davidson calls the surface meaning) and the target (a meaning structure of practical AI merit whose existence Davidson denies on ph"
W16-1105,P10-1071,0,0.0161523,"imitives. Winston (1980), for example, as well as Carbonell (1981), Gentner (1983), Gentner et al. (1989), Holyoak & Thagard (1989) and Veale & Keane (1997) all propose AI models that identify a common core of structure shared by the source (what Davidson calls the surface meaning) and the target (a meaning structure of practical AI merit whose existence Davidson denies on philosophical grounds). Other symbolic approaches employ a liberal mix of the taxonomic and the analogical, from that of Hobbs (1981) to Martin (1990) to Veale (2015). Statistical approaches such as that of Mason (2004) and Shutova (2010a,b), which aim to find metaphors or to find paraphrases for metaphors that use more conventional (if not always literal) terms, avoid the thorny issue of deep meaning by remaining resolutely at the surface and by not building an explicit model of meaning. Nonetheless, such approaches tacitly assume the intended meaning must share a strong overlap with the surface meaning of its paraphrase. In the following sections we aim to craft an explicit representation for this shared meaning. 3 Persons of Interest: Good, Bad and Ugly The Aristotle system of Veale & Hao (2007) is an online generator of m"
W16-1105,C10-1113,0,0.025178,"imitives. Winston (1980), for example, as well as Carbonell (1981), Gentner (1983), Gentner et al. (1989), Holyoak & Thagard (1989) and Veale & Keane (1997) all propose AI models that identify a common core of structure shared by the source (what Davidson calls the surface meaning) and the target (a meaning structure of practical AI merit whose existence Davidson denies on philosophical grounds). Other symbolic approaches employ a liberal mix of the taxonomic and the analogical, from that of Hobbs (1981) to Martin (1990) to Veale (2015). Statistical approaches such as that of Mason (2004) and Shutova (2010a,b), which aim to find metaphors or to find paraphrases for metaphors that use more conventional (if not always literal) terms, avoid the thorny issue of deep meaning by remaining resolutely at the surface and by not building an explicit model of meaning. Nonetheless, such approaches tacitly assume the intended meaning must share a strong overlap with the surface meaning of its paraphrase. In the following sections we aim to craft an explicit representation for this shared meaning. 3 Persons of Interest: Good, Bad and Ugly The Aristotle system of Veale & Hao (2007) is an online generator of m"
W94-0333,1991.mtsummit-papers.9,0,0.266135,"Missing"
