2020.coling-main.69,D17-1209,0,0.0276336,"results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Tai et al. (2015) first attempts to use GNN in the sentiment classification task. Recently, Zhao et al. (2019) proposed to model sentiment dependency within one sentence by building graphs between them, and Sun et al. (2019) introduced dependency parsing and implemented simple graph convolution to propagate their graph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of s"
2020.coling-main.69,D17-1047,0,0.0171665,"60 0.8023 0.8079 0.7780 0.8230 0.8357 0.8312 0.6583 0.7080 0.7084 0.7020 0.7402 0.7647 0.7376 0.6813 0.6870 0.7033 0.7210 0.7449 0.7654 0.7610 0.7719 0.8135 0.7993 0.6409 0.7135 0.7175 0.7250 0.7299 0.7834 0.7631 Our Model 0.7540 0.7417 0.8508 0.7794 0.8037 0.7694 Table 2: Main results. The results of baseline models are from published papers. “-” means not reported. • IAN: an RNN-based approach proposed by (Ma et al., 2017), which encodes contexts and aspects words by LSTM and interacts them with attention to generate the representations for aspects and contexts concerning each other. • RAM: Chen et al. (2017) proposed a method based on Memory Network and represents memory with LSTM. Then a gated recurrent unit network is applied to concatenate all the outputs for sentence representation from attention. • TNet: Li et al. (2018) uses BiLSTM embeddings as target-specific embeddings, and utilizes a CNN model to extract final embeddings. • HSCN: Li et al. (2019) selects target words and extracts target-specific contextual representation to measure the deviation between target-specific contextual representation and target representations by capturing interactions between the context and target. • CDT: a"
2020.coling-main.69,P14-2009,0,0.406341,"uct in a customer review into the sentiment polarities of positive, neutral, or negative according to the contexts (Hu and Liu, 2004) . Taking the sentence “The performance of this laptop is excellent, but the screen is terrible” as example, the correct ABSA results would be to judge the aspect word performance as positive, and screen as negative. The early approaches of aspect-level sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work using various methods like BERT language model and graph neural networks recently. Since BERT language model (Devlin et al."
2020.coling-main.69,C18-1096,0,0.0212529,"evel sentiment is widely used in scenarios like e-commerce and social network (Zhang, 2008). Researchers usually focus on the fusion of context and aspects to obtain corresponding results. Traditional methods utilize handcrafted features like sentiment lexical features and bag-of-words features to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sent"
2020.coling-main.69,D19-1549,0,0.094791,"atures to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeli"
2020.coling-main.69,S14-2076,0,0.022071,"processing task, has attracted lots of attentions recently. Its typical application is to classify the target aspects of a product in a customer review into the sentiment polarities of positive, neutral, or negative according to the contexts (Hu and Liu, 2004) . Taking the sentence “The performance of this laptop is excellent, but the screen is terrible” as example, the correct ABSA results would be to judge the aspect word performance as positive, and screen as negative. The early approaches of aspect-level sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work usin"
2020.coling-main.69,P18-1087,0,0.0252476,".7540 0.7417 0.8508 0.7794 0.8037 0.7694 Table 2: Main results. The results of baseline models are from published papers. “-” means not reported. • IAN: an RNN-based approach proposed by (Ma et al., 2017), which encodes contexts and aspects words by LSTM and interacts them with attention to generate the representations for aspects and contexts concerning each other. • RAM: Chen et al. (2017) proposed a method based on Memory Network and represents memory with LSTM. Then a gated recurrent unit network is applied to concatenate all the outputs for sentence representation from attention. • TNet: Li et al. (2018) uses BiLSTM embeddings as target-specific embeddings, and utilizes a CNN model to extract final embeddings. • HSCN: Li et al. (2019) selects target words and extracts target-specific contextual representation to measure the deviation between target-specific contextual representation and target representations by capturing interactions between the context and target. • CDT: a method based on graph neural network proposed by (Sun et al., 2019). CDT builds graph by dependency parsing, which is similar but not the same to us and propagates graphs by graph convolution network. • SDGCN: also a GNN-"
2020.coling-main.69,S14-2004,0,0.0872265,"he previous outputs by max pooling, concatenate them, and use a fully connected layer to project the concatenated vector into the space of the targeted C classes. Pc = MaxPooling(Nc0 ) (17) MaxPooling(Na0 ) (18) Pa = logits = Wo [Pc ; Pa ] + bo (19) y = Softmax(logits) (20) where ; represents concatenation operation, Wo ∈ Rd×d , bo ∈ Rd . 4 Experiments In this section, we describe our experimental setup and report our experimental results. 4.1 Experimental Setup For experiments, we utilize three datasets, including SemEval 2014 Task 4 dataset composed of Restaurant reviews and Laptop reviews (Manandhar, 2014) and ACL 14 Twitter gathered by Dong et al. (2014). All the cases in these datasets are labeled with three sentiment polarities positive, negative and neutral. The details of these datasets are shown in Table 1. We fine-tune pre-trained BERT1 in our model. Embedding dim d is set to 768. We utilize Adam (Kingma and Ba, 2014) as optimizer with initial learning rate 2 × 10−5 . Dropout with a keep probability of 0.9 is applied after the dense layer. The batch size of our model is 32, and the max sequence length is set to 128. The number of training epochs is 100. We use Spacy (Honnibal and Montani"
2020.coling-main.69,D14-1162,0,0.0898333,"ust parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of syntax information, but they only implement simple graph convolution on their syntax graph, which can not tell the differences between nodes, and they simply using mean pooling to get final results after GCN with deeper integration between aspect and context. 2.3 BERT BERT is one of the key innovations in the recent progress of contextualized representation learning (Peters et al., 2018; Devlin et al., 2018). Traditional word embeddings like Glove (Mikolov et al., 2013; Pennington et al., 2014) are trained among large scale of corpora, while all these methods finally get the superposition of different meanings of words. BERT adopts a fine-tuning mechanism that almost needs no specific requirement architecture for each end task. Recently, some BERT-based models (Zhao et al., 2019; Song et al., 2019) have been adopted in sentiment analysis task. But almost all of these models utilize BERT as an embedding layer, which is better than Glove. 3 Methodology In this section, we will introduce the details of all the layers in our model separately. Figure 2 shows the overall architecture of t"
2020.coling-main.69,N18-1202,0,0.0402638,"raph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of syntax information, but they only implement simple graph convolution on their syntax graph, which can not tell the differences between nodes, and they simply using mean pooling to get final results after GCN with deeper integration between aspect and context. 2.3 BERT BERT is one of the key innovations in the recent progress of contextualized representation learning (Peters et al., 2018; Devlin et al., 2018). Traditional word embeddings like Glove (Mikolov et al., 2013; Pennington et al., 2014) are trained among large scale of corpora, while all these methods finally get the superposition of different meanings of words. BERT adopts a fine-tuning mechanism that almost needs no specific requirement architecture for each end task. Recently, some BERT-based models (Zhao et al., 2019; Song et al., 2019) have been adopted in sentiment analysis task. But almost all of these models utilize BERT as an embedding layer, which is better than Glove. 3 Methodology In this section, we will"
2020.coling-main.69,E09-1077,0,0.051317,"2014 datasets and experiments show that SAGAT achieves outstanding performance. 2 Related Work In this section, we will briefly review works on aspect-level sentiment analysis, graph neural network, and BERT language model. 2.1 Aspect-Level Sentiment Analysis Aspect-level sentiment is widely used in scenarios like e-commerce and social network (Zhang, 2008). Researchers usually focus on the fusion of context and aspects to obtain corresponding results. Traditional methods utilize handcrafted features like sentiment lexical features and bag-of-words features to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduc"
2020.coling-main.69,D19-1569,0,0.329609,"evel sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work using various methods like BERT language model and graph neural networks recently. Since BERT language model (Devlin et al., 2018) based on Transformer can alleviate long-distance dependence of RNN, some studies have designed BERT-based models for the ABSA task and achieved outstanding results (Song et al., 2019; Rietzler et al., 2019; Zhao et al., 2019). To the best of our knowledge, most of these work just utilize BERT simply as an embedding layer, which sequentially models the position embedddings"
2020.coling-main.69,P15-1150,0,0.0556329,"between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Tai et al. (2015) first attempts to use GNN in the sentiment classification task. Recently, Zhao et al. (2019) proposed to model sentiment dependency within one sentence by building graphs between them, and Sun et al. (2019) introduced dependency parsing and implemented simple graph convolution to propagate their graph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax information is introduced. Sun et al. (2019) makes use of syntax information, but they only implement simple graph convolution on"
2020.coling-main.69,D15-1167,0,0.214089,"eview into the sentiment polarities of positive, neutral, or negative according to the contexts (Hu and Liu, 2004) . Taking the sentence “The performance of this laptop is excellent, but the screen is terrible” as example, the correct ABSA results would be to judge the aspect word performance as positive, and screen as negative. The early approaches of aspect-level sentiment analysis mainly use handcrafted features to train a statistical classifier (Kiritchenko et al., 2014). With the rapid progress of deep neural network (DNN), lots of work based on DNN have been proposed (Dong et al., 2014; Tang et al., 2015b; Ma et al., 2017). These studies mostly depend on recurrent neural networks (RNNs) (Bahdanau et al., 2014) to model the semantic relationship between the aspect words and their contexts. However, their performances are affected by the RNN’s limited ability of capturing long-distance dependencies (Werbos, 1990) and loss of syntax information (Sun et al., 2019). To overcome the shortages of RNN-based models mentioned above, there have been a lot of work using various methods like BERT language model and graph neural networks recently. Since BERT language model (Devlin et al., 2018) based on Tr"
2020.coling-main.69,D16-1021,0,0.0420437,"optimized in our model is the cross-entropy loss, which can be defined as: L=− C X yˆi log(y i ) (21) i=1 4.2 Baselines We compare our model with the following baseline models: • TD-LSTM: Tang et al. (2015a) utilizes two dependent LSTM network to model the left context with aspect and the right context, respectively. The left and right representations are concatenated for predicting the sentiment polarity. • ATAE-LSTM: Wang et al. (2016) appends the aspect words embeddings with each context word embeddings. They use LSTM and attention to get the final representation for prediction. • MemNet: Tang et al. (2016) utilizes multi-hops attention on the context word for sentence representation to tell the importance of each context word. 1 We use uncased BERT-base from https://github.com/google-research/bert 804 Models Twitter Restaurant Laptop Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 TD-LSTM ATAE-LSTM MemNet IAN RAM TNet HSCN CDT SDGCN-BERT AEN-BERT 0.7080 0.6850 0.6936 0.7312 0.6960 0.7466 0.7471 0.6900 0.6691 0.6730 0.7101 0.6610 0.7366 0.7313 0.7563 0.7720 0.7816 0.7860 0.8023 0.8079 0.7780 0.8230 0.8357 0.8312 0.6583 0.7080 0.7084 0.7020 0.7402 0.7647 0.7376 0.6813 0.6870 0.7033 0.7210 0"
2020.coling-main.69,D16-1058,0,0.154166,"ct-Level Sentiment Analysis Aspect-level sentiment is widely used in scenarios like e-commerce and social network (Zhang, 2008). Researchers usually focus on the fusion of context and aspects to obtain corresponding results. Traditional methods utilize handcrafted features like sentiment lexical features and bag-of-words features to 800 train sentiment classifiers (Rao and Ravichandran, 2009). With the development of Deep Neural Network, more DNN-based models have been proposed. There are mainly two categories models including semantic-based and syntactic-based methods. Semantic-based models (Wang et al., 2016; Ma et al., 2017; He et al., 2018; Song et al., 2019) usually use the attention mechanism to capture and amplify the key semantic information of sentences and aspects. However, most of the previous works neglect the power of syntax information. Syntactic-based methods (Sun et al., 2019; Zhao et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods"
2020.coling-main.69,P18-1030,0,0.0130283,"ang et al., 2019; Lin et al., 2019) introduce the results of dependency parsing into the DNN models to shorten the distance between the aspect and the keyword and introduce syntactic information. DNN-based models including semantic-based and syntactic-based methods can generate dense vectors of sentences without handcrafted features. 2.2 Graph Neural Network Graph neural networks have recently become very popular in NLP research. GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Tai et al. (2015) first attempts to use GNN in the sentiment classification task. Recently, Zhao et al. (2019) proposed to model sentiment dependency within one sentence by building graphs between them, and Sun et al. (2019) introduced dependency parsing and implemented simple graph convolution to propagate their graph. They all achieve outstanding performance by applying GNN in their model. While Zhao et al. (2019) must parse out all the aspects first in the sentence, and no syntax informa"
2021.acl-long.462,D19-1435,0,0.0121075,"ang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on many languagedependent edit operations such as the conversion of singular nouns to plurals, it is difficult for them to adapt to other languages. LaserTagger (Malmi et al., 2019) uses the similar method but it is datadriven and language-independent by learning operations from training data. However, its performance is not so desirable as its seq2seq counterpart despite its high efficiency. The only two previous effi"
2021.acl-long.462,W19-4406,0,0.0119897,"ational cost for decoding, we propose to use a shallow decoder, which has proven to be an effective strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official e"
2021.acl-long.462,2020.emnlp-main.581,1,0.917671,"nly does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL14 and 72.9 F0.5 in the BEA-19 test set with an almost 10× online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/ Shallow-Aggressive-Decoding. 1 Introduction The Transformer (Vaswani et al., 2017) has become the most popular model for Grammatical Error Correction (GEC). In practice, however, the sequenceto-sequence (seq2seq) approach has been blamed recently (Chen et al., 2020; Stahlberg and Kumar, ∗ This work was done during the author’s internship at MSR Asia. Contact person: Tao Ge (tage@microsoft.com) † Co-first authors with equal contributions 2020; Omelianchuk et al., 2020) for its poor inference efficiency in modern writing assistance applications (e.g., Microsoft Office Word1 , Google Docs2 and Grammarly3 ) where a GEC model usually performs online inference, instead of batch inference, for proactively and incrementally checking a user’s latest completed sentence to offer instantaneous feedback. To better exploit the Transformer for instantaneous GEC in pra"
2021.acl-long.462,N12-1067,0,0.0292118,"ricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages. To compare with the state-of-the-art approaches in English GEC that pretrain with synthetic data, 4 https://github.com/nusnlp/m2scorer Following Chen et al. (2020), we sample 5,000 training instances as the validation set. 5940"
2021.acl-long.462,W13-1703,0,0.0227589,"ctive strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sent"
2021.acl-long.462,P18-1097,1,0.803731,"her languages (e.g., Chinese). As shown in Table 6, our approach consistently performs well in Chinese GEC, showing an around 12.0× online inference speedup over the Transformer-big baseline with comparable performance. 5 Related Work The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on ed"
2021.acl-long.462,D19-1633,0,0.0193807,"g and Kumar (2020) which uses span-based edit operations to correct sentences to save the time for copying unchanged tokens, and Chen et al. (2020) which first identifies incorrect spans with a tagging model then only corrects these spans with a generator. However, all the approaches have to extract edit operations and even conduct token alignment in advance from the error-corrected sentence pairs for training the model. In contrast, our proposed shallow aggressive decoding tries to accelerate the model inference through parallel autoregressive decoding which is related to some previous work (Ghazvininejad et al., 2019; Stern et al., 2018) in neural machine translation (NMT), and the imbalanced encoder-decoder architecture which 5944 is recently explored by Kasai et al. (2020) and Li et al. (2021) for NMT. Not only is our approach language-independent, efficient and guarantees that its predictions are exactly the same with greedy decoding, but also does not need to change the way of training, making it much easier to train without so complicated data preparation as in the edit operation based approaches. 6 Conclusion and Future Work In this paper, we propose Shallow Aggressive Decoding (SAD) to accelerate o"
2021.acl-long.462,W19-4427,0,0.0784347,"eam search (beam=5), and generates exactly the same predictions as greedy decoding, as discussed in Section 3.1.2. Since greedy decoding can achieve comparable overall performance (i.e., F0.5 ) with beam search while it tends 6 Our implementation of greedy decoding is simplified for higher efficiency (1.3× ∼ 1.4× speedup over beam=5) than the implementation of beam=1 decoding in fairseq (around 1.1× speedup over beam=5). 7 https://github.com/pytorch/fairseq 35 30 25 Speedup we also synthesize 300M error-corrected sentence pairs for pretraining the English GEC model following the approaches of Grundkiewicz et al. (2019) and Zhang et al. (2019). Note that in the following evaluation sections, the models evaluated are by default trained without the synthetic data unless they are explicitly mentioned. We use the most popular GEC model architecture – Transformer (big) model (Vaswani et al., 2017) as our baseline model which has a 6-layer encoder and 6-layer decoder with 1,024 hidden units. We train the English GEC model using an encoder-decoder shared vocabulary of 32K Byte Pair Encoding (Sennrich et al., 2016) tokens and train the Chinese GEC model with 8.4K Chinese characters. We include more training details"
2021.acl-long.462,2021.findings-acl.11,0,0.083788,"Missing"
2021.acl-long.462,2020.acl-main.391,0,0.0158019,"emendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on many languagedependent edit operations such as the conversion of singular nouns t"
2021.acl-long.462,2020.acl-main.703,0,0.0829372,"Missing"
2021.acl-long.462,N19-1333,0,0.0138028,"with comparable performance. 5 Related Work The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operatio"
2021.acl-long.462,2021.ccl-1.108,0,0.061983,"Missing"
2021.acl-long.462,D19-1510,0,0.0157882,"the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on many languagedependent edit operations such as the conversion of singular nouns to plurals, it is difficult for them to adapt to other languages. LaserTagger (Malmi et al., 2019) uses the similar method but it is datadriven and language-independent by learning operations from training data. However, its performance is not so desirable as its seq2seq counterpart despite its high efficiency. The only two previous efficient approaches that are both languageindependent and good-performing are Stahlberg and Kumar (2020) which uses span-based edit operations to correct sentences to save the time for copying unchanged tokens, and Chen et al. (2020) which first identifies incorrect spans with a tagging model then only corrects these spans with a generator. However, all the ap"
2021.acl-long.462,W13-3601,0,0.0149261,"l., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages. To compare with the state-of-the-art approaches in English GEC that pretrain with synthetic data, 4 https://github.com/nusnlp/m2scorer Following Chen et al. (2020), we sample 5,000 training instances as the validation set. 5940 5 Model Transformer-big (beam=5) Transformer-big (greedy) Transformer-big (aggressive) Transformer-big (beam=5) Transform"
2021.acl-long.462,2020.bea-1.16,0,0.0686156,"Missing"
2021.acl-long.462,P16-1162,0,0.0282237,"M error-corrected sentence pairs for pretraining the English GEC model following the approaches of Grundkiewicz et al. (2019) and Zhang et al. (2019). Note that in the following evaluation sections, the models evaluated are by default trained without the synthetic data unless they are explicitly mentioned. We use the most popular GEC model architecture – Transformer (big) model (Vaswani et al., 2017) as our baseline model which has a 6-layer encoder and 6-layer decoder with 1,024 hidden units. We train the English GEC model using an encoder-decoder shared vocabulary of 32K Byte Pair Encoding (Sennrich et al., 2016) tokens and train the Chinese GEC model with 8.4K Chinese characters. We include more training details in the supplementary notes. For inference, we use greedy decoding6 by default. All the efficiency evaluations are conducted in the online inference setting (i.e., batch size=1) as we focus on instantaneous GEC. We perform model inference with fairseq7 implementation using Pytorch 1.5.1 with 1 Nvidia Tesla V100-PCIe of 16GB GPU memory under CUDA 10.2. 20 15 10 5 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 Edit Ratio Figure 2: The speedup (over greedy decoding) distribution of all the 1"
2021.acl-long.462,2020.emnlp-main.418,0,0.0927036,"F0.5 Speedup 17.2 29.6 1.0× 15.0 22.0 3.1× 10.5 19.9 38.0× 14.5 28.4 2.7× 20.5 29.4 12.0× Table 6: The performance and online inference efficiency evaluation for the language-independent efficient GEC models in the NLPCC-18 Chinese GEC benchmark. els (e.g., RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019)) with its multi-stage training strategy. Following GECToR’s recipe, we leverage the pretrained model BART (Lewis et al., 2019) to initialize a 12+2 model which proves to work well in NMT (Li et al., 2021) despite more parameters, and apply the multi-stage fine-tuning strategy used in Stahlberg and Kumar (2020). The final single model11 with aggressive decoding achieves the state-of-the-art result – 66.4 F0.5 in the CoNLL-14 test set with a 9.6× speedup over the Transformerbig baseline. Unlike GECToR and PIE that are difficult to adapt to other languages despite their competitive speed because they are specially designed for English GEC with many manually designed languagespecific operations like the transformation of verb forms (e.g., VBD→VBZ) and prepositions (e.g., in→at), our approach is data-driven without depending on language-specific features, and thus can be easily adapted to other language"
2021.acl-long.462,2020.coling-main.200,0,0.0168833,"The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on ma"
2021.acl-long.462,I11-1017,0,0.0389356,"which has proven to be an effective strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al."
2021.acl-long.462,W14-1701,0,0.0175463,"ncy. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages. To compare with the state-of-the-art approaches in English GEC that pretrain with synthetic data,"
2021.acl-long.462,P11-1019,0,0.0203876,"2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We al"
2021.acl-long.462,2020.findings-emnlp.30,1,0.708768,"nce. 5 Related Work The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However,"
2021.findings-acl.430,K18-1048,0,0.0146031,"is response selection, which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficiency. The other drawback is that options in th"
2021.findings-acl.430,2020.acl-main.130,0,0.12872,"ng Model ×N + dialogue option 1 (a) Independent Matching score 1 … score N 2.2 Matching Model … + dialogue option 1 option N (b) Joint Matching Figure 2: Overview of Independent Matching and Joint Matching. urally enables a simple yet effective data augmentation method. The basic idea is that since options are sequentially concatenated in JM, new training instances can be easily created by changing the permutation order of options. Therefore, a dialogue with M response options can create at most M ! (factorial M ) times as many training instances. We conduct experiments on the MuTual dataset (Cui et al., 2020), a publicly available English dataset for multi-turn dialogue response selection. Results show that JM advances IM on three matching models and can significantly reduce training time. Besides, the permutation-based data augmentation method gives further improvement. 2 Model The overview of IM and JM is shown in Figure 2. We describe the details in the following subsections1 . 2.1 Instead of conducting N times of independent matches, we make the first step outside the IM framework and explore a joint matching approach for this task. We first adds a special token [OP]j at the start of the j th"
2021.findings-acl.430,N19-1423,0,0.01245,"scores. Note that JM can complete a prediction with a single match, which means it only requires one gradient computation and context encoding. Besides, thanks to the self-attention mechanism (Vaswani et al., 2017) of BERT-based matching models, options can now directly attend to each other, rather than being agnostic. Another advantage of JM approach is that it nat4872 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4872–4877 August 1–6, 2021. ©2021 Association for Computational Linguistics on this task. Similar to using BERT for sentencepair classification (Devlin et al., 2019), they first concatenate the context (sentence A) and a candidate response (sentence B) as BERT input (i.e., “[CLS] Excuse me ... [SEP] Sorry ... [SEP]”). On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. In order to compete a prediction, M independent matchings have to be made, where M is the number of options. score 1 Matching Model ×N + dialogue option 1 (a) Independent Matching score 1 … score N 2.2 Matching Model … + dialogue option 1 option N (b) Joint Matching Figure 2: Overview of Independent Matching and Joint Ma"
2021.findings-acl.430,W15-4640,0,0.0559652,"Missing"
2021.findings-acl.430,P19-1001,0,0.0126482,"which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficiency. The other drawback is that options in these models are ind"
2021.findings-acl.430,P19-1363,0,0.015173,"anging the concatenation order of the options. For example, from [OP ]1 O1 [OP ]2 O2 ...[OP ]N ON to [OP ]2 O2 [OP ]1 O1 ...[OP ]N ON , we create a new training example (see Figure 2). Correspondingly, the ground-truth label of the training instance may be changed. In this way, a single dialogue can create at most M ! times training instances. 3 3.1 Experiments Dataset We evaluate our model on the Mutual dataset (Cui et al., 2020), a human-labeled, open-domain and reasoning-based dataset for multi-turn response selection. Compared with previous datasets (Lowe et al., 2015; Zhang et al., 2018; Welleck et al., 2019), MuTual is more challenging since it requires some reasoning ability. Models that achieve closeto-human performance on previous datasets, still perform far behind human performance on MuTual. The statistics of MuTual are shown in Table 1. Note that since Mutual has 4 options for each dialogue, PBDA can thus creates at most 24 (4!) times as many training instances. 3.2 Settings We use PyTorch to implement JM on three matching models. We adopt AdamW (Loshchilov and Training set Validation set Test set # Avg. Turns / Dialogue # Avg. Words / Utterance # Options MuTual 7088 886 886 4.73 19.57 4 Ta"
2021.findings-acl.430,P17-1046,0,0.0201444,"e important task in dialogue systems is response selection, which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficienc"
2021.findings-acl.430,D19-1011,0,0.0326048,"Missing"
2021.findings-acl.430,P18-1205,0,0.0131553,"stances by simply changing the concatenation order of the options. For example, from [OP ]1 O1 [OP ]2 O2 ...[OP ]N ON to [OP ]2 O2 [OP ]1 O1 ...[OP ]N ON , we create a new training example (see Figure 2). Correspondingly, the ground-truth label of the training instance may be changed. In this way, a single dialogue can create at most M ! times training instances. 3 3.1 Experiments Dataset We evaluate our model on the Mutual dataset (Cui et al., 2020), a human-labeled, open-domain and reasoning-based dataset for multi-turn response selection. Compared with previous datasets (Lowe et al., 2015; Zhang et al., 2018; Welleck et al., 2019), MuTual is more challenging since it requires some reasoning ability. Models that achieve closeto-human performance on previous datasets, still perform far behind human performance on MuTual. The statistics of MuTual are shown in Table 1. Note that since Mutual has 4 options for each dialogue, PBDA can thus creates at most 24 (4!) times as many training instances. 3.2 Settings We use PyTorch to implement JM on three matching models. We adopt AdamW (Loshchilov and Training set Validation set Test set # Avg. Turns / Dialogue # Avg. Words / Utterance # Options MuTual 7088"
2021.findings-acl.430,D16-1036,0,0.0290692,"Missing"
2021.findings-acl.430,P18-1103,0,0.0166709,"in dialogue systems is response selection, which plays an essential role in retrievalbased chatbots (Ji et al., 2014). It aims to select the best-matched response from a set of response options for a dialogue. As shown in Figure 1, given a dialogue context and four response options, we need to choose the only logically correct one. Previous work in response selection follows an independent matching (IM) approach and computes a matching score for each of the N response options independently. Various matching models following this approach have been proposed (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Chaudhuri et al., 2018; Tao et al., 2019; Yuan et al., 2019). Despite its success in pre-BERT era, we argue that the IM approach does not make full use of the ability of pretrained encoders (such as BERT and RoBERTa) to encode multiple sentences, hence may hinder both efficiency and effectiveness. Specifically, to complete a prediction, the IM approach has to perform N independent matches, which means N gradient computations (where N is the number of response options). Besides, the dialogue context is repeatedly encoded N times, which further contributes to the inefficiency. The other drawba"
C10-1136,W09-3001,0,0.0259072,"motion analysis, words which are expression of emotions such as 哭泣 (cry) or evaluation such as 胆小 (cowardice) were included. Selecting nouns and verbs, Xu and Tao (2003) offered an emotion taxonomy of 390 emotion words. The taxonomy contains 24 classes of emotions and excludes Chinese idioms. By our inspection to the offered emotion words in this taxonomy, the authors tried to exclude expression of emotions, evaluation and cause of emotions from emotions, which is similar with our processing12 . Ours2 is intentionally created to compare with this emotion taxonomy. Based on (Xu and Tao, 2003), Chen et al. (2009) removed the words of single Chinese character; let two persons to judge if a word is an emotional one and only those agreed by the two persons were seen as emotion words. It is worth noting that Chen et al. (2009) merges 怒 (anger) and 烦 (fidget) in (Xu and Tao, 2003) to form the 怒 (anger) lexicon, thus 讨厌 (dislike) appears in anger lexicon. However, we believe 讨厌 (dislike) is different with 怒 (anger), and should be put into another emotion. Also, we distinguish between 恨 (hate) and 怒 (anger). Xu et al. (2008) constructed a large-scale affective lexicon ontology. Given the example words in the"
C10-1136,W06-3808,0,0.168685,"mantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. An approach proposed by (Turney, 2002) for the construction of polarity started with a few positive and negative seeds, then used a similarity method (pointwise mutual information) to grow this seed list from web corpus. Our experiments are similar with these works, but we use a different ranking method and incorporate multiple resources. To perform rating inference on reviews, Goldberg and Zhu (2006) created a graph on both labeled and unlabeled reviews, and then solved an optimization problem to obtain a smooth rating function over the whole graph. Rao and Ravichandran (2009) used three semi-supervised methods in polarity lexicon induction based on WordNet, and compared them with corpus-based methods. Encouraging results show methods using similarity between words can improve the performance. Wan and Xiao (2009) presented a method to use two types of similarity between sentences for document summarization, namely similarity within a document and similarity between documents. The ranking"
C10-1136,E09-1077,0,0.364017,"rds that are associated with the category. An approach proposed by (Turney, 2002) for the construction of polarity started with a few positive and negative seeds, then used a similarity method (pointwise mutual information) to grow this seed list from web corpus. Our experiments are similar with these works, but we use a different ranking method and incorporate multiple resources. To perform rating inference on reviews, Goldberg and Zhu (2006) created a graph on both labeled and unlabeled reviews, and then solved an optimization problem to obtain a smooth rating function over the whole graph. Rao and Ravichandran (2009) used three semi-supervised methods in polarity lexicon induction based on WordNet, and compared them with corpus-based methods. Encouraging results show methods using similarity between words can improve the performance. Wan and Xiao (2009) presented a method to use two types of similarity between sentences for document summarization, namely similarity within a document and similarity between documents. The ranking method in our paper is similar to the ones used in above three papers, which fully exploit the relationship between any pair of sample points (both labeled and unlabeled). When onl"
C10-1136,W97-0313,0,0.0624202,"ws. In Section 2, related works are introduced. In Section 3, we describe a graph-based algorithm and how to incorporate multiple resources. Section 4 gives the details of applying the algorithm on five emotions and shows how to evaluate the results. Section 5 focuses on how to build and evaluate emotion lexicons, linguistic consideration and instruction for identifying emotions are also included. Finally, conclusion is made in Section 6. 1209 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1209–1217, Beijing, August 2010 2 3 Related work 3.1 Riloff and Shepherd (1997) presented a corpusbased method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. An approach proposed by (Turney, 2002) for the construction of polarity started with a few positive and negative seeds, then used a similarity method (pointwise mutual information) to grow this seed list from web corpus. Our experiments are similar with these works, but we use a different ranking method and incorporate"
C10-1136,P02-1053,0,0.00479838,"sideration and instruction for identifying emotions are also included. Finally, conclusion is made in Section 6. 1209 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1209–1217, Beijing, August 2010 2 3 Related work 3.1 Riloff and Shepherd (1997) presented a corpusbased method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. An approach proposed by (Turney, 2002) for the construction of polarity started with a few positive and negative seeds, then used a similarity method (pointwise mutual information) to grow this seed list from web corpus. Our experiments are similar with these works, but we use a different ranking method and incorporate multiple resources. To perform rating inference on reviews, Goldberg and Zhu (2006) created a graph on both labeled and unlabeled reviews, and then solved an optimization problem to obtain a smooth rating function over the whole graph. Rao and Ravichandran (2009) used three semi-supervised methods in polarity lexico"
C12-1070,W02-1001,0,0.064145,"普通的SGD，与ADF 收敛速度接近。 KEYWORDS: Sequence labeling, online learning, stochastic gradient descent, named entity recognition. KEYWORDS IN CHINESE: 序列标注模型，在线学习，随机梯度下降，命名实体识别 ∗ Corresponding author Proceedings of COLING 2012: Technical Papers, pages 1147–1162, COLING 2012, Mumbai, December 2012. 1147 1 Introduction Sequence labeling models have been widely used in a variety of NLP tasks, such as word segmentation, part-of-speech tagging, chunking, and named entity recognition. Various sequence labeling models have been proposed, like hidden Markov models (HMM) (Rabiner, 1989), structured perceptron (Collins, 2002), conditional random fields (CRFs) (Lafferty et al., 2001) and SVM-HMM (Tsochantaridis et al., 2006). In recent years, discriminative models gain significant popularity over generative models on these tasks, and achieve state-of-the-art performance on most above tasks. Their strength and flexibility come from their ability to incorporate arbitrary declarative features. CRFs is one of such discriminative models for sequence labeling built upon maximum entropy principle. The learning algorithms of CRFs can be divided into batch methods and online methods. Batch methods update parameters by estim"
C12-1070,P10-1052,0,0.501316,") − φ(x(t) , y(t) ) + λθ (12) where the expectation of feature vector given current example x(t) is taken over all possible sequences of y, and can be efficiently computed using forward-backward algorithm. Generally regularizer is added to avoid overfitting. Various regularizers have been proposed. L2 regularizer, R(θ ) = λ2 kθ k22 , which is used by various CRF based tools, often leads to superior performance and can be numerically optimized. L1 regularizer, R(θ ) = σkθ k1 , known as Lasso, encourages sparse parameter. Elastic net, a linear interpolation of L1 and L2 regularizer, is used by (Lavergne et al., 2010) to regularize CRFs, and performs as good as L2 regularizer, while still retaining compact model. 2.3.1 SGD and its variants Stochastic gradient descent (SGD) is known for its fast convergence on machine learning tasks (Bottou and Bousquet, 2008) (Vishwanathan et al., 2006) (Shalev-Shwartz et al., 2007). Every time the algorithm randomly draws one sample (or small batch of samples in mini-batch setting), and performs update according to the gradient of this sample. In general, SGD has the following simple update rule: θ (t+1) = θ (t) − η(t) B∇L (13) where η(t) is a scalar learning rate, B is a"
C12-1070,W02-2018,0,0.17624,"Missing"
C12-1070,D11-1139,0,0.0201906,"s is approximating the inverse of Hessian in order to accelerate training (and is why they are called second order SGD). However, the approximation is expensive and much slower than a plain SGD in terms of per iteration running time. (Xu, 2011) proposes averaged SGD (ASGD) that is as fast as SGD and converges within several iterations. However, in several datasets we tested, the testset performance is below standard SGD even after we tried several switch time of SGD and ASGD. Besides the regularizer mentioned above, group Lasso has recently been proposed to regularize a structured classifier (Martins et al., 2011). The author encodes prior structural knowledge of the feature space by grouping different features into M groups and using separate regularizer weight for each group. The resulting model is compact and avoids the problem of overfitting with large number of free parameters. Conclusion We investigate several online learning algorithms for sequence labeling and empirically show how each algorithm performs on datasets with distinct feature design and label set. This will ease the selection of algorithms in similar tasks in future. Our experiments show that most online algorithms outperform batch"
C12-1070,E12-1017,0,0.0195967,"ˆ(t) ) is the penalty we incur if our prediction is y ˆ(t) and the true label is y(t) . y ˆ(t) where l(y(t) , y ˆ(t) ) can be solved with cost augmented decoding, which can be efficiently accomplished if l(y(t) , y decomposes the same way as the feature vector function (Smith, 2011). This is referred to as a max-loss update in (Crammer et al., 2006). When applied to sequence labeling PA is a special case of the general algorithm where output y is a label sequence. Hamming loss (Eq. 6) is often used. However, other loss can also fit when one faces with task specific needs (Song et al., 2012) (Mohit et al., 2012). l(ˆ y, y) = H amming(ˆ y, y) = 2.2 n X i=1 δ yi 6= ˆyi ( ˆyi , yi ) (6) Dual coordinate ascent (DCA) (Martins et al., 2010) present a general framework for online learning of structured classifiers. It bears some resemblance to the PA algorithm in that it shares the passive-aggressive property of PA. This algorithm applies to a wide class of loss functions; CRFs, SVM, structured perceptron can all be deemed as its special cases. Furthermore, learning rate is automatically determined for each instance, hence pesky learning rate tuning is no longer needed. The learning objective is the sum of"
C12-1070,N10-1123,0,0.0181753,"batch learning methods very slow and impractical for large scale datasets. Several online learning algorithms have been proposed to speed the training process of structured prediction problems, such as Passive-Aggressive (PA) algorithm (Crammer et al., 2006), Dual Coordinate Ascent (DCA) (Martins et al., 2010) and Stochastic Gradient Descent (SGD). SGD is known for its performance in the back propagation training of neural network. It also shows extremely good performance on machine learning tasks such as SVM (Bordes et al., 2009), CRFs (Vishwanathan et al., 2006), and Markov Logic Networks (Poon and Vanderwende, 2010). It may reach optimal performance even before it sees the whole training data on large datasets (Bottou and Bousquet, 2008). SGD takes typically 5-10 iterations to converge when training a multiclass Maximum Entropy (ME) model, while it takes over 50 iterations when training CRFs, much slower than training its unstructured counterpart, multiclass ME. We show how simple feature frequency adaptive strategy may help accelerate training of CRFs within 10 iterations. Despite recent progresses in online learning algorithms, little effort has been made to compare these algorithms thoroughly. In this"
C12-1070,W09-1119,0,0.157105,"teaching assistant) . . . 讲课 授课 讲授 讲解 教授(teach) 教书 . . . 安全部(ministry of security) 财政部(ministry of finance) 参谋部 电力部 . . . Table 3: A Snippet from Tongyici Cinlin (Extended) one word belongs to. The category codes with prefix of different lengths give different levels of abstraction of its semantic meaning. Some of the clusters are good indicators of named entities. For instance, the first row is a good indicator of previous word being a location, and the first and second row is a good indicator of next word being a person. This categorized lexicon is quite similar to word cluster features in (Ratinov and Roth, 2009), but is more precise. However, to our certain knowledge, this resource remains unexplored in previous Chinese NER tasks. Words in Chinese do not have space like in English. In Chinese NER task, texts are given without word boundaries, so segmentation is an essential preprocessing step. But this will bring segmentation error to the system, especially most named entities are out-ofvocabulary words. On the other hand, if we perform inference at character level directly, we quickly loss the meaning of words . We propose two simple strategies to alleviate these problems: first, while still perform"
C12-1070,N03-1028,0,0.0894144,"uch discriminative models for sequence labeling built upon maximum entropy principle. The learning algorithms of CRFs can be divided into batch methods and online methods. Batch methods update parameters by estimating gradient over the entire training data, while online methods estimate noisy gradient with a small portion of the training data, and update parameters frequently. Among all the batch methods, L-BFGS is the most widely used and outperforms others by a substantial margin (Malouf et al., 2002); Conjugate-gradient (CG) method with proper preconditioner can converge as fast as L-BFGS (Sha and Pereira, 2003). However, discriminative models for typical sequence labeling tasks are very large and may involve hundreds of thousands of features, rendering even fastest batch learning methods very slow and impractical for large scale datasets. Several online learning algorithms have been proposed to speed the training process of structured prediction problems, such as Passive-Aggressive (PA) algorithm (Crammer et al., 2006), Dual Coordinate Ascent (DCA) (Martins et al., 2010) and Stochastic Gradient Descent (SGD). SGD is known for its performance in the back propagation training of neural network. It als"
C12-1070,P12-1108,0,0.0197107,", y) and y y (4) (5) ˆ(t) ) is the penalty we incur if our prediction is y ˆ(t) and the true label is y(t) . y ˆ(t) where l(y(t) , y ˆ(t) ) can be solved with cost augmented decoding, which can be efficiently accomplished if l(y(t) , y decomposes the same way as the feature vector function (Smith, 2011). This is referred to as a max-loss update in (Crammer et al., 2006). When applied to sequence labeling PA is a special case of the general algorithm where output y is a label sequence. Hamming loss (Eq. 6) is often used. However, other loss can also fit when one faces with task specific needs (Song et al., 2012) (Mohit et al., 2012). l(ˆ y, y) = H amming(ˆ y, y) = 2.2 n X i=1 δ yi 6= ˆyi ( ˆyi , yi ) (6) Dual coordinate ascent (DCA) (Martins et al., 2010) present a general framework for online learning of structured classifiers. It bears some resemblance to the PA algorithm in that it shares the passive-aggressive property of PA. This algorithm applies to a wide class of loss functions; CRFs, SVM, structured perceptron can all be deemed as its special cases. Furthermore, learning rate is automatically determined for each instance, hence pesky learning rate tuning is no longer needed. The learning obj"
C12-1070,P12-1026,0,0.0453805,"n windows of size 1; part-of-speech unigram,bigram,trigram in windows of size 2; English named entity recognition basic word unigram in windows of size 2, word bigram in windows of size 1; part-of-speech unigram,bigram,trigram in windows of size 2; character shape unigram and bigram in windows of size 2 extended basic features plus word cluster code prefix with length 4,10,16,20, both unigram and bigram in windows of size 2; gazetteer list feature of the current token; basic word unigram and bigram in windows of size 2; current word prefix and suffix of size up to 3, which is the baseline of (Sun and Uszkoreit, 2012). Chinese POS tagging Table 2: Features used for different tasks. For basic features, I refer to most simple token based feature and word type (letter,digit,punctuation) within a window of certain size. Extended features vary with available resources. 1. Chinese Word Segmentation (CWS) SigHan 2005 dataset is used for CWS 2 . 2. Chinese Named Entity Recognition (NER) Named entity recognition requires large amount of world knowledge. List lookup features from gazetteer, lexicon and dictionaries can greatly enhance an NER system (Nadeau and Sekine, 2007). So for extended features, we explore the"
C12-1070,D11-1090,0,0.0260534,"ion scenarios. For under-resourced tasks, we simply use token based n-gram features. Table 2 gives a brief view of features used. Table 4 shows the statistics after feature generation. Tasks Type Features Chinese word segmentation basic character unigram w−2 , w−1 , w0 , w1 , w2 , character bigram w−2 w−1 , w−1 w0 , w0 w1 , w1 w2 , whether w j and w j+1 are identical and whether w j and w j+2 are identical in windows of 2 characters on the left and 2 characters on the right; unigram/bigram dictionary features as described in (Sun et al., 2012) extended accessor variety and mutual information (Sun and Xu, 2011) Chinese named entity recognition basic character unigram and bigram in the context window of size 2; bigram of previous character and next character; whether character is word,letter,digit or punctuation in windows of size 1 extended basic features plus Tongyici Cilin (extended) derived word boundary and semantic type, entity list derived from Baidu Baike, in windows of size 2; whether the current character is a single character word or multiple character word through forward maximum matching and backward maximum matching; Chunking basic word unigram in windows of size 2, word bigram in windo"
C12-1070,P12-1027,1,0.923852,"10 iterations to converge when training a multiclass Maximum Entropy (ME) model, while it takes over 50 iterations when training CRFs, much slower than training its unstructured counterpart, multiclass ME. We show how simple feature frequency adaptive strategy may help accelerate training of CRFs within 10 iterations. Despite recent progresses in online learning algorithms, little effort has been made to compare these algorithms thoroughly. In this paper, we focus on several online learning algorithms for sequence labeling. More specifically, we investigate PA, DCA, SGD and SGD’s variant ADF (Sun et al., 2012). We perform comparison on several standard datasets with diverse settings of feature design and label set. We make it as close as possible to real application scenarios whenever resources are available. Experiment reveals distinct behavior of these algorithms under different settings. Our contributions are threefold. First, we make a fair and extensive comparison of state-ofthe-art online learners for sequence labeling and characterize the strength of each algorithm. Second, we confirm the effectiveness of ADF on most datasets despite its lack of theoretical convergence guarantee for now; ins"
C12-1187,P08-2016,0,0.564263,"A."" To achieve this we need to have an abbreviation dictionary, which is laborious to manually maintain because the number of abbreviations increases rapidly (Chang and Schutze, 2006). Therefore, it is helpful to automatically generate abbreviation from full forms. This leads to the idea of ""abbreviation generation"", i.e., finding the correct abbreviation for a full form. The generation of abbreviations in Chinese differs from that for English. The reason is that Chinese itself lacks many commonly considered features in English abbreviation generation methods (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008; Ao and Takagi, 2005). Detailed differences between English abbreviation generation and Chinese abbreviation features are listed in TABLE 1. Due to these differences, specific attention should be paid to Chinese abbreviation generation. Feature Word boundary Case sensitivity English YES YES Chinese NO NO Table 1: Comparison between Chinese and English abbreviation generation with regards to features. Most of Chinese abbreviations are generated by selecting representative characters from the full forms1 . For example, the abbreviation of ""北京大学"" (Peking University) is ""北大"" which is generated by"
C12-1187,nenadic-etal-2002-automatic,0,0.586528,"Missing"
C12-1187,P02-1021,0,0.0350623,"y to include the abbreviation ""USA."" To achieve this we need to have an abbreviation dictionary, which is laborious to manually maintain because the number of abbreviations increases rapidly (Chang and Schutze, 2006). Therefore, it is helpful to automatically generate abbreviation from full forms. This leads to the idea of ""abbreviation generation"", i.e., finding the correct abbreviation for a full form. The generation of abbreviations in Chinese differs from that for English. The reason is that Chinese itself lacks many commonly considered features in English abbreviation generation methods (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008; Ao and Takagi, 2005). Detailed differences between English abbreviation generation and Chinese abbreviation features are listed in TABLE 1. Due to these differences, specific attention should be paid to Chinese abbreviation generation. Feature Word boundary Case sensitivity English YES YES Chinese NO NO Table 1: Comparison between Chinese and English abbreviation generation with regards to features. Most of Chinese abbreviations are generated by selecting representative characters from the full forms1 . For example, the abbreviation of ""北京大学"" (Pe"
C12-1187,W01-0516,0,0.748931,"l documents or other formal materials. Take ""丁型病毒性肝炎""(Viral Hepatitis D) as an example, our method generates ""丁肝"", while the reference is ""丁型肝炎"". Both of these results are acceptable, while the reference is more formal. Interestingly, we find that in this kind of errors, the ""false"" abbreviations are always shorter in length than the standard abbreviations, which is identical to the intuition that these abbreviations are more widely used orally. 6 Related work Previous research on abbreviations mainly focuses on ""abbreviation disambiguation"", and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly because the full form is not always ambiguous. In many cases the full form is definite while we don’t know the corresponding abbreviation. 3067 To solve this problem, some approaches maintain a database of abbreviations and their corresponding ""full form"" pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find t"
C12-1187,P09-1102,0,0.580149,"he query terms are returned, which provides a natural corpus for further analysis. In this paper, we propose a stacked approach to automatically generate Chinese abbreviations. This method consists of a candidate generation phase and a ranking phase. First, we generate a list of candidates for the given full form using sequence labeling method. Then a supervised re-ranking method based on Support Vector Machine (SVM) using web data is applied to find the exact abbreviation. We evaluate on a Chinese abbreviation corpus and compare it with previous methods. A pure sequence labeling approach by (Sun et al., 2009) and a state-of-art method to incorporate web data by (Jain et al., 2007) are chosen as baseline methods. The contribution of this paper is that we integrate sequence labeling and web data to create a robust and automatic abbreviation generator. Experiments show that this combination gets better result than existing methods. Using this method we build a Chinese abbreviation dictionary, which later can be used in other NLP applications to help improve performance. The paper is structured as follows. We first describe our approach. In section 2 we describe the sequence labeling procedure and in"
C12-1187,W05-1304,0,0.493702,"rocedure. Experiments are described in section 4. In section 5 we give a detailed analysis of the results. In section 6 related works are introduced, and the paper is concluded in the last section. 2 Candidate Generation 2.1 Sequence Labeling As mentioned in section 1, the generation of Chinese abbreviations can be formalized as a task of selecting characters from the full form, which can be solved by sequence labeling models. Previous works proved that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in the candidate generation stage. A CRFs model is a type of discriminative probabilistic model most often used for the labeling or parsing of sequential data. Detailed definition of CRF model can be found in (Lafferty et al., 2001; McCallum, 2002; Pinto et al., 2003). 2.2 Labeling strategy Considering both training efficiency and modeling ability, we use a labeling method which uses four tags, ""BIEP"". ""B"" stands for ""Beginning character of skipped characters"", ""I"" stands for ""Internal character of skipped characters,""E"" stands for ""End character of skipp"
C12-2081,esuli-sebastiani-2006-sentiwordnet,0,0.177866,"Missing"
C12-2081,P11-2104,0,0.169438,"Missing"
C12-2081,N10-1119,0,0.0289978,"Missing"
C12-2081,P07-1123,0,0.374643,"Missing"
C12-2081,E09-1077,0,0.249258,"Missing"
C12-2081,D08-1058,0,0.169823,"oreover, in many cases, two or more English sentiment words often are translated to the same foreign word. Both factors lead to smaller translated sentiment lexicons than the original ones. (Mihalcea et al., 2007) study the efectiveness of translating English sentiment lexicon to Romanian using two bilingual dictionaries. The original English sentiment lexicon contains 6,856 entries; after translation, only 4,983 entries are left in the Romanian sentiment lexicon. About 2000 entries are lost or conlated into other entries during the translation process. The translation method is also used in (Wan, 2008, 2011). On the other hand, though bootstrapping methods don't use bilingual dictionaries and hence are not subject to the limitation of the translation methods, they have relatively high demands for semantic resources such as WordNet (Fellbaum, 1998). Bootstrapping methods enlarge the sentiment lexicons from English sentiment seed words. (Hassan et al., 2011) present a method to identify the sentiment polarity of foreign words by using WordNet (or similar semantic resources) in the target foreign language. (Ku and Chen, 2007) create a Chinese Lexicon by translating the General Inquirer, combi"
C12-2081,J11-3005,0,0.0428357,"Missing"
C12-2081,N10-2012,0,0.014788,"eriod or question mark, at the end of the English word. We use this simple rule to limit the possible parts-of-speech of the translations. For example, “efusive.” is translated to “热情洋溢”, while “efusive” is translated to “感情奔放的” ; after adding punctuation context, “efusive” is translated to words that have diferent parts-of-speech. We can also combine this technique with the coordinated phrase technique. Concretely, We use a bi-gram language model for generating possible collocations. Instead of creating our own language model from large corpora, we leverage the Microsoft Web N-gram Services (Wang et al., 2010)3 , an online N-gram corpus that built from Web documents. We choose the bi-gram language model trained on document titles. Given each English polarity word w1 , we use the language model to generate up to the 1000 most frequent bi-grams w1 w2 . To create coordinated phrases, we irst translate all sentiment words using Google Translate. And then we create coordinated phrases for the English sentiment words which are translated into the same Chinese word. We select those English words and join them with the word “and”. The punctuation context are generated by appending a period after the given"
C12-2081,H05-1044,0,0.0366647,"ent dictionaries in other languages as well. Depending on the target language, we might need to make some small modiications. Word segmentation is unnecessary for most European languages. And in some languages, we need to consider the word order issues when extracting the sentiment words from the translation results, since translation engine might reorder the queries. For example, in Arabic, the modifying adjectives are placed before the nouns, which is diferent from English; and also in Arabic, the words are written from right to left. 3 Experimental Study We use the MPQA subjective lexicon (Wilson et al., 2005) as the English lexicon. We only keep the strong subjective entries, which include 1,481 positive and 3,080 negative entries. For the purpose of comparison, we implemented the following baseline approaches. The irst three baselines rely on a bilingual dictionary. We use the LDC (Linguistic Data Consortium) English-Chinese bilingual wordlists6 , which is also used in (Wan, 2008). This dictionary contains 18,195 entries. Each English entry is mapped to a list of Chinese words or expressions. As shown in Table 2, the irst baseline (DICT) looks up the English entry in the bilingual dictionary and"
C12-2081,C10-1136,1,0.894878,"Missing"
C14-1024,C12-1076,0,\N,Missing
C14-1026,W09-3036,0,0.115541,"Missing"
C14-1026,I11-1138,0,0.0433831,"Missing"
C14-1026,W09-2307,0,0.0597728,"Parser / / ZPar-con / / Constituent Parsing len<=40 words Unlimited / / / / / / / / 85.25 84.22 85.02 84.12 Constituent Parsing(DS2PS) len<=40 words Unlimited 84.77 83.43 85.47 84.33 85.53 84.47 85.92 84.84 / / / / Table 5: Parsing results on our treebank using automatic POS-tags. 31K Chinese-English sentence pairs from the Xinhua Corpus (Liu et al., 2006), and we used NIST MT Evaluation 2006 test set as the development set, and the NIST 2003 (MT03), 2004 (MT04) and 2005 (MT05) test sets as the test sets. For Stanford dependency trees, we parsed the source sentences with the Stanford Parser (Chang et al., 2009) (version 3.3.1), which was trained on CTB 7.0. For the PMT treebank, we used the Ours-PS parser, trained with 14000 sentences (the last 463 sentences are used as development data for the parser). All the MT configurations are the same as Xie et al. (2011). The results are shown in Table 6. The Chinese-English translation outputs using our parser and treebank are slightly lower but comparable to those using the Stanford Parser. Note that our treebank contains 336K words on People’s Daily, while the CTB 7.0 contains about 1.19M words, most on Xinhua, the source of the MT training and test data."
C14-1026,A00-2018,0,0.188435,"s of the multi-view framework, we implemented an arc-standard transition-based dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into th"
C14-1026,P99-1065,0,0.351502,"Missing"
C14-1026,J03-4003,0,0.0418252,"he effectiveness of the multi-view framework, we implemented an arc-standard transition-based dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treeba"
C14-1026,D07-1098,0,0.013097,"hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The main reason is that the PS treebanks were designe"
C14-1026,C12-1052,0,0.0788052,"similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PT"
C14-1026,hajic-etal-2012-announcing,0,0.0651536,"Missing"
C14-1026,W00-1205,0,0.0798795,"ross-clause punctuations (PUS). 3.1.2 A Case Study: Generating the Hierarchy of Coordination Structure We take coordination structures as an example to illustrate the PS hierarchy generation process. Typically, researchers treat the rightmost conjunct as the head of a coordinate structure. However, doing so introduces modifier scope ambiguities when modifiers are also attached to the rightmost head. Vice versa, treating the leftmost conjunct as the head will lead to ambiguities when modifiers attached to the left head (Che et al., 2012). Another choice is treating the conjunction as the head (Huang et al., 2000; Xue, 2007). However, this is usually not preferred since it makes parsing more difficult and a choice still has to be made between the left and right elements when there is no conjunction in a coordinate structure (Xue, 2007). Our strategy is as follows: (1) Choose the rightmost conjunct as the head to eliminate the ambiguities when the modifiers are attached to the left; (2) Classify coordinate structures into common coordinate structures (COO) and sharing-right-child coordinate structures (COS). COO words are taken as common left nodes (as shown in Figure 2), while COS words are special le"
C14-1026,W07-2416,0,0.029084,"endents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The main reason is that the PS tre"
C14-1026,P06-1077,0,0.0134568,"rd) i (dawn) q (again) ˜ (one) g(time) ü (come) 3 (in) ô (the Pearl River) • (estuary)). “!” denotes the head constituent. Dependency Parsing Parsers UAS LAS Mate-tools 82.98 79.37 ZPar-dep 82.73 80.20 Ours-standard 82.81 80.04 Ours-PS 83.28 80.50 Berkeley Parser / / ZPar-con / / Constituent Parsing len<=40 words Unlimited / / / / / / / / 85.25 84.22 85.02 84.12 Constituent Parsing(DS2PS) len<=40 words Unlimited 84.77 83.43 85.47 84.33 85.53 84.47 85.92 84.84 / / / / Table 5: Parsing results on our treebank using automatic POS-tags. 31K Chinese-English sentence pairs from the Xinhua Corpus (Liu et al., 2006), and we used NIST MT Evaluation 2006 test set as the development set, and the NIST 2003 (MT03), 2004 (MT04) and 2005 (MT05) test sets as the test sets. For Stanford dependency trees, we parsed the source sentences with the Stanford Parser (Chang et al., 2009) (version 3.3.1), which was trained on CTB 7.0. For the PMT treebank, we used the Ours-PS parser, trained with 14000 sentences (the last 463 sentences are used as development data for the parser). All the MT configurations are the same as Xie et al. (2011). The results are shown in Table 6. The Chinese-English translation outputs using ou"
C14-1026,J93-2004,0,0.0495929,"n and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The main reason is that the PS treebanks were designed without consideration of DS conversion, leading to inherent ambiguities in the mapping, and loss of information in the resulting DS treebanks. To minimize information loss during treebank conversions, a t"
C14-1026,H05-1066,0,0.321181,"Missing"
C14-1026,D13-1108,0,0.0544104,"Missing"
C14-1026,J08-4003,0,0.025968,"treebanks, we perform empirical analysis to the treebank, by the statistical dependency parsing and dependency-tostring machine translation tasks. Several researchers explored joint DS and PS information to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013). Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging. Since our treebank is multi-view, it is possible to combine DS features and PS features directly in the decoding process. We implemented an arc-standard transition-based dependency parser (Nivre, 2008) based on the arceager parser of Zhang and Nivre (2011), which is a state-of-the-art transition-based dependency parser (Zhang and Nivre, 2012). It is more reasonable to derive the phrasal category of a phrase after the complete subtree (phrase) rather than partial subtree headed by a word has been built. The arc-standard parser differs from the arc-eager parser in that it postpones the attachment of right-modifiers until the complete subtrees headed by the modifiers themselves have been built. Because of this, we add PS features into an arc-standard parser rather than an arc-eager one. The pa"
C14-1026,N07-1051,0,0.0127179,"-standard transition-based dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to"
C14-1026,rambow-etal-2002-dependency,0,0.0525225,"d) v (verb) w (punctuation) Table 1: Mapping from PKU POS to our POS. the dependency categories between them (for terminal words, parts-of-speech can be used as phrasal categories). Consequently, in Chinese, the canonical PS, containing information of constituent hierarchies and phrasal categories, can be derived naturally from the canonical DS. As Xia et al. (2009) stated, a rich set of dependency categories should be designed to ensure lossless conversion from DS to PS. When the information of PS has been represented in DS explicitly or implicitly, we can convert DS to PS without ambiguity (Rambow et al., 2002). Given our framework, a multi-view Chinese treebank, containing 14,463 sentences and 336K words, is constructed. This main corpus is based on the Peking University People’s Daily Corpus. We name our treebank the Peking University Multi-view Chinese Treebank (PMT) release 1.0. To verify the usefulness of the treebank for statistical NLP, a transition-based dependency parser is implemented to include PS features produced in the derivation process of phrasal categories. We perform a set of empirical evaluations, with experimental results on both dependency parsing and dependency-to-string machin"
C14-1026,N10-1049,0,0.0173777,"in the mapping, and loss of information in the resulting DS treebanks. To minimize information loss during treebank conversions, a treebank could be designed by considering PS and DS information simultaneously; such treebanks have been proposed as multi-view treebanks (Xia et al., 2009). We develop a multi-view treebank for Chinese, which treats PS and DS as different views of the same internal structures of a sentence. We choose the DS view as the base view, from which PS would be derived. Our choice is based on the effectiveness of information transfer rather than convenience of annotation (Rambow, 2010; Bhatt and Xia, 2012). Research on Chinese syntax (Zhu, 1982; Chen, 1999; Chen, 2009) shows that the phrasal category of a constituent can be derived from the phrasal categories of its immediate subconstituents and This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 257 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 257–268, Dublin, Ireland, August 23-29 2014. PKU POS"
C14-1026,N04-1032,0,0.0394926,"on between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The ma"
C14-1026,Q13-1025,0,0.0657543,"to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1"
C14-1026,N03-1033,0,0.0288741,"Missing"
C14-1026,C10-2148,0,0.088723,"parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such"
C14-1026,H01-1014,0,0.0746166,"Missing"
C14-1026,D11-1020,0,0.124266,"r example. Figure 3(a) shows the correct PS while Figure 3(b) shows an incorrect parser output. In particular, “i (dawn)” is put under the incorrect constituent. When converted into DS, both lead to the correct link, with “i  (dawn)” being the SBV modifier of “ü (come)” (Figure 3(c)). As a result, the PS parser error is erased in the conversion into DS. The same can happen in DS to PS conversion. 6.2 Dependency-to-string Machine Translation We compare the effects of our treebank and the Stanford dependencies converted from CTB on machine translation, using the dependency-to-string system of Xie et al. (2011). Our training corpus consists of 2 https://code.google.com/p/mate-tools/ http://code.google.com/p/berkeley-parser-analyser/ 4 http://sourceforge.net/projects/zpar/ 3 263 Figure 3: An instance where PS parser error is erased in the PS to DS conversion (ðD (bright) (de, an auxiliary word) i (dawn) q (again) ˜ (one) g(time) ü (come) 3 (in) ô (the Pearl River) • (estuary)). “!” denotes the head constituent. Dependency Parsing Parsers UAS LAS Mate-tools 82.98 79.37 ZPar-dep 82.73 80.20 Ours-standard 82.81 80.04 Ours-PS 83.28 80.50 Berkeley Parser / / ZPar-con / / Constituent Parsing len<=40 word"
C14-1026,W03-3023,0,0.206075,"Missing"
C14-1026,D08-1059,1,0.809789,"ed dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Mage"
C14-1026,W09-3825,1,0.878569,"Missing"
C14-1026,J11-1005,1,0.896959,"Missing"
C14-1026,P11-2033,1,0.81137,"he treebank, by the statistical dependency parsing and dependency-tostring machine translation tasks. Several researchers explored joint DS and PS information to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013). Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging. Since our treebank is multi-view, it is possible to combine DS features and PS features directly in the decoding process. We implemented an arc-standard transition-based dependency parser (Nivre, 2008) based on the arceager parser of Zhang and Nivre (2011), which is a state-of-the-art transition-based dependency parser (Zhang and Nivre, 2012). It is more reasonable to derive the phrasal category of a phrase after the complete subtree (phrase) rather than partial subtree headed by a word has been built. The arc-standard parser differs from the arc-eager parser in that it postpones the attachment of right-modifiers until the complete subtrees headed by the modifiers themselves have been built. Because of this, we add PS features into an arc-standard parser rather than an arc-eager one. The parser processes a sentence from left to right, using a s"
C14-1026,C12-2136,1,0.850113,"lation tasks. Several researchers explored joint DS and PS information to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013). Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging. Since our treebank is multi-view, it is possible to combine DS features and PS features directly in the decoding process. We implemented an arc-standard transition-based dependency parser (Nivre, 2008) based on the arceager parser of Zhang and Nivre (2011), which is a state-of-the-art transition-based dependency parser (Zhang and Nivre, 2012). It is more reasonable to derive the phrasal category of a phrase after the complete subtree (phrase) rather than partial subtree headed by a word has been built. The arc-standard parser differs from the arc-eager parser in that it postpones the attachment of right-modifiers until the complete subtrees headed by the modifiers themselves have been built. Because of this, we add PS features into an arc-standard parser rather than an arc-eager one. The parser processes a sentence from left to right, using a stack to maintain partially built derivations and a queue to hold next incoming words. Th"
C14-1026,P13-1043,1,0.901629,"Missing"
C18-1165,P06-4018,0,0.037425,"Missing"
C18-1165,P16-1045,0,0.0246243,"d on triples (q, T (r+ , c+ ), T (r− , c− )). We measure the loss of each triple by: L(q, T (r+ , c+ ), T (r− , c− )) = max (0, α − s+ + s− ) (18) In (18), s+ and s− are relevance scores between question and T (r+ , c+ ), T (r− , c− ), and α is a hyperparameter to control the gap between the two scores. 4 Experiment In this section, we will evaluate our QA system based on semi-structured knowledge. Firstly, we will introduce our dataset, evaluation metrics and setup. Then, we compare our system with other work. 1946 Finally, we analysis the result of evaluation. 4.1 Dataset We use the TabMCQ (Jauhar et al., 2016) dataset to evaluate our system. TabMCQ contains 9092 manually annotated multiple choice questions (MCQs) with their answers, and 63 tables as its knowledge. Tables in this task are semi-structured tables, rows in each table are sentences with well-defined recurring filler patterns. For those tables built by sentences, some of the tables contain some link words to make the sentence complete, while in other tables, all cells are meaningful. Same as simple tables, analogies between rows of tables also exist. The target domain for the tables is the 4th grade science exam, and most tables are cons"
C18-1165,D16-1147,0,0.0220708,"dge graphs contain too much noise and still need manual intervention. In contrast, the semi-structured data are more flexible to be comprehended than raw text corpora, and much easier to build from text automatically than knowledge graphs. Besides, there are many documents which can be easily converted to semi-structured tables, such as announcements published by institutions and knowledge in science books, etc. We would like to utilize these kinds of knowledge in QA systems. There are many works on QA based on other knowledge. Some of them are based on raw text corpora, such as Miller et al. (2016), Min et al. (2017), Yin et al. (2016). Besides, QA models based on knowledge graphs are improving rapidly. Freebase (Bollacker et al., 2008) and Wikidata (Vrandecic and Kr¨otzsch, 2014) are well-known structured knowledge bases, which are built almost manually by their users. Although there are some studies on automatic knowledge graph construction, like Sateli and Witte (2015), it is not good enough. There are also many studies on those structured data, like Yih et al. (2015), Yao and Durme (2014). Some of them focused on text description of each item, while others, such as Zhang et al. (201"
C18-1165,P17-2081,0,0.0136049,"hs contain too much noise and still need manual intervention. In contrast, the semi-structured data are more flexible to be comprehended than raw text corpora, and much easier to build from text automatically than knowledge graphs. Besides, there are many documents which can be easily converted to semi-structured tables, such as announcements published by institutions and knowledge in science books, etc. We would like to utilize these kinds of knowledge in QA systems. There are many works on QA based on other knowledge. Some of them are based on raw text corpora, such as Miller et al. (2016), Min et al. (2017), Yin et al. (2016). Besides, QA models based on knowledge graphs are improving rapidly. Freebase (Bollacker et al., 2008) and Wikidata (Vrandecic and Kr¨otzsch, 2014) are well-known structured knowledge bases, which are built almost manually by their users. Although there are some studies on automatic knowledge graph construction, like Sateli and Witte (2015), it is not good enough. There are also many studies on those structured data, like Yih et al. (2015), Yao and Durme (2014). Some of them focused on text description of each item, while others, such as Zhang et al. (2016), try to obtain e"
C18-1165,D14-1162,0,0.086485,"t. Candidate Table Selection Model: For each (query, table) pair, we will score the relevance between them. Consider a table [x, T ] and a question q = {wi,q }ni=1, where x is the title of a table, T is content of the table, and wi,q is the i-th word in q. We first convert words in questions into representations Iq = Rdw ×n , which are concatenation of the word-level embeddings of the words {ewi,q }ni=1 and their POS-tag embeddings {epi,q }ni=1. The embeddings of POS tags are randomly initialized and are trained together with parameters, and the word-level embeddings are initialized by GloVe (Pennington et al., 2014) vectors. We then use a DiSAN layer described above to encode each question q: E(q) = DiSAN(Iq ). (11) Then we convert tables into their vector representations. A table contains a title and some structured data, and we encode them respectively. In some tables, there are some columns filled with link words, and each row in the table is a complete sentence, while others have few or even no link words (like Figure 3). We put those columns, which link the cells, and tags of other columns into a DiSAN encoder, and then, we measure the relevance between each table and the question by their represent"
C18-1165,P14-1090,0,0.0818745,"Missing"
C18-1165,P15-1128,0,0.0325611,"systems. There are many works on QA based on other knowledge. Some of them are based on raw text corpora, such as Miller et al. (2016), Min et al. (2017), Yin et al. (2016). Besides, QA models based on knowledge graphs are improving rapidly. Freebase (Bollacker et al., 2008) and Wikidata (Vrandecic and Kr¨otzsch, 2014) are well-known structured knowledge bases, which are built almost manually by their users. Although there are some studies on automatic knowledge graph construction, like Sateli and Witte (2015), it is not good enough. There are also many studies on those structured data, like Yih et al. (2015), Yao and Durme (2014). Some of them focused on text description of each item, while others, such as Zhang et al. (2016), try to obtain embeddings of the items. Although there is limited work on semi-structured tables, there has already been research of QA based on simple tables. HILDB (Dua et al., 2013) is a QA system which converts questions in natural language to SQL queries. Vakulenko and Savenkov (2017) offer another data structure of tables, and introduce a QA system based on tabular knowledge. However, those methods are limited to the specific structure of tables, and cannot take advant"
C18-1330,W17-2339,0,0.0495048,"ork models have also been used for the MLC task. Zhang and Zhou (2006) propose the BP-MLL that utilizes a fully-connected neural network and a pairwise ranking loss function. Nam et al. (2013) propose a neural network using cross-entropy loss instead of ranking loss. Benites and Sapozhnikova (2015) increase classification speed by adding an extra ART layer for clustering. Kurata et al. (2016) utilize word embeddings based on CNN to capture label correlations. Chen et al. (2017) propose to represent semantic information of text and model high-order label correlations by combining CNN with RNN. Baker and Korhonen (2017) initialize the final hidden layer with rows that map to co-occurrence of labels based on the CNN architecture to improve the performance of the model. Ma et al. (2018) propose to use the multi-label classification algorithm for machine translation to handle the situation where a sentence can be translated into more than one correct sentences. 5 Conclusions and Future Work In this paper, we propose to view the multi-label classification task as a sequence generation problem to model the correlations between labels. A sequence generation model with a novel decoder structure is proposed to impro"
C18-1330,D14-1181,0,0.00963567,"ines We compare our proposed methods with the following baselines: • Binary Relevance (BR) (Boutell et al., 2004) transforms the MLC task into multiple single-label classification problems by ignoring the correlations between labels. • Classifier Chains (CC) (Read et al., 2011) transforms the MLC task into a chain of binary classification problems and takes high-order label correlations into consideration. • Label Powerset (LP) (Tsoumakas and Katakis, 2006) transforms a multi-label problem to a multiclass problem with one multi-class classifier trained on all unique label combinations. • CNN (Kim, 2014) uses multiple convolution kernels to extract text features, which are then inputted to the linear transformation layer followed by a sigmoid function to output the probability distribution over the label space. The multi-label soft margin loss is optimized. • CNN-RNN (Chen et al., 2017) utilizes CNN and RNN to capture both the global and local textual semantics and model the label correlations. Following the previous work (Chen et al., 2017), we adopt the linear SVM as the base classifier in BR, CC and LP. We implement BR, CC and LP by means of Scikit-Multilearn (Szyma´nski, 2017), an opensou"
C18-1330,N16-1063,0,0.327329,"large datasets. Other methods such as ML-DT (Clare and King, 2001), Rank-SVM (Elisseeff and Weston, 2002), and ML-KNN (Zhang and Zhou, 2007) can only be used to capture the first or second order label correlations or are computationally intractable when high-order label correlations are considered. In recent years, neural networks have achieved great success in the field of NLP. Some neural network models have also been applied in the MLC task and achieved important progress. For instance, fully connected neural network with pairwise ranking loss function is utilized in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style tran"
C18-1330,D15-1099,1,0.784573,"el data directly. Clare and King (2001) construct decision tree based on multi-label entropy to perform classification. Elisseeff and Weston (2002) optimize the empirical ranking loss by using maximum margin strategy and kernel tricks. Collective multi-label classifier (CML) (Ghamrawi and McCallum, 2005) adopts maximum entropy principle to deal with multi-label data by encoding label correlations as constraint conditions. Zhang and Zhou (2007) adopt k-nearest neighbor techniques to deal with multi-label data. F¨urnkranz et al. (2008) make ranking among labels by utilizing pairwise comparison. Li et al. (2015) propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label. Most methods, however, can only be used to capture the first or second order label correlations or are computationally intractable in considering high-order label correlations. Among ensemble methods, Tsoumakas et al. (2011) break the initial set of labels into a number of small random subsets and employ the LP algorithm to train a corresponding classifier. Szyma´nski et al. (2016) propose to construct a label co-occurrence graph a"
C18-1330,P18-2027,1,0.848082,"(2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th International Conference on Computational Linguistics, pages 3915–3926 Santa Fe, New Mexi"
C18-1330,D15-1166,0,0.0436376,"cted neural network with pairwise ranking loss function is utilized in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th Inter"
C18-1330,P18-2053,1,0.840617,"m et al. (2013) propose a neural network using cross-entropy loss instead of ranking loss. Benites and Sapozhnikova (2015) increase classification speed by adding an extra ART layer for clustering. Kurata et al. (2016) utilize word embeddings based on CNN to capture label correlations. Chen et al. (2017) propose to represent semantic information of text and model high-order label correlations by combining CNN with RNN. Baker and Korhonen (2017) initialize the final hidden layer with rows that map to co-occurrence of labels based on the CNN architecture to improve the performance of the model. Ma et al. (2018) propose to use the multi-label classification algorithm for machine translation to handle the situation where a sentence can be translated into more than one correct sentences. 5 Conclusions and Future Work In this paper, we propose to view the multi-label classification task as a sequence generation problem to model the correlations between labels. A sequence generation model with a novel decoder structure is proposed to improve the performance of classification. Extensive experimental results show that the proposed methods outperform the baselines by a substantial margin. Further analysis o"
C18-1330,D15-1044,0,0.0499285,"d in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th International Conference on Computational Linguistics, pages 3915–3926"
C18-1330,D16-1137,0,0.0283203,"ity under the distribution yt−1 . yt−1 is the probability 3917 distribution over the label space L at time-step t − 1 and is computed as follows: ot = Wo f (Wd st + Vd ct ) (8) yt = sof tmax(ot + It ) (9) where Wo , Wd , and Vd are weight parameters, It ∈ RL is the mask vector that is used to prevent the decoder from predicting repeated labels, and f is a nonlinear activation function. ( −∞ if the label li has been predicted at previous t − 1 time steps. (It )i = 0 otherwise. (10) At the training stage, the loss function is the cross-entropy loss function. We employ the beam search algorithm (Wiseman and Rush, 2016) to find the top-ranked prediction path at inference time. The prediction paths ending with the eos are added to the candidate path set. 2.3 Global Embedding In the sequence generation model mentioned above, the embedding vector g(yt−1 ) in Equation (7) is the embedding of the label that has the highest probability under the distribution yt−1 . However, this calculation only takes advantage of the maximum value of yt−1 greedily. The proposed sequence generation model generates labels sequentially and predicts the next label conditioned on its previously predicted labels. Therefore, it is likel"
D12-1114,D08-1031,0,0.595918,"irwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logi"
D12-1114,W11-1904,0,0.0357831,".20 51.46 57.50 54.53 - MUC P 64.05 64.10 68.52 68.40 59.10 62.25 - F 60.89 60.90 59.26 58.73 58.30 58.13 55.8 R 67.11 67.12 60.85 59.79 71.00 63.72 - B-cube P 73.88 74.13 80.15 81.69 69.20 73.83 - F 70.33 70.45 69.18 69.04 70.10 68.40 69.29 R 47.6 47.70 51.6 53.03 48.10 47.20 - CEAF P 41.92 41.96 37.05 37.84 46.50 40.01 - F 44.58 44.65 43.13 44.17 47.30 43.31 43.96 Avg F 58.60 58.67 57.19 57.31 58.60 56.61 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner ("
D12-1114,C10-1019,0,0.0216608,"graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominative"
D12-1114,H05-1013,0,0.0639415,"Missing"
D12-1114,N07-1030,0,0.114892,"m joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework. Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution. Daum´e and Marcu (2005) developed such a model based on the Learning as Search Optimization (LaSO) framework. Rahman and Ng (2009) proposed to learn a cluster-ranker for discourse-new mention detection jointly with coreference resolution. Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. 6 Conclusion In this paper we present a joint learning method with Markov logic which naturally combines pairwise classification and mention clustering. Experimental results show that the joint learning method significantly outperforms baseline methods. Our method is also better than all the learning-based systems in CoNLL-2011 and reaches the same level of performance with the best system. In the future we will try to design more global constraints and"
D12-1114,D08-1069,0,0.0395601,"tems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and m"
D12-1114,P08-2012,0,0.439324,"s used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. We evaluate Markov logic-based method on the dataset from CoNLL-2011 shared task. Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. We examine best-first clustering and transitivity constraint in our methods, and find that both are very useful for coreference resolution. Compared with the state of the art, our method outperforms a baseline that represents a typical system using the mention-pair model. Our method is also better than all learning systems from the Co"
D12-1114,W11-1902,0,0.149477,"Missing"
D12-1114,P04-1018,0,0.0629881,"6.61 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning"
D12-1114,H05-1004,0,0.235132,"rom the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning In this section, we will first describe the dataset and evaluation metrics we use. We will then present the effect of our joint learning method, and finally discuss the comparison with the state of the art. 4.1 and another well-known coreference dataset from ACE. First, OntoNotes does not label any singleton entity cluster, which has only one reference in the text. Second, only identity coreference is tagged in OntoNotes, but not appositives or predicate nomin"
D12-1114,P02-1014,0,0.826267,"ng methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction The task of noun phras"
D12-1114,P10-1142,0,0.0271294,"ms of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a bin"
D12-1114,D08-1068,0,0.455659,"how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logic is based on first-order logic, which makes the learned models readily interpretable by humans. Moreover, joint learning is natural under the Markov logic framework, with local pairwise classification and global mention clustering both formulated as weighted first-order clauses. In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good 1245 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1245–1254, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity const"
D12-1114,D09-1001,0,0.030231,"se variables is modeled by conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annot"
D12-1114,W11-1901,0,0.309737,"tion y ′ (i.e. the one with the highest score s(y ′ , xi ), equivalent to yˆ in Section 3.3)) is at least as big as the loss L(yi , y ′ ), while changing wt−1 as little as possible. The number of false ground atoms of coref predicate is selected as loss function in our experiments. Hard global constraints (i.e. best-first clustering or transitivity constraint) must be satisfied when inferring the best y ′ in each iteration, which can make learned weights more effective. 4 Experiments Data Set We use the dataset from the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different"
D12-1114,D09-1101,0,0.271277,"aset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in"
D12-1114,W11-1903,0,0.0338164,"Missing"
D12-1114,J01-4004,0,0.681245,"the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction"
D12-1114,W11-1908,0,0.0127817,"ion and mention clustering. 2.1 Mention Detection For mention detection, traditional methods include learning-based and rule-based methods. Which kind of method to choose depends on specific dataset. In 1246 this paper, we first consider all the noun phrases in the given text as candidate mentions. Without gold standard mention boundaries, we use a well-known preprocessing tool from Stanford’s NLP group1 to extract noun phrases. After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al. (2011), Uryupina et al. (2011)). Some examples of these erroneous candidates include stop words (e.g. uh, hmm), web addresses (e.g. http://www.google.com), numbers (e.g. $9,000) and pleonastic “it” pronouns. 2.2 Pairwise Classification For pairwise classification, traditional learningbased methods usually adopt a classification model such as maximum entropy models and support vector machines. Training instances (i.e. positive and negative mention pairs) are constructed from known coreference chains, and features are defined to represent these instances. In this paper, we build a baseline system that uses maximum entropy mo"
D12-1114,M95-1005,0,0.527813,"weights more effective. 4 Experiments Data Set We use the dataset from the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning In this section, we will first describe the dataset and evaluation metrics we use. We will then present the effect of our joint learning method, and finally discuss the comparison with the state of the art. 4.1 and another well-known coreference dataset from ACE. First, OntoNotes does not label any singleton entity cluster, which has only one reference in the text. Second, only identity coreference is t"
D12-1114,P08-1096,0,0.0425499,": Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform j"
D12-1114,P09-1046,0,0.0740571,"y conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and"
D12-1114,D08-1067,0,\N,Missing
D13-1031,D10-1077,0,0.371955,"own in table 2. Character: Tag: 我 S 爱 S 北 B 京 E 天 B 安 M 门 E Table 2: An example for the “BMES” representation. The sentence is “我爱北京天安门” (I love Beijing Tian-an-men square), which consists of 4 Chinese words: “我” (I), “爱” (love), “北京” (Beijing), and “天安门” (Tian-an-men square). 2.2 Unlabeled Data Unlabeled data can be divided into in-domain data and out-of-domain data. In previous works, these two kinds of unlabeled data are used separately for diﬀerent purposes. In-domain data only solves the problem of data sparseness (Sun and Xu, 2011). Out-of domain data is used only for domain adaptation (Chang and Han, 2010). These two functionalities are not contradictory but complementary. Our study shows that by correctly designing features and algorithms, both in-domain unlabeled data and outof-domain unlabeled data can work together to help enhancing the segmentation model. In our algorithm, the dynamic features learned from one corpus can be adjusted incrementally with the dynamic features learned from the other corpus. As for the out-of-domain data, it will be even better if the corpus is not limited to a speciﬁc domain. We choose a Chinese encyclopedia corpus which meets exactly this requirement. We use t"
D13-1031,I05-3019,0,0.18303,"Missing"
D13-1031,I08-4022,1,0.836444,"v relaxations or latent variables, or modifying models to ﬁt special conditions. Our system uses a single CRF model. As we can see in table 10, our method achieved higher F-scores than the previous best systems. 3.3 Results on NER task Our method is not limited to the CWS problem. It is applicable to all sequence labeling problems. We applied our method on the Chinese NER task. We used the MSR corpus of the sixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simpliﬁed Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount"
D13-1031,P06-2056,0,0.0141842,"with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same"
D13-1031,J09-4006,0,0.197025,"o et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by introducing extra labels ’L’ and ’R’. Chang and Han (2010), Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework. All these approaches can be viewed as using static statistics features in a supervised approach. Our method is diﬀerent from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the diﬀerent lengths, but also consider term frequency when processing Using one corpus Our method P 0.963 0.965 R 0.955 0.958 F 0.959 0.961"
D13-1031,J04-1004,0,0.0708395,"e. An example is “十全十美” (Perfect), which is a Chinese idiom with structure “ABAC”. 2.4.2 Static statistical features Statistical features are statistics that distilled from the large unlabeled corpus. They are proved useful in the Chinese word segmentation task. We deﬁne Static Statistical Features (SSFs) as features whose value do not change during the training process. The SSFs in our approach includes Mutual information, Punctuation information and Accessor variety. Previous works have already explored the functions of the three static statistics in the Chinese word segmentation task, e.g. Feng et al. (2004); Sun and Xu (2011). We mainly follow their deﬁnitions while considering more details and giving some modiﬁcation. Mutual information Mutual information (MI) is a quantity that measures the mutual dependence of two random variables. Previous works showed that larger MI of two strings claims higher probability that the two strings should be combined. Therefore, MI can show the tendency of two strings forming one word. However, previous works mainly focused on the balanced case, i.e., the MI of strings with the same length. In our study we ﬁnd that, in Chinese, there remains large amount of imba"
D13-1031,I05-3025,0,0.105396,"tasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task. 1 Introduction Chinese is a language without natural word delimiters. Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing. Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al., 2005). However, the resource of manually labeled training corpora is limited. Therefore, semi-supervised learning has become one ∗ of the most natural forms of training for CWS. Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion. The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion. In this paper, we propose a simple and scalable semi-supervised strategy that works by providing semi-supervision at the le"
D13-1031,P07-1104,0,0.0102609,"arest integer as the corresponding discrete value. For dynamic statistical value: Dynamic statistical features are distributions of a label. The values of DSFs are all percentage values. We can solve this by multiply the probability by an integer N and then take the integer part as the ﬁnal feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a speciﬁc baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed deﬁnition of CRF can be found in Laﬀerty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 F = 2×P ×R P +R The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to correctly segment out of vocabulary words. 3.2 Main Results Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoﬀ1 to test our approach. We chose the Peking University (PKU) data in our experiment. Alth"
D13-1031,P98-2206,0,0.0374665,"Missing"
D13-1031,P06-1085,0,0.0117019,"We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking."
D13-1031,N04-1043,0,0.0491019,"ing (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used th"
D13-1031,D11-1090,0,0.287405,"ing the unlabeled data with the trained model on the training corpus. These “pseudo-labels” are not accurate enough. Therefore, we use the label distribution, which is much more accurate. To accurately calculate the precise label distribution, we use a framework similar to the cotraining algorithm to adjust the feature values iteratively. Generally speaking, unlabeled data can be classiﬁed as in-domain data and out-ofdomain data. In previous works these two kinds of unlabeled data are used separately for diﬀerent purposes. In-domain data is mainly used to solve the problem of data sparseness (Sun and Xu, 2011). On the other hand, out-of domain data is used for domain adaptation (Chang and Han, 2010). In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus. We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoﬀ. Experiment results show that our approach yields improvements compared with the state-of-art systems. Even when the labeled data is insuﬃcient, our methods can still work better than traditional methods. Compared to the baseline CWS model, which has alread"
D13-1031,P12-1027,1,0.612773,"nitial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub using Ta LOOP until performance does not improve RETURN the tagger which is trained with in-domain features. Table 3: Algorithm description 2.4 Features 2.4.1 Baseline Features Our baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). These features are widely used in the CWS task. To be convenient, for a character ci with context . . . ci−1 ci ci+1 . . ., its baseline features are listed below: • Character uni-grams: ck (i − 3 &lt; k &lt; i + 3) • Character bi-grams: ck ck+1 (i − 3 &lt; k &lt; i + 2) • Whether ck and ck+1 are identical (i − 2 &lt; k &lt; i + 2) • Whether ck and ck+2 are identical (i − 4 &lt; k &lt; i + 2) The last two feature templates are designed to detect character reduplication, which is a morphological phenomenon in Chinese language. An example is “十全十美” (Perfect), which is a Chinese idiom with structure “ABAC”. 2.4.2 Stat"
D13-1031,N09-1007,1,0.635185,"e value. For dynamic statistical value: Dynamic statistical features are distributions of a label. The values of DSFs are all percentage values. We can solve this by multiply the probability by an integer N and then take the integer part as the ﬁnal feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a speciﬁc baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed deﬁnition of CRF can be found in Laﬀerty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 F = 2×P ×R P +R The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to correctly segment out of vocabulary words. 3.2 Main Results Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoﬀ1 to test our approach. We chose the Peking University (PKU) data in our experiment. Although the benchmark provides another three"
D13-1031,P10-1040,0,0.0391395,"(Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by"
D13-1031,O03-4002,0,0.812618,"enchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task. 1 Introduction Chinese is a language without natural word delimiters. Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing. Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al., 2005). However, the resource of manually labeled training corpora is limited. Therefore, semi-supervised learning has become one ∗ of the most natural forms of training for CWS. Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion. The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion. In this paper, we propose a simple and scalable semi-supervised strategy that works by providing semi-su"
D13-1031,P95-1026,0,0.0927642,"ppose we have labeled data L, two unlabeled corpora Ua and Ub (one is an in-domain corpus and the other is an out-of-domain corpus). Our algorithm is shown in Table 3. During each iteration, we tag the unlabeled corpus Ua using Tb to get pseudo-labels. Then we extract features from the pseudo-labels. We use the label distribution information as dynamic features. We add these features to the training data to train a new tagger Ta . To adjust the feature values, we extract features from one corpus and then apply the statistics to the other corpus. This is similar to the principle of cotraining (Yarowsky, 1995; Blum and Mitchell, 1998; Dasgupta et al., 2002). The diﬀerence is that there are not diﬀerent views of features, but diﬀerent kinds of unlabeled data. Detailed description of features is given in the next section. 313 Algorithm Init: Using baseline features only: Train an initial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub"
D13-1031,N06-2049,0,0.330682,"Missing"
D13-1031,P07-1106,0,0.776253,"Missing"
D13-1031,W06-0127,0,0.0231579,"inese NER task. We used the MSR corpus of the sixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simpliﬁed Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named e"
D13-1031,Y06-1012,0,0.0730331,"inese NER task. We used the MSR corpus of the sixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simpliﬁed Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named e"
D13-1031,P08-1068,0,\N,Missing
D13-1031,C98-2201,0,\N,Missing
D13-1041,E06-1002,0,0.851221,"tion,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, t"
D13-1041,D07-1074,0,0.971336,"e false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives the model enough flexibility and expressivity. It can include any features that describe the similarity or dissimilarity of context d and candidate entity e. They often perform well even on small training set, with carefullydesigned features. This category falls into the local approach as the decision processes for each mention are made independently (Ratinov et al., 2011). (Cucerzan, 2007) first suggests to optimize an objective function that is similar to the collective ap427 Wikipedians annotate entries in Wikipedia with category network. This valuable information generalizes entity-context correlation to category-context correlation. (Bunescu and Pasca, 2006) utilize category-word as features in their ranking model. (Kataria et al., 2011) employ a hierarchical topic model where each inner node in the hierarchy is a category. Both approaches must rely on pruned categories because the large number of noisy categories. We try to address this problem with recent advances of repr"
D13-1041,D11-1072,0,0.338593,"Missing"
D13-1041,D08-1017,0,0.0165384,"global ranker. The differences are that we use stacking to train the local ranker to handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009;"
D13-1041,P11-1138,0,0.060973,"machine readable format, ambiguous names must be resolved in order to tell which realworld entity the name refers to. The task of linking names to knowledge base is known as entity linking or disambiguation (Ji et al., 2011). The resulting text is populated with semantic rich links to knowledge base like Wikipedia, and ready for various downstream NLP applications. ∗ Corresponding author Collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously (Han et al., 2011; Hoffart et al., 2011; Kulkarni et al., 2009; Ratinov et al., 2011). Collective approaches can improve performance when local evidence is not confident enough. They often utilize semantic relations across different mentions, and is why they are called global approaches, while L2R methods fall into local approaches (Ratinov et al., 2011). However, collective inference processes are often expensive and involve an exponential search space. We propose a collective entity linking method based on stacking. Stacked generalization (Wolpert, 1992) is a powerful meta learning algorithm that uses two levels of learners. The predictions of the first learner are taken as"
D13-1041,P05-1044,0,0.0476092,"the mention “Romney” as an examFinally, the semantic relatedness measure of two entities ei ,ej is defined as the common in-links of ei and ej in Wikipedia (Milne and Witten, 2008; Han et al., 2011): 430 where W is learned with supervision like clickthrough data. Given training data {(qi , di )}, training is done by randomly sampling a negative target d− . The model optimizes W such that f (q, d+ ) > f (q, d− ). Thus, the training objective is to minimize the following margin-based loss function: ∑ max(0, 1 − f (q, d+ ) + f (q, d− )) (7) q,d+ ,d− which is also known as contrastive estimation (Smith and Eisner, 2005). W can become very large and inefficient when we have a big vocabulary size. This is addressed by replacing W with its low rank approximation: W = UT V + I (8) here, the identity term I is a trade-off between the latent space model and a vector space model. The gradient step is performed with Stochastic Gradient Descent (SGD): U ←U + λV (d+ − d− )q T , if 1 − f (q, d+ ) + f (q, d− ) > 0 (9) − T V ←V + λU q(d − d ) , + if 1 − f (q, d+ ) + f (q, d− ) > 0. (10) where λ is the learning rate. The query and document are not necessary real query and document. In our case, we treat our problem as: gi"
D13-1041,P11-1139,0,0.022453,"handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives"
D13-1041,N10-1072,0,0.661752,"aboratory of Computational Linguistics (Peking University) Ministry of Education,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning"
D14-1033,W13-1703,0,0.0272002,"ide our system into two modules correspondingly. Task and System Overview Task Description The task of grammar error correction aims to correct grammatical errors in sentences. There are various competitions devoted to the grammar error correction task for L2 learners. The CoNLL2013 shared task is one of the most famous, which focuses on correcting five types of errors that are commonly made by non-native speakers of English, including determiner, preposition, noun number, subject-verb agreement and verb form errors. The training data released by the task organizers come from the NUCLE corpus(Dahlmeier et al., 2013). This corpus contains essays writ• The general module, which is responsible for the verb form errors, noun number errors and subject-verb agreement errors. These errors are all replacement errors, which can be corrected by replacing the wrongly used word with a reasonable candidate word. 267 Figure 1: Dependency parsing results of (a) the original sentence “The books of that boy is on the desk .” (b) the corrected sentence. Position 1 2 3 4 5 6 7 8 9 10 Original The books of that boy is on the desk . Correction Candidates The books, book of that boy, boys is,are,am,was,were,be,being,been on t"
D14-1033,W12-2006,0,0.0576342,"Missing"
D14-1033,W11-2838,0,0.0307519,"ed to verb form, noun form and subject-verb agreement errors can be considered. In the special module, two extra classification models are used to correct the determiner errors and preposition errors . The classifiers are also trained at tree node level. We take special care of these two kinds Introduction The task of grammar error correction is difficult yet important. An automatic grammar error correction system can help second language (L2) learners improve the quality of their writing. In recent years, there are various competitions devoted to grammar error correction, such as the HOO2011(Dale and Kilgarriff, 2011), HOO-2012(Dale et al., 2012) and the CoNLL-2013 shared task (Ng et al., 2013). There has been a lot of work addressing errors made by L2 learners. A significant proportion of the systems for grammar error correction train individual statistical models to correct each special kind of error word by word and ignore error interactions. These methods assume no interactions between different kinds of grammatical errors. In real problem settings errors are correlated, which makes grammar error correction much more difficult. 266 Proceedings of the 2014 Conference on Empirical Methods in Natural Lang"
D14-1033,N10-1019,0,0.0153974,"Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect var"
D14-1033,P11-1092,0,0.0605381,"ra kind of errors. 5 Related Works Early grammatical error correction systems use the knowledge engineering approach (Murata and Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a"
D14-1033,W11-1422,0,0.0185212,"06; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The joint learning and joint inference are still at word/phrase level and are based on the noisy context. In the worst case, th"
D14-1033,P98-1085,0,0.0920123,"ndle the case where other errors such as spelling errors should be considered. In that case, we can modify the candidate generation of the general module. We only need to let the generate correction candidates be any possible words that are similar to the original word, and run the same decoding algorithm to get the corrected sentence. As a comparison, the ILP systems should add extra scoring system to score extra kind of errors. 5 Related Works Early grammatical error correction systems use the knowledge engineering approach (Murata and Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Model"
D14-1033,P12-1101,0,0.0206706,"ces in the news texts use a different writing style against the sentences written by ESL learners. For example, sentences written by ESL learners seldom include dialogues between people, while very often news texts include paragraphs such as “‘I am frightened!’ cried Tom”. We use heuristic rules to eliminate the sentences in the Gigaword corpus that are less likely to appear in the ESL writing. The heuristic rules include deletWe should make it clear that we are not the first to use tree level correction models on ungrammatical sentences. Yoshimoto et al. (2013) uses a Treelet Language model (Pauls and Klein, 2012) to correct agreement errors. However, the performance of Treelet language model is not that good compared with the top-ranked system in CoNLL2013. The reason is that the production rules in the Treelet language model are based on complex contexts, which will exacerbate the data sparseness problem. The “context” in Treelet language model also include words ahead of treelets, which are sometimes unrelated to the current node. In contrast, our TreeNode Language model only needs to consider useful context words related to each node 1 2 268 http://nlp.stanford.edu/software/lex-parser.shtml https:/"
D14-1033,W13-3602,0,0.0654885,"errors because these errors include insertion and deletion errors, which cannot be corrected in the general module. Because there is a fixed number of prepositions and determiners, these two kinds of errors are much easier to be incorporated into a classification framework. Besides, they are the most common errors made by ESL learners and there are lots of previous works that leave valuable guidance for us to follow. Similar to many previous state-of-art systems, we treat the correction of determiner errors and preposition errors as a classification problem. Although some previous works (e.g. Rozovskaya et al. (2013)) use NPs and the head of NPs as Because seq is a word sequence, the maximization can be efficiently calculated using Viterbi algorithm (Forney Jr, 1973). To be specific, the Viterbi algorithm uses the transition scores and emission scores as its input. The transition scores in our model are the tri-gram probabilities from our tri-gram TNLM. The emission scores in our model are the candidate scores of each child: C1 .scores, ..., CK .scores, which have already been calculated. After the bottom-up calculation, we only need to look into the “ROOT” node to find the maximum score of the whole tree"
D14-1033,W13-3603,0,0.0336866,"Missing"
D14-1033,P11-1093,0,0.0198177,"ominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best c"
D14-1033,D10-1104,0,0.0132215,"all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The j"
D14-1033,D13-1074,0,0.485444,"Laboratory of Computational Linguistics (Peking University) Ministry of Education, China zhlongk@qq.com, wanghf@pku.edu.cn Abstract Recent research begins to focus on the error interaction problem. For example, Wu and Ng (2013) decodes a global optimized result based on the individual correction confidence of each kind of errors. The individual correction confidence is still based on the noisy context. Rozovskaya and Roth (2013) uses a joint modeling approach, which considers corrections in phrase structures instead of words. For dependencies that are not covered by the joint learning model, Rozovskaya and Roth (2013) uses the results of Illinois system in the joint inference. These results are still at word level and are based on the noisy context. These systems can consider error interactions, however, the systems are complex and inefficient. In both Wu and Ng (2013) and Rozovskaya and Roth (2013), Integer Linear Programming (ILP) is used for decoding a global optimized result. In the worst case, the time complexity of ILP can be exponent. State-of-art systems for grammar error correction often correct errors based on word sequences or phrases. In this paper, we describe a grammar error correction system"
D14-1033,N12-1068,0,0.0654496,"Missing"
D14-1033,P12-2039,0,0.0231028,"eterminer and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The joint learning and join"
D14-1033,P10-2065,0,0.0398457,"Missing"
D14-1033,W13-3601,0,0.0475951,"pecial module, two extra classification models are used to correct the determiner errors and preposition errors . The classifiers are also trained at tree node level. We take special care of these two kinds Introduction The task of grammar error correction is difficult yet important. An automatic grammar error correction system can help second language (L2) learners improve the quality of their writing. In recent years, there are various competitions devoted to grammar error correction, such as the HOO2011(Dale and Kilgarriff, 2011), HOO-2012(Dale et al., 2012) and the CoNLL-2013 shared task (Ng et al., 2013). There has been a lot of work addressing errors made by L2 learners. A significant proportion of the systems for grammar error correction train individual statistical models to correct each special kind of error word by word and ignore error interactions. These methods assume no interactions between different kinds of grammatical errors. In real problem settings errors are correlated, which makes grammar error correction much more difficult. 266 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 266–277, c October 25-29, 2014, Doha, Qatar. 20"
D14-1033,C08-1109,0,0.0185054,"ra scoring system to score extra kind of errors. 5 Related Works Early grammatical error correction systems use the knowledge engineering approach (Murata and Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011"
D14-1033,P13-1143,0,0.476252,"ed on the individual correction confidence of each kind of errors. The individual correction confidence is still based on the noisy context. Rozovskaya and Roth (2013) uses a joint modeling approach, which considers corrections in phrase structures instead of words. For dependencies that are not covered by the joint learning model, Rozovskaya and Roth (2013) uses the results of Illinois system in the joint inference. These results are still at word level and are based on the noisy context. These systems can consider error interactions, however, the systems are complex and inefficient. In both Wu and Ng (2013) and Rozovskaya and Roth (2013), Integer Linear Programming (ILP) is used for decoding a global optimized result. In the worst case, the time complexity of ILP can be exponent. State-of-art systems for grammar error correction often correct errors based on word sequences or phrases. In this paper, we describe a grammar error correction system which corrects grammatical errors at tree level directly. We cluster all error into two groups and divide our system into two modules correspondingly: the general module and the special module. In the general module, we propose a TreeNode Language Model t"
D14-1033,W13-3605,0,0.0116791,"post-process. 4 4.1 For the preposition errors, we only consider deletion and replacement of an existing preposition. The classification framework is similar to determiner errors. We consider classification on preposition nodes (nodes whose POS tag is preposition). We use prepositions as labels to indicate which preposition should be used. and use “∅” to denote that the preposition should be deleted. We use the same definition of LP and RP as the correction of determiner errors. Detailed feature templates we use to correct preposition errors are listed in table 6. Similar to the previous work(Xing et al., 2013), we find that adding more prepositions will not improve the performance in our experiments. Thus we only consider a fixed set of prepositions: {in, for, to, of, on}. Experiment Experiment Settings In the experiments, we use our parsed Gigaword corpus as the training data, use the training data provided by CoNLL-2013 as the develop data, and use the test data of CoNLL-2013 as test data directly. In the general module, the training data is used for the training of TreeNode Language Model. In the special module, the training data is used for training individual classification models. We use the"
D14-1033,W13-3604,0,0.0650416,"as the develop set to tune all parameters. Some sentences in the news texts use a different writing style against the sentences written by ESL learners. For example, sentences written by ESL learners seldom include dialogues between people, while very often news texts include paragraphs such as “‘I am frightened!’ cried Tom”. We use heuristic rules to eliminate the sentences in the Gigaword corpus that are less likely to appear in the ESL writing. The heuristic rules include deletWe should make it clear that we are not the first to use tree level correction models on ungrammatical sentences. Yoshimoto et al. (2013) uses a Treelet Language model (Pauls and Klein, 2012) to correct agreement errors. However, the performance of Treelet language model is not that good compared with the top-ranked system in CoNLL2013. The reason is that the production rules in the Treelet language model are based on complex contexts, which will exacerbate the data sparseness problem. The “context” in Treelet language model also include words ahead of treelets, which are sometimes unrelated to the current node. In contrast, our TreeNode Language model only needs to consider useful context words related to each node 1 2 268 htt"
D14-1033,N12-1067,0,\N,Missing
D14-1033,P11-1094,0,\N,Missing
D14-1033,C98-1082,0,\N,Missing
D14-1033,D12-1052,0,\N,Missing
D14-1147,P08-2016,0,0.026349,"ses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method CRF+GI DPLVM+GI BIEP Zhang et al. (2012) Our Result Top-1 Accuracy 0.5850 0.5990 0.5812 0.6205 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a ful"
D14-1147,P09-1039,0,0.0127905,"well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating"
D14-1147,W01-0516,0,0.6542,"(2012), which also uses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method CRF+GI DPLVM+GI BIEP Zhang et al. (2012) Our Result Top-1 Accuracy 0.5850 0.5990 0.5812 0.6205 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to"
D14-1147,C04-1197,0,0.047609,"ect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbrevia"
D14-1147,W06-1616,0,0.0362322,"h log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. We propose the MSU, which is more coarsegrained than character but more fine-grained than w"
D14-1147,I13-1073,1,0.679215,"different tagging methods and using ILP decoding. The x-axis represents the length of the full form. The y-axis represents top-1 accuracy. We find that our method works especially Effect of pruning raw pruned Accuracy 0.6103 0.6103 Average length 34.4 25.5 Time(s) 12.5 7.1 Table 14: Comparison of testing time of raw input and pruned input 3.5 Compare with the State-of-the-art Systems We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang 1411 et al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search engine result information to re-rank the original candidates. We do not use any extra web resources. Because Zhang et al. (2012) uses web information only in its second stage, we use “BIEP”(the tag"
D14-1147,P09-1102,1,0.913397,"M SU Set = empty set For each word w in L: If Length(w) ≤ 2 Add w to M SU Set End if End for For each word w in L: If Length(w) &gt; 2 and no word x in M SU Set is a substring of w Add w to M SU Set End if End for Return M SU Set Table 3: Algorithm for collecting MSUs from the PKU corpus 2.2.4 Sequence Labeling Model The MSU-based method gives each MSU an extra indicative label. Therefore any sequence labeling model is appropriate for the method. Previous works showed that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in our system. For a given full form’s MSU list, many candidate abbreviations are generated by choosing the k-best results of the CRFs. We can use the forward-backward algorithm to calculate the probability of a specified tagging result. To reduce the searching complexity in the ILP decoding process, we delete those candidate tagged sequences with low probability. 2.3 Substring Based Tagging As mentioned in the introduction, the sequence labeling method, no matter character-based or MSU-based, perform badly when the “character dupl"
D14-1147,C12-1187,1,0.815659,"Tj have a same position but the position gets different labels, then xi + zj ≤ 1 ∀Si , Sj ∈ S if Si and Sj have a same position but the position gets different labels, then z i + zj ≤ 1 ∀Si , Sj ∈ S if the last character Si keeps is the same as the first character Sj keeps, then z i + zj ≤ 1 Table 6: Constraints for ILP Type Noun Phrase Organization Coordinate phrase Proper noun Full form 优秀稿件(Excellent articles) 作家协会(Writers’ Association) 受伤死亡(Injuries and deaths) 传播媒介(Media) Abbreviation 优稿 作协 伤亡 传媒 Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun) Zhang et al. (2012). The top-K accuracy measures what percentage of the reference abbreviations are found if we take the top N candidate abbreviations from all the results. In our experiment, top-10 candidates are considered in re-ranking phrase and the measurement used is top-1 accuracy (which is the accuracy we usually refer to) because the final aim of the algorithm is to detect the exact abbreviation. CRF++7 , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve8 , which is also an open source tool. The parameters of these tools are tuned through cross-val"
D14-1147,P12-1111,0,0.0222466,"pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. We propose the MSU, which i"
D14-1147,W05-1304,0,0.426492,"set For each word w in L: If Length(w) ≤ 2 Add w to M SU Set End if End for For each word w in L: If Length(w) &gt; 2 and no word x in M SU Set is a substring of w Add w to M SU Set End if End for Return M SU Set Table 3: Algorithm for collecting MSUs from the PKU corpus 2.2.4 Sequence Labeling Model The MSU-based method gives each MSU an extra indicative label. Therefore any sequence labeling model is appropriate for the method. Previous works showed that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in our system. For a given full form’s MSU list, many candidate abbreviations are generated by choosing the k-best results of the CRFs. We can use the forward-backward algorithm to calculate the probability of a specified tagging result. To reduce the searching complexity in the ILP decoding process, we delete those candidate tagged sequences with low probability. 2.3 Substring Based Tagging As mentioned in the introduction, the sequence labeling method, no matter character-based or MSU-based, perform badly when the “character duplication” phenomenon exis"
D14-1193,W07-1013,0,\N,Missing
D14-1202,P06-2005,0,0.0149982,"ses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph rand"
D14-1202,P08-2016,0,0.0319831,"backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assump"
D14-1202,P13-1155,0,0.058357,"is shown in figure 1. (in later steps, vi ) and randomly walks to another node vj with a transition probability pij . In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with w the formula pij = P ijwil . When the graph ranl dom walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-ranking Figure 1: An exampl"
D14-1202,D13-1199,0,0.0191265,"later steps, vi ) and randomly walks to another node vj with a transition probability pij . In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with w the formula pij = P ijwil . When the graph ranl dom walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-ranking Figure 1: An example of the bipartite"
D14-1202,D13-1008,0,0.0142229,"tion”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph random walk to reduce the search space. After we"
D14-1202,nenadic-etal-2002-automatic,0,0.0768595,"Missing"
D14-1202,W01-0516,0,0.891733,"China). Another ambiguity is “清华大学”(Tsinghua University), which has two abbreviations “清 大” and “清 华”. This happens because the full form itself is ambiguous. Word sense disambiguation can be performed first to handle this kind of problem. 6 Related Work Abbreviation generation has been studied during recent years. At first, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of"
D14-1202,P06-1010,0,0.0196934,"xample bipartite graph is shown in figure 1. (in later steps, vi ) and randomly walks to another node vj with a transition probability pij . In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with w the formula pij = P ijwil . When the graph ranl dom walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-"
D14-1202,I13-1073,1,0.58627,"eviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbre"
D14-1202,P09-1102,1,0.839211,"he top-5 accuracy of the candidate generation phase Table 4. We can see that, just like the case of using other feature alone, using the score of random walk alone is far from enough. However, the first 5 candidates contain most of the correct answers. We use the top-5 candidates plus another 5 candidates in the re-ranking phase. We choose the character tagging method as the baseline method. The character tagging strategy is widely used in the abbreviation generation task (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). We choose the ‘SK’ labeling strategy which is used in Sun et al. (2009); Zhang et al. (2012). The ‘SK’ labeling strategy gives each character a label in the character sequence, with ‘S’ represents ’Skip’ and ‘K’ represents ‘Keep’. Same with Zhang et al. (2012), we use the Conditional Random Fields (CRFs) model in the sequence labeling process. The baseline method mainly uses the character context information to generate the candidate abbreviation. To be fair we use the same feature set in Sun et al. (2009); Zhang et al. (2012). One drawback of the sequence labeling method is that it relies heavily on the character context in the full form. With the number of new"
D14-1202,D13-1007,0,0.0140871,"o find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph random walk to reduce the search space. After we get the candidate lists, we"
D14-1202,C12-1187,1,0.745317,"iversity), whose abbreviations correspond to “北 大” and ‘清 华’ respectively. Although sharing a similar character context, the third character ‘大’ is kept in the first case and is skipped in the second case. We believe that a better way is to extract these abbreviation-full pairs from a natural text corpus where the full form and its abbreviation co-exist. Therefore we propose a two stage method. The first stage generates a list of candidates given a large corpus. To reduce the search space, we adopt 1 Details of the difference between English and Chinese abbreviation prediction can be found in Zhang et al. (2012). Full form Status Result 香 Skip 港 Keep 港 大 Keep 大 学 Skip Table 1: The abbreviation “港大” of the full form “香港大学” (Hong Kong University) graph random walk to give a coarse-grained ranking and select the top-ranked ones as the candidates. Then we use a similarity sensitive reranking method to decide the final result. Detailed description of the two parts is shown in the following sections. 3 3.1 Candidate Generation through Graph Random Walk Candidate Generation and Graph Representation Chinese abbreviations are sub-sequences of the full form. We use a brute force method to select all strings in"
D14-1202,W05-1304,0,0.719705,"g data One advantage of our method is that it only requires weak supervision. The baseline method needs plenty of manually collected full-abbreviation pairs to learn a good model. In our method, the candidate generation and coarse-grained ranking is totally unsupervised. The re-ranking phase needs training instances to decide the parameters. However we can use a very small amount of training data to get a reasonably good model. Figure 2 shows the result 5.5 Comparison with previous work We compare our method with the method in the previous work DPLVM+GI in Sun et al. (2009), which outperforms Tsuruoka et al. (2005); Sun et al. (2008). We also compare our method with the web-based method CRF+WEB in Zhang et al. (2012). Because the comparison is performed on different corpora, we run the two methods on our data. Table 6 shows the top-1 accuracy. We can see that our method outperforms the previous 1887 database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quite laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. methods. Syste"
D15-1099,D14-1193,1,0.645722,"Missing"
D15-1099,W07-1013,0,0.033082,"els S exactly. The 0/1 loss is defined as follows: Hammingloss = x) 6= y ) 0/1loss = I(h(x (6) Let pj and rj denote the precision and recall for the j-th label. The macro-averaged F score is a harmonic mean between precision and recall, defined as follows: Experiments 3.1 n 3782 978 1702 28596 Datasets We perform experiments on four real world data sets: 1) the first data set is Slashdot (Read et al., 2011). The Slashdot data set is concerned about predicting multiple labels given science and technology news titles and partial blurbs mined from Slashdot.org. 2) the second data set is Medical (Pestian et al., 2007). This data set involves the assignment of ICD-9-CM codes to radiology reports. 3) The third data set is Enron. The enron data set is a subset of the Enron Email Dataset, as labelled by the UC Berkeley Enron Email Analysis Project2 . It is concerned about classifying emails into some categories. 4) the fourth data set m F score = 1 X 2 ∗ pj ∗ rj m pj + rj (7) i=j 3.3 Method Setup In this paper, we focus on the predictions-asfeatures style methods, and use CC and LEAD as the baselines. Our methods are JCC and JLEAD. JCC(JLEAD) is CC(LEAD) trained by our joint algorithm and we compare JCC(JLEAD)"
D17-1139,A00-2004,0,0.70769,"l information for tasks such as information retrieval, topic tracking etc (Purver, 2011). Due to the lack of large scale annotated topic segmentation dataset, previous work mainly focus on unsupervised models to measure the coherence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of"
D17-1139,N13-1019,0,0.463244,"unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of the following two limitations. First, it is not precise to measure coherence with text similarity, since text similarity is just one aspect to influence coherence. Second, many assumptions and manually set parameters are usually involved in the complex modeling techniques, due to the abs"
D17-1139,D08-1035,0,0.938593,"rence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of the following two limitations. First, it is not precise to measure coherence with text similarity, since text similarity is just one aspect to influence coherence. Second, many assumptions and manually set parameters are usually involved in the"
D17-1139,J97-1003,0,0.968799,"h easier to navigate. It also provides helpful information for tasks such as information retrieval, topic tracking etc (Purver, 2011). Due to the lack of large scale annotated topic segmentation dataset, previous work mainly focus on unsupervised models to measure the coherence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling tex"
D17-1139,D14-1162,0,0.0809434,"Neural Network News 184 Lecture 120 Report 160 Biography 400 Table 1: Overview of four datasets. score Figure 1: Semantic Coherence Neural Network Baselines To compare with our method, TextTiling (Hearst, 1997), TopicTiling (Riedl and Biemann, 2012b) and BayesSeg (Eisenstein and Barzilay, 2008) are adopted as three baselines. We use open source implementations of TextTiling2 and TopicTiling3 , and results of BayesSeg are from (Jeong and Titov, 2010). Hyperparameters Our neural network implementation is based on Tensorflow (Abadi et al., 2015). We use pre-trained 50 dimensional Glove vectors (Pennington et al., 2014)4 for word embeddings initialization. Each text pair consists of 2 text segments, and each text segment consists of To model the text pair instances, we develop a symmetric convolutional neural network (CNN) architecture, as shown in Figure 1. Our model consists of two symmetric CNN models, and the two CNNs share their network configuration and 1 We do not compare with MultiSeg model proposed by (Jeong and Titov, 2010), since our model is for singledocument topic segmentation while MultiSeg is for multidocument topic segmentation. 2 https://github.com/nltk/nltk/tree/develop/nltk/tokenize 3 htt"
D17-1139,J02-1002,0,0.424949,"fine-tuning, etc. no more than 3 sentences. Stop words and digits are removed from input text, and all words are converted to lowercase. We pad input sequence to 40 tokens. In order to capture information of different granularity, convolution window size of both 2 and 3 are used, with 64 filters for each window size. L2 regularization coefficient is set to 0.001. Adam algorithm (Kingma and Ba, 2014) is used for loss function minimization. We set α to 0.7 for pointwise ranking. Evaluation System performance is evaluated according to three metrics: Pk (Beeferman et al., 1999), WindowDiff(W D) (Pevzner and Hearst, 2002) and F 1 score. Pk and W D are calculated based on sliding windows, and can assign partial score to incorrect segmentation. Note that Pk and W D are penalty metrics, smaller value means better performance. 3.2 Results and Analysis Experimental results are shown in Table 2. Our proposed model is examined in 4 different settings, including whether to use pointwise ranking or pairwise ranking algorithm, and whether to fine-tune word embeddings or not. The best model Ours-pointwise-static is able to achieve better or competitive performance compared to BayesSeg and TopicTiling according to all thr"
D17-1139,W12-3307,0,0.696022,"ch as information retrieval, topic tracking etc (Purver, 2011). Due to the lack of large scale annotated topic segmentation dataset, previous work mainly focus on unsupervised models to measure the coherence between two textual segments. The intuition behind unsupervised models is that two adjacent segments from the same topic are more coherent than those from different topics. Under this intuition, one direction of research attempts to measure coherence by computing text similarity. The typical methods include TextTiling (Hearst, 1997) and its variants, such as C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012b) etc. The other direction of research develops topic modeling techniques to explore topic representation of text and topic change between textual segments (Yamron et al., 1998; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012a; Du et al., 2013; Jameel and Lam, 2013). With carefully designed generative process and efficient inference algorithm, topic models are able to model coherence as latent variables and outperform lexical similarity based models. Though unsupervised models make progress in modeling text coherence, they mostly suffer from one of the following two limitations. First,"
D17-1139,Q16-1019,0,\N,Missing
D17-1192,D14-1092,0,0.0222315,"d E is the error (noise) matrix  E= EXtrain EXtest EYtrain 0  . (5) This error (noise) modeling approach has been successfully applied to distantly supervised relation extraction. However, it still has clear limitations. The noise model is limited to a single source without considering the intrinsic clustering structures of data. In addition, the true rank is usually hard to determine, for adaptively modeling the correlations among features and labels. 2.2 Nonparametric Bayesian Modeling The use of nonparametric Bayesian modeling has been widely adopted in Natural Language Processing (NLP) (Chen et al., 2014). Instead of imposing assumptions that might be wrong, it “lets the data speak for itself”, without requiring optimizing parameters blindly by hands (Blei and Jordan, 2004). To take advantage of these merits, we here adopt it for the task, with the following motivations: Motivation 1: Adaptive Noise-Clustered Attention. The goal is to find an adaptive cluster specific noise parameterization for the complex noisy corpus, without making overly strong assumptions about the noise distribution in real applications. Motivation 2: Adaptive Latent Feature Space Selection. The goal is to automatically"
D17-1192,P14-1079,0,0.0818208,"Peking University) Ministry of Education, China {zqicl,wanghf}@pku.edu.cn Abstract For the task of relation extraction, distant supervision is an efficient approach to generate labeled data by aligning knowledge base with free texts. The essence of it is a challenging incomplete multi-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top-precision improvements over the traditional state-of-the-art approaches. 1 Figure 1: Aligned Example (Fan et al., 2014): the relation instances related to the entity pair hBarackObama, U.S.i in the KB, and its mentions in the free text. Introduction To efficiently generate structured relation information from free texts, the research on distantly supervised Relation Extraction (RE) (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011) has been attracting much attention, because it can greatly reduce the manual annotation for training. It essentially based on the assumption that the relation between two entities in a Knowledge Base (KB), is also likely hold within a sentence that mentions the two ent"
D17-1192,P14-2117,0,0.0292843,"Missing"
D17-1192,P11-1055,0,0.0389753,"e and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top-precision improvements over the traditional state-of-the-art approaches. 1 Figure 1: Aligned Example (Fan et al., 2014): the relation instances related to the entity pair hBarackObama, U.S.i in the KB, and its mentions in the free text. Introduction To efficiently generate structured relation information from free texts, the research on distantly supervised Relation Extraction (RE) (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011) has been attracting much attention, because it can greatly reduce the manual annotation for training. It essentially based on the assumption that the relation between two entities in a Knowledge Base (KB), is also likely hold within a sentence that mentions the two entities in free texts. This assumption plays a crucial role in distant supervision, which is quite effective in real applications. However, the assumption of distant alignment can also lead to the noisy training corpus problem (Fan et al., 2014), which is challenging for the task as follows: i) Noisy features. Not all relations ex"
D17-1192,P09-1113,0,0.741448,"i-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top-precision improvements over the traditional state-of-the-art approaches. 1 Figure 1: Aligned Example (Fan et al., 2014): the relation instances related to the entity pair hBarackObama, U.S.i in the KB, and its mentions in the free text. Introduction To efficiently generate structured relation information from free texts, the research on distantly supervised Relation Extraction (RE) (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011) has been attracting much attention, because it can greatly reduce the manual annotation for training. It essentially based on the assumption that the relation between two entities in a Knowledge Base (KB), is also likely hold within a sentence that mentions the two entities in free texts. This assumption plays a crucial role in distant supervision, which is quite effective in real applications. However, the assumption of distant alignment can also lead to the noisy training corpus problem (Fan et al., 2014), which is challenging for the task as fol"
D17-1192,D15-1204,0,0.0529287,"enate-of(Barack Obama, U.S.)) is missing in the knowledge base. Such analogous cases are also common in real applications. iii) Sparse features. Sophisticated features extracted from the mentions can result in a large number of sparse features (Riedel et al., 2013). The generalization ability of feature based prediction models will be badly hurt, when the features do not match between testing and training. To tackle the problem, we develop a novel distant supervision approach from a nonparametric Bayesian perspective (Blei et al., 2016), along with the previously most effective research line (Petroni et al., 2015) of using matrix completion (Fan et al., 2014) for relation extraction. Our goal is to design a noise-tolerant relation extraction model for distantly supervised corpus with noise and sparsity problems. Different from (Fan et al., 2014) as one state-of-the-art method in this line, we model noisy data corpus using adaptive variance modeling approach (Chen et al., 2015), based on Dirichlet Process (Blei and Jordan, 2004) instead of a fixed way of controlling complex noise weighting. To the best of our knowledge, we are 1808 Proceedings of the 2017 Conference on Empirical Methods in Natural Langu"
D17-1192,N13-1008,0,0.33367,"on problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top-precision improvements over the traditional state-of-the-art approaches. 1 Figure 1: Aligned Example (Fan et al., 2014): the relation instances related to the entity pair hBarackObama, U.S.i in the KB, and its mentions in the free text. Introduction To efficiently generate structured relation information from free texts, the research on distantly supervised Relation Extraction (RE) (Mintz et al., 2009; Riedel et al., 2013; Hoffmann et al., 2011) has been attracting much attention, because it can greatly reduce the manual annotation for training. It essentially based on the assumption that the relation between two entities in a Knowledge Base (KB), is also likely hold within a sentence that mentions the two entities in free texts. This assumption plays a crucial role in distant supervision, which is quite effective in real applications. However, the assumption of distant alignment can also lead to the noisy training corpus problem (Fan et al., 2014), which is challenging for the task as follows: i) Noisy featur"
D17-1192,Q13-1030,0,0.0189361,"an also lead to the noisy training corpus problem (Fan et al., 2014), which is challenging for the task as follows: i) Noisy features. Not all relations existed in a KB keep the same meaning of that relation for the corresponding entities in a free text. For example, the second relation mention in Figure 1 does not explicitly describe any relation instance, so features extracted from this sentence can be noisy. Such analogous cases commonly exist in feature extraction. ii) Incomplete labels. Similar to noisy features, the generated label can be incomplete due to the incomplete knowledge base (Ritter et al., 2013). For example, the fourth relation mention in Figure 1 should be labeled by the relation Senate-of. However, the corresponding relation instance (Senate-of(Barack Obama, U.S.)) is missing in the knowledge base. Such analogous cases are also common in real applications. iii) Sparse features. Sophisticated features extracted from the mentions can result in a large number of sparse features (Riedel et al., 2013). The generalization ability of feature based prediction models will be badly hurt, when the features do not match between testing and training. To tackle the problem, we develop a novel d"
D17-1192,D12-1042,0,0.0748599,"Missing"
D17-1192,D15-1203,0,0.027472,"cus on the same noisy corpus problem. Although from different perspectives, we study it along with the same line of using matrix factorization (Petroni et al., 2015) for relation extraction. In this line, (Riedel et al., 2013) initially considered the task as a matrix factorization problem. Their method consists of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, the data noise brought by the assumption of distant supervision (Mintz et al., 2009), is not considered in the work. Another line addressing the problem uses deep neural networks (Zeng et al., 2015; Wang et al., 2015). The difference is that it is a supervised learning approach, while our focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously. In addition, (Han and Sun, 2016) explored Markov logic technique to enrich supervision knowledge, which can incorporate indirect supervision globally. Our method could be further augmented by that idea, using additional logical constraint to reduce the uncertainty for the clustered noise modeling. 5 Conclusion In this paper, building on recent advances from the nonparamet"
D18-1013,P82-1020,0,0.779639,"Missing"
D18-1013,P02-1040,0,0.10125,"ing mechanism to make the model adaptively learn to adjust the balance: γt = σ(S(st ) − S(rt )) (14) ct = γt st + (1 − γt )rt (15) We report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems. They both incorporate the consensus of a set of references for an example. BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation. ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015"
D18-1013,W04-1013,0,0.0862656,"s using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems. They both incorporate the consensus of a set of references for an example. BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation. ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015). where σ is the sigmoid function, γt ∈ [0, 1] indicates how important the topic attention is compared to the visual attention, and S"
D18-1013,N03-1020,0,0.165123,"15) We report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems. They both incorporate the consensus of a set of references for an example. BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation. ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015). where σ is the sigmoid function, γt ∈ [0, 1] indicates how important the topic attention is compared to the visual atten"
D18-1013,C18-1276,1,0.878977,"Missing"
D18-1013,Q14-1006,0,0.0245347,"h the batch size of 80. The learning rate for the LSTM is 0.0004. Then, we switch to jointly train the full model with a learning rate of 0.00001, which exponentially decays with the number of epochs so that it is halved every 50 epochs. We also use momenExperiment We describe the datasets and the metrics used for evaluation, followed by the training details and the evaluation of the proposed approach. 4.1 Settings Datasets and Metrics There are several datasets containing images and their captions. We report results on the popular Microsoft COCO (Chen et al., 2015) dataset and the Flickr30k (Young et al., 2014) dataset. They contain 123,287 images and 31,000 images, respectively, and each image is annotated with 5 sentences. We report results using the widely-used publicly-available splits in the work of Karpathy and Li (2015). There are 5,000 images each in the validation set and the test set for COCO, 1,000 images for Flickr30k. 3 141 We use the pre-trained model from torchvision. Flickr30k SPICE CIDEr METEOR ROUGE-L BLEU-4 HardAtt (Xu et al., 2015) SCA-CNN (Chen et al., 2017) ATT-FCN (You et al., 2016) SCN-LSTM (Gan et al., 2017) AdaAtt (Lu et al., 2017) NBT (Lu et al., 2018) 0.145 0.156 0.531 0."
D18-1013,P18-1090,1,0.871032,"Missing"
D18-1013,D18-1462,1,0.87487,"Missing"
D18-1072,D14-1162,0,0.0800625,"Missing"
D18-1072,P15-1152,0,0.0422368,"Missing"
D18-1072,N15-1020,0,0.0556934,"Missing"
D18-1072,D14-1179,0,0.00964984,"Missing"
D18-1072,W17-5526,0,0.0489816,"Missing"
D18-1072,E17-1042,0,0.0666184,"Missing"
D18-1408,D15-1075,0,0.103344,"ormed at the phrase level instead of the sentence level, thus the memory consumption reduces rapidly as the number of phrases increases. Furthermore, a gated memory component is employed to refine word representations hierarchically by incorporating longer-term context dependencies. As a result, syntactic information can be integrated into the model without expensive recursion computation. At last, multi-dimensional attention is applied on the refined word representations to obtain the final sentence representation. Following Conneau et al. (2017), we trained our sentence encoder on the SNLI (Bowman et al., 2015) dataset, and evaluate the quality of the obtained universal sentence representations on a wide range of transfer tasks. The SNLI dataset is extremely suitable for training sentence encoders because it is the largest high-quality humanannotated dataset that involves reasoning about the semantic relationships within sentences. The main contributions of our work can be summarized as follows: • We propose the Phrase-level Self-Attention mechanism (PSA) for contextualization. The memory consumption can be reduced because self-attention is performed at the phrase level instead of the sentence level"
D18-1408,D16-1053,0,0.178075,"with itself. The output of the attention mechanism is a weighted sum of embeddings from all tokens for each token in the phrase: "" # l X exp (aij ) p˜i = pj (2) Pl k=1 exp (aik ) j=1 where means point-wise product. Note that the alignment score for each token pair is a vector rather than a scalar in the multi-dimensional attention. The final output of Phrase-level Self-Attention is obtained by comparing each input representation with its attention-weighted counterpart. We use a comparison function based on absolute difference and element-wise multiplication which was similar to Wang and Jiang (2016). This comparison function has the advantage of measuring the semantic similarity or relatedness of two sequences. ci = σ (W c [|pi − p˜i |; pi p˜i ] + bc ) where W c ∈ Rd×2d and ba ∈ Rd are parameters to be learned. ci is the representation for the i-th word in the phrase that captures local dependencies within the phrase. At last, we put together the Phrase-level SelfAttention results for non-overlapping phrases from the same phrase division of a sentence. For the t-th phrase division we can get C (t) = [c1 , . . . , cls ], the phrase-level self-attention results for the sentence from the t-"
D18-1408,L18-1269,0,0.0148144,"fied. Minibatch size is set to 16. The number of levels T is fixed to 3 in all of our experiments. The syntactic parse trees of SNLI are provided within the corpus. parse trees for all test corpus are produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), the same parser that produced parse trees for the SNLI dataset. To train the model, Adadelta optimizer (Zeiler, 2012) with a learning rate of 0.75 is used on the SNLI dataset. The dropout (Srivastava et al., 2014) rate and L2 regularization weight decay factor γ are set to 0.5 and 5e-5. To test the model, the SentEval toolkit (Conneau and Kiela, 2018) is used as the evaluation pipeline for fairer comparison. 3.2 Training Setting Natural language inference (NLI) is a fundamental task in the field of natural language processing that involves reasoning about the semantic relationship between two sentences, which makes it a suitable task to train sentence encoding models. We conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The dataset has 570k human-annotated sentence pairs, each labeled with one of the following pre-defined relationships: Entailment (the premise entails the hypothesis), Cont"
D18-1408,D17-1070,0,0.240428,"lit into multiple phrases based on parse tree, selfattention is performed at the phrase level instead of the sentence level, thus the memory consumption reduces rapidly as the number of phrases increases. Furthermore, a gated memory component is employed to refine word representations hierarchically by incorporating longer-term context dependencies. As a result, syntactic information can be integrated into the model without expensive recursion computation. At last, multi-dimensional attention is applied on the refined word representations to obtain the final sentence representation. Following Conneau et al. (2017), we trained our sentence encoder on the SNLI (Bowman et al., 2015) dataset, and evaluate the quality of the obtained universal sentence representations on a wide range of transfer tasks. The SNLI dataset is extremely suitable for training sentence encoders because it is the largest high-quality humanannotated dataset that involves reasoning about the semantic relationships within sentences. The main contributions of our work can be summarized as follows: • We propose the Phrase-level Self-Attention mechanism (PSA) for contextualization. The memory consumption can be reduced because self-atten"
D18-1408,P82-1020,0,0.835639,"Missing"
D18-1408,P14-1062,0,0.0554781,"of NLP’s next challenges has become the hunt for universal sentence encoders. The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks. The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for downstream NLP tasks, especially for those with relatively small datasets. Previous models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation. RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient. CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017). Fully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train. Vaswani et al. (2017) pr"
D18-1408,D14-1181,0,0.00677435,"rsal sentence encoders. The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks. The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for downstream NLP tasks, especially for those with relatively small datasets. Previous models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation. RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient. CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017). Fully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train. Vaswani et al. (2017) proposed the multihead attention to proje"
D18-1408,P03-1054,0,0.0622152,"re hashed to one of 128 random embeddings initialized by uniform distribution between (-0.05, 0.05). All the word embeddings remain fixed during training. Hidden dimension d is set to 300. All other parameters are initialized with Glorot normal initialization (Glorot and Bengio, 2010). Activation function σ (·) is ELU (Clevert et al., 2015) if not specified. Minibatch size is set to 16. The number of levels T is fixed to 3 in all of our experiments. The syntactic parse trees of SNLI are provided within the corpus. parse trees for all test corpus are produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), the same parser that produced parse trees for the SNLI dataset. To train the model, Adadelta optimizer (Zeiler, 2012) with a learning rate of 0.75 is used on the SNLI dataset. The dropout (Srivastava et al., 2014) rate and L2 regularization weight decay factor γ are set to 0.5 and 5e-5. To test the model, the SentEval toolkit (Conneau and Kiela, 2018) is used as the evaluation pipeline for fairer comparison. 3.2 Training Setting Natural language inference (NLI) is a fundamental task in the field of natural language processing that involves reasoning about the semantic relationship between tw"
D18-1408,P16-2022,0,0.0834052,"ce encoders. The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks. The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for downstream NLP tasks, especially for those with relatively small datasets. Previous models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation. RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient. CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017). Fully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train. Vaswani et al. (2017) proposed the multihead attention to project a sentence to mu"
D18-1408,D15-1279,0,0.0762867,"us regarding the appropriate evaluations for universal sentence representations. To facilitate comparison, we use the same sentence evaluation tool as Conneau et al. (2017) to automate evaluation on all the tasks mentioned in this paper. The transfer tasks used in evaluation can be concluded in the following classes: sentence classification (MR, CR, MPQA, SUBJ, SST2, SST5, TREC), natural language inference (SICKE, SICK-R), semantic relatedness (STS14) and paraphrase detection (MRPC). Table 1 presents some statistics about the datasets 2 . 3.4 Model BiLSTM-Max AdaSent TBCNN DiSAN PSAN • TBCNN (Mou et al., 2015) is a tree-based CNN model where convolution is applied over the parse tree. 2 For further information on the datasets, please refer to Conneau et al. (2017). SNLI 84.5 83 .4 82.1 85.6 86.1 Micro 85.2 82.0 81.1 84.7 85.7 Macro 83.7 80.9 79.3 83.4 84.5 various sentence encoders. dim: the size of sentence representation. |θ|: the number of parameters. Test accuracies on SNLI, micro and macro averages of accuracies of dev set on transfer tasks are chosen as evaluation metrics. • DiSAN (Shen et al., 2017) is composed of a directional self-attention block with temporal order encoded, and a multi-di"
D18-1408,E17-1038,0,0.014888,"STM (Tai et al., 2015; Zhu et al., 2015) composed its hidden state from an input vector and the hidden states of arbitrarily many child units. In Tree-based CNN (Mou et al., 2015, 2016), a set of subtree feature detectors slide over the parse tree of a sentence, and a max-pooling layer is utilized to aggregate information along different parts of the tree. Apart from the models that use parse information, there have been several researches that aimed to learn the hierarchical latent structure of text by recursively composing words into sentence representation. Among them, neural tree indexer (Munkhdalai and Yu, 2017b) utilized LSTM or attentive node composition function to construct full n-ary tree for input text. Gumbel TreeLSTM (Choi et al., 2018) used Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically. A major drawback of these models is that the recursion computation can be expensive and hard to be processed in batches. 6 Conclusion We propose the Phrase-level Self-Attention Networks (PSAN), a fully attention-based model that can utilize syntactic information for universal sentence encoding. By applying self-attention at the phrase level, we can filter ou"
D18-1408,E17-1002,0,0.0162227,"STM (Tai et al., 2015; Zhu et al., 2015) composed its hidden state from an input vector and the hidden states of arbitrarily many child units. In Tree-based CNN (Mou et al., 2015, 2016), a set of subtree feature detectors slide over the parse tree of a sentence, and a max-pooling layer is utilized to aggregate information along different parts of the tree. Apart from the models that use parse information, there have been several researches that aimed to learn the hierarchical latent structure of text by recursively composing words into sentence representation. Among them, neural tree indexer (Munkhdalai and Yu, 2017b) utilized LSTM or attentive node composition function to construct full n-ary tree for input text. Gumbel TreeLSTM (Choi et al., 2018) used Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically. A major drawback of these models is that the recursion computation can be expensive and hard to be processed in batches. 6 Conclusion We propose the Phrase-level Self-Attention Networks (PSAN), a fully attention-based model that can utilize syntactic information for universal sentence encoding. By applying self-attention at the phrase level, we can filter ou"
D18-1408,W17-5308,0,0.237399,"al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation. RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient. CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017). Fully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train. Vaswani et al. (2017) proposed the multihead attention to project a sentence to multiple semantic subspaces, then apply self-attention in each subspace and concatenate the attention results. Shen et al. (2017) proposed the directional self-attention, they apply forward and backward masks to the alignment score matrix to encode temporal order information, and computed attention at feature level to select the features that can best describe the word’s meaning in given context. Effective as their models are, the memory required to store the alignment scores of all the token pairs grows quadratically with the sentence"
D18-1408,D16-1244,0,0.165817,"Missing"
D18-1408,D14-1162,0,0.0887343,"(T ) v= mi Pl j=1 exp (ej ) i=1 where W g , W m ∈ Rd×d and bg , bm ∈ Rd are parameters to be learned. After this step, the refined context-aware sentence representation is compressed into a fixed-length vector. 3 Experiments In this section, we conduct a plethora of experiments to study the effectiveness of the PSAN model. Following Conneau et al. (2017), we train our sentence encoder using the SNLI dataset, and evaluate it across a variety of NLP tasks including sentence classification, natural language inference and sentence textual similarity. 3.1 Model Configuration 300-dimensional GloVe (Pennington et al., 2014) word embeddings (Common Crawl, uncased) are used to represent words. Following Parikh et al. (2016), out-of-vocabulary words are hashed to one of 128 random embeddings initialized by uniform distribution between (-0.05, 0.05). All the word embeddings remain fixed during training. Hidden dimension d is set to 300. All other parameters are initialized with Glorot normal initialization (Glorot and Bengio, 2010). Activation function σ (·) is ELU (Clevert et al., 2015) if not specified. Minibatch size is set to 16. The number of levels T is fixed to 3 in all of our experiments. The syntactic parse"
D18-1408,C14-1008,0,0.0330506,"ecome the hunt for universal sentence encoders. The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks. The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for downstream NLP tasks, especially for those with relatively small datasets. Previous models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation. RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient. CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017). Fully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train. Vaswani et al. (2017) proposed the multihead attenti"
D18-1408,P15-1150,0,0.265044,"Missing"
D18-1408,D13-1170,0,0.0110736,"o encode temporal order information, and computed attention at feature level to select the features that can best describe the word’s meaning in given context. Effective as their models are, the memory required to store the alignment scores of all the token pairs grows quadratically with the sentence length. Furthermore, the syntactic property that is intrinsic to natural language is not considered at all. Language is inherently tree structured, and the meaning of a sentence comes largely from composing the meanings of subtrees (Chomsky, 1957). Previous syntactic tree-based sentence encoders (Socher et al., 2013; Tai et al., 2015) mainly rely on recursive networks. Although the composition3729 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3729–3738 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ality can be explicitly modeled, their models need expensive recursion computation and are hard to be trained by batched gradient descent methods. In this paper, we propose the Phrase-level SelfAttention Networks (PSAN), for RNN/CNN-free sentence encoding, it inherits all the advantages of fully attention-based"
D18-1504,D15-1141,0,0.0214216,"res, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or collapsed methods. With the success of deep learning techniques, neural networks have demonstrated their capability of sequence labeling (Collobert et al., 2011; Pei et al., 2014; Chen et al., 2015). However, Zhang et al. (2015) only use word embeddings to enrich features without taking full advantages of neural networks’ potential in automatically capturing important sequence labeling features like long distance dependencies and character-level features. To make better use of neural networks to explore appropriate character-level features and high-level semantic features for the two tasks, we design a hierarchical multi-layer bidirectional gated recurrent units networks (HMBiGRU) which uses a multi-layer Bi-GRU to automatically learn character features (e.g. capitalization, noun suffix,"
D18-1504,W17-1106,0,0.0787827,"formance, we adopt Precision, Recall and F-measure. In our experiments, we evaluate the performance of detecting targets (DT) and targeted sentiment analysis (TSA) which a target is taken as correct only when the boundary and the sentiment are both correctly recognized. We also adopt Precision, Recall and F-measure used in Zhang et al. (2015) to evaluate our model. The reason why we don’t compare with Mitchell et al. (2013) is that they only evaluate the beginning of targets along with the sentiment expressed towards it. In our experiments, we use embeddings from Pennington et al. (2014)2 and Cieliebak et al. (2017)3 for English words and Spanish words respectively. The character embeddings are initialized by Xavier (Glorot and Bengio, 2010) and their dimension is 50. In our model, all unknown words, weight matrices and biases are initialized by Xavier Glorot and Bengio (2010). The dimensions of the character-level and word-level hidden states in MBi-GRU are set to 300 and 600 respectively. The layer number of multi-layer bidirectional GRU is set to 2. To avoid overfitting, we adopt dropout on embeddings, sfi and tfi , and the dropout rate is set to 0.5. The word embeddings and character embeddings will"
D18-1504,P13-2147,0,0.0298715,"al and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA rega"
D18-1504,C10-1074,0,0.200297,"the hierarchical multi-layer bidirectional gated recurrent units (HMBi-GRU) model to learn abstract features for both tasks, and we propose a HMBi-GRU based joint model which allows the target label of word to have influence on its sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HMBi-GRU in learning abstract features. 1 Introduction Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to"
D18-1504,D14-1162,0,0.0862383,"et. To evaluate the system performance, we adopt Precision, Recall and F-measure. In our experiments, we evaluate the performance of detecting targets (DT) and targeted sentiment analysis (TSA) which a target is taken as correct only when the boundary and the sentiment are both correctly recognized. We also adopt Precision, Recall and F-measure used in Zhang et al. (2015) to evaluate our model. The reason why we don’t compare with Mitchell et al. (2013) is that they only evaluate the beginning of targets along with the sentiment expressed towards it. In our experiments, we use embeddings from Pennington et al. (2014)2 and Cieliebak et al. (2017)3 for English words and Spanish words respectively. The character embeddings are initialized by Xavier (Glorot and Bengio, 2010) and their dimension is 50. In our model, all unknown words, weight matrices and biases are initialized by Xavier Glorot and Bengio (2010). The dimensions of the character-level and word-level hidden states in MBi-GRU are set to 300 and 600 respectively. The layer number of multi-layer bidirectional GRU is set to 2. To avoid overfitting, we adopt dropout on embeddings, sfi and tfi , and the dropout rate is set to 0.5. The word embeddings a"
D18-1504,D16-1103,0,0.0402932,"ment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two task"
D18-1504,C16-1311,0,0.0600708,"and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to dif"
D18-1504,D16-1059,0,0.0333403,"Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and p"
D18-1504,D16-1058,0,0.0713024,"Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and p"
D18-1504,P13-1161,0,0.212768,"multi-layer bidirectional gated recurrent units (HMBi-GRU) model to learn abstract features for both tasks, and we propose a HMBi-GRU based joint model which allows the target label of word to have influence on its sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HMBi-GRU in learning abstract features. 1 Introduction Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively. Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jo"
D18-1504,D15-1073,0,0.431616,"them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al. (2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or collapsed methods. With the success of deep learning techniques, neural networks have demonstrated their capability of sequence labeling (Collobert et al., 2011; Pei et al., 2014; Chen et al., 2015). However, Zhang et al. (2015) only use word embeddings to enrich features without taking full advantages of neural networks’ potential in automatically capturing important sequence labeling features like long dis"
D18-1504,D13-1171,0,0.375352,"Missing"
D18-1504,P14-1028,0,0.0187621,"ual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or collapsed methods. With the success of deep learning techniques, neural networks have demonstrated their capability of sequence labeling (Collobert et al., 2011; Pei et al., 2014; Chen et al., 2015). However, Zhang et al. (2015) only use word embeddings to enrich features without taking full advantages of neural networks’ potential in automatically capturing important sequence labeling features like long distance dependencies and character-level features. To make better use of neural networks to explore appropriate character-level features and high-level semantic features for the two tasks, we design a hierarchical multi-layer bidirectional gated recurrent units networks (HMBiGRU) which uses a multi-layer Bi-GRU to automatically learn character features (e.g. capitali"
D19-1345,D17-1209,0,0.0299535,"is text representation learning. With the development of deep learning, neural networks like Convolutional Neural Networks (CNN) (Kim, 2014) and Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997) have been employed for text representation. Recently, a new kind of neural network named Graph Neural Network (GNN) has attracted wide attention (Battaglia et al., 2018). GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018a), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Defferrard et al. (2016) first employed Graph Convolutional Neural Network (GCN) in text classification task and outperformed the traditional CNN models. Further, Yao et al. (2019) improved Defferrard et al. (2016)’s work by applying article nodes and weighted edges in the graph, and their model outperformed the state-of-the-art text classification methods. However, these GNN-based models usually adopt the way of building one graph for the whole corpus, which causes the following problems in practice. First, high memory consumption is requir"
D19-1345,E17-2068,0,0.0537432,"ion as other graph-based models do. Finally, the representations of all nodes in the text are used to predict the label of the text: X r0n + b)) (5) yi = softmax(Relu(W Datasets # Train # Test Categories Avg. Length R8 R52 Ohsumed 5485 6532 3357 2189 2568 4043 8 52 23 65.72 69.82 135.82 Table 1: Datasets overview. • CNN Proposed by (Kim, 2014), perform convolution and max pooling operation on word embeddings to get representation of text. • LSTM Defined in (Liu et al., 2016), use the last hidden state as the representation of the text. Bi-LSTM is a bi-directional LSTM. • fastText Proposed by (Joulin et al., 2017), average word or n-gram embeddings as documents embeddings. • Graph-CNN Operate convolution over word embedding similarity graphs by fourier filter, proposed by (Defferrard et al., 2016). n∈Ni where W ∈ Rd×c is a matrix mapping the vector into an output space, Ni is the node set of text i and b ∈ Rc is bias. The goal of training is to minimize the crossentropy loss between ground truth label and predicted label: loss = −gi log yi , (6) where gi is the “one-hot vector” of ground truth label. 3 Experiments In this section, we describe our experimental setup and report our experimental results."
D19-1345,D14-1181,0,0.19485,"significantly reduce the edge numbers as well as memory consumption. Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory. 1 Introduction Text classification is a fundamental problem of natural language processing (NLP), which has lots of applications like SPAM detection, news filtering, and so on (Jindal and Liu, 2007; Aggarwal and Zhai, 2012). The essential step for text classification is text representation learning. With the development of deep learning, neural networks like Convolutional Neural Networks (CNN) (Kim, 2014) and Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997) have been employed for text representation. Recently, a new kind of neural network named Graph Neural Network (GNN) has attracted wide attention (Battaglia et al., 2018). GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018a), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Defferrard et al. (2016) first employed Graph Convolutional Neural"
D19-1345,D14-1162,0,0.0962989,"from the training set to build validation set. The overview of datasets is listed in Table 1. We compare our method with the following baseline models. It is noted that the results of some models are directly taken from (Yao et al., 2019). 1 2 https://www.cs.umb.edu/˜smimarog/textmining/datasets/ http://disi.unitn.it/moschitti/corpora.htm • Text-GCN A graph based text classification model proposed by (Yao et al., 2019), which builds a single large graph for whole corpus. 3.2 Implementation Details We set the dimension of node representation as 300 and initialize with random vectors or Glove (Pennington et al., 2014). k discussed in Section 2.1 is set to 2. We use the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 10−3 , and L2 weight decay is set to 10−4 . Dropout with a keep probability of 0.5 is applied after the dense layer. The batch size of our model is 32. We stop training if the validation loss does not decrease for 10 consecutive epochs. For baseline models, we use default parameter settings as in their original papers or implementations. For models using pre-trained word embeddings, we used 300-dimensional GloVe word embeddings. 3.3 Experimental Results Table 2 reports the"
D19-1345,P12-2018,0,0.0318507,"r results. In (3), we remove the pre-trained word embeddings from nodes and initialize all the nodes with random vectors. Compared with the original model, the performances are slightly decreased without pre-trained word embeddings. Therefore, we believe that the pre-trained word embeddings have a particular effect on improving the performance of our model. 4 Related Work 4.2 Text Classification Text classification is a classic problem of natural language processing and has a wide range of applications in reality. Traditional text classification like bag-of-words (Zhang et al., 2010), n-gram (Wang and Manning, 2012) and Topic Model (Wallach, 2006) mainly focus on feature engineering and algorithms. With the development of deep learning techniques, more and more deep learning models are applied for text classification. Kim (2014); Liu et al. (2016) applied CNN and RNN into text classification and achieved results which are much better than traditional models. With the development of GNN, some graphbased classification models are gradually emerging (Hamilton et al., 2017; Veliˇckovi´c et al., 2017; Peng et al., 2018). Yao et al. (2019) proposed Text-GCN and achieved state-of-the-art results on several main"
D19-1345,P18-1030,0,0.0141604,"2012). The essential step for text classification is text representation learning. With the development of deep learning, neural networks like Convolutional Neural Networks (CNN) (Kim, 2014) and Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997) have been employed for text representation. Recently, a new kind of neural network named Graph Neural Network (GNN) has attracted wide attention (Battaglia et al., 2018). GNN was first proposed in (Scarselli et al., 2009) and has been used in many tasks in NLP including text classification (Defferrard et al., 2016), sequence labeling (Zhang et al., 2018a), neural machine translation (Bastings et al., 2017), and relational reasoning (Battaglia et al., 2016). Defferrard et al. (2016) first employed Graph Convolutional Neural Network (GCN) in text classification task and outperformed the traditional CNN models. Further, Yao et al. (2019) improved Defferrard et al. (2016)’s work by applying article nodes and weighted edges in the graph, and their model outperformed the state-of-the-art text classification methods. However, these GNN-based models usually adopt the way of building one graph for the whole corpus, which causes the following problems"
I08-1038,W05-0408,0,\N,Missing
I08-1038,H05-2017,0,\N,Missing
I08-1038,H05-1043,0,\N,Missing
I08-1038,W03-1014,0,\N,Missing
I08-1038,W03-0404,0,\N,Missing
I08-1038,P02-1053,0,\N,Missing
I08-1038,P89-1010,0,\N,Missing
I08-4022,W06-0127,0,0.0579711,"Missing"
I08-4022,I05-3025,0,0.0356092,"urns out to perform well. Our system ranks 2nd in the closed track on NER of MSRA, and 4th in the closed track on word segmentation of SXU. 1 2 System Description The system is mainly based on CRFs, while different strategies are introduced in word segmentation task and NER task. 2.1 Introduction Chinese word segmentation and NER are two of the most fundamental problems in Chinese information processing and have attracted more and more attentions. Many methods have been presented, of which, machine learning methods have obviously competitive advantage in such problems. Maximum Entropy (Ng and Low, 2005) and CRFs (Hai Zhao et al. 2006, Zhou Junsheng et CRFs are undirected graphical models which are particularly well suited to sequence labeling tasks, such as NER & word segmentation. In these cases, CRFs are often referred to as linear chain CRFs. CRFs are criminative models, which allow a richer feature representation and provide more natural modeling. 1 128 CRFs http://www.sourceforge.net/ Sixth SIGHAN Workshop on Chinese Language Processing CRFs define the conditional probability of a state sequence given an input sequence as Where F is a feature function set over its arguments, λk is a lea"
I08-4022,W06-0121,0,0.0442595,"Missing"
I08-4022,W06-0140,0,0.0445667,"Missing"
I13-1073,W04-1102,0,0.0904726,"Missing"
I13-1073,W02-1001,0,0.105237,"the precision, recall, and F-score based on a single query. Hence, we finally have six metrics: macro-precision, macro-recall, macro-Fscore, micro-precision, micro-recall, and micro-Fscore. We use the novel training method, adaptive online gradient descent based on feature frequency information (ADF) (Sun et al., 2012), for fast and accurate training of the CRF model. To study the performance of other machine learning models, we also implement on other wellknown sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). 3.2 Results on Abbreviation Prediction The experimental results are shown in Table 2. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also show the discriminate accuracy. The discriminate accuracy checks the accuracy of discriminating positive and negative full forms, without comparing the generated abbreviations with the gold-standard abbreviations. As we can see from Table 2, first, the best system is the system Unified-Assum.1-"
I13-1073,P08-2016,0,0.646784,"Missing"
I13-1073,P02-1021,0,0.0505622,"Missing"
I13-1073,P09-1102,1,0.611546,"Missing"
I13-1073,P12-1027,1,0.813122,"inally, the CRF model outperforms the MEMM and averaged perceptron models. To summarize, the unified system with assumption-1, global information, and CRF model has the best performance. For evaluating web search quality based on a set of queries, we use the macro-averaging and microaveraging of the precision, recall, and F-score based on a single query. Hence, we finally have six metrics: macro-precision, macro-recall, macro-Fscore, micro-precision, micro-recall, and micro-Fscore. We use the novel training method, adaptive online gradient descent based on feature frequency information (ADF) (Sun et al., 2012), for fast and accurate training of the CRF model. To study the performance of other machine learning models, we also implement on other wellknown sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). 3.2 Results on Abbreviation Prediction The experimental results are shown in Table 2. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also s"
I13-1073,N09-2069,0,0.541149,"Missing"
I13-1073,W06-0103,0,\N,Missing
I17-1019,J04-1004,0,0.366809,"he 8th International Joint Conference on Natural Language Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address the domain adaption problem for CWS. One major family proposed to compose boundary features by fitting the relevance of consecutive characters using Accessor Variety (AV) (Feng et al., 2004a,b), or Chi-square Statistics (Chi2) (Chang and Han, 2010). Combining the boundary features with other hand-crafted features, these methods were shown to achieve better performance on OOV words. Inspired by these models, we propose a novel BLSTM-based neural network model which incorporates a global recurrent structure designed to model boundary features dynamically. This structure can learn to utilize the target domain corpus and extract the correlation or irrelevance between characters, which is a reminiscence of the discrete boundary features such as Accessor Variety (AV). The contribution"
I17-1019,P13-1075,0,0.125587,"Missing"
I17-1019,J96-1002,0,0.297775,"Missing"
I17-1019,P16-1039,0,0.266099,"rained on a different corpus. The results are not directly comparable. The results prove the incredible effectiveness of the global recurrent structure on OOV recognition and overall segmentation, comparable to the BLSTM model that directly incorporates discrete AV features. Adding discrete AV features into our model seem not to be a notable improvement, which also confirms that our model already has certain domain adaption ability. OOV Recall 83.01 82.59 83.78 Models (Zheng et al., 2013) (Pei et al., 2014) (Chen et al., 2015a) (Chen et al., 2015b) (Chen et al., 2015a)* (Chen et al., 2015b)* (Cai and Zhao, 2016) (Zhang et al., 2016) BLSTM This work Table 4: IV and OOV recalls on the PKU development data. Methods BLSTM BLSTM-2 GRS-4 IV Recall 96.35 96.11 96.25 OOV Recall 82.67 82.01 83.96 Table 5: IV and OOV recalls on the PKU test data. 5.3 PKU 92.8 95.2 96.4 96.5 94.5 94.8 95.5 95.7 95.9 95.9 MSRA 93.9 97.2 97.6 97.4 95.4 95.6 96.5 97.7 97.0 97.1 Table 7: Comparison of our model with previous neural models on the PKU and MSRA datasets. Results with * are from runs on their released implementation (Cai and Zhao, 2016). Final Results In this section, We compare our BLSTM+GRS-4 model with previous stat"
I17-1019,D10-1077,0,0.110382,"e Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address the domain adaption problem for CWS. One major family proposed to compose boundary features by fitting the relevance of consecutive characters using Accessor Variety (AV) (Feng et al., 2004a,b), or Chi-square Statistics (Chi2) (Chang and Han, 2010). Combining the boundary features with other hand-crafted features, these methods were shown to achieve better performance on OOV words. Inspired by these models, we propose a novel BLSTM-based neural network model which incorporates a global recurrent structure designed to model boundary features dynamically. This structure can learn to utilize the target domain corpus and extract the correlation or irrelevance between characters, which is a reminiscence of the discrete boundary features such as Accessor Variety (AV). The contributions of this paper are two folds: Figure 1: General architectu"
I17-1019,N16-1030,0,0.124294,"s of the structure that are shown in Figure 3. GRS-2 To better fit the boundary features, we add a full-connection hidden layer following the recurrent network. The boundary feature embeddings are calculated as follows: Embbf (bi ) = σ(Wbf hi + bbf ) where trii = ci−1 ci ci+1 and other values have the same meanings as above. 4 Training Instead of using the Max-Margin criterion (Taskar et al., 2005) adopted by previous neural network models for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b), we try to directly maximize the log-probability of the correct tag sequence following Lample et al. (2016): (6) where σ is the logistic sigmoid function. GRS-3 Considering the hidden states are noisy and contains much information of other words, we want the hidden values more relevant to the current bigram, so a gate is introduced to the structure. The boundary feature embeddings are calculated log(p(y|X)) = s(X, y) − log( X es(X,˜y) ) y˜∈YX = s(X, y) − logadd s(X, y˜) y˜∈YX 187 (9) Figure 3: Four variants of the global recurrent structure. where YX represents all possible tag sequences for a sentence X. While decoding, we predict the output sequence which obtains the maximum score as follows: y ∗"
I17-1019,P15-1168,0,0.358598,"001). Furthermore, rich features can be incorporated into these systems to improve their performances and most state-of-the-art systems are still based on feature-based models. Recently, neural network models are drawing increasing attention in Natural Language Processing (NLP) tasks. They significantly reduced feature engineering effort and achieved competitive or state-of-the-art results in many NLP tasks. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many neural network models (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b) have been applied to CWS and some approached state-of-the-art performance. However, these neural network models, as well as other supervised methods, do not work well in domain adaptation. In recent years, manually annotated training corpus mostly come from the news domain. When it shifts to other domains such as literature or medicine, where there are many domain-related words that rarely appear in other domains, Out-of-Vocabulary (OOV) word recognition becomes an important problem. Moreover, different domains means different language usages and contexts. Therefore, the InVocabulary (IV)"
I17-1019,J09-4006,0,0.0145467,"4) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora. Word segmentation has been pursued with considerable efforts in the Chinese NLP community. One mainstream method is regarding word segmentation task as a s"
I17-1019,D14-1093,0,0.750532,"n performance on OOV words, which empirically proves its domain adaption ability. BSLTM-2, similar to LSTM2 (Chen et al., 2015b), is an architecture comprised of two stacking bidirectional LSTM hidden layers. GRS-4 is short for BLSTM+GRS-4 model. Methods BLSTM BLSTM-2 GRS-4 IV Recall 97.12 96.89 96.91 ble 6. We also attempt to integrate discrete boundary features into the models. In our experiments, we choose the Accessor Variety(AV) (Feng et al., 2004a,b) which is a feature widely used in traditional Chinese word segmentation. Our F-scores and OOV recalls are competitive to those reported by Liu et al. (2014) and Jiang et al. (2013). However, following Liu et al. (2014)’s setting, we choose the PKU dataset as the training corpus while Jiang et al. (2013)’s model is trained on a different corpus. The results are not directly comparable. The results prove the incredible effectiveness of the global recurrent structure on OOV recognition and overall segmentation, comparable to the BLSTM model that directly incorporates discrete AV features. Adding discrete AV features into our model seem not to be a notable improvement, which also confirms that our model already has certain domain adaption ability. OO"
I17-1019,D15-1092,0,0.130797,"001). Furthermore, rich features can be incorporated into these systems to improve their performances and most state-of-the-art systems are still based on feature-based models. Recently, neural network models are drawing increasing attention in Natural Language Processing (NLP) tasks. They significantly reduced feature engineering effort and achieved competitive or state-of-the-art results in many NLP tasks. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many neural network models (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b) have been applied to CWS and some approached state-of-the-art performance. However, these neural network models, as well as other supervised methods, do not work well in domain adaptation. In recent years, manually annotated training corpus mostly come from the news domain. When it shifts to other domains such as literature or medicine, where there are many domain-related words that rarely appear in other domains, Out-of-Vocabulary (OOV) word recognition becomes an important problem. Moreover, different domains means different language usages and contexts. Therefore, the InVocabulary (IV)"
I17-1019,P14-1028,0,0.701386,"Lafferty et al., 2001). Furthermore, rich features can be incorporated into these systems to improve their performances and most state-of-the-art systems are still based on feature-based models. Recently, neural network models are drawing increasing attention in Natural Language Processing (NLP) tasks. They significantly reduced feature engineering effort and achieved competitive or state-of-the-art results in many NLP tasks. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many neural network models (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b) have been applied to CWS and some approached state-of-the-art performance. However, these neural network models, as well as other supervised methods, do not work well in domain adaptation. In recent years, manually annotated training corpus mostly come from the news domain. When it shifts to other domains such as literature or medicine, where there are many domain-related words that rarely appear in other domains, Out-of-Vocabulary (OOV) word recognition becomes an important problem. Moreover, different domains means different language usages and contexts. Therefore, th"
I17-1019,I05-3017,0,0.518186,"Missing"
I17-1019,C04-1081,0,0.879334,"Segmentation, especially OOV-Recall, which brings benefits to domain adaptation. We achieved state-of-the-art results on 6 domains of CNKI articles, and competitive results to the best reported on the 4 domains of SIGHAN Bakeoff 2010 data. 1 Introduction Since Chinese writing system does not have explicit word delimiters, word segmentation becomes an essential first step for further Chinese language processing. In recent years, Chinese Word Segmentation (CWS) has experienced great advancement. One mainstream method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004) where each character is assigned a tag indicating its position in the word. This method has been proved ∗ Corresponding author 184 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address the domain adaption"
I17-1019,D11-1090,0,0.150039,"→ ← (2) where Wd ∈ R|T |×H2 , bd ∈ R|T |. H2 is the number of hidden units of the outputs for the BLSTM layer. f (ti |c[i−w/2:i+w/2] ) ∈ R|T |is the score vector for each possible tag. Here in Chinese word segmentation, we set T = {S, B, E, M }. 2.3 Global Recurrent Structure Chinese word segmentation is essentially a task of resolving the relevance of consecutive characters. Lacking knowledge of such relevance, recognizing out-of-domain words has been the bottleneck of domain adaption in CWS. However, Boundary features such as Accessor Variety (AV) (Feng et al., 2004a,b), Mutual Information (Sun and Xu, 2011) and Chi-square Statistics (Chi2) (Chang and Han, 2010) are features designed to fit such relevance. A significant advantage of boundary features is that they can compute the correlation of characters from a large scale corpora, annotated or not, to boost the OOV word recognition performance. As a result, they are especially effective for cross-domain CWS. In this paper, we propose 5 novel global recurrent structures to generate embeddings that mimic the boundary features for further computing, which needs minimal pre-processing and feature engineering. The structures are designed to capture t"
I17-1019,P16-2092,1,0.828577,"Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011; Zheng et al., 2013; Qi et al., 2014) to reduce efforts of feature engineering. Pei et al. (2014) used a neural tensor model to capture the complicated interactions between tags and context characters. Experiments in his paper also show that bigram embeddings are of great benefit. To incorporate complicated combinations and long-term dependency information of the context characters, gated recursive model (Chen et al., 2015a) and LSTM model (Chen et al., 2015b) were used respectively. Moreover, Xu and Sun (2016) proposed a dependency-based gated recursive model which merges the benefits of the two models above. Coincidentally, Cai and Zhao (2016) and Zhang et al. (2016) both addressed the problem of lacking word-based features that previous neural CWS models have. Cai and Zhao (2016) proposed a novel gated combination neural network which thoroughly eliminates context windows and can utilize complete segmentation history. Zhang et al. (2016) proposed a transition-based neural model which replaces manually designed discrete features with neural features. Domain adaption for Chinese word segmentation h"
I17-1019,O03-4002,0,0.924886,"inese Word Segmentation, especially OOV-Recall, which brings benefits to domain adaptation. We achieved state-of-the-art results on 6 domains of CNKI articles, and competitive results to the best reported on the 4 domains of SIGHAN Bakeoff 2010 data. 1 Introduction Since Chinese writing system does not have explicit word delimiters, word segmentation becomes an essential first step for further Chinese language processing. In recent years, Chinese Word Segmentation (CWS) has experienced great advancement. One mainstream method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004) where each character is assigned a tag indicating its position in the word. This method has been proved ∗ Corresponding author 184 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address"
I17-1019,P13-2032,1,0.859292,"e 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora. Word segmentation has"
I17-1019,D13-1031,1,0.891787,"e 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora. Word segmentation has"
I17-1019,E14-1062,0,0.0643886,"r model thereupon performs better with the increase of the size of testing corpus as long as the OOV words appear more. Although the trendline of our model is promising, there are some OOV words that occurs frequently but are wrongly segmented. Some examples are listed in Table 8. Errors involving OOV Word English Correct Total 肾脏 kidney 15 39 甲型H1N1流感 influenza A 0 30 (H1N1) 维生素C vitamin C 2 23 Table 8: Some examples of wrongly segmented OOV words with high frequency. 190 Figure 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li"
I17-1019,P16-1040,0,0.0135546,"feature engineering. Pei et al. (2014) used a neural tensor model to capture the complicated interactions between tags and context characters. Experiments in his paper also show that bigram embeddings are of great benefit. To incorporate complicated combinations and long-term dependency information of the context characters, gated recursive model (Chen et al., 2015a) and LSTM model (Chen et al., 2015b) were used respectively. Moreover, Xu and Sun (2016) proposed a dependency-based gated recursive model which merges the benefits of the two models above. Coincidentally, Cai and Zhao (2016) and Zhang et al. (2016) both addressed the problem of lacking word-based features that previous neural CWS models have. Cai and Zhao (2016) proposed a novel gated combination neural network which thoroughly eliminates context windows and can utilize complete segmentation history. Zhang et al. (2016) proposed a transition-based neural model which replaces manually designed discrete features with neural features. Domain adaption for Chinese word segmentation has been widely exploited before neural CWS models are proposed. Jiang et al. (2013) utilized the web text(160K Wikipedia) to improves seg7 Conclusion and Perspec"
I17-1019,I08-1002,0,0.0345857,"igh frequency. 190 Figure 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora."
I17-1019,W10-4126,0,0.120103,"Missing"
I17-1019,D13-1061,0,0.495213,"Missing"
I17-1050,P14-5010,0,0.00631574,"Missing"
I17-1050,P97-1013,0,0.167118,"is ADJP JJ likely VP TO to Introduction VB expand Figure 1: An example of two sentences with their discourse relation as Expansion.Restatement.Specification. Subfigure (a) and (b) are partial parse trees of the two important phrases with yellow background. It is widely agreed that text units such as clauses or sentences are usually not isolated. Instead, they correlate with each other to form coherent and meaningful discourse together. To analyze how text is organized, discourse parsing has gained much attention from both the linguistic (Weiss and Wodak, 2007; Tannen, 2012) and computational (Marcu, 1997; Soricut and Marcu, 2003) communities, but the current performance is far from satisfactory. The most challenging part is to identify the discourse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in"
I17-1050,P17-1152,0,0.134763,"orporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encoding the arguments. An"
I17-1050,W12-1614,0,0.0606847,"b-component of discourse analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discourse relation identification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Rec"
I17-1050,D14-1162,0,0.0804132,"s are extremely imbalanced in PDTB. However, recent work put more emphasis on the multi-class classification, where the goal is to identify a discourse relation from all possible choices. According to Rutherford and Xue (2014), the multi-class classification setting is more natural and realistic. Moreover, the multi-class classifier can directly serve as one building block of a complete discourse parser (Qin et al., 2017). Therefore, in this work, we will focus on the multiω 50 τ 50 d 250 η 0.01 λ 0.0001 b 10 Table 2: Hyper-parameters of our model The Pre-trained 50-dimentional Glove Vectors (Pennington et al., 2014), which is caseinsensitive, are used for initializing the word embeddings and they are tuned together with other parameters in the same learning rate during training. 501 Systems Zhang et al. (2015) Rutherford and Xue (2014) Rutherford and Xue (2015) Liu et al. (2016) Liu and Li (2016) Ji et al. (2016) Tag-Enhanced Tree-LSTM Tag-Enhanced Tree-GRU We adopt the AdaGrad optimizer (Duchi et al., 2011) for training our model and we validate the performance every epoch. It takes around 5 hours (5 epochs) for the Tag-Enhanced Tree-LSTM and 4 hours (6 epochs) for the Tag-Enhanced TreeGRU model to conv"
I17-1050,P16-1078,0,0.332691,"oposes a framework based on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful p"
I17-1050,P09-1077,0,0.196333,"urse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether su"
I17-1050,N16-1037,0,0.0247637,"al neural networks to encode the arguments. • Rutherford and Xue (2014) manually extracts features to represent the arguments and use a maximum entropy classifier for classification. Rutherford and Xue (2015) further exploits discourse connectives to enrich the training data. • Liu et al. (2016) employs a multi-task framework that can leverage other discourserelated data to help with the training of discourse relation classifier. • Liu and Li (2016) represents arguments with LSTM and introduces a multi-level attention 502 0.8 mechanism to model the interaction between the two arguments. 0.7 • Ji et al. (2016) treats the discourse relation as latent variable and proposes to model them jointly with the sequences of words using a latent variable recurrent neural network architecture. WHPP 0.6 VBD 0.5 CD 0.4 0.3 And in Table 4, we present the following systems, which focus on Level-2 classification: VBG 0.1 0.1 PP WP$ PRP$ WHADJP NX VP WHADVP WHNP NNP S PRT NN ADJP TO RP PDT VBZ RBR -LRB- EX NP-TMP VB FRAG INTJ JJR ADVP LS RRC SBAR MD NNPS RB FW VBP RBS NP SQ LST JJS QP PRN CC WP UCP 0.2 • Lin et al. (2009) uses traditional featurebased model to classify relations. Especially, constituent and dependen"
I17-1050,E17-2093,0,0.24905,"d on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encodi"
I17-1050,prasad-etal-2008-penn,0,0.123225,"or identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve the semantic modeling for implicit discourse relation classification. 2.1 Implicit Discourse Relation Classication Discourse relation identification is an important but difficult sub-component of discourse analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discour"
I17-1050,D09-1036,0,0.0910292,"Missing"
I17-1050,P15-1132,0,0.0242798,"k models show superior ability in a variety of semantic modeling tasks, such as sentiment classification (Kokkinos and Potamianos, 2017), natural language inference (Chen et al., 2017) and machine translation (Eriguchi et al., 2016). The earliest and simplest tree-structure neural network is the Recursive Neural Network proposed by Socher et al. (2011), in which a global matrix is learned to linearly combine the contituent vectors. This work is further extended by replacing the global matrix with a global tensor to form the Recursive Neural Tensor Network (Socher et al., 2013). Based on them, Qian et al. (2015) first proposes to incorporate tag information, which is very similar as our idea described in Section 3.2, by either choosing a composition function according to the tag of a phrase (TagGuided RNN/RNTN) or combining the tag embeddings with word embeddings (Tag-Embedded RNN/RNTN). Our method of incorporating tag information improves from theirs and somewhat combines these two methods by using the tag embedding to dynamically determine the composition function via the gates in LSTM or GRU. One fatal weakness of vanilla RNN/RNTN is the well-known gradient exploding or vanishing problem due to th"
I17-1050,D16-1130,1,0.705053,"“but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve"
I17-1050,D16-1246,0,0.0767037,"tification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Recently, with the popularity of deep learning methods, many cutting-edge models are also applied to our task of implicit discourse relation classification. Qin et al. (2016) tries to model the sentences with Convolutional Neural Networks. Liu and Li (2016) encodes the text with Long Short Term Memory model and employ multi-level attention mechanism to capture important signals. Qin et al. (2017) proposes a framework based on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the represen"
I17-1050,P17-1093,0,0.248193,"plicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve the semantic model"
I17-1050,E14-1068,0,0.216059,"e analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discourse relation identification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Recently, with the popularity"
I17-1050,N15-1081,0,0.0166996,"ompared with other state-of-the-art systems. model have less parameters to train which could alleviate the problem of overfitting and also cost less training time. 4.4 Comparison with Other Systems For a comprehensive study, we compare our models with other state-of-the-art systems. The systems that conduct Level-1 classification are reported in Table 3, including: • Zhang et al. (2015) proposes to use convolutional neural networks to encode the arguments. • Rutherford and Xue (2014) manually extracts features to represent the arguments and use a maximum entropy classifier for classification. Rutherford and Xue (2015) further exploits discourse connectives to enrich the training data. • Liu et al. (2016) employs a multi-task framework that can leverage other discourserelated data to help with the training of discourse relation classifier. • Liu and Li (2016) represents arguments with LSTM and introduces a multi-level attention 502 0.8 mechanism to model the interaction between the two arguments. 0.7 • Ji et al. (2016) treats the discourse relation as latent variable and proposes to model them jointly with the sequences of words using a latent variable recurrent neural network architecture. WHPP 0.6 VBD 0.5"
I17-1050,D13-1170,0,0.00737155,"f text, tree-structured neural network models show superior ability in a variety of semantic modeling tasks, such as sentiment classification (Kokkinos and Potamianos, 2017), natural language inference (Chen et al., 2017) and machine translation (Eriguchi et al., 2016). The earliest and simplest tree-structure neural network is the Recursive Neural Network proposed by Socher et al. (2011), in which a global matrix is learned to linearly combine the contituent vectors. This work is further extended by replacing the global matrix with a global tensor to form the Recursive Neural Tensor Network (Socher et al., 2013). Based on them, Qian et al. (2015) first proposes to incorporate tag information, which is very similar as our idea described in Section 3.2, by either choosing a composition function according to the tag of a phrase (TagGuided RNN/RNTN) or combining the tag embeddings with word embeddings (Tag-Embedded RNN/RNTN). Our method of incorporating tag information improves from theirs and somewhat combines these two methods by using the tag embedding to dynamically determine the composition function via the gates in LSTM or GRU. One fatal weakness of vanilla RNN/RNTN is the well-known gradient explo"
I17-1050,N03-1030,0,0.25336,"kely VP TO to Introduction VB expand Figure 1: An example of two sentences with their discourse relation as Expansion.Restatement.Specification. Subfigure (a) and (b) are partial parse trees of the two important phrases with yellow background. It is widely agreed that text units such as clauses or sentences are usually not isolated. Instead, they correlate with each other to form coherent and meaningful discourse together. To analyze how text is organized, discourse parsing has gained much attention from both the linguistic (Weiss and Wodak, 2007; Tannen, 2012) and computational (Marcu, 1997; Soricut and Marcu, 2003) communities, but the current performance is far from satisfactory. The most challenging part is to identify the discourse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have be"
I17-1050,N09-1064,0,0.0735037,"Missing"
I17-1050,P15-1150,0,0.628509,"parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encoding the arguments. Another important syntactic signal comes from the constituent tags on the tree nodes (e.g., NP, VP, ADJP). Those tags, derived from the production rules, describe the generative process of text and therefore could indicate which part is more important in each constituent. For example, considering a node tagged with NP, its child node tagged with DT is usually neglectable. Thus we propos"
I17-1050,D15-1266,0,0.0357118,"Missing"
I17-1064,D16-1171,0,0.35288,"Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat limited to either of the"
I17-1064,P14-5010,0,0.00535743,"ch sentence plays a different role, and from a specific view all the sentences should be paid different attention, in document-level sentiment classification. Here, we still consider the influence from user, product and their combination, and get the corresponding document representation du , dp , dup , where d∗ (∗ ∈ {u, p, up}) is computed as follows: d∗ = m X t=1 βt∗ s∗t Datasets To validate the effectiveness of our model, we use three real-world datasets: IMDB, Yelp 2013 and Yelp 2014 collected by Tang et al. (2015b). For these data, we preprocess the text including using Stanford CoreNLP (Manning et al., 2014) to split the review documents into sentences and tokenizing all words. Table 1 shows the details of the three datasets including number of documents (#docs), average number of documents per user posts(#docs/user) etc. It is also noted that IMDB is rated with 10 sentiment labels (i.e., 1-10 stars) while Yelp has 5 labels (i.e., 1-5 stars). We also adopt the same data partition used in (Tang et al., 2015b) and (Chen et al., 2016) for training, developing and test. (9) exp(γ(s∗t , δ ∗ )) βt∗ = Pm ∗ ∗ t=1 exp(γ(st , δ )) (10) Evaluation Metrics where s∗t is the sentence representation of sentence"
I17-1064,I13-1156,0,0.309598,"set to 10−5 . 3.1 Method Comparison To comprehensively evaluate the performance of CMA, we list some baseline methods for comparison. The baselines are introduced as follows. • Majority assigns the largest sentiment polarity occurred in the training set to each sample in the test set. • Trigram uses the unigrams, bigrams and trigrams features to train a SVM classifier for sentiment classification. • TextFeature extracts word/character ngrams, sentiment lexicon features, negation features, etc. for a SVM classifier. • UPF extracts user-leniency features and product features from training data (Gao et al., 2013). There features can be concatenated with the features of Trigram and TextFeature. • AvgWordvec averages the word embeddings in a document to generate the document representation as features for a SVM classifier. • SSWE first learns the sentiment-specific word embeddings and then utilizes three kinds of pooling (i.e., max, min and average) to generate the document representation for a SVM classifier (Tang et al., 2014). • PV(Paragraph Vector) is an unsupervised framework to learn distributed representations for text of any length (Le and Mikolov, 2014). (Tang 638 IMDB Acc. RMSE Majority 0.196"
I17-1064,W02-1011,0,0.0225933,"ultiple representation vectors, which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model. 1 Introduction Document-level sentiment classification aims to predict an overall sentiment polarity (e.g., 1-5 stars or 1-10 stars) for a user review document. This task recently draws increasing research concerns and is helpful to many downstream applications, such as user and product recommendation. Early work focuses on traditional machine learning associated with handcraft text features for sentiment classification (Pang et al., 2002; Ding et al., 2008; Taboada et al., 2011). With the development of deep learning techniques, some researchers design neural networks to automati634 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 634–643, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP levels including word level, sentence level, document level, classification level. With word embeddings as input, we can employ convolutional neural networks or recurrent neural networks to obtain deeper semantic of words on the word level. On the sentence level, we design the multiway"
I17-1064,W06-3808,0,0.0534092,"ct (δ up ), with blue, red and green color series respectively. The deeper color means higher weight. Figure 2(a)∼(f) display the three ways of attention to words for sentence1 ∼ sentence6 respectively. We can see that different words are as4 Related work Document-level sentiment classification methods can be divided into two kinds of research lines, i.e., traditional machine learning methods and neural networks methods. For the first kind of research line, Pang et al. (2002) validate the effectiveness of various machine learning methods with bag-of-words features on sentiment classification. Goldberg and Zhu (2006) use a graph-based semi-supervised learning algorithm with unlabeled data to predict the sentiment of reviews. There are also some work which focus on extracting effective features. Ganu et al. (2009) identify user experience information from free text. Qu et al. (2010) introduce a kind of bag-of-opinion representation. 640 . can or jam )1 2 ) ce6 ten sen ce5 ten sen ce4 ten sen ce3 ten sen ce2 ten sen nce1 te sen . re mo say 1 2 ) 1 2 ) no , x jam rry e eb de blu ma me ho . bo a or x mi I ed ne … rs sta e fiv d ly en up . ff sta ing g ga en , fri of gro a . fee cof r ne din e tru 1 2 ) m fro"
I17-1064,C10-1103,0,0.0311466,"ent classification methods can be divided into two kinds of research lines, i.e., traditional machine learning methods and neural networks methods. For the first kind of research line, Pang et al. (2002) validate the effectiveness of various machine learning methods with bag-of-words features on sentiment classification. Goldberg and Zhu (2006) use a graph-based semi-supervised learning algorithm with unlabeled data to predict the sentiment of reviews. There are also some work which focus on extracting effective features. Ganu et al. (2009) identify user experience information from free text. Qu et al. (2010) introduce a kind of bag-of-opinion representation. 640 . can or jam )1 2 ) ce6 ten sen ce5 ten sen ce4 ten sen ce3 ten sen ce2 ten sen nce1 te sen . re mo say 1 2 ) 1 2 ) no , x jam rry e eb de blu ma me ho . bo a or x mi I ed ne … rs sta e fiv d ly en up . ff sta ing g ga en , fri of gro a . fee cof r ne din e tru 1 2 ) m fro t no , ilk rm tte bu l rea th es wi ak nc de pa ma me ho . zen fro t no , h atc scr m fro de ma s it cu bis (1 2 ) 1 2 ) 131 2 ) 1 Figure 2: Case Study: Illustration of Attention Weights. ument. Chen et al. (2016) employ two layers of long-short term memory (LSTM) with"
I17-1064,P12-1092,0,0.0564451,"composed of a sequence of words wt1 , wt2 , · · · , wtnt where wtj denotes a specific word. To represent a word, we embed each word into a low dimensional real-value vector, called word embedding (Bengio et al., 2003). Then, we can get wtj ∈ Rd from M v×d , where t is the sentence index in a document, j denotes the word index in sentence t, d means the embedding dimension and v gives the vocabulary size. Word embeddings can be regarded as parameters of neural networks or pre-trained from proper corpus via language model (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2010; Huang et al., 2012). In our model, we choose the second strategy. Next, deeper word semantics representations can be learned by using the neural network models, such as convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, the LSTM model is employed to obtain the word representation, since it has the good performance of learning the long-term dependencies and can well model the dependence between words. Formally, for sentence St , we input its word embeddings wt1 , wt2 , ..., wtnt to the LSTM networks and get the final word representations rt1 , rt2 , ..., rtnt . Cascading Multiw"
I17-1064,D12-1110,0,0.15676,"Missing"
I17-1064,P14-1062,0,0.103355,"Missing"
I17-1064,D11-1014,0,0.0955439,"the document level, CMA keeps using the multiway attention mechanism to generate attention to sentences. Experimental results on IMDB, Yelp 2013 and Yelp 2014 verify that CMA can learn efficient representations for sentences and documents and provide rich information for judging the document-level sentiment polarity. Recently, neural network approaches have achieved a comparable performance on documentlevel sentiment classification. Glorot et al. (2011) first propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Then, Socher et al. (2011, 2012, 2013) introduce recursive neural networks to document-level sentiment classification. Kim (2014) employ convolutional neural networks to model sentences with two kinds of embeddings for sentiment classification. Le and Mikolov (2014) introduce an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts. Tai et al. (2015) utilize tree-structured longshort memory networks to learn semantic representation for sentiment classification. In addition, user and product information are flexibly modeled for sentiment classification in the neura"
I17-1064,D14-1181,0,0.039791,"Missing"
I17-1064,D13-1170,0,0.0229744,"has a high accuracy and a low RMSE. J =− C X i=1 gi log(yi ) + λr ( X θ2 ) θ∈Θ 637 Dataset IMDB Yelp2013 Yelp2014 #docs 84919 78966 231163 #users 1310 1631 4818 #products 1635 1633 4194 #docs/user 64.82 48.42 47.97 #docs/product 51.94 48.36 55.11 #sents/doc 16.08 10.89 11.41 #words/doc 24.54 17.38 17.26 #labels 10 5 5 Table 1: Data Statistics of IMDB, Yelp2013 and Yelp 2014. et al., 2015a) implements the distributed memory model of paragraph vectors (PV-DM) to get document representations for sentiment classification. • RNTN+RNN models sentences using recursive neural tensor networks (RNTN) (Socher et al., 2013). Then sentence representations are fed into the recurrent neural networks (RNN) and their hidden states are averaged to get the document representation. • UPNN designs preference matrices for each user and product to modify word representations (Tang et al., 2015b). Word representations are then fed into the convolution neural networks (CNNs) and concatenated with the user/product representation to generate document representation before a softmax layer. Without considering user and product information, the UPNN(noUP) method just uses CNN to model the documents. • NSC+UPA proposes the hierarc"
I17-1064,J11-2001,0,0.0482152,"provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model. 1 Introduction Document-level sentiment classification aims to predict an overall sentiment polarity (e.g., 1-5 stars or 1-10 stars) for a user review document. This task recently draws increasing research concerns and is helpful to many downstream applications, such as user and product recommendation. Early work focuses on traditional machine learning associated with handcraft text features for sentiment classification (Pang et al., 2002; Ding et al., 2008; Taboada et al., 2011). With the development of deep learning techniques, some researchers design neural networks to automati634 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 634–643, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP levels including word level, sentence level, document level, classification level. With word embeddings as input, we can employ convolutional neural networks or recurrent neural networks to obtain deeper semantic of words on the word level. On the sentence level, we design the multiway attention networks to generate attention"
I17-1064,P15-1150,0,0.0170995,"erformance on documentlevel sentiment classification. Glorot et al. (2011) first propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Then, Socher et al. (2011, 2012, 2013) introduce recursive neural networks to document-level sentiment classification. Kim (2014) employ convolutional neural networks to model sentences with two kinds of embeddings for sentiment classification. Le and Mikolov (2014) introduce an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts. Tai et al. (2015) utilize tree-structured longshort memory networks to learn semantic representation for sentiment classification. In addition, user and product information are flexibly modeled for sentiment classification in the neural network methods (Tang et al., 2015b; Chen et al., 2016). Tang et al. (2015a) design preference matrices for each user and each product to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole docAcknowledgments We would like to thank the anonymous reviwers for thier insightful suggestions. Our work is supported by National Hi"
I17-1064,D15-1167,0,0.456706,"n Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat lim"
I17-1064,P15-1098,0,0.849911,"n Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat lim"
I17-1064,P14-1146,0,0.105949,"ts word/character ngrams, sentiment lexicon features, negation features, etc. for a SVM classifier. • UPF extracts user-leniency features and product features from training data (Gao et al., 2013). There features can be concatenated with the features of Trigram and TextFeature. • AvgWordvec averages the word embeddings in a document to generate the document representation as features for a SVM classifier. • SSWE first learns the sentiment-specific word embeddings and then utilizes three kinds of pooling (i.e., max, min and average) to generate the document representation for a SVM classifier (Tang et al., 2014). • PV(Paragraph Vector) is an unsupervised framework to learn distributed representations for text of any length (Le and Mikolov, 2014). (Tang 638 IMDB Acc. RMSE Majority 0.196 2.495 Trigram 0.399 1.783 TextFeature 0.402 1.793 AvgWordvec 0.304 1.985 SSWE 0.312 1.973 PV 0.341 1.814 RNTN+RNN 0.400 1.764 UPNN(noUP) 0.405 1.629 0.487 1.381 NSC+LA CA-null 0.491 1.408 Trigram+UPF 0.404 1.764 TextFeature 0.402 1.774 +UPF 0.435 1.602 UPNN NSC+UPA 0.533 1.281 CMA 0.540 1.191 Model Yelp 2013 Acc. RMSE 0.411 1.060 0.569 0.841 0.556 0.814 0.526 0.898 0.549 0.849 0.554 0.832 0.574 0.804 0.577 0.812 0.631"
I17-1064,N16-1174,0,0.21451,"Missing"
J14-3004,P07-1056,0,0.0481432,"Missing"
J14-3004,W02-1001,0,0.609391,"g rate or so-called decaying rate, and Lstoch (zzi , w t ) is the stochastic loss function based on a training sample z i . (More details of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying rate works the best for natural language processing tasks, and it is adopted in our implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). Other well-known on-line training methods include perceptron training (Freund and Schapire 1999), averaged perceptron training (Collins 2002), more recent development/extensions of stochastic gradient descent (e.g., the second-order stochastic gradient descent training methods like stochastic meta descent) (Vishwanathan et al. 2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent method requires the computation or approximation of the inverse of the Hessian matrix of the objective function, which is typically slow, especially for heavily structured classification models. Usually the convergence speed based on number of training iterations is moderately faster, but the time cost per iteration is sl"
J14-3004,W04-1217,0,0.0112629,"tion (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line"
J14-3004,P07-1104,0,0.0145497,"e word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for"
J14-3004,W04-1213,0,0.0135585,"also perform experiments on a nonstructured binary classification task: sentiment-based text classification. For the nonstructured classification task, the ADF training is based on the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996). 4.1 Biomedical Named Entity Recognition (Structured Classification) The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we u"
J14-3004,N01-1025,0,0.136481,"ssification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as befor"
J14-3004,H05-1124,0,0.245783,"Missing"
J14-3004,P06-1059,0,0.0295941,"from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line Training for Natural Lang"
J14-3004,W96-0213,0,0.366727,"xisting gold-standard systems, which are complicated and use extra resources. 2. Related Work Our main focus is on structured classification models with high dimensional features. For structured classification, the conditional random fields model is widely used. To illustrate that the proposed method is a general-purpose training method not limited to a specific classification task or model, we also evaluate the proposal for non-structured classification tasks like binary classification. For non-structured classification, the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996) is widely used. Here, we review the conditional random fields model and the related work of on-line training methods. 2.1 Conditional Random Fields The conditional random field (CRF) model is a representative structured classification model and it is well known for its high accuracy in real-world applications. The CRF model is proposed for structured classification by solving “the label bias problem” (Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequen"
J14-3004,W00-0726,0,0.0354294,"nstraints on j and k. All feature templates are instantiated with values that occurred in training samples. The extracted feature set is large, and there are 2.4 × 107 features in total. Our evaluation is based on a closed test, and we do not use extra resources. Following prior studies, the evaluation metric for this task is the balanced F-score. 4.3 Phrase Chunking (Structured Classification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is t"
J14-3004,W04-1221,0,0.0458102,"Missing"
J14-3004,C10-2139,0,0.0102007,"ent character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for j = i − 2, . . . , i + 1. Whether xj an"
J14-3004,C08-1106,1,0.654936,"fied. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as before. Word-Token–based Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{yi , yi−1 yi } P"
J14-3004,P12-1027,1,0.678739,"Missing"
J14-3004,N09-1007,1,0.809977,"Missing"
J14-3004,I05-3027,0,0.0133462,"R is recall. 4.2 Chinese Word Segmentation (Structured Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at"
J14-3004,P09-1054,0,0.0266947,"Missing"
J14-3004,N06-2049,0,0.0453143,"Missing"
J14-3004,P07-1106,0,0.0122276,"d Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1"
J14-3004,J96-1002,0,\N,Missing
P09-2045,H05-1043,0,0.0299266,"Missing"
P09-2045,E06-1039,0,0.392164,"ummary of those opinions for potential buyers or manufacture companies. The task of mining reviews usually comprises two subtasks: product features extraction and summary generation. Hu and Liu (2004a) use association mining methods to find frequent product features and use opinion words to predict infrequent product features. A.M. Popescu and O. Etzioni (2005) proposes OPINE, an unsupervised information extraction system, which is built on top of the KonwItAll Web information-extraction system. In order to reduce the features redundancy and provide a conceptual view of extracted features, G. Carenini et al. (2006a) enhances the earlier work of Hu and Liu (2004a) by mapping the extracted features into a hierarchy of features which describes the entity of interest. M. Gamon et al. 1. Nicely structured, provide a natural conceptual view of products; 2. Include only relevant information of the product and contain few noise words; 3. Except for the product feature itself, usually also provide a unit to measure this feature. A typical mobile phone specification is partially given below: • Physical features – Form: Mono block with full keyboard – Dimensions: 4.49 x 2.24 x 0.39 inch – Weight: 4.47 oz • Displa"
P09-2045,H05-2017,0,\N,Missing
P09-3011,N06-4007,0,0.0727696,"Missing"
P09-3011,P98-1012,0,0.383924,"nts refers to the same person. This paper develops an agglomerative clustering approach to resolving multi-document personal name disambiguation. In order to represent texts better, a novel weight computing method for clustering features is presented. It is based on the pointwise mutual information between the Related Work Due to the varying ambiguity of personal names in a corpus, existing approaches typically cast it as an unsupervised clustering problem based on vector space model. The main difference among these approaches lies in the features, which are used to create a similarity space. Bagga & Baldwin (1998) first performed within-document coreference resolution, and then explored features in local context. Mann & Yarowsky (2003) extracted local biographical information as features. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. Chen and Martin (2007) explored the use of a range of syntactic and semantic features in unsupervised clustering of documents. Song (2007) learned the PLSA and LDA model as feature sets. Ono et al. (2008) used mixture features including co-occurrences 88 Proceedings of the ACL-IJCNLP 2009 Student Res"
P09-3011,D07-1020,0,0.0477492,"Missing"
P09-3011,W03-0405,0,0.325514,"name disambiguation. In order to represent texts better, a novel weight computing method for clustering features is presented. It is based on the pointwise mutual information between the Related Work Due to the varying ambiguity of personal names in a corpus, existing approaches typically cast it as an unsupervised clustering problem based on vector space model. The main difference among these approaches lies in the features, which are used to create a similarity space. Bagga & Baldwin (1998) first performed within-document coreference resolution, and then explored features in local context. Mann & Yarowsky (2003) extracted local biographical information as features. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. Chen and Martin (2007) explored the use of a range of syntactic and semantic features in unsupervised clustering of documents. Song (2007) learned the PLSA and LDA model as feature sets. Ono et al. (2008) used mixture features including co-occurrences 88 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88–95, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP of named entities, key compound words, a"
P09-3011,P04-1076,0,0.754499,"Missing"
P09-3011,C98-1012,0,\N,Missing
P12-1027,W06-1655,0,0.141199,"entation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number"
P12-1027,I05-3018,0,0.160445,"nefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representativ"
P12-1027,O98-3002,0,0.582016,"accuracies on both word segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review r"
P12-1027,C02-1049,0,0.113216,". 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. T"
P12-1027,I05-3019,0,0.188076,"Missing"
P12-1027,I05-3017,0,0.499942,"R), City University of Hongkong (CU), and Peking University (PKU). Details of the corpora are listed in Table 1. We did not use any extra resources such as common surnames, parts-of-speech, and semantics. Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score defined by 2P R/(P + R), and recall of new word detection (NWD recall). For more detailed information on the corpora, refer to Emerson (2005). 5.2 Features, Training, and Tuning We employed the feature templates defined in Section 3.2. The feature sets are huge. There are 2.4 × 107 features for the MSR data, 4.1 × 107 features for the CU data, and 4.7 × 107 features for the PKU data. To generate word-based features, we extracted high-frequency word-based unigram and bigram lists from the training data. As for training, we performed gradient descent MSR CU ADF SGD 96 LBFGS (batch) 95.5 94 F−score 96.5 F−score F−score 95.5 94.5 97 95 93.5 93 94.5 0 10 20 30 40 Number of Passes 92 50 0 10 20 30 40 Number of Passes MSR 94 50 0 CU 10 20"
P12-1027,P07-1104,0,0.0508865,"Missing"
P12-1027,P03-2039,0,0.03267,"g and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online"
P12-1027,C04-1081,0,0.706516,"new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and ne"
P12-1027,E09-1088,1,0.801001,"ning methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training samples used for this approximation is called the batch size. By using a smaller batch size, one can update the parameters more frequently and speed u"
P12-1027,C08-1106,1,0.0330946,"opular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training samples used for this approximation is called the batch size. By using a smaller batch size, one can update the parameters more"
P12-1027,N09-1007,1,0.647152,"Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training sample"
P12-1027,C10-2139,0,0.725279,"Missing"
P12-1027,I05-3027,0,0.248309,"Missing"
P12-1027,W00-1207,0,0.0506628,"ord segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word"
P12-1027,O11-2013,0,0.0152595,"semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stoch"
P12-1027,O03-4002,0,0.825038,"es are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting sys"
P12-1027,P07-1106,0,0.741552,"ning algorithm later. We will show in experiments that our solution is an order magnitude faster compared with exiting learning methods, and can achieve equal or even higher accuracies. The contribution of this work is as follows: • We propose a general purpose fast online training method, ADF. The proposed training method requires only a few passes to complete the training. • We propose a joint model for Chinese word segmentation and new word detection. • Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003;"
P12-1027,N06-2049,0,0.698283,"Missing"
P12-1027,I05-1047,0,0.0837938,"enters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we revi"
P12-1060,C10-2028,0,0.00911262,"studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creatin"
P12-1060,P11-2075,0,0.118846,"t approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1 http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572–581, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics et al. (2011) show that vocabulary coverage has a strong correlation with sentiment classification accuracy. Second, machine translation may change the sentiment polarity of the original"
P12-1060,C04-1121,0,0.3955,"sifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the words “excellent” and “poor”, and then uses the average scores of the phrases in a document as the sentiment of the document. Corpus-based methods are often built upon machine learning models. Pang et al. (2002) compare the performance of three commonly used machine learning models (Naive Bayes, Maximum Entropy and SVM). Gamon (2004) shows that introducing deeper linguistic features into SVM can help to improve the performance. The interested readers are referred to (Pang and Lee, 2008) for a comprehensive review of sentiment classification. 2.2 Cross-Lingual Sentiment Classification Cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) with labeled data in the source language (e.g. English), has been extensively studied in the very recent years. The basic idea is to explore the abundant labeled sentiment data in source language to alleviate the shorta"
P12-1060,P09-1028,0,0.0398786,"then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e"
P12-1060,N06-1014,0,0.01986,"third term on the right hand side (L(θ|Dt )) is optional. 2 For simplicity, we assume the prior distribution P (C) is uniform and drop it from the formulas. 3.3 Parameter Estimation Instead of estimating word projection probability (P (ws |wt ) and P (wt |ws )) and conditional probability of word to class (P (wt |c) and P (ws |c)) simultaneously in the training procedure, we estimate them separately since the word projection probability stays invariant when estimating other parameters. We estimate word projection probability using word alignment probability generated by the Berkeley aligner (Liang et al., 2006). The word alignment probabilities serves two purposes. First, they connect the corresponding words between the source language and the target language. Second, they adjust the strength of influences between the corresponding words. Figure 2 gives an example of word alignment probability. As is shown, the three words “tour de force” altogether express a positive meaning, while in Chinese the same meaning is expressed with only one word “杰作” (masterpiece). CLMM use word alignment probability to decrease the influences from “杰作” (masterpiece) to “tour”, “de” and “force” individually, using the w"
P12-1060,P11-1033,0,0.531802,". (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e.g. English and Chinese) . the authors use an unlabeled parallel corpus instead of machine translation engines. They assume parallel sentences in the corpus should have the same sentiment polarity. Besides, they assume labeled data in both language are available. They propose a method of training two classifiers based on maximum entropy formulation to maximize their prediction agreement on the parallel corpus. However, this method requires labeled data in both the source language and the targ"
P12-1060,J05-4003,0,0.0182661,"Missing"
P12-1060,W02-1011,0,0.0302846,"d data in the target language are also available. 1 Introduction Sentiment Analysis (also known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is ∗ Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is des"
P12-1060,J11-2001,0,0.018128,"w of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the"
P12-1060,P02-1053,0,0.015491,"abeled data in the target language. The paper is organized as follows. We review related work in Section 2, and present the cross-lingual mixture model in Section 3. Then we present the ex573 perimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment or"
P12-1060,D08-1058,0,0.154858,"directly adapt labeled data from the source language to target language. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. English Labeled data are first translated to Chinese, and then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual"
P12-1060,P09-1027,0,0.805218,"(Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1 http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguisti"
P12-1060,C08-1135,0,0.125358,"lso known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is ∗ Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other"
P12-1060,W06-1615,0,\N,Missing
P13-2006,P11-1138,0,0.511698,"muli,mingzhou}@microsoft.com zhlongk@qq.com wanghf@pku.edu.cn Abstract d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the"
P13-2006,E06-1002,0,0.0494954,"ity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straight"
P13-2006,D07-1074,0,0.185206,"Missing"
P13-2006,P05-1044,0,0.00959837,"es are f (d) and f (e). In Figure 3, each row shares forward path of f (d) while each column shares forward path of f (e). At backpropagation stage, gradient is summed over each row of score nodes for f (d) and over each column for f (e). Till now, our input simply consists of bag-ofwords binary vector. We can incorporate any handcrafted feature f (d, e) as: exp sim(d, e) (3) ei ∈C(m) exp sim(d, ei ) L(d, e) = − log P Finally, we seek to minimize the following training objective across all training instances: X L= L(d, e) (4) d,e The loss function is closely related to contrastive estimation (Smith and Eisner, 2005), which defines where the positive example takes probability mass from. We find that by penalizing more negative examples, convergence speed can be greatly accelerated. In our experiments, the sof tmax loss function consistently outperforms pairwise ranking loss function, which is taken as our default setting. sim(d, e) = Dot(f (d), f (e)) + ~λf~(d, e) (5) In fact, we find that with only Dot(f (d), f (e)) as ranking score, the performance is sufficiently good. So we leave this as our future work. 32 3 Experiments and Analysis different decisions. To our surprise, our method with only local evi"
P13-2006,D11-1072,0,0.84346,"Missing"
P13-2006,N10-1072,0,0.0590816,"Missing"
P13-2006,P11-1115,0,0.0101432,"d pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness"
P13-2006,C10-1142,0,\N,Missing
P13-2006,P14-2013,0,\N,Missing
P13-2006,P14-1062,0,\N,Missing
P13-2006,Q14-1019,0,\N,Missing
P13-2006,W12-6324,0,\N,Missing
P13-2006,P14-1146,1,\N,Missing
P13-2006,W12-6322,0,\N,Missing
P13-2006,W12-6325,0,\N,Missing
P13-2006,W12-6323,0,\N,Missing
P13-2032,W06-0116,0,0.0269942,"paper we have presented an effective yet simple approach to Chinese word segmentation on micro-blog texts. In our approach, punctuation information of unlabeled micro-blog data is used, as well as a self-training framework to incorporate confident instances. Experiments show that our approach improves performance, especially in OOV-recall. Both the punctuation information and the self-training phase contribute to this improvement. Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Acknowledgments This research was partly supported by National Hi"
P13-2032,H89-2048,0,0.0471137,"and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a compromising solution to by using a classifier to select the most confident characters. We do not follow this approach because the initial errors will dramatically harm the performance. Instead, we only add the characters after punctuations which are sure to be the beginning of words (which means labeling ’B’) into our training set. Sun and Xu (2011) uses punctuation information as discrete feature in a sequence labeling framework, which shows improvement compared to the pure sequence labeling approach. Our method is differe"
P13-2032,J04-1004,0,0.0612262,"recall. Both the punctuation information and the self-training phase contribute to this improvement. Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Acknowledgments This research was partly supported by National High Technology Research and Development Program of China (863 Program) (No. 2012AA011101), National Natural Science Foundation of China (No.91024009) and Major National Social Science Fund of China(No. 12&ZD227). 180 References pages 1265–1271. Association for Computational Linguistics. Bickel, S., Br¨uckner, M., and Scheffer, T. (2007"
P13-2032,P04-1075,0,0.0244405,"’N’. The comparison of Maxent and No-punctuation 179 Size 0 10000 50000 100000 200000 P 0.864 0.872 0.875 0.874 0.865 R 0.846 0.869 0.875 0.879 0.865 F 0.855 0.871 0.875 0.876 0.865 OOV-R 0.754 0.765 0.773 0.772 0.759 Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a compromising solution to by using a classifier to select the most confident characters. We do not follow this approach because the initial errors will dramatically harm the"
P13-2032,P06-1085,0,0.0297536,"our approach improves performance, especially in OOV-recall. Both the punctuation information and the self-training phase contribute to this improvement. Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Acknowledgments This research was partly supported by National High Technology Research and Development Program of China (863 Program) (No. 2012AA011101), National Natural Science Foundation of China (No.91024009) and Major National Social Science Fund of China(No. 12&ZD227). 180 References pages 1265–1271. Association for Computational Linguistics."
P13-2032,D11-1090,0,0.194242,"the labels balanced. When enlarging corpus using characters behind punctuations from texts in target domain, only characters labeling ’B’ are added. We randomly reuse some characters labeling ’N’ from labeled data until ratio η is reached. We do not use characters ahead of punctuations, because the single-character words ahead of punctuations take the label of ’B’ instead of ’N’. In summary our algorithm tackles the problem by duplicating labeled data in source domain. We denote our algorithm as ”ADD-N”. We also use baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). Our algorithm is not necessarily limited to a specific tagger. For simplicity and reliability, we use a simple MaximumEntropy tagger. 1 http://open.weibo.com/wiki http://www.sighan.org/bakeoff2005/ 3 http://ictclas.org/ 4 http://nlp.stanford.edu/projects/ chinese-nlp.shtml#cws 2 178 评 B B 论 N 是 B 风 B 格 N ， B 评 B B 论 N 是 B 能 B 力 N 。 B Table 2: The first line represents the original text. The second line indicates whether each character is the Beginning of sentence. The third line is the tag sequence using ”BN” tag set. ADD-N algorithm Input: labeled data {(xi , yi )li−1 },"
P13-2032,P07-1034,0,0.0170336,"179 Size 0 10000 50000 100000 200000 P 0.864 0.872 0.875 0.874 0.865 R 0.846 0.869 0.875 0.879 0.865 F 0.855 0.871 0.875 0.876 0.865 OOV-R 0.754 0.765 0.773 0.772 0.759 Meanwhile semi-supervised methods have been applied into NLP applications. Bickel et al. (2007) learns a scaling factor from data of source domain and use the distribution to resemble target domain distribution. Wu et al. (2009) uses a Domain adaptive bootstrapping (DAB) framework, which shows good results on Named Entity Recognition. Similar semi-supervised applications include Shen et al. (2004); Daum´e III and Marcu (2006); Jiang and Zhai (2007); Weinberger et al. (2006). Besides, Sun and Xu (2011) uses a sequence labeling framework, while unsupervised statistics are used as discrete features in their model, which prove to be effective in Chinese word segmentation. There are previous works using punctuations as implicit annotations. Riley (1989) uses it in sentence boundary detection. Li and Sun (2009) proposed a compromising solution to by using a classifier to select the most confident characters. We do not follow this approach because the initial errors will dramatically harm the performance. Instead, we only add the characters af"
P13-2032,P12-1027,1,0.831372,"ed. When enlarging corpus using characters behind punctuations from texts in target domain, only characters labeling ’B’ are added. We randomly reuse some characters labeling ’N’ from labeled data until ratio η is reached. We do not use characters ahead of punctuations, because the single-character words ahead of punctuations take the label of ’B’ instead of ’N’. In summary our algorithm tackles the problem by duplicating labeled data in source domain. We denote our algorithm as ”ADD-N”. We also use baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). Our algorithm is not necessarily limited to a specific tagger. For simplicity and reliability, we use a simple MaximumEntropy tagger. 1 http://open.weibo.com/wiki http://www.sighan.org/bakeoff2005/ 3 http://ictclas.org/ 4 http://nlp.stanford.edu/projects/ chinese-nlp.shtml#cws 2 178 评 B B 论 N 是 B 风 B 格 N ， B 评 B B 论 N 是 B 能 B 力 N 。 B Table 2: The first line represents the original text. The second line indicates whether each character is the Beginning of sentence. The third line is the tag sequence using ”BN” tag set. ADD-N algorithm Input: labeled data {(xi , yi )li−1 }, unlabeled data {xj"
P13-2032,P06-2056,0,0.0208499,"rformance, especially in OOV-recall. Both the punctuation information and the self-training phase contribute to this improvement. Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Acknowledgments This research was partly supported by National High Technology Research and Development Program of China (863 Program) (No. 2012AA011101), National Natural Science Foundation of China (No.91024009) and Major National Social Science Fund of China(No. 12&ZD227). 180 References pages 1265–1271. Association for Computational Linguistics. Bickel, S., Br¨uckner, M., an"
P13-2032,J09-4006,0,0.794734,"ion Bakeoff2 as the labeled data. We choose the PKU data in our experiment because our baseline methods use the same segmentation standard. We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS3 and Stanford Chinese word segmenter4 , which are widely used in NLP related to word segmentation. Stanford Chinese word segmenter is a CRF-based segmentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours. ICTCLAS, on the other hand, is a HMMbased Chinese word segmenter. Another baseline is Li and Sun (2009), which also uses punctuation in their semi-supervised framework. F-score Algorithm Our algorithm “ADD-N” is shown in TABLE 3. The initially selected character instances are those right after punctuations. By definition they are all labeled with ’B’. In this case, the number of training instances with label ’B’ is increased while the number with label ’N’ remains unchanged. Because of this, the model trained on this unbalanced corpus tends to be biased. This problem can become even worse when there is inexhaustible supply of texts from the target domain. We assume that labeled corpus of the so"
P13-2032,D09-1158,0,0.0487341,"Missing"
P13-2032,I05-3025,0,0.0758168,"rison of BN and EN. 4 Conclusion In this paper we have presented an effective yet simple approach to Chinese word segmentation on micro-blog texts. In our approach, punctuation information of unlabeled micro-blog data is used, as well as a self-training framework to incorporate confident instances. Experiments show that our approach improves performance, especially in OOV-recall. Both the punctuation information and the self-training phase contribute to this improvement. Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Acknowledgments This res"
P13-2032,O03-4002,0,0.544892,"ented an effective yet simple approach to Chinese word segmentation on micro-blog texts. In our approach, punctuation information of unlabeled micro-blog data is used, as well as a self-training framework to incorporate confident instances. Experiments show that our approach improves performance, especially in OOV-recall. Both the punctuation information and the self-training phase contribute to this improvement. Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Acknowledgments This research was partly supported by National High Technolog"
P13-2032,Y06-1012,0,0.0239742,". 4 Conclusion In this paper we have presented an effective yet simple approach to Chinese word segmentation on micro-blog texts. In our approach, punctuation information of unlabeled micro-blog data is used, as well as a self-training framework to incorporate confident instances. Experiments show that our approach improves performance, especially in OOV-recall. Both the punctuation information and the self-training phase contribute to this improvement. Related Work Recent studies show that character sequence labeling is an effective formulation of Chinese word segmentation (Low et al., 2005; Zhao et al., 2006a,b; Chen et al., 2006; Xue, 2003). These supervised methods show good results, however, are unable to incorporate information from new domain, where OOV problem is a big challenge for the research community. On the other hand unsupervised word segmentation Peng and Schuurmans (2001); Goldwater et al. (2006); Jin and Tanaka-Ishii (2006); Feng et al. (2004); Maosong et al. (1998) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. However, they usually are less accurate and more complicated than supervised ones. Acknowledgments This research was partly su"
P13-2032,P98-2206,0,\N,Missing
P13-2032,C98-2201,0,\N,Missing
P13-2032,W06-0127,0,\N,Missing
P15-2047,P03-1054,0,0.0500541,"output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 287 Model relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 SVM MV-RNN CNN FCM DT-RNN DepNN Contributions of different components baseline (Path words) +Depedency relations +Attached subtrees +Lexical features 50-d 73.8 80.3 81.2 82.7 F1 200-d 75.5 81.8 82.8 83.6 We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the"
P15-2047,N07-2032,0,0.0416795,"Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path"
P15-2047,S10-1057,0,0.346496,"ve for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indi"
P15-2047,D12-1110,0,0.0907578,"word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings."
P15-2047,Q14-1017,0,0.023951,"n learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepNN, but not as obvious as NER. Yu et al. (2014) had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly rel"
P15-2047,H05-1091,0,0.250704,"n Li1,2 Heng Ji4 Ming Zhou3 Houfeng Wang1,2 1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 2 Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China 3 Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2"
P15-2047,I08-2119,0,0.124479,"Missing"
P15-2047,C14-1220,0,0.431582,"re, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combinat"
P15-2047,P05-1053,0,\N,Missing
P15-2047,W08-1301,0,\N,Missing
P15-2047,P04-1054,0,\N,Missing
P15-2047,D14-1070,0,\N,Missing
P15-2047,P06-1104,0,\N,Missing
P15-2136,D14-1181,0,0.00276888,"Missing"
P15-2136,W04-1013,0,0.0525643,"ver, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe , shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the regression framework of Li et al. (2007). The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to estimate this saliency. φ = [xp , xe ] (3) wrT (4) yˆ = ×φ AVG-CF Description The position of the sentence. The averaged term frequency values of words in the sentence. The averaged cluster frequency values of words in the sentence. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowl"
P15-2136,W02-0401,0,0.431087,"res beyond word level (e.g., phrases) are seldom involved in current research. The CTSUM system developed by Wan and Zhang (2014) is the most relevant to ours. It attempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization. To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR). HowIntroduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September"
P15-2136,P14-2105,0,0.013617,"The underlined phrases greatly reduce the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summar"
P15-2136,C14-1220,0,0.00457144,"the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental resu"
P15-2136,W06-1643,0,\N,Missing
P15-2136,E14-1075,0,\N,Missing
P16-1072,P15-2047,1,0.759139,"ions, while recurrent neural networks adaptively accumulate the context information in the whole sentence via memory units, thereby encoding the global and possibly unconsecutive patterns for relation classification. Socher et al. (2012) learned compositional vector representations of sentences with a recursive neural network. Kazuma et al. (2013) proposed a simple customizaition of recursive neural networks. Zeng et al. (2014) proposed a convolutional neural network with position embeddings. Recently, more attentions have been paid to modeling the shortest dependency path (SDP) of sentences. Liu et al. (2015) developed a dependency-based neural network, in which a convolutional neural network has been used to capture features on the shortest path and a recursive neural network is designed to model subtrees. Xu et al. (2015b) applied long short term memory (LSTM) based recurrent neural networks (RNNs) along the shortest dependency path. However, SDP is a special structure in which every two neighbor words are separated by a dependency relations. Previous works treated dependency relations in the same Relation classification is an important semantic processing task in the field of natural language p"
P16-1072,P13-1147,0,0.0437117,"Missing"
P16-1072,S10-1057,0,0.150316,"nd R x (e2 , e1 ). Two fine-grained so f tmax classifiers are ap→ − ← − plied to G and G with linear transformation to −y and ← − give the (2K+1)-class distribution → y respectively. Formally, (11) 2K+1 X i=1 (10) − → −y = so f tmax(W · → f G + bf ) Training Objective 3 3.1 Experiments Dataset We evaluated our BRCNN model on the SemEval2010 Task 8 dataset, which is an established benchmark for relation classification (Hendrickx et al., 2010). The dataset contains 8000 sentences for training, and 2717 for testing. We split 800 samples out of the training set for validation. 760 Classifier SVM (Rink and Harabagiu, 2010) RNN (Socher et al., 2011) MVRNN (Socher et al., 2012) CNN (Zeng et al., 2014) FCM (Yu et al., 2014) CR-CNN (dos Santos et al., 2015) SDP-LSTM (Xu et al., 2015b) DepNN (Liu et al., 2015) depLCNN (Xu et al., 2015a) BRCNN (Our Model) Additional Information POS, WordNet, Prefixes and other morphological features, dependency parse, Levin classed, PropBank, FanmeNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner Word embeddings + POS, NER, WordNet Word embeddings + POS, NER, WordNet Word embeddings + word position embeddings, WordNet Word embeddings + dependency parsing, NER Word embeddings +"
P16-1072,H05-1091,0,0.550941,"y squashed by an activation function. Formally, we have The Shortest Dependency Path If e1 and e2 are two entities mentioned in the same sentence such that they are observed to be in a relationship R, the shortest path between e1 and e2 condenses most illuminating information for the relationship R(e1 , e2 ). It is because (1) if entities e1 and e2 are arguments of the same predicate, the shortest path between them will pass through the predicate; (2) if e1 and e2 belong to different predicate-argument structures that share a common argument, the shortest path will pass through this argument. Bunescu and Mooney (2005) first used shortest dependency paths between two entities to capture the predicate-argument sequences, which provided strong evidence for relation classification. Xu et al. (2015b) captured information from the sub-paths separated by the common ancestor node of two entities in the shortest paths. However, the shortest dependency path between two entities is usually short (∼4 on average) , and the common ancestor of some SDPs is e1 or e2 , which leads to imbalance of two sub-paths. ht = f (Win · xt + Wrec · ht−1 + bh ) (1) where Win and Wrec are weight matrices for the input and recurrent conn"
P16-1072,D11-1014,0,0.0457954,"Missing"
P16-1072,P15-1061,0,0.141987,"role in this task. Socher et al. (2012) introduced a recursive neural network model that assigns a matrix-vector representation to every node in a parse tree, in order to learn compositional vector representations for sentences of arbitrary syntactic type and length. Convolutional neural works are widely used in relation classification. Zeng et al. (2014) proposed an approach for relation classification where sentence-level features are learned through a CNN, which has word embedding and position features as its input. In parallel, lexical features were extracted according to given nouns. dos Santos et al. (2015) tackled the relation classification task using a convolutional neural network and proposed a new pairwise ranking loss function, which achieved the state-of-the-art result in SemEval2010 Task 8. Related Work Relation classification is an important topic in NLP. Traditional Methods for relation classification mainly fall into three classes: feature-based, kernel-based and neural network-based. In feature-based approaches, different types of features are extracted and fed into a classifier. Generally, three types of features are often used. Lexical features concentrate on the entities of intere"
P16-1072,D12-1110,0,0.0293901,"res automatically based on neural networks (NN), employing continuous representations of words (word embeddings). The NN research for relation classification has centered around two main network architectures: convolutional neural networks and recursive/recurrent neural networks. Convolutional neural network aims to generalize the local and consecutive context of the relation mentions, while recurrent neural networks adaptively accumulate the context information in the whole sentence via memory units, thereby encoding the global and possibly unconsecutive patterns for relation classification. Socher et al. (2012) learned compositional vector representations of sentences with a recursive neural network. Kazuma et al. (2013) proposed a simple customizaition of recursive neural networks. Zeng et al. (2014) proposed a convolutional neural network with position embeddings. Recently, more attentions have been paid to modeling the shortest dependency path (SDP) of sentences. Liu et al. (2015) developed a dependency-based neural network, in which a convolutional neural network has been used to capture features on the shortest path and a recursive neural network is designed to model subtrees. Xu et al. (2015b)"
P16-1072,D13-1137,0,0.0300435,"Missing"
P16-1072,I08-2119,0,0.056586,"rest, e.g., POS. Syntactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class. Kambhatla (2004) used a maximum entropy model for feature combination. Rink and Harabagiu (2010) collected various features, including lexical, syntactic as well as semantic features. In kernel based methods, similarity between two data samples is measured without explicit feature representation. Bunescu and Mooney (2005) designed a kernel along the shortest dependency path between two entities by observing that the relation strongly relies on SDPs. Wang (2008) provided a systematic analysis of several kernels and showed that relation extraction can benefit from combining convolution kernel and syntactic Yu et al. (2014) proposed a Factor-based Compositional Embedding Model (FCM) by deriving sentence-level and substructure embeddings from word embeddings, utilizing dependency trees and named entities. It achieved slightly higher accuracy on the same dataset than Zeng et al. (2014), but only when syntactic information is used. Nowadays, many works concentrate on extracting features from the SDP based on neural networks. Xu et al. (2015a) learned robu"
P16-1072,W09-2415,0,0.182167,"Missing"
P16-1072,D15-1062,0,0.759395,"her et al. (2012) learned compositional vector representations of sentences with a recursive neural network. Kazuma et al. (2013) proposed a simple customizaition of recursive neural networks. Zeng et al. (2014) proposed a convolutional neural network with position embeddings. Recently, more attentions have been paid to modeling the shortest dependency path (SDP) of sentences. Liu et al. (2015) developed a dependency-based neural network, in which a convolutional neural network has been used to capture features on the shortest path and a recursive neural network is designed to model subtrees. Xu et al. (2015b) applied long short term memory (LSTM) based recurrent neural networks (RNNs) along the shortest dependency path. However, SDP is a special structure in which every two neighbor words are separated by a dependency relations. Previous works treated dependency relations in the same Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between tw"
P16-1072,D15-1206,0,0.758621,"her et al. (2012) learned compositional vector representations of sentences with a recursive neural network. Kazuma et al. (2013) proposed a simple customizaition of recursive neural networks. Zeng et al. (2014) proposed a convolutional neural network with position embeddings. Recently, more attentions have been paid to modeling the shortest dependency path (SDP) of sentences. Liu et al. (2015) developed a dependency-based neural network, in which a convolutional neural network has been used to capture features on the shortest path and a recursive neural network is designed to model subtrees. Xu et al. (2015b) applied long short term memory (LSTM) based recurrent neural networks (RNNs) along the shortest dependency path. However, SDP is a special structure in which every two neighbor words are separated by a dependency relations. Previous works treated dependency relations in the same Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between tw"
P16-1072,C14-1220,0,0.735767,"rchitectures: convolutional neural networks and recursive/recurrent neural networks. Convolutional neural network aims to generalize the local and consecutive context of the relation mentions, while recurrent neural networks adaptively accumulate the context information in the whole sentence via memory units, thereby encoding the global and possibly unconsecutive patterns for relation classification. Socher et al. (2012) learned compositional vector representations of sentences with a recursive neural network. Kazuma et al. (2013) proposed a simple customizaition of recursive neural networks. Zeng et al. (2014) proposed a convolutional neural network with position embeddings. Recently, more attentions have been paid to modeling the shortest dependency path (SDP) of sentences. Liu et al. (2015) developed a dependency-based neural network, in which a convolutional neural network has been used to capture features on the shortest path and a recursive neural network is designed to model subtrees. Xu et al. (2015b) applied long short term memory (LSTM) based recurrent neural networks (RNNs) along the shortest dependency path. However, SDP is a special structure in which every two neighbor words are separa"
P16-1212,P14-1091,1,0.746378,"er to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM"
P16-1212,P14-1133,0,0.0178386,"variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along wit"
P16-1212,D13-1160,0,0.016404,"der framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be"
P16-1212,D14-1179,0,0.00526158,"Missing"
P16-1212,N03-1017,0,0.0311904,"Target Generation can generate a natural language sentence based on the existing semantic tuples; • Combining them, KBSE can be used to translation a source sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there"
P16-1212,P07-2045,0,0.0586397,"ce sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed"
P16-1212,P11-1060,0,0.0109095,"d on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what i"
P16-1212,P13-1078,0,0.0244095,"es for number of objects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a senten"
P16-1212,P14-1140,1,0.808281,"bjects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also c"
P16-1212,P03-1021,0,0.0453806,"periments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed KBSE, the number of hidden units in both parts are 300. Embedding size of both source and target are 200. Adadelta (Zeiler, 2012) 1 http://www.statmt.org/moses/ ht"
P16-1212,P02-1040,0,0.100737,"Missing"
P16-1212,D15-1199,0,0.0201843,"Missing"
P16-1212,P15-1128,0,0.00689599,"ce into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along with the sentence generate"
P16-1212,J07-2003,0,\N,Missing
P17-2029,J92-4003,0,0.0952582,", we use 18 coarse-grained relations and binarize non-binary relations with right-branching (Sagae and Lavie, 2005). For preprocessing, we use the Stanford CoreNLP toolkit (Manning et al., 2014) to lemmatize words, get POS tags, segment sentences and syntactically parse them. To directly compare with other discourse parsing systems, we employ the same evaluation met• N-gram features: the first and the last n words and their POS tags in the text of S1 , S2 , Q1 , where n ∈ {1, 2}. • Nucleus features: the dependency heads of the nucleus EDUs2 for S1 , S2 , Q1 and their POS tags; brown clusters (Brown et al., 1992; 2 Nucleus EDU is defined by recursively selecting the Nucleus in the binary tree until an EDU (leaf node) is reached. 186 rics, i.e. the precision, recall and F-score 3 with respect to span (S), nuclearity (N) and relation (R), as defined by Marcu (2000). 4.2 egy is adopted, Level denotes whether three kinds of relations (i.e., within-sentence, across-sentence, and across-paragraph) are differently classified, and Tree represents whether relation labeling uses tree features generated in the first stage. The simplest model Simp-1 is almost the same as (Heilman and Sagae, 2015) except that we"
P17-2029,P14-1048,0,0.536975,"o that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in the unified processing of span, nuclearity and relation, they report weaker performance than other methods, like CYK-like algorithms (Li et al., 2014, 2016) or greedy bottom-up algorithms that merge adjacent spans (Hernault et al., 2010; Feng and Hirst, 2014). In such cases, we analyze that the labelled data can not sufficiently support the classifier to distinguish among the information-rich actions (e.g., Reduce-NS-Contrast) , since there exist very few labelled text-level discourse corpus available for training. The limited training data will cause unbalanced actions and lead to the problems of data sparsity and overfitting. Thus, we propose to use the transition-based model to parse a naked disPrevious work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and rel"
P17-2029,P14-1002,0,0.71867,"el, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in the unified processing of span, nuclearity and relation, they report weaker performance than other methods, like CYK-like algorithms (Li et al., 2014, 2016) or greedy bottom-up algorithms that merge adjacent spans (Hernault et al., 2010; Feng and Hirst, 2014). In such cases, we analyze that the labelled data can not sufficiently support the classifier to distinguish among the information-rich actions (e.g., Reduce-NS-Contrast) , since there exist very few labelled text-level discourse corpus available for train"
P17-2029,W06-2933,0,0.0128128,"@pku.edu.cn Abstract tions to form larger text spans until the final tree is built. RST also depicts which part is more important in a relation by tagging Nucleus or Satellite. Generally, each relation at least includes a Nucleus and there are three nuclearity types: NucleusSatellite (NS), Satellite-Nucleus (SN) and NucleusNucleus (NN). Therefore, the performance of RST discourse parsing can be evaluated from three aspects: span, nuclearity and relation. To parse discourse trees, transition-based parsing model, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in"
P17-2029,W09-3813,0,0.186245,"e discourse trees, transition-based parsing model, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based models show advantages in the unified processing of span, nuclearity and relation, they report weaker performance than other methods, like CYK-like algorithms (Li et al., 2014, 2016) or greedy bottom-up algorithms that merge adjacent spans (Hernault et al., 2010; Feng and Hirst, 2014). In such cases, we analyze that the labelled data can not sufficiently support the classifier to distinguish among the information-rich actions (e.g., Reduce-NS-Contrast) , since there exist very fe"
P17-2029,W05-1513,0,0.0515507,"sing task. • Structural features: nuclearity type (NN, NS or SN) of S1 , S2 ; number of EDUs and sentences in S1 , S2 ; length comparison of S1 , S2 with respect to EDUs and sentences. • Dependency features: whether dependency relations exist between S1 , S2 or between S1 , Q1 ; the dependency direction and relation type. 4.1 Setup RST-DT annotates 385 documents (347 for training and 38 for testing) from the Wall Street Journal using Rhetorical Structure Theory (Mann and Thompson, 1988). Conventionally, we use 18 coarse-grained relations and binarize non-binary relations with right-branching (Sagae and Lavie, 2005). For preprocessing, we use the Stanford CoreNLP toolkit (Manning et al., 2014) to lemmatize words, get POS tags, segment sentences and syntactically parse them. To directly compare with other discourse parsing systems, we employ the same evaluation met• N-gram features: the first and the last n words and their POS tags in the text of S1 , S2 , Q1 , where n ∈ {1, 2}. • Nucleus features: the dependency heads of the nucleus EDUs2 for S1 , S2 , Q1 and their POS tags; brown clusters (Brown et al., 1992; 2 Nucleus EDU is defined by recursively selecting the Nucleus in the binary tree until an EDU ("
P17-2029,P10-1040,0,0.0081553,"e stack. A parse tree can be finally constructed until the queue is empty and the stack only contains the complete tree. Only one classifier is learned to judge the actions at each step. To derive a discourse tree in a unified framework, prior systems design multiple reduce actions 1 Relation label is actually assigned to the satellite subtree and a “Span” label is assigned to the nucleus substree. 185 whether its left and right subtrees are in different paragraphs, or the same paragraph, or the same sentence. For each level, we predict a relation label using the corresponding classifier. 2.3 Turian et al., 2010) of all the words in the nucleus EDUs of S1 , S2 , Q1 . Next, we list all the features used for the three relation classifiers. Given an internal node P in the naked tree, we aim to predict the relation between its left child Clef t and right child Cright . Dependency features, N-gram features and nucleus features discussed above are also needed, the only difference is that these features are applied to the left and right children. Other features include: • Refined Structural features: nuclearity type of node P ; distance from P , Clef t , Cright to the start and end of the document / paragrap"
P17-2029,P13-1048,0,0.392385,"rs), pages 184–188 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2029 course tree (i.e., identifying span and nuclearity) in the first stage. The benefits are three-fold. First, we can still use the transition based model which is a good tree construction tool. Second, much fewer actions need to be identified in the tree construction process. Third, we could separately label relations, which needs careful consideration. In the second stage, relation labels for each span are determined independently. Prior studies (Joty et al., 2013; Feng and Hirst, 2014) have found that rhetorical relations distribute differently intra-sententially vs. multi-sententially. They discriminate the two levels by training two models with different feature sets. We take a further step and argue that relations between paragraphs are usually more loosely connected than those between sentences within the same paragraph. Therefore we train three separate classifiers for labeling relations at three levels: withinsentence, across-sentence and across-paragraph. Different features are used for each classifier and the naked tree structure generated in"
P17-2029,D14-1220,0,0.152824,"Missing"
P17-2029,W03-3023,0,0.0291753,"{yizhong, lisujian, wanghf}@pku.edu.cn Abstract tions to form larger text spans until the final tree is built. RST also depicts which part is more important in a relation by tagging Nucleus or Satellite. Generally, each relation at least includes a Nucleus and there are three nuclearity types: NucleusSatellite (NS), Satellite-Nucleus (SN) and NucleusNucleus (NN). Therefore, the performance of RST discourse parsing can be evaluated from three aspects: span, nuclearity and relation. To parse discourse trees, transition-based parsing model, which gains significant success in dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006) , was introduced to discourse analysis. Marcu (1999) first employed a transition system to derive a discourse parse tree. In such a system, action labels are designed by combining shift-reduce action with nuclearity and relation labels, so that one classifier can determine span, nuclearity and relation simultaneously via judging actions. More recent studies followed this research line and enhanced the performance by either tuning the models (Sagae, 2009) or using more effective features (Ji and Eisenstein, 2014; Heilman and Sagae, 2015). Though these transition-based mode"
P17-2029,P99-1047,0,\N,Missing
P17-2100,N16-1012,0,0.185993,"Missing"
P17-2100,P16-1154,0,0.0428289,"Missing"
P17-2100,D16-1009,0,0.0234952,"vectors, and the decoder generates summaries and produces semantic vectors of the generated summaries. Finally, the similarity function evaluates the relevance between the sematic vectors of source texts and generated summaries. Our training objective is to maximize the similarity score so that the generated summaries have high semantic relevance with source texts. (1) 3.1 Text Representation There are several methods to represent a text or a sentence, such as mean pooling of RNN output or reserving the last state of RNN. In our model, source text is represented by a gated attention encoder (Hahn and Keller, 2016). Every upcoming word is fed into a gated attention network, which measures its importance. The gated attention network outputs the important score with a feedforward network. At each time step, it inputs a word vector et and its previous context vector ht , then outputs the score βt . Then the word vector et is multiplied by the score βt , and fed into RNN encoder. We select the last output hN of RNN encoder as the semantic vector of the source text Vt . A natural idea to get the semantic vector of a summary is to feed it into the encoder as well. However, this method wastes much time because"
P17-2100,D15-1229,0,0.633711,"literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text literally, but it has low semantic relevance. In this work, our goal is to improve the semantic relevance between source texts and generated summaries for Chinese social media text summarization. To achieve this goal,"
P17-2100,N03-1020,0,0.426438,"Missing"
P17-2100,D15-1166,0,0.236244,"Missing"
P17-2100,P16-1046,0,0.0589016,"联航空机场发生爆炸致多人死亡。 China United Airlines exploded in the airport, leaving several people dead. Gold: 航班多人吸烟机组人员与乘客冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a curren"
P17-2100,K16-1028,0,0.0653475,"Missing"
P17-2100,radev-etal-2004-mead,0,0.073768,"Missing"
P17-2100,D15-1044,0,0.0636919,"ilarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text literally, but it has low semantic relevance. In this work, our goal is to improve the semantic relevance between source texts and generated summaries for Chinese social media text summarization. To"
P17-2100,C16-1019,1,0.802479,"we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initialized word embedding. We tune our parameter on the development set. In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter λ is 0.0001. Both the encoder and decoder are based on LSTM unit. Following the"
P17-2100,P16-1218,0,0.0181009,"summary pairs, constructed from a famous Chinese social media website called Sina Weibo1 . It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the textsummary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5, and we only reserve pairs with scores no less than 3. Following the previous work, we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random"
P17-2100,P10-1058,0,0.0357048,"cial media corpus. 1 RNN: 中联航空机场发生爆炸致多人死亡。 China United Airlines exploded in the airport, leaving several people dead. Gold: 航班多人吸烟机组人员与乘客冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summ"
P17-2100,P16-2092,1,0.794027,"s work, we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initialized word embedding. We tune our parameter on the development set. In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter λ is 0.0001. Both the encoder and decoder are based on LSTM unit. Following the"
P17-2100,P15-1001,0,\N,Missing
P18-1090,I17-1064,1,0.782224,"nal sentence. A related study is “back reconstruction” in machine translation (He et al., 2016; Tu et al., 2017). They couple two inverse tasks: one is for translating a sentence in language A to a sentence in language B; the other is for translating a sentence in language B to a sentence in language A. Different from the previous work, we do not introduce the inverse task, but use collaboration between the neutralization module and the emotionalization module. Sentiment analysis is also related to our work (Socher et al., 2011; Pontiki et al., 2015; Rosenthal et al., 2017; Chen et al., 2017; Ma et al., 2017, 2018b). The task usually involves detecting whether a piece of text expresses positive, negative, or neutral sentiment. The sentiment can be general or about a specific topic. one of the reinforcement learning methods, to reward the output of the neutralization module based on the feedback from the emotionalization module. We add different sentiment to the semantic content and use the quality of the generated text as reward. The quality is evaluated by two useful metrics: one for identifying whether the generated text matches the target sentiment; one for evaluating the content preservation"
P18-1090,N18-1018,1,0.88559,"Missing"
P18-1090,E17-1059,0,0.055843,"Missing"
P18-1090,P02-1040,0,0.103498,"dataset contains 230K, 10K, and 3K pairs for training, validation, and testing, respectively. (9) where Rc is calculated as Rc = R1 + R2 (10) Based on Eq. 8 and Eq. 9, we use the sampling approach to estimate the expected reward. This cycled process is repeated until converge. 3.4.1 Reward The reward consists of two parts, sentiment confidence and BLEU. Sentiment confidence evaluates whether the generated text matches the target sentiment. We use a pre-trained classifier to make the judgment. Specially, we use the proposed selfattention based sentiment classifier for implementation. The BLEU (Papineni et al., 2002) score is used to measure the content preservation performance. Considering that the reward should encourage the model to improve both metrics, we use the harmonic mean of sentiment confidence and BLEU as reward, which is formulated as R = (1 + β 2 ) 2 · BLEU · Conf id (β 2 · BLEU ) + Conf id 4.2 We tune hyper-parameters based on the performance on the validation sets. The self-attention based sentiment classifier is trained for 10 epochs on two datasets. We set β for calculating reward to 0.5, hidden size to 256, embedding size to 128, vocabulary size to 50K, learning rate to 0.6, and batch s"
P18-1090,S15-2082,0,0.0806921,"Missing"
P18-1090,S17-2088,0,0.0733652,"Missing"
P18-1090,W17-3526,0,0.0284011,"Missing"
P18-1162,P15-2113,0,0.390902,"ry words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], and batch size [8, 16, 32]. We take the best configuration based on performance on the development set, and only evaluate that configuration on the test set. In order to mitigate the class imbalance problem, median frequency balancing Eigen and Fergus (2015) is used to reweight each class in the cross-entropy loss. Therefore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but"
P18-1162,P06-4018,0,0.0100513,"we adopt the F1 score and accuracy on two categories for evaluation. SemEval 2017 regards answer selection as a ranking task, which is closer to the application scenario. As a result, mean average precision (MAP) is used as an evaluation measure. For a perfect ranking, a system has to place all Good answers above the PotentiallyUseful and Bad answers. The latter two are not actually distinguished and are considered Bad in terms of evaluation. Additionally, standard classification measures like accuracy and F1 score are also reported. 4.3 Implementation Details We use the tokenizer from NLTK (Bird, 2006) to preprocess each sentence. All word embeddings in the sentence encoder layer are initialized with the 300-dimensional GloVe (Pennington et al., 2014) word vectors trained on the domainspecific unannotated corpus, and embeddings for out-of-vocabulary words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], an"
P18-1162,S17-2045,1,0.847421,"Missing"
P18-1162,S16-1172,0,0.0630292,". The SemEval CQA tasks (Nakov et al., 2015, 2016, 2017) provide universal benchmark datasets for evaluating researches on this problem. Earlier work of answer selection in CQA relied heavily on feature engineering, linguistic tools, and external resource. Nakov et al. (2016) investigated a wide range of feature types including similarity features, content features, thread level/meta features, and automatically generated features for SemEval CQA models. Tran et al. (2015) studied the use of topic model based features and word vector representation based features in the answer re-ranking task. Filice et al. (2016) designed various heuristic features and thread-based features 1753 that can signal a good answer. Although achieving good performance, these methods rely heavily on feature engineering, which requires a large amount of manual work and domain expertise. Since answer selection is inherently a ranking task, a few recent researches proposed to use local features to make global ranking decision. BarrónCedeño et al. (2015) was the first work that applies structured prediction model on CQA answer selection task. Joty et al. (2016) approached the task with a global inference process to exploit the in"
P18-1162,S17-2053,0,0.128356,"fore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but it does not include a development set. Following previous work (Filice et al., 2017), we use the 2016 official test set as the development set. 5.1 SemEval 2015 Results Table 3 compares our model with the following baselines: 1750 Methods (1) JAIST (2) HITSZ-ICRC (3) Graph-cut (4) FCCRF (5) BGMN (6) CNN-LSTM-CRF (7) QCN Table 3: dataset. F1 78.96 76.52 80.55 81.50 77.23 82.22 83.91 Acc 79.10 76.11 79.80 80.50 78.40 82.24 85.65 Methods (1) KeLP (2) Beihang-MSRA (3) ECNU (4) LSTM (5) LSTM-subject-body (6) QCN Comparisons on the SemEval 2015 • JAIST (Tran et al., 2015): It used an SVM classifier to incorporate various kinds of features , including topic model based features and"
P18-1162,S15-2035,0,0.0173659,"development set. 5.1 SemEval 2015 Results Table 3 compares our model with the following baselines: 1750 Methods (1) JAIST (2) HITSZ-ICRC (3) Graph-cut (4) FCCRF (5) BGMN (6) CNN-LSTM-CRF (7) QCN Table 3: dataset. F1 78.96 76.52 80.55 81.50 77.23 82.22 83.91 Acc 79.10 76.11 79.80 80.50 78.40 82.24 85.65 Methods (1) KeLP (2) Beihang-MSRA (3) ECNU (4) LSTM (5) LSTM-subject-body (6) QCN Comparisons on the SemEval 2015 • JAIST (Tran et al., 2015): It used an SVM classifier to incorporate various kinds of features , including topic model based features and word vector representations. • HITSZ-ICRC (Hou et al., 2015): It proposed ensemble learning and hierarchical classification method to classify answers. • Graph-cut (Joty et al., 2015): It modeled the relationship between pairs of answers at any distance in the same question thread, based on the idea that similar answers should have similar labels. • FCCRF (Joty et al., 2016): It used locally learned classifiers to predict the label for each individual node, and applied fully connected CRF to make global inference. Table 4: dataset. MAP 88.43 88.24 86.72 86.32 87.11 88.51 F1 69.87 68.40 77.67 74.41 74.50 78.11 Acc 73.89 51.98 78.43 75.69 77.28 80.71 Com"
P18-1162,D15-1068,0,0.385032,"ry words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], and batch size [8, 16, 32]. We take the best configuration based on performance on the development set, and only evaluate that configuration on the test set. In order to mitigate the class imbalance problem, median frequency balancing Eigen and Fergus (2015) is used to reweight each class in the cross-entropy loss. Therefore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but"
P18-1162,N16-1084,0,0.471171,"ordinary QA does not possess. First, a question includes both a subject that gives a brief summary of the question and a body that describes the question in detail. The questioners usually convey their main concern and key information in the question subject. Then, they provide more extensive details about the subject, seek help, or express gratitude in the question body. Second, the problem of redundancy and noise is prevalent in CQA (Zhang et al., 2017). Both questions and answers contain auxiliary sentences that do not provide meaningful information. Previous researches (Tran et al., 2015; Joty et al., 2016) usually treat each word equally in the question and answer representation. However, due to the redundancy and noise problem, only part of text from questions and answers is useful to determine the answer quality. To make things worse, they ignored the difference between question subject and body, and simply concatenated them as the question representation. Due to the subject-body relationship described above, this simple concatenation can aggravate the redundancy problem in the question. In this paper, we propose the Question Condensing Networks (QCN) to address these problems. In order to ut"
P18-1162,S17-2003,0,0.123451,"Missing"
P18-1162,S15-2047,0,0.104987,"ry words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], and batch size [8, 16, 32]. We take the best configuration based on performance on the development set, and only evaluate that configuration on the test set. In order to mitigate the class imbalance problem, median frequency balancing Eigen and Fergus (2015) is used to reweight each class in the cross-entropy loss. Therefore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but"
P18-1162,S16-1083,0,0.253621,"estion representation using subject-body relationship. In most cases, the question subject can be seen as a summary containing key points of the question, the question body is relatively lengthy in that it needs to explain the key points and add more details about the posted question. We propose to cheat the question subject as the primary part of the question representation, and aggregate question body information from two perspectives: similarity and disparity with the question subject. To achieve this goal, we use an orthogonal decomposition strategy, which is first proposed by Wang et al. (2016), to decompose each question body embedding into a parallel component and an orthogonal compobi,j para = bjemb · siemb i s siemb · siemb emb j i,j bi,j orth = bemb − bpara (1) (2) All vectors in the above equations are of length d. Next we describe the process of aggregating the question body information based on the parallel component in detail. The same process can be applied to the orthogonal component, so at the end of the fusion gate we can obtain Sorth and Sorth respectively. The decomposed components are passed through a fully connected layer to compute the multi-dimensional attention w"
P18-1162,D14-1162,0,0.0811374,"S, B, C). 3 Proposed Model In this paper, we propose Question Condensing Networks (QCN) which is composed of the following modules. The overall architecture of our model is illustrated in Figure 1. 3 An implementation of our model is available at https: //github.com/pku-wuwei/QCN. 1747 ???? ???? ??????? ???? ???? ?????? ?????? MLP ???? ? ????????ℎ ???? ?r?? ? ???? ???? Figure 1: Architecture for Question Condensing Network (QCN). Each block represents a vector. 3.1 Word-Level Embedding nent based on every question subject embedding: Word-level embeddings are composed of two components: GloVe (Pennington et al., 2014) word vectors trained on the domain-specific unannotated corpus provided by the task 4 , and convolutional neural network-based character embeddings which are similar to (Kim et al., 2016). Web text in CQA forums differs largely from normalized text in terms of spelling and grammar, so specifically trained GloVe vectors can model word interactions more precisely. Character embedding has proven to be very useful for out-of-vocabulary (OOV) words, so it is especially suitable for noisy web text in CQA. We concatenate these two embedding vectors for every word to generate word-level embeddings Se"
P18-1162,S15-2038,0,0.3477,"istics of CQA that ordinary QA does not possess. First, a question includes both a subject that gives a brief summary of the question and a body that describes the question in detail. The questioners usually convey their main concern and key information in the question subject. Then, they provide more extensive details about the subject, seek help, or express gratitude in the question body. Second, the problem of redundancy and noise is prevalent in CQA (Zhang et al., 2017). Both questions and answers contain auxiliary sentences that do not provide meaningful information. Previous researches (Tran et al., 2015; Joty et al., 2016) usually treat each word equally in the question and answer representation. However, due to the redundancy and noise problem, only part of text from questions and answers is useful to determine the answer quality. To make things worse, they ignored the difference between question subject and body, and simply concatenated them as the question representation. Due to the subject-body relationship described above, this simple concatenation can aggravate the redundancy problem in the question. In this paper, we propose the Question Condensing Networks (QCN) to address these prob"
P18-1162,C16-1127,0,0.030014,"dense the question representation using subject-body relationship. In most cases, the question subject can be seen as a summary containing key points of the question, the question body is relatively lengthy in that it needs to explain the key points and add more details about the posted question. We propose to cheat the question subject as the primary part of the question representation, and aggregate question body information from two perspectives: similarity and disparity with the question subject. To achieve this goal, we use an orthogonal decomposition strategy, which is first proposed by Wang et al. (2016), to decompose each question body embedding into a parallel component and an orthogonal compobi,j para = bjemb · siemb i s siemb · siemb emb j i,j bi,j orth = bemb − bpara (1) (2) All vectors in the above equations are of length d. Next we describe the process of aggregating the question body information based on the parallel component in detail. The same process can be applied to the orthogonal component, so at the end of the fusion gate we can obtain Sorth and Sorth respectively. The decomposed components are passed through a fully connected layer to compute the multi-dimensional attention w"
P18-1162,S17-2060,0,0.0318471,"Missing"
P18-1162,C16-1117,0,0.0267716,"Missing"
P18-2115,I13-1041,0,0.0464905,"Missing"
P18-2115,D17-1222,0,0.0377903,"ies are extracted from the training sets, and the source contents and the summaries share the same vocabularies. In order to alleviate the risk of word segmentation mistakes, we split the Chinese sentences into characters. We prune the vocabulary size to 4,000, which covers most of the common characters. We tune the hyper-parameters based on the ROUGE scores on the validation sets. We set the word embedding size and the hidden size to 512, and the number of LSTM layers is 2. The batch size is 64, and we do not use dropout (Srivastava et al., 2014) on this dataset. Following the previous work (Li et al., 2017), we implement the beam search, and set the beam size to 10. Experiments Following the previous work (Ma et al., 2017), we evaluate our model on a popular Chinese social media dataset. We first introduce the datasets, evaluation metrics, and experimental details. Then, we compare our model with several state-of-the-art systems. 3.1 3.4 Baselines We compare our model with the following stateof-the-art baselines. • RNN and RNN-cont are two sequence-tosequence baseline with GRU encoder and decoder, provided by Hu et al. (2015). The difference between them is that RNN-context has attention mechani"
P18-2115,N03-1020,0,0.15914,"D (y = 0|zt ) (4) − log PθD (y = 1|zs ) pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. When minimizing the supervision objective, we only update the parameters of the encoders. 2.4 3.2 Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. Loss Function and Training There are several parts of the objective functions to optimize in our models. The first part is the cross entropy losses of the sequence-t"
P18-2115,P16-1046,0,0.020242,"he source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1 1 Introduction Text summarization is to produce a brief summary of the main ideas of the text. Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries. Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder. 2 Proposed Model We introduce our proposed model in detail in this section. 2.1 Notation Given a summarization dataset that consists of N data samples, the ith data sampl"
P18-2115,D15-1166,0,0.0372553,"tasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014). Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks. Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation. Neural attention model is first proposed by Bahdanau et al. (2014). Table 3: A summarization example of our model, compared with Seq2Seq and the reference. the seq2seq model with the text-summary pairs until convergence. Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the parameters of the encoders. T"
P18-2115,N16-1012,0,0.0252682,"ral model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014). Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks. Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation. Neural attention model is first proposed by Bahdanau et al. (2014). Table 3: A summarization example of our model, compared with Seq2Seq and the reference. the seq2seq model with the text-summary pairs until convergence. Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the parameters of the encoders. The classifier is a simple feedforward neural network which m"
P18-2115,N18-1018,1,0.772234,"Missing"
P18-2115,P16-1154,0,0.0185662,"Airport. Some passengers asked for a security check but were denied by the captain, which led to a collision between crew and passengers. Reference: 航班多人吸烟机组人员与乘客 冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Seq2Seq: 中联航空机场发生爆炸致多人死 亡。 China United Airlines exploded in the airport, leaving several people dead. +superAE: 成都飞北京航班多人吸烟机组 人员与乘客冲突。 Several people smoked on a flight from Chendu to Beijing, which led to a collision between crew and passengers. propose a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents. See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism. Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic rel"
P18-2115,D15-1229,0,0.330206,"inimizing the discriminator objective, we only train the parameters of the discriminator, while the rest of the parameters remains unchanged. The supervision objective to be against the discriminator can be written as: LG (θE ) = − log PθD (y = 0|zt ) (4) − log PθD (y = 1|zs ) pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. When minimizing the supervision objective, we only update the parameters of the encoders. 2.4 3.2 Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and RO"
P18-2115,P17-2100,1,0.543419,"share the same points, it is possible to supervise the learning of the semantic representation of the source content with that of the summary. In this paper, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. First, we train an autoencoder, which inputs and reconstructs the summaries, to obtain a better representation to generate the summaries. Then, we supervise the internal representation of Seq2Seq with that of autoencoder by minimizing the distance between two representations. Finally, we use adversarial learning to enhance the supervision. Following the previous work (Ma et al., 2017), We evaluate our proposed model on a Chinese social media dataset. Experimental results show that our model outperforms the state-of-theart baseline models. More specifically, our model outperforms the Seq2Seq baseline by the score of 7.1 ROUGE-1, 6.1 ROUGE-2, and 7.0 ROUGE-L. Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well wri"
P18-2115,P15-1001,0,0.0233635,"of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014). Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks. Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation. Neural attention model is first proposed by Bahdanau et al. (2014). Table 3: A summarization example of our model, compared with Seq2Seq and the reference. the seq2seq model with the text-summary pairs until convergence. Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the paramete"
P18-2115,K16-1028,0,0.0195193,"ome passengers asked for a security check but were denied by the captain, which led to a collision between crew and passengers. Reference: 航班多人吸烟机组人员与乘客 冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Seq2Seq: 中联航空机场发生爆炸致多人死 亡。 China United Airlines exploded in the airport, leaving several people dead. +superAE: 成都飞北京航班多人吸烟机组 人员与乘客冲突。 Several people smoked on a flight from Chendu to Beijing, which led to a collision between crew and passengers. propose a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents. See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism. Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic rel"
P18-2115,P10-1058,0,0.0337129,"hares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1 1 Introduction Text summarization is to produce a brief summary of the main ideas of the text. Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries. Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder. 2 Proposed Model We introduce our proposed model in detail in this section. 2.1 Notation Given a summarization dataset that consists of N data sa"
P18-2115,radev-etal-2004-mead,0,0.116212,"tten. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1 1 Introduction Text summarization is to produce a brief summary of the main ideas of the text. Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries. Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder. 2 Proposed Model We introduce our proposed model in detail in this section. 2.1 Notation Given a summarization dataset"
P18-2115,P18-1090,1,0.81,"Missing"
P18-2115,D15-1044,0,0.705745,"irs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. When minimizing the supervision objective, we only update the parameters of the encoders. 2.4 3.2 Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. Loss Function and Training There are several parts of the objective functions to optimize in our models. The first part is the cross entropy losses of the sequence-to-sequence and the autoencoder: N X LSeq2seq = − pSeq2seq (yi |zs ) (5) i=1 LAE = − N X pAE (yi |zt ) Evaluation Metric (6) i=1 3.3 The second part is the L2 loss of the supervision, as written in Equation 1. The last part is the adversarial learning, which are Equation 3 and Equation 4."
P18-2115,P17-1099,0,0.0300922,"ce: 航班多人吸烟机组人员与乘客 冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Seq2Seq: 中联航空机场发生爆炸致多人死 亡。 China United Airlines exploded in the airport, leaving several people dead. +superAE: 成都飞北京航班多人吸烟机组 人员与乘客冲突。 Several people smoked on a flight from Chendu to Beijing, which led to a collision between crew and passengers. propose a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents. See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism. Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoen"
P18-2115,D16-1112,0,0.031997,"Missing"
P19-1344,S14-2051,0,0.710072,"on term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classification. It has two LSTMs equipped with extended memories, and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions (Li and Lam, 2017). • IHS R&D is the best system of laptop domain, and uses CRF with features extracted using named entity recognition, POS tagging, parsing, and semantic analysis (Chernyshevich, 2014). • CMLA is made up of multi-layer attention network, where each layer consists of a couple of attention with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms (Wang et al., 2017). • NLANGP utilizes CRF with the word, name list and word cluster feature to tackle the task and obtains the best results in the restaurant domain. It also uses the output of a Recurrent Neural Network (RNN) as additional features to enhance their performances (Toh and Su, 2016). • WDEmb first learns embeddings of words and dependency paths based on the opt"
P19-1344,W14-4012,0,0.0400851,"Missing"
P19-1344,D14-1179,0,0.0449907,"Missing"
P19-1344,P17-1036,0,0.0610237,"ntiment analysis (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). In this paper, we only focus on the ATE task, and we solve this task by Seq2Seq learning which is often used in the generative task. We will introduce the recent study progresses in ATE and Seq2Seq learning. 4.1 Aspect Term Extraction Hu and Liu (2004) first propose to evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016;"
P19-1344,C10-1074,0,0.235204,"and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other r"
P19-1344,D17-1310,0,0.817565,"zza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole sentence in advan"
P19-1344,D15-1168,0,0.765517,"nd their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole"
P19-1344,D15-1166,0,0.0448049,"2014b; Sutskever et al., 2014), and first used in the field of machine translation. In addition, Cho et al. (2014a) improves the decoding by beam-search. However, vanilla Seq2Seq model performs worse in generating long sentences. The reason is that the encoder needs to compress the whole sentence into a fix length representation. To address this problem, Bahdanau et al. (2014) introduce an attention mechanism which selects important parts of the source sentence with respect to the previous hidden state in decoding the next state. Afterward, some studies focus on improving attention mechanism (Luong et al., 2015). So far, Seq2Seq models and attention mechanism have been applied to many fields such as dialog (Serban et al., 2016) generation, text summarization (Nallapati et al., 2016) and etc. In this paper, we first attempt to formalize the ATE as a sequence-to-sequence learning task because it can make full use of both the meaning of the sentence and label dependencies compared with existing methods. Furthermore, we design a position-aware attention model and gated unit networks to make Seq2Seq model better suit to this task. Generally, Seq2Seq model is timeconsuming in many fields because the target"
P19-1344,C10-2090,0,0.119147,"timent analysis, and aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is"
P19-1344,D10-1101,0,0.337486,"nd aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the"
P19-1344,K16-1028,0,0.199335,"ng label dependencies because they only use transition matrix to encourage valid label paths and discourage other paths (Collobert et al., 2011). As we know, the label of each word is conditioned on its previous label. For example, “O” is followed by “B/O” but not “I” in the B-I-O tagging schema. To the best of our knowledge, no neural networks based method utilizes the previous label to improve their performances directly. Recently, sequence to sequence (Seq2Seq) learning has been successfully applied to many generation tasks (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014; Nallapati et al., 2016). Seq2Seq learning encodes a source sequence into a fixed-length vector based on which a decoder generates a target sequence. It just has the benefits of first collecting comprehensive information from the source text and then paying more attention to the generation of the target sequence. Thus, we propose to formalize the ATE task as a sequence-to-sequence learning problem, where 3538 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3538–3547 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the source and"
P19-1344,D14-1162,0,0.0893689,"N) model with GloVe embeddings to extract aspect-term (Xu et al., 2018). • BiLSTM-CNN-CRF is the state-of-the-art system for named entity recognition task, which adopts CNN and Bi-LSTM to learn character-level and word-level features respectively, and CRF is used to avoid the illegal transition between labels (Reimers and Gurevych, 2017). Baselines To evaluate the effectiveness of our approach, we compare our model with three groups of baselines. The first group of baselines utilizes conditional randomly fields (CRF): • CRF trains a CRF model with basic feature templates6 and word embeddings (Pennington et al., 2014) for ATE. The third group of baselines are joint methods for aspect term and opinion term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classification. It has two LSTMs equipped with extended memories, and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions (Li and Lam, 2017). • IHS R&D is the best system of laptop domain, and uses CRF with features extra"
P19-1344,P08-1036,0,0.0797458,"(ABSA) is a subfield of sentiment analysis (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). In this paper, we only focus on the ATE task, and we solve this task by Seq2Seq learning which is often used in the generative task. We will introduce the recent study progresses in ATE and Seq2Seq learning. 4.1 Aspect Term Extraction Hu and Liu (2004) first propose to evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014;"
P19-1344,S16-1002,0,0.319126,"Missing"
P19-1344,S16-1045,0,0.584081,"using named entity recognition, POS tagging, parsing, and semantic analysis (Chernyshevich, 2014). • CMLA is made up of multi-layer attention network, where each layer consists of a couple of attention with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms (Wang et al., 2017). • NLANGP utilizes CRF with the word, name list and word cluster feature to tackle the task and obtains the best results in the restaurant domain. It also uses the output of a Recurrent Neural Network (RNN) as additional features to enhance their performances (Toh and Su, 2016). • WDEmb first learns embeddings of words and dependency paths based on the optimization objective formalized as w1 + r ≈ w2 , where w1 , w2 are words, r is the corresponding dependency path. Then, the learned embeddings of words and dependency paths are utilized as features in CRF for ATE (Yin et al., 2016). 5 https://github.com/facebookresearch/ fastText 6 https://sklearn-crfsuite.readthedocs. io/en/latest/ • RNCRF 8 learns structure features for each word from parse tree by Recursive Neural Networks, and the learned features are fed to CRF to decode the label for each word (Wang et al., 20"
P19-1344,S15-2082,0,0.487521,"Missing"
P19-1344,S14-2004,0,0.503276,"here labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism. 1 Introduction Aspect term extraction (ATE) is a fundamental task in aspect-level sentiment analysis, and aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development"
P19-1344,I08-1038,1,0.716497,"evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention mode"
P19-1344,D16-1059,0,0.763714,"taff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they can"
P19-1344,D09-1159,0,0.177878,"s appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 2018) to improve their performances. 4.2 Sequence-to-Sequence Learning Sequence-to-sequence model is a generative mo"
P19-1344,P18-2094,0,0.849769,"the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole sentence in advance. In addition, n"
P19-1344,J11-1002,0,0.267231,"s in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved goo"
P19-1344,P13-1161,0,0.0264517,"; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 20"
P19-1344,D17-1035,0,0.0195966,"he second group of baselines employs neural networks methods to address the ATE problem: • Bi-LSTM applies different kinds of BiRNN (Elman/Jordan-type RNN) with different kinds of embeddings in the ATE task (Liu et al., 2015). • GloVe-CNN7 uses multi-layer Convolution Neural networks (CNN) model with GloVe embeddings to extract aspect-term (Xu et al., 2018). • BiLSTM-CNN-CRF is the state-of-the-art system for named entity recognition task, which adopts CNN and Bi-LSTM to learn character-level and word-level features respectively, and CRF is used to avoid the illegal transition between labels (Reimers and Gurevych, 2017). Baselines To evaluate the effectiveness of our approach, we compare our model with three groups of baselines. The first group of baselines utilizes conditional randomly fields (CRF): • CRF trains a CRF model with basic feature templates6 and word embeddings (Pennington et al., 2014) for ATE. The third group of baselines are joint methods for aspect term and opinion term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classificati"
P19-1344,P17-2023,0,0.0286843,"ned aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 2018) to improve their performances. 4.2 Sequence-to-Sequence Learning Sequenc"
W10-4155,W03-0405,0,0.0250612,"ed as follows. Section 2 introduces related work. Section 3 gives a detailed description about our pipeline approach. It includes preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering. Section 4 presents the experimental results. The conclusions are given in Section 5. 2 Related Work Several important studies have tried to solve the task introduced in the previous section. Most of them treated it as an clustering problem. Bagga & Baldwin (1998) ﬁrst selected tokens from local context as features to perform intra-document coreference resolution. Mann & Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low rec"
W10-4155,P04-1076,0,0.0271538,"description about our pipeline approach. It includes preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering. Section 4 presents the experimental results. The conclusions are given in Section 5. 2 Related Work Several important studies have tried to solve the task introduced in the previous section. Most of them treated it as an clustering problem. Bagga & Baldwin (1998) ﬁrst selected tokens from local context as features to perform intra-document coreference resolution. Mann & Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low recall values. In the ﬁrst stage, some reliable features (like named entiti"
W10-4155,P09-3011,1,0.658654,"tion. Most of them treated it as an clustering problem. Bagga & Baldwin (1998) ﬁrst selected tokens from local context as features to perform intra-document coreference resolution. Mann & Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low recall values. In the ﬁrst stage, some reliable features (like named entities) are used to connect documents about the same person. After that, the connected documents (document cluster) are used as a source from which new features (compound keyword features) are extracted. These new features are used in the second stage to make additional connections between documents. Their approach is to improve clusters step by"
W10-4155,P98-1012,0,\N,Missing
W10-4155,C98-1012,0,\N,Missing
W10-4173,S07-1075,0,0.0713138,"Missing"
W10-4173,E06-1018,0,0.208012,"the Laplace matrix of the affinity matrix, then reform the data points by stacking the largest eigenvectors of the Laplace matrix in columns, finally cluster the new data points using a more simple clustering method like k-means (Ng et al., 2001). 2 Methodology Our approach follows a common cluster model that represents the given context as a word vector and later uses a spectral clustering method to group each instance in its own cluster. Different types of polysemy may arise and the most significant distinction may be the syntactic classes of the word and the conceptually different senses (Bordag, 2006). Thus we must extract the features able to distinguish these differences. They are: Local tokens: the word occuring in the window -3 – +3; Local bigram feature: bigram within -5 – +5 Chinese character range; The above two features model the syntactic usage of a specific sense of a Chinese word. Topical or conceptual feature: the content words (pos-tagged as noun, verb, adjective) within the given sentence. As the sentence in the training set seems generally short, a short window may not contains enough infomation. We represent the words in a 0-1 vector according to their existence in a given"
W10-4173,E09-1013,0,0.0604599,"Missing"
W11-1922,W11-1901,0,0.0833179,"Missing"
W11-1922,M95-1005,0,0.254413,"and PRP-PRP link. Different link types may have different feature preferences. So we train the classification based pre-cluster pair model for each link type separately and use different models to predicate the results. With the predicating results for pre-cluster pairs, we use closest-first clustering to link them and form the final cluster results. 4 Experimental Results We present our evaluation results on development dataset for CoNLL-2011 shared Task in Table 3, Table 4 and Table 5. Official test results are given in Table 6 and Table 7. Three different evaluation metrics were used: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). Finally, the average scores of these three metrics are used to rank the participating systems. The difference between Table 3 and Table 4 is whether gold standard mention boundaries are given. Here ”mention boundaries” means a more broad concept than the mention definition we gave earlier. We should also detect real mentions from them. From the tables, we can see that the scores can be improved litttle by using gold standard mention boundaries. Also the results from Table 5 tell us that combining different link-type based classification mod"
W11-1922,H05-1004,0,0.200238,"Missing"
W11-1922,D08-1067,0,0.116733,"Missing"
W11-1922,D09-1101,0,0.375705,"Missing"
W11-1922,P10-1142,0,0.0354776,"Missing"
W11-1922,P06-1005,0,0.0649394,"Missing"
W12-6321,P11-1115,0,0.0536292,"Missing"
W12-6321,W12-6326,0,0.0311846,"iverse kinds of representative features, the NERD system has to determine which feature is more important. One team uses supervised method to tune the weight of different features (Tian et al., 2012), while another team uses the information gain criterion (Wei et al., 2012). Besides a good representation of both source text and knowledge base entities, there are other aspects that may benefit a NERD system. One team use model combination method: there are several rank score and each with different feature input; a classification model finally determine the relative importance of each scoring (Liu et al., 2012). Training set can be used to decide the threshold in NIL linking and tune the weight of different features and models. One team also uses the extended version of KB from Baidu Baike to enrich the feature set (Liu et al., 2012), and constructs a one-to-one mapping from Baike to KB, because most of the entities is constructed from Baike. 4.2 Analysis of difficult queries Table 4 shows detailed top/median precision/recall/f-score across all teams, for each query name. The result shows that the performance is good for most of the queries, except for a few, like “田野” “黄河” “黄莺” “黄龙”. As we did not"
W12-6321,W12-6322,0,0.0509936,"Missing"
W12-6321,W12-6328,0,0.0260328,"NIL entities (Peng et al., 2012; Zhang et al., 2012). • a separate common word detection step is used after the first entity recognition step, or after the knowledge base linking phase. There are several features which proves useful for accurate disambiguation. The features are listed as follows: • keywords: one team report extracting discriminative keywords from the KB to represent the target entities, besides using bag-of-word feature vector, and the performance is good (Zong et al., 2012). • entity of different types: person, organization, location, and other types are used by many teams (Qing-hu et al., 2012; Peng et al., 2012; Zong et al., 2012; Wang et al., 2012). One team reports cooccuring persons more discriminative than other types (Zong et al., 2012). This is reasonable since a person is largely influenced by its social relations. • entity attributes: several teams (Tian et al., 2012; Wang et al., 2012; Wei et al., 2012) extract attribute of many types, such as title, occupation, gender, nationality, graduate school, education background, publication, etc. Whether the performance is good is largely determined by the extraction technique. • representation of pseudo-entities (i.e. “Other” an"
W12-6321,W12-6325,0,0.116886,"is determined by the task requirements: • preprocessing: the KB and Source text are segmented into Chinese words, and other processing like POS-tagging and named entity recognition are alternatively used; • information extraction: keywords, entities and relevant attributes are extracted, to construct a vector representation of KB and Source text; • similarity calculation: the similarity is computed with feature vector, and entities in KB is generated by the rank score. Most teams use simply the unsupervised method to rank candidates, and some teams use semantic resources like Tongyici Cilin (Tian et al., 2012) or the Web for a better scoring; • “NIL” entity clustering: maximum similarity score below a threshold is a good sign of determining if the entity is in the KB. Hierarchical clustering method is used by many teams to group NIL entities (Peng et al., 2012; Zhang et al., 2012). • a separate common word detection step is used after the first entity recognition step, or after the knowledge base linking phase. There are several features which proves useful for accurate disambiguation. The features are listed as follows: • keywords: one team report extracting discriminative keywords from the KB to"
W12-6321,W12-6327,0,0.0295257,"eparate common word detection step is used after the first entity recognition step, or after the knowledge base linking phase. There are several features which proves useful for accurate disambiguation. The features are listed as follows: • keywords: one team report extracting discriminative keywords from the KB to represent the target entities, besides using bag-of-word feature vector, and the performance is good (Zong et al., 2012). • entity of different types: person, organization, location, and other types are used by many teams (Qing-hu et al., 2012; Peng et al., 2012; Zong et al., 2012; Wang et al., 2012). One team reports cooccuring persons more discriminative than other types (Zong et al., 2012). This is reasonable since a person is largely influenced by its social relations. • entity attributes: several teams (Tian et al., 2012; Wang et al., 2012; Wei et al., 2012) extract attribute of many types, such as title, occupation, gender, nationality, graduate school, education background, publication, etc. Whether the performance is good is largely determined by the extraction technique. • representation of pseudo-entities (i.e. “Other” and “Out n” ): one team benefits from a explicit representat"
W12-6321,W12-6324,0,0.015289,"tracting discriminative keywords from the KB to represent the target entities, besides using bag-of-word feature vector, and the performance is good (Zong et al., 2012). • entity of different types: person, organization, location, and other types are used by many teams (Qing-hu et al., 2012; Peng et al., 2012; Zong et al., 2012; Wang et al., 2012). One team reports cooccuring persons more discriminative than other types (Zong et al., 2012). This is reasonable since a person is largely influenced by its social relations. • entity attributes: several teams (Tian et al., 2012; Wang et al., 2012; Wei et al., 2012) extract attribute of many types, such as title, occupation, gender, nationality, graduate school, education background, publication, etc. Whether the performance is good is largely determined by the extraction technique. • representation of pseudo-entities (i.e. “Other” and “Out n” ): one team benefits from a explicit representation of common words and outof-KB entities (Peng et al., 2012), rather than using same set of feature for classification and clustering. They leverage the Web to discover keywords frequently occurring with common names. They further make the assumption that if all the"
W12-6321,W12-6323,0,0.059223,"Missing"
W12-6321,S07-1012,0,\N,Missing
W14-1713,D12-1052,0,0.0606624,"Missing"
W14-1713,W13-1703,0,0.0142426,"cters. For example, they may spell “believe” as “belive”. However, very few of them may spell “believe” as “pelieve” or “delieve”. In our system, we generate 10 candidates for each word. To keep the decoding of the best word sequence controllable, we do not generate candidates for every word in the original sentence. We only generate the edit distance based candidates for the following words: Task Description The CoNLL-2014 shared task focuses on correcting all errors that are commonly made by L2 learners of English. The training data released by the task organizers come from the NUCLE corpus(Dahlmeier et al., 2013). This corpus contains essays written by L2 learners of English. These essays are then corrected by English teachers. Details of the CoNLL-2014 shared task can be found in Ng et al. (2014). 3 3.1 Correction Candidate Generation System Overview Overview It is time-consuming to train individual models for each kind of errors. We believe a better way is to correct errors in a unified framework. We assume that each word in the sentence may be involved in some kinds of errors. We generate a list of correction candidates for each word. Then a Language Model (LM) is used to find the most probable wor"
W14-1713,W13-3603,0,0.0242842,"Missing"
W14-1713,W14-1701,0,0.0244101,"the decoding of the best word sequence controllable, we do not generate candidates for every word in the original sentence. We only generate the edit distance based candidates for the following words: Task Description The CoNLL-2014 shared task focuses on correcting all errors that are commonly made by L2 learners of English. The training data released by the task organizers come from the NUCLE corpus(Dahlmeier et al., 2013). This corpus contains essays written by L2 learners of English. These essays are then corrected by English teachers. Details of the CoNLL-2014 shared task can be found in Ng et al. (2014). 3 3.1 Correction Candidate Generation System Overview Overview It is time-consuming to train individual models for each kind of errors. We believe a better way is to correct errors in a unified framework. We assume that each word in the sentence may be involved in some kinds of errors. We generate a list of correction candidates for each word. Then a Language Model (LM) is used to find the most probable word sequences based on the original sentence and the correction candidates for each word. An illustrative example is shown in figure 1. Because the LM is designed for the replacement errors"
W14-1713,W13-3601,0,0.0123794,"red tasks for grammar error correction, such as the HOO shared task of 2012 (HOO-2012) and the CoNLL-2013 shared task(CoNLL-2013), focus on limited types of errors. For example, HOO-2012 only considers errors related to determiners and prepositions. CoNLL-2013 further considers errors that are related to noun number, verb form and subject-object agreement. In the CoNLL-2014 shared task, all systems should consider all the 28 kinds of errors, including errors such as spelling errors which cannot be corrected using a single classifier. Most of the top-ranked systems in the CoNLL2013 shared task(Ng et al., 2013) train individual classifiers or language models for each kind of errors independently. Although later systems such as Wu and Ng (2013); Rozovskaya and Roth (2013) use Integer Linear Programming (ILP) to decode a global optimized result, the input scores 96 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 96–102, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics preposition model can improve the performance in our experiment. training data of the two classification models also come from a large unlabele"
W14-1713,W13-3602,0,0.0741296,"e the space between words. We consider whether the space should keep empty, or insert ‘a’ or ‘the’. Therefore, 3 labels are considered to indicate ‘a’, ‘the’ and ‘NULL’. We use ‘’NULL’ to denote that the correct space does not need an article. We leave the clarification between ‘a’ and ‘an’ as a post-process by manually designed rules. We do not consider other determiners such as ‘this’ or ‘’these’ because further information such as the coreference resolution results is needed. Instead of considering all spaces in a sentence, some previous works(AEHAN et al., 2006; Rozovskaya and Roth, 2010; Rozovskaya et al., 2013) only consider spaces at the beginning of noun phrases. Compared to these methods, our system do not need a POS tagger or a phrase chunker (which is sometimes not accurate enough) to filter the positions. All the operations are done on the word level. We list the features we use in table 1. Note that for 3-grams and 4-grams we do not use all combinations of characters because it will generate more sparse features while the performance is not improved. Because there are limited amount of training data, we choose to use the English Gigaword corpus to generate training instances instead of using"
W14-1713,D13-1074,0,0.0112577,"of errors. For example, HOO-2012 only considers errors related to determiners and prepositions. CoNLL-2013 further considers errors that are related to noun number, verb form and subject-object agreement. In the CoNLL-2014 shared task, all systems should consider all the 28 kinds of errors, including errors such as spelling errors which cannot be corrected using a single classifier. Most of the top-ranked systems in the CoNLL2013 shared task(Ng et al., 2013) train individual classifiers or language models for each kind of errors independently. Although later systems such as Wu and Ng (2013); Rozovskaya and Roth (2013) use Integer Linear Programming (ILP) to decode a global optimized result, the input scores 96 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 96–102, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics preposition model can improve the performance in our experiment. training data of the two classification models also come from a large unlabeled news corpus therefore no human annotation is needed. Although we try to use a unified framework to get better performance in the grammar error correction task, t"
W14-1713,P13-1143,0,0.021114,"s on limited types of errors. For example, HOO-2012 only considers errors related to determiners and prepositions. CoNLL-2013 further considers errors that are related to noun number, verb form and subject-object agreement. In the CoNLL-2014 shared task, all systems should consider all the 28 kinds of errors, including errors such as spelling errors which cannot be corrected using a single classifier. Most of the top-ranked systems in the CoNLL2013 shared task(Ng et al., 2013) train individual classifiers or language models for each kind of errors independently. Although later systems such as Wu and Ng (2013); Rozovskaya and Roth (2013) use Integer Linear Programming (ILP) to decode a global optimized result, the input scores 96 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 96–102, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics preposition model can improve the performance in our experiment. training data of the two classification models also come from a large unlabeled news corpus therefore no human annotation is needed. Although we try to use a unified framework to get better performance in the gram"
W14-1713,W13-3604,0,0.0416832,"Missing"
W14-1713,W13-3607,0,0.0271111,"Missing"
W14-1713,N10-1018,0,\N,Missing
W14-1713,N12-1067,0,\N,Missing
W16-4919,C12-1184,0,0.0253139,"corpura, the grammatical error correction related resource for Chinese is far from enough. We are glad to see the shared tasks on CGED (Yu et al., 2014; Lee et al., 2015) in last two years. There were some previous related work for Chinese grammatical error detection or correction. Wu et al. (2010) proposed two types of language models to detect the error types of word order, omission and redundant, corresponding to three of the types in the shared task. Experimental results showed syntactic features, web corpus features and perturbation features are useful for word ordering error detection (Yu and Chen, 2012). A set of handcrafted linguistic rules with syntactic information are used to detect errors occurred in Chinese sentences (Lee et al., 2013), which are shown to achieve good results. Lee et al. (2014) introduced a sentence level judgment system which integrated several predefined rules and N-gram based statistical features. Our submission was an exploration to a neural network model in CGED which didn’t need any feature selection efforts. As a model well known for its good maintainance of both preceding and succeeding information, Bi-LSTM came to be the first choice. 3 Bi-LSTM Neural Network"
W16-4919,W14-1701,0,\N,Missing
Y03-1031,W96-0213,0,0.0501755,"Missing"
Y03-1031,W03-1713,1,\N,Missing
